volume|issue|url|title|abstract
46|-|http://www.sciencedirect.com/science/journal/13837621/46|An on-chip cache compression technique to reduce decompression overhead and design complexity|This research explores a compressed memory hierarchy model which can increase both the effective memory space and bandwidth of each level of memory hierarchy. It is well known that decompression time causes a critical effect to the memory access time and variable-sized compressed blocks tend to increase the design complexity of the compressed memory systems. This paper proposes a selective compressed memory system (SCMS) incorporating the compressed cache architecture and its management method. To reduce or hide decompression overhead, this SCMS employs several effective techniques, including selective compression, parallel decompression and the use of a decompression buffer. In addition, fixed memory space allocation method is used to achieve efficient management of the compressed blocks. Trace-driven simulation shows that the SCMS approach can not only reduce the on-chip cache miss ratio and data traffic by about 35% and 53%, respectively, but also achieve a 20% reduction in average memory access time (AMAT) over conventional memory systems (CMS). Moreover, this approach can provide both lower memory traffic at a lower cost than CMS with some architectural enhancement. Most importantly, the SCMS is a more attractive approach for future computer systems because it offers high performance in cases of long DRAM latency and limited bus bandwidth. 
46|-||Efficient parity placement schemes for tolerating up to two disk failures in disk arrays|In order to achieve high reliability in disk array systems, two new schemes using dual parity placement, called DH1 (diagonal–horizontal) and DH2 schemes, are presented. Both DH schemes can tolerate up to two disk failures by using two types of parity information placed in the diagonal and the horizontal directions, respectively, in a matrix of disk partitions. DH1 scheme can reduce the occurrences of the bottleneck problem significantly because the parity blocks are evenly distributed throughout the disk array. DH2 scheme uses one more disk than DH1 scheme in order to store the horizontal parities, while the diagonal parities are placed in the same way as in DH1 scheme with a minor change. Even though both DH schemes use almost optimal disk space for storing the redundant information, the encoding algorithms for them are quite simple and efficient. Moreover, both DH schemes can recover rapidly from any two disk failures. 
46|-||KAIST image computing system (KICS): A parallel architecture for real-time multimedia data processing|An efficient parallel architecture is proposed for high-performance multimedia data processing using multiple multimedia video processors (MVP; TMS320C80), which are fully programmable general digital signal processors (DSP). This paper describes several requirements for a multimedia data processing system and the system architecture of an image computing system called the KAIST Image Computing System (KICS). The performance of the KICS is evaluated in terms of its I/O bandwidth and the execution time for some image processing functions. An application of the KICS to the real-time Moving Picture Expert Group 2 (MPEG-2) encoder is introduced. The programmability and the high-speed data-access capability of the KICS are its most important features as a high-performance system for real-time multimedia data processing. 
46|-||Agents for information retrieval: Issues of mobility and coordination|This paper focuses on agent-based applications for information retrieval on the Web, by specifically analysing mobility and coordination issues. On the one hand, mobile agents well suit the requirements of information retrieval in the new dynamic scenario derived from the Internet. This is due to their capability of moving to the place where the information is stored – therefore saving bandwidth – and to their robustness in the presence of unreliable connections. On the other hand, the search for information by several mobile active agents calls for suitable models to rule the interactions among agents and between agents and execution environments. The paper surveys different coordination approaches and evaluates their impact in information retrieval applications based on mobile agents. The survey outlines the advantages of uncoupled coordination models and points out the suitability of a coordination model based on reactive and programmable tuple spaces: they may increase the safety and the security of the environment while simplifying the task of programming distributed mobile agent applications. 
46|-||Embedded software verification in hardwareâsoftware codesign|Concurrent Embedded Real-Time Software (CERTS) is intrinsically different from traditional, sequential, independent, and temporally unconstrained software. The verification of software is more complex than hardware due to inherent flexibilities (dynamic behavior) that incur a multitude of possible system states. The verification of CERTS is all the more difficult due to its concurrency and embeddedness. The work presented here shows how the complexity of CERTS verification can be reduced significantly through answering common engineering questions such as when, where, and how one must verify embedded software. First, a new Schedule-Verify-Map strategy is proposed to answer the when question. Second, verification under system concurrency is proposed to answer the where question. Finally, a complete symbolic model checking procedure is proposed for CERTS verification. Several application examples illustrate the usefulness of our technique in increasing verification scalability. 
46|-||A new cache architecture based on temporal and spatial locality|A data cache system is designed as low power/high performance cache structure for embedded processors. Direct-mapped cache is a favorite choice for short cycle time, but suffers from high miss rate. Hence the proposed dual data cache is an approach to improve the miss ratio of direct-mapped cache without affecting this access time. The proposed cache system can exploit temporal and spatial locality effectively by maximizing the effective cache memory space for any given cache size. The proposed cache system consists of two caches, i.e., a direct-mapped cache with small block size and a fully associative spatial buffer with large block size. Temporal locality is utilized by caching candidate small blocks selectively into the direct-mapped cache. Also spatial locality can be utilized aggressively by fetching multiple neighboring small blocks whenever a cache miss occurs. According to the results of comparison and analysis, similar performance can be achieved by using four times smaller cache size comparing with the conventional direct-mapped cache.And it is shown that power consumption of the proposed cache can be reduced by around 4% comparing with the victim cache configuration. 
46|-||Early design stage exploration of fixed-length block structured architectures|An important challenge concerning the design of future microprocessors is that current design methodologies are becoming impractical due to long simulation runs and due to the fact that chip layout considerations are not incorporated in early design stages. In this paper, we show that statistical modeling can be used to speed up the architectural simulations and is thus viable for early design stage explorations of new microarchitectures. In addition, we argue that processor layouts should be considered in early design stages in order to tackle the growing importance of interconnects in future technologies. In order to show the applicability of our methodology which combines statistical modeling and processor layout considerations in an early design stage, we have applied our method on a novel architectural paradigm, namely a fixed-length block structured architecture. A fixed-length block structured architecture is an answer to the scalability problem of current architectures. Two important factors prevent contemporary out-of-order architectures from being scalable to higher levels of parallelism in future deep-submicron technologies: the increased complexity and the growing domination of interconnect delays. In this paper, we show by using statistical modeling and processor layout considerations, that a fixed-length block structured architecture is a viable architectural paradigm for future microprocessors in future technologies thanks to the introduction of decentralization and a reduced register file pressure. 
46|-||Index|
46|-||Index|
46|1|http://www.sciencedirect.com/science/journal/13837621/46/1|Design techniques for low-power systems|Portable products are being used increasingly. Because these systems are battery powered, reducing power consumption is vital. In this report we give the properties of low-power design and techniques to exploit them on the architecture of the system. We focus on: minimizing capacitance, avoiding unnecessary and wasteful activity, and reducing voltage and frequency. We review energy reduction techniques in the architecture and design of a hand-held computer and the wireless communication system including error control, system decomposition, communication and MAC protocols, and low-power short range networks. 
46|1||Characterizing and representing workloads for parallel computer architectures1|Experimental design of parallel computers calls for quantifiable methods to compare and evaluate the requirements of different workloads within an application domain. Such methods can help establish the basis for scientific design of parallel computers driven by application needs, to optimize performance to cost. In this paper, a framework is presented for representing and comparing workloads, based on the way they would exercise parallel machines. This workload characterization is derived from parallel instruction centroid and parallel workload similarity. The centroid is a workload approximation that captures the type and amount of parallel work generated by the workload on the average. The centroid is a simple measure that aggregates average parallelism, instruction mix, and critical path length. When captured with abstracted information about communication requirements, the result is a powerful tool in understanding the requirements of workloads and their potential performance on target machines. The workload similarity is based on measuring the normalized Euclidean distance (ned) between workload centroids. It will be shown that this workload representation method outperforms comparable ones in accuracy, as well as in time and space requirements. Analysis of the NAS Parallel Benchmark workloads and their performance will be presented to demonstrate some of the applications and insight provided by this framework. This will include the use of the proposed framework for predicting the performance of real-life workloads on target machines, with good accuracy. 
46|1||Performance evaluation of a bus-based multistage multiprocessor architecture|This paper proposes and evaluates a class of interconnection networks, which provide performance comparable to a multiple bus network with considerably lower cost. These networks, referred to as hybrid networks, are formed by beginning with a multistage network and substituting buses in the second stage. Analytic models are developed to evaluate the performance of the system. The analysis includes both uniform and non-uniform distribution of requests. The results obtained are compared with simulation results. 
46|1||Scheduling optional computations for adaptive real-time systems|At present, the critical computations of real-time systems are guaranteed before run-time by performing a worst-case analysis of the system's timing and resource requirements. The result is that real-time systems are engineered to have spare capacity, under normal operation. A challenge of current research is to make use of this spare capacity, in order to satisfy requirements for adaptivity in the system. Adaptivity can be implemented by optional computations with firm deadlines, which can be guaranteed at run-time by the use of flexible scheduling. This report assumes that the algorithms which attempt to guarantee optional computations at run-time, actually run on the same processor as the optional and critical computations themselves. The report starts with a brief survey of the complex requirements for adaptivity within real-time systems. Such requirements can include task hierarchies composed of interdependent subtasks each with its own utility. Evidence is cited which indicates that the run-time support for a computational model which supports all such requirements, would incur overheads so large, that little spare capacity would remain for the optional computations themselves. Following this, the report presents a constrained computational model, which, it is claimed, could be cost-effectively supported at run-time. The model is nevertheless general enough to satisfy many of the requirements for adaptivity. The constrained model uses Best Effort Admissions Policy to arbitrate between three categories of optional computation, each with its own utility level. The viability of the constrained model is demonstrated by simulation studies which compare the performance of the model to that of First-Come-First-Served Admissions Policy. 
46|1||Extending conventional languages by distributed/concurrent exception resolution|The state of art in handling and resolving concurrent exceptions is discussed and a brief outline of all research in this area is given. Our intention is to demonstrate that exception resolution is a very useful concept which facilitates joint forward error recovery in concurrent and distributed systems. To do this, several new arguments are considered. We understand resolution as reaching an agreement among cooperating participants of an atomic action. It is provided by the underlying system to make it unified and less error prone, which is important for forward error recovery, complex by nature. We classify atomic action schemes into asynchronous and synchronous ones and discuss exception handling for schemes of both kinds. The paper also deals with introducing atomic action schemes based on exception resolution into existing concurrent and distributed languages, which usually have only local exceptions. We outline the basic approach and demonstrate its applicability by showing how exception resolution can be used in Ada 83, Ada 95 (for both concurrent and distributed systems) and Java. A discussion of ways to make this concept more object-oriented and, with the help of reflection, more flexible and useful, concludes the paper. 
46|1||Traffic analysis in a double grain Dataflow array processor|One of the main problems of Dataflow architecture is the high communication overhead. This paper is dedicated to estimation of traffic densities in the communication links of a proposed Dataflow machine having a double grain array architecture. Results show a distinct advantage of it over existing Dataflow machines so far as communication bottlenecks are concerned. The machine however, is not infinitely scaleable. 
46|10|http://www.sciencedirect.com/science/journal/13837621/46/10|Execution replay of parallel procedural programs|This article describes an execution model for the parallel procedural programming paradigm, which combines multithreading and communications. The model is used to prove sufficient conditions to guarantee the equivalence between two executions of the same program. An efficient mechanism for recording and replaying deterministically parallel procedural programs is derived from the model and implemented in a prototype. Performed on the prototype, systematic measurements of the time overhead of recording traces for replaying various program models indicate that this overhead remains very low. 
46|10||Scheduling optimization through iterative refinement|Scheduling computations with communications is the theoretical basis for achieving efficient parallelism on distributed memory systems. We generalize Graham's task-level in a manner to incorporate the effects of computation and communication. A new scheduling is proposed by combining task priority with efficient management of processor idle time. We also propose an optimization called Iterative Refinement Scheduling (IRS) that iteratively schedules the forward and backward computation graph. The task-level used in some scheduling iteration is obtained from the schedule generated in the previous iteration. Each iteration produces a new schedule and new task-levels. This approach enables searching and optimizing solutions as the result of using more refined task-level in each scheduling iteration. Evaluation and analysis of the results are carried out for different instances of communication granularities and problem parallelism. It is shown that solutions obtained out of few iterations statistically outperforms those generated by other recently proposed scheduling. IRS allows exploring a space of solutions whose size grows with the amount of parallelism and communication granularity. IRS enables optimizing the solution specially for critical instances such as fine-grain computations and large parallelism. 
46|10||Dynamic reconfiguration of node location in wormhole networks|Several techniques have been developed to increase the performance of parallel computers. Reconfigurable networks can be used as an alternative to increase the performance. Network reconfiguration can be carried out in different ways. Our research has focused on distributed memory systems with dynamic reconfiguration of node location. Briefly, this technique consists of positioning the processors in the network depending on the existing communication pattern among them, to suit the requirements of each computation.In this article, we present a dynamic reconfiguration technique for wormhole networks. We have used both a crossbar and a multistage interconnection network to implement a reconfigurable logical two-dimensional (2-D) torus topology. The reconfiguration mechanism is based on a distributed reconfiguration algorithm. The algorithm is based on a cost function that requires only local information. We discuss reconfiguration features and adjust the different parameters of the reconfiguration algorithm. We have also studied the deadlock problem in reconfigurable wormhole networks, and give details of our solution. Finally, we have evaluated the performance of this technique under several workloads. 
46|10||Read-down conflict-preserving serializability as a correctness criterion for multilevel-secure optimistic concurrency control: CSR/RD|It is important to note that conflict-preserving serializability and related theory are artifacts of an era of database development where correctness alone was the overriding concern. With the advent of multilevel-secure databases, there is clearly a need to reexamine such theories. Any correctness criterion to govern transaction processing in the multilevel security context has to incorporate both secureness and correctness in a unified manner. This paper makes original contributions in two different but closely related areas to the optimistic concurrency control in multilevel-secure, single-version databases. First, read-down conflict-preserving serializability (CSR/RD) captures multilevel-secure database consistency requirements and secure transaction correctness properties via a single notion. Second, it presents a multilevel-secure optimistic concurrency control (MLS/OCC) scheme that has several desirable properties: If lower-level transactions were somehow allowed to continue with their executions in spite of the conflict with high-level transactions, covert timing-channel freeness would be satisfied. This sort of optimistic approach for conflict insensitiveness and the properties of non-blocking and deadlock freedom make the optimistic concurrency control scheme especially attractive to multilevel-secure transaction processing. 
46|10||Broadcast directory: A scalable cache coherent architecture for mesh-connected multiprocessors|Large-scale shared memory multiprocessors favor a directory-based cache coherence scheme for its scalability. The directory space needed to record the information for sharers has a complexity of Î(N2) when a full-mapped vector is used for an N-node system. Although this overhead can be reduced by limiting the directory size assuming that the sharing degree is small, it will experience significant inefficiency when data is widely shared.In this paper, we propose a new directory scheme and a cache coherence scheme based on it for a mesh interconnection. Deterministic and wormhole routing enables a pointer to represent a set of nodes. Also a message traversing on the mesh performs a broadcast mission to a set of nodes without extra traffic, which can be utilized for the cache coherence protocol. Only a slight change on a typical router is needed to implement our scheme. This scheme is also applicable to any k-ary n-cube networks including a mesh.The Splash-2 parallel program suite is used in the simulation study, where our scheme is compared with other directory-based schemes. Our scheme is proved to generate much less traffic for cache coherence while the space complexity is more scalable . 
46|10||Efficient path-based multicast in wormhole-routed mesh networks|The capability of multidestination wormhole allows a message to be propagated along any valid path in a wormhole-routed network conforming to the underlying base routing scheme. The multicast on the path-based routing model is highly dependent on the spatial locality of destinations participating in multicasting. In this paper, we propose two proximity grouping schemes for efficient multicast in wormhole-routed mesh networks with multidestination capability by exploiting the spatial locality of the destination set. The first grouping scheme, graph-based proximity grouping, is proposed to group the destinations together with locality to construct several disjoint sub-meshes. This is achieved by modeling the proximity grouping problem to graph partitioning problem. The second one, pattern-based proximity grouping, is proposed by the pattern classification schemes to achieve the goal of the proximity grouping. By simulation results, we show the routing performance gains over the traditional Hamiltonian-path routing scheme. 
46|10||Measurement based analysis of temporal behaviour as support for scheduling problems in parallel and distributed real-time systems|Static analysis, based on scheduling techniques, provides the most typical approach for validation of real-time systems. However, in the case of complex real-time systems such as parallel and distributed systems, many simplifications are made in order to make analysis tractable. This means that even if the system can be statically validated, the real behaviour of the system in execution may be different enough from its theoretical behaviour to make it invalid. In these cases, an analysis based on measurement of the system in execution constitutes an invaluable aid to the static analysis. This article describes a methodology for the analysis of the temporal behaviour of parallel and distributed real-time systems with end-to-end constraints. The analysis is based on the measurement of a prototype of the system in execution and is supported by a behavioural model. The main components of the model are the sequences of activities throughout the system tasks (transactions), which are carried out in response to input events, causing the corresponding output events. Thus, the temporal behaviour of the system is viewed as a set of real-time transactions competing for the available resources. This article also includes experimental results of applying the methodology to the analysis of a well-known case study. 
46|10||Parameterization of efficient dynamic reconfigurable trees|We present an algorithm for dynamic reconfiguration from a set of processor nodes connected using a multistage interconnection network into a set of m-ary trees of height h. The algorithm allows parameterization based on the branching factor m, the height of the tree h and bias B and produces a set of isomorphic trees for each value of the bias B. The computation of the identities of the neighbors by the nodes is performed using simple binary operations in parallel. 
46|11|http://www.sciencedirect.com/science/journal/13837621/46/11|Chained backplane communication architecture for scalable multiprocessor systems|A scalable backplane topology which allows a practically unlimited number of modules with identical interfaces is presented. Short, buffered, point-to-point connections overcome clock skew problems. Synchronized, pipelined data transfer operations ensure high throughput and reasonably low latency times for fine-grain parallel algorithms. A simple bus interface logic without any special hardware configuration guarantees a cheap implementation with standard FPGAs. The measured performance in our FPGA based prototype with 32 bit wide data bus shows a throughput of 160 Mbytes/s for each module with 75 ns latency time between modules. 
46|11||Distributed vector architectures|Integrating processors and main memory is a promising approach to increase system performance. Such integration provides very high memory bandwidth that can be exploited efficiently by vector operations. However, traditional vector applications would easily overflow the limited memory of a single integrated node. To accommodate such workloads, we propose the Distributed Vector Architecture (DIVA), that uses multiple vector-capable processor/memory nodes in a distributed shared-memory configuration, while maintaining the simple vector programming model. The advantages of our approach are twofold: (i) we dynamically parallelize the execution of vector instructions across the nodes, (ii) we reduce external traffic, by mapping vector computation – rather than data – across the nodes. We propose run-time mechanisms to assign elements of the architectural vector registers on nodes, using the data layout across the nodes as a blueprint. We describe DIVA implementations with a traditional request-response memory model and a data-push model. Using traces of vector supercomputer programs, we demonstrate that DIVA generates considerably less external traffic compared to single or multiple-node alternatives that are based solely on caching or paging. With timing simulations we show that a DIVA system with 2–8 nodes is up to three times faster than a single node using its local memory as a large cache and can even outperform a hypothetical system where the application fits in local memory. 
46|11||Fixed priority scheduling of tasks with arbitrary precedence constraints in distributed hard real-time systems|This paper considers the schedulability analysis of real-time distributed applications where tasks may present arbitrary precedence relations. It is assumed that tasks are periodic or sporadic and dynamically released. They have fixed priorities and hard end-to-end deadlines that are equal to or less than the respective period. We develop a method to transform arbitrary precedence relations into release jitter. By eliminating all precedence relations in the task set one can apply any available schedulability test that is valid for independent task sets. 
46|11||An output queueing analysis of multipath ATM switches|The performance of output buffers in multipath ATM switches is closely related to the output traffic distribution, which characterizes the packet arrival rate at each output link connected to the output buffer of a given output port. Many multipath switches produce nonuniform output traffic distributions even if the input traffic patterns are uniform. Focusing on the nonuniform output traffic distribution, we analyze the output buffer performances of several multipath switches under both uniform and nonuniform input traffic patterns. It is shown that the output traffic distributions are different for the various multipath switches and the output buffer performance measured in terms of packet loss probability and mean waiting time improves as the nonuniformity of the output traffic distribution becomes higher. 
46|11||Performance evaluation of system architectures with validated input data|In this paper we have extended our methodology, presented earlier in [1], for generating and validating representative traces. Our novel technique was applied to a relatively realistic and difficult multimedia benchmark suite called MediaMark. We have also introduced a new metric called the K-metric (Khalid – metric) that was used for validation. The aim of our present research was to demonstrate that the proposed methodology can be successful even for complex and challenging benchmarks like multimedia benchmarks. Earlier, our methodology was shown to be the most successful one as compared to the popular contemporary techniques for tracing relatively simple and primitive suite of applications contained within SPEC95 benchmark suite  [1]. Experimental results in this article demonstrate that our methodology works even in the worst case scenarios. 
46|11||An efficient implementation of tree-based multicast routing for distributed shared-memory multiprocessors|This paper presents an efficient routing and flow control mechanism to implement multidestination message passing in wormhole networks. The mechanism is a variation of tree-based multicast with pruning to recover from deadlocks and it is well suited for distributed shared-memory multiprocessors (DSMs) with hardware cache coherence. It does not require any preprocessing of multicast messages reducing notably the software overhead required to send a multicast message. Also, it allows messages to use any deadlock-free routing function. The new scheme has been evaluated by simulation using synthetic loads. It achieves multicast latency reductions of 30% on average. Also it was compared with other multicast mechanisms proving its benefits. Finally, it can be easily implemented in hardware with minimal changes to existing unicast wormhole routers. 
46|11||Performance of simultaneous multithreaded multimedia-enhanced processors for MPEG-2 video decompression|This paper explores microarchitecture models for a simultaneous multithreaded (SMT) processor with multimedia enhancements. We start with a wide-issue superscalar processor, enhance it by the SMT technique, by multimedia units, and by an additional on-chip RAM storage. Our workload is a multithreaded MPEG-2 video decompression algorithm that extensively uses multimedia units. The simulations show that a single-threaded, 8-issue maximum processor (assuming an abundance of resources) reaches an instructions per cycle (IPC) count of only 1.60, while an 8-threaded 8-issue processor is able to reach an IPC of 6.07. A more realistic processor model reaches an IPC of 1.27 in the single-threaded 8-issue vs 3.03 in the 4-threaded 4-issue and 3.21 in the 8-threaded 8-issue modes. Our conclusion on next generation’s microprocessors is that a 2- or 4-threaded 4-issue processor with a small on-chip RAM accessed by a local load/store unit will be superior to a wide-issue (single-threaded) superscalar processor at least for MPEG-2 style video decompression algorithms. 
46|11||Efficient module selections for finding highly acceptable designs based on inclusion scheduling|In high level synthesis, module selection, scheduling, and resource binding are inter-dependent tasks. For a selected module set, the best schedule/binding should be generated in order to accurately assess the quality of a module selection. Exhaustively enumerating all module selections and constructing a schedule and binding for each one of them can be extremely expensive. In this paper, we present an iterative framework, called WiZard to solve module selection problem under resource, latency, and power constraints. The framework associates a utility measure with each module. This measurement reflects the usefulness of the module for a given a design goal. Using modules with high utility values should result in superior designs. We propose a heuristic which iteratively perturbs module utility values until they lead to good module selections. Our experiments show that by keeping modules with high utility values, WiZard can drastically reduce the module exploration space (approximately 99.2% reduction). Furthermore, the module selections formed by these modules belong to superior solutions in the enumerated set (top 15%). 
46|12|http://www.sciencedirect.com/science/journal/13837621/46/12|YOMNA â An efficient deadlock-free multicast wormhole algorithm in 2-D mesh multicomputers|A mesh network is a popular architecture which has been implemented in many multicomputer systems. It is preferred because it offers useful edge connectivity and is partitioned into units that are still meshes. It is also scalable and has a number of features that make it particularly amenable to high-performance computing. The 2-D mesh topology has become increasingly important to multicomputer design because of its many desirable properties including scalability, low bandwidth and fixed degree of nodes.The essential pattern in new multicomputer generations is the multicast wormhole pattern, which corresponds to one-to-many communication in which one source sends the same message to multiple destination nodes. In wormhole routing, a message is decomposed into words or flits, and flits of one message may be spread out among several nodes. Deadlock in the interconnection network occurs when no message can advance towards its destination. Some deadlock-free routing algorithms for wormhole routing were proposed, but the network latency and the network traffic were not taken into consideration. An optimal message routing should achieve both minimum traffic and minimum latency for the communication patterns involved. Unfortunately, finding optimal message routing has been shown to be NP-hard for most common multicomputer topologies.In this paper, an efficient algorithm (YOMNA) is introduced to find a deadlock-free multicast wormhole routing in 2-D mesh parallel machines. YOMNA algorithm is a tree-based technique, in which the router simultaneously sends incoming flits on more than one outgoing channel. YOMNA algorithm is compared with the dual-path multicast routing, which is a path-based technique. YOMNA algorithm has proved to be deadlock free. The network latency and the network traffic are calculated for YOMNA algorithm and for the dual-path multicast routing. The results demonstrate that YOMNA algorithm outperformed the dual-path routing. 
46|12||Stride prefetching for the secondary data cache|A prefetch method that enables stride prefetching at the secondary cache without accessing the processor's internal resources is developed and evaluated. It uses a data-range-table that enables it to detect usable strides and memory access streams which fall into the same data range. By using program driven simulation of scientific applications in the context of shared-memory multiprocessors, it is shown that the proposed method can reduce load stall times by an amount comparable to a conventional stride driven prefetching method which requires access to the processor's instruction address register. 
46|12||On the relative performance merits of hypercube and hypermesh networks|Topology and routing algorithm are among the most important factors that greatly influence network performance. This paper assesses the interaction of these factors on two related but distinct types of multicomputer networks, the hypercube and hypermesh. This study shows that the routing algorithm can have a great influence on deciding the outcome of any comparison between competing network topologies. The results reveal that deterministic routing favours the hypermesh due to its smaller diameter which reduces considerably message blocking compared to the hypercube. However, adaptive routing favours the hypercube as it can benefit from its multiple paths to overcome the degrading effects of its high diameter. 
46|12||Design of a large-scale Gbit/s MAN using a cyclic reservation-based MAC protocol|In this paper, a large-scale Gbit/s metropolitan area network (MAN) based on hierarchical ring topologies has been investigated. The network is constituted by backbone and local rings, which are connected by bridges. In the network, traffic congestion may occur in bridges due to the mismatch of transmission speed between backbone and local rings. To cope with the issue, a scheme based on a cyclic reservation-based access method is proposed for the medium access control (MAC) protocol of the network. With the scheme, the protocol can both achieve the fair access of network bandwidth and resolve the traffic congestion in bridges. Particularly, this approach reduces the implementation complexity and cost and enhances efficiency of the global network. Finally, several simulative experiments are performed, and some optimistic results are revealed. 
46|12||Symbolic forward/backward traversals of large finite state machines|Symbolic traversals are state-of-the-art techniques for proving the input/output equivalence of finite state machines. Due to state space explosion, they are currently limited to medium-small circuits. Starting from the limits of standard techniques, this paper presents a mix of approximate forward and exact backward traversals that results in an exact exploration of the state space of large machines. This is possible, thanks to efficient pruning that restricts the search space during backward traversal using the information coming from the approximate forward traversal step. Experimental results confirm that we are able to explore for the first time some of the larger ISCAS'89 and MCNC circuits, that have been until now outside the scope of exact symbolic techniques. We are also able to generate the test patterns for or to tag as undetectable stuck-at faults with few exceptions. 
46|13|http://www.sciencedirect.com/science/journal/13837621/46/13|Rapid prototyping of an ATM programmable associative operator|In this paper, we describe a semi-automatic method for designing a programmable architecture related to high speed communication protocols. A case study of associative based architecture of high speed communication system is presented with a validation environment. The environment provides an interesting estimation using XILINX prototyping board including memories (content addressable memory, CAM, RAM, DPRAM). In our approach, we try to perform a rapid prototyping of such architecture and allow the designer to interact easily in order to customize the architecture according to application requirements. This method of validation provides important benefits in hardware prototyping: better validation environment and reduced time to give a real estimation for a large variety of applications. 
46|13||Role-based access control in DCOM|The explosive growth of the Web, the increasing popularity of PCs and the advances in high-speed network access have brought distributed computing into the mainstream. To simplify network programming and to realize component-based software architecture, distributed object models have emerged as standards. One of those models is distributed component object model (DCOM) which is a protocol that enables software components to communicate directly over a network in a reliable, and efficient manner. In this paper, we investigate an aspect of DCOM concerning software architecture and security mechanism. Also, we describe the concept of role-based access control (RBAC) which began with multi-user and multi-application on-line systems pioneered in the 1970s. And we investigate how we can enforce the role-based access control as a security provider within DCOM, specially in access security policy. 
46|13||Efficient approaches for constructing a massively parallel processing system|How to construct a massively parallel processing system has drawn a lot of attention. An important feature affecting the performance and characteristics of the architecture with an interconnection of multiple processors is its configuration design and scalability of the system. Good performance of a parallel application is often the result of an appropriate match between the processes of the parallel application and the configuration of multiple processor system. In order to accommodate different parallel applications, it is highly desirable that the configuration of the architecture and the number of processors can be modified easily and extended flexibly. Two efficient technical approaches for constructing a massively parallel processing system with scalability are proposed. One is based on multilayered motherboards and the other on crossbar switches. The technique and the two approaches have been adapted in a real-life application which is a parallel implementation of backpropagation neural computation. Performance evaluation for the two approaches is included. 
46|13||A scheduling policy for preserving cache locality in a multiprogrammed system|In a multiprogrammed system, when the operating system switches contexts, in addition to the cost for handling the processes being swapped out and in, the cache performance of processors also can be affected. If frequent context switching replaces the data loaded into cache memory before they are completely reused, the programs suffer from cache misses due to the damage in cache locality. In particular, for the programs with good cache locality, such as blocked programs, a scheduling mechanism of keeping cache locality against context switching is essential to achieve good processor utilization. To solve this requirement, we propose a preemption-safe policy to exploit the cache locality of blocked programs in a multiprogrammed system. The proposed policy delays context switching until a block is fully reused, but also compensates for the monopolized processor time on processor scheduling mechanisms. Our simulation results show that in a situation where blocked programs are run on multiprogrammed shared-memory multiprocessors, the proposed policy improves the performance of these programs due to a decrease in cache misses. In such situations, it also has a beneficial impact on the overall system performance due to the enhanced processor utilization. 
46|13||Analytical modeling of multithreaded architectures|Multithreading is used for hiding long memory latency in uniprocessors and multiprocessor computer systems and aims at increasing system efficiency. In such an architecture, a number of threads are allocated to each processing element (PE) and whenever a running thread becomes suspended the PE switches to another ready thread.In this paper, we discuss analytical modeling of coarsely multithreaded architectures and present two analytical models: (i) a deterministic model, where the timing parameters (e.g., context switching time, threads's run length, and memory latency) are assumed to be constant, and (ii) a stochastic model where the timing parameters are random variables.Both models provide a framework to study the dependence of the MTA efficiency on design parameters of the target architecture and its workload. The deterministic model, as well as asymptotic bounding analysis of the stochastic model, allows to determine upper bounds and some break points of the MTA efficiency such as stability (saturation) points, whereas the stochastic model provides more accurate prediction of the efficiency. 
46|13||Quantitative evaluation of pipelining and decoupling a dynamic instruction scheduling mechanism|As instruction window size increases, it becomes difficult to maintain processor cycle time. Pipelining the window is not a solution, since it is said that it affects processor performance seriously. However, the pipelining has not been evaluated quantitatively. On the other hand, recent interests on data speculation demand to increase the window size to realize instruction reissue mechanism which deals with misspeculations. For reducing the window size with maintaining the instruction reissue capability, we propose to decouple the reissue mechanism from the scheduling mechanism. Based on simulations, we have evaluated the pipelining and the decoupling of the instruction window. 
46|13||Optimal bandwidth allocation and stability of high-speed networks for CSMA/CD protocols|In this paper, we examine a non-persistent CSMA/CD protocol for high-speed communication systems, where the bandwidth is divided into two separate asymmetric channels. Free stations access the first channel while all retransmissions occur in the second channel. We define the stability regions and the rules for optimal bandwidth allocation among the two channels for optimization of the system performance in case of infinite population. Numerical results show that the optimal behaviour gives performance improvement as compared with the single-channel system with the same capacity. 
46|13||On the derivation of a correct deadlock free communication kernel for loop connected message passing architecture from its user's specification|Method for the derivation of a correct deadlock free communication kernel for loop connected message passing architecture from its user's specification is described. Dijkstra's weakest pre-condition approach is used as the specification language. 
46|14|http://www.sciencedirect.com/science/journal/13837621/46/14|Profiling in the ASP codesign environment|Automation of the hardware/software codesign (HSC) methodology brings with it the need to develop sophisticated high-level profiling tools. This paper presents a profiling tool which uses execution profiling on standard C code to obtain accurate and consistent times at the level of individual compound code sections. This tool is used in the ASP hardware/software codesign project. The results from this tool show that profiling must be performed on dedicated hardware which is as close as possible to the final implementation, as opposed to a workstation. Further, in this paper a formula is derived for the number of times a program has to be profiled in order to get an accurate estimate of the number of times a loop with an indeterminate loop count is executed. 
46|14||Architecture for fractal image compression|The algorithms for fractal image compression impose a heavy demand on the processor's arithmetic unit and the memory interface, failing to utilize the full capabilities of a general-purpose processor. The repetitive nature of the algorithm indicates that parallelization would reduce the time complexity of the otherwise expensive encoding scheme. In this paper, the design of an ASIC for FIC is proposed. This exploits the fact that the algorithm requires only integer arithmetic with repetitive use of the same set of data. Controlled parallelism is introduced by way of multiple functional units. These result in an encoding time 75 times faster than the high-level software implementation and 22 times faster than the assembly level implementation on a DSP processor. 
46|14||A section cache system designed for VLIW architectures|The static specification of operations executed in parallel using No Operations (NOPs) is another culprit to make code size to be increased in VLIW architecture. Some alternatives in the instruction encoding and memory subsystem are proposed to minimize the impact of NOP on the code size. One is the compressed cache using the packed encoding scheme and the other is the decompressed cache using the unpacked encoding scheme. The compressed cache shows high memory utilization but increases the pipeline branch penalty because it requires very complex fetch hardware. On the contrary, the fetch overhead can be decreased in the decompressed cache because the unpacked encoding scheme allows an instruction to be issued to the pipeline without any recovery process. However, it has a shortcoming that the memory utilization is deteriorated due to the memory allocation irrespective of the number of useful operations. In this research, a new instruction encoding scheme called a semi-packed encoding scheme and the section cache, which enables effective store and retrieval of semi-packed instructions, are proposed. This can decrease the hardware complexity to fetch an instruction and the wasted memory space due to NOPs via the partially fixed length of an instruction. The experimental results reveal that the memory utilization in the section cache is 3.4 times higher than in the decompressed cache. The memory subsystem using the section cache can provide about 15% performance improvement with the moderate size of chip area. 
46|14||Synthesising an asynchronous DMA controller with Balsa|A DMA controller has been designed and implemented as part of the AMULET3i asynchronous microprocessor macrocell using a mixture of synchronous and asynchronous circuit techniques. The synthesis language Balsa has been used to implement the major part of the controller, the asynchronous control. The use of Balsa has allowed the controller to be rapidly re-engineered in response to a changing specification. 
46|14||ACTion: Combining logic synthesis and technology mapping for MUX-based FPGAs|Technology mapping for Multiplexor (MUX) based field programmable gate arrays (FPGAs) has widely been considered. Here, a new algorithm is proposed that applies techniques from logic synthesis during technology mapping, i.e., the target technology is considered in the minimization process. Binary decision diagrams (BDDs) are used as an underlying data structure combining both structural and functional properties. The algorithm uses local don't cares obtained by a greedy algorithm. To evaluate a netlist, a fast technology mapper is used. Since most of the changes to a netlist are local, re-mapping can also be done locally, allowing a fast but reliable evaluation after each modification. Both area and delay minimization are addressed in this paper. We compare the approach to several previously published algorithms. In most cases these results can be further improved. Compared to SIS, an improvement of 23% for area and 18% for delay can be observed on average. 
46|14||A novel approach for implementing high-speed and long-distance networking protocols in a limited memory embedded kernel|STREAMS kernel mechanisms are being used to implement networking protocols in limited memory embedded systems. The current approaches for STREAMS-based networking protocols implementation suffer from some shortcomings when these approaches are used to implement high-speed and long distance WAN protocols that require large transport windows. Accumulation of duplicate copies of large transport layer windows in the subnet layer cause unnecessary hogging of kernel memory resources. This memory hogging leads to performance degradation since shortage of buffers forces the protocols to operate at smaller window sizes. The reason for this overhead is that the protocols cannot be implemented efficiently due to strict layering scheme of STREAMS. We have devised new kernel mechanisms to provide solution to this problem. We have defined new mapping mechanisms between protocol layers and have coupled these mechanisms with novel “event-based” flow control mechanisms. These mechanisms provided appropriate flow control handling in the kernel that led to significant reduction in memory buffers hogging. This makes embedded systems handle large windows efficiently. 
46|14||Communication in a multi-layer MIMD system for computer vision|Algorithms in computer vision are computationally intensive and have distinct characteristics. Message-passing MIMD computers are popular architectures for computer vision due to their flexibility and asynchronous structure. However, performance of parallel programs executed on these systems is dependent on the efficiency of inter-processor communication. This paper describes a software system developed to handle complex communication mechanisms for a system designed for computer vision applications. The system is constructed with TI320C40 DSPs and employs the distributed memory MIMD paradigm. A set of experiments has been conducted to determine the hardware parameters of the communication links of the processors. Furthermore, the performances of the communication algorithms developed, which are frequently required in parallel vision algorithms, are presented. These results will be useful in optimizing performance of parallel programs developed for the system. 
46|2|http://www.sciencedirect.com/science/journal/13837621/46/2|GSPN models of bridged LAN configurations1|Communication systems comprising several local area networks (LANs) interconnected by bridges are modeled with Generalized Stochastic Petri Nets (GSPNs). GSPN models are developed at different levels of detail, progressively abstracting from the detailed behavior of real systems, and exploiting symmetries present in simple interconnected LAN topologies by “folding” the detailed GSPN models. The numerical results obtained from the solution of GSPN models are validated against results of very detailed simulation experiments, and found in good agreement, even for the most abstract GSPN models. Advantages and drawbacks of the GSPN modeling approach are discussed at the end of the paper. 
46|2||Optimum reserved resource allocation scheme for handoff in CDMA cellular system|CDMA cellular systems support two types of handoff: soft handoff and hard handoff. Soft handoff has many advantages over hard handoff. Qualitatively, this feature provides more reliable handoff between base stations as a mobile moves from one cell to the adjacent one. Quantitatively, it considerably increases both the capacity of a heavily loaded multicellular system and the coverage of each individual cell in a lightly loaded system. In this paper, in order to increase the probability of soft handoff at the time of handoff, optimum reserved resource allocation scheme for handoff in CDMA cellular system is proposed, which allocates reserved resource to each frequency channel according to the number of neighbor cells using the same frequency channel. Performance analysis results show that the proposed scheme has higher probability of soft handoff at the time of handoff and higher total call processing performance which is a function of both new calls accepted probability and soft handoff probability than the conventional scheme allocating reserved resource irrelevantly to frequency channels being used by neighbor cells. 
46|2||MAP: Design and implementation of a mobile agents' platform|The recent development of telecommunication networks has contributed to the success of applications such as information retrieval and electronic commerce, as well as all the services that take advantage of communication in distributed systems. In this area, the emerging technology of mobile agents aroused considerable interest. Mobile agents are applications that can move through the network for carrying out a given task on behalf of the user. In this work we present a platform called MAP (Mobile Agents Platform) for the development and the management of mobile agents. The language used both for developing the platform and for carrying out the agents is Java. The platform gives the user all the basic tools needed for creating some applications based on the use of agents. It enables us to create, run, suspend, resume, deactivate, reactivate local agents, to stop their execution, to make them communicate each other and migrate. 
46|2||Performance analysis of video storage server under initial delay bounds|Previous studies on video storage servers focused on improving the disk throughput and reducing the server buffer size. However, the initial delay of a new request, one of the most important quality of service (QoS) parameters from the users' point of view, is almost neglected while designing a storage subsystem or evaluating its performance. For different types of video-on-demand (VOD) services such as interactive video game, digital library, or movie-on-demand system, the initial delay can vary from 0.5 s to 5 min. This criterion brings some impacts on designing a storage server for a particular VOD application. In this paper, we investigate the storage server design and the performance evaluation of VOD systems with different initial delay guarantees. We propose a new performance model on evaluating the efficiency of a video storage server so that a cost-effective configuration can be easily obtained under a specified initial delay bound. 
46|2||A scheme for multiple on-chip signature checking for embedded SRAMS|Embedded read/write memories are integral parts of many VLSI chips designed for specific applications in the areas of computer communications, multimedia, and digital signal processing. Testing an embedded memory poses a challenge to a system test engineer, due to its limited controllability and observability. In this paper, we propose a pseudorandom built-in self test (BIST) scheme to solve this problem. Our technique is based on a test architecture known as multiple on-line signature checking (MOSC) which offers a very low aliasing probability and a high degree of confidence in testing. While the MOSC scheme is sufficiently general and applicable to any digital circuit, it can especially be optimized for circuits with embedded memories. We present interesting test scheduling algorithms that reduce the overhead of testing. On several industry-standard benchmark circuits, we report up to 35% savings in test area overhead. 
46|2||A fault tolerant routing algorithm based on cube algebra for hypercube systems|We propose an approach to determine the shortest path between the source and the destination nodes in a faulty or a non-faulty hypercube. The number of faulty nodes and links may be rather large and if any path between the nodes exists, the developed algorithm determines it. To construct this algorithm, some properties of the cube algebra are considered and some transformations based on this algebra are developed. 
46|3|http://www.sciencedirect.com/science/journal/13837621/46/3|An improved registerâtransfer level functional partitioning approach for testability1|This paper presents an improved register–transfer level functional partitioning approach for testability. Based on an earlier work (X. Gu, K. Kuchcinski, Z. Peng, An efficient and economic partitioning approach for testability, in Proceedings of International Test Conference, Washington DC, 1995.), the proposed method identifies the hard-to-test points initially based on data path testability and control state reachability. These points will be made directly accessible by DFT techniques. Then the actual partitioning procedure is performed by a quantitative clustering algorithm which clusters directly interconnected components based on a new global testability of data path and global state reachability of control part. After each clustering step, we use a new estimation method which is based partially on explicit re-calculation and partially on gradient techniques for incremental testability and state reachability analysis to update the test property of the circuit. This process will be iterated until the design is partitioned into several disjoint sub-circuits and each of them can be tested independently. The control part is then modified to control the circuit in normal and test mode accordingly. Therefore, test quality is improved by independent test generation and application for every partition and by combining the effect of data path with control part. Experimental results show the advantages of the proposed algorithm compared to other conventional approaches. 
46|3||An extended-UIO-based method for protocol conformance testing|Verification of protocols is performed through conformance testing. The aim of this paper is to introduce the conformance test generation approach for protocols described by means of Finite State Machines. A functional fault model is adopted and the state discrimination is performed by applying an extended version of Unique Input Output Sequences (UIO), which, different from classical UIO, can always be found in any state. Both algorithms for efficient extended UIO sequence identification and for optimal test sequence generation are presented, together with the experimental results on different protocol descriptions. 
46|3||Multiple context multithreaded superscalar processor architecture|Superscalar architecture is becoming the norm in today's high performance microprocessor design. However, achievable instruction level parallelism in programs limits the scalability of such architectures. In this paper, we introduce the Multiple Context Multithreaded Superscalar Processor (MCMS), which is an extension of conventional superscalar processor architecture to support multithreading. This is motivated by the enormous potential instruction level parallelism present in multithreaded programs. A hardware implementation of multithreaded constructs is also proposed. Results from trace-driven simulation show that with the MCMS, instruction level parallelism is indeed increased significantly. A MCMS processor with four hardware contexts can produce a speedup of up to 2.5 times over superscalar processor with similar hardware resources. We found that the primary limitation shifts from data dependencies in the superscalar processor to resource contentions in MCMS. 
46|3||Impact of the memory interface structure in the memory-processor integrated architecture for computer vision|The memory-based processor array (MPA) was previously designed as an effective memory-processor integrated architecture. The MPA can be easily attached into any host system via memory interface. In this paper, the impact of the memory interface structure is analytically analyzed for computer vision tasks. An analytical model is constructed to describe the characteristics of the memory interface structure. Performance improvement for the memory interface model of the MPA system can be 6–40% for vision tasks consisting of sequential and data parallel tasks. Mapping algorithms to implement convolution and connected component labeling on the MPA are also presented. The asymptotic time complexities of the algorithms are evaluated to verify the cost-effectiveness and the efficiency of the MPA system. 
46|3||Development of process visualization systems: An object-oriented approach|In this paper a process visualization development system and its associated development methodology are presented. This methodology is optimized to systems that have complex structure and are built of large number of components belonging to relatively small number of types. In order to handle the complexity, the input requirements of the method are as close to the “native language” of the application as possible. The elements of the native language are assumed to include engineering drawings and manuals describing the operation of the component types the system is built of. Graphics techniques are used to supply the engineering drawings into the development system while not only the required visual appearance is described but the structure of the underlying system is also defined. The elements of engineering drawings are dynamized to animate the graphics presentation, to reflect the current state of the monitored system. Component manuals are transformed to interface and state definitions from which a code generator generates a C++ class for each component type. This C++ class must be tuned to reflect the operation of a single component type. From these definitions the development system automatically builds up the complete visualization program, providing easy and fast application development. 
46|3||Fault tolerant permutation mapping in multistage interconnection network|An efficient scheme for fault tolerant mapping of permutations is designed. The proposed algorithm uses extra passes through the network, instead of additional hardware. 
46|4|http://www.sciencedirect.com/science/journal/13837621/46/4|Editorial|
46|4||The meaning and role of value in scheduling flexible real-time systems1|The real-time community is devoting considerable attention to flexible scheduling and adaptive systems. One popular means of increasing the flexibility, and hence effectiveness, of real-time systems is to use value-based scheduling. It is surprising however, how little attention has been devoted, in the scheduling field, to the actual assignment of value. This paper deals with value assignment and presents a framework for undertaking value-based scheduling and advises on the different methods that are available. A distinction is made between ordinal and cardinal value functions. Appropriate techniques from utility theory are reviewed. An approach based on constant value modes is introduced and evaluated via a case example. 
46|4||Schedulability analysis of periodic and aperiodic tasks with resource constraints|In this paper, we address the problem of scheduling hybrid task sets consisting of hard periodic and soft aperiodic tasks that may share resources in exclusive mode in a dynamic environment, where tasks are scheduled based on their deadlines. Bounded blocking on exclusive resources is achieved by means of a dynamic resource access protocol which also prevents deadlocks and chained blocking. Aperiodic responsiveness is enhanced by an efficient servicing technique which assigns each aperiodic request a suitable deadline. Feasibility conditions are extended to handle tasks with deadlines different from periods and a reclaiming technique is presented to deal with early completions. 
46|4||Complete worst-case execution time analysis of straight-line hard real-time programs|In this article, the problem of finding a tight estimate on the worst-case execution time (WCET) of a hard real-time program is addressed. The analysis is focused on straight-line code (without loops and recursive function calls) which is quite commonly found in synthesised code for embedded systems. A comprehensive timing analysis system covering both low-level (assembler instruction level) as well as high-level aspects (programming language level) is presented. The low-level analysis covers all speed-up mechanisms used for modern superscalar processors: pipelining, instruction-level parallelism and caching. The high-level analysis uses the results from the low-level to compute the final estimate on the WCET. This is done by a heuristic for searching the longest really executable path in the control flow, based on the functional dependencies between various program parts. 
46|4||Techniques to increase the schedulable utilization of cache-based preemptive real-time systems1|Nowadays, cache memories are applicable to real-time systems with the help of tools that obtain the worst-case execution time (WCET) of cached programs. However, these tools do not allow preemption, because from the point of view of program analysis, the number of preemptions is unknown. To face this problem, the cache-related preemption cost can be considered in the schedulability analysis, or annulled by the use of private cache partitions. This paper comprises a number of techniques using the first or both solutions. This paper also explores the harmonic relationships among tasks to improve the estimation of the cache interference in the analysis. 
46|4||Operating system support for the management of hard real-time disk traffic|Emerging applications like C3I systems, real-time databases, data acquisition systems and multimedia servers require access to secondary storage devices under timing constraints. In this paper, we focus on operating system support needed for managing real-time disk traffic with hard deadlines. We present the design and implementation of a preemptive deadline-driven disk I/O subsystem suitable for real-time disk traffic management. Preemptibility is achieved with a granularity that is automatically controlled by the I/O subsystem according to current workload and filesystem data layout. An admission control test checks the current resource availability for a given workload. We show that contiguous layout is necessary to maintain hard real-time guarantees and a reasonable level of disk throughput. Finally, we show how buffering can be used to obtain utilization factors close to the maximum disk bandwidth possible. 
46|4||Issues and approaches to supporting timeliness and security in real-time database systems1|Databases for real-time systems are essential in supporting time-critical applications. However, there has not been much work in supporting security in real-time database systems, although sensitive information must be safeguarded in real-time systems as well. In this paper we address the issues that need to be considered for supporting both requirements of timeliness and security in real-time database systems. We present an adaptive policy to achieve the balance between the two requirements dynamically. We also present the notion of flexible security and the specification language that allows the designer to specify important properties of the database at an appropriate level. 
46|5|http://www.sciencedirect.com/science/journal/13837621/46/5|On the performance of distributed objects|Early distributed shared memory systems used the shared virtual memory approach with fixed-size pages, usually 1–8 KB. As this does not match the variable granularity of sharing of most programs, recently the emphasis has shifted to distributed object-oriented systems. With small object sizes, the overhead of inter-process communication could be large enough to make a distributed program too inefficient for practical use. To support research in this area, we have implemented a user-level distributed programming testbed, DIPC, that provides shared memory, semaphores and barriers. We develop a computationally-efficient model of distributed shared memory using approximate queueing network techniques. The model can accommodate several algorithms including central server, migration and read-replication. These models have been carefully validated against measurements on our distributed shared memory testbed. Results indicate that for large granularities of sharing and small access bursts, central server performs better than both migration and read-replication algorithms. Read-replication performs better than migration for small and moderate object sizes for applications with high degree of read-sharing and migration performs better than read-replication for large object sizes for applications having moderate degree of read-sharing. 
46|5||A case study of a distributed high-performance computing system for neurocomputing|We model here a distributed implementation of cross-stopping, a combination of cross-validation and early-stopping techniques, for the selection of the optimal architecture of feed-forward networks. Due to the very large computational demand of the method, we use the RAIN system (Redundant Array of Inexpensive workstations for Neurocomputing) as a target platform for the experiments and show that this kind of system can be effectively used for computational intensive neurocomputing tasks. 
46|5||Improving cache performance with Full-Map Block Directory|There are two concurrent paths in a typical cache access – one through the data array and the other through the tag array. In most cases, the path through the tag array is significantly longer than that through the data array. In this paper, we propose a new scheme that exploits this imbalance in the tag and data paths to improve overall cache performance. Under this scheme, an additional tag directory, the Full-Map Block Directory, is used to provide an alternate tag path to speed up cache access for almost all the memory requests. This scheme is based on the observation that spatial locality exists on a cache line basis, i.e., cache lines near one another tend to be referenced together. Performance evaluation using a TPC-C-like benchmark and selected applications from the SPEC92 benchmark suite demonstrates that this scheme has the potential to improve overall system performance by more than 20%. 
46|5||Markovian and analytical models for multiple bus multiprocessor systems with memory blockings|This paper deals with the performance evaluation of asynchronous multiple bus multiprocessor systems where memory modules are temporarily unavailable to processor requests. Such systems cannot be modelled with a product-form queueing network (QN) model because of memory blockings. In this context, we first propose an exact continuous-time Markovian QN model for analyzing small size systems. In order to study medium to large size systems, we also propose several approximate lumped Markovian models and two approximate analytical QN models. The robustness of these approximate models is studied when any memory module gets systematically blocked after each access to this memory module. Results are compared against those obtained either with the exact Markovian QN model or with a stochastic simulation model. 
46|6|http://www.sciencedirect.com/science/journal/13837621/46/6|On the design of IP routers Part 1: Router architectures|Internet Protocol (IP) networks are currently undergoing transitions that mandate greater bandwidths and the need to prepare the network infrastructures for converged traffic (voice, video, and data). Thus, in the emerging environment of high performance IP networks, it is expected that local and campus area backbones, enterprise networks, and Internet Service Providers (ISPs) will use multigigabit and terabit networking technologies where IP routers will be used not only to interconnect backbone segments but also to act as points of attachments to high performance wide area links. Special attention must be given to new powerful architectures for routers in order to play that demanding role. In this paper, we describe the evolution of IP router architectures and highlight some of the performance issues affecting IP routers. We identify important trends in router design and outline some design issues facing the next generation of routers. It is also observed that the achievement of high throughput IP routers is possible if the critical tasks are identified and special purpose modules are properly tailored to perform them. 
46|6||Systolic arrays architecture for computing the time-frequency spectrum|To solve the problem of detecting and displaying the changes in the spectrum of non-stationary signals, there are two possible approaches. Either, one uses the same estimators as for the stationary signals but with shorter length data blocks during which it is assumed to be stationary, or one can use the same length data and apply a time-varying (TV) spectrum estimator which accounts for the non-stationarity. A TV spectrum estimator called time-varying correlogram (TVC) is a well known estimator of the time-frequency (TF) spectrum of a non-stationary signal. In this paper, a VLSI architecture for computing the TVC is proposed. 
46|6||The diagnosability of hypercubes with arbitrarily missing links|We study the problem of determining diagnosability for incomplete hypercubes that have arbitrarily distributed missing links, under the classic PMC diagnostic model and its variant, the BGM model. Based on the result proved in this paper, for both models, in most cases the diagnosability of an incomplete hypercube can be determined by simply checking the link degree of each node. 
46|6||O(n) routing in rearrangeable networks|In (2n−1)-stage rearrangeable networks, the routing time for any arbitrary permutation is Î©(n2) compared to its propagation delay O(n) only. Here, we attempt to identify the sets of permutations, which are routable in O(n) time in these networks. We define four classes of self-routable permutations for Benes network. An O(n) algorithm is presented here, that identifies if any permutation P belongs to one of the proposed self-routable classes, and if yes, it also generates the necessary control vectors for routing P. Therefore, the identification, as well as the switch setting, both problems are resolved in O(n) time by this algorithm. It covers all the permutations that are self-routable by anyone of the proposed techniques. Some interesting relationships are also explored among these four classes of permutations, by applying the concept of ‘group-transformations’ [N. Das, B.B. Bhattacharya, J. Dattagupta, Hierarchical classification of permutation classes in multistage interconnection networks, IEEE Trans. Comput. (1993) 665–677] on these permutations. The concepts developed here for Benes network, can easily be extended to a class of (2n−1)-stage networks, which are topologically equivalent to Benes network. As a result, the set of permutations routable in a (2n−1)-stage rearrangeable network, in a time comparable to its propagation delay has been extended to a large extent. 
46|6||Stripped mirroring RAID architecture|Redundant arrays of independent disks (RAID) provide an efficient stable storage system for parallel access and fault tolerance. The most common fault tolerant RAID architecture is RAID-1 or RAID-5. The disadvantage of RAID-1 lies in excessive redundancy, while the write performance of RAID-5 is only 1/4 of that of RAID-0. In this paper, we propose a high performance and highly reliable disk array architecture, called stripped mirroring disk array (SMDA). It is a new solution to the small-write problem for disk array. SMDA stores the original data in two ways, one on a single disk and the other on a plurality of disks in RAID-0 by stripping. The reliability of the system is as good as RAID-1, but with a high throughput approaching that of RAID-0. Because SMDA omits the parity generation procedure when writing new data, it avoids the write performance loss often experienced in RAID-5. 
46|7|http://www.sciencedirect.com/science/journal/13837621/46/7|Low power architectures for digital signal processing|Low power architectures for digital signal processing algorithms requiring inner product computation are presented. In the first step a power efficient memory organization exploiting data reuse is determined. In the second step an order of evaluation of the partial products that reduces the switching activity at the inputs of the computational units is derived. Information related to both coefficients which are static and data which are dynamic, is used to drive the reordering of computation. Experimental results for several signal processing algorithms prove that the proposed techniques lead to significant savings in net switching activity and thus in power consumption. 
46|7||A conditional abortable priority ceiling protocol for scheduling mixed real-time tasks|Priority Ceiling Protocol (PCP) is a well-known resource access protocol for hard real-time systems. However, it has a problem of ceiling blocking which imposes a great hindrance to task scheduling in mixed real-time systems where tasks may have different criticality. In this paper, a new resource access protocol called the Conditional Abortable Priority Ceiling Protocol (CA-PCP) is proposed. It resolves the problem of ceiling blocking by incorporating a conditional abort scheme into the PCP. The new protocol inherits all the desirable properties of the PCP and the Ceiling Abort Protocol (CAP) which is yet another modification of the PCP. In the proposed protocol, a condition is defined to control the abort of a job so that the schedulability of the system will not be affected. Performance study has been done to compare the CA-PCP with the PCP. The results indicate that CA-PCP can significantly improve the performance of a system if the lengths of the abortable critical sections are well chosen. 
46|7||Algorithms for real-time scheduling of error-cumulative tasks based on the imprecise computation approach|This paper presents several algorithms for real-time scheduling that have been developed following an approach known as Imprecise Computation. This technique prevents timing faults (i.e., results not produced in time) by offering an approximate result of an acceptable quality whenever the exact result of the desired quality cannot be obtained in time. In this work we focused our attention on the problem of scheduling a set of real-time error-cumulative periodic tasks for which errors in different periods have cumulative effects, making it necessary to generate timely, precise results sometimes. This problem is particularly relevant in applications such as route tracking and real-time control of complex industrial plants. The adopted approach consists in applying a transformation of the set of error-cumulative tasks into a set of error-noncumulative tasks that can be easily scheduled by means of an optimal algorithm like the earliest deadline first. This approach permits to obtain scheduling algorithms for some specific, but quite significant, problems. The algorithms have a polynomial complexity, whereas most of the solutions found in literature are NP-hard. 
46|7||Real-time multimedia standards in DQDB|In the present work, the necessary and sufficient conditions for the real-time schedulability of Dual Queue Dual Bus (DQDB) networks are formally proved, strictly adhering to the letter of the standard as it stands today. With those results, the quality of the standardized multimedia applications that a given DQDB network can carry is analysed. 
46|7||Worst-case deadline failure probability in real-time applications distributed over controller area network|Real-time applications distributed over the controller area network (CAN) are generally characterised by stringent temporal and dependability constraints. Our goal is to take account of transmission errors in the design of such applications because the consequences of such disturbances are potentially disastrous. In this study, the concept of worst-case deadline failure probability (WCDFP) is introduced. The motivation of the probabilistic approach is that, in practice, the number of errors occurring during a given time period can with difficulty be bounded. To evaluate the WCDFP, we propose, on the one hand, a method of computing for each message the tolerable threshold of transmission errors under which timing constraints are guaranteed to be met. On the other hand, we also suggest an error model enabling us to consider both error frequency and error gravity. Our error model follows a generalized Poisson process and its stochastic parameters have been derived. We then propose a numerically efficient algorithm to compute the probabilities and apply the analysis to an industrial case-study of the automotive field. 
46|7||Validation of SPEC6â¢ CFP95 traces for accurate performance evaluation of computer systems|Performance evaluation of computer systems and processors is necessary during design, procurement, and capacity planning. The increasing complexity of realistic benchmarks combined with the relatively sluggish rate of detailed performance analysis has resulted in an ever-increasing gap between the size of the workload/traces and the speed of analysis. In this paper, we present a methodology for generating and validating representative traces from SPEC™95 benchmark suite using R-metric and K-metric. Experimental results demonstrate the superiority of our proposed technique over the ubiquitous profile-driven methodology and the uniform sampling approach. 
46|8|http://www.sciencedirect.com/science/journal/13837621/46/8|Heterogeneous distributed and parallel architectures: Hardware, software and design tools|
46|8||Techniques for mapping tasks to machines in heterogeneous computing systems|Heterogeneous computing (HC) is the coordinated use of different types of machines, net-works, and interfaces to maximize their combined performance and/or cost-effectiveness. HC systems are becoming a plausible technique for efficiently solving computationally intensive problems. The applicability and strength of HC systems are derived from their ability to match computing needs to appropriate resources. In an HC system, tasks need to be matched to machines, and the execution of the tasks must be scheduled. The goal of this invited keynote paper is to: (1) introduce the reader to some of the different distributed and parallel types of HC environments; and (2) examine some research issues for HC systems consisting of a network of different machines. The latter purpose is pursued by considering: (1) the quantification of heterogeneity; (2) the characterization of techniques for mapping (matching and scheduling) tasks on such systems; (3) an example HC resource management system; and (4) static and dynamic heuristics for mapping tasks to machines in such HC systems. 
46|8||Efficient use of parallel libraries on heterogeneous Networks of Workstations|The paper is motivated by efficiency considerations about porting mathematical software from Massively Parallel Processors (MPPs) to Networks of Workstations (NOWs). Heterogeneity of the network is the major obstacle to efficient porting: it can be overcome with a specialized system, Programming envIronment for Network of COmputers (PINCO), for monitoring available computational power at different nodes, both statically and dynamically. The structure and functionalities of PINCO are outlined, and a significant porting example, matrix multiplication, is presented. 
46|8||Vertically-partitioned parallel signature file method|Recently, parallel signature file methods have been proposed for better retrieval performance in signature files. In this paper, we propose a vertically-partitioned parallel signature file (VPSF) method which can partition a signature file vertically. Our VPSF method uses an extendable hashing technique for dynamic environment and uses a frame-sliced signature file technique for efficient retrieval. Our VPSF method also can eliminate the data skew and the execution skew by allocating each frame to a processing node. To prove the efficiency of our VPSF method, we compare its performance with those of the conventional parallel signature file methods, i.e., HPSF and Hamming filter, in terms of retrieval time, storage overhead, and insertion time. The experimental result shows that our VPSF achieves about 40% better retrieval performance than the Hamming filter. In addition, we evaluate the performance of our VPSF methods on several normal distributions with half and double standard deviations of the real data. From the performance evaluation on record sets with half standard deviation, we show that our VPSF gains about 20–50% improvement in retrieval time, compared with the Hamming filter and the HPSF. Finally, we show that our VPSF generally outperforms the conventional parallel signature files on retrieval performance when the records of a database are uniform in size. 
46|8||On parallel solvers for sparse triangular systems|In this paper we describe and compare two different methods for solving general sparse triangular systems in distributed memory multiprocessor architectures. The two methods involve some preprocessing overheads so they are primarily of interest in solving many systems with the same coefficient matrix. Both algorithms start off from the idea of the classical substitution method. The first algorithm we present introduces a concept of data driven flow and makes use of non-blocking communications in order to dynamically extract the inherent parallelism of sparse systems. The second algorithm uses a reordering technique for the unknowns, so the final system can be grouped in variable blocksizes where the rows are independent and can be solved in parallel. This latter technique is called level scheduling because of the way it is represented in the adjacency graph. These methods have been tested in the Fujitsu AP1000 and the Cray T3D and T3E multicomputers. The performance has been analysed using matrices from the Harwell-Boeing collection. 
46|8||Achieving high degree of concurrency in multidatabase transaction scheduling: MTOS|A multidatabase system (MDBS), which is also called a heterogeneous distributed database system (HDDBS) or federated database system (FDBS), is a facility that allows users or applications to access data located in multiple local database systems (LDBSs), each of which is autonomously operated. Several practical MDBS concurrency control schemes have been proposed whilst maintaining global consistency without compromising local autonomy. However, they could degrade either local concurrency of local transactions or global concurrency of global transactions for the purpose of ensuring the global serializability. In this paper, we propose a new ticket-based global concurrency control scheme that employs multiple tickets at each site. The basic idea is that, with proper consideration of the potential cyclic global serialization orders, subtransactions with the possibility of the local indirect conflicts must access the same ticket at their sites, otherwise they are allowed to access the different tickets. This scheme alleviates the blockage on local resources, and, as a result, increases both global concurrency and local concurrency, whilst preserving the local autonomy. 
46|8||Transaction multicasting scheme for resilient routing control in parallel cluster database systems|A disk cluster environment (DCE) refers to a distributed architecture for high performance transaction processing in which the computing nodes are locally coupled via a high-speed network and share a common database at the disk level. In the DCE, it is crucial to determine at which node the incoming transactions are processed. This is called transaction routing. The aim of disk sharing in DCE is not only to achieve high performance by distributing the workload among the processing nodes but also to obtain fault-tolerance against possible system failures, like a single node failure. Although a number of transaction routing schemes have been reported for DCE, it is true that most of them are not sufficiently resilient against system dynamics, which inevitably requires changing the routing information. In this paper, we propose a new dynamic transaction routing scheme for DCE, called multicast transaction routing scheme, MTR for short, that is able to change the transaction routing information in the presence of critical events without imposing too much overhead to the transaction processing system. In our scheme, when it is required to change the routing information dynamically, the routing algorithm sends multiple clones of a transaction to a group of candidate processing nodes and selects the processing node that first completes the multicasted transaction as a new processing node for re-routed transaction. The selected processing node is expected to be a best affinity node when the system load is evenly distributed, or a relatively unloaded processing node that is idle enough to process a transaction faster than other nodes. The novel aspect of MTR is that it automatically achieves an optimal balance between affinity-based routing and load balancing. The simulation study shows that MTR rapidly stabilizes the system and produces an optimal routing information so that it finally guarantees faster response time. 
46|9|http://www.sciencedirect.com/science/journal/13837621/46/9|Testing and built-in self-test â A survey|As the density of VLSI circuits increases it becomes attractive to integrate dedicated test logic on a chip. This built-in self-test (BIST) approach not only offers economic benefits but also interesting technical opportunities with respect to hierarchical testing and the reuse of test logic during the application of the circuit.Starting with an overview of test problems, test applications and terminology this survey reviews common test methods and analyzes the basic test procedure. The concept of BIST is introduced and discussed, BIST strategies for random logic as well as for structured logic are shown. 
46|9||Register bypassing in an asynchronous superscalar processor|Register bypassing, universally provided in synchronous processors, is more difficult to implement in an asynchronous design. Asynchronous bypassing requires synchronization between the forwarding and receiving units, with the danger that the advantages of asynchronous operation may be nullified by reintroducing the lock-step operation of synchronous processors. We present a novel implementation of register bypassing in an asynchronous processor architecture. Our technique of Decoupled Operand Forwarding provides centralized control over the bypassing operation, yet allows multiple execution units to function asynchronously. Our ideas are presented within the context of the development of Hades, a generic asynchronous processor architecture. We employ single-issue and dual-issue simulations of Hades to quantify the benefits of Decoupled Operand Forwarding and conclude that Decoupled Operand Forwarding yields significant speedups because of its success in removing register files from the critical timing path. 
46|9||Multistage ring network: An interconnection network for large scale shared memory multiprocessors|Unidirectional ring-based networks are currently popular choices for high performance large scale shared memory multiprocessors. This class of networks is attractive for their simple hardware interfaces, high speed communication, wider data path, and easy addition of extra nodes. However, a single ring does not scale well due to the fixed bandwidth, and the hierarchical ring networks as a natural extension of a single ring show limited scalability due to their limited bandwidth near the root. In this paper we present a new interconnection network called the Multistage Ring Network (MRN). The MRN has a 2-level hierarchy of rings, and its interconnection of global rings forms a type of the multistage network. The architecture of the MRN is effective at diffusing the global traffic on the network to all global rings, and the bandwidth of the network increases proportionally with increases in the system size. Our results show that in a peak throughput, the MRN performs seven times better than the hierarchical ring network for system size of 1024. 
46|9||On the design of hypermesh interconnection networks for multicomputers|Topology, routing algorithm, and router structure are among the most important factors that greatly influence network performance. This paper assesses the interaction of these elements on two related but distinct types of multicomputer networks, the binary n-cube (or cube) and the hypermesh. The analysis will show that the topological properties of the hypermesh confer an important advantage over the cube that makes the former a promising option for use in high-performance multicomputers. The hypermesh can use simple routing algorithms and cheap routers with little performance penalty. The cube, on the other hand, is constrained to the use of a specific routing algorithm and complex routers to take advantage of its rich connectivity. 
46|9||Integrated dynamic scheduling of hard and QoS degradable real-time tasks in multiprocessor systems|Many time-critical applications require predictable performance and tasks in these applications have deadlines to be met. For tasks with hard deadlines, a deadline miss can be catastrophic while for Quality of Service (QoS) degradable tasks (soft real-time tasks) timely approximate results of poorer quality or occasional deadline misses are acceptable. Imprecise computation and (m,k)-firm guarantee are two workload models that quantify the trade-off between schedulability and result quality. In this paper, we propose dynamic scheduling algorithms for integrated scheduling of real-time tasks, represented by these workload models, in multiprocessor systems. The algorithms aim at improving the schedulability of tasks by exploiting the properties of these models in QoS degradation. We also show how the proposed algorithms can be adapted for integrated scheduling of multimedia streams and hard real-time tasks, and demonstrate their effectiveness in quantifying QoS degradation. Through simulation, we evaluate the performance of these algorithms using the metrics – success ratio (measure of schedulability) and quality. Our simulation results show that one of the proposed algorithms, multilevel degradation algorithm, outperforms the others in terms of both the performance metrics. 
46|9||An atomic commit protocol for gigabit-networked distributed database systems|In the near future, different database sites will be interconnected via gigabit networks, forming a very powerful distributed database system. In such an environment, the propagation latency will be the dominant component of the overall communication cost while the migration of large amounts of data will not pose a problem. Furthermore, computer systems are expected to become even more reliable than today’s systems with long mean time between failures and short mean time to repair. In this paper, we present implicit yes-vote (IYV), a one-phase atomic commit protocol, that exploits these new domain characteristics to minimize the cost of distributed transaction commitment. IYV eliminates the explicit voting phase of the two-phase commit protocol, hence reducing the number of sequential phases of message passing during normal processing. In the case of a participant’s site failure, IYV supports the option of forward recovery by enabling partially executed transactions that are still active in the system to resume their execution when the failed participant is recovered. 
volume|issue|url|title|abstract
47|-|http://www.sciencedirect.com/science/journal/13837621/47|Editorial Board|
47|-||Reconfigurable models of finite state machines and their implementation in FPGAs|This paper examines some models of finite state machines (FSMs) that can be implemented in dynamically and statically reconfigurable FPGAs. They enable circuits for the FSMs to be constructed in such a way that allows their behavior to be modified before and during run-time. This is achieved either by swapping pre-allocated areas on a chip in partially dynamically reconfigurable FPGAs, or by reloading memory-based cells in statically configured FPGAs. The initial behavioral description is presented in the form of hierarchical graph-schemes that can be formally translated to traditional FSM specifications such as state diagrams and state transition tables. The description supports modularity and a hierarchical structure, both of which are important for modifiable circuits. The results of experiments with software models that permit reconfigurable systems to be simulated and verified and with a hardware implementation of a FSM have shown that such reusable circuits require very limited FPGA resources and they can be reprogrammed in much the same way as for software development. 
47|-||A banked-promotion translation lookaside buffer system|We present a simple but high performance translation lookaside buffer (TLB) system with low power consumption for use in embedded systems. Our TLB structure supports two page sizes dynamically and selectively to achieve high performance with low hardware cost. To minimize power consumption, a banked-TLB is constructed by dividing one fully associative (FA) TLB space into two separate FA TLBs. These two structures are integrated to form a banked-promotion (BP) TLB. Promotion overcomes the unbalanced utilization of a banked-TLB by moving adjacent entries out of the primary banks into a separate super-page TLB. Simulation results show that the Energy*Delay product can be reduced by about 99.8%, 19.2%, 24.2%, and 24.4% compared with a FA TLB, a micro-TLB, a banked-TLB, and a victim-TLB respectively. Therefore, the BP TLB offers high performance with low power consumption and low hardware cost. 
47|-||Fast modular exponentiation of large numbers with large exponents|"In many problems, modular exponentiation |xb|m is a basic computation, often responsible for the overall time performance, as in some cryptosystems, since its implementation requires a large number of multiplications.It is known that |xb|m=|x|b|Ï(m)|m for any x in [1,m−1] if m is prime; in this case the number of multiplications depends on Ï(m) instead of depending on b. It was also stated that previous relation holds in the case m=pq, with p and q prime; this case occurs in the RSA method.In this paper it is proved that such a relation holds in general for any x in [1,m−1] when m is a product of any number n of distinct primes and that it does not hold in the other cases for the whole range [1,m−1].Moreover, a general method is given to compute |xb|m without any hypothesis on m, for any x in [1,m−1], with a number of modular multiplications not exceeding those required when m is a product of primes.Next, it is shown that representing x in a residue number system (RNS) with proper moduli mi allows to compute |xb|m by n modular exponentiations |xib|mi in parallel and, in turn, to replace b by |b|Ï(mi) in the worst case, thus executing a very low number of multiplications, namely ⌈log2mi⌉ for each residue digit.A general architecture is also proposed and evaluated, as a possible implementation of the proposed method for the modular exponentiation. "
47|-||Configurable parallel memory architecture for multimedia computers|This paper presents a novel parallel memory architecture for multimedia computers. Applying a configurable or programmable addressing circuitry capable of parallel memory accesses, the memory management of multimedia applications can be enhanced. Necessary computer architecture changes to virtual address representation, paging, virtual memory, address computation circuitry and data permutation are discussed. These changes allow the memory to be partitioned for different access functions. In addition, the same memory area can be accessed by multiple access patterns. Therefore, a general-purpose computing system that is capable of exploiting the repeating memory access patterns in its applications can be built. Performance of the configurable parallel memory architecture (CPMA) is analyzed in the case of a selection of algorithms from a video encoder. These motion estimation algorithms and zigzag scanning benefit from the multiple memory access functions, which is apparent from the comparisons to the traditional sequential memory accesses. 
47|-||Demand-driven logic simulation using a network of loosely coupled processors|Logic simulation is used extensively in the design of digital systems for the purpose of studying the behaviour of circuits under various conditions and for verifying the required performance of circuits. There is considerable interest in methods which reduce the simulation time during the design process. In this paper, we investigate how this can be achieved by simulating the action of logic circuits using a network of loosely coupled processors. Circuits modelled as directed graphs comprising clocked sequential components and (unclocked) arbitrary combinational logic gates can be partitioned into separate tasks each consisting of a sequential component with an associated network of combinational components. We present cost functions for evaluating a task subject to probabilistic assumptions about the functioning of the circuits. The circuit evaluation method used in the simulation process is significant. We apply lazy evaluation, a demand-driven evaluation strategy in which signals in the circuit are evaluated on a `need to do' basis, resulting in a considerable saving in circuit simulation time. We achieve distributed logic simulation using a network of workstations and show from experimental results that by using such a configuration, we essentially obtain a single computation engine which can be used to obtain speedups in circuit simulation when compared with uniprocessor simulation systems. Interprocess communications between tasks on different workstations proceed via remote procedure calls while local communications between tasks take place via shared memory. The method of partitioning used in the circuit model ensures that communications between tasks take place only at defined times in the simulation sequence. 
47|-||Author index|
47|-||Subject index|
47|1|http://www.sciencedirect.com/science/journal/13837621/47/1|High-level co-simulation based on the extension of processor simulators|Hardware–software co-design is the cornerstone in the design of complex systems that involve both hardware and software. This paper presents a co-design approach where the co-simulation between hardware and software takes place early enough in the design cycle. The proposed platform is based on the extension of existing instruction set processor simulators in order to encapsulate hardware block description in the required and adequate accuracy level. The simplicity of the proposed technique as well as the use of homogeneous simulation environment, leads to a co-simulation alternative that is easy to implement and use. The applicability of the co-simulation environment developed is exhibited through the design and co-simulation, at various abstraction layers, of a telecommunication application. The latter, is based on the MAC layer and the RF-IF part of the Physical layer of the DECT protocol stack. 
47|1||Symbolic two-dimensional minimization of strongly unspecified finite state machines|A generalization of the classical state minimization problem for finite state machine (FSM) is proposed and discussed. In contrast to classical state minimization algorithms that minimize in one dimension (states), our algorithm minimizes the FSM in two dimensions: the numbers of both input symbols and internal state symbols are minimized in an iterative sequence of input minimization and state minimization procedures. This approach leads to an input decomposition of the FSM. A modified formulation of the state minimization problem is also introduced, and an efficient branch-and-bound program, FMINI, is presented. FMINI produces an exact minimum result for each component minimization process and a globally quasi-minimum solution to the entire two-dimensional (2D) FSM minimization problem. For some benchmarks, especially those with a high percentage of don't cares, as those that occur in machine learning (ML), this approach produces a more optimum result than those produced by the state minimization alone. 
47|1||Labeled rough partitions â a new general purpose representation for multiple-valued functions and relations|In this paper, we present a new data structure for representing multiple-valued (MV) relations (functions in particular), both completely and incompletely specified, and an associated set of manipulation algorithms. Relations are represented by labeled rough partitions, structure similar to rough partitions introduced in [T. Luba, Decomposition of multiple-valued functions, in: Proceedings of the 25th ISMVL, 1995, pp. 256–261] but extended with labels to store the full information about relations. We present experimental results from comparison of our data structure to binary decision diagrams (BDDs) on binary functions (MCNC benchmarks) showing its superiority in terms of memory requirements in 73% cases. The new representation can be used to a large class of MV, completely and incompletely specified functions and relations, typical for machine learning and complex finite state machine (FSM) controller optimization applications. 
47|1||Secure communication protocols with discrete nonlinear chaotic maps|The discrete nonlinear chaotic maps (DNCMs) exploit a novel approach to encryption: the information is injected to a properly designed DNCM system and affects its dynamics. The evolution of a proper variable of this system composes the transmitted ciphertext. Consequently, this variable controls the dynamics of another DNCM system that acts as the decipher.The paper proposes two new secure communications protocols that utilise the peculiarities of the DNCM systems. At the first one, the DNCM systems are used to construct a unique authentication scheme. With the second protocol the participants instead of exchanging the symmetric key (encrypted with the public key) exchange the encryption components themselves. This protocol is well suited to the possibility for dynamic “randomized” construction of DNCM systems. Therefore, a new dimension to security is added: not only the symmetric encryption key of a communication session is randomized but also the encryption algorithm itself is generated randomly (subject to some design rules). 
47|1||Congestion-free embedding of 2(nâk) spanning trees in an arrangement graph|The arrangement graph An,k is not only a generalization of star graph (n−k=1), but also more flexible. In this investigation, we elucidate the problem of embedding of multiple spanning trees in an arrangement graph with the objective of congestion-free. This result is to report how to exploit 2(n−k) edge disjoint spanning trees in an arrangement graph, where each congestion-free spanning tree's height is 2k−1. Our scheme is based on a subgraph-partitioning scheme. First, we construct 2(n−k) base spanning trees in every An−k+2,2. Then, we recursively construct 2(n−k) spanning trees from every An−k+2,2 up to An,k by a bottom-up approach. This is a near-optimal result since all of possible edges in the base subarrangement An−k+2,2 are fully utilized. 
47|10|http://www.sciencedirect.com/science/journal/13837621/47/10|Behavioral test generation for the selection of BIST logic|The identification of the most suited BIST architecture is one of the bottlenecks in the actual application of self-testing techniques. The aim of this paper is the investigation of possible relations between the behavioral level specification of the circuit, and the structural level, where BIST logic is inserted. We propose to use behavioral test patterns to guide the selection of the most appropriate BIST architecture with respect to the given application as a trade-off between fault coverage and area overhead. The correlation between the behavioral analysis and the actual fault coverage of the inserted BIST logic has been shown on a number of benchmarks. 
47|10||Studies of the SEMATECH IDDq test data|In the first part of this paper, we studied a few variations of a current signature method on the SEMATECH test data. We discovered two important facts: (1) many troublesome IDDq behaviors in the present and future technology were found in the SEMATECH data, (2) for those troublesome data, a relation was observed between the mean and the variance of the test data. Based on these findings, we proposed the second order current signature technique which considers both the first order mean and the second order variance information to provide a more robust and more effective mean of die selection. We examined the IDDq testing data from SEMATECH by the proposed second order current signature technique and compared the results to those of traditional single threshold and delta-IDDq techniques. We found that the proposed second order analysis may enable a more robust way to IDDq testing. In particular, the proposed method correctly identified all 12 known bad dies, while the delta-IDDq allows five of them to pass. 
47|10||A prototype of a VHDL-based fault injection tool: description and application|This paper presents the prototype of an automatic and model-independent fault injection tool, to be used on an IBM-PC (or compatible) platform. The tool has been built around a commercial VHDL simulator and it is thought to implement different fault injection techniques. With this tool, a wide range of transient and permanent faults can be injected into medium-complexity models. Another remarkable aspect of the tool is the fact that it can analyse the results obtained from injection campaigns, in order to study the Error Syndrome of the system model and/or validate its fault-tolerance mechanisms. Some results of various fault injection campaigns carried out to validate the Dependability of a fault-tolerant microcomputer system are shown. We have analysed the pathology of the propagated errors, measured their latencies, and calculated both error detection and recovery latencies and coverages. 
47|10||A new approach for critical area estimation in VLSI|Calculation of the critical area is the main computational problem in the estimation of the yield of a VLSI layout. For efficient critical area calculations, a real defect related to photolithography is usually modeled as a circular disc for which only the diameter of the disc is used. The actual critical area of a real defect, however, is determined by its fluctuant directional extensions and should be the average over all the critical areas corresponding to those directional extensions. The available simple defect outline models may cause large errors in the critical area estimations. This paper presents a new approach for critical area estimation. This new approach consists of two integrated parts: a new theoretical framework for critical area evaluation where actual defect extensions can be estimated by a function rather than a constant, and a new defect outline model for estimating the directional extensions of a real defect based on the idea of piecewise linear interpolation. The new theoretical framework and the new defect outline model can be used to estimate the critical area in a more accurate way as shown in this paper. 
47|10||Quality-effective repair of multichip module systems|This paper proposes a new analytical approach for evaluating the effects of a repair process on the defect level of multichip module (MCM) systems during assembly, thereby identifying the optimal number of repair cycles. Repair of complex MCMs is required to improve the yield and quality of these systems, while preserving cost effectiveness. No analytical evaluation has been reported in the current technical literature. In the proposed approach, we employ a novel Markov-chain model, which is solved analytically in O(rN3) (where r is the maximum number of allowed repair cycles and N is the number of chips in the MCM). The proposed model is based on a previously proposed quality model  [36] for MCMs which does not incorporate the effect of a repair process on the defect level.The proposed model effectively relates the defect level to different figures of merit of repair, such as the probability of successfully repairing a fault (referred to as repairability) and the maximum allowed number of repair cycles. Parametric results show that the overall defect level decreases as the MCM yield increases due to the repair process; however, there exists a bound in the number of repair cycles, while still permitting an increase in repairability. The bound value provides the optimal number of repair cycles.Using these results, it is possible to predict a more accurate value of the defect level of MCMs by taking into account the different parameters affecting the repair process, while realistically reducing the defect level of the final MCM product. The cost of the repair process is analyzed and an example using industrial data is provided. The validity of the proposed approach is further accomplished through extensive Monte Carlo simulation. 
47|10||A fault tolerant multistage interconnection network with partly duplicated switches|A multistage interconnection network (MIN) with partly duplicated stages is proposed in this paper, and network performance and fault tolerance are analyzed. The MIN is a hybrid of a non-redundant baseline network and a conventional fault-tolerant MIN called an ELMIN. In the case of a MIN with N input terminals and N output terminals, switching elements (SEs) in the first and nth stages are duplicated where n=log2N, and four-input two-output SEs and two-input four-output SEs are employed in the second and (n−1)th stages respectively. These extra SEs and links are useful in improving the fault tolerance and performance of the MIN and do not complicate the routing algorithm. A comparison of an ELMIN with the proposed MIN shows that this new approach is superior both in theoretical throughput and performance in faulty cases, even though it requires at most 1.33 times as many links and cross points in the SEs. 
47|11|http://www.sciencedirect.com/science/journal/13837621/47/11|Folded-crossed hypercube: a complete interconnection network|In this paper, the complete folded-crossed hypercube FCQn is proposed based on the folded hypercube [IEEE Trans. Parall. Distrib. Syst. 2 (1) (1991) 31], the enhanced hypercube [IEEE Trans. Comput. 40 (3) (1991) 284] and the crossed hypercube CQn [IEEE Trans. Parall. Distrib. Syst. 3 (5) (1992) 513]. FCQn has such appealing properties as diameter of ⌈n/2⌉, degree of n+1, much short mean internode distance about 75–62.5% of that of n-cube for nâ©¾6, and very low message traffic density only about 0.667–0.625 for nâ©¾4. Therefore, FCQn is a high-performance-low-cost architecture. Additionally, a general formula of mean internode distance of the complete hypercube-type architecture is given in order to analyze different hypercube-type architectures easily. 
47|11||Studies on striping and buffer caching issues for the software RAID file system|A software RAID file system is defined as a system that distributes data redundantly across an array of disks attached to each of the workstations connected on a high-speed network. This configuration provides higher throughput and availability compared to conventional file systems. In this paper, we consider two specific issues regarding the distribution of data among the cluster, namely, striping and buffer caching for such an environment. Through simulation studies we compare the performance of various striping methods and show that for effective striping in software RAID file systems, it must take advantage of its flexible nature. Further, for buffer caching, we show that conventional caching schemes developed for distributed systems are insufficient, and that the Exclusively Old Data and Parity scheme that is presented in this paper, overcomes the limitations of the previously proposed schemes. 
47|11||A novel three-level architecture for large data warehouses|Classical architectures proposed so far for data warehouses show some drawbacks when adopted to work over large numbers of heterogeneous operational sources. In this paper we propose a variant of a three-level architecture for data warehouses that overcomes these drawbacks. However, in the application context under consideration, having a suitable architecture may be not enough for the design purposes. Indeed, data warehouse design in very large operational environments can be a quite hard problem to attack with traditional manual methodologies. In this paper, automatic techniques are also illustrated that are capable to produce the data warehouse design according to the proposed architecture, with a limited human intervention. 
47|12|http://www.sciencedirect.com/science/journal/13837621/47/12|A global approach to improve conditional hardware reuse in high-level synthesis|The degree of conditional hardware reuse achieved after a high-level synthesis process depends on two factors: the number of mutually exclusive (mutex) operations pairs that an algorithm can detect and the description style used by the designer when specifying the system. In this paper, we propose a method that deals with both aspects. First, we propose a simple and homogeneous mechanism to analyze the input description and identify all the mutex operations pairs, independently of the conditional constructs (IF or CASE) used to specify the control flow of the system, and independently of the operators (logic or relational) needed to specify the conditions. Second, we provide a collection of formal transformations on the input description in order to overcome the “non-intended” design decisions (related to implicit hardware reuse) taken by the designer when writing the system description. Their application produces a specification of the same behavior that leads to improved implementations––in terms of the degree of conditional reuse that is achieved. Both facilities are possible thanks to the chosen internal representation mechanism, which is a mathematical model of the description, and to an underlying formal calculus that allows the description to be correctly manipulated. These ideas have been implemented in an algorithm that obtains better results than previous approaches. 
47|12||NULL convention multiply and accumulate unit with conditional rounding, scaling, and saturation|Approaches for maximizing throughput of self-timed multiply–accumulate units (MACs) are developed and assessed using the NULL convention logic paradigm. In this class of self-timed circuits, the functional correctness is independent of any delays in circuit elements, through circuit construction, and independent of any wire delays, through the isochronic fork assumption  and , where wire delays are assumed to be much less than gate delays. Therefore self-timed circuits provide distinct advantages for System-on-a-Chip applications.First, a number of alternative MAC algorithms are compared and contrasted in terms of throughput and area to determine which approach will yield the maximum throughput with the least area. It was determined that two algorithms that meet these criteria well are the Modified Baugh–Wooley and Modified Booth2 algorithms. Dual-rail non-pipelined versions of these algorithms were first designed using the threshold combinational reduction method [3]. The non-pipelined designs were then optimized for throughput using the gate-level pipelining method [4]. Finally, each design was simulated using Synopsys to quantify the advantage of the dual-rail pipelined Modified Baugh–Wooley MAC, which yielded a speedup of 2.5 over its initial non-pipelined version. This design also required 20% fewer gates than the dual-rail pipelined Modified Booth2 MAC that had the same throughput. The resulting design employs a three-stage feed-forward multiply pipeline connected to a four-stage feedback multifunctional loop to perform a 72+32×32 MAC in 12.7 ns on average using a 0.25 Î¼m CMOS process at 3.3 V, thus outperforming other delay-insensitive/self-timed MACs in the literature. 
47|13|http://www.sciencedirect.com/science/journal/13837621/47/13|Dual-tree-based multicasting on wormhole-routed irregular switch-based networks|Recently, network of workstations (NOWs) is emerging as an inexpensive alternative to massively parallel processors (MPPs). The irregular switch-based networks are proposed to build NOWs for high performance parallel computing. In this paper, we address a dual-tree-based routing model and propose an efficient dual-tree-based multicasting algorithm with optimum level-based destination-switch partition strategy on irregular switch-based networks. The dual-tree-based routing scheme supports adaptive, distributed, and deadlock-free multicast on switch-based networks with double channels. We first describe a dual-tree structure established, based on the concept of network partitioning, from the irregular networks and prove that the multicasting based on this structure is deadlock-free. Then, an efficient multicast routing algorithm with optimum level-based partition strategy is proposed. Finally, the experimental results are given to show that the multicast performance based on tree-based multicasting scheme can be promoted by the dual-tree-based multicasting scheme with optimum level-based destination-switch partition strategy. 
47|13||Architectural differences of efficient sequential and parallel computers|In this paper we try to conclude what kind of a computer architecture is efficient for executing sequential problems, and what kind of an architecture is efficient for executing parallel problems from the processor architect's point of view. For that purpose we analytically evaluate the performance of eight general purpose processor architectures representing widely both commercial and scientific processor designs in both single processor and multiprocessor setups. The results are interesting. The most efficient architecture for sequential problems is a two-level pipelined VLIW (very long instruction word) architecture with few parallel functional units. The most efficient architecture for parallel problems is a deeply inter-thread superpipelined architecture in which functional units are chained. Thus, designing a computer for efficient sequential computation leads to a very different architecture than designing one for efficient parallel computation and there exists no single optimal architecture for general purpose computation. 
47|2|http://www.sciencedirect.com/science/journal/13837621/47/2|Interfaces for mixed-level simulation with sequential elements|It is well known that techniques such as performance modeling that can be used to effectively evaluate design alternatives early in the design process can greatly increase the quality of the ultimate implementation, while at the same time, decrease the design time. In order to gain the maximum benefit from performance modeling, it must be integrated into the design process such that the performance model can be directly refined into an implementation. This paper presents techniques for developing interfaces between abstract performance models and detailed behavioral models to enable this refinement process. These mixed-level modeling interfaces, as they are called, allow abstract performance models to be cosimulated with detailed behavioral models. 
47|2||The architecture of a Gaussian mixture Bayes (GMB) robot position estimation system|Modelling and reducing uncertainty are two essential problems with mobile robot localisation. In this paper, a new robot position estimator, the Gaussian mixture of Bayes (GMB) which utilises a density estimation technique, is introduced in particular. The proposed system, namely the GMB robot position estimator, which allows a robot's position to be modelled as a probability distribution, and uses Bayes' theorem to reduce the uncertainty of its location. In addition, we describe, in this paper, how our proposed system is capable of dealing with multiple sensors, as well as a single sensor only. Nevertheless, it is known that such multiple sensors could be used to raise more robust than the single sensor, in terms of obtaining accurate estimate over a robot's position. The GMB position estimator mainly consists of four modules such as sonar-based, sensor selection, sensor fusion, and sensor selection improved by combining it with sensor fusion. The proposed system is also illustrated with respect to minimising the uncertainty of a robot's position, using the Nomad200 mobile robot shown in Fig. 1. Eventually, it was found that the proposed system was capable of constraining the position error of the robot by the modularity of the system. 
47|2||Self-repairable GALs|This paper describes the concept of self-testable and self-repairable Generic Array Logic (GAL) devices for high security and safety applications. A design methodology is proposed for self-repairing of a GAL which is a kind of Electrically Programmable Logic Devices (EPLDs). The fault-locating and fault-repairing architecture with electrically re-configurable GALs is presented. It uses universal test sets, fault-detecting logic, and self-repairing circuits with spare devices. The design method allows to detect, diagnose, and repair of all multiple stuck-at faults which might occur on E2CMOS cells in programmable AND plane. A self-repairing methodology is presented, based on our design architecture. A “column replacement” method with extra columns is introduced that discards each faulty column entirely and replaces it with an extra column. The evaluation methodology proves that a self-repairable GAL will last longer in the field. It gives also information on how many extra columns a GAL needs to reach a lifetime goal, in terms of simulation looping time, until the GAL is not useful any more. Therefore, an ideal point could be estimated, where the maximum reliability can be reached with the minimum cost. 
47|2||Functional decomposition with an efficient input support selection for sub-functions based on information relationship measures|The functional decomposition of binary and multi-valued discrete functions and relations has been gaining more and more recognition. It has important applications in many fields of modern digital system engineering, such as combinational and sequential logic synthesis for VLSI systems, pattern analysis, knowledge discovery, machine learning, decision systems, data bases, data mining etc. However, its practical usefulness for very complex systems has been limited by the lack of an effective and efficient method for selecting the appropriate input supports for sub-systems. In this paper, a new effective and efficient functional decomposition method is proposed and discussed. This method is based on applying information relationship measures to input support selection. Using information relationship measures allows us to reduce the search space to a manageable size while retaining high-quality solutions in the reduced space. Experimental results demonstrate that the proposed method is able to construct optimal or near-optimal supports very efficiently, even for large systems. It is many times faster than the systematic support selection method, but delivers results of comparable quality. 
47|2||Validation of Patient Headache Care Education System (PHCES) using a software reuse reference model|The research goal in our study of educational medical information systems tools is to improve the performance of patient decision making by integrating medical professional information with computer-augmented information and to benefit patients for short and long periods of time. In part, this goal may be reached by cost-effective software development with validated reusable Patient Care Education Systems object design models, user interface, object-oriented approach for product line and with the help of a valid Patient Headache Care Education System (PHCES) derived from these models. 
47|2||Reachability analysis of large circuits using disjunctive partitioning and partial iterative squaring|Reachability analysis is an orthogonal, state-of-the-art technique for the verification and validation of finite state machines (FSMs). Due to the state space explosion problem, it is currently limited to medium-small circuits, and extending its applicability is still a key issue. Among the factors that limit reachability analysis, let us list: the peak binary decision diagrams (BDD) size during image computation, the BDD size to represent state sets, and very high sequential depth. Following the promising trend of partitioning, we decompose a finite state machine into “functioning-modes”. We operate on a disjunctive partitioned transition relation. Decomposition is obtained heuristically based on complexity, i.e., BDD size, or functionality, i.e., dividing memory elements into “active” and “idle” ones. We use an improved iterative squaring algorithm to traverse high-depth subcomponents. The resulting methodology attacks the above problems, lowering intermediate peak BDD size, and dealing with high-depth subcomponents. Experiments on a few industrial circuits and on some large benchmarks show the feasibility of the approach. 
47|2||Using fundamental electrical theory for varying time quantum uni-processor scheduling|
47|2||Conversations with fixed and potential participants|
47|3-4|http://www.sciencedirect.com/science/journal/13837621/47/3-4|Modern methods and tools in digital system design|
47|3-4||Quality-driven design in the system-on-a-chip era: Why and how?|Modern microelectronic technology enables implementation of a complete complex information processing system on a single chip. Progress in microelectronic technology is extremely fast and is outstripping the system designers' abilities to make use of the created opportunities. The complexity and quality of the microelectronics-based systems as well as their design and production cost and time tend to be more limited by the design methods and tools than by the microelectronic technology. Substantial improvement can only be achieved through development and application of a new generation of more suitable design paradigms, methods and tools. In this paper, some new opportunities and difficulties related to the system-on-a-chip technology are considered, the nature of the complex system design problems is analyzed and a quality-driven system design methodology is proposed and discussed. Some important implications are also considered of the modern quality concepts for design modeling, design exploration techniques and tools, design decision making, design reuse and design validation. 
47|3-4||Grammar-based design of embedded systems|Grammars define syntax of languages and as such have not been commonly considered as methods for design, despite well-known applications in computer science. Only in recent years grammar-based design has become a promising research field and the first commercial tools have appeared on the market. This paper reviews the basic concepts of applying grammars to electronic design – in particular to the device driver synthesis of communication protocols for embedded software, to the design of custom-hardware, and to the virtual prototyping of DSP systems. The paper shows the power of these methods, presents the latest research results and discusses future developments in this field. 
47|3-4||Constraints-driven design space exploration for distributed embedded systems|This paper presents a new method for design space exploration for distributed embedded systems. The method is based on constraint logic programming (CLP) and make it possible to model distributed embedded systems and design requirements using finite domain constraints. Design space exploration tools can then use this model to find different solutions satisfying constraints. An advantage of this method is its flexibility to defined design constraints and ability to mix both manual design decisions and automatic optimization methods. The solution of the set of constraints provides a final system implementation which, by definition, satisfies all imposed design constraints. Both algorithms guaranteeing optimal solutions and heuristic methods can be used in the optimization phase. 
47|3-4||V-SAT: A visual specification and analysis tool for system-on-chip exploration|We describe V-SAT, a tool for performing design space exploration of system-on-chip (SOC) architectures. The key components of V-SAT include EXPRESSION, a language for specification of the architecture, SIMPRESS, a simulator generator for analysis/evaluation of the architecture, and the V-SAT GUI front-end for easy specification and detailed analysis. We give a brief overview of the components (EXPRESSION, SIMPRESS and GUI) and, using an example DLX architecture, demonstrate V-SAT's usefulness in exploration for an embedded SOC codesign flow by specifying and evaluating several modifications to the pipeline structure of the processor. We believe that V-SAT provides a powerful environment, both for early design space exploration, as well as for the detailed design of SOC architectures. 
47|3-4||Kernel scheduling techniques for efficient solution space exploration in reconfigurable computing|Run-time reconfigurable systems have emerged as a flexible way of facing with a single device a wide range of applications. The adequate exploitation of the available hardware resources requires the development of design tools that support the design process. Task scheduling becomes a very critical issue in achieving the high performance that DSP and multimedia applications demand. This paper addresses task scheduling in reconfigurable computing for these kinds of applications. First, we discuss the main issues involved in the problem. Then, we propose a new methodology with some bounding and pruning techniques in order to produce an efficient exploration of the design space. This work has been developed for Morphosys, a coarse-grain multi-context reconfigurable architecture, and it can be successfully applied to systems with similar features. 
47|3-4||High-level synthesis using hierarchical conditional dependency graphs in the CODESIS system|In previous work in behavioral high-level synthesis (HLS), data-flow and control-flow dominated descriptions are treated separately. A result of such a separation is that efficient techniques have been developed for the HLS of data-flow dominated behavioral descriptions. However, HLS of control-flow dominated descriptions still lags behind. To close this gap in this paper we propose a complete HLS framework based on an internal design representation where control- and data-flows are uniformly represented and disposes a formal foundation. Based on it conditional behaviors can be efficiently scheduled combining conditional resource sharing (CSR), speculative execution (SE) and other formal graph transformations. These techniques and tools covering all HLS activities have been organized in the CODESIS tool destined for both research and educational purposes. 
47|3-4||An integrated system for developing regular array designs|This paper describes an integrated system for developing regular array designs based on the block description language Ruby. Ruby supports concise design description and formal verification. A parametrised Ruby description can be used in simulating, refining and visualising designs, and in compiling hardware implementations such as field programmable gate arrays. Our system enables rapid design production, while good design quality is achieved by (a) the efficient instantiation of device-specific libraries, (b) the size optimisation of bit-level components using the design refiner, and (c) the exploitation of regularity information at source level in the library composition process. The development and implementation of several median filters are used to illustrate the system. 
47|3-4||Execution cost interval refinement in static software analysis|Embedded system software timing and power consumption or, in general, execution costs are state- and input-data dependent. Therefore, formal analysis of such dependencies leads to execution cost intervals rather than single values. These intervals depend on system concurrency, execution paths and process states, as well as on target architecture properties. This paper presents an approach to modeling and analysis of process behavior using intervals. Unlike other static software analysis approaches, it considers program properties and the execution context, i.e. the current state and input of a process. The example of an ATM switch component demonstrates significant improvements in analysis precision. 
47|3-4||Design-for-testability to achieve complete coverage of delay faults in standard full scan circuits|We propose a testability enhancement technique for delay faults in standard scan circuits that does not involve modifications to the scan chain. Extra logic is placed on next-state variables, and if necessary, on primary inputs, and can be resynthesized with the circuit to minimize its hardware and performance overheads. The proposed technique allows us to achieve complete coverage of detectable delay faults by allowing any two-pattern test to be applied to the circuit through its functional path. In addition to the basic approach, we study the proposed procedure in the presence of a constraint that requires that extra logic would not be placed on the critical paths of the circuit. 
47|3-4||POPS: A tool for delay/power performance optimization|Based on an incremental path search algorithm, this paper addresses the problem of performance-driven path classification by sizing selected gates on the shortest and the longest identified paths of the circuit. Delay and power/area constraints are managed using circuit path sizing alternatives defined through a realistic evaluation of gate power and delay. Implemented in the performance optimization by path selection (POPS) tool, the accuracy of this technique is compared to evaluation obtained from industrial tools [23] on examples of path enumeration and optimization evaluated on several ISCAS'85 benchmarks. 
47|5|http://www.sciencedirect.com/science/journal/13837621/47/5|Theory and application of non-group cellular automata for message authentication|An ASIC based on cellular automata (CA) for data-authentication has been proposed in this paper. The scheme is designed from the analytical study of the state transition behaviour of non-group CA, and is significantly different from conventional approaches. Experimental studies confirm the superiority in terms of CPU time (50% reduction) of the software implementation of the proposed scheme compared to standard MD-5 algorithm, the most commonly used method. Regular, modular and cascadable structure of CA with local interconnections makes the scheme ideally suitable for VLSI implementation. The CA based hardware can achieve further speed improvement approximately by at least three order, depending on the message length. The proposed scheme supports existing IPv4 data rates on high bandwidth links (800 Mbps HiPPI). A register in the datapath of a processor can be easily converted to a CA to realize the hardware. The design has been specified in Verilog, simulated for functional correctness, and synthesized using the tool Synergy from Cadence. 
47|5||Fundamental principles of modeling timing in hardware description languages|A fundamental property of digital hardware designs including VLSI designs is timing which underlies the occurrence of hardware activities and their relative ordering. In essence, timing constitutes the external manifestation of the causal relation between the relevant hardware activities. The constituent components of a hardware system are inherently concurrent and the relative time ordering of the hardware activities is critical to the correct functioning of complex hardware system. Hardware description languages (HDLs) are primarily developed for describing and simulating hardware systems faithfully and correctly and must, therefore, be capable of describing timing, accurately and precisely. This paper examines the fundamental nature of timing in hardware designs and develops, through reasoning, the basic principles of modeling timing in HDLs. This paper then traces the evolution of the key syntactic and semantic timing constructs in HDLs starting with CDL and up to the contemporary HDLs including ADLIB–SABLE, Verilog HDL, and VHDL, and critically examines them from the perspective of the basic principles of modeling timing in HDLs. Classical HDLs including CDL are limited to synchronous digital designs. In the contemporary hardware description languages including ADLIB–SABLE, CONLAN, Verilog, and VHDL, the timing models fail to encapsulate the true nature of hardware. While ADLIB and Verilog HDL fail to detect inconsistent events leading to the generation of potentially erroneous results, the concept of delta delay in VHDL which, in turn, is based on the BCL time model of CONLAN, suffers from a serious flaw. 
47|5||Specification-based program slicing and its applications|More precise program slices could be obtained by considering the semantic relations between variables of interest. In this paper, we present specification-based slicing that allows a better decomposition of the program by taking a specification as its slicing criterion. A specification-based slice consists of a subset of program statements which preserve the behavior and the correctness of the original program with respect to a specification given by a pre–postcondition pair. Because specification-based slicing enables one to focus attention on only those program statements which realize the functional abstraction specified by the given specification, it can be widely used in many software engineering areas. In order to investigate its possible applications, we show how specification-based slicing can improve the process for extracting reusable functions from existing programs and restructuring complex programs for better maintainability. 
47|5||Gray code clustering of wireless data for partial match queries|This paper proposes a broadcast data clustering method for partial match queries in mobile distributed systems. An effective broadcast data clustering method enables mobile clients to access the data in short latency. Our method utilizes the properties of the Gray coding scheme – Gray codewords have high locality. We describe the way the Gray code method (GCM) effectively clusters wireless data for partial match queries. And we analyze and evaluate the performance of the Gray code clustering method through comparison with other methods. 
47|5||Analysing value substitution and confidence estimation for value prediction|Value Prediction is one of the newest techniques used to break down ILP limits. Despite being under continuous study during the last few years, a few aspects related to this emerging technique remain unanalysed in depth. Exhaustively investigated in the context of control speculation, confidence estimation has usually played a secondary role on value prediction and speculation. Closely linked to confidence estimation, value substitution also represents a relegated subject of research. This paper is focussed on analysing, in an isolated way, the respective impact on predictor performance of both confidence estimation and value substitution mechanisms. By using detailed pipeline-level simulations, we prove that improvements in these mechanisms are as important as reducing the predictor aliasing or even improving the prediction model. 
47|6|http://www.sciencedirect.com/science/journal/13837621/47/6|Baldwinian learning utilizing genetic and heuristic algorithms for logic synthesis and minimization of incompletely specified data with Generalized ReedâMuller (ANDâEXOR) forms|This research applies a new heuristic combined with a genetic algorithm (GA) to the task of logic minimization for incompletely specified data, with both single and multi-outputs, using the Generalized Reed–Muller (GRM) equation form. The GRM equation type is a canonical expression of the Exclusive-Or Sum-of-Products (ESOPs) type, in which for every subset of input variables there exists not more than one term with arbitrary polarities of all variables. This AND–EXOR implementation has been shown to be economical, generally requiring fewer gates and connections than that of AND–OR logic. GRM logic is also highly testable, making it desirable for FPGA designs. The minimization results of this new algorithm tested on a number of binary benchmarks are given. This minimization algorithm utilizes a GA with a two-level fitness calculation, which combines human-designed heuristics with the evolutionary process, employing Baldwinian learning. In this algorithm, first a pure GA creates certain constraints for the selection of chromosomes, creating only genotypes (polarity vectors). The phenotypes (GRMs) are then learned in the environment and contribute to the GA fitness (which is the total number of terms of the best GRM for each output), providing indirect feedback as to the quality of the genotypes (polarity vectors) but the genotype chromosomes (polarity vectors) remain unchanged. In this process, the improvement in genotype chromosomes (polarity vectors) is the product of the evolutionary processes from the GA only. The environmental learning is achieved using a human-designed GRM minimization heuristic. As much previous research has presented the merit of AND–EXOR logic for its high density and testability, this research is the first application of the GRM (a canonical AND–EXOR form) to the minimization of incompletely specified data. 
47|6||Strategies for solving the Boolean satisfiability problem using binary decision diagrams|The Boolean satisfiability (SAT) problem is the problem of finding a solution to the equation f=1, where f is a Boolean formula to be satisfied. Binary decision diagrams (BDDs) have been widely used to solve this problem; each of the individual output requirements of a multiple-output function is represented as a BDD and the conjunction of these requirements (product BDD) provides all satisfying solutions. However, these techniques suffer from BDD size explosion problems. This paper presents two BDD-based algorithms to solve the SAT problem that attempt to contain the growth of BDD size while identifying solutions quickly.The first algorithm, called BSAT, is a recursive, backtracking algorithm that uses an exhaustive search to find a SAT solution. It exploits the well-known unate recursive paradigm to reduce the effective size of search space for the SAT problem. We recursively apply orthonormal expansion on highly binate functions that may eventually lead to unate cofactors. The second algorithm, called IS-USAT (for INCOMPLETE-SEARCH-USAT), incorporates an incomplete search to find a solution. The search is incomplete inasmuch as it is restricted to only those regions that have a high likelihood of containing the solution, discarding the rest. Using our techniques we were able to find SAT solutions not only for all MCNC and ISCAS benchmarks, but also for a variety of industry standard designs, solutions for many of which could not be found by contemporary BDD-based SAT techniques. 
47|6||Efficient local memory sequence generation for data parallel programs using permutations|Generating local memory access sequence is a critical issue in distributed-memory implementations of data-parallel languages. In this paper, for arrays distributed block-cyclically on multiple processors, we introduce a novel approach to the local memory access sequence generation using the theory of permutation. By compressing the active elements in a block into an integer, called compress number, and exploiting the fact that there is a repeating pattern in the access sequence, we obtain the global block cycle. Then, we show that the local block cycle can be efficiently enumerated as closed forms using the permutation of global block cycle. After decompressing the compress number in the local block cycle, the local block patterns are restored and the local memory access sequence can be quickly generated. Unlike other works, our approach incurs no run-time overhead. 
47|6||Efficient parallel timing simulation of synchronous models on networks of workstations|In this paper we address the parallel timing simulation of synchronous VLSI designs on a network of workstations (NOWs). We suggest combining cycle based and conventional timing simulation techniques to achieve fast timing simulation even on NOWs which are typically characterized by low bandwidth and high communication latency. In particular we execute a timing simulator on each node of the NOW and use cycle based simulation to produce synchronization information required by the timing simulators. As synchronization information is generated exclusively by the cycle based simulator there is no need for any communication between the timing simulators. To verify the feasibility and performance of our approach we simulated several circuits using our approach. The results show that a significant speedup can be achieved even for very small circuits. 
47|6||Buffer management control in data transport network node|The application of analytical queuing theory results in behaviour analysis of a distributed computer network or mobile data system (data transport network). It belongs to the preferred method in comparison to the simulation method. The use of analytical methods allows us to calculate effectively various values of parameters in equilibrium, including the total input intensity of data units to every node of transport network even for the more realistic models than the M/M/1 systems. However, these results are derived assuming an infinite buffer in size at a given node. For practical application, we need to project the concrete number of buffers in every node. This paper describes the method of buffer management control for each decomposed network's node of a data transport network in two real cases. For this purpose, the linear dependence between buffer memory size and input queue size at each node of a data transport network was used.For these two real statistical distributions of incoming data units closed expressions are derived, enabling to calculate the required queue size for both queue size limitation methods (assumption of the unlimited and limited queue sizes). For practical use, a very efficient way of computing queue overflow probabilities was developed. 
47|6||Embedding of complete binary tree with 2-expansion in a faulty Flexible Hypercube|Although the embedding of complete binary trees in faulty hypercubes has received considerable attention, to our knowledge, no paper has demonstrated how to embed a complete binary tree in a faulty Flexible Hypercube. Therefore, this investigation presents an algorithm to facilitate the embedding job when the Flexible Hypercube contains faulty nodes. Of particular concern are the network structures of the Flexible Hypercube that balance the load before as well as after faults start to degrade the performance of the Flexible Hypercube. Furthermore, to obtain the replaceable node of the faulty node, 2-expansion is permitted such that up to (n−2) faults can be tolerated with congestion 1, dilation 4 and load 1. 
47|6||Maximizing reliability of distributed computing system with task allocation using simple genetic algorithm|Reliability is one of the very important characteristics of the distributed computing system (DCS), and articles on task allocation (an NP-Hard problem) to maximize the reliability of DCS have appeared in the past [S. Kartik, C.S. Ram Murthy, IEEE Trans. Comput. 46 (6) (1997) 719; S.M. Shatz, Wang, Goto, IEEE Trans. Comput. 41 (90) (1992) 1156]. Genetic Algorithm (GA) has emerged as a successful tool for optimization purposes. We, in this work, have used a simple GA to optimize the reliability of DCS with task allocation. 
47|7|http://www.sciencedirect.com/science/journal/13837621/47/7|Special issue on Evolutionary computing|
47|7||Scalable architecture for parallel distributed implementation of genetic programming on network of workstations|We present an approach for developing a scalable architecture for parallel distributed implementation of genetic programming (PDIGP). The approach is based on exploitation of the inherent parallelism among semi-isolated subpopulations in genetic programming (GP). Proposed implementation runs on cost-efficient configurations of networks on workstations in LAN and Internet environment. Developed architecture features single global migration broker and centralized manager of the semi-isolated subpopulations, which contribute to achieving quick propagation of the globally fittest individuals among the subpopulations, reducing the performance demands to the communication network, and achieving flexibility in system configurations by introducing dynamically scaling up opportunities. PDIGP exploits distributed component object model (DCOM) as a communication paradigm, which as a true system model offers generic support for the issues of naming, locating and protecting the distributed entities in proposed architecture of PDIGP. Experimentally obtained results of computational effort of proposed PDIGP are discussed. The results show that computational effort of PDIGP marginally differs from the computational effort in canonical panmictic GP evolving single large population. For PDIGP running on systems configurations with 16 workstations the computational effort is less than panmictic GP, while for smaller configurations it is insignificantly more. Analytically obtained and empirically proved results of the speedup of computational performance indicate that PDIGP features linear, close to ideal characteristics. Experimentally obtained results of PDIGP running on configurations with eight workstations show close to 8-fold overall speedup. These results are consistent with the anticipated cumulative effect of the insignificant increase of computational effort for the considered configuration and the close to linear speedup of computational performance. 
47|7||Coevolving functions in genetic programming|In this paper we introduce a new approach to the use of automatically defined functions (ADFs) within genetic programming. The technique consists of evolving a number of separate sub-populations of functions which can be used by a population of evolving main programs. We present and refine a set of mechanisms by which the number and constitution of the function sub-populations can be defined and compare their performance on two well-known classification tasks. A final version of the general approach, for use explicitly on classification tasks, is then presented. It is shown that in all cases the coevolutionary approach performs better than traditional genetic programming with and without ADFs. 
47|7||System identification using evolutionary Markov chain Monte Carlo|System identification involves determination of the functional structure of a target system that underlies the observed data. In this paper, we present a probabilistic evolutionary method that optimizes system architectures for the identification of unknown target systems. The method is distinguished from existing evolutionary algorithms (EAs) in that the individuals are generated from a probability distribution as in Markov chain Monte Carlo (MCMC). It is also distinguished from conventional MCMC methods in that the search is population-based as in standard evolutionary algorithms. The effectiveness of this hybrid of evolutionary computation and MCMC is tested on a practical problem, i.e., evolving neural net architectures for the identification of nonlinear dynamic systems. Experimental evidence supports that evolutionary MCMC (or eMCMC) exploits the efficiency of simple evolutionary algorithms while maintaining the robustness of MCMC methods and outperforms either approach used alone. 
47|7||Conditions for the convergence of evolutionary algorithms|This paper presents a theoretical analysis of the convergence conditions for evolutionary algorithms. The necessary and sufficient conditions, necessary conditions, and sufficient conditions for the convergence of evolutionary algorithms to the global optima are derived, which describe their limiting behaviors. Their relationships are explored. Upper and lower bounds of the convergence rates of the evolutionary algorithms are given. 
47|7||Evolving good hierarchical decompositions of complex systems|Many application areas represent the architecture of complex systems by means of hierarchical graphs containing basic entities with directed links between them, and showing the decomposition of systems into a hierarchical nested “module” structure. An interesting question is then: How best should such a complex system be decomposed into a hierarchical tree of nested “modules”? This paper describes an interesting complexity measure (based on an information theoretic minimum description length principle) which can be used to compare two such hierarchical decompositions. This is then used as the fitness function for a genetic algorithm (GA) which successfully explores the space of possible hierarchical decompositions of a system. The paper also describes the novel crosssover and mutation operators that are necessary in order to do this, and gives some examples of the system in practice. 
47|7||Hybrid evolutionary motion planning using follow boundary repair for mobile robots|This paper presents a hybrid evolutionary motion planning simulation system for mobile robots operating in unstructured environments. We have designed a new obstacle representation method named cross-line, a follow boundary repair approach, and a hybrid evolutionary motion planning algorithm. A high-level navigator uses the approach and algorithms to implement mobile robot motion planning in various environments. An interactive graphical user interface is developed and the DLL technique is used to implement this system. A group of experiments was conducted. The results demonstrate that this system has high performance, effectiveness and flexibility. 
47|7||Learning feed-forward and recurrent fuzzy systems: A genetic approach|In this paper, we present a new learning method for rule-based feed-forward and recurrent fuzzy systems. Recurrent fuzzy systems have hidden fuzzy variables and can approximate the temporal relation embedded in dynamic processes of unknown order. The learning method is universal i.e., it selects optimal width and position of Gaussian like membership functions and it selects a minimal set of fuzzy rules as well as the structure of the rules. A genetic algorithm (GA) is used to estimate the fuzzy systems which capture low complexity and minimal rule base. Optimization of the “entropy” of a fuzzy rule base leads to a minimal number of rules, of membership functions and of subpremises together with an optimal input/output (I/O) behavior. Most of the resulting fuzzy systems are comparable to systems designed by an expert but offers a better performance. The approach is compared to others by a standard benchmark (a system identification process). Different results for feed-forward and first-order recurrent fuzzy systems with symmetric and non-symmetric membership functions are presented. 
47|7||Autotuning a PID controller: A fuzzy-genetic approach|A new method for tuning the parameters of the proportional integral derivative (PID) controller is presented in this paper. The technique adopted in this proposition is based on the format of dead-beat control. Fuzzy inference mechanism has been used here for predicting the future values of the controller output while crisp consequent values of the rulebase of the Takagi–Sugeno model are optimized using a genetic algorithm. The proposition is an extension of the work in R. Bandyopadhyay, D. Patranabis (A new autotuning algorithm for PID controllers using dead-beat format, ISA Trans., accepted for publication) where the rulebase was prepared based on the knowledge of process experts. The use of genetic algorithm for optimizing the crisp values of the rulebase has considerably improved the performance of the PID autotuner. The proposed algorithm seems to be a complete and generalized PID autotuner as can be seen from the simulated and experimental results. In all the cases the method shows substantial improvement over the controller tuned with Ziegler–Nichols formula and the PID controller proposed in (loc cit). 
47|8|http://www.sciencedirect.com/science/journal/13837621/47/8|Host-diagnosis algorithms for parallel systems|This paper presents an off-line diagnosis strategy for parallel message-passing systems. This strategy, called host-diagnosis, allows a host system to perform centralized diagnosis of the system state, given results of distributed tests performed among the system processors. It is useful for a manufacturing or a maintenance test. Three algorithms that use the host-diagnosis strategy are proposed. Their performance are evaluated using a queuing network model and compared to those of a classic distributed self-diagnosis algorithm. Obtained results show important diagnosis latency savings in case of host-diagnosis algorithms in comparison with the self-diagnosis algorithm. 
47|8||A robust stack folding approach for Java processors: an operand extraction-based algorithm|Data dependency in stack operations limits the performance of Java processors. To enhance Java's performance, existing literature suggests using stack operations folding. We extend this concept in a new folding algorithm that identifies principle operations in folding groups and extracts necessary operands from the bytecode queue. The proposed algorithm permits nested pattern folding and multiple issue of folding groups. Hence, the need for and therefore the limitations of a stack are eliminated. This paper discusses various aspects of the proposed algorithm and illustrates different folding scenarios as well as possible hazards. Benchmarking using SPECjvm98 shows excellent performance gains as compared to existing algorithms. 
47|8||Adding static data dependence collapsing to a high-performance instruction scheduler|State-of-the-art processors achieve high performance by executing multiple instructions in parallel. However, the parallel execution of instructions is ultimately limited by true data dependencies between individual instructions. The objective of this paper is to present and quantify the benefits of static data dependence collapsing, a non-speculative technique for reducing the impact of true data dependencies on program execution time. Data dependence collapsing involves combining a pair of instructions when the second instruction is directly dependent on the first. The two instructions are then treated as a single entity and are executed together in a single functional unit that is optimised to handle functions with three input operands instead of the traditional two inputs. Dependence collapsing can be accomplished either dynamically at run time or statically at compile time. Since dynamic dependence collapsing has been studied extensively elsewhere, this paper concentrates on static dependence collapsing. To quantify the benefits of static dependence collapsing, we added a new dependence collapsing option to the Hatfield Superscalar Scheduler (HSS), a state-of-the-art instruction scheduler that targets the Hatfield Superscalar Architecture (HSA). We demonstrate that the addition of dependence collapsing to HSS delivers a significant performance increase of up to 15%. Furthermore, since HSA already executes over four instructions in each processor cycle without dependence collapsing, dependence collapsing enables 0.4 additional instructions to be executed in each processor cycle. 
47|8||Modelling adaptive routing in circuit switched networks|A performance model of circuit switched k-ary n-cubes with deterministic routing has recently been proposed by Sharma and Varvarigos [IEEE Trans. Parallel Distrib. Syst. 8 (4) (1997) 349]. Many studies have revealed that using adaptive routing along with virtual channels considerably improves network performance over deterministic routing. This paper presents the first analytical model for calculating the message latency in circuit switched k-ary n-cube networks with adaptive routing. The main feature of the proposed model is the use of Markov chains to compute the path set-up time and to capture the effects of using virtual channels to reduce message blocking in the network. The mean waiting time that a message experiences at a source node is calculated using an M/G/1 queueing system. The validity of the model is demonstrated by comparing analytical results with those obtained through simulation experiments. 
47|9|http://www.sciencedirect.com/science/journal/13837621/47/9|Segmenting endoscopic images using adaptive progressive thresholding: a hardware perspective|Hardware realization of a novel technique based on adaptive progressive thresholding (APT) for the real-time segmentation of endoscopic images is presented. The APT algorithm is mapped onto a linear array of processing elements with each element of a particular segment communicating with its nearest neighbours. The efficiency and hardware portability of this technique justifies its use in applications that require high performance in real-time. 
47|9||An adaptive approach to achieving hardware and software fault tolerance in a distributed computing environment|This paper focuses on the problem of providing tolerance to both hardware and software faults in independent applications running on a distributed computing environment. Several hybrid-fault-tolerant architectures are identified and proposed. Given the highly varying and dynamic characteristics of the operating environment, solutions are developed mainly exploiting the adaptation property. They are based on the adaptive execution of redundant programs so as to minimise hardware resource consumption and to shorten response time, as much as possible, for a required level of fault tolerance. A method is introduced for evaluating the proposed architectures with respect to reliability, resource utilisation and response time. Examples of quantitative evaluations are also given. 
47|9||Unsafety vectors: a new fault-tolerant routing for the binary n-cube|This paper presents a new fault-tolerant routing algorithm for the binary n-cube which overcomes the limitations of the recently-proposed safety vectors algorithm (IEEE Trans. Parallel Distribut. Syst. 9 (4) (1998) 321). The algorithm is based on the concept of “unsafety vectors”. Each node A starts by computing a first level unsafety set, S1A, composed of the set of unreachable neighbours. It then performs (m−1) exchanges with its neighbours to determine the k-level unsafety set, SkA, for all 1â©½kâ©½m, where m is an adjustable parameter between 1 and n. SkA represents the set of all nodes at Hamming distance k from node A which are faulty or unreachable from A due to faulty nodes (or links). Equipped with these unsafety sets, each node calculates unsafety vectors, which are then used to achieve an efficient fault-tolerant routing in the binary n-cube. The kth element of the unsafety vector of node A represents a measure of the routing unsafety at distance k from A. We present an analytical study proving some properties of the proposed algorithm. We also conduct a comparative analysis through extensive simulation experiments that reveal the superiority of the proposed algorithm over the safety vectors algorithm (IEEE Trans. Parallel Distribut. Syst. 9 (4) (1998) 321) in terms of different performance measures, e.g. routing distances and percentage of reachability. 
47|9||On the merits of hypermeshes and tori with adaptive routing|Most existing multicomputers employ the torus topology along with deterministic routing to ensure simple router implementation, and thus fast communication. Efficient adaptive routing algorithms with minimum implementation requirements have recently been proposed to overcome the limitations of deterministic routing. Such algorithms have been incorporated in the latest generation of multicomputers, e.g. the Cray T3E, which are still based on low-dimensional k-ary n-cubes. Our previous studies have shown that a hypergraph network, referred to as the distributed crossbar switch hypermesh (DCSH), has several topological and performance advantages over traditional k-ary n-cubes when deterministic routing is used. This paper evaluates the relative merits of the DCSH and a variant of k-ary n-cubes, the torus, in the context of adaptive routing. The evaluation takes into account the effects of increased switching delays due to adaptivity, and implementation costs for various technologies (e.g. VLSI and multiple-chip technology). The results reveal that the DCSH is a potential alternative as a future high-performance multicomputer network, which can fully exploit the benefits of adaptive wormhole routing. Even though the torus has higher bandwidth channels than its DCSH counterpart, due to its simpler interconnect structure, adaptivity cannot reduce its higher message blocking delays inherent in its topology. 
47|9||A PCI bus simulation framework and some simulation results on PCI standard 2.1 latency limitations|We describe a simulation environment that allows us to simulate the standard peripheral component interconnect (PCI) bus protocol, as well as modified PCI protocols. While there are standard benchmarks (such as the SPEC [IEEE Comput. 33 (7) (2000) 28] benchmarks) available for processor simulation, database system simulation, and now even for simulating embedded systems (from EDN Embedded Microprocessor Benchmarking Consortium, EEMBC, http://www.eembc.org), there are no standard benchmarks for simulating computer buses in general and specifically, for simulating the PCI bus. To address this problem we describe a methodology for gathering information about the PCI traffic from a real system, and to use this information in order to generate PCI cycles that drive the simulator for both standard and modified PCI protocols. Finally, we use the simulation environment to run experiments with various parameters of the standard PCI protocols, and an extension that involves transferring a hint about the expected latency on the data bus at the time the target ends the current burst transaction. 
volume|issue|url|title|abstract
48|-|http://www.sciencedirect.com/science/journal/13837621/48|Editorial board|
48|-||Approximate prefix coding for system-on-a-chip programs|The redundancy available in binary programs presents an opportunity for better utilization of limited memory resources in systems-on-a-chip by compressing the instruction memory. Class-based coding, a form of approximate prefix coding, simplifies the code and thus is a suitable compression method for low-cost systems. We present a detailed frequency analysis of the SPEC2000 Alpha binary programs. Based on the results of this analysis, we introduce a new method for constructing classes. Then we apply this method and use it to compress the SPEC2000 Alpha binaries, and show that, in comparison with optimal prefix coding, the loss in compression efficiency is minimal. With some hardware support, the proposed method reduces the on-chip code by 43%. 
48|-||Modeling and evaluating the time overhead induced by BER in COMA multiprocessors|Designing multiprocessors based on distributed shared memory (DSM) architecture considerably increases their scalability. But as the number of nodes in a multiprocessor increases, the probability of encountering failures in one or more nodes of the system raises as a serious problem. Thus, every large-scale multiprocessor should be equipped with mechanisms that tolerate node failures. Backward error recovery (BER) is one of the most feasible strategies to build fault tolerant multiprocessors and it can be shown that among various DSM-based architectures, cache only memory architecture (COMA) is the most suitable for implementing BER. The main reason is the existence of built-in mechanisms for data replication in COMA memory system. BER is applicable to COMA multiprocessors with minor hardware redundancy, but it will obviously cause some other kinds of overheads. The most important overhead induced by BER is the time required to produce and store recovery data. This paper introduces an analytical model for predicting the amount of this time overhead and then verifies the correctness of the model through comparing the results predicted from this model with the previously published simulation results. Both the analytical model and simulation results show that the overhead is nearly independent of the number of nodes. The immediate result is that BER is a cost-effective strategy for tolerating node failures in large-scale COMA multiprocessors with large numbers of nodes. 
48|-||Permutation routing in double-loop networks: design and empirical evaluation|A double-loop network is an undirected graph whose nodes are integers 0,1,…,n−1 and each node u is adjacent to four nodes u±h1(mod>n), u±h2(mod>n), where 0<h1<h2<n/2. There are initially n packets, one at each of the n nodes. The packet at node u is destined to node Ï(u), where the mapping uâ¦Ï(u) is a permutation. The aim is to minimize the number of routing steps to route all the packets to their destinations. If â is the tight lower bound for this number, then the best known permutation routing algorithm takes, on average, 1.98â routing steps (and 2â routing steps in the worst-case).Because the worst-case complexity cannot be improved, we design four new static permutation routing algorithms with gradually improved average-case performances, which are 1.37â, 1.35â, 1.18â, and 1.12â. Thus, the best of these algorithms exceeds the optimal routing by at most 12% on average.To support our algorithm design we develop a program which simulates permutation routing in a network according to the given topology, routing model as well as communication pattern and measure several quality criteria. We have tested our algorithms on a large number of double-loop networks and permutations (randomly generated and standard). 
48|-||High-level algorithmic complexity evaluation for system design|The increasing complexity of processing algorithms has led to the need of more and more intensive specification and validation by means of software implementations. As the complexity grows, the intuitive understanding of the specific processing needs becomes harder and harder. Hence, the architectural implementation choices or the choices between different possible software/hardware partitioning become extremely difficult tasks. Moreover, it is also desirable to understand and measure the algorithm complexity at the highest possible level near to the algorithmic level so as to be able to take the more appropriate actions. Automatic tools to perform such analysis become nowadays a fundamental need.In this paper, the requirements of a suitable algorithmic complexity evaluation technology are discussed, with a particular emphasis to the problem of the analysis of multimedia systems and signal processing algorithms. A brief review about limitations and weaknesses of existing tools is given, specifying the characteristics of ideal “complexity evaluation systems”. A new approach is described, called here Software Instrumentation Tool, SIT, yielding an automatic software tool able to extract information not depending on the simulation platform, keeping into account specific input data and resulting in a good and useful measure of the desired high-level algorithmic complexity. 
48|-||Highly accurate and efficient evaluation of randomising set index functions|Randomising set index functions can reduce the number of conflict misses in data caches by spreading the cache blocks uniformly over all sets. Typically, the randomisation functions compute the exclusive ors of several address bits. Not all randomising set index functions perform equally well, which calls for the evaluation of many set index functions. This paper discusses and improves a technique that tackles this problem by predicting the miss rate incurred by a randomisation function, based on profiling information. A new way of looking at randomisation functions is used, namely the null space of the randomisation function. The members of the null space describe pairs of cache blocks that are mapped to the same set. This paper presents an analytical model of the error made by the technique and uses this to propose several optimisations to the technique. The technique is then applied to generate a conflict-free randomisation function for the SPEC benchmarks. 
48|-||Author index to Volume|
48|-||Subject index to Volume|
48|1-3|http://www.sciencedirect.com/science/journal/13837621/48/1-3|Editorial Board|
48|1-3||An analytical POC stack operations folding for continuous and discontinuous Java bytecodes|The execution performance of a stack-based Java virtual machine (JVM) is limited by the true data dependency. To enhance the performance of the JVM, a stack operations folding mechanism for the picoJava-I/II processor was proposed by Sun Microsystems to fold 42.3% stack operations. By comparing the continuous bytecodes with pre-defined folding patterns in instruction decoder, the number of push/pop operations in between the operand stack and the local variable could be reduced. In this study, an enhanced POC (EPOC) folding model is proposed to further fold the discontinuous bytecodes that cannot be folded in continuous bytecodes folding mechanisms. By proposing a stack re-order buffer (SROB) to help the folding check processes, the EPOC folding model can fold the stack operations perfectly with a small size of SROB implementation. Statistical data shows that the four-foldable strategy of the EPOC folding model can eliminate 98.8% of push/pop operations with an instruction buffer size of 7 bytes and the SROB size of eight entries. 
48|1-3||Virtual prototyping of PLC-based embedded system using object model of target and behavior model by converting RLL-to-statechart directly|A domain-specific virtual prototyping approach is proposed that can reduce the risks involved in programmable logic controllers (PLCs)-based embedded system programming. The proposed approach is based on an object-oriented real-time modeling concept, plus an algorithm is defined that can mechanically convert a PLC program, written in relay ladder logic (RLL), to a statechart model. In the field of virtual prototyping, statechart models are widely accepted as representing the behavior of target systems. Accordingly, the direct use of an RLL program enables a virtual prototype to be built quite easily, thereby eliminating complex behavior remodeling of the objects used in PLC embedded systems. As a case study, virtual prototyping of a target example was performed and analyzed to evaluate the benefit of the proposed approach. 
48|1-3||Hypermeshes: implementation and performance|Common multicomputer networks, including the torus, mesh, and hypercube, are graph topologies where a channel interconnects exactly two nodes. Hypergraphs are generalisations of the graph model, where a channel interconnects an arbitrary number of nodes. The spanning-bus hypercube is a well-known network that belongs to the hypergraph family. Regular multidimensional hypergraphs, also known as hypermeshes, have been proposed as potential alternatives to traditional graph networks for future multicomputers due to their superior topological and performance features. This paper compares the different schemes that have been proposed in the literature for implementing the hypermesh. The results reveal that one particular version of the hypermesh, known as the distributed crossbar switch hypermesh, provides the best performance when implementation costs are taken into account. 
48|1-3||Interface synthesis between software chip model and target board|This paper reports on the synthesis of interface between software chip model and target board in a behavioral emulation system called in-system algorithm verification engine (iSAVE). iSAVE performs in-system verification of the behavioral description of a chip in such high-level languages as C in the context of its application board at the early chip design stage. The interface between the target chip and the target board is implemented as two parts; software part running on a microprocessor in a multi-thread fashion and hardware part mapped into field programmable gate array logic. The proposed idea is validated by successfully demonstrating the behavioral emulation of MP3 decoder chip, i.e., running the MP3 decoding algorithm written in C along with the real MP3 player board minus the MP3 decoder chip itself through the proposed interface scheme. 
48|1-3||Design and analysis of static memory management policies for CC-NUMA multiprocessors|In this paper, we characterize the performance of three existing memory management techniques, namely, buddy, round-robin, and first-touch policies. With existing memory management schemes, we find several cases where requests from different processors arrive at the same memory simultaneously. To alleviate this problem, we present two improved memory management policies called skew-mapping and prime-mapping policies. By utilizing the properties of skewing and prime, the improved memory management designs considerably improve the application performance of cache coherent non-uniform memory access multiprocessors. We also re-evaluate the performance of a multistage interconnection network using these existing and improved memory management policies. Our results effectively present the performance benefits of different memory management techniques based on the sharing patterns of applications. Applications with a low degree of sharing benefit from the data locality provided by first-touch. However, several applications with significant sharing degrees as well as those with single processor initialization routines benefit highly from the intelligent distribution of data provided by skew-mapping and prime-mapping schemes. Improvements due to the new schemes are found to be as high as 35% in stall time. 
48|1-3||Building a dependable system from a legacy application with CORBA|This paper presents a dependability oriented, fault tolerance based system design, development, and deployment approach. The approach relies on an architectural framework, which allows legacy software modules to be reused as the basic building blocks of a distributed dependable application. Different levels of replication and alternative adjudication strategies are implemented behind a unified interface. These can be configured for achieving the optimal compromise between dependability and performance, according to application, deployment environment, and fault characteristics. The suggested solution can be implemented on top of any CORBA infrastructure. The architecture has been developed and tested. Experimental results are presented and discussed. 
48|1-3||Genetic engineering versus natural evolution: Genetic algorithms with deterministic operators|Genetic algorithms (GA) have several important features that predestine them to solve design problems. Their main disadvantage however is the excessively long run-time that is needed to deliver satisfactory results for large instances of complex design problems. The main aims of this paper are (1) to demonstrate that the effective and efficient application of the GA concept to design problem solving requires substitution of the basic GAs natural evolution by genetic engineering (GE), (2) to propose and discuss the concept of a genetic engineering algorithm (GEA), and (3) to show how to apply the GEA to solve synthesis problems. In this paper, an effective and efficient GE scheme is proposed and applied to solve an important design problem: the minimal input support problem. In almost all cases, our GEA produces strictly optimal results and realizes a very good trade-off between effectiveness and efficiency. The experimental results clearly demonstrate that the proposed GE scheme is suitable for solving design problems and its application results in very effective and efficient GEAs. 
48|11-12|http://www.sciencedirect.com/science/journal/13837621/48/11-12|Editorial board|
48|11-12||Permutation routing in optical MIN with minimum number of stages|In a hybrid optical multistage interconnection network (MIN), optical signals are routed by electronically controlled switches using directional couplers. A relevant design problem is to minimize the path-dependent loss of the optical signal, which is directly proportional to the number of couplers, i.e., the number of switches through which the signal has to pass. In general, given the network size and the type of the MIN, the number of stages is a constant. Hence, an input signal has to pass through a fixed number of couplers to reach the output.In this paper, it is shown that the routing delay and path-dependent loss in a fixed-stage N×N MIN, can be significantly reduced on the average by using a variable-stage shuffle-exchange network instead. An arbitrary N×N permutation P can be routed with minimum delay and minimum path-dependent loss, if the minimum number of stages of the MIN necessary to route P is known. An O(Nn) algorithm (N=2n) is presented here for checking the admissibility of a given permutation P in an m-stage shuffle-exchange network (SEN), where 1â©½mâ©½n. The minimum-stage SEN needed to pass P can then be determined in O(Nnlogn) time. Furthermore, for n<mâ©½2n−1, a necessary condition for permutation admissibility is derived which is shown to be necessary as well as sufficient for the special class of BPC (bit-permute-complement) permutations. It has been shown that, for 1â©½mâ©½2n−1, the minimum number of stages required to pass a BPC permutation P through a SEN can be determined in O(n2) time, and P can be routed through a variable-stage SEN using the minimum number of stages only. In an optical MIN, this technique helps to reduce the path-dependent loss by limiting the number of stages to be traversed by the optical signal. 
48|11-12||On the topological properties of the arrangementâstar network|This paper proposes a new interconnection network, referred to as the arrangement–star network, which is constructed from the product of the star and arrangement networks. Studying this new network is motivated by the good qualities it exhibits over its constituent networks, the star and arrangement networks. The star network has been a research focus for quite a long time until recently when the algorithm development on the star network turned out to be cumbersome. The arrangement network as a generalized class for the star network offers no solution in that direction. The arrangement–star network, on the other hand, makes it possible to efficiently embed grids, pipelines, as well as other computationally important topologies in a very natural manner. Furthermore, the fact that the product of the star and arrangement networks comes with little increase in the network diameter and a better result on communication cost, motivates further investigation for this new alternative, the arrangement–star network. 
48|11-12||A VLSI architecture for 3-D self-organizing map based color quantization and its FPGA implementation|Color quantization is the process of computing a color palette containing few best colors from a full color image and then associating to each pixel of the image, a color from the palette to yield a color quantized image that is close to the original image. It is of interest in applications such as digital display and image capture. To satisfy real time requirements, it is of vital importance to perform color quantization as fast as possible. This paper presents a novel architecture of a hardware unit for color quantization that is based on Kohonen’s self-organizing map. The proposed architecture is of the SIMD type and results in a scheme with linear time complexity (in the size of the image). The architecture has been implemented in Xilinx FPGA and results show that the proposed design achieves high speed taking only a few milliseconds for color quantization of images up to size of 512 × 512 with low area requirement. 
48|11-12||Bandwidth constrained smoothing for multimedia streaming with scheduling support|Providing a satisfactory multimedia service in networking environments requires an effective media delivery mechanism. However, a common network such as the Internet does not provide a guaranteed network bandwidth to accommodate multimedia service in a reliable fashion. A typical approach to assist multimedia delivery is via buffer management and task scheduling in end-systems. Buffer management techniques are classified into two categories; one is to adapt the changes in network load and the other is to smooth the bandwidth requirement. The former may cause a serious loss of service quality whereas the latter is unable to adapt to the dynamic network condition. In this paper, we propose a bandwidth-adaptive media smoothing technique which smoothes the bandwidth requirement for media delivery at run time by considering the availability of network bandwidth. Meanwhile, the bandwidth smoothing technique still has the possibility of causing jitter because the policy runs on the application layer so that it cannot guarantee task completion in time. Thus, we also propose a task scheduling algorithm optimized for the bandwidth adaptive smoothing. This scheduling technique handles the media data appropriately in order to minimize jitter. Simulation results with prerecorded MPEG videos show that the quality of delivered video is improved with the proposed bandwidth adaptive smoothing and task scheduling mechanisms. 
48|4-5|http://www.sciencedirect.com/science/journal/13837621/48/4-5|Editorial Board|
48|4-5||A fast and accurate delay dependent method for switching estimation of large combinational circuits|Assuming inertial gate delay model, the first-order temporal correlation and the structural dependencies, a probabilistic method to estimate the switching activity of a combinational circuit, is introduced. To capture the first temporal correlation a novel mathematical model and the associated new formulas are derived. Also, a modified boolean function, which describes the logic and timing behavior of each signal, is introduced. To capture the structural dependencies an efficient new method to partition a large circuit into small independent sub-circuits is proposed. Finally, an algorithm that evaluates the switching activity of any circuit node is presented. 
48|4-5||On the design of low power BIST for multipliers with Booth encoding and Wallace tree summation|Low power dissipation (PD) during testing is emerging as one of the major objectives of a built-in self-test (BIST) designer. In this paper we examine the testability of multipliers based on Booth encoding and Wallace tree summation of the partial products and we present a methodology for deriving a low power BIST scheme for them. We propose several design rules for designing the Wallace tree in order to be fully testable under the cell fault model. The proposed low power BIST scheme for the derived multipliers is achieved by: (a) introducing suitable test pattern generators (TPGs), (b) properly assigning the TPG outputs to the multiplier inputs and (c) significantly reducing the test set length. Results indicate that the total power dissipated, the average power per test vector and the peak PD during testing can be reduced up to 73%, 27% and 36% respectively with respect to earlier schemes, depending on the implementation of the basic cells and the size of the multiplier. The test application time is also significantly reduced, while the introduced BIST scheme implementation area is small. 
48|4-5||Increasing hardware data prefetching performance using the second-level cache|Techniques to reduce or tolerate large memory latencies are critical for achieving high processor performance. Hardware data prefetching is one of the most heavily studied solutions, but it is essentially applied to first-level caches where it can severely disrupt processor behavior by delaying normal cache requests, inducing cache pollution and occupying the heavily used bus to the second-level cache. In this article, we show that applying hardware data prefetching to the second level cache exhibits most of the benefits of first-level cache prefetching with almost none of its drawbacks. Moreover, we outline that second-level hardware data prefetching is particularly well suited to out-of-order (OoO) processors because it can hide the long memory latencies due to second-level cache misses while OoO execution of memory instructions can hide the lower latencies due to first-level cache misses that hit in the second-level cache. Finally, we show that when the full memory system is taken into account, especially bus traffic, first-level cache prefetching can actually degrade overall processor performance while second-level cache prefetching consistently improves overall performance. Our experimental results show that the instructions per cycle of floating-point programs (SPEC95) increases by 20% on a average using second-level cache hardware data prefetching while it decreases by 5% on a average using first-level cache hardware data prefetching. 
48|4-5||Scheduling expression trees for delayed-load architectures|In this paper we consider the problem of scheduling expression trees on delayed-load architectures. The problem tackled here takes root from the one considered in [Proceedings of the ACM SIGPLAN ’91 Conf. on Programming Language Design and Implementation, 1991. p. 256] in which the leaves of the expression trees all refer to memory locations. A generalization of this involves the situation in which the trees may contain register variables, with the registers being used only at the leaves. Solutions to this generalization are given in [ACM Trans. Prog. Lang. Syst. 17 (1995) 740, Microproc. Microprog. 40 (1994) 577]. This paper considers the most general case in which the registers are reusable. This problem is tackled in [Comput. Lang. 21 (1995) 49] which gives an approximate solution to the problem under certain assumptions about the contiguity of the evaluation order. Here we propose an optimal solution (which may involve even a non-contiguous evaluation of the tree). The schedule generated by the algorithm given in this paper is optimal in the sense that it is an interlock-free schedule which uses the minimum number of registers required. An extension to the algorithm incorporates spilling. The problem as stated in this paper is an instruction scheduling problem. However, the problem could also be rephrased as an operations research problem with a difference in terminology. 
48|6-7|http://www.sciencedirect.com/science/journal/13837621/48/6-7|Editorial Board|
48|6-7||Scheduling length for switching element disjoint multicasting in Banyan-type switching networks|Due to the stringent bit-error requirement in fiber optics, preventing the forthcoming crosstalk at switching elements (SEs) is very crucial in the large-scale photonic switches. In this paper, we consider the SE-disjoint multicasting for a photonic Banyan-type switching network. This ensures that at most, one connection holds each SE in a given time thus, neither photonic crosstalk nor link blocking will arise in the switching network implemented with optical devices such as directional couplers, splitters and combiners. Routing a set of connections under such constraint usually takes several routing rounds hence, it is desirable to keep the number of rounds (i.e., scheduling length) to a minimum. Unfortunately, finding the optimal length is NP-complete in general, we propose an algorithm that seeks an approximation solution less than double of the optimal upper bound. The bound permits the Banyan-type multicasting network to be rearrangeable nonblocking as well as crosstalk-free. It is given that the same bound guarantees wide-sense nonblocking for multicast connections, provided that the multicasting capability of the network is restricted to the second half of the whole stages, and strictly nonblocking for one-to-one connections, yet allowing to be free from the crosstalk. We also consider other crosstalk-free multicasting and the link-disjoint multicasting for the Banyan network. The theory developed in this paper gives a unified foundation on designing nonblocking and crosstalk-free photonic Banyan-type networks under the multicast connections. 
48|6-7||Methods for distributed unicast in hypercubes|"Unicast algorithms in off-line routing have been used for one-to-one communication between a source node and a destination node in an n-dimensional hypercube, denoted as Hn. A node is called k-safe, where 0â©½kâ©½n, if it has at least k healthy neighbors, and Hn is called k-safe if every node in it is k-safe. A k-safe Hn is connected if the number of faulty nodes, |F|, does not exceed 2k(n−k)−1. In this paper, we propose two methods for distributed routing. The first method has been presented in [Proc. 7th Int. IEEE Conf. Electron., Circ. Syst., Jounieh, Lebanon, December, 2000, p. 194]. The second method that has not been addressed before, can be used for off-line routing. In the case of off-line routing we avoid the cost of collecting global information about the faulty nodes, and the cost of getting information about Hn, whether it is k-safe or not. The minimum requirements of the proposed methods is to have the path between the source and the destination connected. Hence, they may work when Hn is disconnected, which is an important advantage. The time cost of the first method may be O(mn4), and the expected length of the routing path between source and destination may be O(mn5), where 1â©½mâ©½n. The time cost of the second method may be O(enn/2), the space cost may be O(enn/2), and the expected length of the routing path between source and destination may be d(s,t). "
48|6-7||Quantifying behavioral differences between multimedia and general-purpose workloads|Multimedia workloads are becoming increasingly more common on general-purpose computing systems. However, little quantitative results are available about the behavior of these workloads. This paper is a first step in quantifying and understanding the behavioral differences (if any) between multimedia and general-purpose workloads. This is done by comparing program characteristics of multimedia applications (coming from the MediaBench suite, the X benchmarks from the SimpleScalar distribution, plus a set of MPEG-4 like algorithms) and general-purpose applications (coming from the SPECint95 and the SPECint2000 benchmark suite). In addition to presenting a database of program characteristics, we conclude that (i) multimedia applications have less memory operations and less control operations in their instruction mix than general-purpose workloads, and thus are computationally intensive; (ii) multimedia and general-purpose applications exhibit comparable levels of instruction-level parallelism; (iii) multimedia applications also suffer from hard-to-predict branches and (iv) the instruction stream as well as the data stream of multimedia applications exhibit more spatial and more temporal locality than general-purpose applications. These results were obtained using statistical tests, namely the t-test and the Mann–Whitney test. 
48|6-7||PET, a software monitoring toolkit for performance analysis of parallel embedded applications|Since the late 1980s a great deal of research has been dedicated to the development of software monitoring and visualization toolkits for parallel systems. These toolkits have traditionally been oriented to measuring and analyzing parallel scientific applications. However, nowadays, other types of parallel applications, using signal-processing or image-processing techniques, are becoming increasingly importance in the field of embedded computing. Such applications are executed on special parallel computers, normally known as embedded multiprocessors, and they exhibit structural and behavioral characteristics very different from scientific applications. Because of this, monitoring tools with specific characteristics are required for measuring and analyzing parallel embedded applications.In this paper we present performance execution tracer (PET), a monitoring and visualization toolkit specifically developed to measure and analyze this type of application. Special emphasis is placed on explaining how PET deals with the particular characteristics of these applications. In addition, in order to demonstrate the capabilities of PET, the measurement and analysis of a real application using this tool are described. 
48|8-10|http://www.sciencedirect.com/science/journal/13837621/48/8-10|Editorial board|
48|8-10||Secure checkpointing|Fault-tolerant computer systems are increasingly being used in such applications as e-commerce, banking, and stock trading, where privacy and integrity of data are as important as the uninterrupted operation of the service provided. While much attention has been paid to the protection of data explicitly communicated over the Internet, there are also other sources of information leakage that must be addressed. This paper addresses one such source of information leakage caused by checkpointing, which is a common method used to provide continued operation in the presence of faults.Checkpointing requires the communication of memory state information, which may contain sensitive data, over the network to a reliable backing store. Although the method of encrypting all of this memory state information can protect the data, such a simplistic method is an overkill that can result in a significant slowdown of the target application. A much more efficient method is to use incremental checkpointing (IC), in which only the modified memory data is saved in stable storage. This paper examines ways to combine the operations required to perform IC with those required to encrypt this memory state data. Our analysis show that the proposed secure checkpointing schemes increase the overhead by 1.57 when compred to conventional checkpointing schemes, which shows the proposed schemes are feasible. 
48|8-10||Efficient communication sets generation for blockâcyclic distribution on distributed-memory machines|How to generate local memory access sequence and communication sets efficiently is an important issue in compiling a data-parallel language into a single program multiple data (SPMD) code for distributed-memory machines. Many methods have been developed for generating local memory access sequence. In this paper, we focus on the problem of communication sets generation. The local block distance between two active elements with the same offset and destination (source) in a processor will be investigated. We develop an algorithm for the sending phase and receive–execute phase, respectively. Our algorithms do not need to compute send and receive patterns and lose no communication sets while there exist incomplete blocks that cannot constitute a send or receive pattern. Experimental results showed that our method outperforms other previous work. 
48|8-10||Clustering and reassignment-based mapping strategy for message-passing architectures|A fundamental issue affecting the performance of a parallel application running on message-passing parallel systems is the assignment of tasks to processors. In this paper we present a compilation-time two stage mapping strategy (denoted as Task Allocation by Clustering, Reassignment and Embedding, TACRE) used for mapping arbitrary programs (modeled as task interaction graphs) onto message-passing parallel systems. The first stage is based on task clustering and task reassignment algorithms that contract the original task graph. The second stage takes the contracted graph and tries to well match the physical properties of the target system. The results shown that TACRE provides a good trade-off between mapping quality and computational complexity. 
48|8-10||New design methodology with efficient prediction of quality metrics for logic level design towards dynamic reconfigurable logic|The importance of efficient area and timing estimation is well established in high level synthesis (HLS) since it allows more efficient exploration of the design space while providing HLS tools with the capability of predicting the effects of technology specific tools on the design space. Much of the previous work has focused on estimation techniques that use very simple cost models based solely on functional units (FUs). Those models are not accurate enough to allow effective design space exploration since the effects of interconnects can indeed dominate the final design cost. The situation becomes even worst when the design is targeted to dynamically reconfigurable logic (DRL) technologies since the multiplexer delay may contribute heavily on the overall delay. In addition, large number of configurable logic blocks could be used for communication rather than for implementing FUs. In this paper we present a new HLS design flow, which performs an accurate estimation on area and timing for DRL circuits. It takes into account not only FUs area and delay, but also the interconnection and communication effects. We select our DRL LSI circuit [M. Meribout, M. Motomura, Method for compiling high level programs into hardware, Japanese Patent: JSP2000-313818, 2000; M. Motomura et al., An embedded DRAM-FPGA chip with instantaneous logic reconfiguration, in: Symposium on VLSI Circuits, July 1997, pp. 55–56] as our main concentration. We tested our method with several benchmarks and the results show that we receive good performance of the design, with area and timing estimated efficiently. 
volume|issue|url|title|abstract
49|-|http://www.sciencedirect.com/science/journal/13837621/49|Editorial board|
49|-||Preface|
49|-||Global approach to assignment and scheduling of complex behaviors based on HCDG and constraint programming|This paper presents global high-level synthesis (HLS) approach which addresses the problem of synthesis of conditional behaviors under resource constraints. In proposed methodology, the conditional behaviors are represented by hierarchical conditional dependency graphs (HCDG) and synthesized using derived constraints programming (CP) models. Our synthesis methods exploit multicycle operations and chaining as well as conditional resource sharing and speculative execution at the same time. We assign both functional units and registers while making possible to conditionally share these components. These techniques are essential in HLS and the experiments carried out using the developed prototype system showed good performance of the synthesized designs and proved the feasibility of the presented approach. 
49|-||Allocation of multiple precision behaviors for maximal bit level reuse of hardware resources|This paper proposes an allocation algorithm able to perform the combined resource selection and operation binding of multiple precision specifications. The common operative kernel of additive specification operations is extracted, and an allocation independent of the operations widths is performed. As a result, one operation may be executed over either one wider functional unit, or a set of linked narrower functional units. This allocation approach maximizes the bit level reuse of hardware resources, thus substantially reducing the area of the final implementations. The maximum number of bits computed per cycle becomes the sole determining factor affecting the cost of circuits, in contrast with circuits proposed by conventional algorithms which are influenced by the number and widths of the operations executed in every cycle.Additionally an analytical method is presented to estimate the amount of area potentially saved in comparison with conventional allocation algorithms. 
49|-||Recursive bi-partitioning of netlists for large number of partitions|In many application in VLSI CAD, a given netlist has to be partitioned into smaller sub-designs which can be handled much better. In this paper we present a new recursive bi-partitioning algorithm that is especially applicable, if a large number of final partitions, e.g., more than 1000, has to be computed. The algorithm consists of two steps. Based on recursive splits the problem is divided into several sub-problems, but with increasing recursion depth more run time is invested. By this an initial solution is determined very fast. The core of the method is a second step, where a very powerful greedy algorithm is applied to refine the partitions. Experimental results are given that compare the new approach to state-of-the-art tools. The experiments show that the new approach outperforms the standard techniques with respect to run time and quality. Furthermore, the memory usage is very low and is reduced in comparison to other methods by more than a factor of four. 
49|-||Theoretical comparison between sequential redundancy addition and removal and retiming optimization techniques|This paper attempts to determine the capabilities of existing redundancy addition and removal (SRAR) techniques for logic optimization of sequential circuits. To this purpose, we compare this method with the retiming and resynthesis (RaR) techniques. For the RaR case the set of possible transformations has been established by relating them to STG transformations by other authors. Following these works, we first formally demonstrate that logic transformations provided by RaR are covered by SRAR as well. Then we also show that SRAR is able to identify transformations that cannot be found by RaR. This way we prove that the sequential redundancy addition and removal technique provides more possibilities for logic optimization. 
49|-||Analysis of the impact of different methods for division/square root computation in the performance of a superscalar microprocessor|An analysis of the impact of different methods for double-precision computation of division and square root in the performance of a superscalar processor is presented in this paper. This analysis is carried out combining the SimpleScalar toolset, estimates of the latency and throughput for the compared methods and a set of benchmarks with typical features of intensive computing applications. Simulation results show the importance of having an efficient unit for the computation of these operations, since changes in the density of division and square root below 1% lead to changes in the performance around a 20%. 
49|-||Two-level branch prediction using neural networks|Dynamic branch prediction in high-performance processors is a specific instance of a general time series prediction problem that occurs in many areas of science. Most branch prediction research focuses on two-level adaptive branch prediction techniques, a very specific solution to the branch prediction problem. An alternative approach is to look to other application areas and fields for novel solutions to the problem. In this paper, we examine the application of neural networks to dynamic branch prediction. We retain the first level history register of conventional two-level predictors and replace the second level PHT with a neural network. Two neural networks are considered: a learning vector quantisation network and a backpropagation network. We demonstrate that a neural predictor can achieve misprediction rates comparable to conventional two-level adaptive predictors and suggest that neural predictors merit further investigation. 
49|-||Modeling and formal verification of embedded systems based on a Petri net representation|In this paper we concentrate on aspects related to modeling and formal verification of embedded systems. First, we define a formal model of computation for embedded systems based on Petri nets that can capture important features of such systems and allows their representation at different levels of granularity. Our modeling formalism has a well-defined semantics so that it supports a precise representation of the system, the use of formal methods to verify its correctness, and the automation of different tasks along the design process. Second, we propose an approach to the problem of formal verification of embedded systems represented in our modeling formalism. We make use of model checking to prove whether certain properties, expressed as temporal logic formulas, hold with respect to the system model. We introduce a systematic procedure to translate our model into timed automata so that it is possible to use available model checking tools. We propose two strategies for improving the verification efficiency, the first by applying correctness-preserving transformations and the second by exploring the degree of parallelism characteristic to the system. Some examples, including a realistic industrial case, demonstrate the efficiency of our approach on practical applications. 
49|-||Implementation of a streaming execution unit|The Complex Streamed Instruction (CSI) set is an instruction set extension targeted at multimedia applications. CSI instructions process two-dimensional data streams stored in memory and the streams can be of any length. Sectioning (the process of splitting up arbitrary-length streams into fixed-size sections that fit in a vector register), data alignment, and conversion between different packed data types are all performed in hardware. It has been shown previously that CSI provides significant speedups compared to current media ISA extensions such as MMX and VIS. This paper presents a detailed design of a unit that can execute CSI instructions under the assumption that it is interfaced with the first-level data cache. In particular, it is shown that the complex, two-dimensional, address-generation calculations can be performed in a pipelined fashion and implemented using a three-stage pipeline with acceptable delay and hardware cost. 
49|-||A scalable single-chip multi-processor architecture with on-chip RTOS kernel|Now that system-on-chip technology is emerging, single-chip multi-processors are becoming feasible. A key problem of designing such systems is the complexity of their on-chip interconnects and memory architecture. It is furthermore unclear at what level software should be integrated. An example of a single-chip multi-processor for real-time (networked) embedded systems is the multi-microprocessor (MÎ¼P). Its architecture consists of a scalable number of identical master processors and a configurable set of shared co-processors. Additionally, an on-chip real-time operating system kernel is included to support transparent multi-tasking over the set of master processors. In this paper, we explore the main design issues of the architecture platform on which the MÎ¼P is based. In addition, synthesis results are presented for a lightweight configuration of this architecture platform. 
49|-||A flexible architecture for H.263 video coding|In this paper a flexible and efficient architecture that implements the core of a video coder according to Rec. H.263 is presented. It consists of a RISC processor that controls the scheduling of a set of specialized processors that perform the discrete cosine transform (DCT), the inverse discrete cosine transform (IDCT), the direct and inverse quantization (DQ and IQ), the motion estimation (ME) and the motion compensation (MC). The architecture also includes pre-processing modules for the input video signal from the camera and interfaces for the external video memory and the H.263 stream generation.The processors have been written in synthesizeable Verilog and the firmware for the RISC (a commercial processor) has been developed in C language.The design has been tested with hardware–software co-simulations in a Verilog testbench using standard video sequences and has also been prototyped onto a development system based on an FPGA and a RISC. It performs 30 QCIF frames/s with a system clock of 12 MHz or 30 CIF frames/s with a system clock of 48 MHz, which is better than other reported designs with similar degree of flexibility. Also, the low frequency system clock makes it suitable for low-power applications such as mobile videotelephony. 
49|-||Author Index to Volume 49|
49|-||Subject Index to Volume 49|
49|1-2|http://www.sciencedirect.com/science/journal/13837621/49/1-2|Editorial board|
49|1-2||Fault-tolerance of Complete Josephus Cubes|The Complete Josephus Cube is proposed as a fault-tolerant node cluster architecture A reliable and cost-effective communications strategy is also presented. For a Complete Josephus Cube of dimension r, the strategy tolerates up to (r+1) encountered faults in its routes that are deadlock-free and livelock-free. The message is optimally (respectively, sub-optimally) delivered in at most r (respectively, 2r+1) hops. Message overhead is one of the lowest reported for the specified fault tolerance––with only a single (r+2)-bit routing vector accompanying the message to be communicated. Associated routing hardware may be implemented with standard logic. 
49|1-2||An improved reconfiguration algorithm for degradable VLSI/WSI arrays|This paper discusses the NP-complete problem of reconfiguring a two-dimensional degradable VLSI/WSI array under the row and column routing constraints. A new strategy for row selection in the logical array is proposed and Low’s algorithm is simplified. Experimental results show that our algorithm is approximately 50% faster than the most efficient algorithm, cited in the literature, without loss of performance. 
49|1-2||Multiple-path execution for chip multiprocessors|The increased dependence of clock cycle time on interconnect delay favors chip multiprocessors (CMP) for future microprocessor designs. This paper studies multiple-path execution (MPE) on a CMP to provide speedup on unmodified sequential code by exploring different paths of a conditional branch on separate processors. MPE performance due to processor complexity and count, cache and branch prediction architecture, processor-to-path allocation strategies, and limited interprocessor communication capabilities is explored. Simulation shows 12.7% speedup of SPECint95 with up to 33.5% on components with poor branch prediction accuracy using an 8-processor, 8-issue CMP with a simple mesh interconnect, realistic latencies, and limited bandwidth. 
49|1-2||Residue number system to binary converter for the moduli set (2nâ1,2nâ1,2n+1)|In many earlier publications, different authors have suggested residue to binary converters for the moduli sets: (2n,2n−1,2n+1) and (2n,2n−1,2n−1−1), where n is a positive integer. In this paper, we are introducing the moduli set (2n−1,2n−1,2n+1), which is one bit less in its dynamic range than that of (2n,2n−1,2n+1). However, it has a similar dynamic range to that of (2n,2n−1,2n−1−1). Closed form multiplicative inverses for the new set are introduced. Based on these inverses, a residue to binary converter design is proposed which requires less time and hardware than all published converters for the other two moduli sets. 
49|1-2||Erratum to âBandwidth constrained smoothing for multimedia streaming with scheduling supportâ [Journal of Systems Architecture 48 (2003) 353â366]|
49|10-11|http://www.sciencedirect.com/science/journal/13837621/49/10-11|Editorial board|
49|10-11||Evolutions in parallel distributed and network-based processing|
49|10-11||HPC the easy way: new technologies for high performance application development and deployment|With the increase of both computing power available and computer application size and complexity, existing programming methodologies and technologies for parallel and distributed computing demonstrated their inadequacy. New techniques have therefore been designed and are currently being developed that aim at providing the user/programmer with higher level programming methodologies, environments and run time supports.In this work, we take into account some of these new technologies and we discuss their features, both positive and negative. Eventually, exploiting our experience in structured parallel programming environment design, we try to summarize which features have to be included in the programming environments of the near future, those answering (or trying to answer) the pressures and urgencies of current days claiming for new, efficient, easy to use high performance programming environments. 
49|10-11||Automatic performance analysis of hybrid MPI/OpenMP applications|The EXPERT performance-analysis environment provides a complete tracing-based solution for automatic performance analysis of MPI, OpenMP, or hybrid applications running on parallel computers with SMP nodes. EXPERT describes performance problems using a high level of abstraction in terms of execution patterns that result from an inefficient use of the underlying programming model(s). The set of predefined problems can be extended to meet application-specific needs. The analysis is carried out along three interconnected dimensions: class of performance behavior, call tree, and thread of execution. Each dimension is arranged in a hierarchy so that the user can investigate the behavior on varying levels of detail. All three dimensions are interactively accessible using a single integrated view. 
49|10-11||Supporting adaptive routing in IBA switches|InfiniBand is a new standard for communication between processing nodes and I/O devices as well as for interprocessor communication. The InfiniBand Architecture (IBA) supports distributed deterministic routing because forwarding tables store a single output port per destination ID. This prevents packets from using alternative paths when the requested output port is busy. Despite the fact that alternative paths could be selected at the source node to reach the same destination node, this is not effective enough to improve network performance. However, using adaptive routing could help to circumvent the congested areas in the network, leading to an increment in performance.In this paper, we propose a simple strategy to implement forwarding tables for IBA switches that supports adaptive routing while still maintaining compatibility with the IBA specs. Adaptive routing can be individually enabled or disabled for each packet at the source node. The proposed strategy enables the use in IBA of any adaptive routing algorithm with an acyclic channel dependence graph. In this paper, we have taken advantage of the partial adaptivity provided by the well-known up*/down* routing algorithm. Evaluation results show that extending IBA switch capabilities with adaptive routing may noticeably increase network performance. In particular, network throughput improvement can be, on average, as high as 66%. 
49|10-11||Architectural concerns in distributed and mobile collaborative systems|Organizations increasingly coordinate their product and service development processes to deliver their products and services as fast as possible, and to involve employees, customers, suppliers, and business partners seamlessly in different stages of the processes. These processes have to consider that their participants are increasingly on the move or distributed while they are working. Expertise needs to be shared across locations and different mobile devices. This paper describes a framework for distributed and mobile collaboration, defines a set of requirements for virtual communities, and discusses a mobile teamwork support software architecture that has been developed in the EU-project MOTION. The framework together with the architecture enables to enhance current collaboration approaches to include the dimension of mobile participants and virtual communities for distributed product development. This is achieved by integrating process and workspace management requirements with Peer-to-Peer Middleware, Publish-Subscribe, and Community and User Management components. 
49|10-11||Video transmission adaptation on mobile devices|The development of multimedia streaming over wireless network is facing a lot of challenges. Taking into account mobility and highly variable bandwidth are the two major ones. Using scalable video content can solve the variable bandwidth problem only if the streaming architecture is able to react without latency. In this article, we present NetMoVie, an intermediate architecture based on real-time protocol which is able to adapt streams to the constraints of the wireless channel. 
49|3|http://www.sciencedirect.com/science/journal/13837621/49/3|Editorial board|
49|3||Publisherâs note|
49|3||Parallel, distributed and network-based processing|
49|3||Optimization techniques for parallel irregular reductions|Different parallelization techniques have been proposed in the literature for irregular reductions in the context of shared memory multiprocessors. They may be classified into two broad families: those based on privatization of the reduction arrays and those based on the partitioning of the reduction arrays. Methods in the first family are simple but no data locality is exploited and their memory scalability is low. On the other hand, methods in the second family are more complex as they require an inspection phase but they exploit data locality and scale up better in memory. Focusing on partitioning-based methods, although they exhibit a good performance in a wide variety of irregular codes, some specific input data patterns may exist for which the performance is lowered. In particular these kind of access patterns may reduce the exploited parallelism by the method or introduce workload unbalances. In order to mitigate these negative effects, we propose three optimizations for a specific partitioning-based method (DWA–LIP). These optimizations try to increase the exploited parallelism, balance the workload and reduce the effect of high contention degree regions in the reduction arrays. Efficient implementations of the proposed optimizations for the DWA–LIP method have been tested experimentally, and compared with other methods for parallelizing irregular reductions. 
49|3||Motion-compensated wavelet packet zerotree video coding on multicomputers|In this work we describe and analyze algorithms for advanced video coding on distributed memory MIMD architectures. In particular, we consider a wavelet packet based codec using the concept of zerotree encoding. The main contribution of this work is the design of a parallel motion-compensated video coder composed of a wavelet packet decomposition in conjunction with the best basis algorithm followed by zerotree coding. Whereas two sensible parallelization techniques can be employed for the wavelet packet decomposition (subband based partitioning and stripe partitioning), the zerotree coding and motion compensation stages only allow one reasonable parallelization method (stripe partitioning). We investigate the advantages and drawbacks of the resulting different overall data distribution strategies and show experimental results obtained on a Siemens hpcLine cluster and a Cray T3E. 
49|3||Efficient implementation of reduce-scatter in MPI|We discuss the efficient implementation of a collective operation called reduce-scatter, which is defined in the MPI standard. The reduce-scatter is equivalent to the combination of a reduction on vectors of length n with a scatter of the resulting n-vector to all processors.We describe the implementation issues and the performance characterization of two recently proposed algorithms for the reduce-scatter that have been proven to be highly efficient in theory under the assumption of fully connected parallel system.A performance comparison with existing mainstream implementations of the operation is presented which confirms the practical advantage of the new algorithms. Experiments show that the two algorithms have different characteristics which make them complementary in providing a performance gain over standard algorithms.Our study has been carried out on two different platforms: an SP2 and a Myrinet interconnected cluster of Pentium PRO. However, most of the results reported here are not specific for either MPI or the platforms used, and they hold in general for any message passing programming system. 
49|3||Incorporating memory layout in the modeling of message passing programs|One of the most fundamental tasks any automatic parallelization and optimization tool is confronted with is to find an optimal domain decomposition for an application at hand. For regular domain problems (such as simple matrix manipulations) this task may seem trivial. However, communication costs in message passing programs often significantly depend on the capabilities and particular behavior of the applied communication primitives. As a consequence, straightforward domain decompositions may deliver non-optimal performance.In this paper we introduce a new point-to-point communication model (called P-3PC, or the ‘Parameterized model based on the Three Paths of Communication’) that is specifically designed to overcome this problem. In comparison with related models (e.g., LogGP) P-3PC is similar in complexity, but more accurate in many situations. Although the model is aimed at MPI’s standard point-to-point operations, it is applicable to similar message passing definitions as well.The effectiveness of the model is tested in a framework for automatic parallelization of image processing applications. Experiments are performed on two Beowulf-type commodity clusters, each having a different interconnection network, and a different MPI implementation. Results show that, where other models frequently fail, P-3PC correctly predicts the communication costs related to any type of domain decomposition. 
49|4-6|http://www.sciencedirect.com/science/journal/13837621/49/4-6|Editorial board|
49|4-6||Special-issue on reconfigurable systems|
49|4-6||Configware and morphware going mainstream|The paper addresses a broad readership in information technology, computer science and related areas, and gives an introduction to fine grain and coarse grain morphware, reconfigurable computing, and its impact on classical computer science and business models. It points out trends driven by microelectronics technology, EDA, and the mind set of data-stream-based computing. 
49|4-6||Polymorphous fabric-based systems: Model, tools, applications|A polymorphous fabric-based systems is a parameterized cellular architecture in which an array of computing cells communicates with an embedded processor through a global memory. This architecture is customizable to different classes of applications by functional unit, interconnect, and memory parameters, and can be instantiated efficiently on platform FPGAs. In previous work [IEEE Micro 22(5) (2002)], we have demonstrated the advantage of reconfigurable fabrics for image and signal processing applications. Recently, we have build a fabric generator (FG), a Java-based toolset that greatly accelerates construction of the fabrics. A module-generation library is used to define, instantiate, and interconnect cells’ datapaths. FG also generates customized sequencers for individual cells or collections of cells. We describe the fabric-based system model, the FG toolset, and concrete realizations of fabric architectures generated by FG on the Altera Excalibur ARM that can deliver 4.5 GigaMACs/s (8/16 bit data, multiply-accumulate). 
49|4-6||Realization of wireless multimedia communication systems on reconfigurable platforms|Wireless multimedia communication systems become increasingly more computational intensive and demand for higher flexibility. The realization of these systems on reconfigurable hardware offers a good balance for these requirements. In this paper the suitability of commercially available reconfigurable hardware platforms for the target application domain is evaluated. Based on this evaluation a heterogeneous partly reconfigurable system-on-chip platform is identified as ideal implementation platform for the targeted systems. Systems from different target domains are analysed and different cases where the inclusion of reconfigurable hardware in their realizations would lead to improved quality in terms of implementation efficiency and flexibility are identified. Design methodology requirements for the realization of systems from the target application domain on the targeted platform are analysed and issues not covered by existing methodologies are identified. The principles of a methodology handling these open issues are described. Results from the prototyping of different systems are also presented and show the potentials of a reconfigurable hardware platform, which in the future will lead to reduced costs and increased flexibility of the wireless multimedia communication systems. 
49|4-6||Functionally partitioned module-based programmable architecture for wireless base-band processing|A specialised reconfigurable architecture is targeted at wireless base-band processing. It is built to cater for multiple wireless standards. It has lower power consumption than the processor-based solution. It can be scaled to run in parallel for processing multiple channels. Test resources are embedded on the architecture and testing strategies are included.This architecture is functionally partitioned according to the common operations found in wireless standards, such as CRC error correction, convolution and interleaving. These modules are linked via Virtual Wire Hardware modules and route-through switch matrices. Data can be processed in any order through this interconnect structure. Virtual Wire ensures the same flexibility as normal interconnects, but the area occupied and the number of switches needed is reduced.The testing algorithm scans all possible paths within the interconnection network exhaustively and searches for faults in the processing modules. The testing algorithm starts by scanning the externally addressable memory space and testing the master controller. The controller then tests every switch in the route-through switch matrix by making loops from the shared memory to each of the switches. The local switch matrix is also tested in the same way. Next the local memory is scanned. Finally, pre-defined test vectors are loaded into local memory to check the processing modules.This paper compares various base-band processing solutions. It describes the proposed platform and its implementation. It outlines the test resources and algorithm. It concludes with the mapping of Bluetooth and GSM base-band onto the platform. 
49|4-6||Performance of reconfigurable architectures for image-processing applications|Reconfigurable architectures combine a programmable–visible interface and the high-level aspects of a computer’s design. The goal of this work is to explore the architectural behaviour of remote reconfigurable systems that are part of general-purpose computers. Our approach analyses various issues arising from the connection of processors with FPGA-based microarchitecture to an existing commodity microprocessor via a standard bus. The quantitative evaluation considers image-processing applications and shows that the maximum performance depends on the amount of data processed by the reconfigurable hardware. Taking images with 256 × 256 pixels, a moderate FPGA capacity of 1E+5 logic blocks provides two orders of magnitude of performance improvement over a Pentium III processor for most of our benchmarks. However, the performance benefits exhibited by reconfigurable architectures may be deeply influenced by some design parameters. This paper studies the impact of hardware capacity, reconfiguration time, memory organisation, and bus bandwidth on the performance achieved by FPGA-based systems. Those image-processing benchmarks that can exhibit high-performance improvement would require about 150 memory banks of 256 bytes each and a bus bandwidth as high as 30 GB/s. This quantitative approach can be applied to the design of high-performance reconfigurable coprocessors for multimedia applications. 
49|4-6||The design and implementation of a reconfigurable processor for problems of combinatorial computation|The paper analyses different techniques that might be employed in order to solve various problems of combinatorial optimization and argues that the best results can be achieved by the use of software running on a general-purpose computer together with an FPGA-based reconfigurable co-processor. It suggests an architecture for a combinatorial co-processor that is based on hardware templates and consists of reconfigurable functional and control units. Finally the paper demonstrates how the co-processor can be applied to two practical applications formulated over discrete matrices, the Boolean satisfiability and covering problems. 
49|4-6||Fast and compact sequential circuits for the FPGA-based reconfigurable systems|Reconfigurable systems fill the flexibility, performance, power dissipation, and development and fabrication cost gap between the application specific systems implemented with hard-wired application specific integrated circuits and systems based on the standard (general purpose) programmable microprocessors. During the last decade they became the mainstream implementation technology for custom computation and embedded system products in such fields as telecommunication, image processing, video processing, multimedia, DSP, cryptography, embedded control, etc. To efficiently develop, implement and use the reconfigurable systems, adequate computer-aided support tools are necessary. Since most reconfigurable systems are implemented using the look-up table (LUT) field programmable gate arrays (FPGA) technology, the circuit synthesis tools targeting this technology are of primary importance for their effective and efficient implementation. In this paper, a new sequential circuit synthesis methodology is discussed that targets LUT FPGAs and FPGA-based reconfigurable system-on-a-chip platforms. The methodology is based on the information-driven approach to circuit synthesis, general decomposition and theory of information relationship measures that we previously developed. Our synthesis methods considerably differ from all other known methods. The experimental results from the automatic circuit synthesis tools that implement our methods demonstrate that the information-driven approach consistently applied in the whole sequential circuit synthesis chain efficiently produces very fast and compact sequential circuits. 
49|4-6||Effective and efficient FPGA synthesis through general functional decomposition|In this paper, a new information-driven circuit synthesis method is discussed that targets LUT-based FPGAs and FPGA-based reconfigurable system-on-a-chip platforms. The method is based on the bottom–up general functional decomposition and theory of information relationship measures that we previously developed. It differs considerably from all other known methods. The experimental results from the automatic circuit synthesis tool that implements the method clearly demonstrate that the information-driven general functional decomposition based on information relationship measures efficiently produces very fast and compact FPGA circuits. 
49|4-6||Run-time support for dynamically reconfigurable computing systems|Reconfigurable computing systems normally consist of an instruction-set processor connected to a block of reconfigurable logic. The reconfigurable logic, for example, an field programmable gate arrays (FPGA), can usually be adapted during the run-time of an application to perform different tasks. This paper describes a novel FPGA support system (FSS) that facilitates the execution of hardware-based tasks on a reconfigurable Xilinx 6264 FPGA connected to an ARM 7 processor. The FSS provides the mechanisms to support the placement, execution, and removal of tasks on the FPGA. A key feature of the FSS is the ability to provide communication facilities between concurrently active hardware and software tasks during the run-time of an application. The design, implementation and status of the FSS are discussed, together with initial results based on the implementation of a wavelet image compression application. The paper concludes by considering how our experiences with this system have influenced the development of an enhanced FSS for the later generation of Xilinx Virtex FPGAs. 
49|4-6||Evaluation of delay fault testability of LUTs for the enhancement of application-dependent testing of FPGAs|Testing delay faults in FPGAs differs significantly from testing delay faults in circuits whose combinational sections are represented as gate networks. Based on delay fault testability conditions, formulated in a form suitable for analysis of LUT-based FPGAs, a new method for the evaluation of delay fault testability of LUT functions has been developed. It relies on an indicator called delay fault activation profile. The proposed method supports an analysis and comparison of different procedures for the enhancement of detectability of FPGA delay faults that rely on transformations of user-defined functions of LUTs in the section under test. The effectiveness of the method is demonstrated by applying it to prove the efficiency and to optimize a specific procedure for the transformation of user-defined LUT functions. 
49|7-9|http://www.sciencedirect.com/science/journal/13837621/49/7-9|Editorial board|
49|7-9||An optical switching architecture for hierarchical group communication|We show that the problem of finding the maximum passable subset of a permutation through a banyan is NP-complete. We then describe a two-level optical interconnection structure for groups or clusters of end-nodes (LANs or processors). At the higher level, an optical banyan is used to switch wavelength multiplexed packets from the groups. Technologically difficult switching of individual wavelengths is avoided by prearranging transmissions from the groups in a way that they can be switched in a wavelength insensitive manner. Further, by keeping the banyan conflict-free, we allow each of the groups to access the entire set of wavelengths for multiplexing, thus maximizing the bisection bandwidth. The proposed optical interconnect can support multiple multicast connections between each pair of groups, in which source nodes may simultaneously multicast several different packets to different subsets within a destination group. System-wide multicasts and broadcasts can be achieved through repetitive group-to-group transmissions. The network uses readily available components such as opto-electronic directional couplers, fixed wavelength transmitters, and diffraction-based parallel receivers while avoiding the use of relatively slow and expensive tunable components. 
49|7-9||Use of embedded DRAMs in video and image computing|We have evaluated the role of embedded dynamic random access memory (eDRAM) in the performance of programmable mediaprocessors, focusing on video/image computing. eDRAM’s contributions to improving the total system performance can be assessed by measuring the number of CPU stall cycles caused by the memory transactions. We decomposed the CPU stall cycles into three components: latency due to row access, latency due to the pipeline of memory transactions, and burst transfer time. We used a cycle-accurate cache and eDRAM model to measure the system performance in executing selected low-level video/image computing functions on a mediaprocessor core. We simulated various values for data bus width, page size, and row-access time of eDRAM, pipeline delay of a memory transaction, and data cache line size. While the wider data width of eDRAM does reduce the burst transfer time, the actual reduction in the total stall cycles when the width was expanded from 8 to 16 bytes was lower than expected, ranging from 6.2% to 18.9%. Instead, we found that the row-access latency and memory transaction pipeline delay represent the major portion of the CPU stall cycles. For example, in case of 32-byte wide data bus, they account for 85.3–95.1% of the memory busy time during which data cache misses are serviced. We show how to lower the CPU stall time further, e.g., using no-write-allocate data cache to reduce the total burst transfer time, efficient memory banking to reduce the number of eDRAM page misses, and various software/hardware methods to bring data to the cache before they are needed by the CPU. In particular, the regular memory access pattern in video/image computing allows several methods to enhance the memory performance in using eDRAM, e.g., enlarging the cache line size and data prefetching. This paper presents our methodology, experimental results, and findings, which would be useful to the design of highly integrated systems on a chip with eDRAM in the future. 
49|7-9||A VLSI architecture for video object motion estimation using a novel 2-D hierarchical mesh|This paper proposes a novel hierarchical mesh-based video object model and a motion estimation architecture that generates a content-based video object representation. The 2-D mesh-based video object is represented using two layers: an alpha plane and a texture. The alpha plane consists of two layers: (1) a mesh layer and (2) a binary layer that defines the object boundary. The texture defines the object’s colors. A new hierarchical adaptive structured mesh represents the mesh layer. The proposed mesh is a coarse-to-fine hierarchical 2-D mesh that is formed by recursive triangulation of the initial coarse mesh geometry. The proposed technique reduces the mesh code size and captures the mesh dynamics.The proposed motion estimation architecture generates a progressive mesh code and the motion vectors of the mesh nodes. The performance analysis for the proposed video object representation and the proposed motion estimation architecture shows that they are suitable for very low bit rate online mobile applications and the motion estimation architecture can be used as a building block for MPEG-4 codec. 
49|7-9||On the weakest failure detector for hard agreement problems|Chandra and Toueg [J. ACM 43 (1996) 225] and Fromentin et al. [Proc. IEEE Internat. Conf. on Distrib. Comput., 1999, p. 470], respectively, stated that the weakest failure detector for any of non-blocking atomic commitment and terminating reliable broadcast is the perfect failure detector P. Recently, Guerraoui [IPL 79 (2001) 99] presented a counterexample of those results, exhibiting a failure detector called Marabout (M) that is incomparable to P and yet solves those problems.In this paper we present three new perfect failure detector classes as alternatives to P and M. All our classes are weaker than P. Furthermore, two of them are also weaker than M, and yet solve non-blocking atomic commitment and terminating reliable broadcast. Interestingly, our failure detector classes are implementable whenever P is implementable (e.g., in a synchronous system), which is not the case with M. 
49|7-9||A new cryptography system and its VLSI realization|In this paper, a new cryptography system (CS) is proposed and its VLSI architecture is designed and verified. The new system performs both random position permutation and random value transformation. The input data is processed by the swap and XOR/XNOR functions under the control of a binary sequence from a chaotic system. The scheme is analyzed and simulated by MATLAB to be high security. In order to maintain the requirement of real-time, the VLSI architecture with low hardware cost, high computing speeds, high modularity, and regularity is designed and implemented respectively with Altera FPGA and Avanti cell-library. According to the simulation result, the throughput rates of the proposed design with the two implementations are larger than 0.64 and 2.74 Gbps. Hence the proposed new cryptography system is strongly suitable for most of real-time video and audio applications. 
49|7-9||A multi-queue TCP window control scheme with dynamic buffer allocation|Explicit transmission control protocol (TCP) window control through the modification of the receiver’s advertised window in ACK packets is one of the ways intermediate network elements can contribute to the end-to-end TCP control. The TCP receiver’s advertised window, which indicates the level of the receive buffer of a TCP connection, limits the maximum window and consequently the throughput that can be achieved by the TCP sender. Thus, appropriate reduction of the advertised window by intermediate network elements can control the number of packets sent from a TCP sender. This paper describes a TCP window control scheme for a shared memory system with multiple queues. A dynamic buffer threshold, computed using a simple recursive algorithm, is used to dynamically allocate buffer space to the queues. 
49|7-9||Fast reconfigurable systolic hardware for modular multiplication and exponentiation|Modular multiplication and modular exponentiation are fundamental operations in most public-key cryptosystems such as RSA. In this paper, we propose a novel implementation of these operations using systolic arrays based architectures. For this purpose, we use the Montgomery algorithm to compute the modular product and the left-to-right binary exponentiation method to yield the modular power. In the proposed design, we invest hardware area in the hope of improving encryption/decryption throughput. Our implementation improves time requirement as well as the time area factor when compared that of Blum’s and Paar’s. 
volume|issue|url|title|abstract
50|-|http://www.sciencedirect.com/science/journal/13837621/50|A tamper resistant hardware accelerator for RSA cryptographic applications|This paper presents an hardware accelerator which can effectively improve the security and the performance of virtually any RSA cryptographic application. The accelerator integrates two crucial security- and performance-enhancing facilities: an RSA processor and an RSA key-store. An RSA processor is a dedicated hardware block which executes the RSA algorithm. An RSA key-store is a dedicated device for securely storing RSA key-pairs. We chose RSA since it is by far the most widely adopted standard in public key cryptography. We describe the main functional blocks of the hardware accelerator and their interactions, and comment architectural solutions we adopted for maximizing security and performance while minimizing the cost in terms of hardware resources. We then present an FPGA-based implementation of the proposed architecture, which relies on a Commercial Off The Shelf (COTS) programmable hardware board. Finally, we evaluate the system in terms of performance and chip area occupation, and comment the design trade-offs resulting from different levels of parallelism. 
50|-||Exploitation of parallelism to nested loops with dependence cycles|In this paper, we analyze the recurrences from the breakability of the dependence links formed in general multi-statements in a nested loop. The major findings include: (1) A sin k variable renaming technique, which can reposition an undesired anti-dependence and/or output-dependence link, is capable of breaking an anti-dependence and/or output-dependence link. (2) For recurrences connected by only true dependences, a dynamic dependence concept and the derived technique are powerful in terms of parallelism exploitation. (3) By the employment of global dependence testing, link-breaking strategy, Tarjan’s depth-first search algorithm, and a topological sorting, an algorithm for resolving a general multi-statement recurrence in a nested loop is proposed. Experiments with benchmark cited from Vector loops showed that among 134 subroutines tested, 3 had their parallelism exploitation amended by our proposed method. That is, our offered algorithm increased the rate of parallelism exploitation of Vector loops by approximately 2.24%. 
50|-||Design of an efficient VLSI architecture for non-linear spatial warping of wide-angle camera images|Endoscopic images are subjected to spatial distortion due to the wide-angle configuration of the camera lenses. This barrel type of non-linear distortion should be corrected before these images are subjected to further analysis for diagnostic purposes. An efficient digital architecture suitable for an embedded system which can correct the barrel distortion in real-time is presented in this paper. The theoretical approach of this spatial warping technique is based on least-squares estimation. The images in the distorted image space are mapped onto the corrected image space by using a polynomial mapping model. The polynomial parameters include the expansion coefficients, back-mapping coefficients, distortion centre and corrected centre. Several experiments were conducted by applying the spatial warping algorithm on many endoscopic images. A digital architecture suitable for hardware implementation of the distortion correction technique is developed by mapping the algorithmic steps onto a linear array of processing modules. Each module of a particular unit communicates with its nearest neighbours. The spatial warping architecture implemented and simulated with Altera’s Quartus II software shows an overall computation time of 1.8 ms with 50 MHz clock for an image of size 256 × 192 pixels, which confirms that the spatial warping module could be mounted as a dedicated unit in an endoscopy system for real-time applications. 
50|-||Author Index to Volume 50 (2004)|
50|-||Subject Index to Volume 50 (2004)|
50|-||Guide for Authors|
50|1|http://www.sciencedirect.com/science/journal/13837621/50/1|Editorial board|
50|1||SAGE: an automatic analyzing system for a new high-performance SoC architectureââprocessor-in-memory|Continuous improvements in semiconductor fabrication density are supporting new classes of System-on-a-Chip (SoC) architectures that combine extensive processing logic/processor with high-density memory. Such architectures are generally called Processor-in-Memory (PIM) or Intelligent Memory (I-RAM) and can support high-performance computing by reducing the performance gap between the processor and the memory. The PIM architecture combines various processors in a single system. These processors are characterized by their computation and memory-access capabilities. Therefore, a novel strategy must be developed to identify their capabilities and dispatch the most appropriate jobs to them in order to exploit them fully. Accordingly, this study presents an automatic source-to-source parallelizing system, called statement-analysis-grouping-evaluation (SAGE), to exploit the advantages of PIM architectures. Unlike conventional iteration-based parallelizing systems, SAGE adopts statement-based analyzing approaches. This study addresses the configuration of a PIM architecture with one host processor (i.e., the main processor in state-of-the-art computer systems) and one memory processor (i.e., the computing logic integrated with the memory). The strategy of the SAGE system, in which the original program is decomposed into blocks and a feasible execution schedule is produced for the host and memory processors, is investigated as well. The experimental results for real benchmarks are also discussed. 
50|1||Reducing disk I/O times using anticipatory movements of the disk head|Finding a good rest position for the disk head is very important for the performance of a hard disk. It has been shown in the past that rest positions obtained through anticipatory movements of the disk head can indeed improve response time, but practical algorithms have not been described yet. In this paper we describe a software technique for perfoming anticipatory movements of the disk head. In particular, we show that by partitioning the disk controller memory into a part used for caching and a part used for predictive movements, lower I/O times as compared with the usual read-ahead cache configurations are obtained. Through trace-driven simulations we show in fact that significant improvements in the disk I/O times can be obtained as compared to standard disk caching. Since the technique should be realized at the firmware level in the disk controller and no hardware modifications are needed, the implementation cost is low. 
50|1||Switch fabric architecture analysis for a scalable bi-directionally reconfigurable IP router|This paper provides an in-depth analysis using six basic router functional requirements, a primary switch fabrics (SFs) selection criterion, and a semi-quantitative compliance scoring scheme for 10 SFs. The goal is to select candidates that can serve a hardware (HW)-wise scalable and bi-directionally reconfigurable Internet Protocol (IP) router. HW scalability and bi-directional HW reconfigurability for an IP router denote respectively its ability to (1) expand according to network traffic capacity growth; and (2) be functionally converted to perform in two conceptual directions on-demand: “downward” as “edge”, or “upward” as “hub” or “backbone” router according to the layer of the internet services provider’s network hierarchy it is targeted to serve at the moment. Overall result points to Hypercube, Multistage Interconnection Network (MIN), and 3-Dimensional Torus Mesh as potential candidates. 
50|10|http://www.sciencedirect.com/science/journal/13837621/50/10|Editorial board|
50|10||Replication algorithms for the World-Wide Web|This paper addresses the two fundamental issues in replication, namely deciding on the number and placement of the replicas and the distribution of requests among replicas. We first introduce a centralized algorithm for replicating objects that can keep a balanced load on sites. In order to meet the requirement due to the dynamic nature of the Internet traffic and the rapid change in the access pattern of the World-Wide Web (Web), we also propose a distributed algorithm where each site relies on some collected information to decide on where to replicate and migrate objects to achieve good performance. The performance of the proposed algorithms is evaluated experimentally and a comparison of their measured performance is presented. 
50|10||Efficiently tolerating failures in asynchronous real-time distributed systems|We present a proactive resource allocation algorithm, called BEA, for fault-tolerant asynchronous real-time distributed systems. BEA considers an application model where trans-node application timeliness requirements are expressed using benefit functions, and anticipated workload during future time intervals are expressed using adaptation functions. Furthermore, BEA considers an adaptation model where subtasks of application tasks are replicated at run-time for tolerating failures as well as for sharing workload increases. Given such models, the objective of the algorithm is to maximize the aggregate real-time benefit and the ability to tolerate host failures during the time window of adaptation functions. Since determining the optimal solution is computationally intractable, BEA heuristically computes suboptimal resource allocations in polynomial-time. We show that BEA can achieve almost the same fault-tolerance ability as full replication, and accrue most of real-time benefit that full replication can accrue. In the meanwhile, BEA requires much fewer replicas than full replication, and hence is cost effective. 
50|10||Stabilizing ring clustering|In this paper, we first present simple stabilizing algorithms for finding clustering of ring networks on a distributed model of computation. Clustering is defined as partitioning of nodes of a network into non-overlapping sets of nodes based on certain criteria. Our criterion for partitioning the network is that the difference between the sizes of the largest cluster and the smallest cluster is minimal. We first present a uniform algorithm that evenly partitions the network into nearly the same size clusters. The clusters may continuously move in one direction while maintaining the difference of at most one between the size of the largest and the size of the smallest cluster. Then, we present a non-uniform self-stabilizing algorithm for the same problem that terminates after O(n2) moves. When resources are placed at cluster boundaries (or centers), the cost of sharing resources is minimized. The algorithms can withstand transient faults and do not require initialization. In addition, when the ring size changes, the proposed algorithms automatically identify the clusterings of the new ring. The paper includes correctness proofs of the algorithms. It concludes with remarks on issues such as open and related problems, and the application areas of the algorithm. 
50|10||A parallel algorithm for constructing reduced visibility graph and its FPGA implementation|A central geometric structure in applications such as robotic path planning and hidden line elimination in computer graphics is the visibility graph. A new parallel algorithm to construct the reduced visibility graph in a convex polygonal environment is presented in this paper. The computational complexity is O(p2log(n/p)) where p is the number of objects and n is the total number of vertices. A key feature of the algorithm is that it supports easy mapping to hardware. The algorithm has been simulated (and verified) using C. Results of hardware implementation show that the design operates at high speed requiring only small space. In particular, the hardware implementation operates at approximately 53 MHz and accommodates the reduced visibility graph of an environment with 80 vertices in one XCV3200E device. 
50|10||Guide for Authors|
50|11|http://www.sciencedirect.com/science/journal/13837621/50/11|Editorial board|
50|11||Software environment for integrating critical real-time control systems|In the recent few years, integration of multiple real-time control modules has gained increased acceptance in the industry. Such integration can achieve lower overall hardware costs and reduced level of spares by sharing hardware resources among multiple applications. Single contemporary CPU can now harbor several applications which have been traditionally running on several older and slower computing platforms. However, the integrated approach faces new challenges such as the reusability of existing software and the prevention of fault propagation. The reuse of legacy application code, with minimal modifications, is strongly desirable since the cost of application re-development can be prohibitive. Resource sharing introduces dependencies among applications and thus requires additional design precautions to ensure that the effect of a failure in one application will not spread and impact other applications. This paper describes a two-layer software architecture, which enables the integration of multiple real-time applications while maintaining strong spatial and temporal partitioning among application modules. At the lower layer, a system executive creates multiple virtual machines. Each module accommodates an application with its choice of a real-time operating system. This architecture allows the reusability of existent software modules by enabling the integration of applications written for different real-time operating systems. The paper also addresses some issues related to the inter-application communication and to the handling of I/O devices. 
50|11||Using a serial cache for energy efficient instruction fetching|The design of a high performance fetch architecture can be challenging due to poor interconnect scaling and energy concerns. Way prediction has been presented as one means of scaling the fetch engine to shorter cycle times, while providing energy efficient instruction cache accesses. However, way prediction requires additional complexity to handle mispredictions.In this paper, we examine a high-bandwidth fetch architecture augmented with an instruction cache way predictor. We compare the performance and energy efficiency of this architecture to both a serial access cache and a parallel access cache. Our results show that a serial fetch architecture achieves approximately the same energy reduction and performance as way prediction architectures, without the added structures and recovery complexity needed for way prediction. 
50|11||A clocking technique for FPGA pipelined designs|This paper presents a clocking pipeline technique referred to as a single-pulse pipeline (PP-Pipeline) and applies it to the problem of mapping pipelined circuits to a Field Programmable Gate Array (FPGA). A PP-pipeline replicates the operation of asynchronous micropipelined control mechanisms using synchronous-orientated logic resources commonly found in FPGA devices. Consequently, circuits with an asynchronous-like pipeline operation can be efficiently synthesized using a synchronous design methodology. The technique can be extended to include data-completion circuitry to take advantage of variable data-completion processing time in synchronous pipelined designs. It is also shown that the PP-pipeline reduces the clock tree power consumption of pipelined circuits. These potential applications are demonstrated by post-synthesis simulation of FPGA circuits. 
50|11||Optical transpose k-ary n-cube networks|This paper derives a number of results related to the topological properties of OTIS k-ary n-cube interconnection networks. The basic topological metrics of size, degree, shortest distance, and diameter are obtained. Then results related to embedding in OTIS k-ary n-cubes of OTIS k-ary (n−1)-cubes, cycles, meshes, cubes, and spanning trees are derived. The OTIS k-ary n-cube is shown to be Hamiltonian. Minimal one-to-one routing and optimal broadcasting algorithms are proposed. The OTIS k-ary n-cube is shown to be maximally fault-tolerant. These results are derived based on known properties of k-ary n-cube networks and general properties of OTIS networks. 
50|11||Guide for Authors|
50|2-3|http://www.sciencedirect.com/science/journal/13837621/50/2-3|Editorial board|
50|2-3||Special issue on networks on chip|
50|2-3||Interconnect intellectual property for Network-on-Chip (NoC)|As technology scales down, the interconnect for on-chip global communication becomes the delay bottleneck. In order to provide well-controlled global wire delay and efficient global communication, a Network-on-Chip (NoC) architecture was proposed by different authors [Route packets, not wires: on-chip interconnection networks, in: Design Automation Conference, 2001, Proceedings, p. 684; Network on chip: an architecture for billion transistor era, in: Proceeding of the IEEE NorChip Conference, November 2000; Network on chip, in: Proceedings of the Conference Radio vetenskap och Kommunication, Stockholm, June 2002]. NoC uses Interconnect Intellectual Property (IIP) to connect different resources. Within an IIP, the switch has the central function. Depending on the network core of the NoC, the switch will have different architectures and implementations. This paper first briefly introduces the concept of NoC. It then studies NoC from an interconnect point of view and makes projections on future NoC parameters. At last, the IIP and its components are described, the switch is studied in more detail and a time–space–time (TST) switch designed for a circuit switched time-division multiplexing (TDM) NoC is proposed. This switch supports multicast traffic and is implemented with random access memory at the input and output. The input and output are then connected by a fully connected interconnect network. 
50|2-3||Packetization and routing analysis of on-chip multiprocessor networks|Some current and most future systems-on-chips use and will use network architectures/protocols to implement on-chip communication. On-chip networks borrow features and design methods from those used in parallel computing clusters and computer system area networks. They differ from traditional networks because of larger on-chip wiring resources and flexibility, as well as constraints on area and energy consumption (in addition to performance requirements). In this paper, we analyze different routing schemes for packetized on-chip communication on a mesh network architecture, with particular emphasis on specific benefits and limitations of silicon VLSI implementations. A contention-look-ahead on-chip routing scheme is proposed. It reduces the network delay with significantly smaller buffer requirement. We further show that in the on-chip multiprocessor systems, both the instruction execution inside node processors, as well as data transaction between different processing elements, are greatly affected by the packetized dataflows that are transported on the on-chip networks. Different packetization schemes affect the performance and power consumption of multiprocessor systems. Our analysis is also quantified by the network/multiprocessor co-simulation benchmark results. 
50|2-3||QNoC: QoS architecture and design process for network on chip|We define Quality of Service (QoS) and cost model for communications in Systems on Chip (SoC), and derive related Network on Chip (NoC) architecture and design process. SoC inter-module communication traffic is classified into four classes of service: signaling (for inter-module control signals); real-time (representing delay-constrained bit streams); RD/WR (modeling short data access) and block-transfer (handling large data bursts). Communication traffic of the target SoC is analyzed (by means of analytic calculations and simulations), and QoS requirements (delay and throughput) for each service class are derived. A customized Quality-of-Service NoC (QNoC) architecture is derived by modifying a generic network architecture. The customization process minimizes the network cost (in area and power) while maintaining the required QoS.The generic network is based on a two-dimensional planar mesh and fixed shortest path (X–Y based) multi-class wormhole routing. Once communication requirements of the target SoC are identified, the network is customized as follows: The SoC modules are placed so as to minimize spatial traffic density, unnecessary mesh links and switching nodes are removed, and bandwidth is allocated to the remaining links and switches according to their relative load so that link utilization is balanced. The result is a low cost customized QNoC for the target SoC which guarantees that QoS requirements are met. 
50|2-3||OCCN: a NoC modeling framework for design exploration|The On-Chip Communication Network (OCCN) project provides an efficient framework, developed within SourceForge, for the specification, modeling, simulation, and design exploration of network on-chip based on an object-oriented C++ library built on top of SystemC. OCCN is shaped by our experience in developing communication architectures for different System-on-Chip. OCCN increases the productivity of developing communication driver models through the definition of a universal Application Programming Interface (API). This API provides a new design pattern that enables creation and reuse of executable transaction level models across a variety of SystemC-based environments and simulation platforms. It also addresses model portability, simulation platform independence, interoperability, and high-level performance modeling issues. 
50|2-3||Guide for Authors|
50|4|http://www.sciencedirect.com/science/journal/13837621/50/4|Editorial board|
50|4||A multiple disk failure recovery scheme in RAID systems|In this paper, we propose a practical disk error recovery scheme tolerating multiple simultaneous disk failures in a typical RAID system, resulting in improvement in availability and reliability. The scheme is composed of the encoding and the decoding processes. The encoding process is defined by making one horizontal parity and a number of vertical parities. The decoding process is defined by a data recovering method for multiple disk failures including the parity disks. The proposed error recovery scheme is proven to correctly recover the original data for multiple simultaneous disk failures regardless of the positions of the failed disks. The proposed error recovery scheme only uses exclusive OR operations and simple arithmetic operations, which can be easily implemented on current RAID systems without hardware changes. 
50|4||Task migration in n-dimensional wormhole-routed mesh multicomputers|In a mesh multicomputer, performing jobs needs to schedule submeshes according to some processor allocation scheme. In order to assign the incoming jobs to a free submesh, a task compaction scheme is needed to generate a larger contiguous free region. The overhead of compaction depends on the efficiency of the task migration scheme. In this paper, two simple task migration schemes are first proposed in n-dimensional mesh multicomputers with supporting dimension-ordered wormhole routing in one-port communication model. Then, a hybrid scheme which combines advantages of the two schemes is discussed. Finally, we evaluate the performance of all of these proposed approaches. 
50|4||Multi-mesh of trees with its parallel algorithms|In recent years the multi-mesh network [Proceedings of the Ninth International Parallel Processing Symposium, Santa Barbara, CA, April 25–28, 1995, 17; IEEE Trans. on Comput. 68 (5) (1999) 536] has created a lot of interests among the researchers for its efficient topological properties. Several parallel algorithms for various trivial and nontrivial problems have been mapped on this network. However, because of its O(n) diameter, a large class of algorithms that involves frequent data broadcast in a row or in a column or between the diametrically opposite processors, requires O(n) time on an n×n multi-mesh. In search of faster algorithms, we introduce, in this paper, a new network topology, called multi-mesh of trees. This network is built around the multi-mesh network and the mesh of trees. As a result it can perform as efficiently as a multi-mesh network and also as efficiently as a mesh of trees. Several topological properties, including number of links, diameter, bisection width and decomposition are discussed. We present the parallel algorithms for finding sum of n4 elements and the n2-point Lagrange interpolation both in O(logn) 1 time. The solution of n2-degree polynomial equation, n2-point DFT computation and sorting of n2 elements are all shown to run in O(logn) time too. The communication algorithms one-to-all, row broadcast and column broadcast are also described in O(logn) time. This can be compared with O(n) time algorithms on multi-mesh network for all these problems. 
50|4||On the development of a communication-aware task mapping technique|Clusters have become a very cost-effective platform for high-performance computing. In these systems, although currently existing networks actually provide enough bandwidth for the existing applications and workstations, the trend is towards the interconnection network becoming the system bottleneck. Therefore, in the future, scheduling strategies will have to take into account the communication requirements of the applications and the communication bandwidth that the network can offer. One of the key issues in these strategies is the task mapping technique used when the network becomes the system bottleneck.In this paper, we propose a communication-aware mapping technique that tries to match as well as possible the existing network resources to the communication requirements of the applications running on the system. Also, we evaluate the mapping technique using real MPI application traces with timestamps. Evaluation results show that the use of the proposed mapping technique better exploits the available network bandwidth, improving load balancing and increasing the throughput that can be delivered by the network. Therefore, the proposed technique can be used in the design of communication-aware scheduling strategies for those situations where the communication requirements lead the network bandwidth to become the system performance bottleneck. 
50|4||Optimal all-ports collective communication algorithms for the k-ary n-cube interconnection networks|The need for collective communication procedures such as One-to-All broadcast, All-to-All broadcast arises in many parallel or distributed applications. Many of these communication procedures have been studied for many topologies of interconnection networks such as hypercubes, meshes, De Bruijn, star graphs and butterflies. In this paper we propose a construction of multiple edge-disjoint spanning trees for the k-ary n-cube which can be used to derive an optimal and fault tolerant broadcasting algorithm. We propose also an optimal All-to-All broadcasting algorithm. We consider the k-ary n-cube as a point-to-point interconnection, using store-and-forward, all-port assumption and a linear communication model. 
50|4||Guide for Authors|
50|5|http://www.sciencedirect.com/science/journal/13837621/50/5|Editorial board|
50|5||Guest editorial|
50|5||Efficient analysis of single event transients|The effects of charged particles striking VLSI circuits and producing single event transients (SETs) are becoming an issue for designers who exploit deep sub-micron technologies; efficient and accurate techniques for assessing their impact on VLSI designs are thus needed. This paper presents a new approach for generating the list of faults to be addressed during fault injection experiments tackling SET effects, which resorts to static timing analysis. Moreover, it proposes a simplified SET fault model, which is suitable for being adopted within a zero-delay fault simulation tool. Experimental results are reported on both standard benchmarks and real-life circuits assessing the effectiveness of the proposed techniques. 
50|5||Matrix-based software test data decompression for systems-on-a-chip|This paper describes a new compression/decompression methodology for using an embedded processor to test the other components of a system-on-a-chip (SoC). The deterministic test vectors for each core are compressed using matrix-based operations that significantly reduce the amount of test data that needs to be stored on the tester. The compressed data is transferred from the tester to the processor's on-chip memory. The processor executes a program which decompresses the data and applies it to the scan chains of each core-under-test. The matrix-based operations that are used to decompress the test vectors can be performed very efficiently by the embedded processor thereby allowing the decompression program to be very fast and provide high throughput of the test data to minimize test time. Experimental results demonstrate that the proposed approach provides greater compression than previous methods. 
50|5||Fast and energy-frugal deterministic test through efficient compression and compaction techniques|Conversion of the flip-flops of the circuit into scan cells helps ease the test challenge; yet test application time is increased as serial shift operations are employed. Furthermore, the transitions that occur in the scan chains during these shifts reflect into significant levels of circuit switching unnecessarily, increasing the power dissipated. Judicious encoding of the correlation among the test vectors and construction of a test vector through predecessor updates helps reduce not only test application time but also scan chain transitions as well. Such an encoding scheme, which additionally reduces test data volume, can be further enhanced through appropriately ordering and padding of the test cubes given. The experimental results confirm the significant reductions in test application time, test data volume and test power achieved by the proposed compression methodology. 
50|5||A novel FPGA local interconnect test scheme and automatic TC derivation/generation|This paper presents a novel local interconnect testing scheme for field programmable gate arrays (FPGAs). To maximize parallel testing, error-detecting code is used for testing one portion of interconnects and functional test of D latch for another in a test configuration (TC). A polynomial run time algorithm is introduced for deriving a minimal set of TCs. An in-house CAD tool is developed to automate the generation of device configurations from the set of TCs. 
50|5||Balanced dual-stage repair for dependable embedded memory cores|Advances in revolutionary system-on-chip (SoC) technology mainly depend on the high-performance ultra-dependable system core components. Among those core components, embedded memory system core, currently acquiring 54% of SoC area share, will continue its domination of SoC area share as it is anticipated to approach about 94% of SoC area share by the year 2014. Since memory cells are considered as more prone to defects and faults than logic cells, redundancy and repair have been extensively practiced for enhancing defect and fault tolerance. Unlike in legacy PCB (printed circuit board) or MCM (multichip module) based systems, embedded core components cannot be physically replaced once they are fabricated onto a SoC. To realize enhanced manufacturing yield and field reliability, both ATE (automated test equipment) and BISR (built-in-self-repair) are commonly utilized to allocate redundancy for embedded memory system cores. Since ATE (for repairing manufacturing defects) and BISR (for repairing field faults) share the given redundancy, probabilistic redundancy partitioning and utilization techniques are proposed in this paper to achieve optimal combination of yield and reliability of the embedded memory system core. Parametric simulation results are shown extensively. 
50|5||IDDQ data analysis using neighbor current ratios|IDDQ test loses its effectiveness for deep sub-micron chips since it cannot distinguish between faulty and fault-free currents. The concept of current ratios, in which the ratio of maximum to minimum IDDQ is used to screen faulty chips, has been previously proposed. However, it is incapable of screening some defects. The neighboring chips on a wafer have similar fault-free properties and are correlated. In this paper, the use of spatial correlation in combination with current ratios is investigated. By differentiating chips based on their non-conformance to local IDDQ variation, outliers are identified. The analysis of SEMATECH test data is presented. 
50|6|http://www.sciencedirect.com/science/journal/13837621/50/6|Editorial board|
50|6||Socket-based RR scheduling scheme for tightly coupled clusters providing single-name images|Server clusters for Internet services can yield both high performance and cost effectiveness. Contemporary approaches to cluster design must confront the tradeoff between dynamic load-balancing and efficiency when dispatching client requests to servers in the cluster. In this paper we describe a packet filtering-based RR scheduling scheme, intended to be an easily implemented scheme for hosting Internet services on a server cluster in a way transparent to clients. We take a non-centralized approach, in which client packets sent to the cluster's single IP address are broadcast to all of its servers. Packets requesting that a new TCP session be set up cause counters in each server to be incremented; if a server's counter matches its fixed unique ID, it takes charge of the session, else it ignores the packet. This yields a round-robin type algorithm. We describe this approach in detail, and present results of simulations that show it achieves higher performance (in terms of throughput and reliability) than similar approaches based on client-IP hashing and dispatcher-based RR. 
50|6||Developing a reusable workflow engine|Every time a workflow solution is conceived there is a large amount of functionality that is eventually reinvented and redeveloped from scratch. Workflow management systems from academia to the commercial arena exhibit a myriad of approaches having as much in common as in contrast with each other. Efforts in standardizing a workflow reference model and the gradual endorsement of those standards have also not precluded developers from designing workflow systems tailored to specific user needs. This article is written in the belief that an appropriate set of common workflow functionality can be abstracted and reused in forthcoming systems or embedded in applications intended to become workflow-enabled. Specific requirements and a prototype implementation of such functionality, named Workflow Kernel, are discussed. 
50|6||On the performance analysis of ABR in ATM LANs with Stochastic Petri Nets|In this paper we use Generalized Stochastic Petri Nets (GSPNs) and Stochastic Well-formed Nets (SWNs) for the performance analysis of Asynchronous Transfer Mode (ATM) Local Area Networks (LANs) that adopt the Available Bit Rate (ABR) service category in its Relative Rate Marking (RRM) version. We also consider a peculiar version of RRM ABR called Stop & Go ABR; this is a simplified ABR algorithm designed for the provision of best-effort services in low-cost ATM LANs, according to which sources can transmit only at two different cell rates, the Peak Cell Rate (PCR) and Minimum Cell Rate (MCR). Results obtained from the solution of GSPN models of simple ATM LAN setups comprising RRM or Stop & Go ABR users, as well as Unspecified Bit Rate (UBR) users, are first validated through detailed simulations, and then used to show that Stop & Go ABR is capable of providing good performance and fairness in a number of different LAN configurations. We also develop SWN models of homogeneous ABR LANs, that efficiently and automatically exploit system symmetries allowing the investigation of larger LAN configurations. 
50|6||Performance evaluation of a Windows NT based PC cluster for high performance computing|In recent times the computational power of personal computers has remarkably increased and the use of groups of PCs and workstations, connected by a network and dedicated to parallel computations, is today frequent. Computing clusters are mainly based on UNIX workstations and Linux PCs but, in the last few years, different implementations of message passing systems were made available also for Microsoft Windows. In this work we test the performance of two implementations of MPI for Windows platforms, and we compare the results with those obtained from Linux systems. 
50|6||Guide for Authors|
50|7|http://www.sciencedirect.com/science/journal/13837621/50/7|Editorial board|
50|7||Adaptable system/software architectures|
50|7||Relating evolving business rules to software design|In order to remain useful, it is important for software to evolve according to the changes in its business environment. Business rules, which can be used to represent both user requirements and conditions to which the system should conform, are considered as the most volatile part in today's software applications. Their changes bring high impact on both the business processes and the software itself. In this paper, we present an approach that considers business rules as an integral part of a software system and its evolution. The approach transcends the areas of requirements specification and software design. We develop the Business Rule Model to capture and specify business rules, and the Link Model to relate business rules to the metamodel level of software design elements. The aim is to improve requirements traceability in software design, as well as minimizing the efforts of software changes due to the changes of business rules. The approach is demonstrated using examples from an industrial application. 
50|7||Assessing systems adaptability to a product family|In many cases, product families are established on top of a successful pilot product. While this approach provides an option to measure many concrete attributes like performance and memory footprint, adequateness and adaptability of the architecture of the pilot cannot be fully verified. Yet, these properties are crucial business enablers for the whole product family. In this paper, we discuss an architectural assessment of one such seminal system, intended for monitoring electronic subsystems of a mobile machine, which is to be extended to support a wide range of different types of products. This paper shows how well the assessment reveals possible problems and existing flexibilities in assessed system, and this way helps different stakeholders in their further decisions. 
50|7||Measures for mobile users: an architecture|Software measures are important to evaluate software properties like complexity, reusability, maintainability, effort required, etc. Collecting such data is difficult because of the lack of tools that perform acquisition automatically. It is not possible to implement a manual data collection because it is error prone and very time expensive. Moreover, developers often work in teams and sometimes in different places using laptops. These conditions require tools that collect data automatically, can work offline and merge data from different developers working in the same project. This paper presents PROM (PRO Metrics), a distributed Java based tool designed to collect automatically software measures. This tool uses a distributed architecture based on plug-ins, integrated in most popular development tools, and the SOAP communication protocol. 
50|7||A Software System evolutionary and adaptive framework: application to Agent-based systems|In this paper we present part of our current work: a proposal on a Software System evolutionary framework. This proposal is based mainly on previous work carried out by the GEDES (Group of Specification, Development and Evolution of Software) Research Group. Within this framework, we try to model the way a Software System can evolve, and especially, the evolution of Agent-based systems. We present the way systems evolve based on the application of operators and the understanding of definition of focusing on which should be these operators, and invariants in Agent-based systems, as well as introducing examples of actions and restrictions applied. 
50|7||A model of runtime transformation for distributed systems based on directed acyclic graph model|This paper presents a formal model of runtime program transformation to optimize concurrent processes during executions based on a new representation of the scopes of names in distributed systems. We represent a site in a distributed system and the scopes of the local names in the site using a directed acyclic graph. It is possible to represent local names that their scope are not nested in a site in the model. Local names with overlapping scopes make possible to formalize folding transformation of process definitions for runtime program transformation. Computations in the system are represented with inferences on multisets of formulas of linear logic. We define rewriting system of directed acyclic graphs as the operational semantics of our model based on inference rules for linear logic. Each of the steps for runtime transformations are also represented using a rewriting for directed acyclic graphs. 
50|7||Generative and incremental implementation for a scripting interface|Many systems may benefit from scripting support, but the implementation of it is seldom trivial, especially if the system has not originally been developed with scripting support in mind. In this paper we describe a generative, incremental process for creating an intuitive Python interface to a large, hierarchic COM library. The approach is illuminated with the original, real-life case study. 
50|7||Guide for Authors|
50|8|http://www.sciencedirect.com/science/journal/13837621/50/8|Editorial board|
50|8||General decomposition of incompletely specified sequential machines with multi-state behavior realization|This paper is devoted to decomposition of sequential machines, discrete functions and relations. Sequential machine decomposition consists in representation of a given machine as a network of collaborating partial machines that together realize behavior of the given machine. A good understanding of possible decomposition structures and of conditions under which the corresponding structures exist is a prerequisite for any adequate circuit or system synthesis. The paper discusses the theory of general decomposition of incompletely specified sequential machines with multi-state behavior realization. The central point of this theory is a constructive theorem on the existence of the general decomposition structures and conditions under which the corresponding structures exist. The theory of general decomposition presented in this paper is the most general known theory of the binary, multi-valued and symbolic sequential and combinational discrete network structures. The correct circuit generator defined by the general decomposition theorem covers all other known structural models of sequential and combinational circuits as its special cases. Using this theory, in recent years we developed a number of effective and efficient methods and EDA tools for sequential and combinational circuit synthesis that consistently construct much better circuits than other academic and commercial state-of-the-art synthesis tools. This demonstrates the practical soundness of our theory. This theory can be applied to any sort of binary, multi-valued and symbolic systems expressed as networks of relations, functions or sequential machines, and can be very useful in such fields as circuit and architecture synthesis of VLSI systems, knowledge engineering, machine learning, neural network training, pattern analysis, etc. 
50|8||Implementing a replicated service with group communication|Distributed computing systems are quickly pervading many aspects of everyday life. The public demand for high reliability of these systems can only grow, together with their penetration in critical application domains. Group communication is a technology that may greatly simplifying the deployment of reliable distributed applications, even in environments composed of off-the-shelf hardware and software components. In this paper we attempt to provide an introductory and unified view to group communication. To make the presentation concrete, we shall analyze in detail how group communication may help in implementing a highly-available service based on replication. We shall consider a service whose interface allows determining the outcome of a non-idempotent operation previously submitted to the service, e.g., an update, that returned prematurely at the client because of a communication error––a key practical problem. We shall also provide a discussion of the performance that can be obtained in practice with group communication platforms. 
50|8||Guide for Authors|
50|9|http://www.sciencedirect.com/science/journal/13837621/50/9|Editorial board|
50|9||The use of economic agents under price driven mechanism in grid resource management|This paper presents multi-economic agent for grid resource management. A system model is described that allows agents representing various grid resources and grid users to interact without assuming priori cooperation. The system model consists of three layers. The lower layer is the underlying grid resource. The middle layer is the agent-based grid resource management system. It consists of three types of agent and market institution that allocates resources. The grid task agents buy resources to complete tasks. Grid resource agents charge the task agents for the amount of resource capacity allocated. Grid resource agents are registered with a Grid Manager. The third layer is the user layer at which grid request agents provide interfaces to the grid user' request. The three processes involved in grid resource management are given. A price-directed algorithm for solving the grid task agent resource allocation problem is presented. A basic performance evaluation is given. Finally, some conclusions are given. 
50|9||A comparative evaluation of hardware-only and software-only directory protocols in shared-memory multiprocessors|The hardware complexity of hardware-only directory protocols in shared-memory multiprocessors has motivated many researchers to emulate directory management by software handlers executed on the compute processors, called software-only directory protocols.In this paper, we evaluate the performance and design trade-offs between these two approaches in the same architectural simulation framework driven by eight applications from the SPLASH-2 suite. Our evaluation reveals some common case operations that can be supported by simple hardware mechanisms and can make the performance of software-only directory protocols competitive with that of hardware-only protocols. These mechanisms aim at either reducing the software handler latency or hiding it by overlapping it with the message latencies associated with inter-node memory transactions. Further, we evaluate the effects of cache block sizes between 16 and 256 bytes as well as two different page placement policies. Overall, we find that a software-only directory protocol enhanced with these mechanisms can reach between 63% and 97% of the baseline hardware-only protocol performance at a lower design complexity. 
50|9||On the performance of multicomputer interconnection networks|Several researchers have analysed the performance of k-ary n-cubes taking into account channel bandwidth constraints imposed by implementation technology, namely the constant wiring density and pin-out constraints for VLSI and multiple-chip technology respectively. For instance, Dally [IEEE Trans. Comput. 39(6) (1990) 775], Abraham [Issues in the architecture of direct interconnection networks schemes for multiprocessors, Ph.D. thesis, University of Illinois at Urbana-Champaign, 1992], and Agrawal [IEEE Trans. Parallel Distributed Syst. 2(4) (1991) 398] have shown that low-dimensional k-ary n-cubes (known as tori) outperform their high-dimensional counterparts (known as hypercubes) under the constant wiring density constraint. However, Abraham and Agrawal have arrived at an opposite conclusion when they considered the constant pin-out constraint. Most of these analyses have assumed deterministic routing, where a message always uses the same network path between a given pair of nodes. More recent multicomputers have incorporated adaptive routing to improve performance. This paper re-examines the relative performance merits of the torus and hypercube in the context of adaptive routing. Our analysis reveals that the torus manages to exploit its wider channels under light traffic. As traffic increases, however, the hypercube can provide better performance than the torus. Our conclusion under the constant wiring density constraint is different from that of the works mentioned above because adaptive routing enables the hypercube to exploit its richer connectivity to reduce message blocking. 
50|9||Multi-node broadcasting in all-ported 3-D wormhole-routed torus using an aggregation-then-distribution strategy|In this paper, we investigate the multi-node broadcasting problem in a 3-D torus, where there are an unknown number of s source nodes located at unknown positions each intending to broadcast a message of size m bytes to the rest of the network. The torus is assumed to use the all-port model and the popular dimension-ordered routing. Existing congestion-free results are derived based on finding multiple edge-disjoint spanning trees in the network. This paper shows how to efficiently perform multi-node broadcasting in a 3-D torus. The main technique used in this paper is an aggregation-then-distribution strategy, which is characterized by the following features: (i) the broadcast messages are aggregated into some positions on the 3-D torus, then a number of independent subnetworks are constructed from the 3-D torus; and (ii) these subnetworks, which are responsible for distributing the messages, fully exploit the communication parallelism and the characteristic of wormhole routing. It is shown that such an approach is more appropriate than those using edge-disjoint trees for fixed-connection networks such as tori. Extensive simulations are conducted to evaluate this multi-broadcasting algorithm. 
50|9||Guide for Authors|
volume|issue|url|title|abstract
51|-|http://www.sciencedirect.com/science/journal/13837621/51|Publisherâs Note - New editorial system|
51|-||Time-constrained scheduling of large pipelined datapaths|This paper addresses the most crucial optimization problem of high-level synthesis: scheduling. A formal framework is described that was tailored specifically for the definition and investigation of the time-constrained scheduling problem of pipelined datapaths. Theoretical results are presented on the complexity of the problem. Moreover, two new heuristic algorithms are introduced. The first one is a genetic algorithm, which, unlike previous approaches, searches the space of schedulings directly. The second algorithm realizes a heuristic search using constraint logic programming methods. The performance of the proposed algorithms has been evaluated on a set of benchmarks and compared to previous approaches. 
51|-||Heterogeneous system level co-simulation for the design of telecommunication systems|The advanced complexity and heterogeneity of modern telecommunication systems mostly lead to the incorporation of heterogeneous implementation technologies and design styles. Consequently, the design representation of such systems often requires the mixed use of distinct model of computations at different abstraction layers. Therefore, heterogeneous co-simulation is needed in order to enable the effective communication and interaction among the involved models of computation. This paper resolves this issue by proposing the heterogeneous co-modelling of telecom systems based on the combination of SDL semantics with C language running on an instruction set simulator, coupling in that way the specification and the first refinement steps of the co-design flow. The missing test link between the corresponding tools that support the SDL-C co-model is addressed by proposing a heterogeneous co-simulation scheme through the development of a mediator. Finally, the proposed methodology and the efficiency of the built environment are evaluated through a case study associated with the design of the MAC layer of the DECT telecom system.1,2 
51|-||Author Index to Volume 51 (2005)|
51|-||Subject Index to Volume 51 (2005)|
51|-||Guide for Authors|
51|1|http://www.sciencedirect.com/science/journal/13837621/51/1|Hierarchical star: a new two level interconnection network|We propose a new two level interconnection network topology, hierarchical star networks, HSn, that uses the star graphs as building blocks. Two level networks have been previously proposed that use hypercube and its variants as building blocks; it has been shown that these two level networks are superior to the networks, that are used as building blocks, in terms of various performance metrics including diameter, cost, fault tolerance, fault diameter etc. Our results show that the proposed family of hierarchical star networks perform very competitively in comparison to star graphs; in addition, the proposed network outperforms all of the two level hierarchical networks proposed earlier that uses hypercubes (or its variations) as building blocks. Thus, our results further reinforce the notion that the star graphs are strong competitors of hypercubes for large multiprocessor design. We also investigate various topological properties of the network including embedding, mapping of parallel algorithms, fault tolerance and broadcasting algorithms. 
51|1||Architecture optimization for multimedia application exploiting data and thread-level parallelism|The characteristics of multimedia applications when executed on general-purpose processors are not well understood. Such knowledge is extremely important in guiding the development of multimedia applications and the design of future processors.In this paper, we characterize and optimize the performance of multimedia applications on superscalar processor exploiting data-level parallelism and thread-level parallelism with SIMD (Single Instruction Multiple Data) and SMT (Simultaneous MultiThreading) capacities. We show that SMT and SIMD superscalar processor is suitable for 3D geometry application and we characterize the execution in term of memory hierarchy, which is the main bottleneck. The results show that the latency is not fully recovered by SMT; the use of second-level data prefetching does not succeed in increasing the performance.With detailed analysis, we show that this problem comes from a pollution of the instruction window by the threads experiencing second-level cache misses, thus reducing the window available for the other threads. We thus propose a hardware mechanism (an architecture optimization) to predict second-level misses and control this pollution. 
51|1||An FPGA-based coprocessor for real-time fieldbus traffic schedulingâarchitecture and implementation|Distributed computer control systems used nowadays in the industry need often to meet requirements of on-line reconfigurability so they can adjust dynamically to changes in the application environment or to evolving specifications. The communication network connecting the computer nodes, commonly a fieldbus system, must use therefore dynamic scheduling strategies, together with on-line admission control procedures that test the validity of all changes in order to guarantee the satisfaction of real-time constraints. These are both very computationally demanding tasks, something that has precluded their wide adoption. However, these algorithms also embed sufficient levels of parallelism to grant them benefits from implementations in dedicated hardware.This paper presents a scheduling coprocessor that executes dynamic real-time traffic scheduling and schedulability analysis. The FPGA-based implementation described here supports multiple scheduling policies and was tailored for the FTT-CAN protocol, but it can be used also in other fieldbuses relying on centralized scheduling. The coprocessor generates schedules in about two orders of magnitude less time than any practical network elementary cycle duration. The time to execute a schedulability test is deterministic. An evaluation based on the SAE benchmark yielded a worst-case execution time of 1.4 ms.The paper starts by discussing the scheduling problem being addressed. It describes then the coprocessor functionality and architecture, highlighting important design decisions, and its latest implementation. Finally the coprocessor performance evaluation is presented. 
51|1||A novel min-process checkpointing scheme for mobile computing systems|In distributed computing systems, processes in different hosts take checkpoints to survive failures. For mobile computing systems, due to certain new characteristics such as mobility, low bandwidth, disconnection, low power consumption and limited memory, conventional distributed checkpointing schemes need to be reconsidered. In this paper, a novel min-process coordinated checkpointing algorithm that makes full use of the computation ability and power of mobile support stations is proposed. During normal computation message transmission, the checkpoint dependency information among mobile hosts is recorded in the corresponding mobile support stations. When a checkpointing procedure begins, the initiator concurrently informs relevant mobile hosts, which minimizes the identifying time. Moreover, compared with the existing coordinated checkpointing schemes, our algorithm blocks the minimum number of mobile support stations during the identifying procedure, which leads to the improvement of the system performance. In addition, the proposed algorithm is a min-process, domino-free checkpointing algorithm, which is especially desirable for mobile computing systems. Quantitative analysis and experimental simulation show that our algorithm outperforms other coordinated checkpointing schemes in terms of the identifying time and the number of blocked mobile support stations and then can provide a better system performance for mobile computing systems. 
51|1||The impact of x86 instruction set architecture on superscalar processing|Performance improvement of x86 processors is a relevant matter. From the point of view of superscalar processing, it is necessary to complement the studies on instruction use with analogous ones on data use and, furthermore, analyze the data flow graphs, as its dependencies are responsible for limitations on ILP. In this work, using instruction traces from common applications, quantitative analyses of implicit operands, memory addressing and condition codes have been performed, three sources of significant limitations on the maximum achievable parallelism in the x86 architecture. In order to get a deeper knowledge of these limitations, the data dependence graphs are built from traces. By means of graph matrix representation, potentially exploitable parallelism is quantified and parallelism distributions from the traces are shown. The method has also been applied to measure the impact of the use of condition codes. Results are compared with previous work and some conclusions are presented relating the obtained degree of parallelism with negative characteristics of x86 instruction set architecture. 
51|1||Guide for Authors|
51|10-11|http://www.sciencedirect.com/science/journal/13837621/51/10-11|Switch fabric design for high performance IP routers: A survey|Traditionally, besides vendor product descriptions on high performance Internet Protocol (IP) router hardware (HW) architectures, materials on this subject area seldom appear in research literature. Recently, we introduced an architectural concept of HW scalability and bi-directional HW reconfigurability for high performance IP routers. Application of these two conceptual attributes enables router HW flexibility to adapt to today’s IP network environment with rapid changes in capacity and traffic characteristics. We analyzed 10 switch fabrics (SFs), selected and also presented brief survey of HW architectural techniques that enable the attributes for three candidates that can serve such a router. In this paper, we present a full survey of these 10 SFs. The intention is to provide background reference material on an area not yet frequently visited in formal literature. 
51|10-11||Implementation and performance study of a hardware-VIA-based network adapter on Gigabit Ethernet|This paper presents the implementation and performance of a hardware-VIA-based network adapter on Gigabit Ethernet. VIA is a user–level communication interface for high performance PC clustering. The network adapter is a 64-bit/66 MHz PCI plug-in card containing an FPGA for the VIA Protocol Engine and a Gigabit Ethernet chip to construct a high performance system area network. The network adapter performs virtual-to-physical address translation, doorbell, RDMA write, and send/receive completion operations in hardware without kernel intervention. In particular, the Address Translation Table (ATT) is stored on the local memory of the network adapter, and the VIA Protocol Engine efficiently controls the address translation process by directly accessing the ATT. In addition, Address Prefetch Buffer is used to reduce the time of address translation process in the receiver. As a result, the communication overhead during send/receive transactions is greatly reduced. Our experimental results show a minimum latency of 8.2 Î¼s, and a maximum bandwidth of 112.1 MB/s. In terms of minimum latency, the hardware-VIA-based network adapter performs 2.8 times and 3.3 times faster than M-VIA, which is a software implementation of VIA, and TCP/IP, respectively, over Gigabit Ethernet. In addition, the maximum bandwidth of the hardware-VIA-based network adapter is 24% and 55% higher than M-VIA and TCP/IP, respectively. These results show that the performance of HVIA-GE is far better than that of ServerNet II, which is a hardware version of VIA developed by Tandem/Compaq. 
51|10-11||A plane-based broadcast algorithm for multicomputer networks|Maximising the performance of parallel systems requires matching message-passing algorithms and application characteristics with a suitable underling interconnection network. Broadcast algorithms for wormhole-switched meshes have been widely reported in the literature. However, most of these algorithms handle broadcast in a sequential manner and do not scale well with the network size. As a consequence, many parallel applications cannot be efficiently supported using existing techniques. Motivated by these observations, this paper presents a new efficient broadcast algorithm for the mesh, called the Plane-Based (PB) algorithm. The main feature of this approach is its ability to perform broadcast operation with a high degree of scalability and parallelism. Furthermore, performance is insensitive to the network size, i.e., only three message-passing steps are required to implement a broadcast operation irrespective of the network size. Results from a comparative analysis demonstrate that the PB algorithm exhibits superior performance characteristics over those of the well-known Recursive Doubling and Extending Dominating Node algorithms. 
51|10-11||Dynamic voltage scaling techniques for power efficient video decoding|This paper presents a comparison of power-aware video decoding techniques that utilize dynamic voltage scaling (DVS). These techniques reduce the power consumption of a processor by exploiting high frame variability within a video stream. This is done through scaling of the voltage and frequency of the processor during the video decoding process. However, DVS causes frame deadline misses due to inaccuracies in decoding time predictions and granularity of processor settings used. Four techniques were simulated and compared in terms of power consumption, accuracy, and deadline misses. In addition, this paper proposes the frame-data computation aware (FDCA) technique, which is a useful power-saving technique not only for stored video but also for real-time video applications. The FDCA method is compared with the GOP, Direct, and Dynamic methods, which tend to be more suited for stored video applications. The simulation results indicated that the Dynamic per-frame technique, where the decoding time prediction adapts to the particular video being decoded, provides the most power saving with performance comparable to the ideal case. On the other hand, the FDCA method consumes more power than the Dynamic method but can be used for stored video and real-time time video scenarios without the need for any preprocessing. Our findings also indicate that, in general, DVS improves power savings, but the number of deadline misses also increase as the number of available processor settings increases. More importantly, most of these deadline misses are within 10–20% of the playout interval and thus have minimal affect on video quality. However, video clips with high variability in frame complexities combined with inaccurate decoding time predictions may degrade the video quality. Finally, our results show that a processor with 13 voltage/frequency settings is sufficient to achieve near maximum performance with the experimental environment and the video workloads we have used. 
51|10-11||A low energy cache design for multimedia applications exploiting set access locality|An architectural technique is proposed to reduce power dissipation in conventional caches. Our technique is based on the observation of cache access locality: current access is likely to touch the same cache set including the tags as the last access. We show that considerable amount of power driving the cache tag and data banks can be saved if this cache access locality is fully exploited. This is achieved through buffering and accessing the last accessed cache set instead of driving the tag and data banks. Unlikely previous designs, our technique does not incur performance degradation. Experimental results carried out on 8 KB/16 KB/32 KB data and instruction caches have respectively shown 31%/35%/36% and 51%/58%/66% power savings. 
51|10-11||Guide for Authors|
51|2|http://www.sciencedirect.com/science/journal/13837621/51/2|A new distributed storage scheme for cluster video server|For cluster video servers, it is very important to design a good distributed storage system with high performance. One of the important issues in designing a good distributed storage system is how to store multimedia data on many storage nodes. This issue includes two topics: the scheme of splitting an entire file into many clips, and the storage of these clips on many nodes. We have designed a new multimedia data storage scheme for cluster video server. In the new system, a novel multimedia file splitting scheme, named Owl, and a clips striping scheme have been proposed. In contrast with traditional media data splitting schemes based on fixed space length and constant time length, Owl is addressed with the consideration of spatial and temporal information. This scheme Owl has universality for every media format encoded based on time and makes cluster video servers work efficiently. Besides, the scheme Owl is feasible and easy to implement. With the data splitting scheme and the clips striping scheme, cluster video servers have good performance. 
51|2||Dual and multiple token based approaches for load balancing|In distributed systems uneven arrivals of the tasks may overload a few hosts while some of the hosts may be lightly loaded. This load imbalance prevents distributed systems from delivering its performance to its capacity. Load balancing has been advocated as a means of improving performance and reliability of distributed systems. We propose a distributed load balancing algorithm LoGTra to deal with this problem. LoGTra uses load graph and token based policy. The extensions to LoGTra based on dual tokens DTLB and multiple tokens m-LoGTra are proposed in this paper. m-LoGTra allows host to generate multiple tokens. This allows system to search host for load balancing in multiple directions and can avoid starvation of remote hosts. Local maxima and local minima are responsible for initiating transfer and that limits the number of hosts generating tokens at a time. As overheads are kept under control by limiting number of tokens, the algorithm promises for improved performance. 
51|2||A new NAND-type flash memory package with smart buffer system for spatial and temporal localities|This research is to design a high performance NAND-type flash memory package with a smart buffer cache that enhances the exploitation of spatial and temporal locality. The proposed buffer structure in a NAND flash memory package, called as a smart buffer cache, consists of three parts, i.e., a fully-associative victim buffer with a small page size, a fully-associative spatial buffer with a large page size, and a dynamic fetching unit. This new NAND-type flash memory package can achieve dramatically higher performance and lower power consumption compared with any conventional NAND-type flash memory module. Our results show that the NAND flash memory package with a smart buffer cache can reduce the miss ratio by around 70% and the average memory access time by around 67%, over the conventional NAND flash memory configuration. Also, the average miss ratio and the average memory access time of the package module with smart buffer cache for a given buffer space (e.g., 3 KB) can achieve better performance than package modules with a conventional direct-mapped buffer with eight times (e.g., 32 KB) or than a fully-associative configuration with twice as much space (e.g., 8 KB). 
51|2||CORBA-based distributed and replicated resource repository architecture for hierarchically configurable home network|Home networks are typically ubiquitous computing networks consisting of various consumer devices and services. Although many middlewares have already been developed and used to implement partial services in home networks, none has yet emerged as a generic home network middleware architecture, as existing middlewares are designed to accommodate specific services, making them unable to cover all services or satisfy the desires of home users. Accordingly, the current paper presents a hierarchically configurable home network architecture as a generic and conceptual home network model, plus a middleware framework is proposed to realize the conceptual model. In the middleware framework, a distributed and replicated resource repository architecture is introduced to realize multi-services within a single home network platform. The proposed middleware was developed and implemented using CORBA within an IEEE1394-based home network, and its performance verified experimentally. 
51|2||Optimal broadcasting on incomplete star graph interconnection networks|Broadcasting is an important collective communication operation in many applications which use parallel computing. In this paper, we focus on designing broadcasting algorithms for general incomplete star graphs. We propose two optimal one-to-all broadcasting algorithms for incomplete star graphs with a single-port communication model. An incomplete star graph with N nodes, where (n − 1)! < N < n!, is a subgraph of an n-dimensional star graph. The first scheme is single-message one-to-all broadcasting that takes O(n log n) steps. The second one is multi-message one-to-all broadcasting that takes O(n log n + m) steps. 
51|2||Guide for Authors|
51|3|http://www.sciencedirect.com/science/journal/13837621/51/3|Instruction level redundant number computations for fast data intensive processing in asynchronous processors|Instruction level parallelism (ILP) is strictly limited by various dependencies. In particular, data dependency is a major performance bottleneck of data intensive applications. In this paper we address acceleration of the execution of instruction codes serialized by data dependencies. We propose a new computer architecture supporting a redundant number computation at the instruction level. To design and implement the scheme, an extended data-path and additional instructions are also proposed. The architectural exploitation of instruction level redundant number computations (IL-RNC) makes it possible to eliminate carry propagations. As a result execution of instructions which are serialized due to inherent data dependencies is accelerated. Simulations have been performed with data intensive processing benchmarks and the proposed architecture shows about a 1.2–1.35 fold speedup over a conventional counterpart. The proposed architecture model can be used effectively for data intensive processing in a microprocessor, a digital signal processor and a multimedia processor. 
51|3||Multicast communication in wormhole-routed symmetric networks with hamiltonian cycle model|In this paper, we first introduce a new hamiltonian cycle model for exploiting the features of symmetric networks. Based on this model, we propose two efficient multicast routing algorithms, uniform multicast routing algorithm and fixed multicast routing algorithm, in symmetric networks with wormhole routing. The proposed multicast routing algorithms utilizes channels uniformly to reduce the path length of message worms, making the multicasting more efficient in symmetric networks. We present two symmetric networks, the torus and star graph, to illustrate the superiority of the proposed schemes. Simulations are conducted to show that the proposed routing schemes outperform the previous scheme. 
51|3||A generalized fault-tolerant sorting algorithm on a product network|A product network defines a class of topologies that are very often used such as meshes, tori, and hypercubes, etc. This paper proposes a generalized algorithm for fault-tolerant parallel sorting in product networks. To tolerate r − 1 faulty nodes, an r-dimensional product network containing faulty nodes is partitioned into a number of subgraphs such that each subgraph contains at most one fault. Our generalized sorting algorithm is divided into two steps. First, a single-fault sorting operation is presented to correctly performed on each faulty subgraph containing one fault. Second, each subgraph is considered a supernode, and a fault-tolerant multiway merging operation is presented to recursively merge two sorted subsequences into one sorted sequence. Our generalized sorting algorithm can be applied to any product network only if the factor graph of the product graph can be embedding in a ring. Further, we also show the time complexity of our sorting operations on a grid, hypercube, and Petersen cube. Performance analysis illustrates that our generalized sorting scheme is a truly efficient fault-tolerant algorithm. 
51|3||More on rearrangeability of combined (2n â 1)-stage networks|This paper considers a class of combined (2n − 1)-stage N × N interconnection networks composed of two n(= log2N)-stage omega-equivalent networks M(n) and M′(n). The two networks are concatenated with the last stage of M(n) overlapped with the first stage of M′(n), forming a combined (2n − 1) stage network. Though both Benes network and (2n − 1)-stage shuffle-exchange network belong to this class, the former one is a rearrangeable network, whereas the rearrangeability of the latter one is still an open problem. So far, there is no algorithm, in general, that may determine whether a given (2n − 1)-stage combined network is rearrangeable or not. In this paper, a sufficient condition for rearrangeability of a combined (2n − 1)-stage network has been formulated. An algorithm with time complexity O(Nn) is presented to check it. If it is satisfied, a uniform routing algorithm with time complexity O(Nn) is developed for the combined network. Finally, a novel technique is presented for concatenating two omega-equivalent networks, so that the rearrangeability of the combined network is guaranteed, and hence the basic difference between the topologies of a Benes network and a (2n − 1)-stage shuffle-exchange network has been pointed out. 
51|3||Guide for Authors|
51|4|http://www.sciencedirect.com/science/journal/13837621/51/4|Generating cache hints for improved program efficiency|One of the new extensions in EPIC architectures are cache hints. On each memory instruction, two kinds of hints can be attached: a source cache hint and a target cache hint. The source hint indicates the true latency of the instruction, which is used by the compiler to improve the instruction schedule. The target hint indicates at which cache levels it is profitable to retain data, allowing to improve cache replacement decisions at run time. A compile-time method is presented which calculates appropriate cache hints. Both kind of hints are based on the locality of the instruction, measured by the reuse distance metric.Two alternative methods are discussed. The first one profiles the reuse distance distribution, and selects a static hint for each instruction. The second method calculates the reuse distance analytically, which allows to generate dynamic hints, i.e. the best hint for each memory access is calculated at run-time.The implementation of the static hints scheme in the Open64-compiler for the Itanium processor shows a speedup of 10% on average on a set of pointer-intensive and regular loop-based programs. The analytical approach with dynamic hints was implemented in the FPT-compiler and shows up to 34% reduction in cache misses. 
51|4||Evaluating IA-32 web servers through simics: a practical experience|Nowadays, the use of multiprocessor systems is not just limited to typical scientific applications, but these systems are increasingly being used for executing commercial applications, such as databases and web servers. Therefore, it becomes essential to study the behavior of multiprocessor architectures under commercial workloads. To accomplish this, we need simulators able to model not only the CPU, memory and interconnection network but also other aspects that are critical in the execution of commercial workloads, such as I/O subsystem and operating system. In this paper, we present our first experiences using Simics, a simulator which allows full-system simulation of multiprocessor architectures covering all the topics previously mentioned. Using Simics we carry out a detailed performance study of a static web content server, showing how changes in some architectural parameters, such as number of processors and cache size, affect final performance. The results we have obtained corroborate the intuition of increasing performance of a dual-processor web server opposite to a single-processor one, and at the same time, allow us to check out Simics limitations. Finally, we compare these results with those that are obtained on real machines. 
51|4||On-chip short-time interval measurement system for high-speed signal timing characterization|In this paper, we present an on-chip short-time interval measurement circuit to characterize the ever-faster communication systems. The proposed circuit consists of time parameters extraction subcircuit and measurement subcircuit, which contain a charge pump, a comparator with hysteresis, a digital counter, and a capacitor. During the measurement, we control a current source and a current sink to charge or discharge a common capacitor, and record the time length ratio of the charging and discharging process. Using this method, we can easily measure most of the time-domain parameters. Also this method minimizes the impacts of the process variations to get high measurement accuracy. 
51|4||Comments on âSign detection in residue arithmetic unitsâ [Journal of Systems Architecture 45 (1998) 251â258]|It has been shown that the hardware architecture of sign detector presented in [G. Alia, E. Martinelli, Sign detection in residue arithmetic units, Journal of Systems Architecture 45 (1998) 251] is incorrect because the interpretation and the derivation of the rank function is wrong. 
51|4||Guide for Authors|
51|5|http://www.sciencedirect.com/science/journal/13837621/51/5|Generalized parallel divide and conquer on 3D mesh and torus|In this paper, we handle the problem of 1mapping divide-and-conquer idea to 3D mesh and torus interconnection networks. Binary tree is not an efficient computation structure, thus, we select the computation structure as binomial tree. We propose an algorithm for divide and conquer on 3D meshes/torus. After that we give dilation of this algorithm for any 3D mesh whose size is power of 2 and the congestion of this embedding is 1, since each binomial tree consists of two edge-disjoint binomial tree B(n − 1)s.The communication times of proposed algorithm for store-and-forward routing mechanisms are evaluated with respect to some specific values of message ratio Î±. The results of wormhole routing mechanism are better than the results of store-and-forward routing mechanism due to the nonunit dilation of embedding.The efficiency of the proposed algorithm is also investigated in this paper. If sequential algorithm has the complexity or number of computation as the quadratic form of size of data, then the proposed algorithm is cost-optimal depending on the routing mechanism being wormhole. In the store-and-forward routing mechanism, the number of computation in the sequential algorithm does not make the proposed algorithm be cost-optimal or not. The communication time is dominant and computation time is less effective than communication time. 
51|5||High-performance architecture for anisotropic filtering|Anisotropic filtering has been developed as a way of increasing the quality of texture mapping. However, a real hardware implementation of anisotropic filters implies approximations and simplifications that result in lower quality. In this paper we present a new, efficient hardware oriented anisotropic filtering technique. Specifically, the new algorithm we propose is based on the utilisation of a new distance computation method. This distance generation scheme permits simplification of the algorithm without reducing its quality. Additionally, a classification procedure for the identification of the position of each texel inside the footprint is proposed. This simple procedure permits the reduction of the computational requirements associated with the algorithm. We also present a new method for the computation of the coverage of a texel, based on the storage of a reduced set of possible coverage patterns. The new distance scheme obtains the patterns of all texels from a given texel through simple shift operations. The good quality results obtained with the filtering algorithm we propose, together with the low computational and storage requirements of the architecture we present, makes it a good candidate for hardware implementation. 
51|5||Profiling soft-core processor applications for hardware/software partitioning|In this paper, we present an efficient approach to HW/SW partitioning of applications targeted for embedded soft-core SoPC and programmable logic. The methodology is based on the iterative performance analysis of the initial functional SW description and performance estimation of various HW/SW partitioning configurations. The main focus is on adequate profiling of arbitrary SW code regions (function or single instruction level) with clock-cycle accuracy without introducing additional execution overhead. In order to support the profiling for partitioning, we have developed the COMET Profiler tool. The performance analysis and estimation in the simulation and implementation domains are supported, necessitating no design and implementation of HW co-processing blocks for the partitioning evaluation. The design process is illustrated with two case studies. 
51|5||Run-time analysis of time-critical systems|AnaTempura is a tool based on Interval Temporal Logic (ITL). It is used to analyse time-critical systems at run-time. It validates code (implementation) against a formal specification. In this paper, we will describe a tool, AnaTempura and its supporting logic, ITL. A small but illustrative case study is presented. 
51|5||Guide for Authors|
51|6-7|http://www.sciencedirect.com/science/journal/13837621/51/6-7|Reconfigurable embedded systems: Synthesis, design and application|
51|6-7||A configurable system-on-chip architecture for embedded and real-time applications: concepts, design and realization|This paper presents a Configurable System-on-Chip (CSoC) architecture that includes programmable and reconfigurable hardware to cope with the flexibility and real-time signal processing demands in future telecommunication and multimedia systems. A programmable micro Task Controller (mTC) with a small instruction set and a novel pipelined configuration technique with descriptors as configuration templates allows a dynamic use of physical processing resources. The CSoC architecture provides a micro-task based programming model, approves a library-based design approach to reduce developing time and costs and allows forward compatibility to other architecture families. It is shown to be easy scalable to future VLSI technologies where over a hundred processing cells on a single chip will be feasible to deal with the inherent dynamics of future applications and system requirements. Several mappings of commonly used signal processing algorithms and implementation results are given for a standard cell ASIC design realization in 0.18 Î¼m 6-layer UMC CMOS technology. 
51|6-7||Distance-aware L2 cache organizations for scalable multiprocessor systems|In order to provide the scalability to the multiprocessor systems, it is important to keep the remote memory access time in bounds so that it does not impose much additional overhead as the system grows.In this paper, we suggest an LRU/distance-aware combined second-level(L2) cache for scalable CC-NUMA multiprocessors, which is composed of a traditional LRU cache and an additional distance-aware cache that maintains the distance information of individual cache block for replacement purposes. The LRU cache selects a victim using age information as it typically does, while the distance-aware cache does using distance information. Both work together to reduce effectively the overall distance the cache miss goes through by keeping long-distance blocks as well as recently used blocks. It has been observed that the proposed cache outperforms the traditional LRU cache by up to 28% in the execution time. It is also found to perform even better than an LRU cache of twice the size. 
51|6-7||Multiple voltage and frequency scheduling for power minimization|The design description for an integrated circuit may be described in terms of three domains, namely: (1) behavioral domain, (2) structural domain and (3) physical domain. These domains may be hierarchically divided into several levels of abstraction. Classically, these level of abstraction are (1) Architectural or Functional level, (2) Register-transfer level, (3) Logic level and (4) Circuit level. Some of the design problems associated with VLSI circuit design are area, speed, reliability and power consumption. With the development of portable devices, power consumption has become a dominant design consideration in the modern VLSI design area. In each of these domains there are a number of design challenges to reduce power. For instance, at the behavioral level, the freedom to choose multiple voltages and frequencies to minimize power to meet the given hard time constraints is considered as an active field of research to minimize power. Various past researches have showed that higher the level of abstraction, better the ability to address the problems associated with the design. Therefore this work proposes an algorithm that allocates both voltage and frequency simultaneously to the operations of the directed flow graph to optimize power given the time constraints. The resources required for multiple voltage-frequency scheduling is derived using the classical force directed scheduling algorithm. This algorithm has been implemented and tested on High-Level synthesis benchmarks for both non-pipelined and pipeline instances. 
51|6-7||Novel source-independent characterization methodology for embedded software energy estimation and optimization|In order to design a successful low-energy VLSI system, concurrent energy reduction at hardware and software levels is needed. The available techniques for embedded software energy estimation either provide unusable average-case results or require prohibitively complex hardware setups for cycle-accurate results. This paper introduces a new methodology for high-level software energy estimation for embedded systems. The methodology produces cycle-accurate results independent of the energy characterization process. The executed instructions as well as the transitions on the wires are taken into consideration for estimating the energy. This allows tradeoff between the accuracy and the complexity of the model. The methodology is generic and makes no assumptions about the measurement techniques or the architecture of the processor. The introduced methodology also allows successive improvements in the estimation accuracy with each step towards final silicon. The embedded ARM7TDMI RISC processor is modeled with this methodology and the errors are found to be less than 10%. For energy optimization, the model provides excellent relative accuracy too. Taking advantage of the relative accuracy, different code transformation techniques are discussed and employed to gain 32% energy savings. 
51|6-7||Information-driven circuit synthesis with the pre-characterized gate libraries|The opportunities created by modern microelectronic technology cannot effectively be exploited, because of weaknesses in traditional circuit synthesis methods used in today’s CAD tools. In this paper, a new information-driven circuit synthesis method is discussed that targets combinational circuits implemented with gates from the pre-characterized gate libraries. The method is based on our original information-driven approach to circuit synthesis, bottom–up general functional decomposition and theory of information relationship measures. It differs considerably from all other known methods. The experimental results from the automatic circuit synthesis tool that implements the method demonstrate that the information-driven general decomposition produces very fast and compact gate-based circuits. 
51|6-7||An application of functional decomposition in ROM-based FSM implementation in FPGA devices|Modern FPLD devices have very complex structure. They combine PLA like structures, as well as FPGA and even memory-based structures. However lack of appropriate synthesis methods do not allow fully exploiting the possibilities the modern FPLDs offer. The paper presents a general method for the synthesis targeted to implementation of sequential circuits using embedded memory blocks. The method is based on the serial decomposition concept and relies on decomposing the memory block into two blocks: a combinational address modifier and a smaller memory block. An appropriately chosen decomposition strategy may allow reducing the required memory size at the cost of additional logic cells for address modifier implementation. This makes possible implementation of FSMs that exceed available memory by using embedded memory blocks and additional programmable logic. 
51|6-7||Design and FPGA implementation of an MPEG based video scalar with reduced on-chip memory utilization|A new algorithm and a novel architecture suitable for FPGA/ASIC implementation of a video scalar is presented in this paper. The scheme proposed here results in enormous savings of memory normally required, without compromising on the image quality. In the present work, SVGA compatible video sequence is scaled up to XGA format. The up scaling operation for a video sequence is carried out by scaling up the image input, followed by down scaling and filtering. The FPGA implementation of the proposed video-scaling algorithm is capable of processing high-resolution, color pictures of sizes of up to 1024 × 768 pixels at the real time video rate of 30 frames/s. The video scalar is capable of scaling down XGA format to SVGA format as well. The design has been realized by RTL compliant Verilog coding, and fits into a single chip with a gate count utilization of two million gates. For lower resolution pictures, the mapped device can be scaled down. The present FPGA implementation compares favorably with another ASIC implementation.Also, an MPEG-2 codec implementation is presented for use in applications, where the video scalar and codec may be used to reduce transmission bit rate. Transmission of high resolution pictures of XGA format and above, even after effecting compression, demand very high serial channel bandwidth requirement, far exceeding the prescribed maximum by MPEG-2 standards. This can be circumvented by down scaling and then effecting compression before transmission, trading off for a little image quality, as presented in this paper. 
51|6-7||Guide for Authors|
51|8|http://www.sciencedirect.com/science/journal/13837621/51/8|Exploring the performance of split data cache schemes on superscalar processors and symmetric multiprocessors|Current technology continues providing smaller and faster transistors, so processor architects can offer more complex and functional ILP processors, because manufacturers can fit more transistors on the same chip area. As a consequence, the fraction of chip area reachable in a single clock cycle is dropping, and at the same time the number of transistors on the chip is increasing. However, problems related with power consumption and heat dissipation are worrying. This scenario is forcing processor designers to look for new processor organizations that can provide the same or more performance but using smaller sizes. This fact especially affects the on-chip cache memory design; therefore, studies proposing new smaller cache organizations while maintaining, or even increasing, the hit ratio are welcome. In this sense, the cache schemes that propose a better exploitation of data locality (bypassing schemes, prefetching techniques, victim caches, etc.) are a good example.This paper presents a data cache scheme called filter cache that splits the first level data cache into two independent organizations, and its performance is compared with two other proposals appearing in the open literature, as well as larger classical caches. To check the performance two different scenarios are considered: a superscalar processor and a symmetric multiprocessor.The obtained results show that (i) in the superscalar processor the split data caches perform similarly or better than larger conventional caches, (ii) some splitting schemes work well in multiprocessors while others work less well because of data localities, (iii) the reuse information that some split schemes incorporate for managing is also useful for designing new competitive protocols to boost performance in multiprocessors, (iv) the filter data cache achieves the best performance in both scenarios. 
51|8||A pipeline architecture for computing the Euler number of a binary image|Euler number of a binary image is a fundamental topological feature that remains invariant under translation, rotation, scaling, and rubber-sheet transformation of the image. In this work, a run-based method for computing Euler number is formulated and a new hardware implementation is described. Analysis of time complexity and performance measure is provided to demonstrate the efficiency of the method. The sequential version of the proposed algorithm requires significantly fewer number of pixel accesses compared to the existing methods and tools based on bit-quad counting or quad-tree, both for the worst case and the average case. A pipelined architecture is designed with a single adder tree to implement the algorithm on-chip by exploiting its inherent parallelism. The architecture uses O(N) 2-input gates and requires O(N log N) time to compute the Euler number of an N × N image. The same hardware, with minor modification, can be used to handle arbitrarily large pixel matrices. A standard cell based VLSI implementation of the architecture is also reported. As Euler number is a widely used parameter, the proposed design can be readily used to save computation time in many image processing applications. 
51|8||Functional test generation based on word-level SAT|Functional test generation coupled with symbolic simulation offers a good compromise between formal verification and numerical simulation for design validation. The generation of functional test vectors guided by miscellaneous coverage metrics can be posed as a satisfiability problem (SAT). While a number of efficient Boolean SAT engines have been developed for gate level designs, they are not directly applicable to behavioral and RTL designs containing significant arithmetic components. This paper presents two approaches that enhance the capability of functional test generation by preserving arithmetic operators in the design. They are based on word-level SAT techniques: (1) LPSAT, based on integer linear programming, and (2) CLP-SAT, based on constraint logic programming. The proposed SAT solvers allow to efficiently handle the designs with mixed word-level arithmetic operators and bit-level logic gates. The experimental results are quite encouraging compared to traditional CNF-based and BDD-based SAT solvers. The paper also suggests a method to build an integrated SAT solving framework where different SAT solvers work together to provide a more complete solution to functional test generation and other verification applications. 
51|8||Guide for Authors|
51|9|http://www.sciencedirect.com/science/journal/13837621/51/9|Optimal sample length for efficient cache simulation|Architectural simulations of microprocessors are extremely time-consuming nowadays due to the ever increasing complexity of current applications. In order to get realistic workloads on current hardware, benchmarks need to be constructed with huge dynamic instruction counts. For example, SPEC released the CPU2000 benchmark suite containing benchmarks that have a dynamic instruction count of several hundreds of billions of instructions. This is beneficial for real hardware evaluation. However, simulating these workloads is impractical if not impossible if we take into account that many simulation runs are needed in order to evaluate a large number of design points. Trace sampling is often used as a practical solution for this problem. In trace sampling, several representative samples are chosen from a real program trace. Since the sampled trace is much shorter than the original trace, a significant simulation speedup is obtained. In this paper, we study what is the optimal sample size to achieve a given level of accuracy while maximizing the total simulation speedup. From various experiments using SPEC CPU2000, we conclude that the optimal sample length (i) is not fixed over benchmarks, and (ii) increases with increasing warmup lengths. As such, we propose an algorithm that determines the optimal sample length per benchmark under different warmup scenarios. This is done within the context of sampled cache simulation. 
51|9||A slot swapping protocol for time-critical internetworking|This paper proposes an architecture and communication protocol called the Slot Swapping Protocol which is used to interconnect, by means of a backbone, different subsystems that generate and consume real-time data. The proposed communication architecture fits well into the general Computer Integrated Manufacturing (CIM) model, thus representing an appealing solution for FieldBus interconnection in factory automation systems.Inspired by the concept of session swapping, the Slot Swapping Protocol makes a step forward in slotted rings by including explicit time constraints in traffic scheduling, as it assigns a dynamic priority to slots and uses an Earliest Deadline First-based swapping policy. The paper describes the communication protocol, discusses three slot swapping approaches and presents a set of experiments characterizing the protocol’s ability to meet traffic deadlines in several working conditions. 
51|9||Equation-based TCP-friendly congestion control under lossy environment|A major problem in supporting multimedia streaming in the Internet is that the streaming protocol used tends to take bandwidth away from competing TCP traffic streams. Thus it is important to devise rate based protocols that are TCP-friendly. In the paper we examine the effectiveness of two important models in devising TCP-friendly rate based protocols in lossy environment such as wireless networks. Using simulation for a variety of network scenarios, it is shown that the rate based protocols based on one model underestimates while the other overestimates TCP bandwidth in the presence of errors. A modification is made to the models which allows us to construct a rate based protocol that can track TCP bandwidth better in the presence of random error and hence exhibit behavior that is more TCP-friendly. 
51|9||Guide for Authors|
volume|issue|url|title|abstract
52|-|http://www.sciencedirect.com/science/journal/13837621/52|Support for partial run-time reconfiguration of platform FPGAs|Run-time partial reconfiguration of programmable hardware devices can be applied to enhance many applications in high-end embedded systems, particularly those that employ recent platform FPGAs. The effective use of this approach is often hampered by the complexity added to the system development process and by limited tool support.The paper is concerned with several aspects related to the effective exploitation of run-time partial reconfiguration, with particular emphasis on the generation of partial configurations and the run-time utilisation of the reconfigurable resources. The paper presents an approach inspired by the traditional software development: partial configurations are produced by assembling components from a previously created library, thus enabling the embedded application developer to produce the configuration data required for run-time modifications with less effort than is needed with the conventional design flow. A tool that supports this approach is also described. A second set of issues is addressed by a run-time support library that provides facilities for managing the hardware reconfiguration process and the communication with the reconfigured circuits.The use of run-time partial reconfiguration requires a high level of system support. The paper describes one possible approach, presenting a demonstration system developed to support the present work and characterising its performance. In order to clarify the advantages of the approach to run-time reconfiguration discussed in the paper, two small case studies are described, the first on the use of dedicated datapaths for subword operations and the second on two-dimensional pattern-matching for bilevel images. Timing measurements for both cases are included. 
52|-||Evaluating the reliability of computational grids from the end userâs point of view|Reliability, in terms of Grid component fault tolerance and minimum quality of service, is an important aspect that has to be addressed to foster Grid technology adoption. Software reliability is critically important in today’s integrated and distributed systems, as is often the weak link in system performance. In general, reliability is difficult to measure, and specially in Grid environments, where evaluation methodologies are novel and controversial matters. This paper describes a straightforward procedure to analyze the reliability of computational grids from the viewpoint of an end user. The procedure is illustrated in the evaluation of a research Grid infrastructure based on Globus basic services and the GridWay meta-scheduler. The GridWay support for fault tolerance is also demonstrated in a production-level environment. Results show that GridWay is a reliable workload management tool for dynamic and faulty Grid environments. Transparently to the end user, GridWay is able to detect and recover from any of the Grid element failure, outage and saturation conditions specified by the reliability analysis procedure. 
52|-||Decentralized media streaming infrastructure (DeMSI): An adaptive and high-performance peer-to-peer content delivery network|Hosting an on-demand media content streaming service has been a challenging task mainly because of the outrageously enormous network and server bandwidth required to deliver large amount of content data to users simultaneously. We propose an infrastructure that helps online media content providers offload their server and network resources for media streaming. Using application level resource diversity together with the peer-to-peer resource-sharing model is a feasible approach to decentralize the content storage, server and network bandwidth. Each subscriber is responsible for only a small fraction of such resources. Most importantly, the cost of maintaining the service can also be shared amongst subscribers, especially when the subscriber base is large. As a result, subscribers can be benefit from lower subscription cost. There have been a few solutions out there that focused only on sharing the load of network bandwidth by division of a streaming task to be carried out by multiple sources. However, existing solutions require that the content to be replicated in full and stored in each source, which is impractical for a subscriber as the owner of the storage resource that is of consumer capacity. Our solution focuses on the division of responsibility on both the network bandwidth and content storage such that each subscriber is responsible for only a small portion of the content. We propose a light-weighted candidate peer selection strategy based on avoidance of network congestion and an adaptive re-scheduling algorithm in order to enhance smoothness of the aggregated streaming rate perceived at the consumer side. Experiments show that the performance of our peer-selection strategy out performs the traditional strategy based on end-to-end streaming bandwidth. 
52|1|http://www.sciencedirect.com/science/journal/13837621/52/1|A core generator for arithmetic cores and testing structures with a network interface|We present Eudoxus, a tool for generation of architectural variants for arithmetic soft cores and testing structures targeting a wide variety of functions, operand sizes and architectures. Eudoxus produces structural and synthesizable VHDL and/or Verilog descriptions for: (a) several arithmetic operations including addition, subtraction, multiplication, division, squaring, square rooting and shifting, and (b) several testing structures that can be used as test pattern generators and test response compactors. Interaction with the user is made through a network interface. Since the end user is presented with a variety of unencrypted structural cores, each one describing an architecture with its own area, delay and power characteristics, he can choose the one that best fits his specific needs which he can further optimize or customize. Therefore, designs utilizing these cores are completed in less time and with less effort. 
52|1||Memory latency consideration for load sharing on heterogeneous network of workstations|With the development of cheap personal computer and high-speed network, heterogeneous network of workstation has become the trend in high performance computing. This paper focuses on the load sharing problem for heterogeneous network of workstations. Load sharing means even workloads of all coordinated computers in the heterogeneous system without leaving any computer idle. When some nodes suffer from heavy loading, it is necessary to migrate some processes to the nodes with light loading. However, most load sharing policies focus only on different CPU speed and/or memory capacity without taking the effect of memory access latencies into consideration. In the paper, we propose a new load sharing policy, CPU-Memory-Power-based policy, to improve CPU-Memory-based policy. In addition to CPU speed and memory capacity, this policy also puts emphasis on memory access latency. Experimental results show that this method performs better than the other policies, and that memory access latency is actually an important consideration in the design of load sharing policies on heterogeneous network of workstation. 
52|1||Replacing media caches in streaming proxy servers|This paper presents a cache replacement policy which has specifically been developed for the efficient media caching in streaming media cache servers. For efficient media caching, the proposed policy takes into account the periodic patterns of users’ requests in addition to the parameters such as reference count, amount of media contents delivered to the clients, and reference time. These values are collected at run-time for each cached object. In order to adequately and promptly adopt to the changing characteristics of users preferences, the policy introduces, in particular, the concept of weighted-window for replacement with which higher priorities are given to more recently referenced media contents and consequently they are less likely to be replaced. We present and analyze the simulation results showing that the proposed policy has outperformed the conventional replacement policies such as LRU, LFU, and SEG in terms of hit ratio, byte-hit ratio, delayed start, and cache input. 
52|1||Intelligent memory manager: Reducing cache pollution due to memory management functions|In this work, we show that data-intensive and frequently-used service functions such as memory allocation and de-allocation entangle with application’s working set and become a major cause for cache misses. We present our technique that transfers the allocation and de-allocation functions’ executions from main CPU to a separate processor residing on chip with DRAM (Intelligent Memory Manager). The results manifested in the paper state that, 60% of the cache misses caused by the service functions are eliminated when using our technique. We believe that cache performance of applications in computer system is poor due to their indulgence for the service functions. 
52|1||Performance analysis of multistage interconnection networks with a new high-level net model|This paper introduces a new high-level net, named S-net, for modelling multistage interconnection networks. Based on the stochastic behaviour of GSPN and coupled with the flexibility and compactness of Coloured Petri net uses tokens for storage and manipulation of data besides modelling the flow of control. It requires exactly 3N/2 number of places to model an N × N multistage interconnection network irrespective of the number of stages and buddy properties of the network. A polynomial time algorithm is developed to check feasibility of mapping a permutation using the proposed S-net model of an MIN. 
52|1||Guide for Authors|
52|10|http://www.sciencedirect.com/science/journal/13837621/52/10|Bidirectional liveness analysis, or how less than half of the Alphaâs registers are used|Interprocedural data flow analyses of executable programs suffer from the conservative assumptions that need to be made because no precise control flow graph is available and because registers are spilled onto the stack. This paper discusses the exploitation of calling-conventions in executable code data flow analyses to avoid the propagation of the conservative assumptions throughout a program. Based on this exploitation, the existing backward liveness analyses are improved, and a complementary forward liveness analysis is proposed. For the SPECint2000 programs compiled for the Alpha architecture, the combined forward and improved backward analysis on average finds 62% more free registers than the existing state-of-the-art liveness analysis. With the improved analysis, we are able to show that on an average less than half of the registers of the RISC Alpha architecture are used. This contradicts the common wisdom that compilers can exploit large, uniform register files as found on RISC architectures. 
52|10||Dual-mode floating-point multiplier architectures with parallel operations|Although most modern processors have hardware support for double precision or double-extended precision floating-point multiplication, this support is inadequate for many scientific computations. This paper presents the architecture of a quadruple precision floating-point multiplier that also supports two parallel double precision multiplications. Since hardware support for quadruple precision arithmetic is expensive, a new technique is presented that requires much less hardware than a fully parallel quadruple precision multiplier. With this architecture, quadruple precision multiplication has a latency of three cycles and two parallel double precision multiplications have latencies of only two cycles. The multiplier is pipelined so that two double precision multiplications can begin every cycle or a quadruple precision multiplication can begin every other cycle. The technique used for the dual-mode quadruple precision multiplier is also applied to the design of a dual-mode double precision floating-point multiplier that performs a double precision multiplication or two single precision multiplications in parallel. Synthesis results show that the dual-mode double precision multiplier requires 43% less area than a conventional double precision multiplier. The correctness of all the multipliers presented in this paper is tested and verified through extensive simulation. 
52|10||A schema version model for complex objects in object-oriented databases|In this paper, we propose a schema version model which allows to restructure complex object hierarchy in object-oriented databases. This model extends a schema version model, called RiBS, which is based on the concept of Rich Base Schema. In the RiBS model, each schema version is in the form of updatable class hierarchy view over one base schema, called the RiBS layer, which has richer schema information than any existing schema version in the database.In this paper, we introduce new operations for restructuring composite object hierarchy in schema versions, and explain their semantics. We also touch upon the ways to transform queries posed against a restructured composite object hierarchy into one against the base schema. In addition, we identify several types of conflicts during schema version merging which result from the restructuring operations, and provide a semi-automatic algorithm to resolve the conflicts. The originality of this paper lies in that (1) we introduce several new operations to restructure composite object hierarchy, and (2) this extended RiBS model operations raise the concept of data independence in OODBs upto the schema level. 
52|10||An XML data allocation method on disks|XML recently has expanded its application areas: data formats in various information systems, communication protocols in distributed systems, and so on. Generally, XML data can be logically modeled as rooted tree. For the query processing of such data, path queries are widely used. In this paper, we present an optimal algorithm that places XML data on disks such that the number of disk accesses for path query processing is minimized. The proposed algorithm consists of two steps. First, we assign a number (called the mapping indicator) for each node of a tree in a bottom–up fashion, and in the next step we map the nodes to disk blocks using the assigned number. We analyze the optimality of the proposed method with some relevant proofs. We also show the proposed method provides good performance for various query types with XML data set. 
52|10||A scalable VLSI speed/area tunable sorting network|This work presents a novel sorting network based on the “sorting by counting” algorithm. The proposed implementation of the algorithm is very regular. Further, its realization depends on a design parameter, that permits different tradeoffs between speed and area to be chosen. For example, we can fix this parameter to obtain a feasible SN with n inputs and O(log(n)) elaboration time with a reasonable multiplicative constant. Comparisons with previous works show that under some metrics for a wide range of values of n we obtain the best results. 
52|10||Dynamic reuse of subroutine results|The paper discusses a concept of dynamic reuse of subroutine results. The described technique uses a hardware mechanism that caches the address of the called subroutine along with its arguments and returned value. When the same subroutine is called again using the same arguments, the returned value can be accurately predicted without an actual execution of the subroutine. Although this approach can be highly effective in some cases, it is limited to subroutines that do not use side effects, and use only by-value parameter passing. Since the proposed method requires that both the user and the compiler be aware of this mechanism, it might be more appropriate for specific computing-intensive applications, rather than standard all-purpose programming. 
52|11|http://www.sciencedirect.com/science/journal/13837621/52/11|Preface|
52|11||Modeling and simulation of open source development using an agile practice|The goal of this work is to study the effects of the adoption of agile practices on open source development. In particular, we started to evaluate the effects of TDD (Test Driven Development) since it is easer to apply in a distributed environment than most other agile practices. In order to reach this goal we used the simulation modeling approach. We developed a simulation model of open source software development process. The model was tuned using data from a real FLOSS project: Apache HTTP Server. To introduce the TDD practice in our FLOSS simulation model, we made some assumptions based on empirical results. The two FLOSS development models (nonTDD and TDD) were compared. The one incorporating the agile practice yields better results in terms of code quality. 
52|11||Integrating XP project management in development environments|Extreme Programming (XP) is an Agile Methodology (AM) which does not require any specific supporting tool for being successfully applied. Despite this starting observation, there are many reasons leading a XP team to adopt Web based tools to support XP practices. For example, such tools could be useful for process and product data collection and analysis or for supporting distributed development. In this article, we describe XPSuite, a tool composed of two parts: XPSwiki, a tool for managing XP projects and XP4IDE, a plug-in for integrating XPSwiki with an Integrated Development Environment (IDE). Moreover, we will show how the full Object Oriented implementation provides a powerful support for extracting all data represented in the model that the system implements. 
52|11||FMESP: Framework for the modeling and evaluation of software processes|Nowadays, organizations face with a very high competitiveness and for this reason they have to continuously improve their processes. Two key aspects to be considered in the software processes management in order to promote their improvement are their effective modeling and evaluation. The integrated management of these key aspects is not a trivial task, the huge number and diversity of elements to take into account makes it complex the management of software processes. To ease and effectively support this management, in this paper we propose FMESP: a framework for the integrated management of the modeling and measurement of software processes. FMESP incorporates the conceptual and technological elements necessary to ease the integrated management of the definition and evaluation of software processes. From the measurement perspective of the framework and in order to provide the support for the software process measurement at model level a set of representative measures have been defined and validated. 
52|11||Ontology-based multi-site software development methodology and tools|The disadvantages associated with remote communication rather than face-to-face communication is a key problem in the multi-site distributed software development environment. Awareness of what work has been done, what task has been misunderstood, what problems have been raised, what issues have been clarified, and understanding of why a team or a software engineer does not follow the project plan, and how to carry out a discussion over a multi-site distributed environment and to make a just-in-time decision are the challenge. Different teams might not be aware of what tasks are being carried out by others, potentially leading to problems such as two groups overlapping in some work or other work not being performed due to misinterpretation of the task. Wrong tasks may be carried out due to ignorance of who to contact to get the proper details. If everyone working on a certain project is located in the same area, then situational awareness is relatively straightforward but the overheads in communications to get together to discuss the problems, to raise issues, to make decisions and to find answers in a multi-site distributed environment can become very large. Consequently, these problems cause project delay and anxiety among teams and managers. Ontologies coupled with a multi-agents system allow greater ease of communication by aggregating the agreed knowledge about the project, the domain knowledge, the concepts of software engineering into a shared information resource platform and allow them to be shared among the distributed teams across the sites and enable the intelligent agents to use the ontology to carry out initial communication and classification with developers when the problem is raised in the first instance. In this paper, we present the key challenges in multi-site software engineering and the ontology representation of commonly shared conceptualisations in software development. We demonstrate the agent communication with developers in the form of man–machine interactions and the great potential of such a system to be used in the future for software engineering in multi-site environments. 
52|11||Motivations and measurements in an agile case study|With the recent emergence of agile software development technologies, the software community is awaiting sound, empirical investigation of the impacts of agile practices in a live setting. One means of conducting such research is through industrial case studies. There are a number of influencing factors that contribute to the success of such a case study. In this paper, we describe a case study performed at Sabre Airline SolutionsTM evaluating the effects of adopting Extreme Programming (XP) practices with a team that had characteristically plan-driven risk factors. We compare the team’s business-related results (productivity and quality) to two published sources of industry averages. Our case study found that the Sabre team yielded above-average post-release quality and average to above-average productivity. We discuss our experience in conducting this case study, including specifics of how data was collected, the rationale behind our process of data collection, and what obstacles were encountered during the case study. We identify four factors that potentially impact the outcome of industrial case studies: availability of data, tool support, cooperative personnel and project status. Recognizing and planning for these factors is essential to conducting industrial case studies. 
52|11||A non-invasive approach to product metrics collection|Software metrics are useful means in helping software engineers to develop large and complex software systems. In the past years, many software metrics have been proposed in order to represent several different concepts such as complexity, coupling, inheritance, reuse, etc. However, this requires the collection of large volumes of metrics and, without flexible and transparent tools, is nearly impossible to collect data accurately. This paper presents the design and the implementation of a tool for collecting and analyzing product metrics in a non-invasive way. 
52|11||Managing non-invasive measurement tools|Software measures are of paramount importance to evaluate both processes and products. However, collecting such data requires a large effort and this activity produces benefits only in the long term. For these reasons, companies have to use automated tools able to collect data without affecting both the productivity and the process. This approach is useful to reduce the effort related to the metrics collection but managing the different tools required for a comprehensive acquisition is not an easy task and it requires a considerable effort. This paper presents some tools and techniques for managing large metrics collection projects based on the usage of the PROM metrics collection tool. 
52|11||Discovering the software process by means of stochastic workflow analysis|A fundamental feature of the software process consists in its own stochastic nature. A convenient approach for extracting the stochastic dynamics of a process from log data is that of modelling the process as a Markov model: in this way the discovery of the short/medium range dynamics of the process is cast in terms of the learning of Markov models of different orders, i.e. in terms of learning the corresponding transition matrices. In this paper we show that the use of a full Bayesian approach in the learning process helps providing robustness against statistical noise and over-fitting, as the size of a transition matrix grows exponentially with the order of the model. We give a specific model–model similarity definition and the corresponding calculation procedure to be used in model-to-sequence or sequence-to-sequence conformance assessment, this similarity definition could also be applied to other inferential tasks, such as unsupervised process learning. 
52|11||Using a role scheme to derive software project metrics|Roles’ playing is common in our lives. We play different roles with our family, at work as well as in other environments. Role allocation in software development projects is also accepted though it may be implemented differently by different software development methods. In a previous work [Y. Dubinsky, O. Hazzan, Roles in agile software development teams, in: 5th International Conference on Extreme Programming and Agile Processes in Software Engineering, 2004, pp. 157–165] we have found that personal roles may raise teammates’ personal accountability while maintaining the essence of the software development method. In this paper we present our role scheme, elaborate on its implementation and explain how it can be used to derive metrics. We illustrate our ideas by data gathered in student projects in the university. 
52|11||Quantitative logic-based framework for agile methodologies|Agile methodologies and Extreme programming are the new and highly promising endeavors in Software Engineering. By addressing the important issue of dealing with continuously changing requirements we are faced with panoply of new problems and a genuine need to revisit some principles and classic models of software developments. When it comes to the management of software projects and a way in which we are looking at the software processes and underlying practices, it becomes apparent that in the management practices the issue of uncertainty needs to be quantified and fully addressed. Similarly, it becomes of interest to develop lightweight models of software quality and software processes that are easy to construct and modify as well are transparent to the developer and manager. Given these arguments, in this study we propose logic models based upon the mechanisms of multivalued and fuzzy logic. The realization of such models gives rise to so-called logic networks that are easy to construct, calibrate and interpret. 
52|11||Guide for Authors|
52|2|http://www.sciencedirect.com/science/journal/13837621/52/2|Parallel, distributed and network-based processing|
52|2||An abstraction model for a Grid execution framework|Computational Grids have been identified as one of the paradigms revolutionizing the discipline of distributed computing. The contributions within the Grid community have resulted in new Grid technologies and continuous improvements to Grid standards and protocols. Though crucial to the success of the Grid approach, such an incremental evolution of Grid standards has become a primary cause of frustration for scientific and commercial application communities aspiring to adopt the Grid paradigm. Motivated by our rich experience and the need to decouple the application development and the Grid technology development processes, we propose an abstraction-based Grid middleware layer as part of the Java CoG Kit. In this paper, we showcase our abstraction model and verify its extensibility by integrating it with an advanced quality-of-service-based execution framework. 
52|2||Scheduling tasks sharing files on heterogeneous masterâslave platforms|This paper is devoted to scheduling a large collection of independent tasks onto heterogeneous clusters. The tasks depend upon (input) files which initially reside on a master processor. A given file may well be shared by several tasks. The role of the master is to distribute the files to the processors, so that they can execute the tasks. The objective for the master is to select which file to send to which slave, and in which order, so as to minimize the total execution time. The contribution of this paper is twofold. On the theoretical side, we establish complexity results that assess the difficulty of the problem. On the practical side, we design several new heuristics, which are shown to perform as efficiently as the best heuristics in [H. Casanova, A. Legrand, D. Zagorodnov, F. Berman, Heuristics for scheduling parameter sweep applications in Grid environments, in: Ninth Heterogeneous Computing Workshop, IEEE Computer Society Press, Silver Spring, MD, 2000, pp. 349–363; H. Casanova, A. Legrand, D. Zagorodnov, F. Berman, Using simulation to evaluate scheduling heuristics for a class of applications in Grid environments, Research Report RR-1999-46, LIP, ENS Lyon, France, 1999] although their cost is an order of magnitude lower. 
52|2||The masterâslave paradigm on heterogeneous systems: A dynamic programming approach for the optimal mapping|We study the master–slave paradigm over heterogeneous systems. According to an analytical model, we develop a dynamic programming algorithm that allows to solve the optimal mapping for such paradigm. Our proposal considers heterogeneity due both to computation and also to communication. The optimization strategy used allows to obtain the set of processors for an optimal computation. The computational results show that considering heterogeneity also on the communication increases the performance of the parallel algorithm. 
52|2||RDMA control support for fine-grain parallel computations|The paper concerns parallel computations with communication based on Remote Direct Memory Access (RDMA), which provides for low level un-buffered access to distributed memory of computational nodes. Fine grain computation involves very frequent transmissions of small messages. For their efficient execution with RDMA communication a special memory infrastructure—rotating buffers (RB)—is proposed. Their organization is adjusted to program needs in advance—before program execution. It allows intensive use of all communication resources available in the system based on additional synchronization between involved processes. The proposed method is illustrated by an example of a typical fine-grain problem, which is the discrete Fast Fourier Transform (FFT). “The Transpose Algorithm” of FFT has been implemented with the RDMA rotating buffers and its efficiency is compared with a solution based on standard message passing library MPI. 
52|2||Optimizing bus energy consumption of on-chip multiprocessors using frequent values|Chip multiprocessors (CMP) are a convenient way of leveraging from the technological trends to build high-end and embedded systems that are performance and power efficient, while exhibiting attractive properties such as scalability, reliability and ease of design. However, the on-chip interconnect for moving the data between the processors, and between the processors and memory subsystem, plays a crucial role in CMP design. This paper presents a novel approach to optimizing its power by exploiting the value locality in data transfers between processors. A communicating value cache (CVC) is proposed to reduce the number of bits transferred on the interconnect, and simulation results with several parallel applications show significant energy savings with this mechanism. Results show that the importance of our proposal will become even more significant in the future. 
52|3|http://www.sciencedirect.com/science/journal/13837621/52/3|Improved composite confidence mechanisms for a perceptron branch predictor|In 2001, Jiménez and Lin [Dynamic branch prediction with perceptrons, Proceedings of the 7th International Symposium on High Performance Computer Architecture, 2001, pp. 197–206] introduced the perceptron branch predictor, the first dynamic branch predictor to successfully use neural networks. This simple neural network achieves higher accuracies (95% at a 4 KiB hardware budget) compared to other existing branch predictors and provides a free confidence level. In this paper, we first gain insight into this inherent confidence mechanism of the perceptron predictor and explain why (additional) counter based confidence strategies can complement it. Second, we evaluate several composite confidence estimation strategies and compare them to the described technique by Jiménez and Lin [Composite confidence estimators for enhanced speculation control, Tech. rep., Department of Computer Sciences, The University of Texas at Austin, 2002]. We conclude that our overruling AND-combination of perceptron confidence and resetting counter mechanism outperforms the previously proposed confidence scheme. 
52|3||Circulating shared-registers for multiprocessor systems|The techniques for fine-grained data sharing are generally available only on specialized architectures, usually involving a shared-bus. The CIRculating Common-Update Sharing (CIRCUS) mechanism has low latency user-level contention-free access to a set of shared circulating data registers. The local access latency is near zero for both read and write operations. These operations can be mapped into more complex operations, such as arithmetic, logical, or data reduction operations such as minimum or sum to be performed by the circulating register hardware (CRH) on the circulating copy of a register. The CRH can also be used to perform atomic operations, such as fetch&add or swap. For a two-dimensional hierarchy of N processing elements (PEs), the write-latency (until the circulating register is updated with a new value) and the update-latency (when all CRH modules can see the updated value) have an optimum cluster size proportional to (N · I/D)1/2, where I is the intercluster time and D is the inter-PE time, including the time between and through one node. The latencies, when optimally clustered, are proportional to (N · I · D)1/2. Sub-microsecond write-latency is expected for up to 15,255 PEs or 660 workstations. For higher levels of hierarchy, the expected write-latency is shown to be proportional to the sum of the latencies of all loop hierarchies. CIRCUS is applicable to a wide variety of system architectures and topologies. 
52|3||On-line evolvable fuzzy system for ATM cell-scheduling|The algorithms for solving ATM cell-scheduling problem include first-in-first-out (FIFO), static priority (SPR), dynamically weighted priority scheduling (DWPS) as well as other traditional schemes. However, these traditional algorithms lack flexibility. FIFO and SPR cannot adapt to changes in the cell flow environment. DWPS on the other hand, is more adaptable to changing traffic flow. But if the cell flow changes dramatically, the performance of this method is also not very good. In order to address these issues, we propose the framework of evolvable fuzzy system (EFS). The system is intrinsically evolvable and able to carry out on-line adaptation to meet the desired QoS requirement. The EFS is realizable as a form of evolvable fuzzy hardware (EFH) by means of a reconfigurable fuzzy inference chip (RFIC). With an implementation of the EFS as EFH which carries out intrinsic evolution and on-line adaptation, some open issues pertinent to evolvable hardware (EHW) can be addressed. 
52|3||Class-based request control system in multimedia contents service|This paper proposed the system and the algorithm for controlling clients’ service requests using the CRM of multimedia contents service through client segmentation, based on a site’s revenues. Thus, clients with higher profitability will have more chances of accessing media server resources. Likewise, controlling the service requests of all clients will enable controlling the use of network resources. To realize the system, K-Means Method was applied to the analysis of the clients’ data as the first step. The clients were classified into four classes (First Class, Second Class, Third Class and Fourth Class). Two experiments were conducted to evaluate the performance of the system. The first experiment is the comparison of the number of admitted streams of proposed algorithm with other algorithm. In the second experiment, the renewal rates in the two cases were compared. Specifically, the renewal rates of the group of clients with expired contracts where the new algorithm was applied and those of the control group were evaluated. The renewal rate was higher in the group where the new algorithm was applied, suggesting an improvement in the satisfaction level of clients and revenues. 
52|3||Guide for Authors|
52|4|http://www.sciencedirect.com/science/journal/13837621/52/4|Pattern-driven prefetching for multimedia applications on embedded processors|Multimedia applications in general and video processing, such as the MPEG4 Visual stream decoders, in particular are increasingly popular and important workloads for future embedded systems. Due to the high computational requirements, the need for low power, high performance embedded processors for multimedia applications is growing very fast. This paper proposes a new data prefetch mechanism called pattern-driven prefetching. PDP inspects the sequence of data cache misses and detects recurring patterns within that sequence. The patterns that are observed are based on the notions of the inter-miss stride (memory address stride between two misses) and the inter-miss interval (number of cycles between two misses). According to the patterns being detected, PDP initiates prefetch actions to anticipate future accesses and hide memory access latencies. PDP includes a simple yet effective stop criterion to avoid cache pollution and to reduce the number of additional memory accesses. The additional hardware needed for PDP is very limited making it an effective prefetch mechanism for embedded systems. In our experimental setup, we use cycle-level power/performance simulations of the MPEG4 Visual stream decoders from the MoMuSys reference software with various video streams. Our results show that PDP increases performance by as much as 45%, 24% and 10% for 2KB, 4KB and 8KB data caches, respectively, while the increase in external memory accesses remains under 0.6%. In conjunction with these performance increases, system-level (on-chip plus off-chip) energy reductions of 20%, 11.5% and 8% are obtained for 2KB, 4KB and 8KB data caches, respectively. In addition, we report significant speedups (up to 160%) for various other multimedia applications. Finally, we also show that PDP outperforms stream buffers. 
52|4||Dynamic feature selection for hardware prediction|Most hardware predictors are table based (e.g. two-level branch predictors) and have exponential size growth in the number of input bits or features (e.g. previous branch outcomes). This growth severely limits the amount of predictive information that such predictors can use. To avoid exponential growth we introduce the idea of “dynamic feature selection” for building hardware predictors that can use a large amount of predictive information. Based on this idea, we design the dynamic decision tree (DDT) predictor, which exhibits only linear size growth in the number of features. Our initial evaluation, in branch prediction, shows that the general-purpose DDT, using only branch-history features, is comparable on average to conventional branch predictors, opening the door to practically using large numbers of additional features. 
52|4||The algorithm of pipelined gossiping|A family of gossiping algorithms depending on a parameter permutation is introduced, formalized, and discussed. Several of its members are analyzed and their asymptotic behaviour is revealed, including a member whose model and performance closely follows the one of hardware pipelined processors. This similarity is exposed. An optimizing algorithm is finally proposed and discussed as a general strategy to increase the performance of the base algorithms. 
52|4||Guide for Authors|
52|5|http://www.sciencedirect.com/science/journal/13837621/52/5|Software-based self-testing of microprocessors|Hardware-based self-testing techniques have limitations in the performance and area overhead. Those can be eliminated using software-based self-testing. In this paper, we investigate capabilities of the microprocessor testing by software procedures taking into account system environment constraints. Special attention is paid to microarchitectural features of pipelined and superscalar processors. New test strategies are proposed combining deterministic and pseudorandom approaches supported by the available hardware mechanisms (test registers, on-chip monitoring circuitry, etc.), which improve testability features. The test effectiveness is studied using various test coverage measures (stimuli, circuit stressing), statistical and fault injection tools. To demonstrate the utility of the proposed methodology, it has been applied to commercial microprocessors and experimental results are presented in this paper. 
52|5||Design of a compact reversible binary coded decimal adder circuit|Reversible logic is an emerging research area and getting remarkable interests over the past few years. Interest is sparked in reversible logic by its applications in several technologies, such as quantum, optical, thermodynamics and adiabatic CMOS. This paper represents a synthesis method to realize reversible binary coded decimal adder circuit. Firstly, a reversible full-adder circuit has been proposed that shows the improvement over the two existing circuits. A lower bound is also proposed for the reversible full-adder circuit on the number of garbage outputs (bits needed for reversibility, but not required for the output of the circuit). After that, a final improvement is presented for the reversible full-adder circuit. Finally, a new reversible circuit has been proposed, namely reversible binary coded decimal (BCD) adder, which is the first ever proposed in reversible logic synthesis. In the way to propose reversible BCD adder, a reversible n-bits parallel adder circuit is also shown. Lower bounds for the reversible BCD adder in terms of number of garbage outputs and number of reversible gates are also shown. Delay has also been calculated for each circuit. 
52|5||The design and utility of the ML-RSIM system simulator|Execution-driven simulation has become the primary method for evaluating architectural techniques as it facilitates rapid design space exploration without the cost of building prototype hardware. To date, most simulation systems have either focused on the cycle-accurate modeling of user-level code while ignoring operating system and I/O effects, or have modeled complete systems while abstracting away many cycle-accurate timing details. The ML-RSIM simulation system presented here combines detailed hardware models with the ability to simulate user-level as well as operating system activity, making it particularly suitable for exploring the interaction of applications with the operating system and I/O activity. This paper provides an overview of the design of the simulation infrastructure and discusses its strengths and weaknesses in terms of accuracy, flexibility, and performance. A validation study using LMBench microbenchmarks shows a good correlation for most of the architectural characteristics, while operating system effects show a larger variability. By quantifying the accuracy of the simulation tool in various areas, the validation effort not only helps gauge the validity of simulation results but also allows users to assess the suitability of the tool for a particular purpose. 
52|5||An optimal message routing algorithm for circulant networks|
52|5||A power-efficient TCAM architecture for network forwarding tables|Stringent memory access and search speed requirements are two of the main bottlenecks in wire speed processing. Most viable search engines are implemented in content addressable memories (CAMs). CAMs have high operational speed advantage over other memory search algorithms. However, this performance advantage comes with a price of higher silicon area, and higher power consumption. Ternary CAMs (TCAM) are widely used for route lookup operations in networking applications. IP address prefix length distribution in the core routers shows a similar characteristic such that the prefixes with 24 or longer bits attract more than 50% of the traffic. Based on this statistical observation, we propose a TCAM architecture that can be used on top of the previously reported power saving techniques and it offers additional 30% reduction in power consumption. Furthermore, we model the dynamic power consumption in TCAM circuits due match, mismatch and don’t care activities. 
52|5||Guide for Authors|
52|6|http://www.sciencedirect.com/science/journal/13837621/52/6|MEMPHIS: A mobile agent-based system for enabling acquisition of multilingual content and providing flexible format internet premium services|Mobile Agent technology is an interesting concept for large network-based system structures. In this paper we describe a Java-based agent environment which integrates agent execution platforms into World Wide Web servers promoting a world wide infrastructure for Mobile Agents. This system has been developed within the framework of MEMPHIS IST project, which involves the definition, development and validation of a new system for commercially significant multilingual premium services, focused on thin clients such as mobile phones, Personal Digital Assistants, etc. The concept relies on the extraction of multilingual information from various web-based content sources in different formats. The system is composed by three different Modules, namely the Acquisition, the Transformation and the Distribution Module. The content acquisition is based on dynamic user profiles. The acquired content is transformed into a meta-representation, which enables content storage independent both of source and destination format and language. The output format is based on the requirements of employed thin client terminals and the output document is distributed via the appropriate networks. The end-user receives the required information as premium services via the chosen output medium. The system supports both pull and push services. An evaluation of the system is also presented. 
52|6||High-frequency pulse width modulation implementation using FPGA and CPLD ICs|Pulse width modulation (PWM) has been widely used in power converter control. Most high power level converters operate at switching frequencies up to 500 kHz, while operating frequencies in excess of 1 MHz at high power levels can be achieved using the planar transformer technology. The contribution of this paper is the development of a high-frequency PWM generator architecture for power converter control using FPGA and CPLD ICs. The resulting PWM frequency depends on the target FPGA or CPLD device speed grade and the duty cycle resolution requirements. The post-layout timing simulation results are presented, showing that PWM frequencies up to 3.985 MHz can be produced with a duty cycle resolution of 1.56%. Additionally, experimental results are also presented for low cost functional verification of the proposed architecture. 
52|6||High-performance adaptive routing for networks with arbitrary topology|A strategy to implement adaptive routing in irregular networks is presented and analyzed in this work. A simple and widely applicable deadlock avoidance method, applied to a ring embedded in the network topology, constitutes the basis of this high-performance packet switching. This adaptive router improves the network capabilities by allocating more resources to the fastest and most used virtual network, thus narrowing the performance gap with regular topologies. A thorough simulation process, which obtains statistically reliable measurements of irregular network behavior, has been carried out to evaluate it and compare with other state-of-the-art techniques. In all the experiments, our router exhibited the best behavior in terms of maximum/sustained performance and sensitivity to the network topology. 
52|6||Random early detection with flow number estimation and queue length feedback control|We evaluate effects of parameters in queue management on performance using 2k factorial designs. Among these systematic and environmental parameters, number of flows and Pmax are two dominant factors affecting RED performance. Then, we propose a novel active queue management scheme, NRED, employing flow number estimation and queue length feedback control, which are motivated from Bloom filter and control theory, respectively. By estimation of number of active flows, NRED is more scalable than FRED which employs per-active-flow accounting. Furthermore, fluctuation of queue length under NRED is smaller than BLUE, REM, SRED and DRED. Especially, NRED can stabilize queue length very fast even when a lot of flows suddenly crowd into the network. NRED is suitable to be deployed in today’s networks where hot spot frequently occurs. 
52|6||Guide for Authors|
52|7|http://www.sciencedirect.com/science/journal/13837621/52/7|Deferred locking with shadow transaction for clientâserver DBMSs|Data-shipping systems that allow inter-transaction caching raise the need of a transactional cache consistency maintenance (CCM) protocol because each client is able to cache a portion of the database dynamically. Deferred locking (DL) is a new CCM scheme that is capable of reducing the communication overhead required for cache consistency checking. Due to its low communication overhead, DL could show a superior performance, but it tends to exhibit a high ratio of transaction abort. To cope with this drawback, we develop a new notion of shadow transaction, which is a backup-purpose one that is kept ready to replace an aborted transaction. This notion and the locking mechanism of DL have been incorporated into deferred locking with shadow transaction. Using a distributed database simulation model, we evaluate the performance of the proposed schemes under a wide range of workloads. 
52|7||Analytical modeling of codes with arbitrary data-dependent conditional structures|Several analytical models that predict the memory hierarchy behavior of codes with regular access patterns have been developed. These models help understand this behavior and they can be used successfully to guide compilers in the application of locality-related optimizations requiring small computing times. Still, these models suffer from many limitations. The most important of them is their restricted scope of applicability, since real codes exhibit many access patterns they cannot model. The most common source of such kind of accesses is the presence of irregular access patterns because of the presence of either data-dependent conditionals or indirections in the code. This paper extends the Probabilistic Miss Equations (PME) model to be able to cope with codes that include data-dependent conditional structures too. This approach is systematic enough to enable the automatic implementation of the extended model in a compiler framework. Validations show a good degree of accuracy in the predictions despite the irregularity of the access patterns. This opens the possibility of using our model to guide compiler optimizations for this kind of codes. 
52|7||Speedup of NULL convention digital circuits using NULL cycle reduction|A NULL Cycle Reduction (NCR) technique is developed to increase the throughput of NULL Convention Logic systems, by reducing the time required to flush complete DATA wavefronts, commonly referred to as the NULL cycle. The NCR technique exploits parallelism by partitioning input wavefronts, such that one circuit processes a DATA wavefront, while its duplicate processes a NULL wavefront. A NCR architecture is developed for both dual-rail and quad-rail circuits, using either full-word or bit-wise completion. To illustrate the technique, NCR is applied to case studies of a dual-rail non-pipelined 4-bit × 4-bit unsigned multiplier using full-word completion, a quad-rail non-pipelined 4-bit × 4-bit unsigned multiplier using full-word completion, and a dual-rail optimally-pipelined 4-bit × 4-bit unsigned multiplier using bit-wise completion. The application of NCR yields a speedup of 1.57, 1.55, and 1.34, respectively, over the standalone versions, while maintaining delay-insensitivity. Furthermore, NCR is applied to a single slow stage of two pipelined designs to boost the pipelines’ overall throughput by 20% and 26%, respectively. 
52|7||An analytical model for hypercubes in the presence of multiple time-scale bursty traffic|The interconnection network is one of the most critical architectural components in a parallel computer as any interaction between individual processors ultimately depends on its effectiveness. Recent studies have shown that traffic workloads in many parallel computation environments reveal burstiness over a number of time-scales, which has a considerable impact on network performance. Analytical modelling plays an important role in obtaining a clear understanding of the performance of interconnection networks with various design spaces and operating under different traffic conditions. This paper proposes a concise analytical model for the hypercube operating under multiple time-scale bursty traffic. The validity of the model is demonstrated by comparing analytical results to those obtained through simulation experiments of the actual system. The analytical model is then applied to evaluate the impact of multiple time-scale bursty traffic on the performance of hypercubes and investigate the effects of Hurst parameters of the traffic. 
52|7||Distributed computing using Java: A comparison of two server designs|This paper proposes a new concurrent data structure, called parallel hash table, for synchronizing the access of multiple threads to resources stored in a shared buffer. We prove theoretically the complexity of the operations and the upper limit on the thread conflict probability of the parallel hash table. To empirically evaluate the proposed concurrent data structure, we compare the performance of a TCP multi-threaded parallel hash table-based server to a conventional TCP multi-threaded shared buffer-based server implemented in Java. The experimental results on a network of 36 workstations running Windows NT, demonstrate that the parallel hash table-based server outperforms the conventional multi-threaded server. 
52|7||Guide for Authors|
52|8-9|http://www.sciencedirect.com/science/journal/13837621/52/8-9|Special issue on nature-inspired applications and systems|
52|8-9||Case studies for self-organization in computer science|Self-organization is bound to greatly affect computer science. The simplicity and yet power of self-organized models will allow researchers to propose efficient solutions to problems never before thought possible to be addressed efficiently. The published works in the field clearly demonstrate the potential of this approach. This paper first reviews a number of interesting self-organization phenomena found in nature, then it discusses their potential applicability in several computer science application scenarios. 
52|8-9||A comprehensive review of nature inspired routing algorithms for fixed telecommunication networks|The major contribution of the paper is a comprehensive survey of existing state-of-the-art Nature inspired routing protocols for fixed telecommunication networks developed by researchers who are trained in novel and different design doctrines and practices. Nature inspired routing protocols have been becoming the focus of research because they achieve the complex task of routing through simple agents which traverse the network and collect the routing information in an asynchronous fashion. Each node in the network has a limited information about the state of the network, and it routes data packets to their destination based on this local information. The agent-based routing algorithms provide adaptive and efficient utilization of network resources in response to changes in the network catering for load balancing and fault management. The paper describes the important features of stigmergic routing algorithms, evolutionary routing algorithms and artificial intelligence routing algorithms for fixed telecommunication networks. We also provide a summary of the protocols developed by the networking community. We believe that the survey will be instrumental in bridging the gap among different communities involved in research of telecommunication networks. 
52|8-9||ANSI: A swarm intelligence-based unicast routing protocol for hybrid ad hoc networks|We present a hybrid routing protocol for both pure and hybrid ad hoc networks which uses the mechanisms of swarm intelligence to select next hops. Our protocol, Ad hoc Networking with Swarm Intelligence (ANSI), is a congestion-aware routing protocol, which, owing to the self-organizing mechanisms of swarm intelligence, is able to collect more information about the local network and make more effective routing decisions than traditional MANET protocols. Once routes are found, ANSI maintains routes along a path from source to destination effectively by using swarm intelligence techniques, and is able to gauge the slow deterioration of a link and restore a path along newer links as and when necessary. ANSI is thus more responsive to topological fluctuations. ANSI is designed to work over hybrid ad hoc networks: ad hoc networks which consist of both lower-capability, mobile wireless devices and higher-capability, wireless devices which may or may not be mobile. In addition, ANSI works with multiple interfaces and with both wired and wireless interfaces.Our simulation study compared ANSI with AODV on both hybrid and pure ad hoc network scenarios using both TCP and UDP data flows. The results show that ANSI is able to achieve better results (in terms of packet delivery, number of packets sent, end-to-end delay, and jitter) as compared to AODV in most simulation scenarios. In addition, ANSI achieves this performance with fewer route errors as compared to AODV. Lastly, ANSI is able to perform more consistently, considering the lower variation (measured as the width of the confidence intervals) of the observed values in the results of the experiments. We show that ANSI’s performance is aided by both its superior handling of routing information and also its congestion awareness properties, though we see that congestion awareness in ANSI comes at a price. 
52|8-9||A flocking based algorithm for document clustering analysis|Social animals or insects in nature often exhibit a form of emergent collective behavior known as flocking. In this paper, we present a novel Flocking based approach for document clustering analysis. Our Flocking clustering algorithm uses stochastic and heuristic principles discovered from observing bird flocks or fish schools. Unlike other partition clustering algorithm such as K-means, the Flocking based algorithm does not require initial partitional seeds. The algorithm generates a clustering of a given set of data through the embedding of the high-dimensional data items on a two-dimensional grid for easy clustering result retrieval and visualization. Inspired by the self-organized behavior of bird flocks, we represent each document object with a flock boid. The simple local rules followed by each flock boid result in the entire document flock generating complex global behaviors, which eventually result in a clustering of the documents. We evaluate the efficiency of our algorithm with both a synthetic dataset and a real document collection that includes 100 news articles collected from the Internet. Our results show that the Flocking clustering algorithm achieves better performance compared to the K-means and the Ant clustering algorithm for real document clustering. 
52|8-9||Evolving classifiers on field programmable gate arrays: Migrating XCS to FPGAs|The paper presents the first results of the prototype implementation of the eXtended learning Classifier System (XCS) in hardware and precisely on Field Programmable Gate Arrays. For this purpose we introduce a version of the XCS classifier system completely based on integer arithmetic, that we name XCSi, instead of the usual floating point one, to exploit the peculiarities and overcome the limitations of the hardware platform. We present an analysis of XCSi performance and the guidelines for a hardware implementation, showing that, although there is a dramatic reduction of available precision, the integer version of XCS can reach optimal performance in all problems considered, though it often converges more slowly than the original floating point version. Guidelines for a hardware implementation are provided, by analyzing how XCSi functional components can be designed on an FPGA. 
52|8-9||Guide for Authors|
volume|issue|url|title|abstract
53|-|http://www.sciencedirect.com/science/journal/13837621/53|STAFF: A flash driver algorithm minimizing block erasures|Recently, flash memory is widely used in embedded applications since it has strong points: non-volatility, fast access speed, shock resistance, and low power consumption. However, due to its hardware characteristics, it requires a software layer called flash translation layer (FTL). The main functionality of FTL is to convert logical addresses from the host to physical addresses of flash memory. We propose a new FTL algorithm called state transition applied fast flash translation layer (STAFF). Compared to the previous FTL algorithms, STAFF shows higher performance and requires less memory. We provide performance results based on our implementation of STAFF and previous FTL algorithms. 
53|-||Enhanced fault tolerant routing algorithms using a concept of âbalanced ringâ|Fault rings can be used to guide messages bypass faulty nodes/links in a fault tolerant interconnection network. However, nodes on the fault ring become hot spots, thus causing uneven distribution of the traffic loads. To avoid such traffic congestion, a concept of the balanced ring is proposed in this paper. The proposed balanced ring, defined as concentric rings of a given fault ring, can be applied to the fault tolerant routing algorithms for mesh and torus topologies. By properly guiding messages to route on the balanced ring and the fault ring, more balanced link utilization and greatly reduced traffic congestion can be achieved on a fault tolerant network. Methods of applying the balanced ring concept to some published fault tolerant routing algorithms are discussed. Proof of deadlock and livelock freedom is also presented. The use of balanced ring does not need to add new virtual channels. The performance of two routing algorithms with and without the balanced ring is simulated and evaluated. The results indicate that routing algorithms with the balanced rings constantly yield larger throughput and smaller latency than those without. 
53|-||Dual actuator logging disk architecture and modeling|In this paper, we present a dual actuator logging disk architecture to minimize write access latencies. We reduce small synchronous write latency using the notion of logging writes, i.e. writing to free sectors near the current disk head location. However, we show through analytic models and simulations that logging writes by itself is not sufficient to reduce write access latencies, particularly in environments with writes to new data and intermixed reads and writes. Therefore, we augment the logging write method with the addition of a second disk actuator. Our models and simulations show that the addition of the second actuator offers significant performance benefits over a normal disk over a wide range of disk access patterns, and comparisons to strictly logging disk architectures show advantages over a range of disk access patterns. 
53|-||Feasibility of decoupling memory management from the execution pipeline|In conventional architectures, the central processing unit (CPU) spends a significant amount of execution time allocating and de-allocating memory. Efforts to improve memory management functions using custom allocators have led to only small improvements in performance. In this work, we test the feasibility of decoupling memory management functions from the main processing element to a separate memory management hardware. Such memory management hardware can reside on the same die as the CPU, in a memory controller or embedded within a DRAM chip. Using Simplescalar, we simulated our architecture and investigated the execution performance of various benchmarks selected from SPECInt2000, Olden and other memory intensive application suites.Hardware allocator reduced the execution time of applications by as much as 50%. In fact, the decoupled hardware results in a performance improvement even when we assume that both the hardware and software memory allocators require the same number of cycles. We attribute much of this improved performance to improved cache behavior since decoupling memory management functions reduces cache pollution caused by dynamic memory management software. We anticipate that even higher levels of performance can be achieved by using innovative hardware and software optimizations. We do not show any specific implementation for the memory management hardware. This paper only investigates the potential performance gains that can result from a hardware allocator. 
53|-||A platform-based SoC design and implementation of scalable automaton matching for deep packet inspection|String matching plays a central role in packet inspection applications such as intrusion detection, anti-virus, anti-spam and Web filtering. Since they are computation and memory intensive, software matching algorithms are insufficient to meet the high-speed performance. Thus, offloading packet inspection to a dedicated hardware seems inevitable. This paper presents a scalable automaton matching (SAM) coprocessor that uses Aho-Corasick (AC) algorithm with two parallel acceleration techniques, root-indexing and pre-hashing. The root-indexing can match multiple bytes in one single matching, and the pre-hashing can be used to avoid bitmap AC matching which is a cycle-consuming operation. In the platform-based SoC implementation of the Xilinx ML310 FPGA, the proposed hardware architecture can achieve almost 10.7 Gbps and support over 10,000 patterns for virus, which is the largest pattern set from among the existing works. On the average, the performance of SAM is 7.65 times faster than the original bitmap AC. Furthermore, SAM is feasible for either internal or external memory architecture. The internal memory architecture provides high performance, while the external memory architecture provides high scalability in term of the number of patterns. 
53|1|http://www.sciencedirect.com/science/journal/13837621/53/1|Trace-based leakage energy optimisations at link time|Energy-aware compilers are becoming increasingly important for embedded systems due to the need to meet a variety of design constraints on time, code size and power consumption. This paper introduces for the first time a trace-based, link-time compiler framework on binaries for embedded systems and evaluates its potential benefits in supporting energy optimisations, especially those that exploit the interaction between compilers and architecture. We present two algorithms for reducing leakage energy in functional units and data caches, respectively. Both algorithms work uniformly at the granularity of optimisation regions that are formed by the hot traces of a program. Our experimental results using Mediabench benchmarks show that good leakage energy savings can be achieved at the cost of some small performance and code size penalties. Furthermore, by varying the granularity of optimisation regions, which is a tunable parameter, embedded application programmers can make the tradeoffs between energy savings and these associated costs. 
53|1||Fault tolerant Web Services|Zwass suggested that middleware and message service is one of the five fundamental technologies used to realize Electronic Commerce (EC). The Simple Object Access Protocol (SOAP) is recognized as a more promising middleware for EC applications among other leading candidates such as CORBA. Many recent polls reveal however that security and reliability issues are major concerns that discourage people from engaging in EC transactions. We notice that the fault-tolerance issue is somewhat neglected in the current standard, i.e., SOAP 1.2. We therefore propose a fault tolerant Web Services called fault tolerant SOAP or FT-SOAP through which Web Services can be built with higher resilience to failure. FT-SOAP is based on our previous experience with an object fault tolerant service (OFS) and OMG’s fault tolerant CORBA (FT-CORBA). There are many architectural differences between SOAP and CORBA. One of the major contributions of this work is to discuss the impact of these architectural differences on FT-SOAP design. Our experience shows that Web Services built on a SOAP framework enjoy higher flexibility compared to those built on CORBA. We also point out the limitations of the current feature sets of SOAP 1.2, e.g. the application of the intermediary. In addition, we examine two implementation approaches; namely, one based on the SOAP 1.2’s intermediary, and the other on Axis handler. We conclude that the intermediary approach is infeasible due to the backward compatibility issue. We believe our experience is valuable not only to the fault-tolerance community, but also to other communities as well, in particular, to those who are familiar with the CORBA platform. 
53|1||Resource consumption-aware QoS in cluster-based VOD servers|For Video-On-Demand (VOD) systems, it is important to provide Quality of Service (QoS) to more clients under limited resources. In this paper, the performance scalability in cluster-based VOD servers is studied with several grouping configurations of cluster nodes. To find performance bottlenecks, the monitoring functions are employed and the maximum QoS streams are measured under the various requests including VCR functions. To support more user friendly interface, an embedded set-top model is suggested for the QoS of TV clients. From our detailed experiment results, a new admission control method is proposed that is based on available system resources and the actual amount of resource consumed for QoS streams. The proposed method provides not only more scalable QoS in cluster-based VOD servers but also the enhancement of resource utilization by guaranteeing the maximum number of QoS streams. 
53|1||An efficient variable partitioning approach for functional decomposition of circuits|Functional decomposition is a process of splitting a complex circuit into smaller sub-circuits. There exist two major strategies in decomposition, namely, serial and parallel decomposition. In serial decomposition the problem the complex function represented as a truth table with support set variables and partitioned into free and bout set variables. The minterms corresponding to the bound set variables are represented as an equivalent function called the predecessor function. Equivalent minterms of the bound set variables are assigned an output code. The assigned output codes and the free set variable minterms are represented as the successor function. Serial decomposition is further categorized into disjoint and non-disjoint decomposition, when the free and bound set variables are disjoint and non-disjoint respectively. This paper deals with the problem of determining the set of best free and bound variables (variable partitioning problem) for disjoint serial decomposition. Variable partitioning is the first step in decomposition process. An efficient variable partition algorithm is one that determines the set of all free and bound set variables that satisfy the decomposition theorem in minimal time and by exploring the search space effectively. This will allow the decomposition algorithm to determine the best variable partition of a function that results in smaller decomposed functions and with maximum number of do not cares in these functions. Classical approaches to determine the best free and bound set use exhaustive search methods. The time and memory requirements for such approaches are exponential or super exponential.A novel heuristic search approach is proposed to determine the set of good variable partitions in minimal time by minimally exploring the search space. There are two heuristics employed in the proposed search approach, (1) r-admissibility based heuristic or pruned breadth first search (PBFS) approach and (2) Information relation based heuristic or improved pruned breadth first search (IPBFS) approach. The r-admissibility based heuristic is based on r-partition characteristics of the free and bound set variables. The information relation and measure based heuristic is based on information relationship of free and bound set variables that are expressed as r-partition heuristics. The proposed variable partition search approach has been successfully implemented and test with MCNC and Espresso benchmarks and the results indicate that the time complexity is comparable to r-admissible heuristic algorithm and the quality of solution is comparable to exact variable partitioning algorithm. A comparison of PBFS and IPBFS heuristics for certain benchmarks are also discussed in this paper. 
53|10|http://www.sciencedirect.com/science/journal/13837621/53/10|Editorial|
53|10||Effects of program compression|The size of the program code has become a critical design constraint in embedded systems, especially in handheld, battery operated devices. Large program codes require large memories, which increase the size and cost of the chip. In addition, the power consumption is increased due to higher memory I/O bandwidth. Program compression is one of the most often used methods to reduce the size of the program code. In this paper, two compression approaches, dictionary-based compression and instruction template-based compression, were evaluated on a customizable processor architecture with parallel resources. The effects on area and power consumption were measured. Dictionary-based compression reduced the area at best by 77% and power consumption by 73%. Instruction template-based compression resulted in increase in both area and power consumption and hence turned out to be impractical. 
53|10||Hybrid functional- and instruction-level power modeling for embedded and heterogeneous processor architectures|In this contribution the concept of functional- level power analysis (FLPA) for power estimation of programmable processors is extended in order to model embedded as well as heterogeneous processor architectures featuring different embedded processor cores. The basic FLPA approach is based on the separation of the processor architecture into functional blocks like, e.g. processing unit, clock network, internal memory, etc. The power consumption of these blocks is described by parameterized arithmetic models. By application of a parser based automated analysis of assembler codes the input parameters of the arithmetic functions like e.g. the achieved degree of parallelism or the kind and number of memory accesses can be computed. For modeling an embedded general purpose processor (here, an ARM940T) the basic FLPA modeling concept had to be extended to a so-called hybrid functional-level and instruction-level (FLPA/ILPA) model in order to achieve a good modeling accuracy. In order to show the applicability of this approach even a heterogeneous processor architecture (OMAP5912) featuring an ARM926EJ-S core and a C55x DSP core has been modeled using the hybrid FLPA/ILPA technique described before. The approach is exemplarily demonstrated and evaluated applying a variety of basic digital signal processing tasks ranging from basic filters to complete audio decoders or classical benchmark suits. Estimated power figures for the inspected tasks are compared to physically measured values for both inspected processor architectures. A resulting maximum estimation error of 9% for the ARM940T and less than 4% for the OMAP5912 is achieved. 
53|10||Simulated and measured performance evaluation of RISC-based SoC platforms in network processing applications|This paper presents results of a simulated performance evaluation of RISC-based SoC platforms for networking applications and compares them to measurement results on an FPGA prototype. We use our SystemC simulation environment, which is calibrated with a reference implementation. Starting with an analysis of the reference scenario, two approaches for improvements are investigated: at first, hardware assists are added, which offload the CPU from compute-intensive bit-level manipulations. Secondly, the concept of flexible processing paths as proposed in FlexPath NP with AutoRoute is evaluated, in which certain parts of the traffic can bypass the central CPU cluster. For each of the three scenarios we determine the maximum throughput and discuss the improvements and limitations of each solution. It can be shown that a FlexPath NP may achieve up to 2.5 times the throughput of the reference scenario. Simulation results are compared to additional measurements on the FPGA platform, which led to a further refinement of our system model. The investigations provide a deeper insight on the practical benefits and limitations of system-level performance simulations. 
53|10||Exploration of distributed shared memory architectures for NoC-based multiprocessors|Multiprocessor system-on-chip (MP-SoC) platforms represent an emerging trend for embedded multimedia applications. To enable MP-SoC platforms, scalable communication-centric interconnect fabrics, such as networks-on-chip (NoCs), have been recently proposed. The shared memory represents one of the key elements in designing MP-SoCs to provide data exchange and synchronization support.This paper focuses on the energy/delay exploration of a distributed shared memory architecture, suitable for low-power on-chip multiprocessors based on NoC. A mechanism is proposed for the data allocation on the distributed shared memory space, dynamically managed by an on-chip hardware memory management unit (HwMMU). Moreover, the exploitation of the HwMMU primitives for the migration, replication, and compaction of shared data is discussed. Experimental results show the impact of different distributed shared memory configurations for a selected set of parallel benchmark applications from the power/-performance perspective. Furthermore, a case study for a graph exploration algorithm is discussed, accounting for the effects of the core mapping and the network topology on energy and performance at the system level. 
53|10||Efficient design space exploration for application specific systems-on-a-chip|A reduction in the time-to-market has led to widespread use of pre-designed parametric architectural solutions known as system-on-a-chip (SoC) platforms. A system designer has to configure the platform in such a way as to optimize it for the execution of a specific application. Very frequently, however, the space of possible configurations that can be mapped onto a SoC platform is huge and the computational effort needed to evaluate a single system configuration can be very costly. In this paper we propose an approach which tackles the problem of design space exploration (DSE) in both of the fronts of the reduction of the number of system configurations to be simulated and the reduction of the time required to evaluate (i.e., simulate) a system configuration. More precisely, we propose the use of Multi-objective Evolutionary Algorithms as optimization technique and Fuzzy Systems for the estimation of the performance indexes to be optimized. The proposed approach is applied on a highly parameterized SoC platform based on a parameterized VLIW processor and a parameterized memory hierarchy for the optimization of performance and power dissipation. The approach is evaluated in terms of both accuracy and efficiency and compared with several established DSE approaches. The results obtained for a set of multimedia applications show an improvement in both accuracy and exploration time. 
53|10||Design space exploration of reliable networked embedded systems|In this paper, a new methodology is presented for topology optimization of networked embedded systems as they occur in automotive and avionic systems as well as wireless sensor networks. By introducing a model which is (1) suitable for heterogeneous networks with different communication bandwidths, (2) modeling of routing restrictions, and (3) flexible binding of tasks onto processors, current design issues of networked embedded systems can be investigated. On the basis of this model, the presented methodology firstly allocates the required resources which can be communication links as well as computational nodes and secondly binds the functionality onto the nodes and the data dependencies onto the links such that no routing restrictions will be violated or capacities on communication links will be exceeded. Due to the often error-prone communication in networks, we allow for routing each data dependency over multiple routes in the networks. With this strategy, our methodology is able to increase the reliability of the entire system. This reliability analysis is based on Binary Decision Diagrams (BDDs) and is integrated in our multi-objective design space exploration. By applying Evolutionary Algorithms, we are able to consider multiple objectives simultaneously during the optimization process and allow for a subsequent unbiased decision making. An experimental evaluation as well as a demonstration of a case study from the field of automotive electronics will show the applicability of the presented approach. 
53|10||Chip size estimation for SOC design space exploration|At early design space exploration phases of architectures for Systems On a Chip (SOC) chip size estimation is of high interest. An accurate chip size estimation needs detailed knowledge of the transistor densities of a semiconductor process. This paper introduces a novel and simplified chip size estimator, which is independent of manufacturer specific process data. CMOS processes are characterized by only three parameters. These are the drawn gate length and the used numbers of metal layers for logic and for memories. The chip size estimator has been derived from a comprehensive analysis of realized VLSI chips. It has been investigated and confirmed either for published VLSIs as well as for latest SOC designs with 221 million transistors and 333 million transistors. The proposed model has been implemented as a web based tool and contributes to analytical modeling of cost and performance tradeoffs of SOC concepts. 
53|10||FLUX interconnection networks on demand|In this paper, we introduce the FLUX interconnection networks, a scheme where the interconnections of a parallel system are established on demand before or during program execution. We present a programming paradigm which can be utilized to make the proposed solution feasible. We perform several experiments to show the viability of our approach and the potential performance gain of using the most suitable network configuration for a given parallel program. We experiment on several case studies, evaluate different algorithms, developed for meshes or trees, and map them on “grid”-like or reconfigurable physical interconnection networks. Our results clearly show that, based on the underlying network, different mappings are suitable for different algorithms. Even for a single algorithm different mappings are more appropriate, when the processing data size, the number of utilized nodes or the hardware cost of the processing elements changes. The implication of the above is that changing interconnection topologies/mappings (dynamically) on demand depending on the program needs can be beneficial. 
53|11|http://www.sciencedirect.com/science/journal/13837621/53/11|Automated memory-aware application distribution for Multi-processor System-on-Chips|Mapping of applications on a Multi-processor System-on-Chip (MP-SoC) is a crucial step to optimize performance, energy and memory constraints at the same time. The problem is formulated as finding solutions to a cost function of the algorithm performing mapping and scheduling under strict constraints. Our solution is based on simultaneous optimization of execution time and memory consumption whereas traditional methods only concentrate on execution time. Applications are modeled as static acyclic task graphs that are mapped on MP-SoC with customized simulated annealing. The automated mapping in this paper is especially purposed for MP-SoC architecture exploration, which typically requires a large number of trials without human interaction. For this reason, a new parameter selection scheme for simulated annealing is proposed that sets task mapping specific optimization parameters automatically. The scheme bounds optimization iterations to a reasonable limit and defines an annealing schedule that scales up with application and architecture complexity. The presented parameter selection scheme compared to extensive optimization achieves 90% goodness in results with only 5% optimization time, which helps large-scale architecture exploration where optimization time is important. The optimization procedure is analyzed with simulated annealing, group migration and random mapping algorithms using test graphs from the Standard Task Graph Set. Simulated annealing is found better than other algorithms in terms of both optimization time and the result. Simultaneous time and memory optimization method with simulated annealing is shown to speed up execution by 63% without memory buffer size increase. As a comparison, optimizing only execution time yields 112% speedup, but also increases memory buffers by 49%. 
53|11||Optimization decomposition approach for layered QoS scheduling in grid computing|The paper presents optimization decomposition based layered Quality of Service (QoS) scheduling for computational grid. Cross layer joint QoS scheduling is studied by considering the global problem as decomposed into three sub-problems: resource allocation at the fabric layer, service composing at the collective layer, and user satisfaction degree at the application layer. The paper proposes a complete solution from optimization modeling, Lagrange relaxation based decomposition, to solutions for each sub-problem Lagrange relaxation based decomposition. These aspects match the vertical organization of the considered grid system: each layer trade with adjacent layers to find a global optimum of the whole grid system. Through multi-layer QoS joint optimization approach, grid global QoS optimization can be achieved. The cross layer policy produces an optimal set of grid resources, service compositions, and user’s payments at the fabric layer, collective layer and application layer, respectively, to maximize global grid QoS. The owner of each layer obtains inputs from other layers, tries to maximize its own utility and provides outputs back to other layers. This iterative process lasts until presumably all layers arrive at the same solution. 
53|11||Efficient segment-based video transcoding proxy for mobile multimedia services|To support various bandwidth requirements for mobile multimedia services for future heterogeneous mobile environments, such as portable notebooks, personal digital assistants (PDAs), and 3G cellular phones, a transcoding video proxy is usually necessary to provide mobile clients with adapted video streams by not only transcoding videos to meet different needs on demand, but also caching them for later use. Traditional proxy technology is not applicable to a video proxy because it is less cost-effective to cache the complete videos to fit all kinds of clients in the proxy. Since transcoded video objects have inheritance dependency between different bit-rate versions, we can use this property to amortize the retransmission overhead from transcoding other objects cached in the proxy. In this paper, we propose the object relation graph (ORG) to manage the static relationships between video versions and an efficient replacement algorithm to dynamically manage video segments cached in the proxy. Specifically, we formulate a transcoding time constrained profit function to evaluate the profit from caching each version of an object. The profit function considers not only the sum of the costs of caching individual versions of an object, but also the transcoding relationship among these versions. In addition, an effective data structure, cached object relation tree (CORT), is designed to facilitate the management of multiple versions of different objects cached in the transcoding proxy. Experimental results show that the proposed algorithm outperforms companion schemes in terms of the byte-hit ratios and the startup latency. 
53|11||Accumulator-based pseudo-exhaustive two-pattern generation|
53|11||Static scheduling techniques for dependent tasks on dynamically reconfigurable devices|Dynamically reconfigurable hardware not only has high silicon reusability, but it can also deliver high performance for computation-intensive tasks. Advanced features such as run-time reconfiguration allow multiple tasks to be mapped onto the same device either simultaneously or multiplexed in time domain. These tasks need to be scheduled optimally or near optimally in order to efficiently utilize the device. It is a NP-hard problem, because task scheduling, allocation and configuration prefetching all need to be considered. In this paper, we target dependent task models and propose three static schedulers that use different problem solving strategies. The first is a heuristic approach developed from traditional list-based schedulers. It presents high efficiency but the least accuracy. The second is based on a full-domain search using constraint programming. It can guarantee to produce optimal solutions but requires significant searching effort. The last is a guided random search technique based on a genetic algorithm, which shows reasonable efficiency and much better accuracy than the heuristic approach. 
53|11||Test data compression scheme based on variable-to-fixed-plus-variable-length coding|A test data compression scheme based on Variable-to-Fixed-Plus-Variable-Length (VTFPVL) coding is presented, by using which the test data can be compressed efficiently. In this scheme, code words are divided into fixed-length head section and variable-length tail section. In order to attain further compression, the highest bit of the tail is omitted from the code words, because all of the highest bits in the tail section of the code words are the same as 1. A special shift counter is also used, which further eases the control circuit. Experimental results of the MinTest fault sets which are part of ISCAS-89 benchmark circuits show that the proposed scheme is obviously better than traditional coding methods in the compression ratio and the implementation of decompression, such as Golomb, FDR, VIHC, v9C coding. 
53|2-3|http://www.sciencedirect.com/science/journal/13837621/53/2-3|Embedded cryptographic hardware|
53|2-3||High-speed hardware implementations of Elliptic Curve Cryptography: A survey|For the last decade, Elliptic Curve Cryptography (ECC) has gained increasing acceptance in the industry and the academic community and has been the subject of several standards. This interest is mainly due to the high level of security with relatively small keys provided by ECC. To sustain the high throughput required by applications like network servers, high-speed implementations of public-key cryptosystems are needed. For that purpose, hardware-based accelerators are often the only solution reaching an acceptable performance-cost ratio. The fundamental question that arises is how to choose the appropriate efficiency–flexibility tradeoff.In this survey, techniques for implementing Elliptic Curve Cryptography at a high-speed are explored. A classification of the work available in the open literature in function of the level of efficiency and flexibility is also proposed. In particular, the subjects of reconfigurable, dedicated, generator, versatile and general purpose scalar multipliers are addressed. Finally, some words about future work that should be tackled are provided. 
53|2-3||Hardware acceleration of the Tate pairing on a genus 2 hyperelliptic curve|Many novel and interesting cryptographic protocols have recently been designed with bilinear pairings comprising their main calculation. The Î·T method for pairing calculation is an efficient computation technique based on a generalisation and optimisation of the Duursma–Lee algorithm for calculating the Tate pairing. The pairing can be computed very efficiently on hyperelliptic curves of genus 2. In this paper it is demonstrated that the Î·T method is ideally suited for hardware implementation since much of the more intensive arithmetic can be performed in parallel in hardware. A Tate pairing processor is presented and the architectures required for such a system are discussed. The processor returns a fast pairing computation when compared to the best results in the literature to date. Results are provided when the processor is implemented on an FPGA over the base field F2103F2103. 
53|2-3||Fast hardware for modular exponentiation with efficient exponent pre-processing|Modular exponentiation is an important operation in several public-key cryptosystems. It is performed using successive modular multiplications. For the sake of efficiency, one needs to reduce the total number of required modular multiplications. In this paper, we propose two efficient hardware implementations for computing modular exponentiations, which are based on the m-ary method. The m-ary method includes a power pre-computation step. The first design ignores the exponent and pre-computes all possible powers while the second takes advantage of the formation of the exponent to compute only those powers that are really necessary for the rest of the computation. As it can be expected, the implementation of the first architecture, compared with that of the second one, requires more time to complete an exponentiation. In compensation, however, the first should require less hardware area than the second. We provide a comparison of these two implementations using the performance factor, which takes into account both space and time requirements. 
53|2-3||Efficient parallel multiplier in shifted polynomial basis|In this paper we study the multiplication in fields F2nF2n using the Shifted Polynomial Basis (SPB) representation of Fan and Dai [H. Fan, Y. Dai, Fast bit-parallel GF(2n) multiplier for all trinomials, IEEE Transactions on Computers 54 (4) (2005) 485–490]. We give a simpler construction than in Fan and Dai (2005) of the matrix associated to the SPB used to perform the field multiplication. We present also a novel parallel architecture to multiply in SPB. This multiplier have a smaller time complexity (for good field it is equal to TA + ⌈log2(n)⌉TX) than all previously presented architecture. For practical field F2nF2n, i.e., for n ≅ 163, this roughly improves the delay by 10%. On the other hand the space complexity is increased by 25%: the space complexity is a little greater than the time gain. 
53|2-3||Scalable hardware implementing high-radix Montgomery multiplication algorithm|This paper presents a new scalable hardware implementing modular multiplication. A high radix Montgomery multiplication algorithm without final subtraction is used to perform this operation. An alternative proof for the final Montgomery multiplication by 1, removing the condition on the modulus, is given. This hardware fits in any chip area and is able to work with any size of modulus. Unlike other scalable designs only one cell is used. This cell contains standard and well optimized digit multiplier and adder. Time–area trade-offs are also available before hardware synthesis for differents sizes of internal data path. The pipeline architecture of the multiplier component increases the clock frequency and the throughput. Time–area trade-offs are analyzed in order to make the best choice for given time and area constraints. This architecture seems to provide a better time–area compromise than previous scalable hardware. 
53|2-3||Multi-mode operator for SHA-2 hash functions|We propose an improved implementation of the SHA-2 hash family, with minimal operator latency and reduced hardware requirements. We also propose a high frequency version at the cost of only two cycles of latency per message. Finally we present a multi-mode architecture able to perform either a SHA-384 or SHA-512 hash or to behave as two independent SHA-224 or SHA-256 operators. Such capability adds increased flexibility for applications ranging from a server running multiple streams to independent pseudorandom number generation. We also demonstrate that our architecture achieves a performance comparable to separate implementations while requiring much less hardware. 
53|2-3||Robust codes and robust, fault-tolerant architectures of the Advanced Encryption Standard|Hardware implementations of cryptographic algorithms are vulnerable to fault analysis attacks. Methods based on traditional fault-tolerant architectures are not suited for protection against these attacks. To detect these attacks we propose an architecture based on robust nonlinear systematic error-detecting codes. These nonlinear codes are capable of providing uniform error detecting coverage independently of the error distributions. They make no assumptions about what faults or errors will be injected by an attacker. Architectures based on these robust constructions have fewer undetectable errors than linear codes with the same n, k. We present the general properties and construction methods of these codes as well as their application for the protection of a cryptographic devices implementing the Advanced Encryption Standard. 
53|4|http://www.sciencedirect.com/science/journal/13837621/53/4|The SegBus platform â architecture and communication mechanisms|In this study, we introduce the SegBus architecture, a synchronous segmented bus platform for systems on chip. We present the envisioned structure in detail, and also address aspects of communication on the platform. The motivation behind SegBus is the search for performance improvements, in several directions, such as global throughput, power consumption, modularity, adaptability. By means of an example, we illustrate the capabilities of the described architecture. The implementation strategy targets FPGA technology, and allows for the utilization of multiple clock domains. The platform emerges as a highly design-time configurable system, adaptable to various design constraints. 
53|4||A comparison of two policies for issuing instructions speculatively|Value speculation is a speculative technique proposed to reduce the execution time of programs. It relies on a predictor, a checker and a recovery mechanism. The predictor predicts the result of an instruction in order to issue speculatively its dependent instructions, the checker checks the prediction after issuing the predicted instruction, and the recovery mechanism deals with mispredictions in order to maintain program correctness.Previous works on value speculation have considered that the instructions dependent on a predicted instruction can be issued before issuing the predicted instruction (non-delayed issue policy). In this work we propose delaying the issue time of the instructions dependent on a value-predicted instruction until issuing the value-predicted instruction (delayed issue policy). Although the potential performance benefits of the delayed issue policy are smaller than that of the non-delayed issue policy, the recovery mechanism required by the delayed issue policy is simpler than the recovery mechanism required by the non-delayed issue policy.We have evaluated both issue policies in the context of load-value prediction by means of address prediction in order to determine in which scenarios the performance of the delayed issue policy is competitive with that of the non-delayed issue policy. Our results show that the delayed policy is a cost-effective alternative to the non-delayed policy, especially for realistic issue-queue sizes. 
53|4||Efficient FPGA hardware development: A multi-language approach|This paper presents a multi-language framework to FPGA hardware development which aims to satisfy the dual requirement of high-level hardware design and efficient hardware implementation. The central idea of this framework is the integration of different hardware languages in a way that harnesses the best features of each language. This is illustrated in this paper by the integration of two hardware languages in the form of HIDE: a structured hardware language which provides more abstract and elegant hardware descriptions and compositions than are possible in traditional hardware description languages such as VHDL or Verilog, and Handel-C: an ANSI C-like hardware language which allows software and hardware engineers alike to target FPGAs from high-level algorithmic descriptions. On the one hand, HIDE has proven to be very successful in the description and generation of highly optimised parameterisable FPGA circuits from geometric descriptions. On the other hand, Handel-C has also proven to be very successful in the rapid design and prototyping of FPGA circuits from algorithmic application descriptions. The proposed integrated framework hence harnesses HIDE for the generation of highly optimised circuits for regular parts of algorithms, while Handel-C is used as a top-level design language from which HIDE functionality is dynamically invoked. The overall message of this paper posits that there need not be an exclusive choice between different hardware design flows. Rather, an integrated framework where different design flows can seamlessly interoperate should be adopted. Although the idea might seem simple prima facie, it could have serious implications on the design of future generations of hardware languages. 
53|4||An efficient immersion-based watershed transform method and its prototype architecture|This paper describes an improved immersion-based watershed algorithm to compute the watershed lines for segmentation of digital gray scale images and its hardware implementation. The proposed algorithm is devoid of certain disadvantages inherent in a conventional immersion-based algorithm originally proposed by Vincent and Soille. Flooding of catchment basins from pre-determined regional minima and conditional neighborhood comparisons while processing the eight neighboring pixels of a labeled center pixel ensures thin continuous watershed lines. Reduced computational complexity and increased throughput compared to the conventional algorithm occurs from simultaneous determination of labels of various neighboring pixels. The complexity of the proposed algorithm is analyzed. The results of running both the proposed and the conventional algorithm on different test images clearly establish the superiority of the proposed algorithm. A prototype architecture designed to implement the proposed watershed algorithm has been modelled in VHDL and synthesized for Virtex FPGA. The FPGA implementation results show acceptable performance of the proposed architecture. 
53|4||Highly fault-tolerant cycle embeddings of hypercubes|The hypercube Qn is one of the most popular networks. In this paper, we first prove that the n-dimensional hypercube is 2n − 5 conditional fault-bipancyclic. That is, an injured hypercube with up to 2n − 5 faulty links has a cycle of length l for every even 4 â©½ l â©½ 2n when each node of the hypercube is incident with at least two healthy links. In addition, if a certain node is incident with less than two healthy links, we show that an injured hypercube contains cycles of all even lengths except hamiltonian cycles with up to 2n − 3 faulty links. Furthermore, the above two results are optimal. In conclusion, we find cycles of all possible lengths in injured hypercubes with up to 2n − 5 faulty links under all possible fault distributions. 
53|4||Hardware support for adaptive tessellation of BÃ©zier surfaces based on local tests|Bézier representations have been widely employed as a standard way of designing complex scenes with very good quality results. These surfaces are usually tessellated, in the software application, into triangle models to be rendered. Then, the final image is generated in the graphics card so that its triangle rendering capabilities are exploited.In this work we present an adaptive tessellation algorithm and the corresponding architecture to be implemented in hardware. The objective of the proposal is to avoid the potential bottleneck associated with the transmission of complex triangular models from CPU to graphics cards. The algorithm we propose is based on a layer strip representation method and a new data management that permits generation and efficient storage of the tessellated mesh. The corresponding architecture has to be included as an additional unit at the input of the graphics card. As a consequence, the transmission requirements from CPU to graphics card are greatly reduced as the tessellation is performed in the graphics card. On the other hand, the adaptive strategy employed permits selection of the number of triangles of the final mesh as a trade off between computational requirements and quality of the final mesh. The efficient data management proposed, together with the low storage requirements of the architecture, makes it a good candidate for its hardware implementation and inclusion in future graphics cards. 
53|5-6|http://www.sciencedirect.com/science/journal/13837621/53/5-6|Editorial|
53|5-6||Asynchronous arbiter for micro-threaded chip multiprocessors|This paper presents a scalable and partitionable asynchronous bus arbiter for use with chip multiprocessors and its corresponding pre-layout simulation results using VHDL. The arbiter exploits the advantage of a concurrency control instruction (Brk) provided by the micro-threaded microprocessor model to set the priority processor and move the circulated arbitration token to the most likely processor to issue the create instruction. This mechanism provides latency hiding during token circulation by decoupling the micro-threaded processor from the ring’s timing. The arbiter provides a very simple arbitration mechanism and can be used for chip multiprocessor arbitration purposes. 
53|5-6||Supporting multiple-input, multiple-output custom functions in configurable processors|Configurable processors have emerged as a promising solution for high performance embedded systems. Many of these processors extend a RISC core with configurable functional units that execute dual-input, single-output (DISO) custom functions. Although studies have shown that supporting multiple-input, multiple-output (MIMO) custom functions can lead to significant speedups, mechanisms to efficiently achieve this have not been adequately addressed. The underlying reason is that a custom function is normally invoked by a single instruction, which usually transfers only two inputs and one output. Attempts to transfer more inputs and outputs in one instruction are impeded by the instruction length and the register file’s R/W ports. This paper proposes a simple extension to transfer multiple inputs and outputs of the custom functions using repeated instructions. While transferring the inputs and outputs may take a few extra cycles, our experiments show that the MIMO extension can still achieve an average 51% increase in speedup compared to a DISO extension and an average 27% increase in speedup compared to a multiple-input, single-output (MISO) extension. 
53|5-6||A consistency-free memory architecture for sort-last parallel rendering processors|Current rendering processors are aiming to process triangles as fast as possible and they have the tendency of equipping with multiple rasterizers to be capable of handling a number of triangles in parallel for increasing polygon rendering performance. However, those parallel architectures may have the consistency problem when more than one rasterizer try to access the data at the same address. This paper proposes a consistency-free memory architecture for sort-last parallel rendering processors, in which a consistency-free pixel cache architecture is devised and effectively associated with three different memory systems consisting of a single frame buffer, a memory interface unit, and consistency-test units. Furthermore, the proposed architecture can reduce the latency caused by pixel cache misses because the rasterizer does not wait until cache miss handling is completed when the pixel cache miss occurs. The experimental results show that the proposed architecture can achieve almost linear speedup upto four rasterizers with a single frame buffer. 
53|5-6||Resource efficiency of the GigaNetIC chip multiprocessor architecture|In this article, we present the prototypical implementation of the scalable GigaNetIC chip multiprocessor architecture. We use an FPGA-based rapid prototyping system to verify the functionality of our architecture in a network application scenario before fabricating the ASIC in a modern CMOS standard cell technology. The rapid prototyping environment gives us the opportunity to test our multiprocessor architecture with Ethernet-based data streams in a real network scenario. Our system concept is based on a massively parallel processor structure. Due to its regularity, our architecture can be easily scaled to accommodate a wide range of packet processing applications with various performance and throughput requirements at high reliability. Furthermore, the composition based on predefined building blocks guarantees fast design cycles and simplifies system verification. We present standard cell synthesis results as well as a performance analysis for a firewall application with various couplings of hardware accelerators. Finally, we compare implementations of our architecture with state-of-the-art desktop CPUs. We use simple, general-purpose applications as well as the introduced packet processing tasks to determine the performance capabilities and the resource efficiency of the GigaNetIC architecture. We show that, if supported by the application, parallelism offers more opportunities than increasing clock frequencies. 
53|5-6||Efficient control generation for mapping nested loop programs onto processor arrays|Processor array architectures are optimal platforms for computationally intensive applications. Such architectures are characterized by hierarchies of parallelism and memory structures, i.e. processor arrays apart from different levels of cache have a large number of processing elements (PE) where each PE can further contain sub-word parallelism. In order to handle large scale problems, balance local memory requirements with I/O-bandwidth, and use different hierarchies of parallelism and memory, one needs a sophisticated transformation called hierarchical partitioning. Innately the applications are data flow dominant and have almost no control flow, but the application of hierarchical partitioning techniques has the disadvantage of a more complex control flow. In a previous paper, the authors presented first time a methodology for the automated control path synthesis for the mapping of partitioned algorithms onto processor arrays. However, the control path contained complex multiplication and division operators. In this paper, we propose a significant extension to the methodology which reduces the hardware cost of the global controller and memory address generators by avoiding these costly operations. 
53|5-6||A system architecture for high-speed deep packet inspection in signature-based network intrusion prevention|Pattern matching is one of critical parts of Network Intrusion Prevention Systems (NIPS). Pattern matching hardware for NIPS should find a matching pattern at wire speed. However, that alone is not good enough. First, pattern matching hardware should be able to generate sufficient pattern match information including the pattern index number and the location of the match found at wire speed. Second, it should support pattern grouping to reduce unnecessary pattern matches. Third, it should guarantee worst-case performance even if the number of patterns is increased. Finally it should be able to update patterns in a few minutes or seconds without stopping its operations. We propose a system architecture to meet the above requirements. Using Xilinx FPGA simulation, we show that the new system scales well to achieve a high speed over 10 Gbps and satisfies all of the above requirements. 
53|5-6||High-speed, low-leakage integrated circuits: An evolutionary algorithm perspective|The markets today observe users having increasing demands on processing speed and energy consumption of their mobile devices. However, processing speed as well as functionality always comes at the expense of energy and thus limits, among other things, mobility and integration density. Recent technological developments allow for the simultaneous realization of slow, low-energy consuming as well as fast, high-energy consuming gates on the very same chip. In this respect, a particular design is an abstract optimization task for which this paper applies evolutionary algorithms. These algorithms are heuristic population-based search procedures that utilize certain mechanisms known from natural evolution. In comparison to currently available deterministic optimization procedures, the evolutionary algorithms achieved some energy savings of about 10–40% on standard ISCAS test problems, while still yielding the highest processing speed possible. 
53|5-6||Energy consumption analysis for two embedded Java virtual machines|In this paper we present a general framework for estimating the energy consumption of an embedded Java virtual machine (JVM). We have designed a number of experiments to find the constant overhead and establish an energy consumption cost for individual Java opcodes for two JVMs. The results show that there is a basic constant overhead for every Java program, and that a subset of Java opcodes have an almost constant energy cost. We also show that memory access is a crucial energy consumption component. 
53|7|http://www.sciencedirect.com/science/journal/13837621/53/7|Genetic algorithms for hardwareâsoftware partitioning and optimal resource allocation|A scheme for time and power efficient embedded system design, using hardware and software components, is presented. Our objective is to reduce the execution time and the power consumed by the system, leading to the simultaneous multi-objective minimization of time and power. The goal of suitably partitioning the system into hardware and software components is achieved using Genetic Algorithms (GA). Multiple tests were conducted to confirm the consistency of the results obtained and the versatile nature of the objective functions. An enhanced resource constrained scheduling algorithm is used to determine the system performance. To emulate the characteristics of practical systems, the influence of inter-processor communication is examined. The suitability of introducing a reconfigurable hardware resource over pre-configured hardware is explored for the same objectives. The distinct difference in the task to resource mapping with the variation in design objective is studied. Further, the procedure to allocate optimal number of resources based on the design objective is proposed. The implementation is constrained for power and time individually, with GA being used to arrive at the resource count to suit the objective. The results obtained are compared by varying the time and power constraints. The test environment is developed using randomly generated task graphs. Exhaustive sets of tests are performed on the set design objectives to validate the proposed solution. 
53|7||A low-cost strategy to provide full QoS support in Advanced Switching networks|Advanced Switching (AS) is an open-standard fabric-interconnect technology that is built over the same physical and link layers as PCI Express technology. Moreover, it includes an optimized transaction layer to enable essential communication capabilities, including protocol encapsulation, peer-to-peer communications, mechanisms to provide quality of service (QoS), enhanced fail-over, high availability, multicast communications, and congestion and system management.In this paper, we propose a strategy to use the AS resources that provides a good performance and QoS support at a low cost. When the system is considered as a whole rather than each element being taken separately, it is possible to use only two virtual channels (VCs) at the switches to provide a service like that with many more VCs. As a result, we obtain a noticeable reduction of silicon area and arbitration time. Our proposal is fully compatible with the AS specification and permits us to provide an adequate performance both for typical multimedia applications and for best-effort traffic. 
53|7||Efficient FPGA implementation of DWT and modified SPIHT for lossless image compression|In this paper, we present an implementation of the image compression technique set partitioning in hierarchical trees (SPIHT) in programmable hardware. The lifting based Discrete Wavelet Transform (DWT) architecture has been selected for exploiting the correlation among the image pixels. In addition, we provide a study on what storage elements are required for the wavelet coefficients. A modified SPIHT (Set Partitioning in Hierarchical Trees) algorithm is presented for encoding the wavelet coefficients. The modifications include a simplification of coefficient scanning process, use of a 1-D addressing method instead of the original 2-D arrangement for wavelet coefficients and a fixed memory allocation for the data lists instead of the dynamic allocation required in the original SPIHT. The proposed algorithm has been illustrated on both the 2-D Lena image and a 3-D MRI data set and is found to achieve appreciable compression with a high peak-signal-to-noise ratio (PSNR). 
53|7||Efficient automatic gain control algorithm and architecture for wireless LAN receivers|The performance of a receiver front-end limits the quality and range of the given communication link. An appropriate design based on well-defined system parameters and architecture can make a huge difference in the performance, cost and marketability of the entire system. In particular, there is a need for improved digital automatic gain control (AGC) for use in multi-input multi-output orthogonal frequency division multiplexing (MIMO-OFDM) systems with application to wireless local area networks (WLANs), targeted for the upcoming 802.11n standard [Heejung Yu et al., IEEE 802.11 wireless LANs ETRI proposal specification for IEEE 802.11 TGn, IEEE 802.11 document, doc. No. 11-04-0923-00-000n, August, 2004; H. Yu, T. Jeon, S. Lee, Design of dual-band MIMO-OFDM system for next generation wireless LAN, in: IEEE International Conference on Communications (ICC), May, 2005]. In this paper, we propose an efficient algorithm and implementation of the digital AGC for next generation WLANs. The proposed AGC algorithm has two feedback loops for gain control to improve convergence speed, and at the same time maintains the stability of the AGC circuit. Also, a complete set of parameters for practical implementation is obtained by various experiments with fixed point constraints and accuracy requirements. 
53|7||Hardware-oriented ant colony optimization|A new kind of ant colony optimization (ACO) algorithm is proposed that is suitable for an implementation in hardware. The new algorithm – called Counter-based ACO – allows to systolically pipe artificial ants through a grid of processing cells. Various features of this algorithm have been designed so that it can be mapped easily to field-programmable gate arrays (FPGAs). Examples are a new encoding of pheromone information and a new method to define the decision sequence of ants. Experimental results that are based on simulations for the traveling salesperson problem and the quadratic assignment problem are presented to evaluate the proposed techniques. 
53|7||Effectiveness of caching in a distributed digital library system|Today independent publishers are offering digital libraries with fulltext archives. In an attempt to provide a single user-interface to a large set of archives, the studied Article-Database-Service offers a consolidated interface to a geographically distributed set of archives. While this approach offers a tremendous functional advantage to a user, the fulltext download delays caused by the network and queuing in servers make the user-perceived interactive performance poor.This paper studies how effective caching of articles at the client level can be achieved as well as at intermediate points as manifested by gateways that implement the interfaces to the many fulltext archives. A central research question in this approach is: What is the nature of locality in the user access stream to such a digital library? Based on access logs that drive the simulations, it is shown that client-side caching can result in a 20% hit rate. Even at the gateway level temporal locality is observable, but published replacement algorithms are unable to exploit this temporal locality. Additionally, spatial locality can be exploited by considering loading into cache all articles in an issue, volume, or journal, if a single article is accessed. But our experiments showed that improvement introduced a lot of overhead. Finally, it is shown that the reason for this cache behavior is the long time distance between re-accesses, which makes caching quite unfeasible. 
53|7||Systematic methodology for exploration of performance â Energy trade-offs in network applications using Dynamic Data Type refinement|Modern network applications require high performance and consume a lot of energy. Their inherent dynamic nature makes the dynamic memory subsystem a critical contributing factor to the overall energy consumption and to the execution time performance. This paper presents a novel, systematic methodology for generating performance-energy trade-offs by implementing optimal Dynamic Data Types, finely tuned and refined for network applications. Our systematic methodology is supported by a new, fully automated tool. We assess the effectiveness of the proposed approach in four representative, real-life case studies and provide significant energy savings and performance improvements compared to the original implementations. 
53|7||Achieving multipoint-to-multipoint fairness with RCNWA|In current Internet, applications employ lots of sessions with multiple connections between multiple senders and receivers. Sessions or users with more connections gain higher throughput. To obtain more network resource, applications tend to create more connections. It causes unfair bandwidth allocation by per-connection TCP rate allocation and the network suffers lots of TCP overheads. In this paper, we explore the issue on fair share allocation of bandwidth among sessions or users. Various fairness definitions are discussed in this study. Then, we propose a novel distributed scheme to achieve various fairness definitions. Simulation results show that our distributed scheme could achieve fair allocation according to each fairness definition. 
53|7||Quantum ternary parallel adder/subtractor with partially-look-ahead carry|Multiple-valued quantum circuits are a promising choice for future quantum computing technology since they have several advantages over binary quantum circuits. Binary parallel adder/subtractor is central to the ALU of a classical computer and its quantum counterpart is used in oracles – the most important part that is designed for quantum algorithms. Many NP-hard problems can be solved more efficiently in quantum using Grover algorithm and its modifications when an appropriate oracle is constructed. There is therefore a need to design standard logic blocks to be used in oracles – this is similar to designing standard building blocks for classical computers. In this paper, we propose quantum realization of a ternary full-adder using macro-level ternary Feynman and Toffoli gates built on the top of ion-trap realizable ternary 1-qutrit and Muthukrishnan–Stroud gates. Our realization has several advantages over the previously reported realization. Based on this realization of ternary full-adder we propose realization of a ternary parallel adder with partially-look-ahead carry. We also show the method of using the same circuit as a ternary parallel adder/subtractor. 
53|8|http://www.sciencedirect.com/science/journal/13837621/53/8|Editorial|
53|8||Application of deterministic and stochastic Petri-Nets for performance modeling of NoC architectures|The design of appropriate communication architectures for complex Systems-on-Chip (SoC) is a challenging task. One promising alternative to solve these problems are Networks-on-Chip (NoCs). Recently, the application of deterministic and stochastic Petri-Nets (DSPNs) to model on-chip communication has been proven to be an attractive method to evaluate and explore different communication aspects. In this contribution the modeling of basic NoC communication scenarios featuring different processor cores, network topologies and communication schemes is presented. In order to provide a testbed for the verification of modeling results a state-of-the-art FPGA-platform has been utilized. This platform allows to instantiate a soft-core processor network which can be adapted in terms of communication network topologies and communication schemes. It will be shown that DSPN modeling yields good communication performance prediction results at low modeling effort. Different DSPN modeling aspects in terms of accuracy and computational effort are discussed. 
53|8||Benchmarking mesh and hierarchical bus networks in system-on-chip context|The performance and area of a System-on-Chip depend on the utilized communication method. This paper presents simulation-based comparison of generic, synthesizable single bus, hierarchical bus, and 2-dimensional mesh on-chip networks. Performance of the network depends heavily on the application and therefore six test cases with multiple parameter values are used. Furthermore, two versions of each network topology are compared. The results show that hierarchical bus scales well to large number of agents and offers a good performance and area trade-off although it has smaller aggregate bandwidth and area than mesh. Hierarchical HIBI bus achieves runtimes comparable to 2-dimensional cut-through mesh with about 50% smaller network logic. However, depending on the test case, the runtime can be reduced by 20–50% when wider bus links are utilized. 
53|8||Exploiting program phase behavior for energy reduction on multi-configuration processors|Energy consumption is a major design issue for modern microprocessors. In previous work, several techniques were presented to reduce the overall energy consumption by dynamically adapting various hardware structures. Most approaches however lack the ability to deal efficiently with the configuration space explosion in case of multiple adaptive structures.In this paper, we present a mechanism that is able to deal with this configuration space problem. We first identify phases through profiling using a new phase classification method and determine the optimal hardware configuration per phase using an efficient offline search algorithm. During program execution, we inspect the phase behavior and adapt the hardware on a per-phase basis. Using SPEC2000 benchmarks, we show that the proposed mechanism achieves an energy reduction of 36% on average with an average performance degradation of only 2.9%. We also show that online processor configuration optimization is far less effective for multi-configuration processors, with an average energy reduction of less than 20% for comparable performance degradations. 
53|8||Ultra fast cycle-accurate compiled emulation of inorder pipelined architectures|Emulation of one architecture on another is useful when the architecture is under design, when software must be ported to a new platform or is being developed for systems which are still under development, or for embedded systems that insufficient resources to support the software development process. Emulation using an interpreter is typically slower than normal execution by up to 3 orders of magnitude. Our approach instead translates the program from the original architecture to another architecture while faithfully preserving its semantics at the lowest level. The emulation speeds are comparable to, and often faster than, programs running on the original architecture. Partial evaluation of architectural features is used to achieve such impressive performance, while permitting accurate statistics collection. Accuracy is at the level of the number of clock cycles spent executing each instruction (hence the description cycle-accurate). 
53|8||Rapid implementation and optimisation of DSP systems on FPGA-centric heterogeneous platforms|The emergence of programmable logic devices as single-chip heterogeneous processing platforms for digital signal processing applications poses challenges concerning rapid implementation and high level optimisation of algorithms on these platforms. This paper describes Abhainn, a rapid implementation methodology and toolsuite for translating an algorithmic expression of the system to a working solution on FPGA-centric embedded platforms. Two particular focuses for Abhainn are the automated but configurable realisation of inter-processor communication fabrics, and the establishment of novel dedicated hardware component design methodologies allowing algorithm level transformation for system optimisation. This paper outlines the approaches employed in these areas and demonstrates their effectiveness on high end signal processing beamforming applications. 
53|8||A scalable embedded JPEG 2000 architecture|The latest image compression standard, JPEG 2000 is well tuned for diverse applications, thus raising various throughput demands on its building blocks. Therefore, a JPEG 2000 encoder with the feature of scalability is favorable for its ability of meeting different throughput requirements. On the other hand, the large amounts of data streams underline the importance of bandwidth optimization in designing the encoder. The initial specification, especially in terms of loop organization and array indices, describes the data manipulations and, subsequently, influences the outcome of the architecture implementation. Therefore, there is a clear need for the exploiting support, and we believe the emphasis should lie on the loop level steering. In this paper, we apply loop transformation techniques to a scalable embedded JPEG 2000 encoder design during the architectural exploration stage, considering not only the balance of throughput among different blocks, but also the reduction of data transfer. The architecture is prototyped onto Xilinx FPGA. 
53|8||Optimizing data structures at the modeling level in embedded multimedia|Traditional design techniques for embedded systems apply transformations on the source code to optimize hardware-related cost factors. Unfortunately, such transformations cannot adequately deal with the highly dynamic nature of today’s multimedia applications. Therefore, we go one step back in the design process. Starting from a conceptual UML model, we first transform the model before refining it into executable code. This paper presents: various model transformations, an estimation technique for the steering cost parameters, and three case studies that show how our model transformations result in factors improvement in memory footprint and performance with respect to the initial implementation. 
53|9|http://www.sciencedirect.com/science/journal/13837621/53/9|Designing layoutâtiming independent quantum-dot cellular automata (QCA) circuits by global asynchrony|The concept of clocking for QCA, referred to as the four-phase clocking, is widely used. However, inherited characteristics of QCA, such as the way to hold state, the way to synchronize data flows, and the way to power QCA cells, make the design of QCA circuits quite different from VLSI and introduce a variety of new design challenges and the most severe challenges are due to the fact that the overall timing of a QCA circuit is mainly dependent upon its layout. This fact is commonly referred to as the “layout = timing” problem. To circumvent the problem, a novel self-timed QCA circuit design methodology referred to as the Globally Asynchronous, Locally Synchronous (GALS) Design for QCA is proposed in this paper. The proposed technique can significantly reduce the layout–timing dependency from the global network of QCA devices in a circuit; therefore, considerably flexible QCA circuit design and floorplanning will be possible. 
53|9||Low power data processing system with self-reconfigurable architecture|In this paper, a low power data processing system with a self-reconfigurable architecture and USB interface is presented. A single FPGA performs all processing and controls the multiple configurations without any additional elements, such as microprocessor, host computer or additional FPGAs. This architecture allows high performance with very low power consumption, a comprehensive alternative to microprocessor or DSP systems. In addition, a hierarchical reconfiguration system is used to support a large number of different processing tasks without the power consumption penalty of a big local configuration memory. Due to its simplicity and low power, this data processing system is especially suitable for portable applications, reducing the disadvantage of FPGAs against ASICS in low power consumption applications [A. Amara, F. Amiel, T. Ea, FPGA vs. ASIC for low power applications, Microelectronics Journal 37 (8) (2006) 669–677]. 
53|9||A new Mixed Radix Conversion algorithm MRC-II|In this paper, we present an efficient and simplified algorithm for the Residue Number System (RNS) conversion to weighted number system which in turn will simplify the implementation of RNS sign detection, magnitude comparison, and overflow detection. The algorithm is based on the Mixed Radix Conversion (MRC). The new algorithm simplifies the hardware implementation and improves the speed of conversion by replacing a number of multiplication operations with small look-up tables. The algorithm requires less ROM size compared to those required by existing algorithms. For a moduli set consisting of eight moduli, the new algorithm requires seven tables to do the conversion with a total table size of 519 bits, while Szabo and Tanaka MRC algorithm [N.S. Szabo, R.I. Tanaka, Residue Arithmetic and its Application to Computer Technology, McGraw-Hill, New York, 1967; C.H. Huang, A fully parallel mixed-radix conversion algorithm for residue number applications, IEEE Transactions on Computers c-32 (4) (1983)] requires 28 tables with a total table size of 8960 bits; and Huang MRC algorithm (Huang, 1983) requires 36 tables with a total table size of 5760 bits. 
53|9||Speculative trivialization point advancing in high-performance processors|Trivial instructions are those instructions whose output can be determined without performing the actual computation. This is due to the fact that for these instructions the output is often either one of the source operands or zero (e.g., addition with or multiplication by zero). In this work we study trivial instructions and use our findings to improve performance in high-performance processors.In particular, we introduce speculative trivialization point advancing to detect and bypass trivial instructions as soon as possible and as early as the decode stage. Consequently, we improve performance over a conventional processor (up to 30%) and a processor that detects and bypasses trivial instructions at their conventional point of trivialization (up to 5%). 
53|9||Real-time shape description system based on MPEG-7 descriptors|The paper presents a real-time shape description system based on MPEG-7 standard descriptors. The proposed system architecture is used for description in real-time image object’s shape basing on features such as contour and occupied region. The proposed hardware architecture splits the computational burden into several processes where calculations of mentioned object’s features are made simultaneously in order to improve system’s speed. A novel approximation method for estimation of peak’s height in the curvature scale space (CSS) representation of the shape as well as the artificial neural network (ANN) used in system development process for generalization of the data samples received from the approximation module. These methods make hardware realizations of main computational-consuming modules of the system more time efficient. 
53|9||A heuristic fault-tolerant routing algorithm in mesh using rectilinear-monotone polygonal fault blocks|A new, rectilinear-monotone polygonally shaped fault block model, called Minimal-Connected-Component (MCC), was proposed in [D. Wang, A rectilinear-monotone polygonal fault block model for fault-tolerant minimal routing in mesh, IEEE Trans. Comput. 52 (3) (2003) 310–320] for minimal adaptive routing in mesh-connected multiprocessor systems. This model refines the widely used rectangular model by including fewer non-faulty nodes in fault blocks. The positions of source/destination nodes relative to faulty nodes are taken into consideration when constructing fault blocks. Adaptive routing algorithm was given in Wang (2003), that constructs a minimal “Manhattan” route avoiding all fault blocks, should such routes exist. However, if there are no minimal routes, we still need to find a route, preferably as short as possible. In this paper, we propose a heuristic algorithm that takes a greedy approach, and can compute a nearly shortest route without much overhead. The significance of this algorithm lies in the fact that routing is a frequently performed task, and messages need to get to their destinations as soon as possible. Therefore one would prefer to have a fast answer about which route to take (and then take it), rather than spend too much time working out an absolutely shortest route. 
53|9||Tornado: A self-reconfiguration control system for core-based multiprocessor CSoPCs|In this work we present a self-reconfiguration control focused on multiprocessor core-based systems implemented on FPGA technology. An infrastructure of signals, protocols, interfaces and a controller is exposed to perform safe hardware/software reconfigurations. This infrastructure is part of the Tornado framework that includes other elements such as a multi-context assembler for a reconfigurable processor or a custom design flow developed for the Wishbone IP-Core interconnection specification. We present two applications where the presented control system has been applied, and it is compared with other available approaches. 
53|9||A multi-channel architecture for high-performance NAND flash-based storage system|Many mobile devices demand a large-capacity and high-performance storage system in order to store, retrieve, and process large multimedia data quickly. In this paper, we present a high-performance NAND flash-based storage system based on a multi-channel architecture. The proposed system consists of multiple independent channels, where each channel has multiple NAND flash memory chips. On this hardware, we investigate three optimization techniques to exploit I/O parallelism: striping, interleaving, and pipelining. By combining all the optimization techniques carefully, our system has shown 3.6 times higher overall performance compared to the conventional single-channel architecture. 
53|9||Multiprogrammed non-blocking checkpoints in support of optimistic simulation on myrinet clusters|CCL (checkpointing and communication library) is a software layer in support of optimistic parallel discrete event simulation (PDES) on myrinet-based COTS clusters. Beyond classical low latency message delivery functionalities, this library implements CPU offloaded, non-blocking (asynchronous) checkpointing functionalities based on data transfer capabilities provided by a programmable DMA engine on board of myrinet network cards. These functionalities are unique since optimistic simulation systems conventionally rely on checkpointing implemented as a synchronous, CPU-based data copy. Releases of CCL up to v2.4 only support monoprogrammed non-blocking checkpoints. This forces re-synchronization between CPU and DMA activities, which is a potential source of overhead, each time a new checkpoint request must be issued at the simulation application level while the last issued one is still being carried out by the DMA engine. In this paper we present a redesigned release of CCL (v3.0) that, exploiting hardware capabilities of more advanced myrinet clusters, supports multiprogrammed non-blocking checkpoints. The multiprogrammed approach allows higher degree of concurrency between checkpointing and other simulation specific operations carried out by the CPU, with benefits on performance. We also report the results of the experimental evaluation of those benefits for the case of a Personal Communication System (PCS) simulation application, selected as a real world test-bed. 
volume|issue|url|title|abstract
54|-|http://www.sciencedirect.com/science/journal/13837621/54|FPGA implementations of elliptic curve cryptography and Tate pairing over a binary field|Elliptic curve cryptography (ECC) and Tate pairing are two new types of public-key cryptographic schemes that become popular in recent years. ECC offers a smaller key size compared to traditional methods without sacrificing security level. Tate pairing is a bilinear map commonly used in identity-based cryptographic schemes. Therefore, it is more attractive to implement these schemes by using hardware than by using software because of its computational expensiveness. In this paper, we propose field programmable gate array (FPGA) implementations of the elliptic curve point multiplication in Galois field GF(2283)GF(2283) and Tate pairing computation in GF(2283)GF(2283). Experimental results demonstrate that, compared with previously proposed approaches, our FPGA implementations of ECC and Tate pairing can speed up by 31.6 times and 152 times, respectively. 
54|-||DLL-conscious instruction fetch optimization for SMT processors|Simultaneous multithreading (SMT) processors can issue multiple instructions from distinct processes or threads in the same cycle. This technique effectively increases the overall throughput by keeping the pipeline resources more occupied at the potential expense of reducing single thread performance due to resource sharing. In the software domain, an increasing number of dynamically linked libraries (DLL) are used by applications and operating systems, providing better flexibility and modularity, and enabling code sharing. It is observed that a significant amount of execution time in software today is spent in executing standard DLL instructions, that are shared among multiple threads or processes. However, for an SMT processor with a virtually-indexed cache implementation, existing instruction fetching mechanisms can induce unnecessary false I-TLB and I-Cache misses caused by the DLL-based instructions that are intended to be shared. This problem is more prominent when multiple independent threads are executing concurrently on an SMT processor.In this work, we investigate a neglected form of contention between running threads in the I-TLB and I-Cache (including both VIVT and VIPT) due to DLLs. To address these shortcomings, we propose a system level technique involving a light-weight modification in the microarchitecture and the OS. By exploiting the nature of the DLLs in our optimized system, we can reinstate the intended sharing of the DLLs in an SMT machine. Using Microsoft Windows based applications, our simulation results show that the optimized instruction fetching mechanism can reduce the number of DLL misses up to 5.5 times and improve the instruction cache hit rates by up to 62%, resulting in up to 30% DLL IPC improvements and up to 15% overall IPC improvements. 
54|-||A low-complexity microprocessor design with speculative pre-execution|Current superscalar architectures strongly depend on an instruction issue queue to achieve multiple instruction issue and out-of-order execution. However, the issue queue requires a centralized structure and mainly causes globally broadcasting operations to wakeup and select the instructions. Therefore, a large issue queue ultimately results in a low clock rate along with a high circuit complexity. In other words, the increasing demands for a larger issue queue correspondingly impose a significant burden on achieving a higher clock speed.This paper discusses our Speculative Pre-Execution Assisted by compileR (SPEAR), a low-complexity issue queue design. SPEAR is designed to manage the small window superscalar architecture more efficiently without increasing the window size. To this end, we have first recognized that the long memory latency is one of the factors that demand a large window, and we aim at achieving early execution of the miss-causing load instructions using another hierarchy of an issue queue. We pre-execute those miss-causing instructions speculatively as an additional prefetching thread. Simulation results show that the SPEAR design achieves performance comparable to or even better than what would be obtained in superscalar architectures with a large issue queue. However, SPEAR is designed with smaller issue queues which consequently can be implemented with low hardware complexity and high clock speed. 
54|-||A recursive method for synthesizing quantum/reversible quaternary parallel adder/subtractor with look-ahead carry|Multiple-valued quantum logic circuits are a promising choice for future quantum computing technology since they have several advantages over binary quantum logic circuits. Adder/subtractor is the major component of the ALU of a computer and is also used in quantum oracles. In this paper, we propose a recursive method of hand synthesis of reversible quaternary full-adder circuit using macro-level quaternary controlled gates built on the top of ion-trap realizable 1-qudit quantum gates and 2-qudit Muthukrishnan–Stroud quantum gates. Based on this quaternary full-adder circuit we propose a reversible circuit realizing quaternary parallel adder/subtractor with look-ahead carry. We also show the way of adapting the quaternary parallel adder/subtractor circuit to an encoded binary parallel adder/subtractor circuit by grouping two qubits together into quaternary qudit values. 
54|-||An area-efficient VLSI implementation for programmable FIR filters based on a parameterized divide and conquer approach|In this paper, we propose an optimal VLSI implementation for a class of programmable FIR filters with binary coefficients, whose architecture is based on a parameterized divide and conquer approach. The proposed design is shown to be easily extendable to FIR filters with multibit coefficients of arbitrary sign. The area efficiency achieved in comparison to direct form realization is demonstrated by VLSI implementation examples, synthesized in TSMC 0.18-Î¼m single poly six metal layer CMOS process using state-of-art VLSI EDA tools. The possible saving in average power consumption is estimated using gate-level power analysis. Suggestions for applications and topics for further research conclude the paper. 
54|-||Dual-mode floating-point adder architectures|Most modern microprocessors provide multiple identical functional units to increase performance. This paper presents dual-mode floating-point adder architectures that support one higher precision addition and two parallel lower precision additions. A double precision floating-point adder implemented with the improved single-path algorithm is modified to design a dual-mode double precision floating-point adder that supports both one double precision addition and two parallel single precision additions. A similar technique is used to design a dual-mode quadruple precision floating-point adder that implements the two-path algorithm. The dual-mode quadruple precision floating-point adder supports one quadruple precision and two parallel double precision additions. To estimate area and worst-case delay, double, quadruple, dual-mode double, and dual-mode quadruple precision floating-point adders are implemented in VHDL using the improved single-path and the two-path floating-point addition algorithms. The correctness of all the designs is tested and verified through extensive simulation. Synthesis results show that dual-mode double and dual-mode quadruple precision adders designed with the improved single-path algorithm require roughly 26% more area and 10% more delay than double and quadruple precision adders designed with the same algorithm. Synthesis results obtained for adders designed with the two-path algorithm show that dual-mode double and dual-mode quadruple precision adders requires 33% and 35% more area and 13% and 18% more delay than double and quadruple precision adders, respectively. 
54|-||Design space exploration of an open-source, IP-reusable, scalable floating-point engine for embedded applications|This paper describes an open-source and highly scalable floating-point unit (FPU) for embedded systems. Our FPU is fast and efficient, due to the high parallelism of its architecture: the functional units inside the datapath can operate in parallel and independently from each other. A comparison between different versions of the FPU has been made to highlight how performance scales accordingly. Logic synthesis results show that our FPU requires 105 Kgates and runs at 400 MHz on a low-power 90 nm std-cells low-power technology, and requires 20 K Logic Elements running at 67 MHz of an Altera Stratix FPGA. The proposed FPU is supported by a software tool suite which compiles programs written using the C/C++ language. A set of DSP and 3D graphics algorithms have been benchmarked, showing that using our FPU the amount of clock cycles required to perform each algorithm is one order of magnitude smaller than what is required by its corresponding software implementation. 
54|-||Optimizing CAM-based instruction cache designs for low-power embedded systems|Energy consumption and power dissipation are important concerns in the design of embedded systems and they will become even more crucial with finer process geometry, higher frequencies, deeper pipelines and wider issue designs. In particular, the instruction cache consumes more energy than any other processor module, especially with commonly used highly associative CAM-based implementations.Two energy-efficient approaches for highly associative CAM-based instruction cache designs are presented by means of using a segmented wordline and a predictor-based instruction fetch mechanism. The latter is based on the fact that not all instructions in a given I-cache fetch are used due to taken branches. The proposed Fetch Mask Predictor unit determines which instructions in a cache access will actually be used to avoid fetching any of the other instructions. Both proposed approaches are evaluated for an embedded 4-wide issue processor in 100 nm technology. Experimental results show average I-cache energy savings of 48% and overall processor energy savings of 19%. 
54|-||Unicast-based fault-tolerant multicasting in wormhole-routed hypercubes|A unicast-based fault-tolerant multicasting method is proposed for hypercubes, which can still work well when the system contains enough faults. A multicast message may be unable to reach a destination if Hamming distance between the destination and the multicast source is large enough. A multicast message fails if any one of the destinations is unreachable from the source. An effective destination ordering scheme of the destinations is proposed for one-port systems first, it is extended to all-port systems for unicast-based fault-tolerant multicasting. Unreachable destinations from the source based on the local safety information are forwarded to a reachable destination, where the multicast message can be routed reliably. Destination ordering is completed based on Hamming distance. A multiple round p-cube routing scheme is presented for a deadlock-free fault-tolerant routing for each unicast step in hypercubes, where the same virtual channel is used for each round of p-cube routing. Sufficient simulation results are presented by comparing with the previous methods. 
54|1-2|http://www.sciencedirect.com/science/journal/13837621/54/1-2|Announcing JSA embedded software design|
54|1-2||TSB: A DVS algorithm with quick response for general purpose operating systems|DVS is becoming an essential feature of state-of-the-art mobile processors. Interval-based DVS algorithms are widely employed in general purpose operating systems thanks to their simplicity and transparency. Such algorithms have a few problems, however, such as delayed response, prediction inaccuracies, and underestimation of the performance demand. In this paper we propose TSB (time slice based), a new DVS algorithm that takes advantage of the high transition speeds available in state-of-the-art processors. TSB adjusts processor performance at every context switch in order to match the performance demand of the next scheduled task. The performance demand of a task is predicted by analyzing its usage pattern in the previous time slice. TSB was evaluated and compared to other interval-based power management algorithms on the Linux kernel. The results show that TSB achieved similar or better energy efficiency compared to existing interval-based algorithms. In addition, TSB dramatically reduced the side effect of prolonging short-term execution times. For a task requiring 50 ms to run without a DVS algorithm, TSB prolonged the execution time by only 6% compared to results of 136% for CPUSpeed and 20% for Ondemand. 
54|1-2||Formal verification of ASMs using MDGs|We present a framework for the formal verification of abstract state machine (ASM) designs using the multiway decision graphs (MDG) tool. ASM is a state based language for describing transition systems. MDG provides symbolic representation of transition systems with support of abstract sorts and functions. We implemented a transformation tool that automatically generates MDG models from ASM specifications. Then formal verification techniques provided by the MDG tool, such as model checking or equivalence checking, can be applied on the generated models. We illustrate this work with the case study of an ATM switch controller, in which behavior and structure were specified in ASM and, using our ASM-MDG facility, are successfully verified with the MDG tool. 
54|1-2||Processor array architectures for flexible approximate string matching|In this paper, we present linear processor array architectures for flexible approximate string matching. These architectures are based on parallel realization of dynamic programming and non-deterministic finite automaton algorithms. The algorithms consist of two phases, i.e. preprocessing and searching. Then, starting from the data dependence graphs of the searching phase, parallel algorithms are derived, which can be realized directly onto special purpose processor array architectures for approximate string matching. Further, the preprocessing phase is also accommodated onto the same processor array designs. Finally, the proposed architectures support flexible patterns i.e. patterns with a “don’t care” symbol, patterns with a complement symbol and patterns with a class symbol. 
54|1-2||A novel caching mechanism for peer-to-peer based media-on-demand streaming|In recent years, peer-to-peer networks and application-level overlays without dedicated infrastructure have been widely proposed to provide on-demand media services on the Internet. However, the scalability issue, which is caused by the asynchronism and the sparsity of the online peers, is a major problem for deploying P2P-based MoD systems, especially when the media server’s capacity is limited. In this paper, we propose a novel probabilistic caching mechanism for P2P-based MoD systems. Theoretical analysis is presented to show that by engaging our proposed mechanism with a flexible system parameter, better scalability could be achieved by a MoD system with less workload imposed on the server, and the service capacity of the MoD system could be tradeoff with the peers’ gossip cost. We verify these properties with simulation experiments. Moreover, we show by simulation results that our proposed caching mechanism could improve the quality of the streaming service conceived by peers when the capacity of the server is limited, but will not cause notable performance degradation under highly lossy network environments, compared with the conventional continuous caching mechanism. 
54|1-2||RISC: A resilient interconnection network for scalable cluster storage systems|The explosive growth of data generated by information digitization has been identified as the key driver to escalate storage requirements. It is becoming a big challenge to design a resilient and scalable interconnection network which consolidates hundreds even thousands of storage nodes to satisfy both the bandwidth and storage capacity requirements. This paper proposes a resilient interconnection network for storage cluster systems (RISC). The RISC divides storage nodes into multiple partitions to facilitate the data access locality. Multiple spare links between any two storage nodes are employed to offer strong resilience to reduce the impact of the failures of links, switches, and storage nodes. The scalability is guaranteed by plugging in additional switches and storage nodes without reconfiguring the overall system. Another salient feature is that the RISC achieves a dynamic scalability of resilience by expanding the partition size incrementally with additional storage nodes along with associated two network interfaces that expand resilience degree and balance workload proportionally. A metric named resilience coefficient is proposed to measure the interconnection network. A mathematical model and the corresponding case study are employed to illustrate the practicability and efficiency of the RISC. 
54|1-2||Nash equilibria in bandwidth allocation for non-cooperative peer-to-peer networks|In peer-to-peer networks, peers act as clients and servers, i.e., they can download files from others and allow others to download from them, at the same time. Since the bandwidth of a peer acting as server is shared among all its clients, the download rate experienced by a peer depends on the server choices of the other peers.We focus our investigation on the bandwidth allocation among the peers and model this system with non-cooperative game theory. We assume that peers are rational players that maximize their utility, corresponding to minimize their download time. We study the existence of Nash equilibrium points under a very simple server selection policy and show that it is efficiency from the point of view of the bandwidth utilization. 
54|1-2||Pipelined circuit switching: Analysis for the torus with non-uniform traffic|Pipelined circuit switching (PCS) has been suggested as an efficient switching method for supporting interprocessor communication in multicomputer networks due to its ability to preserve both communication performance and fault-tolerant demands in such networks. The torus has been the underlying topology for a number of practical multicomputers. Analytical models of fully adaptive routing have recently been proposed for PCS in the torus under the uniform traffic pattern. However, there has not been any similar analytical model of PCS under the non-uniform traffic pattern, such as that generated by the presence of hot spots in the network. This paper proposes a new analytical model of PCS in the torus operating under the hot spot traffic pattern. Results from simulation experiments show close agreement with those predicted by the analytical model. 
54|1-2||Optimal load distribution in nondedicated heterogeneous cluster and grid computing environments|We consider optimal load distribution in a nondedicated cluster or grid computing system with heterogeneous servers processing both generic and dedicated applications. The goal of load balancing is to find an optimal load distribution strategy for generic tasks on heterogeneous servers preloaded by different amount of dedicated tasks such that the overall average response time of generic applications is minimized. The optimization problem is solved for three different queueing disciplines, namely, dedicated applications without priorities, prioritized dedicated applications without preemption, and prioritized dedicated applications with preemption. For each case, we derive equations that permit us to find optimal load distribution of generic tasks such that their average response time is minimized. 
54|1-2||Analyzing concurrency in streaming applications|We present a concurrency model that allows reasoning about concurrency in executable specifications of streaming applications. It provides measures for five different concurrency properties. The aim of the model is to provide insight into concurrency bottlenecks in an application and to provide global direction when performing implementation-independent concurrency optimization. The model focuses on task-level concurrency. A concurrency optimization method and a prototype implementation of a supporting analysis tool have been developed. We use the model and tool to optimize the concurrency in a number of multimedia applications. The results show that the concurrency model allows target-architecture-independent concurrency optimization. 
54|1-2||A new architecture for efficient hybrid representation of terrains|Interactive visualization of highly detailed terrain models is a challenging problem as the size of the data sets can easily exceed the capabilities of current hardware. Hybrid representation of terrains tries to solve the problem by combining the advantages in terms of reduced size associated with multiple levels-of-detail of digital elevation models with the high quality associated with the triangulated irregular networks. However, as the measurements of both representations have different origins, a direct representation of the hybrid system would result in discontinuities.In this paper, we present an architecture for hybrid representation of terrains based on a local convexification algorithm. The architecture we propose permits the generation of the additional triangles required to join the models and thereby avoid the discontinuities. The architecture is simple and regular and a high triangle generation rate is achieved. Different optimizations have been performed that avoid waiting cycles between tessellator units and so increase the productivity rate of the system. Implementation results are shown for a Virtex-II FPGA as an application example. 
54|1-2||A small data cache for multimedia-oriented embedded systems|This paper proposes a data cache with small space for low power, but high performance on multimedia applications. The basic architecture is a split-cache consisting of a direct-mapped cache with small block size (DMC) and a fully-associative buffer with large block size (FAB). To overcome the disadvantage caused by small cache areas, two hardware mechanisms are enhanced considering the operational behaviors of multimedia applications: an adaptive multi-block prefetching to initiate various fetch sizes for FAB and an efficient block filtering to remove the data likely to be rarely reused for DMC. The simulations on MediaBench show that the proposed 5kB cache can achieve up to 57% and 50% of power saving while providing almost equal and better performance compared with the 16kB 4-way set associative cache and 17kB stream caches, respectively. 
54|1-2||Configurable folded array for FIR filtering|The synthesis of configurable bit-plane processing array for FIR filtering is described in this paper. Possibilities for configuration are explored and encompassed by application of folding technique. The proposed folded architecture supports on-the-fly configuration of number of taps and coefficient length. This is achieved by dynamic operations mapping on the different hardware units in array structure. Dynamic operations mapping, involved in application of folding technique, allows recognition of user defined parameters, such as number of coefficients and coefficient length on implemented array size. The architecture provides flexible computations and offers the possibility of increasing the folded system throughput, by reducing the number of operations performed on a single functional unit, at cost of decreasing the coefficient number or length. Effects of folding technique application to architecture configuration capabilities are presented. The configurable folded FIR filter synthesis process is presented in detail. The obtained folded system architecture is described by block diagram, DFG, functional block diagram and the data flow diagram. The method of operation and operations mapping on the processing units are described. The algorithms for data reordering are given. With the aim to illustrate the functionality, configuration capabilities, and “trade-offs” relating to occupation of the chip resources and achieved throughputs of synthesized folded architecture, we present results of FPGA prototyping. The proposed configurable folded array is used for H.264/AVC deblocking filter implementation with extremely low-gate count that is achieved at the cost of time, but the design meets the requirement for real-time deblocking in mobile embedded computing platforms. 
54|1-2||Architectural designs for a scalable reconfigurable IP router|We present two flexible Internet Protocol (IP) router hardware (HW) architectures that enable a router to (1) readily expand in capacity according to network traffic volume growth; and (2) reconfigure its functionalities according to the hierarchical network layer in which it is placed. Reconfigurability is effectuated by a novel method called Methodology for Hardware Unity and an associated functional unit: Special Processing Agent, which can be built using state-of-the-art technology. More specifically, reconfiguration between an “edge” and a “hub” or “backbone” router can be done rapidly via a simple “OPEN/CLOSE” connection approach. Such architectures may, among other benefits, significantly extend the intervals between router HW upgrades for Internet services providers. They can serve as the basis for development of the next generation IP routers, and are directly applicable to the emerging concept of a single-layer IP network architecture. 
54|1-2||Applying neural networks to performance estimation of embedded software|High-level performance estimation of embedded software implemented in a particular processor is essential for a fast design space exploration, when the designer needs to evaluate different processor architectures (and their different versions) and also different task allocations in a multiprocessor system. The development of fast and adequate performance estimators is required to achieve the necessary speed in this design phase. However, advanced architectures present many features, such as pipelines, branch prediction mechanisms, and caches, which have a non-linear impact on the execution time, which thus becomes hard to evaluate using simple linear methods. In order to cope with this problem, this paper presents a high-level performance estimator based on a neural network, which easily adapts to the non-linear behaviour of the execution time in advanced architectures and presents a speed-up up to 190 times in comparison with cycle-accurate simulators, using the PowerPC 750 as target architecture. A method for automatic domain classification is proposed to group applications with similar characteristics, resulting in an increase of the estimation precision. For the PowerPC 750, the mean estimation error has been reduced from 7.90% to 6.41% thanks to domain-specific estimators. This precision level and the fast estimation time are suitable for high-level design space exploration, when different architectures or processor versions and different task allocations need to be evaluated in a fast way. 
54|1-2||Combinatorial performance modelling of toroidal cubes|Although several analytical models have been proposed in the literature for deterministic routing in different interconnection networks, very few of them have considered the effects of virtual channel multiplexing on network performance. This paper proposes a new analytical model to compute message latency in a general n-dimensional torus network with an arbitrary number of virtual channels per physical channel. Unlike the previous models proposed for toroidal-based networks, this model uses a combinatorial approach to consider all different possible cases for the source–destination pairs, thus resulting in an accurate prediction. The results obtained from simulation experiments confirm that the proposed model exhibits a high degree of accuracy for various network sizes, under different operating conditions, compared to a similar model proposed very recently, which considers virtual channel utilization in the k-ary n-cube network. 
54|1-2||Design of a hardware accelerator for path planning on the Euclidean distance transform|This paper presents a novel hardware-directed algorithm for finding a path for a mobile robot using the Euclidean distance transform of the binary image of an environment. The robot can translate as well as rotate. The path obtained from start to goal is the shortest in terms of the number of steps. The mapping of the algorithm to hardware is described. Results of efficient implementation on a Xilinx FPGA device show that the device can be operated at a clock rate of about 65 MHz. Such a high frequency of operation leads to computing a collision-free path on sample images of size 128 × 128 in less than 3 ms and hence the hardware can process images at a video rate. This is necessary for real-time path planning in a dynamic environment. 
54|1-2||A Java processor architecture for embedded real-time systems|Architectural advancements in modern processor designs increase average performance with features such as pipelines, caches, branch prediction, and out-of-order execution. However, these features complicate worst-case execution time analysis and lead to very conservative estimates. JOP (Java Optimized Processor) tackles this problem from the architectural perspective – by introducing a processor architecture in which simpler and more accurate WCET analysis is more important than average case performance.This paper presents a Java processor designed for time-predictable execution of real-time tasks. JOP is the implementation of the Java virtual machine in hardware. JOP is intended for applications in embedded real-time systems and the primary implementation technology is in a field programmable gate array. This paper demonstrates that a hardware implementation of the Java virtual machine results in a small design for resource-constrained devices. 
54|1-2||System level design of telecom systems using formal model refinement: Applying the B method/language in practice|The increasing complexity of modern telecommunication systems is one of the main issues encountered in most telecom products. Despite the plethora of methods and tools for efficient system design, verification and validation phases are still consuming significant part of the overall design time. The proposed approach outlines the use of the B method/language for producing correct-by-construction implementations of telecommunication systems. The method described is supported by appropriate tools that automate the process of proving that system properties are maintained during the various design stages. The feasibility of the latter is evaluated in practice through the design of a real world telecom application, borrowed from the domain of wireless telecommunication networks. 
54|1-2||Improving stability for peer-to-peer multicast overlays by active measurements|The instability of the tree-like multicast overlay caused by nodes’ abrupt departures is considered as one of the major problems for peer-to-peer (P2P) multicast systems. In this paper, we present a protocol for improving the overlay’s stability by actively estimating the nodes’ lifetime model, and combining the nodes’ lifetime information with the overlay’s structural properties. We use the shifted Pareto distribution to model the nodes’ lifetimes in designing our protocol. To support this model, we have measured the residual lifetimes of the nodes in a popular IPTV system named PPLive [PPLive. http://www.pplive.com], and have formally analyzed the relationships between the distribution of the nodes’ lifetimes, ages and their residual lifetimes under the shifted Pareto distribution model. We evaluate the overlay construction strategies, which are essential in improving the overlay’s stability in our protocol, by comparing them with a number of other strategies in simulation. The experimental results indicate that our proposed protocol could improve the overlay’s stability considerably, with informative but not necessarily accurate lifetime model estimation, and with limited overhead imposed on the network as well as negligible sacrifice regarding the end-to-end service latencies for the nodes on the overlay. 
54|1-2||DS2IS: Dictionary-based segmented inversion scheme for low power dynamic bus design|As technology scales down to nanometer technology, coupling effects between neighboring wires become very important, and have a significant impact on the power consumption of on-chip interconnects. Especially, on-chip inductive effects need to be taken into account due to low-resistance metal interconnections and faster clock rates in today’s SoC design. In this paper, we propose a low power dynamic bus encoding scheme which simultaneously reduces capacitive and inductive effects by the measurement of the real RLC model. Our experimental results show that our approach can save the power consumption of the bus up to 12%. 
54|1-2||A request distribution method for clustered VOD servers considering buffer sharing effects|The clustering of multiple video servers is a popular architecture for huge VOD systems. In a clustered VOD system, user requests are distributed among the video servers by the dispatcher. Each video server consists of independent disks and a local buffer memory. The buffer mechanism is recognized helpful to the improvement of the disk performance. However, its effects have not been effectively utilized so far for the request distribution strategies of the clustered VOD system. In this work, we investigate the effects of buffer sharing on the throughput of the clustered VOD system. And, based on the results, we propose a request distribution method for the clustered VOD system, where the buffer sharing effects are utilized for improving the system’s throughput. By experiments, we evaluate the performance of the proposed method compared with the conventional request distribution methods. 
54|10|http://www.sciencedirect.com/science/journal/13837621/54/10|FPGA implementation of high performance elliptic curve cryptographic processor over |In this paper, we propose a high performance elliptic curve cryptographic processor over GF(2163)GF(2163), one of the five binary fields recommended by National Institute of Standards and Technology (NIST) for Elliptic Curve Digital Signature Algorithm (ECDSA). The proposed architecture is based on the López–Dahab elliptic curve point multiplication algorithm and uses Gaussian normal basis for GF(2163)GF(2163) field arithmetic. To achieve high throughput rates, we design two new word-level arithmetic units over GF(2163)GF(2163) and derive parallelized elliptic curve point doubling and point addition algorithms with uniform addressing based on the López–Dahab method. We implement our design using Xilinx XC4VLX80 FPGA device which uses 24,263 slices and has a maximum frequency of 143 MHz. Our design is roughly 4.8 times faster with two times increased hardware complexity compared with the previous hardware implementation proposed by Shu et al. Therefore, the proposed elliptic curve cryptographic processor is well suited to elliptic curve cryptosystems requiring high throughput rates such as network processors and web servers. 
54|10||Design and implementation of a vehicle interface protocol using an IEEE 1394 network|A wide variety of in-vehicle devices such as camera sensors, navigation systems, telematics and communication equipments have been incorporated into a vehicle to realize Intelligent Transport Systems (ITS) applications. Because an efficient standardized network is required, ITS Data Bus (IDB) has been discussed to carry high-speed multimedia data for audio, video and other real-time ITS applications. For connecting devices in a standardized manner, the IDB network has architecture with a gateway called vehicle interface which is located between automaker’s proprietary network and the standardized IDB network. IEEE 1394 (also known as iLink or FireWire), which can transport multimedia data for consumer electronics, is a good candidate for IDB network. In this paper, we analyze the issues for existing AV/C protocol (application layer protocol over IEEE 1394) to comprise the IDB network. In addition, we designed and implemented the vehicle interface protocol as a higher layer of IEEE 1394 to address the AV/C protocol issues for realizing the whole IDB network architecture. 
54|10||Optimum RNS sign detection algorithm using MRC-II with special moduli set|In this paper, we present a generic sign detection algorithm based on mixed radix conversion algorithm, MRC-II [M. Akkal, P. Siy, A new mixed radix conversion algorithm MRC-II, Journal of System Architecture (2006)] and also we present an optimum algorithm for sign detection based on a special moduli set where mn is even. The described algorithm requires only one comparison for sign detection. A new moduli set will also be presented which simplifies MRC-II conversion algorithm by eliminating the need for table lookup normally used in MRC hardware implementation. The algorithm does not require ROM table like other algorithms. For a moduli set of four moduli that satisfies the special moduli set conditions, 0 tables are needed to do the conversion, while Szabo and Tanaka MRC algorithm [N.S. Szabo, R.I. Tanaka, Residue Arithmetic and Its Application to Computer Technology, McGraw-Hill, New York, 1967] requires 6 tables with a total table size of 4608 bits; and Huang MRC algorithm [C.H. Huang, A fully ParallelMixed-radix conversion algorithm for residue number applications, IEEE Transactions on Computers c-32 (4) (1983)] requires 10 tables with a total table size of 3840 bits. 
54|10||TTPM â An efficient deadlock-free algorithm for multicast communication in 2D torus networks|A torus network has become increasingly important to multicomputer design because of its many features including scalability, low bandwidth and fixed degree of nodes. A multicast communication is a significant operation in multicomputer systems and can be used to support several other collective communication operations. This paper presents an efficient algorithm, TTPM, to find a deadlock-free multicast wormhole routing in two-dimensional torus parallel machines. The introduced algorithm is designed such that messages can be sent to any number of destinations within two start-up communication phases; hence the name Torus Two Phase Multicast (TTPM) algorithm. An efficient routing function is developed and used as a basis for the introduced algorithm. Also, TTPM allows some intermediate nodes that are not in the destination set to perform multicast functions. This feature allows flexibility in multicast path selection and therefore improves the performance. Performance results of a simulation study on torus networks are discussed to compare TTPM algorithm with a previous algorithm. 
54|10||An efficient architecture for designing reverse converters based on a general three-moduli set|In this paper, a high-speed, low-cost and efficient design of reverse converter for the general three-moduli set {2Î±, 2Î² − 1, 2Î² + 1} where Î± < Î² is presented. The simple proposed architecture consists of a carry save adder (CSA) and a modulo adder. As a result it can be efficiently implemented in VLSI circuits. The values of Î± and Î² are set in order to provide the desired dynamic range and also to obtain a balanced moduli set. Based on the above, two new moduli sets {2n+k, 22n − 1, 22n + 1} and {22n−1, 22n+1 − 1, 22n+1 + 1}, which are the special cases of the moduli set {2Î±, 2Î² − 1, 2Î² + 1} are proposed. The reverse converters for these new moduli sets are derived from the proposed general architecture with better performance compared to the other reverse converters for moduli sets with similar dynamic range. 
54|10||PORCE: An efficient power off recovery scheme for flash memory|Flash memory is now replacing hard disk in many embedded applications including cellular phones, digital cameras, car navigation systems, and so on. However, because flash memory has its own characteristics such as “erase-before-write” and wear-leveling, a software layer called FTL (flash translation layer) should be provided. However, most FTL algorithms did not include the power off recovery module though it is very important in portable devices. In this paper, we suggest an efficient power off recovery scheme for flash memory called PORCE (Power Off Recovery sChEme for flash memory). PORCE is tightly coupled to FTL operations and minimizes performance degradation during normal operations by storing recovery information as small as possible. Additionally, PORCE provides cost-based reclamation protocols which include the wear-leveling module. Our empirical study shows that PORCE is an efficient recovery protocol. 
54|10||Execution coordination in mobile agent-based distributed job workflow execution|Mobile agent-based distributed job workflow execution requires the use of execution coordination techniques to ensure that an agent executing a subjob can locate its predecessors’ execution results. This paper describes the classification, implementation, and evaluation of execution coordination techniques in the mobile agent-based distributed job workflow execution system. First, a classification of the existing execution coordination techniques is developed for mobile agent systems. Second, to put the discussion into perspective, our framework for mobile agent-based distributed job workflow execution over the Grid (that is, MCCF: Mobile Code Collaboration Framework) is described. How the existing coordination techniques can be applied in the MCCF is also discussed. Finally, a performance study has been conducted to evaluate three coordination techniques using real and simulated job workflows. The results are presented and discussed in the paper. 
54|10||Modular array structure for non-restoring square root circuit|Square root is an operation performed by the hardware in recent generations of processors. The hardware implementation of the square root operation is achieved by different means. One of the popular methods is the non-restoring algorithm. In this paper, the classical non-restoring array structure is improved in order to simplify the circuit. This reduction is done by eliminating a number of circuit elements without any loss in the precision of the square root or the remainder. For a 64-bit non-restoring circuit the area of the suggested circuit is about 44% smaller than that of a conventional non-restoring array circuit. Furthermore, in order to create an environment for modular design of the non-restoring square root circuit, a number of modules are suggested. Using these modules it is possible to construct any square root circuit with an arbitrary number of input bits. The suggested methodology results in an expandable design with reduced-area. Analytical and simulation results show that the delay of the proposed circuit, for a 64-bit radicand, is 80% less than that of a conventional non-restoring array circuit. 
54|10||Some topological and combinatorial properties of WK-recursive mesh and WK-pyramid interconnection networks|The WK-recursive mesh and WK-pyramid networks are recursively defined hierarchical interconnection networks with excellent properties which well idealize them as alternatives for mesh and traditional pyramid interconnection topologies. They have received much attention due to their favorable attributes such as small diameter, large connectivity, and high degree of scalability and expandability. In this paper, we deal with pancyclicity and surface area of these networks. These properties are of great importance in the implementation of a variety of parallel algorithms in multicomputers. We show that WK-recursive mesh network is 1-partially pancyclic, i.e. any cycle of length 3, 4, 6,…, and N can be constructed in the WK-recursive mesh. Also, we prove that the WK-pyramid is pancyclic, that is all cycles of length 3, 4, … , and N can be formed in a WK-pyramid. It is also proved that two link-disjoint Hamiltonian paths/cycles can be embedded in the WK-recursive mesh/WK-pyramid. We then study the surface area of WK-recursive mesh and WK-pyramid networks and put forth some equations for calculating the surface area in these networks. 
54|10||Synthesis of quaternary reversible/quantum comparators|Multiple-valued quantum circuits are promising choices for future quantum computing technology, since they have several advantages over binary quantum circuits. Quaternary logic has the advantage that classical binary functions can be very easily represented as quaternary functions by grouping two bits together into quaternary values. Grover’s quantum search algorithm requires a sub-circuit called oracle, which takes a set of inputs and gives an output stating whether a given search condition is satisfied or not. Equality, less-than, and greater-than comparisons are widely used as search conditions. In this paper, we show synthesis of quaternary equality, less-than, and greater-than comparators on the top of ion-trap realizable 1-qudit gates and 2-qudit Muthukrishnan–Stroud gates. 
54|10||A novel hardware-oriented Kohonen SOM image compression algorithm and its FPGA implementation|Kohonen self-organizing map (K-SOM) has proved to be suitable for lossy compression of digital images. The major drawback of the software implementation of this technique is its very computational intensive task. Fortunately, the structure is fairly easy to convert into hardware processing units executing in parallel. The resulting hardware system, however, consumes much of a microchip’s internal resources, i.e. slice registers and look-up table units. This results in utilising more than a single microchip to realize the structure in pure hardware implementation. Previously proposed K-SOM realizations were mainly targetted on implementing on an application specific integrated circuit (ASIC) with low restriction on resource utilization. In this paper, we propose an alternative architecture of K-SOM suitable for moderate density FPGAs with acceptable image quality and frame rate. In addition, its hardware architecture and synthesis results are presented. The proposed K-SOM algorithm compromises between the image quality, the frame rate throughput, the FPGA’s resource utilization and, additionally, the topological relationship among neural cells within the network. The architecture has been proved to be successfully synthesized on a single moderate resource FPGA with acceptable image quality and frame rate. 
54|10||COMPASS â A tool for evaluation of compression strategies for embedded processors|A major concern of embedded system architects is the design for low power. We address one aspect of the problem in this paper, namely the effect of executable code compression. There are two benefits of code compression – firstly, a reduction in the memory footprint of embedded software, and secondly, potential reduction in memory bus traffic and power consumption. Since decompression has to be performed at run time it is achieved by hardware. We describe a tool called COMPASS which can evaluate a range of strategies for any given set of benchmarks and display compression ratios. Also, given an execution trace, it can compute the effect on bus toggles, and cache misses for a range of compression strategies. The tool is interactive and allows the user to vary a set of parameters, and observe their effect on performance. We describe an implementation of the tool and demonstrate its effectiveness. To the best of our knowledge this is the first tool proposed for such a purpose. 
54|10||A unified architecture for a public key cryptographic coprocessor|This paper presents a unified architecture for public key cryptosystems that can support the operations of the Rivest–Shamir–Adleman cryptogram (RSA) and the elliptic curve cryptogram (ECC). A hardware solution is proposed for operations over finite fields GF(p)GF(p) and GF(2p)GF(2p). The proposed architecture presents a unified arithmetic unit which provides the functions of dual-field modular multiplication, dual-field modular addition/subtraction, and dual-field modular inversion. A new adder based on the signed-digit (SD) number representation is provided for carry-propagated and carry-less operations. The critical path of the proposed design is reduced compared with previous full adder implementation methods. Experimental results show that the proposed design can achieve a clock speed of 1 GHz using 776 K gates in a 0.09 Î¼m CMOS standard cell technology, or 150 MHz using 5227 CLBs in a Xilinx Virtex 4 FPGA. While the different technologies, platforms and standards make a definitive comparison difficult, based on the performance of our proposed design, we achieve a performance improvement of between 30% and 250% when compared with existing designs. 
54|11|http://www.sciencedirect.com/science/journal/13837621/54/11|Editorial|
54|11||OpenMP-based parallelization on an MPCore multiprocessor platform â A performance and power analysis|In this contribution, the potential of parallelized software that implements algorithms of digital signal processing on a multicore processor platform is analyzed. For this purpose various digital signal processing tasks have been implemented on a prototyping platform i.e. an ARM MPCore featuring four ARM11 processor cores. In order to analyze the effect of parallelization on the resulting performance-power ratio, influencing parameters like e.g. the number of issued program threads have been studied. For parallelization issues the OpenMP programming model has been used which can be efficiently applied on C-level. In order to elaborate power efficient code also a functional and instruction level power model of the MPCore has been derived which features a high estimation accuracy. Using this power model and exploiting the capabilities of OpenMP a variety of exemplary tasks could be efficiently parallelized. The general efficiency potential of parallelization for multiprocessor architectures can be assembled. 
54|11||Exploration methodology of dynamic data structures in multimedia and network applications for embedded platforms|In the last years, there is a trend towards network and multimedia applications to be implemented in portable devices. These applications usually contain complex dynamic data structures. The appropriate selection of the dynamic data type (DDT) combination of an application affects the performance and the energy consumption of the whole system. Thus, DDT exploration methodology is used to perform trade-offs between design factors, such as performance and energy consumption. In this paper we provide a new approach to the DDT exploration procedure, based on a new library of DDTs which remedies the limitations of an existing solution and allows the DDT optimization of a wider range of application domains. Using the new library, we performed DDT exploration in network and multimedia benchmarks and achieved performance and energy consumption improvements up to 22% and 5.8%, respectively. 
54|11||Efficiency measures for SOC concepts|This paper discusses efficiency measures for the evaluation of high performance systems on a chip (SOC), considering a throughput rate R, chip size A, power dissipation P, and a flexibility criterion F. Based on the analysis of recently published multimedia chips, the paper shows equivalences between the ratio of R over AP, a weighted sum on 1/R, A, P, and a fuzzy multicriteria analysis on R, A, P. The paper indicates the fuzzy multicriteria analysis as generalization of the other efficiency measures, which can be easily extended to multiple cost and performance criteria. Because of the application of fuzzy set theory, the multicriteria approach supports quantitative criteria with a physical background as well as qualitative criteria by linguistic variables. Uncertainties at early conceptual phases are considered by a possibilistic measure, which is based on fuzzy set theory. 
54|11||Improving evolutionary exploration to area-time optimization of FPGA designs|This paper presents a new methodology based on evolutionary multi-objective optimization (EMO) to synthesize multiple complex modules on reprogrammable devices. It starts from a behavioral description written in a common high-level language (for instance C) to automatically produce the register-transfer level (RTL) design in a hardware description language (e.g. Verilog). Since all high-level synthesis problems (scheduling, resource allocation and binding) are notoriously NP-complete and interdependent, these problems should be considered simultaneously. This drives to a wide design space, that needs to be thoroughly explored to obtain solutions able to satisfy the design constraints (e.g. area and performance).Since evolutionary algorithms are good candidates to tackle such complex explorations, in this paper we provide a solution based on the non-dominated sorting genetic algorithm (NSGA-II) to explore the design space and obtain the best solutions in terms of performance given the area constraints of a target reprogrammable device, for instance a Field Programmable Gate Array (FPGA). To further reduce the time needed for the exploration, that theoretically requires the complete logic synthesis of each visited point, the evaluation of the solutions have been speed-up by using two techniques: a good cost estimation model and a technique to exploit fitness inheritance by substituting the expensive actual evaluations with estimations based on closeness in an hypothetical design space.We show on the JPEG case study that the proposed approach provides good results in terms of trade-off between total area occupied and execution time. The results shows also that the Pareto-optimal set obtained by applying the proposed fitness inheritance model well approximates the set obtained without the proposed technique and reduces the overall execution time up to the 25% in average. 
54|11||Resource conflict detection in simulation of function unit pipelines|Processor simulators are important parts of processor design toolsets in which they are used to verify and evaluate the properties of the designed processors. While simulating architectures with independent function unit pipelines using simulation techniques that avoid the overhead of instruction bit-string interpretation, such as compiled simulation, the simulation of function unit pipelines can become one of the new bottlenecks for simulation speed. This paper evaluates several resource conflict detection models, commonly used in compiler instruction scheduling, in the context of function unit pipeline simulation. The evaluated models include the conventional reservation table based-model, the dynamic collision matrix model, and an finite state automata (FSA) based model. In addition, an improvement to the simulation initialization time by means of lazy initialization of states in the FSA-based approach is proposed. The resulting model is faster to initialize and provides comparable simulation speed to the actively initialized FSA. 
54|11||Secure communication in microcomputer bus systems for embedded devices|The protection of the microcomputer bus system in embedded devices is essential to prevent eavesdropping and the growing number of todays hardware hacking attacks. This contribution presents a hardware solution to ensure microcomputer bus systems via the Tree Parity Machine Rekeying Architecture (TPMRA). For this purpose a scalable TPMRA IP-core is designed and implemented in order to meet adaptability, low cost terms and variable bus performance requirements. It allows the authentication of different bus participants as well as the encryption of chip-to-chip buses from a single primitive. The solution is transparent and easy applicable to an arbitrary microcomputer bus system for embedded devices on the market. A proof of concept implementation shows the applicability of the TPMRA in the standardized Advanced Microprocessor Bus Architecture (AMBA) by implementing the IP-core extension into the peripheral AMBA bus-to-bus interface. It will be shown that the proposed solution is latency free and can be easy implemented into the AMBA bus interface bridge in order to protect the ARM bus system with a low hardware overhead considering all AMBA bus features. 
54|3-4|http://www.sciencedirect.com/science/journal/13837621/54/3-4|Editorial|
54|3-4||Quality-driven model-based architecture synthesis for real-time embedded SoCs|The recent spectacular progress in modern microelectronics created a big stimulus towards development of embedded systems. Unfortunately, it also introduced unusual complexity which results in many serious issues that cannot be resolved without new more adequate development methods and electronic design automation tools for the system-level design. This paper discusses the problem of an efficient model-based multi-objective optimal architecture synthesis for complex hard real-time embedded systems, when using as an example a system-level architecture exploration and synthesis method that we developed. 
54|3-4||Analyzing composability of applications on MPSoC platforms|Modern day applications require use of multi-processor systems for reasons of performance, scalability and power efficiency. As more and more applications are integrated in a single system, mapping and analyzing them on a multi-processor platform becomes a multi-dimensional problem. Each possible set of applications that can be concurrently active leads to a different use-case (also referred to as scenario) that the system has to be verified and tested for. Analyzing the feasibility and resource utilization of all possible use-cases becomes very demanding and often infeasible.Therefore, in this paper, we highlight this issue of being able to analyze applications in isolation while still being able to reason about their overall behavior – also called composability. We make a number of novel observations about how arbitration plays an important role in system behavior. We compare two commonly used arbitration mechanisms, and highlight the properties that are important for such analysis. We conclude that none of these arbitration mechanisms provide the necessary features for analysis. They either suffer from scalability problems, or provide unreasonable estimates about performance, leading to waste of resources and/or undesirable performance.We further propose to use a Resource Manager (RM) to ensure applications meet their performance requirements. The basic functionalities of such a component are introduced. A high-level simulation model is developed to study the performance of RM, and a case study is performed for a system running an H.263 and a JPEG decoder. The case study illustrates at what granularity of control a resource manager can effectively regulate the progress of applications such that they meet their performance requirements. 
54|3-4||Designing efficient irregular networks for heterogeneous systems-on-chip|Networks-on-chip will serve as the central integration platform in future complex systems-on-chip (SoC) designs, composed of a large number of heterogeneous processing resources. Most researchers advocate the use of traditional regular networks like meshes, tori or trees as architectural templates which gained a high popularity in general-purpose parallel computing. However, most SoC platforms are special-purpose tailored to the domain-specific requirements of their application. They are usually built from a large diversity of heterogeneous components which communicate in a very specific, mostly irregular way.In this work, we propose a methodology for the design of customized irregular networks-on-chip, called INoC. We take advantage of a priori knowledge of the communication characteristic of the application to generate an optimized network topology and routing algorithm. We show that customized irregular networks are clearly superior to traditional regular architectures in terms of performance at comparable implementation costs for irregular workloads. Even more, they inherently offer a high degree of scalability and expansibility which allows to adapt the network to an arbitrary number of nodes with a given communication demand. This can normally not be accomplished by traditional approaches. 
54|3-4||A monitoring-aware network-on-chip design flow|Networks-on-chip (NoC) are a scalable interconnect solution for systems on chip and are rapidly becoming reality. Monitoring is a key enabler for debugging or performance analysis and quality-of-service techniques. The NoC design problem and the NoC monitoring problem cannot be treated in isolation. We propose a monitoring-aware NoC design flow able to take into account the monitoring requirements in general. We illustrate our flow with a debug driven monitoring case study of transaction monitoring. By treating the NoC design and monitoring problems in synergy, the area cost of monitoring can be limited to 3–20% in general. We also investigate run-time configuration options for the NoC monitoring system resulting in acceptable configuration times. 
54|3-4||Resource-efficient routing and scheduling of time-constrained streaming communication on networks-on-chip|Network-on-chip-based multiprocessor systems-on-chip are considered as future embedded systems platforms. One of the steps in mapping an application onto such a parallel platform involves scheduling the communication on the network-on-chip. This paper presents different scheduling strategies that minimize resource usage by exploiting all scheduling freedom offered by networks-on-chip. It also introduces a technique to take the dynamism in applications into account when scheduling the communication of an application on the network-on-chip while minimizing the resource usage. Our experiments show that resource-utilization is improved when compared to existing techniques. 
54|3-4||Deadlock free routing algorithms for irregular mesh topology NoC systems with rectangular regions|The simplicity of regular mesh topology Network on Chip (NoC) architecture leads to reductions in design time and manufacturing cost. A weakness of the regular shaped architecture is its inability to efficiently support cores of different sizes. A proposed way in literature to deal with this is to utilize the region concept, which helps to accommodate cores larger than the tile size in mesh topology NoC architectures. Region concept offers many new opportunities for NoC design, as well as provides new design issues and challenges. One of the most important among these is the design of an efficient deadlock free routing algorithm. Available adaptive routing algorithms developed for regular mesh topology cannot ensure freedom from deadlocks. In this paper, we list and discuss many new design issues which need to be handled for designing NoC systems incorporating cores larger than the tile size. We also present and compare two deadlock free routing algorithms for mesh topology NoC with regions. The idea of the first algorithm is borrowed from the area of fault tolerant networks, where a network topology is rendered irregular due to faults in routers or links, and is adapted for the new context. We compare this with an algorithm designed using a methodology for design of application specific routing algorithms for communication networks. The application specific routing algorithm tries to maximize adaptivity by using static and dynamic communication requirements of the application. Our study shows that the application specific routing algorithm not only provides much higher adaptivity, but also superior performance as compared to the other algorithm in all traffic cases. But this higher performance for the second algorithm comes at a higher area cost for implementing network routers. 
54|3-4||Energy reduction through crosstalk avoidance coding in networks on chip|Commercial designs are currently integrating from 10 to 100 embedded processors in a single system on chip (SoC) and the number is likely to increase significantly in the near future. With this ever increasing degree of integration, design of communication architectures for large, multi-core SoCs is a challenge. Traditional bus-based systems will no longer be able to meet the clock cycle requirements of these big SoCs. Instead, the communication requirements of these large multi processor SoCs (MP-SoCs) are convened by the emerging network-on-chip (NoC) paradigm. Crosstalk between adjacent wires is an important signal integrity issue in NoC communication fabrics and it can cause timing violations and extra energy dissipation. Crosstalk avoidance codes (CACs) can be used to improve the signal integrity by reducing the effective coupling capacitance, lowering the energy dissipation of wire segments. As NoCs are built on packet-switching, it is advantageous to modify data packets by including coded bits to protect against the negative effects of crosstalk. By incorporating crosstalk avoidance coding in NoC data streams and organizing the CAC-encoded data packets in an efficient manner, so that total number of encoding/decoding operations can be reduced over the communication channel, we are able to achieve lower communication energy, which in turn will help to decrease the overall energy dissipation. 
54|3-4||Dependable design technique for system-on-chip|A technique for highly reliable digital design for two FPGAs under a processor control is presented. Two FPGAs are used in a duplex configuration system design, but better dependability parameters are obtained by the combination of totally self-checking blocks based on a parity predictor. Each FPGA can be reconfigured when a SEU fault is detected. This reconfiguration is controlled by a control unit implemented in a processor. Combinational circuit benchmarks have been considered in all our experiments and computations. All our experimental results are obtained from a XILINX FPGA implementation using EDA tools. The dependability model and dependability calculations are presented to document the improved reliability parameters. 
54|3-4||Mixed hierarchical-functional fault models for targeting sequential cores|Current work presents a set of fault models allowing high coverage for sequential cores in systems-on-a-chip. We propose a novel approach combining a hierarchical fault model for functional blocks, a functional fault model for multiplexers and a mixed hierarchical-functional fault model for comparison operators, respectively. The fault models are integrated into a fast high-level decision diagram based test path activation tool. According to the experiments, the proposed method significantly outperforms state-of-the-art test pattern generation tools. The main new contribution of this paper is a formal definition of high-level decision diagram representations and the combination of the three fault models in order to target high gate-level stuck-at fault coverage for sequential cores. 
54|5|http://www.sciencedirect.com/science/journal/13837621/54/5|Speedups from extending embedded processors with a high-performance coarse-grained reconfigurable data-path|In this paper, an embedded system that extends microprocessor cores with a high-performance coarse-grained reconfigurable data-path is introduced. The data-path have been previously introduced by the authors. It is composed by computational resources able to realize complex operations which aid in improving the performance of time critical application parts, called kernels. A compilation flow is defined for mapping high-level software descriptions to the microprocessor system. The kernel code is mapped using a properly developed mapping algorithm for the reconfigurable data-path, while the non-critical segments are executed on the microprocessor. Extensive exploration is performed by mapping four real-life applications on six different instances of the system. The results show that the speedup from executing kernels on the reconfigurable logic ranges from 6.3 to 154.3, relative to the software execution on the processor since the available processing elements of the data-path are efficiently utilized. Important overall application speedups, due to the kernels’ acceleration, have been reported for the four applications. These overall performance improvements range from 1.70 to 3.70 relative to an all-processor execution. Furthermore, the experiments show that the proposed data-path achieves faster kernels’ execution compared with other high-performance data-paths. 
54|5||A versatile timing unit for traffic shaping, policing and charging in packet-switched networks|Timing has a key role in several traffic control functions encountered in modern packet-switched networks. In order to be effective, a timing unit must provide fine resolution, be simple to implement and scale well with the number of controlled traffic streams. This paper addresses the design, implementation and evaluation of a timing unit that can support accurate and efficient implementations of traffic shaping, policing and charging in packet-switched networks. The timing unit is implemented in hardware and, therefore, overcomes constraints associated with software-based timers. It accommodates a pool of independently-clocked timers and counters, organised in timing blocks, and, consequently, is able to support, in parallel, traffic streams with diverse timing requirements. The design supports shaping and policing through token buckets, leaky buckets and a scheme, variation of the token bucket, that aims at providing statistical quality of service guarantees by exploiting the effective rate concept. Charging is supported by dedicated counters that measure the utilization of the effective rate. The granularity of the timing unit is adjustable in run-time to adapt to changes in the rate parameters of the shaping and policing functions. The validation of the timing unit is done through the development of a prototype board consisting of programmable hardware and embedded software blocks. The temporal resolution of the timing unit and the advantages of the hardware/software co-design are experimentally evaluated. 
54|5||Using supplier locality in power-aware interconnects and caches in chip multiprocessors|Conventional snoopy-based chip multiprocessors take an aggressive approach broadcasting snoop requests to all nodes. In addition each node checks all received requests. This approach reduces the latency of cache to cache transfer misses at the expense of increasing power. In this paper we show that a large portion of interconnect/cache transactions are redundant as many snoop requests miss in the remote nodes.We exploit this inefficiency and introduce power optimization techniques for chip multiprocessors. Our optimizations rely on the observation that in a snoopy-based shared memory system the data supplier can be predicted with high accuracy. Our optimizations reduce power by eliminating unnecessary activity at both the requester and the supplier end of snoop requests.We reduce power as we (a) avoid broadcasting snoop requests to all processors and (b) avoid tag lookup for all nodes and for all requests arriving. In particular, we use supplier locality and introduce the following two optimizations.First, and at the requester end, we introduce speculative selective request (SSR) to reduce power dissipation in the binary tree interconnect. In SSR, we send the request only to the node more likely to have the missing data. We reduce power as we limit access only to the interconnect components between the requestor and the supplier node.Second, and at the supplier end, we propose speculative tag lookup (STL) to reduce power dissipation in data caches. We filter those accesses more likely to miss in the L1 cache.Using shared memory applications, we show that by limiting snoop requests to the speculated nodes we reduce interconnect power by 25% in a four-way multiprocessor. Moreover, we show that speculative tag lookup reduces power in tag arrays by 14.1% in a four-way multiprocessor. Both optimizations come with negligible performance loss and hardware overhead. 
54|5||Task scheduling in multiprocessing systems using duplication|"Task scheduling continues to be one of the most challenging problems in both parallel and distributed computing environments. In this paper, we present a task scheduling algorithm, which uses duplication, to optimally schedule any application represented in the form of a directed acyclic graph (DAG). It has a time complexity of O(d|V|3)O(d|V|3), where â£Vâ£ represents the number of tasks and d the maximum indegree of tasks. "
54|5||A multilevel partitioning approach for efficient tasks allocation in heterogeneous distributed systems|This work addresses the problem of allocating parallel application tasks to heterogeneous distributed computing resources, such as multiclusters or Grid environments. The proposed allocation scheme is based on a multilevel graph partitioning and mapping approach. The objective is to find an efficient allocation that minimizes the application completion time, subject to the specified constraints pertinent to the application and system environment. The allocation scheme consists of three phases; the clustering phase, the initial mapping phase and the refinement and remapping phase. The scheme introduces an efficient heuristic in the clustering phase for contracting (coarsening) large size application graphs to the number of processors, called the VHEM method. An initial mapping technique based on a tabu-search approach has been introduced as a basis for the process of refinement and remapping phase. The simulation study shows that the VHEM coarsening heuristic can achieve optimal or near-optimal communication, compared to the HEM method, when the ratio of the number of tasks to the number of processors exceeds a threshold value. The simulation study shows that those optimal or near-optimal VHEM-coarsened graphs have an effect of generating very efficient mappings, when they are compared to the HEM-coarsened graphs. 
54|6|http://www.sciencedirect.com/science/journal/13837621/54/6|Editorial|
54|6||Model-typed component interfaces|Component based software engineering (CBSE) allows to design and develop reusable software components that can be assembled to construct software systems via well defined interfaces. However, designing such reusable components for data intensive business logic often requires heavy data transfer between components over interfaces. Static interface definitions using basic data types or structures of such lead to large interfaces susceptible to modifications. The goal of this paper is to present model-typed interfaces based on generic interface parameters, which allows to transfer complex structured data between components. Providing such generic, model-defined types (MDT) with data models specifying the parameter structure supports compatibility checks of model-typed interfaces at platform independent system design time. The methodology is described platform independently and the coherency with our system development process is discussed. Moreover, a technology mapping to IDL and the CORBA component model (CCM) is illustrated. 
54|6||Architecting reconfigurable component-based operating systems|Dynamic reconfiguration allows modifying a system during its execution, and can be used to apply patches and updates, to implement adaptive systems, dynamic instrumentation, or to support third-party modules. Dynamic reconfiguration is important in embedded systems, where one does not necessarily have the luxury to stop a running system. While several proposals have been presented in the literature supporting dynamic reconfiguration in operating system kernels, these proposals in general hardwire a fixed reconfiguration mechanism, which may be far from optimal in certain configurations.In this paper, we present a software-architecture-based approach to the construction of reconfigurable operating systems, and we show that it allows us (i) to support different mechanisms for dynamic reconfiguration, and (ii) to select between them at build time, with little or no changes in operating system and application components. Our approach relies on the use of a reflective component model and of its associated architecture description language. 
54|6||A product management challenge: Creating software product value through requirements selection|It is important for a software company to maximize value creation for a given investment. The purpose of requirements engineering activities is to add business value that is accounted for in terms of return on investment of a software product. This paper provides insight into the release planning processes used in the software industry to create software product value, by presenting three case studies. It examines how IT professionals perceive value creation through requirements engineering and how the release planning process is conducted to create software product value. It also presents to what degree the major stakeholders’ perspectives are represented in the decision-making process. Our findings show that the client and market base of the software product represents the most influential group in the decision to implement specific requirements. This is reflected both in terms of deciding the processes followed and the decision-making criteria applied when selecting requirements for the product. Furthermore, the management of software product value is dependant on the context in which the product exists. Factors, such as the maturity of the product, the marketplace in which it exists, and the development tools and methods available, influence the criteria that decide whether a requirement is included in a specific project or release. 
54|6||A model for service-oriented communication systems|Using innovative protocols at the transport or network layer is difficult today. Even if such protocols become available, most applications are not able to utilize them because usage of TCP/IP is hard coded into the application. Service-oriented communication systems (SOCS) aim to decouple applications from lower level protocols. Therefore, a service-oriented interface between applications and the transport layer is introduced. A broker mediates transport service requests to appropriate configurations of transport service providers. A flexible and protocol independent specification schema for defining service requirements and offers is regarded as a key element for such an interface. The specification schema enables short and simple descriptions as well as detailed and sophisticated descriptions and can thus scale with information available about service providers, network status, as well as application and user requirements. 
54|6||Authentication in stealth distributed hash tables|Most existing DHT algorithms assume that all nodes have equal capabilities. This assumption has previously been shown to be untrue in real deployments, where the heterogeneity of nodes can actually have a detrimental effect upon performance. We now acknowledge that nodes on the same overlay may also differ in terms of their trustworthiness. However, implementing and enforcing security policies in a network where all nodes are treated equally is a non-trivial task. We therefore extend our previous work on Stealth DHTs to consider the differentiation of nodes based on their trustworthiness rather than their capabilities alone. 
54|7|http://www.sciencedirect.com/science/journal/13837621/54/7|A methodology to design arbitrary failure detectors for distributed protocols|Nowadays, there are many protocols able to cope with process crashes, but, unfortunately, a process crash represents only a particular faulty behavior. Handling tougher failures (e.g. sending omission failures, receive omission failures, arbitrary failures) is a real practical challenge due to malicious attacks or unexpected software errors. This is usually achieved either by changing, in an ad hoc manner, the code of a crash resilient protocol or by devising a new protocol from scratch. This paper proposes an alternative methodology to detect processes experiencing arbitrary failures. On this basis, it introduces the notions of liveness failure detector and safety failure detector as two independent software components. With this approach, the nature of failures experienced by processes becomes transparent to the protocol using the components. This methodology brings a few advantages: it makes possible to increase the resilience of a protocol designed in a crash failure context without changing its code by concentrating only on the design of a few well-specified components, and second, it clearly separates the task of designing the protocol from the task of detecting faulty processes, a methodological improvement. Finally, the feasibility of this approach is shown, by providing an implementation of liveness failure detectors and of safety failure detectors for two protocols: one solving the consensus, and the second solving the problem of global data computation. 
54|7||High-performance computing of  for a vector of inputs  on Alpha and IA-64 CPUs|The modern microprocessors have become more sophisticated, the performance of software on modern architectures has grown more and more difficult to dissect and prognosticate. The execution of a program nowadays entails the complex interaction of code, compiler and processor micro-architecture. The built-in functions to compute 1/x or exp(±x)exp(±x) of math library and hardware are often incapable of achieving the challenging performance of high-performance numerical computing. To meet this demand, the current trend in constructing high-performance numerical computing for specific processors Alpha 21264 & 21364, and IA-64 has been optimized for 1/xi and exp(±xi)exp(±xi) for a vector of inputs xixi which is significantly faster than optimized library routines. A detailed deliberation of how the processor micro-architecture as well as the manual optimization techniques improve the computing performance has been developed. 
54|7||Evaluation and optimization of a peer-to-peer video-on-demand system|Video-on-demand (VoD) is increasingly popular with internet users. However, VoD is costly due to the load placed on video servers. Peer-to-peer (P2P) techniques are an approach to alleviating server load through peer-assisted sharing. Existing studies on P2P VoD are mostly based on simulation and focus on areas such as overlay topology, but little is known about the effectiveness of P2P in a real VoD system.In this paper we present a comprehensive measurement study of GridCast, a deployed experimental P2P VoD system. Using a 2-month log of GridCast, we evaluate its scalability and end user experience. Motivated by the observations on user behavior and unused peer resource, we further optimize its performance. Our key findings are: (1) a moderate number of concurrent users can derive satisfactory user experience. However, good network bandwidth at peers and adequate server provisioning are still critical to good user experience; (2) a simple prefetching algorithm can be effective to improve random seeks; (3) a simple caching across multiple videos has great potential to further improve system scalability. Overall, we believe that it is feasible to provide a cost-effective P2P VoD service with acceptable user experience, and there is a fundamental tradeoff between good user experience and system scalability. 
54|7||A proposal for managing ASI fabrics|Recent years, computer performance has been significantly increased. As a consequence, data I/O systems have become bottlenecks within systems. To alleviate this problem, Advanced Switching was recently proposed as a new standard for future interconnects. The Advanced Switching specification establishes a fabric management infrastructure, which is in charge of updating the set of fabric paths each time a topological change takes place. The use of source routing and passive switches makes unfeasible the adaptation to this new technology of many existing proposals to handle topological changes in switched interconnection networks. This paper presents a fabric management mechanism for Advanced Switching, but also suitable for other source routing interconnects. Furthermore, the work presents a detailed performance evaluation for this proposal. This evaluation allows us to identify the main drawbacks of the mechanism and to define future improvements. 
54|7||A quantitative analysis of the .NET common language runtime|Microsoft’s .NET platform has been developed to simplify development of Windows applications. The execution environment at the heart of this platform is a virtual machine known as the common language runtime (or CLR). The goal of this paper is to present a comprehensive behavioral analysis of the CLR instruction set and the high level language support. This will aid in the development of a hardware implementation of the CLR, similar to techniques applied to the Java virtual machine. The pertinent data is extracted using a profiling application while executing a benchmark application. We have analyzed this data with respect to access patterns for data types, addressing modes, instruction set utilization, execution time requirements, method invocation behavior and the effects of object orientation. Conclusions and recommendations are presented that will aid in the future development of a hardware implementation. 
54|7||Optimized reversible binary-coded decimal adders|Babu and Chowdhury [H.M.H. Babu, A.R. Chowdhury, Design of a compact reversible binary coded decimal adder circuit, Journal of Systems Architecture 52 (5) (2006) 272–282] recently proposed, in this journal, a reversible adder for binary-coded decimals. This paper corrects and optimizes their design. The optimized 1-decimal BCD full-adder, a 13 × 13 reversible logic circuit, is faster, and has lower circuit cost and less garbage bits. It can be used to build a fast reversible m-decimal BCD full-adder that has a delay of only m + 17 low-power reversible CMOS gates. For a 32-decimal (128-bit) BCD addition, the circuit delay of 49 gates is significantly lower than is the number of bits used for the BCD representation. A complete set of reversible half- and full-adders for n-bit binary numbers and m-decimal BCD numbers is presented. The results show that special-purpose design pays off in reversible logic design by drastically reducing the number of garbage bits. Specialized designs benefit from support by reversible logic synthesis. All circuit components required for optimizing the original design could also be synthesized successfully by an implementation of an existing synthesis algorithm. 
54|7||A predecoding technique for ILP exploitation in Java processors|Java processors have been introduced to offer hardware acceleration for Java applications. They execute Java bytecodes directly in hardware. However, the stack nature of the Java virtual machine instruction set imposes a limitation on the achievable execution performance. In order to exploit instruction level parallelism and allow out of order execution, we must remove the stack completely. This can be achieved by recursive stack folding algorithms, such as OPEX, which dynamically transform groups of Java bytecodes to RISC like instructions. However, the decoding throughputs that are obtained are limited. In this paper, we explore microarchitectural techniques to improve the decoding throughput of Java processors. Our techniques are based on the use of a predecoded cache to store the folding results, so that it could be reused. The ultimate goal is to exploit every possible instruction level parallelism in Java programs by having a superscalar out of order core in the backend being fed at a sustainable rate. With the use of a predecoded cache of 2 × 2048 entries and a 4-way superscalar core we have from 4.8 to 18.3 times better performance than an architecture employing pattern based folding. 
54|8|http://www.sciencedirect.com/science/journal/13837621/54/8|Cost-driven repair optimization of reconfigurable nanowire crossbar systems with clustered defects|With the recent development of nanoscale materials and assembly techniques, it is envisioned to build high-density reconfigurable systems which have never been achieved by the photolithography. Various reconfigurable architectures have been proposed based on nanowire crossbar structure as the primitive building block. Unfortunately, high-density systems consisting of nanometer-scale elements are likely to have many imperfections and variations; thus, defect tolerance is considered as one of the most exigent challenges. In this paper, we evaluate three different logic mapping algorithms with defect tolerance to circumvent clustered defective crosspoints in nanowire reconfigurable crossbar architectures. The effectiveness of inherited redundancy and configurability utilization is demonstrated through extensive parametric simulations. Then, costs associated with the repair process are analyzed and a method to find the most cost-effective repair solution is presented. 
54|8||Enhanced-functionality multipliers|High-speed arithmetic units in modern processors are expected to support multiplication operations with integers, fractions, and floating-point numbers. This paper presents hardware designs that can perform three modes of multiplication: (1) A double-width multiplication that returns a 2n2n-bit product. (2) A single-width integer multiplication that returns the n least-significant product bits and an overflow signal. (3) A truncated-fractional multiplication that returns the n most-significant product bits. The presented multipliers achieve up to 50% reduced power dissipation in integer and truncated-fractional multiplication modes of operation. For 16-bit or greater operand sizes the enhanced-functionality multiplier (EFM) designs use less than 10% more hardware compared to the conventional multipliers. 
54|8||A unified fault-tolerant routing scheme for a class of cluster networks|Large cluster systems with thousands of nodes have become a cost-effective alternative to traditional supercomputers. In these systems cluster nodes are interconnected using high-degree switches. Regular direct interconnection network topologies including tori (k-ary n-cubes) and meshes are among adapted choices for interconnecting these high-degree switches. We propose a generalized fault-tolerant routing scheme for highly connected regular interconnection networks and derive conditions for its applicability. The scheme is based on the availability of efficiently identifiable disjoint routes between network nodes. When routing paths become faulty, alternative disjoint routes are identified and taken. The methods used to identify the routing paths, to propagate failure information, and to switch from a routing path to another incur little communication and computation overhead. If the faults occur reasonably apart in time, then packets are efficiently routed along paths of minimal or near-minimal lengths. In the unlikely case where several faults occur in a short period of time, the scheme still delivers packets but possibly along longer paths. The proposed scheme and its properties are first presented in general terms for any interconnection topology satisfying certain derived connectivity conditions. The applicability of the general scheme is then illustrated on examples of well known regular topologies satisfying the derived connectivity conditions including the binary hypercube, the k-ary n-cube and the star graph networks. 
54|8||A software defined approach for common baseband processing|We present a novel software defined approach for designing and implementing common baseband processing tasks. Our focus is on exploring the algorithmic and architectural design spaces of 3G and 4G systems to identify the computational and geometric structures shared by diverse coding schemes, services and hardware platforms, and the efficient and flexible integration of these structures on innovative extensible hardware. With an existing protocol processor design framework as our starting point, we add flexibility to the physical layer of the radio application domain by defining a methodology and a hardware platform for designing programmable open wireless architecture-enabled device instances. The proposed methodology executes in two phases: (a) initial design, which is done to a single standard using our design principles and methods, and (b) extension phase where the system upgrade is done component by component. The approach standardizes control structures, component abstractions, implementation of the architecture itself as well as methods for interactive optimization. Thus, in both design phases there is only a need to consider changes in component functionality and connectivity. We demonstrate the approach by initially targeting digital television, and then extending the system with minimal effort to support GSM. The costs of the GSM extension in the system were an area increase of 2.4%, a power increase of 2.7% and four days in hardware design and verification. 
54|8||Exploring the performance impact of stripe size on network attached storage systems|Network attached storage (NAS) integrates redundant array of independent disks (RAID) subsystem that consists of multiple disk drives to aggregate storage capacity, I/O performance and reliability based on data striping and distribution. Traditionally, the stripe size is an important parameter that has a great influence on the RAID subsystem performance, whereas the performance impact has been changed due to the development of disk drive technologies and some I/O optimization methods. Based on disk drive access time, this paper constructs a performance analysis model to exploit the impact of some I/O optimization approaches including sub-commands combination, storage interface augment, and I/O scatter/gather on the stripe size of NAS. The analysis results and experimental validation indicate that due to the evolution of hardware and software, the stripe size has a negligible performance impact on NAS when the disk drives involved are organized in a RAID0 pattern. 
54|8||On the effectiveness of phase based regression models to trade power and performance using dynamic processor adaptation|Microarchitecture optimizations, in general, exploit the gross program behavior for performance improvement. Programs may be viewed as consisting of different “phases” which are characterized by variation in a number of processor performance metrics. Previous studies have shown that many of the performance metrics remain nearly constant within a “phase”. Thus, the change in program “phases” may be identified by observing the change in the values of these metrics. This paper aims to exploit the time varying behavior of programs for processor adaptation. Since the resource usage is not uniform across all program “phases”, the processor operates at varying levels of underutilization. During phases of low available Instruction Level Parallelism (ILP), resources may not be fully utilized while in other phases, more resources may be required to exploit all the available ILP. Thus, dynamically scaling the resources based on program behavior is an attractive mechanism for power–performance trade-off. In this paper we develop per-phase regression models to exploit the phase behavior of programs and adequately allocate resources for a target power–performance trade-off. Modeling processor performance–power using such a regression model is an efficient method to evaluate an architectural optimization quickly and accurately. We also show that the per-phase regression model is better suited than an “unified” regression model that does not use phase information. Further, we describe a methodology to allocate processor resources dynamically by using regression models which are developed at runtime. Our simulation results indicate that average energy savings of 20% can be achieved with respect to a maximally configured system with negligible impact on performance for most of the SPEC-CPU and MEDIA benchmarks. 
54|8||Memory hierarchy performance measurement of commercial dual-core desktop processors|As chip multiprocessor (CMP) has become the mainstream in processor architectures, Intel and AMD have introduced their dual-core processors. In this paper, performance measurement on an Intel Core 2 Duo, an Intel Pentium D and an AMD Athlon 64 × 2 processor are reported. According to the design specifications, key derivations exist in the critical memory hierarchy architecture among these dual-core processors. In addition to the overall execution time and throughput measurement using both multi-program-med and multi-threaded workloads, this paper provides detailed analysis on the memory hierarchy performance and on the performance scalability between single and dual cores. Our results indicate that for better performance and scalability, it is important to have (1) fast cache-to-cache communication, (2) large L2 or shared capacity, (3) fast L2 to core latency, and (4) fair cache resource sharing. Three dual-core processors that we studied have shown benefits of some of these factors, but not all of them. Core 2 Duo has the best performance for most of the workloads because of its microarchitecture features such as the shared L2 cache. Pentium D shows the worst performance in many aspects due to its technology-remap of Pentium 4 without taking the advantage of on-chip communication. 
54|8||A BCD-based architecture for fast coordinate rotation|Although radix 10 based arithmetic has been gaining renewed importance over the last few years, decimal systems are not efficient enough and techniques are still under development. In this paper, an improvement of the CORDIC (coordinate rotation digital computer) method for decimal representation is proposed and applied to produce fast rotations. The algorithm uses BCD operands as inputs, combining the advantages of both decimal and binary systems. The result is a reduction of 50% in the number of iterations if compared with the original Decimal CORDIC method. Finally, we present a hardware architecture useful to produce BCD coordinates rotations accurately and fast, and different experiments demonstrating the advantages of the new method are shown. A reduction of 75% in a single stage delay is obtained, whereas the circuit area just increases in about 5%. 
54|9|http://www.sciencedirect.com/science/journal/13837621/54/9|Parallel, distributed and network-based processing|
54|9||A business process monitor for a mobile phone recharging system|Dependable (i.e. accurate and timely) monitoring is a key aspect of business process management, since it provides information which is crucial for determining the actual Quality of Service (QoS) delivered to individual parties, and for promptly handling off-plan deviations. This paper describes a business process monitor for the recharging system of a mobile phone network provider. The monitored system is currently in operation for the major mobile phone company in Italy, namely Telecom Italia Mobile (TIM). Due to the amazingly high throughput of the monitored system, meeting the performance requirements for the monitor was a challenging issue. A buffer-based implementation of the monitor system failed to meet such requirements. In this paper, we propose a stream-based architecture, which exceeds the performance requirements imposed by the monitored application. The paper provides a detailed description of the monitor system architecture, including a discussion of technology choices, and an experimental evaluation of the performance boost achieved by resorting to a streaming approach. The proposed solution also exploits grammar-based pluggable parsers for rapid and seamless integration of heterogeneous data feeds. 
54|9||Simulation-based development of Peer-to-Peer systems with the RealPeer methodology and framework|In the process of developing Peer-to-Peer (P2P) systems, simulation has proved to be an essential tool for the evaluation of existing and conceived P2P systems. So far, in practice, there has been a clear separation between a simulation model of a P2P system and a real P2P system that operates on a real physical network. This separation hinders the transition of models to real systems and the evaluation of already deployed systems by means of simulation.To bridge this gap, we put forward the idea of simulation-based development of P2P systems. In this approach, an initial simulation model of a P2P system is iteratively transformed into the intended real P2P system. As a concretion of this approach, we propose a methodology and a framework for the simulation-based development of P2P systems. The presented framework effectively supports a developer in modelling, simulating and ultimately developing P2P systems. We demonstrate the validity of our approach and the framework by constructing an example P2P application. This application is simulated in a series of experiments as well as deployed in a large-scale internet-based P2P system. 
54|9||Failure-tolerant distributed storage with compressed (1 out-of N) codes|Deletion-tolerant codes provide data availability despite storage failures and are commonly used for disk arrays and reliable storage in distributed systems. The codes used for that base on binary parity or on sophisticated cyclic codes with minimal storage overhead. But the calculations for these codes cause either a noticeable number of computation cycles or require a huge number of logic gates. In this paper, a different class of deletion-tolerant codes – compressed (1 out-of N) codes – are analyzed with a focus on their application for distributed storage systems. It is shown that these codes when combined with compression can provide nearly the same low storage overhead as the traditional codes and allow a proper parallelization. Several variants of the code within the design space are discussed. A projection of the (1 out-of N) coding principle to a distributed protocol across storage units shows that the compression effort can be hidden in the communication and storage principles. 
54|9||Securing skeletal systems with limited performance penalty: The muskel experience|Algorithmic skeletons have been exploited to implement several parallel programming environments, targeting workstation clusters as well as workstation networks and computational grids. When targeting non-dedicated clusters, workstation networks and grids, security has to be taken adequately into account in order to guarantee both code and data confidentiality and integrity. However, introducing security is usually an expensive activity, both in terms of the effort required to managed security mechanisms and in terms of the time spent performing security related activities at run time.We discuss the cost of security introduction as well as how some features typical of skeleton technology can be exploited to improve the efficiency code and data securing in a typical skeleton based parallel programming environment and we evaluate the performance cost of security mechanisms implemented exploiting state of the art tools. In particular, we take into account the cost of security introduction in muskel, a Java based skeletal system exploiting macro data flow implementation technology. We consider the adoption of mechanisms that allow securing all the communications involving remote, unreliable nodes and we evaluate the cost of such mechanisms. Also, we consider the implications on the computational grains needed to scale secure and insecure skeletal computations. 
54|9||An agent based platform for task distribution in virtual environments|This paper focuses on automatic mechanisms for task distribution and execution in virtual and mobile environments. In particular, the goal is the implementation of Utility Computing services that enable users to submit their source code and to have their applications executed without concerning about resource allocation, task distribution, and load-balancing.The proposed solution consists in a distributed agent-based software infrastructure that grants a high level of transparency from the user point of view. As a matter of fact, accordingly with the Utility Computing model, the user has just to submit its tasks and their input parameters; after that, the software infrastructure takes care of (1) encapsulating tasks in mobile agents; (2) distributing them in the virtual environment; (3) launching and controlling execution; (4) picking up results; and (5) handling computing stations.Finally, it is important to note that the infrastructure is able to integrate both fixed and mobile hardware resources to build a community of computing stations for task executions. As a consequence, such an infrastructure can get useful computing power even from stations (mobile devices) that have always been neglected by classic task execution platforms. 
54|9||Activity pre-scheduling for run-time optimization of grid workflows|The capability to support resource sharing between different organizations and high-level performance are noteworthy features of grid computing. Applications require significant design effort and complex coordination of resources to define, deploy and execute components on heterogeneous and often unknown resources. A common trend today aims at diffusing workflow management techniques to reduce the complexity of grid systems through model-driven approaches that significantly simplify application design through the composition of distributed services often belonging to different organizations. With this approach, the adoption of efficient workflow enactors becomes a key aspect to improve efficiency through run-time optimizations, so reducing the burden for the developer, who is only responsible of defining the functional aspects of complex applications since he/she has only to identify the activities that characterize the application and the causal relationships among them. This paper focuses on performance improvements of grid workflows by presenting a new pattern for workflow design that ensures activity pre-scheduling at run-time through a technique that generates fine-grained concurrency with a couple of concepts: asynchronous invocation of services and continuation of execution. The technique is implemented in a workflow enactment service that dynamically optimizes process execution with a very limited effort for application developer. 
volume|issue|url|title|abstract
55|-|http://www.sciencedirect.com/science/journal/13837621/55|Analysis of network processing workloads|Network processing is becoming an increasingly important paradigm as the Internet moves towards an architecture with more complex functionality in the data path. Modern routers not only forward packets, but also process headers and payloads to implement a variety of functions related to security, performance, and customization. It is important to get a detailed understanding of the workloads associated with this processing in order to be able to develop efficient network processing engines. We present a tool called PacketBench, which provides a framework for implementing network processing applications and obtaining an extensive set of workload characteristics. For statistics collection, PacketBench provides the ability to derive a number of microarchitectural and networking related metrics. We show a range of workload results that focus on individual packets and the variation between them. The understanding of workload details of network processing has many practical applications. We discuss how PacketBench results can be used to estimate network processing delay that are very close to those obtained from measurement. 
55|-||Access region cache with register guided memory reference partitioning|Wide-issue and high-frequency processors require not only a low-latency but also high-bandwidth memory system to achieve high performance. Previous studies have shown that using multiple small single-ported caches instead of a monolithic large multi-ported one for L1 data cache can be a scalable and inexpensive way to provide higher bandwidth. Various schemes on how to direct the memory references have been proposed in order to achieve a close match to the performance of an ideal multi-ported cache. However, most existing designs seldom take dynamic data access patterns into consideration, thus suffer from access conflicts within one cache and unbalanced loads between the caches. It is observed in this paper that if one can group data references defined in a program into several regions (access regions) to allow parallel accesses, providing separate small caches – access region cache for these regions may prove to have better performance. A register-guided memory reference partitioning approach is proposed and it effectively identifies these semantic regions and organizes them into multiple caches adaptively to maximize concurrent accesses. The base register name, not its content, in the memory reference instruction is used as a basic guide for instruction steering. With the initial assignment to a specific access region cache per the base register name, a reassignment mechanism is applied to capture the access pattern when program is moving across its access regions. In addition, a distribution mechanism is introduced to adaptively enable access regions to extend or shrink among the physical caches to reduce potential conflicts further. The simulations of SPEC CPU2000 benchmarks have shown that the semantic-based scheme can reduce the conflicts effectively, and obtain considerable performance improvement in terms of IPC; with 8 access region caches, 25–33% higher IPC is achieved for integer benchmark programs than a comparable 8-banked cache, while the benefit is less for floating-point benchmark programs, 19% at most. 
55|-||Reusability-aware cache memory sharing for chip multiprocessors with private L2 caches|In this paper, we propose a novel on-chip L2 cache organization for chip multiprocessors (CMPs) with private L2 caches. The proposed approach, called reusability-aware cache sharing (RACS), combines the advantages of both a private L2 cache and a shared L2 cache. Since a private L2 cache organization has a short access latency, the RACS scheme employs a private L2 cache organization. However, when a cache block in a private L2 cache is selected for eviction, RACS first evaluates its reusability. If the block is likely to be reused in the near future, it may be saved to a peer L2 cache which has space available. In this way, the RACS scheme effectively simulates the larger capacity of a shared L2 cache. Simulation results show that RACS reduced the number of off-chip memory accesses by 24% compared to a pure private L2 cache organization on average for the SPLASH 2 multi-threaded benchmarks, and by 16% for multi-programmed benchmarks. 
55|-||Accurate and efficient processor performance prediction via regression tree based modeling|Computer architects usually evaluate new designs using cycle-accurate processor simulation. This approach provides a detailed insight into processor performance, power consumption and complexity. However, only configurations in a subspace can be simulated in practice due to long simulation time and limited resource, leading to suboptimal conclusions which might not be applied to a larger design space. In this paper, we propose a performance prediction approach which employs state-of-the-art techniques from experiment design, machine learning and data mining. According to our experiments on single and multi-core processors, our prediction model generates highly accurate estimations for unsampled points in the design space and show the robustness for the worst-case prediction. Moreover, the model provides quantitative interpretation tools that help investigators to efficiently tune design parameters and remove performance bottlenecks. 
55|-||A secure digital camera architecture for integrated real-time digital rights management|This paper presents a novel concept of a secure digital camera (SDC) with a built-in watermarking and encryption facility. The motivation is to facilitate real-time digital rights management (DRM) by using the SDC right at the source end of multimedia content. The emerging field of DRM systems addresses the issues related to the intellectual-property rights of digital content. The use of digital watermarking along with encryption for effective DRM is proposed. In this context, a novel discrete cosine transform domain invisible-robust watermarking method that uses cryptography and watermarking methods simultaneously to provide a double-layer of protection to digital media is presented. The proposed method securely hides binary images in color image media and securely extracts and authenticates it by using a secret key. Experimental results prove that the proposed technique is resilient to stringent watermarking attacks. Hence, it is an effective method for providing protection of ownership rights. The corresponding application-specific architectures for invisible-robust watermarking and Rijndael advanced encryption standard (AES) towards the prototyping of the SDC are presented. The proposed architectures are modeled and synthesized for field programmable gate array (FPGA). The soft cores in the form of hardware description language resulting from this research can serve as intellectual-property core and can be integrated with any multimedia-producing electronic appliances which are built as embedded systems using system-on-a-chip (SoC) technology. 
55|1|http://www.sciencedirect.com/science/journal/13837621/55/1|Rapid design of area-efficient custom instructions for reconfigurable embedded processing|RISPs (Reconfigurable Instruction Set Processors) are increasingly becoming popular as they can be customized to meet design constraints. However, existing instruction set customization methodologies do not lend well for mapping custom instructions on to commercial FPGA architectures. In this paper, we propose a design exploration framework that provides for rapid identification of a reduced set of profitable custom instructions and their area costs on commercial architectures without the need for time consuming hardware synthesis process. A novel clustering strategy is used to estimate the utilization of the LUT (Look-Up Table) based FPGAs for the chosen custom instructions. Our investigations show that the area costs computations using the proposed hardware estimation technique on 20 custom instructions are shown to be within 8% of those obtained using hardware synthesis. A systematic approach has been adopted to select the most profitable custom instruction candidates. Our investigations show that this leads to notable reduction in the number of custom instructions with only marginal degradation in performance. Simulations based on domain-specific application sets from the MiBench and MediaBench benchmark suites show that on average, more than 25% area utilization efficiency (performance/area) can be achieved with the proposed technique. 
55|1||Improving energy efficiency for flash memory based embedded applications|The JFFS2 file system for flash memory compresses files before actually writing them into flash memory. Because of this, multimedia files, for instance, which are already compressed in the application level go through an unnecessary and time-consuming compression stage and cause energy waste. Also, when reading such multimedia files, the default use of disk cache results in unnecessary main memory access, hence an energy waste, due to the low cache hit ratio. This paper presents two techniques to reduce the energy consumption of the JFFS2 flash file system for power-aware applications. One is to avoid data compression selectively when writing files, and the other is to bypass the page caching when reading sequential files. The modified file system is implemented on a PDA running Linux and the experiment results show that the proposed mechanism effectively reduces the overall energy consumption when accessing continuous and large files. 
55|1||Protocol offload analysis by simulation|In the last years, diverse network interface designs have been proposed to cope with the link bandwidth increase that is shifting the communication bottleneck towards the nodes in the network. The main point behind some of these network interfaces is to reach an efficient distribution of the communication overheads among the different processing units of the node, thus leaving more host CPU cycles for the applications and other operating systems tasks. Among these proposals, protocol offloading searches for an efficient use of the processing elements in the network interface card (NIC) to free the host CPU from network processing. The lack of both, conclusive experimental results about the possible benefits and a deep understanding of the behavior of these alternatives in their different parameter spaces, have caused some controversy about the usefulness of this technique.The contributions of this paper deal with the implementation and evaluation of offloading strategies and with the need of accurate tools for researching the computer system issues that, as networking, require the analysis of interactions among applications, operating system, and hardware. Thus, in this paper, a way to include timing models in a full-system simulator (Simics) to provide a suitable tool for network subsystem simulation is proposed.Moreover, we compare two kinds of simulators, a hardware description language level simulator and a full-system simulator (including our proposed timing models), in the analysis of protocol offloading at different levels. We also explain the results obtained from the perspective of the previously described LAWS model and propose some changes in this model to get a more accurate approach to the experimental results. From these results, it is possible to conclude that offloading allows a relevant throughput improvement in some circumstances that can be qualitatively predicted by the LAWS model. 
55|1||A comparative evaluation of hybrid distributed shared-memory systems|Distributed Shared-Memory (DSM) systems are shared-memory multiprocessor architectures in which each processor node contains a partition of the shared memory. In hybrid DSM systems coherence among caches is maintained by a software-implemented coherence protocol relying on some hardware support. Hardware support is provided to satisfy every node hit (the common case) and software is invoked only for accesses to remote nodes.In this paper we compare the design and performance of four hybrid distributed shared memory (DSM) organizations by detailed simulation of the same hardware platform. We have implemented the software protocol handlers for the four architectures. The handlers are written in C and assembly code. Coherence transactions are executed in trap and interrupt handlers. Together with the application, the handlers are executed in full detail in execution-driven simulations of six complete benchmarks with coarse-grain and fine-grain sharing. We relate our experience implementing and simulating the software protocols for the four architectures.Because the overhead of remote accesses is very high in hybrid systems, the system of choice is different than for purely hardware systems. 
55|1||Exploiting an abstract-machine-based framework in the design of a Java ILP processor|Abstract machines bridge the gap between the high-level of programming languages and the low-level mechanisms of a real machine. The paper proposed a general abstract-machine-based framework (AMBF) to build instruction level parallelism processors using the instruction tagging technique. The constructed processor may accept code written in any (abstract or real) machine instruction set, and produce tagged machine code after data conflicts are resolved. This requires the construction of a tagging unit which emulates the sequential execution of the program using tags rather than actual values. The paper presents a Java ILP processor by using the proposed framework. The Java processor takes advantage of the tagging unit to dynamically translate Java bytecode instructions to RISC-like tag-based instructions to facilitate the use of a general-purpose RISC core and enable the exploitation of instruction level parallelism. We detailed the Java ILP processor architecture and the design issues. Benchmarking of the Java processor using SpecJVM98 and Linpack has shown the overall ILP speedup improvement between 78% and 173%. 
55|1||IP Routing table compaction and sampling schemes to enhance TCAM cache performance|Routing table lookup is an important operation in packet forwarding. This operation has a significant influence on the overall performance of the network processors. Routing tables are usually stored in main memory which has a large access time. Consequently, small fast cache memories are used to improve access time. In this paper, we propose a novel routing table compaction scheme to reduce the number of entries in the routing table. The proposed scheme has three versions. This scheme takes advantage of ternary content addressable memory (TCAM) features. Two or more routing entries are compacted into one using don’t care elements in TCAM. A small compacted routing table helps to increase cache hit rate; this in turn provides fast address lookups. We have evaluated this compaction scheme through extensive simulations involving IPv4 and IPv6 routing tables and routing traces. The original routing tables have been compacted over 60% of their original sizes. The average cache hit rate has improved by up to 15% over the original tables. We have also analyzed port errors caused by caching, and developed a new sampling technique to alleviate this problem. The simulations show that sampling is an effective scheme in port error-control without degrading cache performance. 
55|1||Multicast communication in wormhole-routed 2D torus networks with hamiltonian cycle model|In this paper, we propose an efficient multipath multicast routing algorithm in wormhole-routed 2D torus networks. We first introduce a hamiltonian cycle model for exploiting the feature of torus networks. Based on this model, we find a hamiltonian cycle in torus networks. Then, an efficient multipath multicast routing algorithm with hamiltonian cycle model (mulitpath-HCM) is presented. The proposed multipath multicast routing algorithm utilizes communication channels more uniformly in order to reduce the path length of the routing messages, making the multicasting more efficient. Simulation results show that the multicast latency of the proposed multipath-HCM routing algorithm is superior to that of fixed and dual-path routing algorithms. 
55|2|http://www.sciencedirect.com/science/journal/13837621/55/2|Using age registers for a simple loadâstore queue filtering|One of the main challenges of modern processor design is the implementation of a scalable and efficient mechanism to detect memory access order violations as a result of out-of-order execution. Traditional age-ordered associative load and store queues are complex, inefficient, and power-hungry. In this paper, we introduce two new LSQ filtering mechanisms with different design tradeoffs, but both explicitly rely on timing information as a primary instrument to rule out dependence violation and enforce memory dependences. Our timing-centric design operates at a fraction of the energy cost of an associative LQ and SQ with no performance degradation. 
55|2||Power saving and fault-tolerance in real-time critical embedded systems|In this paper, a method with the double purpose of reducing the consumption of energy and giving a deterministic guarantee on the fault tolerance of real-time embedded systems operating under the Rate Monotonic discipline is presented. A lower bound exists on the slack left free by tasks being executed at their worst-case execution time. This deterministic slack can be redistributed and used for any of the two purposes. The designer can set the trade-off point between them. In addition, more slack can be reclaimed when tasks are executed in less than their worst-case time. Fault-tolerance is achieved by using the slack to recompute the faulty task. Energy consumption is reduced by lowering the operating frequency of the processor as much as possible while meeting all time-constraints. This leads to a multifrequency method; simulations are carried out to test it versus two single frequency methods (nominal and reduced frequencies). This is done under different trade-off points and rates of faults’ occurrence. The existence of an upper bound on the overhead caused by the transition time between frequencies in Rate Monotonic scheduled real-time systems is formally proved. The method can also be applied to multicore or multiprocessor systems. 
55|2||Dependability assessment of by-wire control systems using fault injection|This paper is focused on the validation by means of physical fault injection at pin-level of a time-triggered communication controller: the TTP/C versions C1 and C2. The controller is a commercial off-the-shelf product used in the design of by-wire systems. Drive-by-wire and fly-by-wire active safety controls aim to prevent accidents. They are considered to be of critical importance because a serious situation may directly affect user safety. Therefore, dependability assessment is vital in their design.This work was funded by the European project ‘Fault Injection for TTA’ and it is divided into two parts. In the first part, there is a verification of the dependability specifications of the TTP communication protocol, based on TTA, in the presence of faults directly induced in communication lines. The second part contains a validation and improvement proposal for the architecture in case of data errors. Such errors are due to faults that occurred during writing (or reading) actions on memory or during data storage. 
55|2||An embedded implementation of the Common Language Infrastructure|The Common Language Infrastructure provides a unified instruction set which may be targeted by a variety of high level language compilers. This unified instruction set simplifies the construction of compilers and gives application designers the ability to choose the high level programming language that best suits the problem being solved. While the Common Language Infrastructure solves many problems related to design of applications and compilers, it is not without its own problems. The Common Language Infrastructure is based upon a virtual machine, much like the Java Virtual Machine. This requires that all instructions being executed on the Common Language Infrastructure be translated to native machine instructions before they can be executed on the host processor. This leads to degradation in performance. In order to overcome this problem it is proposed that an embedded processor capable of natively executing the CLI instruction set be developed. The objective of this work is the design and implementation, using VHDL and simulation, of an embedded processor capable of natively executing the CLI instruction set. This processor provides a platform easily targeted by software developers. 
55|2||A framework for low energy data management in reconfigurable multi-context architectures|In this paper, we present an approach to the problem of low energy data scheduling for reconfigurable architectures targeting digital signal processing (DSP) and multimedia applications. The main goal is the reduction of the energy consumed by these applications through the integration of the proposed data management framework within a compilation tool specifically conceived for these architectures. Two levels of on-chip data storage are assumed to be available in the reconfigurable architecture. Then, the data manager tries to optimally exploit this storage hierarchy by saving data transfers among on-chip and external memories, so reducing the energy consumption. To do that, specific algorithms for finding the data shared among the different computation kernels of the application have been developed. Also, a data placement and replacement policy has been designed. We also show how an adequate data scheduling could decrease the number of operations required to implement the dynamic reconfiguration of the system. 
55|2||Conditional diagnosability of hypercubes under the comparison diagnosis model|Processor fault diagnosis plays an important role in multiprocessor systems for reliable computing, and the diagnosability of many well-known networks has been explored. Lai et al. proposed a novel measure of diagnosability, called conditional diagnosability, by adding an additional condition that any faulty set cannot contain all the neighbors of any vertex in a system. We make a contribution to the evaluation of diagnosability for hypercube networks under the comparison model and prove that the conditional diagnosability of n-dimensional Hypercube Qn is 3(n − 2) + 1 for n â©¾ 5. The conditional diagnosability of Qn is about three times larger than the classical diagnosability of Qn. 
55|3|http://www.sciencedirect.com/science/journal/13837621/55/3|Guest Editorâs introduction|
55|3||Implementation and evaluation of a microthread architecture|Future many-core processor systems require scalable solutions that conventional architectures currently do not provide. This paper presents a novel architecture that demonstrates the required scalability. It is based on a model of computation developed in the AETHER project to provide a safe and composable approach to concurrent programming. The model supports a dynamic approach to concurrency that enables self-adaptivity in any environment so the model is quite general. It is implemented here in the instruction set of a dynamically scheduled RISC processor and many such processors form a microgrid. Binary compatibility over arbitrary clusters of such processors and an inherent scalability in both area and performance with concurrency exploited make this a very promising development for the era of many-core chips. This paper introduces the model, the processor and chip architecture and its emulation on a range of computational kernels. It also estimates the area of the structures required to support this model in silicon. 
55|3||An implementation of the SANE Virtual Processor using POSIX threads|The SANE Virtual Processor (SVP) is an abstract concurrent programming model that is both deadlock free and supports efficient implementation. It is captured by the Î¼TC programming language. The work presented in this paper covers a portable implementation of this model as a C++ library on top of POSIX threads. Programs in Î¼TC can be translated to the standard C++ syntax and linked with this library to run on conventional systems. The motivation for this work was to provide an early implementation on conventional processors as well as supporting work from programming FPGA chips to Grids. 
55|3||Coordinated management of hardware and software self-adaptivity|Self-adaptivity is the capability of a system to adapt itself dynamically to achieve its goals. Self-adaptive systems will be widely used in the future both to efficiently use system resources and to ease the management of complex systems. The frameworks for self-adaptivity developed so far usually concentrate either on self-adaptive software or on self-adaptive hardware, but not both.In this paper, we propose a model of self-adaptive systems and we describe how to manage self-adaptivity at all levels (both hardware and software) by means of a decentralized control algorithm. The key advantage of decentralized control is in the simplicity of the local controllers. Simulation results are provided to show the main characteristics of the model and to discuss it. 
55|3||Applying inherent capabilities of quantum-dot cellular automata to design: D flip-flop case study|Nowadays, quantum cellular automata (QCA) has been considered as the pioneer technology in next generation computer designs. QCA provides the computer computations at nano level using molecular components as computation units. Although the QCA technology provides smaller chip area and eliminates the spatial constraints than earlier CMOS technology, but different characteristics and design limitations of QCA architectures have led to essential attentions in replacement of traditional structures with QCA ones. Inherent information flow control, limited wire length, and consumed area are of such features and restrictions. In this paper, D flip-flop structure has been considered and we have proposed two new D flip-flop structures which employ the inherent capabilities of QCA in timing and data flow control, rather than ordinary replacement of CMOS elements with equivalent QCA ones. The introduced structures involve small number of cells in contrast to earlier proposed ones in presence of the same or even lower input to output delay. The proposed structures are simulated using the QCADesigner and the validity of them has been proved. 
55|3||Exploiting selective instruction reuse and value prediction in a superscalar architecture|In our previously published research we discovered some very difficult to predict branches, called unbiased branches. Since the overall performance of modern processors is seriously affected by misprediction recovery, especially these difficult branches represent a source of important performance penalties. Our statistics show that about 28% of branches are dependent on critical Load instructions. Moreover, 5.61% of branches are unbiased and depend on critical Loads, too. In the same way, about 21% of branches depend on MUL/DIV instructions whereas 3.76% are unbiased and depend on MUL/DIV instructions. These dependences involve high-penalty mispredictions becoming serious performance obstacles and causing significant performance degradation in executing instructions from wrong paths. Therefore, the negative impact of (unbiased) branches over global performance should be seriously attenuated by anticipating the results of long-latency instructions, including critical Loads. On the other hand, hiding instructions’ long latencies in a pipelined superscalar processor represents an important challenge itself. We developed a superscalar architecture that selectively anticipates the values produced by high-latency instructions. In this work we are focusing on multiply, division and loads with miss in L1 data cache, implementing a dynamic instruction reuse scheme for the MUL/DIV instructions and a simple last value predictor for the critical Load instructions. Our improved superscalar architecture achieves an average IPC speedup of 3.5% on the integer SPEC 2000 benchmarks, of 23.6% on the floating-point benchmarks, and an improvement in energy-delay product (EDP) of 6.2% and 34.5%, respectively. We also quantified the impact of our developed selective instruction reuse and value prediction techniques in a simultaneous multithreaded architecture (SMT) that implies per thread reuse buffers and load value prediction tables. Our simulation results showed that the best improvements on the SPEC integer applications have been obtained with 2 threads: an IPC speedup of 5.95% and an EDP gain of 10.44%. Although, on the SPEC floating-point programs, we obtained the highest improvements with the enhanced superscalar architecture, the SMT with 3 threads also provides an important IPC speedup of 16.51% and an EDP gain of 25.94%. 
55|3||Broadcast filtering: Snoop energy reduction in shared bus-based low-power MPSoCs|In multiprocessor system-on-a-chips (MPSoCs) that use snoop-based cache coherency protocols, a miss in the data cache triggers the broadcast of coherency request to all the remote caches, to keep all data coherent. However, the majority of these requests are unnecessary because remote caches do not have the matching blocks and so their tag lookups fail. Both the coherency requests and the tag lookups corresponding to a remote miss consume unnecessary energy.We propose an architecture-level technique for snoop energy reduction, called broadcast filtering, which prevents unnecessary coherency requests from being broadcast to remote caches, and thus reduces snoop energy consumption by both the cache and bus. Broadcast filtering is implemented using a snooping cache and a split bus. The snooping cache checks if a block that cannot be obtained locally exists in remote caches before broadcasting a coherency request. If no remote cache has the matching block, there is no broadcast; and if broadcasting is necessary, the split bus allows coherency requests to be broadcast selectively to the remote caches which have matching blocks.Experimental results show a reduction by 90% of cache lookups, by 60% of bus usage, and by 40% of snoop energy consumption, at a small cost in reduced performance. An analysis result based on the energy model shows the broadcast filtering technique can reduce by up to 55% of energy consumption per cache coherency operation. 
55|4|http://www.sciencedirect.com/science/journal/13837621/55/4|Editorial|
55|4||Model-driven business process security requirement specification|Various types of security goals, such as authentication or confidentiality, can be defined as policies for service-oriented architectures, typically in a manual fashion. Therefore, we foster a model-driven transformation approach from modelled security goals in the context of process models to concrete security implementations. We argue that specific types of security goals may be expressed in a graphical fashion at the business process modelling level which in turn can be transformed into corresponding access control and security policies. In this paper we present security policy and policy constraint models. We further discuss a translation of security annotated business processes into platform specific target languages, such as XACML or AXIS2 security configurations. To demonstrate the suitability of this approach an example transformation is presented based on an annotated process. 
55|4||Security architecture for virtual organizations of business web services|Virtual organizations (VO) temporarily aggregate resources of different domains to achieve a common goal. Web services are being positioned as the technological framework for achieving this aggregation in the context of cross-organizational business applications. Numerous architectures have been proposed for securing VOs, mostly for scientific research, such that they do not address all the requirements of business-oriented applications. This paper describes these additional requirements and proposes a novel architecture and approach to managing VO access control policies. Business users can focus on designing business processes, exposing web services and managing their VO partnerships, while the architecture supports and secures the web service interactions involved. 
55|4||Secure web services using two-way authentication and three-party key establishment for service delivery|With the advance of web technologies, a large quantity of transactions have been processed through web services. Service Provider needs encryption via public communication channel in order that web services can be delivered to Service Requester. Such encryptions can be realized using secure session keys. Traditional approaches which can enable such transactions are based on peer-to-peer architecture or hierarchical group architecture. The former method resides on two-party communications while the latter resides on hierarchical group communications. In this paper, we will use three-party key establishment to enable secure communications for Service Requester and Service Provider. The proposed protocol supports Service Requester, Service Broker, and Service Provider with a shared secret key established among them. Compared with peer-to-peer architecture and hierarchical group architecture, our method aims at reducing communication and computation overheads. 
55|4||Landscape-aware location-privacy protection in location-based services|Mobile network providers have developed a variety of location-based services (LBSs), such as friend-finder, point of interest services, emergency rescue and many other safety and security services. The protection of location-privacy has consequently become a key aspect to the success of LBSs, since users consider their own physical location and movements highly privacy-sensitive, and demand for solutions able to protect such an information in a variety of environments. The idea behind location-privacy protection is that the individual should be able to set the level at which the location information is released to avoid undesired exploitation by a potential attacker: one of the approaches to this problem is given by the application of spatial obfuscation techniques, actuated by a trusted agent, and consisting in artificial perturbations of the location information collected by sensing technologies, before its disclosure to third parties. In many situations, however, landscape/map information can help a third party to perform Bayesian inference over spatially obfuscated data and to refine the user’s location estimate up to a violation of the original user’s location-privacy requirements. The goal of this paper is to provide a map-dependent obfuscation procedure that enables the release of the maximum possible user’s location information, that does not lead to a violation of the original user’s location-privacy requirements, even when refined through map-based inference. 
55|4||Qualitative trust modeling in SOA|Trust among cooperating agents is an essential precondition for every e-business transaction. It is becoming increasingly vital in service oriented architectures (SOAs), where services from various administration domains are deployed. Traditional hard security mechanisms with different techniques of authorization, access control and information security services give a solid foundation, but they fail when cooperating entities act deceitfully. Trust as a soft social security mechanism can protect against such threats and consequently improves the quality of services and reliability of service providers. This paper presents an abstract trust model that applies complementary qualitative methodology which addresses the core of trust as socio-cognitive phenomenon. The model complements existing quantitative methodologies and is applied in the web services environment that enables trust management in SOAs. 
55|4||Enforcing role based access control model with multimedia signatures|Recently ubiquitous technology has invaded almost every aspect of the modern life. Several application domains, have integrated ubiquitous technology to make the management of resources a dynamic task. However, the need for adequate and enforced authentication and access control models to provide safe access to sensitive information remains a critical matter to address in such environments. Many security models were proposed in the literature thus few were able to provide adaptive access decisions based on the environmental changes. In this paper, we propose an approach based on our previous work [B.A. Bouna, R. Chbeir, S. Marrara, A multimedia access control language for virtual and ambient intelligence environments, In Secure Web Services (2007) 111–120] to enforce current role based access control models [M.J. Moyer, M. Ahama, Generalized role-based access control, in: Proceedings of International Conference on Distributed Computing Systems (ICDCS), Phoenix, Arizona, USA, 2001, pp. 391–398] using multimedia objects in a dynamic environment. In essence, multimedia objects tend to be complex, memory and time consuming nevertheless they provide interesting information about users and their context (user surrounding, his moves and gesture, people nearby, etc.). The idea behind our approach is to attribute to roles and permissions, multimedia signatures in which we integrate conditions based on users’ context information described using multimedia objects in order to limit role activation and the abuse of permissions in a given environment. We also describe our architecture which extends the known XACML [XACML, XACML Profile for Role Based Access Control (RBAC), <http://docs.oasis-open.org/xacml/cd-xacml-rbac-profile-01.pdf>, 2008] terminology to incorporate multimedia signatures. We provide an overview of a possible implementation of the model to illustrate how it could be valuable once integrated in an intelligent environment. 
55|4||A security policy framework for context-aware and user preferences in e-services|In today’s dynamic and distributed markets a large spectrum of services is delivered through information and communication technologies. Emerging markets of e-services lie at the intersection of non-traditional user behaviour, and cyber-partnerships of enterprises to deliver innovative services. Current approaches to manage and control security demonstrate lacks in terms of security policy matching and integration in heterogeneous e-service environments. In this paper, we introduce a framework to support role-based access control for distributed services focusing on the integration of customer preferences. The framework aims to collect and generate policy-based security measures in cross-organisational scenarios. In addition to catering to specifications of security and business policies, the ability to integrate contextual information and user preferences make the role-based framework flexible and express a variety of access policies that provide a just-in-time permission activation. 
55|4||Towards the homogeneous access and use of PKI solutions: Design and implementation of a WS-XKMS server|Nowadays, there exists certain important scenarios where different WS-* security related protocols and technologies are being used, such as e-commerce, resource control, or secure access to grid nodes. Additionally, most of these scenarios require the interaction with a trust management infrastructure (such as a PKI -Public Key Infrastructure-), usually to validate the digital certificates provided by communication peers belonging, in most cases, to different administrative domains. For doing this with WS-enabled technologies the W3C proposed the XKMS (XML Key Management Specification) standard a few years ago. However, few implementations exist so far of this standard, and most of them with important limitations. This paper presents an open-source WS-enabled implementation of the XKMS standard named Open XKMS, certain key scenarios where it can be used and the details of how it has been designed and implemented. This paper tries to motivate and foster the use of the XKMS standard and describe a software solution that can help to designers and developers of WS-based security scenarios. 
55|5-6|http://www.sciencedirect.com/science/journal/13837621/55/5-6|An architectural co-synthesis algorithm for energy-aware Network-on-Chip design|Network-on-Chip (NoC) has been proposed to overcome the complex on-chip communication problem of System-on-Chip (SoC) design in deep sub-micron. A complete NoC design contains exploration on both hardware and software architectures. The hardware architecture includes the selection of Processing Elements (PEs) with multiple types and their topology. The software architecture contains allocating tasks to PEs, scheduling of tasks and their communications. To find the best hardware design for the target tasks, both hardware and software architectures need to be considered simultaneously. Previous works on NoC design have concentrated on solving only one or two design parameters at a time. In this paper, we propose a hardware–software co-synthesis algorithm for a heterogeneous NoC architecture. The design goal is to minimize energy consumption while meeting the real-time requirements commonly seen in embedded applications. The proposed algorithm is based on Simulated-Annealing (SA). To compare the solution quality and efficiency of the proposed algorithm, we also implement the branch-and-bound and iterative algorithm to solve the hardware–software co-synthesis problem of a heterogeneous NoC. With the given synthetic task sets, the experimental results show that the proposed SA-based algorithm achieves near-optimal solution in a reasonable time, while the branch-and-bound algorithm takes a very long time to find the optimal solution, and the iterative algorithm fails to achieve good solution quality. When applying the co-synthesis algorithms to a real-world application with PE library that has little variation in PE performance and energy consumption, the iterative algorithm achieves solution quality comparable to that of the proposed SA-based algorithm. 
55|5-6||FPGA/DSP-based implementation of a high-performance multi-channel counter|A high-performance configurable multi-channel counter is presented. The system has been implemented on a small-size and low-cost Commercial-Off-The-Shelf (COTS) FPGA/DSP-based board, and features 64 input channels, a maximum counting rate of 45 MHz, and a minimum integration window (time resolution) of 24 Î¼s with a 23 b counting depth. In particular, the time resolution depends on both the selected counting bit-depth and the number of acquisition channels: indeed, with a 8 b counting depth, the time resolution reaches the value of 8 Î¼s if all the 64 input channels are enabled, whereas it lowers to 378 ns if only 2 channels are used. Thanks to its flexible architecture and performance, the system is suitable in highly demanding photon counting applications based on SPAD arrays, as well as in many other scientific experiments. Moreover, the collected counting results are both real-time processed and transmitted over a high-speed IEEE 1394 serial link. The same link is used to remotely set up and control the entire acquisition process, thus giving the system a even higher degree of flexibility. Finally, a theoretical model of general use which immediately provides the overall system performance is described. The model is then validated by the reported experimental results. 
55|5-6||Dual-Mode Execution Environment for active network|In active networks, the Execution Environment in an active node executes user codes to tailor the processing of packets to application needs. However, the existing implementation of a software-based active node has a dilemma of only using the user mode Execution Environment or, alternatively, only using the kernel mode Execution Environment because neither mode of the Execution Environments meets application needs. The dilemma stems from the choice between high overheads of a user mode Execution Environment and poor services of a kernel mode Execution Environment in executing user codes. In this paper, we propose the Dual-Mode Execution Environment to overcome this dilemma, i.e. arguing for the necessity of an Execution Environment that supports dual modes. Currently, we develop and make the Lifetime-Sensitive Quantity-based Policy the workload adjustment policy in the Dual-Mode Execution Environment. We implement the prototype in Windows 2000. We conduct experiments to compare the prototype with active nodes that uses either the user mode Execution Environment or the kernel mode Execution Environment. We prove that the prototype meets application needs and outperforms the compared active nodes. 
55|5-6||A survey of Flash Translation Layer|Recently, flash memory is widely adopted in embedded applications as it has several strong points, including its non-volatility, fast access speed, shock resistance, and low power consumption. However, due to its hardware characteristics, specifically its “erase-before-write” feature, it requires a software layer known as FTL (Flash Translation Layer). This paper surveys the state-of-the-art FTL software for flash memory. It defines the problems, addresses algorithms to solve them, and discusses related research issues. In addition, the paper provides performance results based on our implementation of each FTL algorithm. 
55|5-6||Reducing message-length variations in resource-constrained embedded systems implemented using the Controller Area Network (CAN) protocol|The Controller Area Network (CAN) protocol is widely used in low-cost embedded systems. CAN uses “Non Return to Zero” (NRZ) coding and includes a bit-stuffing mechanism. Whilst providing an effective mechanism for clock synchronization, the bit-stuffing mechanism causes the CAN frame length to become (in part) a complex function of the data contents: variations in frame length can have a detrimental impact on the real-time behaviour of systems employing this protocol. In this paper, two software-based mechanisms for reducing the impact of CAN bit stuffing are considered and compared. The first approach considered is a modified version of a technique described elsewhere (e.g. Nolte et al. [T. Nolte, H.A. Hansson, C. Norström, Minimizing CAN response-time jitter by message manipulation, in: Proceedings of the Eighth IEEE Real-Time and Embedded Technology and Applications Symposium (RTAS 2002), San Jose, California, 2002]). The second approach considered is a form of software bit stuffing (SBS). In both cases, not only the impact on message-length variations is addressed but also the implementation costs (including CPU and memory requirements) involved in creating practical implementation of each technique on a range of appropriate hardware platforms. It is concluded that the SBS technique is more effective in the reduction of message-length variations, but at the cost of an increase in CPU time and memory overheads and a reduction in the available data bandwidth. The choice of the most appropriate technique will, therefore, depend on the application requirements and the available resources. 
55|7-9|http://www.sciencedirect.com/science/journal/13837621/55/7-9|An efficient signed digit montgomery multiplication for RSA|In this paper we present an efficient Montgomery multiplier using the signed digit number representation suitable for modular exponentiation, which is the main operation of RSA. The multiplier consists of one level of signed digit adder plus multiplexer through a precomputation. We design the multiplier with the improved signed digit adder using SAMSUNG STD 130 0.18Î¼m 1.8 V CMOS Standard Cell Library and compare to multipliers with other previous adders. The proposed modular multiplier can be applied to public key cryptosystems based on integer arithmetic such as RSA, DSA or ECC. 
55|7-9||Computation and communication schedule optimization for data-sharing tasks on uniprocessor|Almost every computation task requires input data in order to find a solution. This is not a problem for a centralized system because data is usually available locally. However, in a parallel and distributed system, e.g., computation grids, the data may be in remote sites and must be transferred to the local site before the computation can proceed. As a result, the interleaved sequence of data transfer and job execution has a significant impact on the overall computational efficiency. In this paper, we analyze the computational complexity of the shared-data job scheduling problem on uniprocessor, with and without consideration of the storage capacity constraint on the local site.We show that if there is an upper bound on the server capacity, the problem is NP-complete, even when each job depends on at most two data items. For the case where there is no upper bound on the server capacity, we show that there exists an efficient algorithm that can provide an optimal job schedule when each job depends on at most two data items. We also propose an efficient heuristic algorithm that can determine good schedules for cases where there is no limit on the amount of data a job may access. The reported experiment results demonstrate that this heuristic algorithm performs very well, and derives near optimal solutions. 
55|7-9||Quantitative analysis of packet-processing applications regarding architectural guidelines for network-processing-engine development|This paper presents a simulation-based profile-driven quantitative analysis of packet-processing applications. In this domain, demands for increasing the performance and the ongoing development of network protocols both call for flexible and performance-optimized engines. Based on the achieved profiling results, we introduce platform-independent analysis that locates the performance bottlenecks and architectural challenges of a packet-processing engine. Finally based on these results, we extract helpful architectural guidelines for design of a flexible and high-performance embedded processor that is optimized for packet-processing operations in high-performance and cost-sensitive network embedded applications. 
55|7-9||Evaluating the energy consumption and the silicon area of on-chip interconnect architectures|Sophisticated on-chip interconnects using packet and circuit switching techniques were recently proposed as a solution to non-scalable shared-bus schemes currently used in Systems-on-Chip (SoCs) implementation. Different interconnect architectures have been studied and adapted for SoCs to achieve high throughput, low latency and energy consumption, and efficient silicon area. Recently, a new on-chip interconnect architecture by adapting the WK-recursive network topology structure has been introduced for SoCs. This paper analyses and compares the energy consumption and the area requirements of Wk-recursive network with five common on-chip interconnects, 2D Mesh, Ring, Spidergon, Fat-Tree and Butterfly Fat-Tree. We investigated the effects of load and traffic models and the obtained results show that the traffic models and load that ends processing elements has a direct effect on the energy consumption and area requirements. In these results, WK-recursive interconnect generally has a higher energy consumption and silicon area requirements in heavy traffic load. 
55|7-9||Maintaining constraints of UML models in distributed collaborative environments|Constraint maintenance plays an important role in keeping the integrity and validity of UML models in embedded software design. While constraint maintenance capabilities are reasonably adequate in existing UML modeling applications, little work has been done to address the distributed constraint maintenance issue in multi-user collaborative modeling environments. The nature of the issue is to maintain constraint consistently across distributed sites in a collaborative modeling environment in the face of concurrency. In this paper, we propose a novel solution to this issue, which can retain the effects of all concurrent modeling operations even though they may cause constraint violations. We further contribute a distributed constraint maintenance framework in which the solution is encapsulated as a generic engine that can be mounted in a variety of single-user UML modeling applications to support collaborative UML modeling and distributed constraint maintenance in embedded software design processes. This framework has been implemented in a prototype distributed collaborative UML modeling application CoRSA. 
55|7-9||A platform-based design framework for joint SW/HW multiprocessor systems design|We present P-Ware, a framework for joint software and hardware modelling and synthesis of multiprocessor embedded systems. The framework consists of (1) component-based annotated transaction-level models for joint modelling of parallel software and multiprocessor hardware, and (2) exploration-driven methodology for joint software and hardware synthesis. The methodology has the advantage of combining real-time requirements of software with efficient optimization of hardware performance. We describe and apply the methodology to synthesize a scheduler of a H264 video encoder on the Cake multiprocessor. Moreover, experiments show that the framework is scalable while achieving rapid and efficient designs. 
