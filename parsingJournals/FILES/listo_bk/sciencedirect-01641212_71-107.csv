volume|issue|url|title|abstract
71|-|http://www.sciencedirect.com/science/journal/01641212/71|Editorial board|
71|-||Computer systems|
71|-||Designing embedded systems using patterns: A case study|If software for embedded processors is based on a time-triggered architecture, using co-operative task scheduling, the resulting system can have very predictable behaviour. Such a system characteristic is highly desirable in many applications, including (but not restricted to) those with safety-related or safety-critical functions. In practice, a time-triggered, co-operatively scheduled (TTCS) architecture is less widely employed than might be expected, not least because care must be taken during the design and implementation of such systems if the theoretically predicted behaviour is to be obtained. In this paper, we argue that the use of appropriate ‘design patterns’ can greatly simplify the process of creating TTCS systems. We briefly explain the origins of design patterns. We then illustrate how an appropriate set of patterns can be used to facilitate the development of a non-trivial embedded system. 
71|-||Agent-based distance vector routing: a resource efficient and scalable approach to routing in large communication networks|In spite of the ever-increasing availability of computation and communication resources in modern networks, the overhead associated with network management protocols, such as traffic control and routing, continues to be an important aspect in the design of new methodologies. Resource efficiency of such protocols has become even more prominent with the recent developments of wireless and ad hoc networks, which are marked by much more severe resource constraints in terms of bandwidth, memory, and computational capabilities. This paper presents an agent-based approach to distance vector routing (ADVR) that addresses these resource constraints. ADVR is a resource efficient implementation of distance vector routing that is fault tolerant and scales well for large networks. ADVR draws upon some basic biologically inspired principles to facilitate coordination among the mobile agents that implement the routing task. Specifically, simulated pheromones are used to control the movement of agents within the network and to dynamically adjust the number of agents in the population. The behavior of ADVR is analyzed and compared to that of traditional distance vector routing. 
71|-||Dynamic adaptation of application aspects|In today’s fast changing environments, adaptability has become an important feature in modern computing systems, programming languages and software engineering methods. Different approaches and techniques are used to achieve the development of adaptable systems. Following the principle of separation of concerns, aspect-oriented programming (AOP) distinguishes application functional code from specific concerns that cut across the system, creating the final application by weaving the program’s main code and its specific aspects. In many cases, dynamic application adaptation is needed, but few existing AOP tools offer it in a limited way. Moreover, these tools use a fixed programming language: aspects cannot be implemented regardless of its programming language.We identify reflection as a mechanism capable of overcoming the deficiencies previously mentioned. We have developed a non-restrictive reflective technique that achieves a real computational-environment jump, making every application and language feature adaptable at runtime––without any previously defined restriction. Moreover, our reflective platform is independent of the language selected by the programmer. Using the reflective capabilities of the platform presented, an AOP framework that achieves dynamic aspect weaving in a language-independent way has been constructed, overcoming the common limitations of existing AOP tools. 
71|-||Genetic-algorithm-based real-time task scheduling with multiple goals|This paper presents and evaluates a new method for real-time task scheduling in multiprocessor systems. Its objectives are to minimize the number of processors required and the total tardiness of tasks. The minimization is carried out through a multiobjective genetic algorithm (GA), because the problem has non-commensurable and competing objectives to be optimized. The experimental results showed that when compared to five methods used previously, such as list-scheduling algorithms and a specific GA, the performance of our algorithm was comparable or better for 178 out of 180 randomly generated task graphs. Also shown is the impact of the sparsity of a task graph on the performance of our algorithm. 
71|-||Analysis of true fully adaptive routing with software-based deadlock recovery|Several analytical models of fully adaptive routing (AR) in wormhole-routed networks have recently been reported in the literature. All these models, however, have been discussed for routing algorithms with deadlock avoidance. Recent studies have revealed that deadlocks are quite rare in the network, especially when enough routing freedom is provided. Thus the hardware resources, e.g. virtual channels, dedicated for deadlock avoidance are not utilised most of the time. This consideration has motivated researchers to introduce fully adaptive routing algorithms with deadlock recovery. This paper describes a new analytical model of a true fully AR algorithm with software-based deadlock recovery, proposed in Martinez et al. (Software-based deadlock recovery techniques for true fully AR in wormhole networks, Proc. Int. Conf. Parallel Processing (ICPP’97), 1997, p. 182) for k-ary n-cubes. The proposed model uses the results from queueing systems with impatient customers to capture the effects of the timeout mechanism used in this routing algorithm for deadlock detection. Results obtained through simulation experiments confirm that the model predicts message latency with a good degree of accuracy under different working conditions. 
71|-||Adaptable architecture generation for embedded systems|Architecture generation is the first step in the design of software systems. Many of the qualities that the final software system possesses are usually decided at the architecture development stage itself. Thus, if the final system should be usable, testable, secure, high performance, mobile and adaptable, then these qualities or non-functional requirements (NFRs) should be engineered into the architecture itself. In particular, recently adaptability is emerging as an important attribute required by almost all software systems. Briefly, adaptability is the ability of a software system to accommodate changes in its environment. Embedded systems are usually constrained both in hardware and software. Current adaptable architecture development methods for embedded systems are usually manual and ad hoc––there are almost no comprehensive systematic approaches to consider NFRs at the architecture development stage. While there are several examples of approaches to generate architectures based on functional requirements, we believe that there are very few techniques that consider NFRs such as adaptability during the process of architecture generation. In this paper we present an automated design method that helps develop adaptable architectures for embedded systems by developing a tool called Software Architecture Adaptability Assistant (SA3). SA3 helps the developer during the process of software architecture development by selecting the architectural constituents such as components, connections, patterns, constraints, styles, and rationales that best fit the adaptability requirements for the architecture. The developer can then complete the architecture from the constituents chosen by the tool. SA3 uses the knowledge base properties of the NFR Framework in order to help automatically generate the architectural constituents. The NFR Framework provides a systematic method to consider NFRs, in particular their synergies and conflicts. We validated the architectures generated by SA3 in a type of embedded system called the vocabulary evolution system (VES) by implementing the codes from the generated architectures in the VES and confirming that the resulting system satisfied the requirements for adaptability. A VES responds to external commands through an interface such as ethernet and the vocabulary of these commands changes with time. The validation process also led to the discovery of some of the shortcomings of our automated design method. 
71|-||Contents Volume 71|
71|1-2|http://www.sciencedirect.com/science/journal/01641212/71/1-2|Editorial board|
71|1-2||Advanced obfuscation techniques for Java bytecode|There exist several obfuscation tools for preventing Java bytecode from being decompiled. Most of these tools simply scramble the names of the identifiers stored in a bytecode by substituting the identifiers with meaningless names. However, the scrambling technique cannot deter a determined cracker very long. We propose several advanced obfuscation techniques that make Java bytecode impossible to recompile or make the decompiled program difficult to understand and to recompile. The crux of our approach is to over use an identifier. That is, an identifier can denote several entities, such as types, fields, and methods, simultaneously. An additional benefit is that the size of the bytecode is reduced because fewer and shorter identifier names are used. Furthermore, we also propose several techniques to intentionally introduce syntactic and semantic errors into the decompiled program while preserving the original behaviors of the bytecode. Thus, the decompiled program would have to be debugged manually. Although our basic approach is to scramble the identifiers in Java bytecode, the scrambled bytecode produced with our techniques is much harder to crack than that produced with other identifier scrambling techniques. Furthermore, the run-time efficiency of the obfuscated bytecode is also improved because the size of the bytecode becomes smaller after obfuscation. 
71|1-2||Formally analyzing software architectural specifications using SAM|In the past decade, software architecture has emerged as a major research area in software engineering. Many architecture description languages have been proposed and some analysis techniques have also been explored. In this paper, we present a graphical formal software architecture description model called software architecture model (SAM). SAM is a general software architecture development framework based on two complementary formalisms––Petri nets and temporal logic. Petri nets are used to visualize the structure and model the behavior of software architectures while temporal logic is used to specify the required properties of software architectures. These two formal methods are nicely integrated through the SAM software architecture framework. Furthermore, SAM provides the flexibility to choose different compatible Petri net and temporal logic models according to the nature of system under study. Most importantly, SAM supports formal analysis of software architecture properties in a variety of well-established techniques––simulation, reachability analysis, model checking, and interactive proving. In this paper, we show how to formally analyze SAM software architecture specifications using two well-known techniques––symbolic model checking with tool Symbolic Model Verifier, and theorem proving with tool STeP. 
71|1-2||Systems analyst activities and skills in the new millennium|The nature of systems development has continued to undergo change as new technologies emerge and impact the environment in which systems must function. A nationwide survey of systems analysts was conducted to assess what tasks are most important, what skills are most important in completing the tasks, and how the tasks and skills needed may have changed over the last decade. The most important tasks were those associated with traditional systems development, namely defining system scope, objectives, system requirements, as well as assessing the impact of systems and evaluating their performance. Analytical skills were considered the most important skills overall, followed by technical and communication skills. The least important skills overall were interpersonal skills. When compared to results from a similar survey conducted in the early 1990s, the data from the present study revealed that while the most important tasks remained relatively unchanged, a number of shifts occurred in other aspects of the systems analyst’s work that reflect changes in the technological environment. 
71|1-2||Anomaly-free component adaptation with class overriding|Software components can be implemented and distributed as collections of classes, then adapted to the needs of specific applications by means of subclassing. Unfortunately, subclassing in collections of related classes may require re-implementation of otherwise valid classes just because they utilize outdated parent classes, a phenomenon that is referred to as the subclassing anomaly. The subclassing anomaly is a serious problem since it can void the benefits of component-based programming altogether. We propose a code adaptation language mechanism called class overriding that is intended to overcome the subclassing anomaly. Class overriding does not create new and isolated derived classes as subclassing does, but rather extends and updates existing classes across collections of related classes. If adopted in new languages for component-based programming, or in existing compiled languages such as C# and Java, class overriding can help maintain the integrity of evolving collections of related classes and thus enhance software component adaptability.While other techniques such as reflection and binary code adaptation can be used to reduce the magnitude of the subclassing anomaly, class overriding has the advantage of being easy-to-use and efficient. 
71|1-2||Design and implementation of the just-in-time retrieving policy for schedule-based distributed multimedia presentations|In order to provide the smooth playback of a distributed multimedia presentation, the object-retrieving engine for the player must fetch each object before its playback time. In this paper, a smart object-retrieving engine is proposed, which adopts a retrieving policy named the just-in-time policy. The policy expects the retrieval process of an object to finish right before the playback time of the object to achieve a better buffer utilization and network bandwidth efficiency. The proposed object-retrieving engine focuses on SMIL1.0-based multimedia presentations. By converting the synchronization relationship of objects in the SMIL1.0 document to Real-Time Synchronization Model, which simplifies the handling of the synchronization relationship, and considering the end-to-end bandwidth as well as the user interactions, the object-retrieving engine determines the object request time for each object. The engine issues the request to fetch each object for the ongoing presentation at the object request time, and provides the player with proper media objects. The feasibility and better performance of the proposed just-in-time retrieving policy had been proved by performance measurements of system implementation. 
71|1-2||Image retrieval system based on color-complexity and color-spatial features|This paper proposes a color-complexity image feature which can effectively describe the color variation of the pixels in an image. This paper also presents a color-spatial feature to state the pixel color distributions on different locations in an image. Because these two features are highly complementary, this paper integrates them to provide an image retrieval system. Experimental results show that the recognition ability of the system can be drastically enhanced after integrating these two image features. 
71|1-2||Improving workload balance and code optimization on processor-in-memory systems|Processor-in-memory (PIM) architectures have recently been proposed, with the objective of reducing the performance gap between processor and memory. An earlier study of Huang and Chu [Proceedings of 2nd Workshop on Intelligent Memory Systems, Cambridge, MA, 2000] designed a statement-based parallelizing system, SAGE, to exploit the potential benefits of PIM. This study extends this system to achieve better performance. Several comprehensive optimization approaches, including self-patch weight evaluation, loop splitting for PIM, intelligent memory operation (IMOP) recognition, and tiling for PIM, are devised to produce execution schedules with improved load balance. Experimental results confirm the effectiveness of the proposed method. 
71|1-2||Case study: an infrastructure for C/ATLAS environments with object-oriented design and XML representation|In this paper, we present an ATLAS compiler environment used for automatic testing as a case study to demonstrate the design of the state of the art compiler environments with object-oriented designs and XML representations. ATLAS is a testing language which is applied on the automatic test equipments (ATEs). Currently, the programming language is used in the fields of avionics, industry facilities, and precision transport system. In this work, we develop the ATLAS compiler aiming to provide the control ability of the PC-based ATEs. First, it comes with an objected-oriented representation of program trees. The object-oriented program graph allows the flexibility for the manipulations of program trees. Second, it employs the object serialization technology for storing and retrieving the syntax trees and program graphs. The employment of object serialization techniques significantly reduces the programming effort from traditional compiler work in retrieving binary representations of program graphs from secondary storages. In addition, we establish the connection between the object-oriented program graph and XML representations. With the support of DTD and XSL files of XML environments, we can perform machine validation on XML-based representations, transform XML representations into graph structures, and annotate the representations for human browsing. Our software infrastructure can be used for subsequent controls and specifications for ATEs. 
71|1-2||Analysing failure behaviours in component interaction|In order to facilitate the process of safety analysis of an evolving software system, this paper presents an architectural approach that enhances the safety analysis by providing appropriate abstractions for modelling and analysing interactions between components, since faulty interactions are the usual cause of accidents. For that, instead of considering components as the locus of change, the proposed approach assumes that components remain unchanged while their interactions (captured by connectors) adapt to the different changes made in the system. The behavioural description of connectors is provided in terms of extended time automata, and the safety analysis is performed using model checking, which verifies whether safe behaviour is maintained when interactions between components change. The feasibility of the approach is demonstrated in terms of a case study that deals with the safety procedures associated with the launching of a sounding rocket. 
71|1-2||Dual link fault diagnosis agreement|Previously, most Byzantine Agreement protocols can reach an agreement by the way of fault masking. Few of them can detect and locate the faulty components. And on the other hand, the most fault diagnosis algorithms can detect and locate faulty components but few of them can make all fault free processors reach an agreement. This study analyzes the messages received at the period of reaching agreement, and then, to detect and to locate the faulty links of the network. Finally, the proposed protocol can further make all fault free processors agree on the common failure report of the synchronous fully connected network. The symptoms of the faults include the malicious fault and the dormant fault. 
71|1-2||NT-SwiFT: software implemented fault tolerance on Windows NT|Today, there are increasing demands to make application software more tolerant to failures. Fault-tolerant applications detect and recover from failures that are not handled by the application’s underlying hardware or operating system. In recent years, an increasing number of highly available applications are being implemented on Windows NT. However, the current version of Windows (NT4.0, 2000) and its utilities, such as Microsoft Cluster Server (MSCS), do not provide some facilities (such as transparent checkpointing, and message logging) that are needed to implement fault-tolerant applications. In this paper, we describe a set of reusable software components collectively named software implemented fault tolerance (NT-SwiFT) that facilitates building fault-tolerant and highly available applications on Windows NT, 2000. NT-SwiFT provides components for automatic error detection and recovery, checkpointing, event logging and replay, and communication error recovery, and incremental data replication. Using NT-SwiFT, we conducted fault injection experiments on three commercial server applications––Apache web server, Microsoft IIS web server, and Microsoft SQL––to study the failure coverage and the overhead of NT-SwiFT components. Preliminary results show that NT-SwiFT can detect and recover more application failures than MSCS does in all three applications. 
71|1-2||Embedding role-based access control model in object-oriented systems to protect privacy|The role-based access control (RBAC) approach has been recognized as useful in information security and many RBAC models have been proposed. Current RBAC researches focus on developing new models or enhancing existing models. In our research, we developed an RBAC model that can be embedded in object-oriented systems to control information flows (i.e. to protect privacy) within the systems. This paper proposes the model. The model, which is named OORBAC, is an extension of RBAC96. OORBAC offers the following features: (a) precisely control information flows among objects, (b) control method invocation through argument sensitivity, (c) allow purpose-oriented method invocation and prevent leakage within an object, (d) precisely control write access, and (e) avoid Trojan horses. We implemented a prototype for OORBAC using JAVA as the target language. The implementation resulted in a language named OORBACL, which can be used to implement secure applications. We evaluated OORBAC using experiments. The evaluation results are also shown in this paper. 
71|1-2||Hyppocrates: a new proactive password checker|In this paper, we propose a new proactive password checker, a program which prevents the choice of easy-to-guess passwords. The checker uses a decision tree, constructed applying the minimum description length principle and a pessimistic pruning technique. Experimental results show a substantial improvement in performance of this checker compared to previous proposals. Moreover, the whole software package we provide has a user-friendly interface, enabling the system administrator to configure an ad hoc password proactive checker, in order to satisfy certain policy requirements. 
71|1-2||A component based methodology for Web application development|Today’s enterprises increasingly rely on the Web to support their operations, global business alliances, and integration of their business processes with those of their suppliers, partners, and customers. In order to stay competitive, businesses must also respond to changing business and competitive environments in near real-time. This requires that the architecture and design of business systems and applications should allow quick reconfiguration as well as collaboration among distributed software components. Existing software design methodologies often lack the Web focus and explicit support for componentization. This paper presents a methodology for requirements analysis and high-level design, specifically for component-based Web applications.Using the Web page as the fundamental building block of Web applications, the proposed methodology introduces page and component classifications, and a set of tags to implement link semantics. Based on these constructs, the methodology then specifies procedures for requirements analysis and design. A running business example is used to illustrate various stages of the analysis and design process. 
71|1-2||DIPS: an efficient pointer swizzling strategy for incremental uncaching environments|Pointer swizzling improves the performance of OODBMSs by reducing the number of table lookups. However, the object replacement incurs the unswizzling overhead. In this paper, we propose a new pointer swizzling strategy, the dynamic indirect pointer swizzling (DIPS). DIPS dynamically applies pointer swizzling techniques in order to reduce the overhead of unswizzling. DIPS uses the temporal locality information which is gathered by the object buffer manager. The information is used to select the object to whose pointers the pointer swizzling techniques are applied and to dynamically bind the pointer swizzling techniques using the virtual function mechanism. We show the efficiency of the proposed strategy through experiments over various object buffer sizes and workloads. 
72|1|http://www.sciencedirect.com/science/journal/01641212/72/1|Editorial board|
72|1||QoS management specification support for multimedia middleware|Middleware technologies are now widely used in order to provide support for the interaction of systems relying on different hardware and operating systems. At present middleware platforms, however, do not provide enough support for both the configuration and reconfiguration of quality of service (QoS) management aspects of real-time applications such as distributed multimedia systems. That is, current middleware only provides support for the low-level specification of QoS properties. This paper presents an architecture description language (ADL) called Xelha for the high-level specification of QoS management in multimedia middleware whereas lower-level aspects can be tuned by using an aspect-oriented suite of languages referred to as resource configuration description language (RCDL). Tool support is also provided for the interpretation of the Xelha and RCDL languages. 
72|1||Fair blind threshold signatures in wallet with observers|In this paper, we propose efficient fair blind (t,n) threshold signature schemes in wallet with observers. By these schemes, any t out of n signers in a group can represent the group to sign fair blind threshold signatures, which can be used in anonymous e-cash systems. Since blind signature schemes provide perfect unlinkability, such e-cash systems can be misused by criminals, e.g. to safely obtain a ransom or to launder money. Our schemes allow the judge (or the judges) to deliver information allowing anyone of the t signers to link his view of the protocol and the message–signature pair. 
72|1||Traps in Java|Though the Java programming language was designed with extreme care, there are still a few ambiguities and irregularities left in the language. The ambiguities are those issues that are not defined clearly in the Java language specification. The different results produced by different compilers on several example programs justify our observations. The irregularities are issues that often confuse programmers. It is hard for a confused programmer to write robust programs. Furthermore, a few issues of the Java language are left intentionally open to the compiler writers. Their effects on Java programs are discussed. The problems of ambiguity, irregularity, and dependence on implementations frequently trap an incautious Java programmer. Some suggestions and solutions for the problems are provided in this paper. 
72|1||Extraction of Java program fingerprints for software authorship identification|Computer programs belong to the authors who design, write, and test them. Authorship identification is concerned with determining the likelihood of a particular author having written some piece(s) of code, usually based on other code samples from the same programmer. Java is a popular object-oriented computer programming language. Programming fingerprints attempt to characterize the features that are unique to each programmer. In this study, we investigated the extraction of a set of software metrics of a given Java source code––by a program written in Visual C++––that could be used as a fingerprint to identify the author of the Java code. The contributions of the selected metrics to authorship identification were measured by a statistical process, namely canonical discriminant analysis, using the statistical software package SAS. Out of the 56 extracted metrics, 48 metrics were identified as being contributive to authorship identification. The authorship of 62.6–67.2% of the Java programs considered could be correctly identified with the extracted metrics. The identification rate could be as high as 85.8%, with derived canonical variates. Moreover, layout metrics played a more important role in authorship identification than the other metrics. 
72|1||An uncaught exception analysis for Java|Current JDK Java compiler relies on programmer's declarations (by throws clauses) for checking against uncaught exceptions of the input program. It is not elaborate enough to remove programmer's unnecessary handlers nor suggest to programmers for specialized handlings (when programmer's declarations are too broad). We propose a static analysis of Java programs that estimates their uncaught exceptions independently of the programmer's declarations. This analysis is designed and implemented based on set-based framework. Its cost-effectiveness is suggested by sparsely analyzing the program at method level (hence reducing the number of unknowns in the flow equations). We have shown that our interprocedural exception analysis is more precise than JDK-style intraprocedural analysis, and also that our analysis can effectively detect uncaught exceptions for realistic Java programs. 
72|1||Resource space model, its design method and applications|A resource space model (RSM) is a model for specifying, sharing and managing versatile Web resources with a universal resource view. A normal resource space is a semantic coordinate system with independent coordinates and mutual-orthogonal axes. This paper first introduces the main viewpoint and basic content of the RSM, and then proposes a four-step method for designing the logical-level resource spaces: resource analysis, top-down resource partition, design two-dimensional resource spaces, and join between resource spaces. Design strategies and tools include: reference model, analogy and abstraction strategy, resource dictionary, independency checking tool, and orthogonality checking tool. The study on using the RSM to manage relational tables shows that the RSM is also suitable for managing structured resources. Applications show that the RSM together with the proposed development method is an applicable solution to realize normal and effective management of versatile web resources. Comparisons show the differences between the proposed model and the relational data model. 
72|1||A unified approach to a fair document exchange system|This paper aims to propose a solution to unify different approaches to fair document exchange for the development of a flexible, general and secure fair document exchange system capable of guaranteeing strong fairness. The solution caters for a document exchange among any number of parties under various situations. At the heart of this unified solution are methods for a verifiable and recoverable encryption of a symmetric (or conventional) key for document encryption and decryption in a simple and efficient manner. The verifiability means that any party can verify the correctness of a key encrypted without actually viewing the key, and the recoverability implies that the party can be assured that a designated party or a group of designated parties can decrypt the encrypted key to recover the original key. These methods are essential for the unified solution to achieve strong fairness. The main novel contribution of the work presented in this paper is that the proposed unified solution represents the first effort on providing an efficient and unified approach to achieving strong fairness in document exchange under various situations. 
72|1||Password-based user authentication and key distribution protocols for clientâserver applications|Up to now, most of the literature on three-party authentication and key distribution protocols has focused on the environment in which two users (clients) establish a session key through an authentication server. In this paper, we discuss another environment in which a user (client) requests service from an application server through an authentication server. We propose two secure and efficient authentication protocols (KTAP: key transfer authentication protocol and KAAP: key agreement authentication protocol) to fit this environment. These two proposed protocols can be efficiently applied to various communication systems in distributed computing environments since they provide security, efficiency, and reliability. 
72|2|http://www.sciencedirect.com/science/journal/01641212/72/2|Editorial board|
72|2||Rule-based generation of requirements traceability relations|The support for traceability between requirement specifications has been recognised as an important task in the development life cycle of software systems. In this paper, we present a rule-based approach to support the automatic generation of traceability relations between documents which specify requirement statements and use cases (expressed in structured forms of natural language), and analysis object models for software systems. The generation of such relations is based on traceability rules of two different types. More specifically, we use requirement-to-object-model rules to trace the requirements and use case specification documents to an analysis object model, and inter-requirements traceability rules to trace requirement and use case specification documents to each other. By deploying such rules, our approach can generate four different types of traceability relations. To implement and demonstrate our approach, we have implemented a traceability prototype system. This system assumes requirement and use case specification documents and analysis object models represented in XML. It also uses traceability rules which are represented in an XML-based rule mark-up language that we have developed for this purpose. This XML-based representation framework makes it easier to deploy our prototype in settings characterised by the use of heterogeneous software engineering and requirements management tools. The developed prototype has been used in a series of experiments that we have conducted to evaluate our approach. The results of these experiments have provided encouraging initial evidence about the plausibility of our approach and are discussed in the paper. 
72|2||A controlled experiment investigation of an object-oriented design heuristic for maintainability|The study presented in this paper is a controlled experiment, aiming at investigating the impact of a design heuristic, dealing with the `god class' problem, on the maintainability of object-oriented designs. In other words, we wish to better understand to what extent a specific design heuristic contributes to the quality of designs developed. The experiment has been conducted using undergraduate students as subjects, performing on two system designs using the Coad & Yourdon method. The results of this study provide evidence that the investigated design heuristic: (a) affects the evolution of design structures; and (b) considerably affects the way participants apply the inheritance mechanism. 
72|2||Domain-oriented software development environment|During software development, one of the most critical activities for software engineers is the correct description and identification of product requirements. This activity involves understanding the problem, which is essential to allow one to define a solution for it. To do this, it is important not only to understand the tasks routinely performed which are part of the problem, but more importantly to understand the domain in which the system will take place. Believing that the use of domain knowledge during software development can be very useful to support software development activities, we define the concept of “Domain-Oriented Software Development Environment” (DOSDE). This kind of environment readies knowledge about a specific domain in a symbolic representation (a domain ontology). It also considers a library of potential tasks from the domain to support problem understanding. This paper presents the main concepts of DOSDE, its features, examples of implementation, and how one can use its embedded knowledge, on a domain and its tasks, to assist in the software development activities. 
72|2||A development environment for customer-oriented Internet business: eBizBench|An important challenge in the age of Internet business is the proper alignment of customers’ needs with the business system. In order to improve an Internet business system according to customers’ needs continuously, reuse of design results is of particular importance. This paper proposes a development environment, called eBizBench, which provides support for this alignment and reusability. The eBizBench includes seven functions: (1) project management, (2) customer analysis, (3) value analysis, (4) web design, (5) implementation design, (6) repository management, and (7) repository. The eBizBench can help develop and maintain Internet business systems in a systematic fashion. The repository is useful for the conversion of design results among development environments. A real-life web site is illustrated to demonstrate the usefulness of the eBizBench. 
72|2||Information theory-based software metrics and obfuscation|A new approach to software metrics using concepts from information theory, data compression, complexity theory and analogies with real physical systems is described. A novel application of software obfuscation allows an existing software package to be analysed in terms of the effects of perturbations caused by the obfuscator. Parallels are drawn between the results of the software analysis and the behaviour of physical systems as described by classical thermodynamics. 
72|2||Virtual reality systems estimation vs. traditional systems estimation|This paper examines the problems of applying traditional function points count rules to virtual reality systems (VRS). From the analysis of the differences between traditional and VRS systems, a set of deficiencies in the IFPUG 4.1 function points count method was detected. Due to the increasing importance of these kinds of applications, it is necessary to study how traditional function points count rules can be adapted to estimate VRS. In this paper, we are going to focus on the possibility of estimating function points accurately using a proposed guideline which was successfully applied to estimate two VRS. 
72|2||Architecture modeling and evaluation for design of agent-based system|As the Internet computing environment is expanded, service requests from users are increasing and systems become large and complex. To provide users with services efficiently, system must react flexibly to changing requests in distributed and mobile environments. Recently, software agent technology has been recognized as a solution technology to cope with this situation. Many researches are primarily related to system implementation or application by an ad hoc engineering approach. However, researches on the design of agent-based systems are not enough. This paper deals with architectural design of agent-based system. In detail, this paper identifies the possible architectural models of agent-based system, evaluates the identified architectural models using metrics and compares the architectural models based on evaluation results. Also, this paper applies the evaluation approach to real architectures of agent-based system. The proposed performance analysis activity can be included in agent-based systems engineering activity. 
72|2||ISO quality standards for measuring architectures|The main concern of this paper is measuring the quality of the architectural design. The goal of this work is to use the architectural design process proposed in the unified process framework, adapting and detailing it to include the quality requirements specification at architectural level. There is general agreement on the fact that in modern applications the selection of the architecture must be addressed early in the development process, to mitigate risks. Moreover, the integration of enterprise applications is a component-based development requiring quality values associated to the services offered by the components. The services depend mostly on the architecture. In consequence, methods arise for guiding the selection or for constructing software architectures. Our approach allows associating the quality requirements (nonfunctional properties) for the architecture expressed using the ISO 9126-1 standard quality model, with the use cases, to facilitate the selection of the “key” use cases. Measures for the architecture's quality characteristics are specified in details, precising attributes, units, numerical systems and scale types. A case study of a real-time application for monitoring stock exchanges illustrates our approach. We hope that our results will be particularly useful for practitioners, such as software architects, analysts and designers. 
72|2||A flexible method for maintaining software metrics data: a universal metrics repository|A neglected aspect of software measurement programs is what will be done with the metrics once they are collected. Often databases of metrics information tend to be developed as an afterthought, with little, if any concessions to future data needs, or long-term, sustaining metrics collection efforts. A metric repository should facilitate an on-going metrics collection effort, as well as serving as the “corporate memory” of past projects, their histories and experiences. In order to address these issues, we describe a transformational view of software development that treats the software development process as a series of artifact transformations. Each transformation has inputs and produces outputs. The use of this approach supports a very flexible software engineering metrics repository. 
72|2||Modeling change requests due to faults in a large-scale telecommunication system|It is widely known that a small number of modules in any system are likely to contain the majority of faults. Early identification and consequent attention to such modules may mitigate or prevent many defects. The objective of this study is to use product metrics to build a prediction model of the number of change requests (CRs) that are likely to occur in individual modules during testing. The study first empirically validates eight product metrics, while considering the confounding effects of code size (lines of code). Next, a prediction model of CR outcomes is developed with the validated metrics by utilizing a negative binomial regression that allows over-dispersion. In total, 816 modules written in the Chill programming language were analyzed in a large-scale telecommunication system. There is a positive association between the number of CRs and four product metrics (number of unique operators, unique operands, signals, and library calls) after considering the confounding effect of code size. A prediction model that includes only code size and the number of unique operands provides the best empirical fit. 
72|2||Captureârecapture in software inspections after 10 years researchââtheory, evaluation and application|Software inspection is a method to detect faults in the early phases of the software life cycle. In order to estimate the number of faults not found, capture–recapture was introduced for software inspections in 1992 to estimate remaining faults after an inspection. Since then, several papers have been written in the area, concerning the basic theory, evaluation of models and application of the method. This paper summarizes the work made in capture–recapture for software inspections during these years. Furthermore, and more importantly, the contribution of the papers are classified as theory, evaluation or application, in order to analyse the performed research as well as to highlight the areas of research that need further work. It is concluded that (1) most of the basic theory is investigated within biostatistics, (2) most software engineering research is performed on evaluation, a majority ending up in recommendation of the Mh–JK model, and (3) there is a need for application experiences. In order to support the application, an inspection process is presented with decision points based on capture–recapture estimates. 
72|2||A case study of a reusable component collection in the information retrieval domain|This paper reports on practical issues in the development, distribution, use, and evolution of a reusable component collection in the domain of information retrieval. 
72|2||Changing perceptions of CASE technology|The level to which CASE technology has been successfully deployed in IS and software development organisations has been at best variable. Much has been written about an apparent mismatch between user expectations of the technology and the products which are developed for the growing marketplace. In this paper we explore how this tension has developed over time, with the aim of identifying and characterising the major factors contributing to it. We identify three primary themes: volatility and plurality in the marketplace; the close relationship between tools and development methods; and the context sensitivity of feature assessment. By exploring the tension and developing these themes we hope to further the debate on how to improve evaluation of CASE prior to adoption. 
72|2||Prototyping an integrated information gathering system on CORBA|The sheer volume of information and variety of sources from which it may be retrieved on the Web make searching the sources a difficult task. Usually, meta-search engines can be used only to search Web pages or documents; other major sources such as data bases, library corpuses and the so-called Web data bases are not involved. Faced with these restrictions, an effective retrieval technology for a much wider range of sources becomes increasingly important. In our previous work, we proposed an Integrated Retrieval (IIR), which is based on Common Object Request Broker Architecture, to spare clients the trouble of complicated semantics when federating multiple sources. In this paper, we present an IIR-based prototype for integrated information gathering system. It offers a unified interface for querying heterogeneous interfaces or protocols of sources and uses SQL compatible query language for heterogeneous backend targets. We use it to link two general search engines (Yahoo and AltaVista), a science paper explorer (IEEE), and two library corpus explorers. We also perform preliminary measurements to assess the potential of the system. The results shown that the overhead spent on each source as the system queries them is within reason, that is, that using IIR to construct an integrated gathering system incurs low overhead. 
72|3|http://www.sciencedirect.com/science/journal/01641212/72/3|Editorial board|
72|3||Code compression by register operand dependency|This paper proposes a dictionary-based code compression technique that maps the source register operands to the nearest occurrence of a destination register in the predecessor instructions. The key idea is that most destination registers have a great possibility to be used as source registers in the following instructions. The dependent registers can be removed from the dictionary if this information can be specified otherwise. Such destination–source relationships are so common that making use of them can result in much better code compression. After removing the dependent register operands, the original dictionary size can be reduced significantly. As a result, the compression ratio can benefit from: (a) the reduction of dictionary size due to the removal of dependent registers, and (b) the reduction of program encoding due to the reduced number of dictionary entries.A set of programs has been compressed using this feature. The compression results show that the average compression ratio is reduced to 38.41% on average for MediaBench benchmarks compiled for MIPS R2000 processor, as opposed to 45% using operand factorization. 
72|3||A page-coherent, causally consistent protocol for distributed shared memory|Distributed Shared Memory (DSM) provides a virtual address space that is shared among processors in a distributed system. It allows application programmers to elude message passing and use the more familiar shared-memory paradigm. To increase efficiency, DSM implementations replicate memory pages, introducing the problem of consistency. As fewer restrictions are imposed to the replicas, more efficient implementations are possible, but application programming becomes more difficult. Causal consistency is a model that offers a balance, by allowing efficient implementations without significantly increasing programming difficulty. This work presents a page-coherent, causally consistent DSM protocol. This protocol requires that each time a node sends the contents of a page p, it piggybacks a list of all pages the receiver must invalidate. This list includes all pages written by operations unknown to the receiver, and that potentially causally precede the most recent write operation over p known by the sender. The protocol’s main feature is less communication overhead than other causally consistent protocols in the literature. This behavior was confirmed running test applications on top of a simple simulator. 
72|3||Adaptive message scheduling for supporting causal ordering in wide-area group communications|
72|3||A new Petri net modeling technique for the performance analysis of discrete event dynamic systems|An interesting modeling problem is the need to model one or more of the system modules without exposition to the other system modules. This modeling problem arises due to our interest in these modules or incomplete knowledge, or inherent complexity, of the rest of the system modules. Whenever the performance measures (one or more) of the desired modules are available through previous performance studies, data sheets, or previous experimental works, the required performance measures for the whole system can be predicted from our proposed modeling technique. The incomplete knowledge problem of the dynamic behavior of some system modules has been studied by control theory. In the control area, such systems are known as partially observed discrete event dynamic systems, or POS systems. To the best of our knowledge, the performance evaluation of the POS system has not been addressed by the Petri net theory yet.Therefore, in this paper, we propose a new modeling technique for solving this kind of problem based on using the Petri net theory (i.e. Stochastic Reward Nets (SRNs)) in conjunction with the optimal control theory. In this technique, we develop an SRN Equivalent Model (EM) for the modeled system. The SRN EM-model consists of two main nets and their interface nets. One of the main nets represents the part(s) of interest or the known part(s) of the overall POS system that allows us to model its dynamic behavior and evaluate its performance measures. The other main net represents the remaining part(s) of the overall POS system that feeds the part(s) of interest. The well-known maximum principles have been used to develop an algorithm for determining the unknown transition rates of the proposed model. Numerical simulations are given to show that the proposed approach is more effective than the conventional modeling techniques, especially when dealing with systems having a large number of states. 
72|3||A new disk-based technique for solving the largeness problem of stochastic modeling formalisms|Stochastic modeling formalisms such as stochastic Petri nets, generalized stochastic Petri nets, and stochastic reward nets can be used to model and evaluate the dynamic behavior of realistic computer systems. Once we translate the stochastic system model to the underlying corresponding Markov Chain (MC), the developed MC grows wildly to several hundred thousands states. This problem is known as the largeness problem. To tolerate the largeness problem of Markov models, several iterative and direct methods have been proposed in the literature. Although the iterative methods provide a feasible solution for most realistic systems, a major problem appears when these methods fail to reach a solution. Unfortunately, the direct method represents an undesirable numerical technique for tolerating large matrices due to the fill-in problem. In order to solve such problem, in this paper, we develop a disk-based segmentation (DBS) technique based on modifying the Gauss Elimination (GE) technique. The proposed technique has the capability of solving the consequences of the fill-in problem without making assumptions about the underlying structure of the Markov processes of the developed model. The DBS technique splits the matrix into a number of vertical segments and uses the hard disk to store these segments. Using the DBS technique, we can greatly reduce the memory required as compared to that of the GE technique. To minimize the increase in the solution time due to the disk accessing processes, the DBS utilizes a clever management technique for such processes. The effectiveness of the DBS technique has been demonstrated by applying it to a realistic model for the Kanban manufacturing system. 
72|3||Implementing innovative services supporting user and terminal mobility: the SCARAB architecture|The paper shows how secure telecommunication services supporting user and terminal mobility and service personalization can be designed using the SCARAB architecture, developed within the EU ACTS SCARAB project. The most important aspects of this architecture are the combined use of smart cards and mobile code technologies to solve the mobility and personalization issues, and the ability to support different types of terminals, ranging from powerful PC terminals to mobile personal digital assistants. These features are illustrated through the design of a demonstrative service. 
72|3||A software/hardware cooperated stack operations folding model for Java processors|Java has become the most important language in the Internet area, but the execution performance of Java processors is severely limited by the true data dependency inherited from the stack architecture defined by Sun's Java Virtual Machine. A sequential hardware-based folding algorithm––POC folding model was proposed in the earlier research to eliminate up to 80.1% of stack push and pop bytecodes. The remaining stack push and pop bytecodes cannot be folded due to the sequential checking characteristic of the POC folding model. In this paper, a new software/hardware cooperated folding algorithm––T-POC (Tagged-POC) folding model is proposed to enhance the folding ability of the POC-based Java processors to fold the remaining stack push and pop bytecodes. While executing the bytecodes, bytecode grouping and rescheduling are done by a T-POC bytecode rescheduler to generate the new binary class images in memory. With the cooperation of the hardware-based POC folding model, higher execution performance can be achieved by executing the newly generated class images. Statistical data show that 94.8% of stack push and pop bytecodes can be folded, and the overall execution speedups of 2-, 3-, and 4-foldable strategies are 1.72, 1.73 and 1.74, respectively, as compared to a single-pipelined stack machine without folding. 
72|3||The design and implementation of a runtime system for graph-oriented parallel and distributed programming|Graph has been widely used in modeling, specification, and design of parallel and distributed systems. Many parallel and distributed programs can be expressed as a collection of parallel functional modules whose relationships can be defined by a graph. Often, the basic functions of communication and coordination of the parallel modules are expressed in terms of the underlying graph. Furthermore, parallel/distributed graph algorithms are used to realize various control functions. To facilitate the implementation of these algorithms, it is desirable to have an integrated approach that provides direct support for efficient operations on graphs. We have proposed a graph-oriented programming model, called GOP, which aims at providing high-level abstractions for configuring and programming cooperative parallel processes. GOP enables the programmer to configure the logical structure of a distributed program by using a logical graph and to write the program using communications and synchronization primitives based on the logical structure. In this paper, we describe the design and implementation of a portable run-time system for the GOP framework. The runtime system provides an interface with a library of programming primitives to the low-level facilities required to support graph-oriented communications and synchronization. The implementation is on top of the parallel virtual machine in a local area network of Sun workstations. We focus our discussion on the following four aspects: the software architecture, including the structure of runtime system and interfaces between user programs and the runtime kernel; graph representation; implementation of graph operations; and performance of the run-time in terms of the implementation of graph-oriented communications. 
72|3||A reactive system architecture for building fault-tolerant distributed applications|Most fault-tolerant application programs cannot cope with constant changes in their environments and user requirements because they embed policies and mechanisms together so that if the policies or mechanisms are changed the whole programs have to be changed as well. This paper presents a reactive system approach to overcoming this limitation. The reactive system concepts are an attractive paradigm for system design, development and maintenance because they separate policies from mechanisms. In the paper we propose a generic reactive system architecture and use group communication primitives to model it. We then implement it as a generic package which can be applied in any distributed applications. The system performance shows that it can be used in a distributed environment effectively. 
72|3||Analyzing reconfigurable algorithms for managing replicated data|We analyze reconfigurable algorithms for managing replicated data to determine how often one should detect and react to failure conditions so that reorganization operations can be performed at the appropriate time to improve the availability of replicated data. We use dynamic voting as a case study to reveal design trade-offs for designing such reconfigurable algorithms and illustrate how often failure detection and reconfiguration activities should be performed so as to maximize data availability. Stochastic Petri nets are used as a tool to facilitate our analysis. 
72|3||A novel algorithm for multimedia multicast routing in a large scale network|A new efficient multiple-searching genetic algorithm is presented for constructing minimum-cost multicast trees. In order to have computers successfully become consumer electronics for multimedia multicast communication applications, we propose a novel multimedia multicast routing approach for the large scale network in this paper. According to our simulation results, our algorithm is robust and can obtain optimal solution of a given problem with probability for both the sparse and dense networks. It outperforms some existing promising genetic algorithms. 
72|3||The design and analysis of a quantitative simulator for dynamic memory management|The use of object-oriented programming in software development allows software systems to be more robust and more maintainable. At the same time, the development time and expense are also reduced. To achieve these benefits, object-oriented applications use dynamic memory management (DMM) to create generic objects that can be reused. Consequently, these applications are often highly dynamic memory intensive. For the last three decades, several DMM schemes have been proposed. Such schemes include first fit, best fit, segregated fit, and buddy systems. Because the performance (e.g., speed, memory utilization, etc.) of each scheme differs, it becomes a difficult choice in selecting the most suitable approach for an application and what parameters (e.g., block size, etc.) should be adopted.In this paper, a DMM simulation tool and its usage are presented. This tool receives DMM traces of C/C++ or Java programs and performs simulation according to the scheme (first fit, best fit, buddy system, and segregated fit) defined by the user. Techniques required to obtain memory traces are presented. At the end of each simulation run, a variety of performance metrics are reported to the users. By using this tool, software engineers can evaluate system performance and decide which algorithm is the most suitable. Moreover, hardware engineers can perform a system analysis before hardware (e.g., modified buddy system, first fit, etc.) is fabricated. 
72|3||A client-based logging technique using backward analysis of log in client/server environment|The existing recovery technique using the logging technique in client/server database systems only administers the log as a whole in a server. This recovery technique contains the logging record transmission cost on transactions that are potentially executed in each client and increases network traffic. In this paper, a logging technique for redo-only log is suggested. The new technique removes redundant before-images and supports client-based logging to eliminate the transmission cost of the logging record. Also, in the case of a client crash, redo recovery through a backward client analysis log is performed in a self-recovering way. In the case of a server crash, the after-images of pages which need recovery through simultaneous backward analysis log are only transmitted and redo recovery is accomplished through the received after-image and backward analysis log. We also select a comparison model to estimate the performance of the proposed recovery technique. Finally, in this paper, the superiority of the suggested technique was proved by comparing and analyzing redo time and recovery time of various techniques under changing number of clients and updated operations. 
72|3||Contents Volume 72|
73|1|http://www.sciencedirect.com/science/journal/01641212/73/1|Editorial board|
73|1||Performance modeling and analysis of computer systems and networks|
73|1||Broadcasting schemes for hypercubes with background traffic|Optimal broadcasting schemes for interconnection networks (INs) are most essential for the efficient interprocess communication amongst parallel computers. In this paper two novel broadcasting schemes are proposed for hypercube computers with bursty background traffic and a single-port mode of message passing communication. The schemes utilize a maximum entropy (ME) based queue-by-queue decomposition algorithm for arbitrary queueing network models (QNMs) [D.D. Kouvatsos, I. Awan, Perform. Eval. 51 (2003) 191] and are based on binomial trees and graph theoretic concepts. It is shown that the overall cost of the one-to-all broadcasting scheme is given by max{Ï1,Ï2,…,Ï2n/2}, where Ïi, i=1,2,…,2n/2 is the total weight at each leaf node of the binomial tree and n is the degree of the hypercube. Moreover, the upper bound of the total cost of the neighbourhood broadcasting scheme is determined by ∑i=1Fmax{Ïi}, where F is an upper bound of the number of steps and is equal to 1.33⌈log2(n−1)⌉+1. Evidence based on empirical studies indicates the suitability of the schemes for achieving optimal broadcasting costs. 
73|1||Design and analysis of a replicated elusive server scheme for mitigating denial of service attacks|The paper proposes a scheme, referred to as proactive server roaming, to mitigate the effects of denial of service (DoS) attacks. The scheme is based on the concept of “replicated elusive service”, which through server roaming, causes the service to physically migrate from one physical location to another. Furthermore, the proactiveness of the scheme makes it difficult for attackers to guess when or where servers roam. The combined effect of elusive service replication and proactive roaming makes the scheme resilient to DoS attacks, thereby ensuring a high-level of quality of service. The paper describes the basic components of the scheme and discusses a simulation study to assess the performance of the scheme for different types of DoS attacks. The details of the NS2-based design and implementation of the server roaming strategy to mitigate the DoS attacks are provided, along with a thorough discussion and analysis of the simulation results. 
73|1||File distribution using a peer-to-peer networkââa simulation study|As the Internet became widespread, people saw it as a way of distributing content. But while the average bandwidth capacity is increasing, users around the world are trying to share more data. Although the servers are able to acquire more bandwidth, they cannot keep up with the rapidly increasing requests of the users. Several systems appeared that alleviate the server from the dissemination process. But such systems are not always suitable for disseminating highly anticipated files of considerable size. The evolution of peer-to-peer systems gave a new way of attacking this problem. Each user can assist the dissemination process by acting as a server as well. In this paper the way traditional systems meet users' demands is demonstrated, and simulation results of a peer-to-peer approach based on a mobile agent platform are presented. Our focus is on how different parameters can affect the dissemination process of a highly anticipated file in this network. 
73|1||How accurate should early design stage power/performance tools be? A case study with statistical simulation|To cope with the widening design gap, the ever increasing impact of technology, reflected in increased interconnect delay and power consumption, and the time-consuming simulations needed to define the architecture of a microprocessor, computer engineers need techniques to explore the design space efficiently in an early design stage. These techniques should be able to identify a region of interest with desirable characteristics in terms of performance, power consumption and cycle time. In addition, they should be fast since the design space is huge and the design time is limited. In this paper, we study how accurate early design stage techniques should be to make correct design decisions. In this analysis we focus on relative accuracy which is more important than absolute accuracy at the earliest stages of the design flow. As a case study we demonstrate that statistical simulation is capable of making viable microprocessor design decisions efficiently in early stages of a microprocessor design while considering performance, power consumption and cycle time. 
73|1||Approximating the connectivity between nodes when simulating large-scale mobile ad hoc radio networks|
73|1||Proactive QoS negotiation in asynchronous real-time distributed systems|We present a fast, proactive, quality of service (QoS) negotiation algorithm called Best Effort Negotiation (or BEN), for asynchronous real-time distributed systems. BEN considers an application model where trans-node application timeliness and fault-tolerance requirements are expressed using benefit functions, and anticipated workload and system failure rates during future time intervals are expressed using adaptation functions and reliability functions, respectively. Furthermore, BEN considers an adaptation model where subtasks of application tasks are replicated at run-time for tolerating failures as well as for sharing workload increases. Given such models, the objective of the algorithm is to maximize the sum of aggregate real-time and fault-tolerance benefits during the time window of adaptation functions. Since determining the optimal solution is computationally intractable, BEN heuristically computes sub-optimal resource allocations in polynomial-time. To determine how well BEN performs, we describe another algorithm called HLC, that is inspired by the well-known Hill Climbing heuristic. We show that HLC is significantly slower than BEN. However, our experimental studies reveal that the performance of BEN, in general, is as good as that of HLC. 
73|1||Feasibility analysis of hard real-time periodic tasks|The problem of determining the feasibility of hard real-time periodic tasks is known to be co-NP-complete when there exists a task with relative deadline shorter than its period. Thus “Processor Demand Approach (PDA)” for synchronous task sets has been considered as a practical tool to solve the feasibility problem. PDA determines the feasibility of a task set by checking whether there exists a task whose deadline is missed by a certain time which is computed based on the characteristic of the task set. In this paper, we present a new method for feasibility test by combining PDA with the analysis methods for aperiodic scheduling. It is shown that the number of tests required of the new method to determine the feasibility is never greater than the smallest number of tests the existing algorithm requires. Although our method has pseudo-polynomial time complexity, experimental results show that the new method requires significantly less computation to determine feasibility. 
73|1||A cost model for spatio-temporal queries using the TPR-tree|A query optimizer requires cost models to calculate the costs of various access plans for a query. An effective method to estimate the number of disk (or page) accesses for spatio-temporal queries has not yet been proposed. The TPR-tree is an efficient index that supports spatio-temporal queries for moving objects. Existing cost models for the spatial index such as the R-tree do not accurately estimate the number of disk accesses for spatio-temporal queries using the TPR-tree, because they do not consider the future locations of moving objects, which change continuously as time passes.In this paper, we propose an efficient cost model for spatio-temporal queries to solve this problem. We present analytical formulas which accurately calculate the number of disk accesses for spatio-temporal queries. Extensive experimental results show that our proposed method accurately estimates the number of disk accesses over various queries to spatio-temporal data combining real-life spatial data and synthetic temporal data. To evaluate the effectiveness of our method, we compared our spatio-temporal cost model (STCM) with an existing spatial cost model (SCM). The application of the existing SCM has the average error ratio from 52% to 93%, whereas our STCM has the average error ratio from 11% to 32%. 
73|1||An efficient query optimization strategy for spatio-temporal queries in video databases|The interest for multimedia database management systems has grown rapidly due to the need for the storage of huge volumes of multimedia data in computer systems. An important building block of a multimedia database system is the query processor, and a query optimizer embedded to the query processor is needed to answer user queries efficiently. Query optimization problem has been widely studied for conventional database systems; however it is a new research area for multimedia database systems. Due to the differences in query processing strategies, query optimization techniques used in multimedia database systems are different from those used in traditional databases. In this paper, a query optimization strategy is proposed for processing spatio-temporal queries in video database systems. The proposed strategy includes reordering algorithms to be applied on query execution tree. The performance results obtained by testing the reordering algorithms on different query sets are also presented. 
73|1||ACODF: a novel data clustering approach for data mining in large databases|In this paper, we present an efficient clustering approach for large databases. Our simulation results indicate that the proposed novel clustering method (called ant colony optimization with different favor algorithm) performs better than the fast self-organizing map (SOM) combines K-means approach (FSOM+K-means) and genetic K-means algorithm (GKA). In addition, in all the cases we studied, our method produces much smaller errors than both the FSOM+K-means approach and GKA. 
73|1||On accessing data in high-dimensional spaces: A comparative study of three space partitioning strategies|While experience shows that contemporary multi-dimensional access methods perform poorly in high-dimensional spaces, little is known about the underlying causes of this important problem. One of the factors that has a profound effect on the performance of a multi-dimensional structure in high-dimensional situations is its space partitioning strategy. This paper investigates the partitioning strategies of KDB-trees, the Pyramid Technique, and a new point access method called the Îs Technique. The paper reveals important dimensionality problems associated with these strategies and shows how each strategy affects the retrieval performance across a range of spaces with varying dimensionalities. The Pyramid Technique, which is frequently regarded as the state-of-the-art access method for high-dimensional data, suffers from numerous problems that become particularly severe with highly skewed data in heavily sparse spaces. Although the partitioning strategy of KDB-trees incurs several problems in high-dimensional spaces, it exhibits a remarkable adaptability to the changing data distributions. However, the experimental evidence gathered on both simulated and real data sets shows that the Îs Technique generally outperforms the other two schemes in high-dimensional spaces, usually by a significant margin. 
73|1||Supporting metasearch with XSL|Metasearch engines offer better coverage and are more fault-tolerant and expandable than single search engines. A metasearch engine is required to post queries with and obtain retrieval results from several other Internet search engines. In this paper, we describe the use of the extensible style language (XSL) to support metasearches. We show how XSL can transform a query, expressed in XML, into different forms for different search engines. We show how the retrieval results could be transformed into a standard format so that the metasearch engine can interpret the retrieved data, filtering the irrelevant information (e.g. advertisement). The proposed structure treats the metasearch engine and the individual search engines as separate modules with a clearly defined communication structure through XSL. Thus, the system is more extensible than coding the structure and syntactic transformation processes. It allows other new search engines to be included just through plug-and-play, requiring only that the new transformation of XML for this search engine be included in the XSL. 
73|1||A top-down approach for density-based clustering using multidimensional indexes|Clustering on large databases has been studied actively as an increasing number of applications involve huge amount of data. In this paper, we propose an efficient top-down approach for density-based clustering, which is based on the density information stored in index nodes of a multidimensional index. We first provide a formal definition of the cluster based on the concept of region contrast partition. Based on this notion, we propose a novel top-down clustering algorithm, which improves the efficiency through branch-and-bound pruning. For this pruning, we present a technique for determining the bounds based on sparse and dense internal regions and formally prove the correctness of the bounds. Experimental results show that the proposed method reduces the elapsed time by up to 96 times compared with that of BIRCH, which is a well-known clustering method. The results also show that the performance improvement becomes more marked as the size of the database increases. 
73|2|http://www.sciencedirect.com/science/journal/01641212/73/2|Editorial board|
73|2||Applications of statistics in software engineering|
73|2||Statistical significance testingââa panacea for software technology experiments?|Empirical software engineering has a long history of utilizing statistical significance testing, and in many ways, it has become the backbone of the topic. What is less obvious is how much consideration has been given to its adoption. Statistical significance testing was initially designed for testing hypotheses in a very different area, and hence the question must be asked: does it transfer into empirical software engineering research? This paper attempts to address this question. The paper finds that this transference is far from straightforward, resulting in several problems in its deployment within the area. Principally problems exist in: formulating hypotheses, the calculation of the probability values and its associated cut-off value, and the construction of the sample and its distribution. Hence, the paper concludes that the topic should explore other avenues of analysis, in an attempt to establish which analysis approaches are preferable under which conditions, when conducting empirical software engineering studies. 
73|2||BBN-based software project risk management|This paper presents a scheme to incorporate BBNs (Bayesian belief networks) in software project risk management. A theoretical model is defined to provide insights into risk management. Based on these insights, we have developed a BBN-based procedure using a feedback loop to predict potential risks, identify sources of risks, and advise dynamic resource adjustment. This approach facilitates the visibility and repeatability of the decision-making process of risk management. Both analytical and simulated cases are reported. 
73|2||Using multiple adaptive regression splines to support decision making in code inspections|Inspections have been shown to be an effective means of detecting defects early on in the software development life cycle. However, they are not always successful or beneficial as they are affected by a number of technical and managerial factors. To make inspections successful, one important aspect is to understand what are the factors that affect inspection effectiveness (the rate of detected defects) in a given environment, based on project data. In this paper we collected data from over 230 code inspections and performed a multivariate statistical analysis in order to look at how management factors, such as the effort assigned and the inspection rate, affect inspection effectiveness. Because the functional form of effectiveness models is a priori unknown, we use a novel exploratory analysis technique: multiple adaptive regression splines (MARS). We compare the MARS model with more classical regression models and show how it can help understand the complex trends and interactions in the data, without requiring the analyst to rely on strong assumptions. Results are reported and discussed in light of existing studies. 
73|2||Computing system reliability using Markov chain usage models|Markov chains have been used successfully to model system use, generate tests, and compute statistics about anticipated system use in the field. Several reliability models are in use for Markov chain-based testing, but each has certain limitations. A Bayesian reliability model that is gaining support in field use is presented here. 
73|2||Applications of clustering techniques to software partitioning, recovery and restructuring|The artifacts constituting a software system are sometimes unnecessarily coupled with one another or may drift over time. As a result, support of software partitioning, recovery, and restructuring is often necessary. This paper presents studies on applying the numerical taxonomy clustering technique to software applications. The objective is to facilitate those activities just mentioned and to improve design, evaluation and evolution. Numerical taxonomy is mathematically simple and yet it is a useful mechanism for component clustering and software partitioning. The technique can be applied at various levels of abstraction or to different software life-cycle phases. We have applied the technique to: (1) software partitioning at the software architecture design phase; (2) grouping of components based on the source code to recover the software architecture in the reverse engineering process; (3) restructuring of a software to support evolution in the maintenance stage; and (4) improving cohesion and reducing coupling for source code. In this paper, we provide an introduction to the numerical taxonomy, discuss our experiences in applying the approach to various areas, and relate the technique to the context of similar work. 
73|2||Assessing the cost-effectiveness of software reuse: A model for planned reuse|Information systems development is typically acknowledged as an expensive and lengthy process, often producing code that is of uneven quality and difficult to maintain. Software reuse has been advocated as a means of revolutionizing this process. The claimed benefits from software reuse are reduction in development cost and time, improvement in software quality, increase in programmer productivity, and improvement in maintainability. Software reuse entails undeniable costs of creating, populating, and maintaining a library of reusable components. There is anecdotal evidence to suggest that some organizations benefit from reuse. However, many software developers practicing reuse claim these benefits without formal demonstration thereof. There is little research to suggest when the benefits are expected and to what extent they will be realized. For example, does a larger library of reusable components lead to increased savings? What is the impact of component size on the effectiveness of reuse? This research seeks to address some of these questions. It represents the first step in a series wherein the effects of software reuse on overall development effort and costs are modeled with a view to understanding when it is most effective. 
73|2||Applying sampling to improve software inspections|The main objective of software inspections is to find faults in software documents. The benefits of inspections are reported from researchers as well as software organizations. However, inspections are time consuming and the resources may not be sufficient to inspect all documents. Sampling of documents in inspections provides a systematic solution to select what to be inspected in the case resources are not sufficient to inspect everything. The method presented in this paper uses sampling, inspection and resource scheduling to increase the efficiency of an inspection session. A pre-inspection phase is used in order to determine which documents need most inspection time, i.e. which documents contain most faults. Then, the main inspection is focused on these documents. We describe the sampling method and provide empirical evidence, which indicates that the method is appropriate to use. A Monte Carlo simulation is used to evaluate the proposed method and a case study using industrial data is used to validate the simulation model. Furthermore, we discuss the results and important future research in the area of sampling of software inspections. 
73|2||Resource constraints analysis of workflow specifications|A workflow specification is a formal description of business processes in the real world. Its correctness is critical to the workflow execution and hence the realisation of business objectives. In addition to structural and temporal constraints, resource constraints are also implied in workflow specifications. Therefore, they should be analysed to ensure that the workflow specification is resource consistent at build-time. In this paper, we first identify the problem of resource constraints in a workflow specification. Then we propose an innovative approach with corresponding algorithms to the checking of resource consistency for a workflow specification. Furthermore, we extend our analysis work to timed workflow specifications, where time information is taken into consideration for the checking of the resource consistency of a workflow specification. The work reported in this paper provides a theoretical foundation for workflow modeling and analysis in workflow management. 
73|2||Multi-devices âMultipleâ user interfaces: development models and research opportunities|Today, Internet-based appliances can allow a user to interact with the server-side services and information using different kinds of computing platforms including traditional office desktops, palmtops, as well as a large variety of wireless devices including mobile telephones, Personal Digital Assistants, and Pocket Computers. This technological context imposes new challenges in user interface software engineering, as it must run on different computing platforms accommodating the capabilities of various devices and the different contexts of use. Challenges are triggered also because of the universal access requirements for a diversity of users. The existing approaches of designing a single user interface using one computing platform do not adequately address the challenges of diversity, cross-platform consistency, universal accessibility and integration. Therefore, there is an urgent need for a new integrative framework for modeling, designing and evaluating multi-device user interfaces for the emerging generation of interactive systems. This paper begins by describing a set of constraints and characteristics intrinsic to multi-device user interfaces, and then by examining the impacts of these constraints on the specification, design and validation processes. Then, it discusses the research opportunities in important topics relevant to multi-device user interface development, including task and model-based, pattern-driven and device-independent development. We will highlight how research in these topics can contribute to the emergence of an integrative framework for Multiple-User Interface design and validation. 
73|2||A simple and powerful type system for programming languages|A type system serves to guarantee that programs mix data of distinct types only as allowed by programmer declared rules. For example, a function declaration corresponds to one of these rules, providing a way to mix data of distinct types, possibly producing data of another type as a result. A type system also discovers the type of each (sub)expression throughout a program.This paper introduces a type system where types are parts of speech, and all relations among types are represented by a grammar, a grammar whose rules are written in terms of the types. In order to resolve types, i.e., discover which type each (sub)expression is, the type grammar may be used to parse a given program, as a second pass.A brief, Polish postfix programming language rich in types is presented, called the type grammar, as well as a way to translate down from a more natural syntax. We show how to extend the type grammar with new rules based on program declarations. By parsing with this postfix type grammar, unrestricted overloading (more powerful than Java’s) and unrestricted programmer-defined coercions (in some ways more powerful than multiple inheritance) may be implemented and understood in a straightforward manner. To realize these, a powerful parser like Earley’s efficient context-free parser, or others, which tolerate ambiguity efficiently, are required. We conclude by showing how a pair of coercions, in a cycle, proves useful. 
73|2||Knowledge centered assessment pattern: an effective tool for assessing safety concerns in software architecture|In software-based systems, the notion of software failure is magnified if the software in question is a component of a safety critical system. Hence, to ensure a required level of safety, the product must undergo expensive rigorous testing and verification/validation activities. To minimize the cost of quality (COQ) associated with the development of safety critical systems, it becomes imperative that the assessment of intermediate artifacts (e.g., requirement, design documents or models) is done efficiently and effectively to maximize early defect detection and/or defect prevention. However, as a human-centered process, the assessment of software architecture for safety critical systems relies heavily on the experience and knowledge of the assessment team to ensure that the proposed architecture is consistent with the software functional and safety requirements.The knowledge centered assessment pattern (KCAP) acts as effective tool to assist assessment teams by providing key information on what architectural elements should be assessed, why they should to be assessed, and how they should be assessed. Furthermore, the use of KCAP highlights cases where the software architecture has been properly, over, under, or incoherently engineered. 
73|2||Selecting components in large COTS repositories|The growing availability of COTS (commercial-off-the-shelf) components in the software market has concretized the possibility of building whole systems based on components. In this multitude, a recurrent problem is the location and selection of the components that best fit the requirements. Commercial repositories that offer search mechanisms have reduced these difficulties: system integrators can rely on a wider variety of components and can focus better on the composition of systems. The size of the repository can be an initial obstacle but iterative approaches allow integrators to familiarize with the repository's structure and to formulate effective queries. This paper discusses the search techniques in CLARiFi, a component broker project that supports integrators in the selection of components for systems. 
73|2||A framework instantiation approach based on the Features Model|The extreme competitiveness of the contemporary economy generates a huge demand for cheaper, efficient and reliable software products, which often are developed under great pressures of time and budget. These premises suggest that software development must take place in an environment where proven solutions can be modified, combined and adapted to be used in the development of new products. Therefore, the use of object-oriented frameworks, or frameworks for short, seems to be one of the most promising techniques for code and design reuse. Nevertheless, one of the obstacles that has to be removed before the widespread use of frameworks is the great amount of time for study necessary to become proficient in the use of a specific framework. This situation occurs due to the inherent complexity of the design of the frameworks, which are conceived to fulfill the requirements of an entire application family.In this context, this work presents a detailed description of a high-level framework instantiation process based on the Features Model. This model is responsible for presenting a simplified view of the framework's main functional and technological characteristics to application developers. In a subsequent step, the instantiation process adapts the original design of the framework to produce a new design for a specific member of an application family. All of the adaptation's steps will be based on choices available in the Features Model. 
73|2||Quantitative evaluation of safety critical software testability based on fault tree analysis and entropy|One of the definitions for testability is the probability whether tests will detect a fault, given that a fault in the program exists. The testability can be estimated from the probability of each statement fault leading to output failure. The probability of the test detecting a fault depends on the probability of individual statement faults appearing as an output failure when a fault exists at a statement. The testability measure of the software has been introduced based on output failure probability and the entropy of the importance of basic statements to the output failure from the software fault tree analysis. The output failure probability and the importance of statements are calculated from software fault tree analysis. The suggested testability measure has been applied to the two modules of the safety system in a nuclear power plant. The proposed testability measure can be used for the selection of output variables or to determine the modules that are more vulnerable to undetected faults. 
73|2||Task-directed software inspection|Software inspection is recognized as an effective verification technique. Despite this fact, the use of inspection is surprisingly low. This paper describes a new inspection technique, called task-directed inspection (TDI), and a light-weight process, that were used to introduce inspection in a particular industrial environment. This environment had no history of inspections, was resistant to the idea of inspection, but had a situation where confidence in a safety-related legacy suite of software had to be increased. The characteristics of TDI are explored. They give rise to a variety of approaches that may encourage more widespread use of inspections. This paper examines the industrial exercise as a case study, with the intent that it be useful in other situations that share characteristics with the situation described. 
73|3|http://www.sciencedirect.com/science/journal/01641212/73/3|Editorial board|
73|3||Solving the invalid signer-verified signature problem and comments on XiaâYou group signature|In general, a signature that has been verified by the signers should be valid when the verifiers verify it later. However, a signer-verified signature would become invalid in Wang et al.'s generalized threshold signature and authenticated encryption scheme and Hsu et al.'s revisions. This problem has not been reported before, and it is called the invalid signer-verified signature problem. This paper discusses the problem and proposes the solution. By the way, we also point out the forgery attack on Xia–You recently published group signature scheme. 
73|3||Role-based authorizations for workflow systems in support of task-based separation of duty|Role-based authorizations for assigning tasks of workflows to roles/users are crucial to security management in workflow management systems. The authorizations must enforce separation of duty (SoD) constraints to prevent fraud and errors. This work analyzes and defines several duty-conflict relationships among tasks, and designs authorization rules to enforce SoD constraints based on the analysis. A novel authorization model that incorporates authorization rules is then proposed to support the planning of assigning tasks to roles/users, and the run-time activation of tasks. Different from existing work, the proposed authorization model considers the AND/XOR split structures of workflows and execution dependency among tasks to enforce separation of duties in assigning tasks to roles/users. A prototype system is developed to realize the effectiveness of the proposed authorization model. 
73|3||Fuzzy resource space model and platform|The resource space model (RSM) is a model for organizing versatile resources in normal forms and providing uniform resource management operations. In applications, we find three important factors that influence the effectiveness of organizing and operating resources: natural semantics of resources, resource providers' beliefs, and resource users' beliefs. The relationship between the three factors influences the effectiveness of resource management operations. This paper proposes a fuzzy resource space model (FRSM), which consists of a fuzzy resource space and a fuzzy operation language expressing the resource providers' beliefs and the resource users' beliefs. By properly dealing with the two kinds of beliefs, the FRSM improves the RSM in effectively managing resources. The proposed model has been implemented in the Knowledge Grid platform VEGA-KG. 
73|3||A new design of efficient partially blind signature scheme|Recently, the techniques of the partially blind signatures played an important role in many electronic commerce applications. Therefore, to guarantee the quality of the growing popular communication services, it is urgent to construct low-computation partially blind signature scheme for both parties of the signer and the requester to obtain a signature. So far there is no efficient and secure partially blind signature scheme for the signer and a signature requester. Based on the discrete logarithm and the Chinese Remainder Theorem, we propose an efficient and secure partially blind signature scheme for both the signer and the requester to obtain a signature. Moreover, the proposed scheme is easier than the existing partially blind signature schemes in the blinding procedure. Hence, our proposed scheme is suitable for the limited computation capacities of requesters such as smart cards or mobile units. Similarly, our scheme can let the signer provide more efficient services to the signature requester. So, it is very useful for many applications. 
73|3||Secret image sharing with steganography and authentication|A novel approach to secret image sharing based on a (k,n)-threshold scheme with the additional capabilities of steganography and authentication is proposed. A secret image is first processed into n shares which are then hidden in n user-selected camouflage images. It is suggested to select these camouflage images to contain well-known contents, like famous character images, well-known scene pictures, etc., to increase the steganographic effect for the security protection purpose. Furthermore, an image watermarking technique is employed to embed fragile watermark signals into the camouflage images by the use of parity-bit checking, thus providing the capability of authenticating the fidelity of each processed camouflage image, called a stego-image. During the secret image recovery process, each stego-image brought by a participant is first verified for its fidelity by checking the consistency of the parity conditions found in the image pixels. This helps to prevent the participant from incidental or intentional provision of a false or tampered stego-image. The recovery process is stopped if any abnormal stego-image is found. Otherwise, the secret image is recovered from k or more authenticated stego-images. Some effective techniques for handling large images as well as for enhancing security protection are employed, including pixelwise processing of the secret image in secret sharing, use of parts of camouflage images as share components, adoption of prime-number modular arithmetic, truncation of large image pixel values, randomization of parity check policies, etc. Consequently, the proposed scheme as a whole offers a high secure and effective mechanism for secret image sharing that is not found in existing secret image sharing methods. Good experimental results proving the feasibility of the proposed approach are also included. 
73|3||A practical experience in workspace separation for developing multiple storefronts on customized commerce engines|In this paper, we describe our experience in separating workspaces, using the IBM VisualAge for Java development tool, for multiple web storefronts development for several major IBM public and private commerce sites. There was a need to create multiple workspaces since different storefronts may be developed at the same time. In addition, we wanted to keep the completed software packages for compatibility checking. Instead of tuning the current program setting to a desired mixed-version environment for each project, we generate a dedicated workspace by applying dependency analysis. New icons are created to conveniently invoke the development tool under different software packages and workspaces. We also present a systematic approach to facilitate the generation of a plan with instruction steps to produce new workspaces for their dedicated and required resources. 
73|3||Providing flexible access control to an information flow control model|Protecting privacy within an application is essential. Many information flow control models have been developed for that protection. We developed an information flow control model based on role-based access control (RBAC) for object-oriented systems, which is called OORBAC (object-oriented role-based access control). According to the experiences of using OORBAC, we found that a model allowing every secure information flow and blocking every non-secure flow is too restricted. We propose that the following flexible access control features should be offered: (a) non-secure but harmless information flows should be allowed and (b) secure but harmful information flows should be blocked. According to our survey, no existing model offers the above control. We thus revised OORBAC to offer the control. The revised OORBAC have been implemented and evaluated. This paper presents flexible access control in the revised OORBAC and the evaluation result. 
73|3||Secure one snapshot protocol for concurrency control in real-time stock trading systems|To prevent any data from being accessed by unauthorized users, it is necessary for stock trading systems (STS) to use multilevel secure database management systems in controlling concurrent executions among multiple transactions. In STS, analytical transactions as well as mission critical transactions are executed concurrently, which makes it difficult to use traditional secure real-time transaction management schemes for STS environment. In this paper, we propose the read-down relationship-based secure one snapshot protocol (SOS) that is devised for the secure real-time transaction management in STS. By maintaining an additional one snapshot as well as working database, SOS blocks covert-channels without causing the priority inversion phenomenon. We introduce the process of SOS protocol with some examples, present the proofs of devised protocol, and then evaluate the performance gains by means of simulation method. 
73|3||Retrieve images by understanding semantic links and clustering image fragments|The main obstacle to realize real semantic-based image retrieval is that semantic description of versatile images is difficult. The basic ideas of this paper are that the semantics of an object can be refined through top–down orthogonal semantic classification and that the semantics of an object can be reflected by the semantics of relevant objects and the semantic relationships between them. To reflect the semantic relationships between images, we propose a set of primitive semantic links as the enhancement of the hyperlinks connecting Web pages. The semantic link network is the natural extension of the hyperlink network so it can inherit the existing theory and method on hyperlink network. Based on the single semantic image established upon the orthogonal semantic space and the semantic link space, the proposed image retrieval approach enables users to obtain the semantic clustering of relevant images rather than a list of isolated images as the output of the current search engine and to browse images along semantic paths with the support of semantic link reasoning. Semantic matching and reasoning for realizing intelligent semantic image retrieval is based on graph operation and matrix operation. 
73|3||A simulated annealing approach for multimedia data placement|
73|3||Temporal moving pattern mining for location-based service|The primary objective of location-based service (LBS) which is generally described as a mobile information service is to provide useful location aware information, at a minimum cost and resources, to its users. This functionality can be implemented through data mining techniques. However, since the conventional studies on data mining do not consider spatial and temporal aspects of data simultaneously, these techniques have limited application in studying the moving objects of LBS with respect to the spatial attributes that is changing over time. Defining individual users of LBS as moving objects, this paper proposes a new data mining technique and algorithms for identifying temporal patterns from series of locations of moving objects that have temporal and spatial dimensions. For this purpose, we use the spatial operation to generalize a location of moving point, applying time constraints between locations of moving objects to make valid moving sequences. Through the experiments, we show that our technique generates temporal patterns found in frequent moving sequences in efficient. Finally, the spatio-temporal technique proposed in this work is an innovative approach in providing knowledge applicable to improving the quality of LBS. 
73|3||Performance evaluation of a database of repetitive elements in complete genomes|The analysis of repetitive elements reveals repetitive elements in our genome may have been very important in the evolutionary genomics. In this work, we propose approaches to improve the performance of the repetitive elements in a database, namely Repetitive Sequence Database (RSDB).1 There are hundreds of millions of repetitive elements in RSDB. Performance evaluation and tuning are critical for us to manage such a large database. Not only does this study have a lot of performance improvements on centralized RSDB, but also this study provides a comparison of performance between centralized RSDB and distributed RSDB. From the experimental results, we find the improvements proposed speed up our RSDB very much. 
73|3||On the efficiency of nonrepudiable threshold proxy signature scheme with known signers|In the (t,n) proxy signature scheme, the signature signed by the original signer can be signed by t or more proxy signers out of a proxy group of n proxy signers. Recently, Hsu et al. proposed a nonrepudiable threshold proxy signature scheme with known signers. In this article, we shall propose an improvement of Hsu et al.'s scheme that is more efficient in terms of computational complexity and communication cost. 
73|3||A video caching policy for providing differentiated service grades and maximizing system revenue in hierarchical video servers|A video server normally targets at providing abundant bandwidth access and massive storage in supporting large-scale video archival applications. Its performance is sensitive to the deployment of the stored contents. In this paper, we propose a video caching policy for a video server, based on the knowledge of video profiles, namely: access rate, video size and bandwidth, tolerable rejection probability, and rental price. We consider the video server as having a hierarchical architecture which consists of a set of high-speed disk drives located in the front end for caching a subset of videos, and another set of high-capacity tertiary devices located in the back end for archiving the entire video collection. The front-end disks particularly, are organized together by employing a proposed data striping scheme, termed the adaptive striping (AS), which is flexible on heterogeneous disk integration. The proposed policy determines what video set should be cached, and how to arrange them in the front-end disks with two objectives in mind: (1) offering differentiated service grades conforming to the video profiles as well as (2) maximizing the overall system revenue. We simulate the system with various configurations, and the results affirm our effective approach. 
73|3||Modeling and implementation of digital rights|The widespread use of the Internet raises issues regarding intellectual property. After content is downloaded, no further protection is provided on that content. DRM (Digital Rights Management) technologies were developed to ensure the protection of digital information. In this paper, we explore the issues associated with content distribution and digital rights management. We develop a modified Imprimatur model based on a multi-level content distribution model and, based on those two models, we present the formal model of a new DRM system and identify the properties that a DRM system should have. We design a prototype DRM system that distributes due royalties among participants, enforces content usage rules, and distributes content keys securely in the content distribution channel. 
73|3||An adaptable vertical partitioning method in distributed systems|Vertical partitioning is a process of generating the fragments, each of which is composed of attributes with high affinity. The concept of vertical partitioning has been applied to many research areas, especially databases and distributed systems, in order to improve the performance of query execution and system throughput. However, most previous approaches have focused their attention on generating an optimal partitioning without regard to the number of fragments finally generated, which is called best-fit vertical partitioning in this paper. On the other hand, there are some cases that a certain number of fragments are required to be generated by vertical partitioning, called n-way vertical partitioning in this paper. The n-way vertical partitioning problem has not fully investigated.In this paper, we propose an adaptable vertical partitioning method that can support both best-fit and n-way vertical partitioning. In addition, we present several experimental results to clarify the validness of the proposed algorithm. 
73|3||Contents Volume 73|
74|1|http://www.sciencedirect.com/science/journal/01641212/74/1|Editorial board|
74|1||Contents|
74|1||Automated Component-Based Software Engineering|
74|1||Deployed software component testing using dynamic validation agents|Software component run-time characteristics are highly dependent on their actual deployment situation. Validating that software components meet required functional and non-functional properties is time consuming and for some properties quite challenging. We describe the use of “validation agents” to automate the testing of deployed software components to verify that component functional and non-functional properties are met. Our validation agents utilise “component aspects” that describe functional and non-functional cross-cutting concerns impacting software components. Aspect information is queried by our validation agents and these construct and run automated tests on the deployed software components. The agents then determine if the deployed components meet their aspect-described requirements. Some agents deploy an existing performance test-bed generation tool to run realistic loading tests on these components. We describe the motivation for our work, how component aspects are designed and encoded, our automated agent-based testing process, the architecture and implementation or our validation agents, and our experiences using them. 
74|1||Towards automatic monitoring of component-based software systems|The quality of software components is very important for the overall service quality of the component-based software systems. Several factors make exhaustive testing of components very difficult. Furthermore, the behavioral correctness of each independently produced component does not guarantee the behavioral correctness of the composed software system. Experience shows that there are faults in components which elude the testing effort and do not surface until the system is operating. In this paper, a specification-based software monitor is presented which can be used for detecting certain kinds of errors and failures of a component as well as the whole system while the system is operating. The behavior of each component is assumed to be specified in a formalism based on communicating finite state machines with addressing variables, and inter-component communications are achieved via asynchronous message passing. The monitor passively observes the external input/output and receives partial state information of the target system or component. These are used to interpret the specification. The approach is compositional as it achieves global monitoring by analyzing the behavior of the components of a system individually, and then combining the results obtained from the independent component analyses. The paper describes the architecture and operations of the monitor and includes illustrative examples. Techniques for dealing with non-determinism and concurrency issues in monitoring a concurrent component-based system are also discussed. 
74|1||A data-centric approach to composing embedded, real-time software components|Software for embedded systems must cope with a variety of stringent constraints, such as real-time requirements, small memory footprints, and low power consumption. It is usually implemented using low-level programming languages, and as a result has not benefitted from component-based software development techniques. This paper describes a data-centric component model for embedded devices that (i) minimizes the number of concurrent tasks needed to implement the system, (ii) allows one to verify whether components meet their deadlines by applying rate monotonic analysis, and (iii) can generate and verify schedules using constraint logic programming. This model forms the foundation for a suite of tools for specifying, composing, verifying and deploying embedded software components developed in the context of the Pecos project. 
74|1||Performance prediction of component-based applications|One of the major problems in building large-scale enterprise systems is anticipating the performance of the eventual solution before it has been built. The fundamental software engineering problem becomes more difficult when the systems are built on component technology. This paper investigates the feasibility of providing a practical solution to this problem. An empirical approach is proposed to determine the performance characteristics of component-based applications by benchmarking and profiling. Based on observation, a model is constructed to act as a performance predictor for a class of applications based on the specific component technology. The performance model derived from empirical measures is necessary to make the problem tractable and the results relevant. A case study applies the performance model to an application prototype implemented by two component infrastructures: CORBA and J2EE. 
74|1||A formal approach to component adaptation|Component adaptation is widely recognised to be one of the crucial problems in Component-Based Software Engineering (CBSE). We present a formal methodology for adapting components with mismatching interaction behaviour. The three main ingredients of the methodology are: (1) the inclusion of behaviour specifications in component interfaces, (2) a simple, high-level notation for expressing adaptor specifications, and (3) a fully automated procedure to derive concrete adaptors from given high-level specifications. 
74|1||âComputer, please, tell me what I have to doâ¦â: an approach to agent-aided application composition|The process of starting to use any reuse technology is usually one of the most frustrating factors for novice users. For this reason, tools able to reduce the learning curve are valuable to augment the potential of the technology to rapidly build new applications. In this work, we present Hint, an environment for assisting the instantiation of Java applications based on software agents technology. Hint is built around a software agent that has the knowledge about how to use a reusable asset and, using this knowledge, is able to propose a sequence of programming activities that should be carried out in order to implement a new application satisfying the functionality the user wants to implement. The most relevant contribution of this work is the use of planning techniques to guide the execution of instantiation activities for a given technology. 
74|1||Automated support for service-based software development and integration|A service-based development paradigm is one in which components are viewed as services. In this model, services interact and can be providers or consumers of data and behavior. Applications in this paradigm dynamically integrate services at runtime-based on available resources. This paper describes an architecture-based approach for the creation of services and their subsequent integration with service-requesting client applications. 
74|1||A formal software requirements specification method for digital nuclear plant protection systems|This article describes NuSCR, a formal software requirements specification method for digital plant protection system in nuclear power plants. NuSCR improves the readability and specifiability by providing graphical or tabular notations depending on the type of operations. NuSCR specifications can be formally analyzed for completeness, consistency, and against the properties specified in temporal logic. We introduce the syntax and semantics of NuSCR and demonstrate the effectiveness of the approach using reactor protection system, digital protection system being developed in Korea, as a case study. 
74|1||Some successful approaches to software reliability modeling in industry|Over the past three years, we have been actively engaged in both software reliability growth modeling and architecture-based software reliability modeling for projects at Lucent Technologies. Our goal has been to include software into the overall reliability evaluation of a product design using either or both of these two fundamentally different approaches. During the course of our application efforts to real projects, we have identified practical difficulties with each approach. The application of software reliability growth models, for example, is plagued by widespread use of ad hoc test environments, and the use of architecture-based software reliability models is plagued by a large number of unknown parameters. In this paper, we discuss our methods for overcoming these and other practical difficulties. In particular, we show how calibration factors can be defined and used to adjust for the mismatch between the test and operational profiles of the software. We also present two useful ways to do sensitivity analyses that help alleviate the problem of so many uncertainties in the architecture-based modeling approach. We illustrate our methods with case studies, and offer comments on further work that is required to more satisfactorily bridge the gap between theory and applications in this research area. 
74|1||Software requirement understanding using Pathfinder networks: discovering and evaluating mental models|Understanding and communicating user requirements in a software requirement analysis effort is very important. Misunderstandings of user requirements between software developers and users, will cause problems in terms of satisfying user needs, defects, cost and schedule during the software development process. This paper proposes a new technique that has the ability to represent the mental models of the user and developer communities as network representations using Pathfinder networks. Graphs (mental models) are generated for each of the user and developer groups and compared for similarities/dissimilarities using a graph similarity metric. This paper overviews how this technique is used to categorize requirements and to identify ambiguous and duplicate requirements. We also propose to extend this technique to enhance communication and reduce misunderstanding surrounding the user requirements during the requirement analysis phase. 
74|2|http://www.sciencedirect.com/science/journal/01641212/74/2|Editorial board|
74|2||Contents|
74|2||The new context for software engineering education and training|
74|2||Developing and using a web-based project process throughout the software engineering curriculum|In order to facilitate the study and use of software process, which is essential to the education of future software professionals, a standard and tailorable project process has been developed over the last five years at Texas Tech University for use in both undergraduate and graduate curricula, with a total of 12 courses involved. The process is entirely web-based, and includes a complete set of HTML document templates in order to facilitate the creation of project artifacts which are posted to the course web page. This method enhances communication between team members, including distance education students, and between the project team and client. The project process has received positive feedback from all stakeholders involved.This paper discusses the benefits of the web-based project process, its relation to curriculum models, and plans for a more formal assessment of the process. The portability of process to other institutions is also discussed, with an example provided involving the software engineering courses at the Rose-Hulman Institute of Technology. 
74|2||eXtreme Programmingââhelpful or harmful in educating undergraduates?|Criticism is sometimes leveled at the academic Software Engineering community on the basis that current educational practices are too document-centric. Both students and practitioners have suggested that one of the popular, lighter-weight, agile methods would be a better choice. This paper examines the educational goals for undergraduate Software Engineering education and considers how they might be met by the practices of eXtreme Programming. Our judgment is that education about some agile practices could be beneficial for small-scale development. However, as it stands now, eXtreme Programming as a package does not lend itself for use in educating about large-scale system development in tertiary education. 
74|2||Teaching extreme programming to large groups of students|We find the extreme programming methodology highly suitable for introducing undergraduate students to software engineering. To be able to apply this methodology at a reasonable teaching cost for large student groups, we have developed two courses that work in tandem: a team programming course taken by more than 100 students, and a coaching course taken by around 25 students. In this paper we describe our view of how extreme programming fits into the software engineering curriculum, our approach to teaching it, and our experiences, based on two years of running these courses. Particularly important aspects of our set up include team coaching (by older students), fixed working hours, and colocation during development. Our experiences so far are very positive, and we see that students get a good basic understanding of the important concepts in software engineering, rooted in their own practical experience. 
74|2||Integrating formalism into undergraduate software engineering|This paper describes an approach and rational for using logic and formal methods in undergraduate software engineering education. Formal methods and logic provide a mathematical basis for modeling software analogous to the role of continuous mathematics in traditional engineering disciplines. Traditional software engineering techniques provide means for modeling software development processes and structuring specifications. Neither formal methods nor traditional approaches subsume the other, but are complimentary in software engineering education and practice. The course described here was a part of the standard Computer Engineering curriculum at The University of Cincinnati from 1993 through 1999. This paper reports on the course and observations over six years of teaching the course to undergraduate and graduate students. 
74|2||A maturity model for the implementation of software process improvement: an empirical study|Different advances have been made in the development of software process improvement (SPI) standards and models, e.g. capability maturity model (CMM), more recently CMMI, and ISO's SPICE. However, these advances have not been matched by equal advances in the adoption of these standards and models in software development which has resulted in limited success for many SPI efforts. The current problem with SPI is not a lack of standard or model, but rather a lack of an effective strategy to successfully implement these standards or models. The importance of SPI implementation demands that it be recognised as a complex process in its own right and that organizations should determine their SPI implementation maturity through an organized set of activities. In the literature, much attention has been paid to “what activities to implement” instead of “how to implement” these activities. We believe that identification of only “what” activities to implement is not sufficient and that knowledge of “how” to implement is also required for successful implementation of SPI programmes.We have adopted a CMMI approach and developed a maturity model for SPI implementation in order to guide organizations in assessing and improving their SPI implementation processes. The basis of this model is what we have studied in the SPI literature and an empirical study we have carried out. In the design of this maturity model we have extended the concept of critical success factors (CSFs). We have conducted CSF interviews with 23 Australian practitioners. We have also analysed CSFs and critical barriers using 50 research articles (published experience reports and case studies). This maturity model has three dimensions––maturity stage dimension, CSF dimension and assessment dimension. It provides a very practical structure with which to assess and improve SPI implementation processes. 
74|2||Product derivation in software product families: a case study|From our experience with several organizations that employ software product families, we have learned that, contrary to popular belief, deriving individual products from shared software assets is a time-consuming and expensive activity. In this paper we therefore present a study that investigated the source of those problems. We provide the reader with a framework of terminology and concepts regarding product derivation. In addition, we present several problems and issues we identified during a case study at two large industrial organizations that are relevant to other, for example, comparable or less mature organizations. 
74|2||Modification of standard Function Point complexity weights system|Function Point (FP) is a software size measure, which includes the standard FP and many different models derived from it. The standard FP method created by Albrecht in 1979 is currently known as the International FP User group (IFPUG) version, which consists of three main parts: The first part is five components, and the second is the complexity weights that include three levels of complexity; simple, average, and complex. The third part is the general system characteristics of software projects, which consists of 14 technical complexity factors. Although, FP was widely used as a software size measure, but it still suffers from many weaknesses. One of which is the subjectivity in the weights system. In this paper a new FP weights system was established using Artificial Neural Networks. This method is a modification of the complexity weights of FP measure (IFPUG version). The final results were very accurate and much suitable when they were applied on real data sets of software projects. 
74|2||Characterizing a data model for software measurement|In order to develop or acquire a software product with appropriate quality, it is widely accepted that quality must be identified, planned, measured and controlled during the development process using quality measures based on a quality model. However, few practitioners in the software industry would call measurement a success story. This weakness arises, on one hand because the people involved are not always aware of the importance of collecting measures. The policy of the management board must make sure that people are sufficiently motivated and that data is actually collected in the specified way. On the other hand, software measures have been often poorly defined in industry. When software measurement definitions are incomplete and/or poorly documented, it is easy to collect invalid or incomparable measures from different data collectors. Thus, the primary issue is not only whether a definition for a measure is theoretically correct, but that everyone understands what the measured values represent. Then, the values can be collected consistently and other people, different from the collectors, can interpret the results correctly and apply them to reach valid conclusions. The objective of this paper is to present a data MOdel for Software MEasurement (MOSME) to explicitly define software measures, providing the elements required to describe a consistent measurement process. MOSME can be used for defining and modeling data sets of software products involving several software projects. The inspiration of this work comes from the SQUID (Software QUality In the Development process) approach, which combines many results from previous research on software quality and the European Commission funded projects SQUAD and CLARiFi. The application of MOSME is illustrated with a case study. We believe that a conceptual model of fully defined meaningful measures will help both the management board to give support to the data collection policy and the practitioner to avoid ambiguity in the definitions of the data measures. 
74|2||Erratum to âAn empirical study of maintenance and development estimation accuracyâ [The Journal of Systems and Software 64 (2002) 57â77]|
74|3|http://www.sciencedirect.com/science/journal/01641212/74/3|Editorial board|
74|3||ECEM: an event correlation based event manager for an I/O-intensive application|In Internet servers that run on general purpose operating systems, network subsystems and disk subsystems cooperate with each other for user requests. Many studies have focused on optimizing the data movement across the subsystems to reduce data copying overhead among kernel buffers, a network send buffer and a disk buffer. When data are moved across the subsystems, events such as read requests and write requests for data movement are also delivered across the subsystems by the servers and the operating system. However, there have been fewer studies on the optimization of event delivery across the subsystems. In conventional operating systems, an event from a disk subsystem is delivered to a network subsystem regardless of the status of the network subsystem. If the network subsystem is not ready for data sending, the execution of the server will be blocked, which causes scheduling and context switching overheads. This non-contiguous execution will incur deficiencies such as avoidable process blocking, context switching, cache pollution and long response time. To alleviate the deficiencies, we have developed inter-subsystem event delivery mechanisms that define event dependencies among the subsystems involved. We define an event correlation based on the happened-before relation. We propose deferred event delivery (DED) and disk-to-network splicing (DNS) to suppress scheduling and context switching during I/O request processing. We performed experiments on Linux 2.4 and the experimental results show that the number of context switching is reduced by up to 20% and server data transmit rate is improved by 4.0–8.1%. 
74|3||Measurement-based end to end latency performance prediction for SLA verification|End-to-end delay is one of the important metrics used to define perceived quality of service. Measurement is a fundamental tool of network management to assess the performance of the network. The conventional approach to measure the performance metrics is on an intrusive basis that may cause extra-burden to the network. In contrast to this, our scheme can be considered as non-intrusive. The main idea relies on the knowledge of the queuing behaviour. The queue length is non-intrusively monitored, and then we capture the parameters of the queue state distribution of every queue along the path in order to deduce the end-to-end delay performance. 
74|3||Simulating autonomous agents in augmented reality|In many critical applications such as airport operations (for capacity planning), military simulations (for tactical training and planning), and medical simulations (for the planning of medical treatment and surgical operations), it is very useful to conduct simulations within physically accurate and visually realistic settings that are represented by real video imaging sequences. Furthermore, it is important that the simulated entities conduct autonomous actions which are realistic and which follow plans of action or intelligent behavior in reaction to current situations. We describe the research we have conducted to incorporate synthetic objects in a visually realistic manner in video sequences representing a real scene. We also discuss how the synthetic objects can be designed to conduct intelligent behavior within an augmented reality setting. The paper discusses both the computer vision aspects that we have addressed and solved, and the issues related to the insertion of intelligent autonomous objects within an augmented reality simulation. 
74|3||An empirical study of system design instability metric and design evolution in an agile software process|Software project tracking and project plan adjustment are two important software engineering activities. The class growth shows the design evolution of the software. The System Design Instability (SDI) metric indicates the progress of an object-oriented (OO) project once the project is set in motion. The SDI metric provides information on project evolution to project managers for possible adjustment to the project plan. The objectives of this paper are to test if the System Design Instability metric can be used to estimate and re-plan software projects in an XP-like agile process and study system design evolution in the Agile software process. We present an empirical study of the class growth and the SDI metric in two OO systems, developed using an agile process similar to Extreme Programming (XP). We analyzed the system evolutionary data collected on a daily basis from the two systems. We concluded that: the systems' class growth follows observable trends, the SDI metric can indicate project progress with certain trends, and the SDI metric is correlated with XP activities. In both of the analyzed systems, we observed two consistent jumps in the SDI metric values in early and late development phases. Part of the results agrees with a previous empirical study in another environment. 
74|3||Software failure prediction based on a Markov Bayesian network model|Due to the complexity of software products and development processes, software reliability models need to possess the ability of dealing with multiple parameters. Also in order to adapt to the continually refreshed data, they should provide flexibility in model construction in terms of information updating. Existing software reliability models are not flexible in this context. The main reason for this is that there are many static assumptions associated with the models. Bayesian network is a powerful tool for solving this problem, as it exhibits strong ability to adapt in problems involving complex variant factors. In this paper, a software prediction model based on Markov Bayesian networks is developed, and a method to solve the network model is proposed. The use of our model is illustrated with an example. 
74|3||Static and dynamic distance metrics for feature-based code analysis|In this paper we present metrics to determine the distance between the features of a software system. Such a measurement can elucidate how features of the system being examined are close to each other. We first use an execution slice-based technique to identify a set of code (basic blocks in our case) that is used to implement each feature. Then, depending on whether the execution frequency of each block is considered during the construction of such sets of code, a static as well as a dynamic distance is computed for each pair of features. These two types of distance differ in that the former computes the distance between two features only by how these features are implemented in the system, while the latter also takes into account how each feature is executed based on a user's operational profile. In other words, the static distance quantitatively gives the closeness of two features from the system implementation point of view, whereas the dynamic distance presents such closeness from the users' execution point of view. To illustrate the use of our metrics, we report a case study on a Symbolic Hierarchical Automated Reliability and Performance Evaluator (SHARPE). The results of our study suggest that the distance metrics discussed in this paper can provide a good measurement, in a quantitative way, of how close two program features are. Such information can also serve as a good start to understanding how a modification made to one feature is likely to affect other features. 
74|3||On the security of some proxy blind signature schemes|A proxy blind signature scheme is a digital signature scheme which combines the properties of proxy signature and blind signature schemes. Recently, Tan et al. proposed two proxy blind signature schemes based on DLP and ECDLP respectively. Later, compared with Tan et al.'s scheme, Lal and Awasthi further proposed a more efficient proxy blind signature scheme. In this paper, we show that both Tan et al.'s schemes do not satisfy the unforgeability and unlinkability properties. Moreover, we also point out that Lal and Awasthi's scheme does not possess the unlinkability property either. 
74|3||Improving the performance of client Web object retrieval|The growth of the Internet has generated Web pages that are rich in media and that incur significant rendering latency when accessed through slow communication channels. The technique of Web-object prefetching can potentially expedite the presentation of Web pages by utilizing the current Web page's view time to acquire the Web objects of likely future Web pages. The performance of the Web object prefetcher is contingent on the predictability of future Web pages and quickly determining which Web objects to prefetch during the limited view time interval of the current Web page. The proposed Markov–Knapsack method uses an approach that combines a Multi-Markov Web-application centric prefetch model with a Knapsack Web object selector to enhance Web page rendering performance. The Markov Web page model ascertains the most likely next Web page set based on the current Web page and the Web object Knapsack selector determines the premium Web objects to request from these Web pages. The results presented in the paper show that the proposed methods can be effective in improving a Web browser cache-hit percentage while significantly lowering Web page rendering latency. 
74|3||Adaptive index management for future location-based queries|Many location-based applications have arisen in various areas including mobile communications, traffic control and military command and control (C2) systems. And one of the important research issue in these areas is tracking and managing moving objects through spatiotemporal indexing for the efficient location-based services. However, managing exact geometric location information is difficult to be achieved due to continual changes of moving objects.Traditionally spatiotemporal index structures focus on optimizing the node accesses during construction and massive updates that do not refer on-line movement updates. In this paper we propose an indexing framework for future location queries based on space partitioning and the dual transformation. Our method provides a constraint database approach for constructing indexes to improve the efficiency of spatiotemporal query answering. In addition, the performance enhancement is achieved by our cost-based dynamic management algorithm that determines the appropriate index reorganization probabilistically induced by various mobility models and query cost functions. This approach can be applied for the predictive range queries on moving object's trajectories specifically in the mobile communication environments. We evaluate our method and compare the performance with the related spatiotemporal index structures in the simulated environments. 
74|3||An investigation of software engineering curricula|We adapted a survey instrument developed by Timothy Lethbridge to assess the extent to which the education delivered by four UK universities matches the requirements of the software industry. We propose a survey methodology that we believe addresses the research question more appropriately than the one used by Lethbridge. In particular, we suggest that restricting the scope of the survey to address the question of whether the curricula for a specific university addressed the needs of its own students, allowed us to identify an appropriate target population. However, our own survey suffered from several problems. In particular the questions used in the survey are not ideal, and the response rate was poor.Although the poor response rate reduces the value of our results, our survey appears to confirm several of Lethbridge's observations with respect to the over-emphasis of mathematical topics and the under-emphasis on business topics. We also have a close agreement with respect to the relative importance of different software engineering topics. However the set of topics, that we found were taught far less than their importance would suggest, were quite different from the topics identified by Lethbridge. 
74|3||Contents Volume 74|
75|1-2|http://www.sciencedirect.com/science/journal/01641212/75/1-2|Contents|
75|1-2||Editorial|
75|1-2||An experimental card game for teaching software engineering processes|The typical software engineering course consists of lectures in which concepts and theories are conveyed, along with a small “toy” software engineering project which attempts to give students the opportunity to put this knowledge into practice. Although both of these components are essential, neither one provides students with adequate practical knowledge regarding the process of software engineering. Namely, lectures allow only passive learning and projects are so constrained by the time and scope requirements of the academic environment that they cannot be large enough to exhibit many of the phenomena occurring in real-world software engineering processes. To address this problem, we have developed Problems and Programmers, an educational card game that simulates the software engineering process and is designed to teach those process issues that are not sufficiently highlighted by lectures and projects. We describe how the game is designed, the mechanics of its game play, and the results of an experiment we conducted involving students playing the game. 
75|1-2||Engineering-based processes and agile methodologies for software development: a comparative case study|The emergence of various software development methodologies raises the need to evaluate and compare their efficiencies. One way of performing such a comparison is to have different teams apply different process models in the implementation of multiple versions of common specifications. This study defines a new cognitive activity classification scheme which has been used to record the effort expended by six student teams producing parallel implementations of the same software requirements specifications. Three of the teams used a process based on the Unified Process for Education (UPEDU), a teaching-oriented process derived from the Rational Unified Process. The other three teams used a process built around the principles of the Extreme Programming (XP) methodology. Important variations in effort at the cognitive activity level between teams shows that the classification could scarcely be used without categorization at a higher level. However, the relative importance of a category of activities aimed at defining “active” behaviour was shown to be almost constant for all teams involved, possibly showing a fundamental behaviour pattern. As secondary observations, aggregate variations by process model tend to be small and limited to a few activities, and coding-related activities dominate the effort distribution for all the teams. 
75|1-2||An industry/university collaboration to upgrade software engineering knowledge and skills in industry|This paper describes an ongoing collaboration between Boeing Australia Limited and the University of Queensland to develop and deliver an introductory course on software engineering. The aims of the course are to provide a common understanding of the nature of software engineering for all Boeing Australia's engineering staff, and to ensure they understand the practices used throughout the company. The course is designed so that it can be presented to people with varying backgrounds, such as recent software engineering graduates, systems engineers, quality assurance personnel, etc. The paper describes the structure and content of the course, and the evaluation techniques used to collect feedback from the participants and the corresponding results. The immediate feedback on the course indicates that it has been well received by the participants, but also indicates a need for more advanced courses in specific areas. The long-term feedback from participants is less positive, and the long-term feedback from the managers of the course participants indicates a need to expand on the coverage of the Boeing-specific processes and methods. 
75|1-2||The role of modelling in the software engineering curriculum|This paper argues that the concept of modelling, and particularly the modelling of software system structures, is not being given sufficient attention within current sources that describe aspects of the software engineering curriculum. The paper describes the scope of modelling as a general concept, and explains the role that the modelling of software system structures plays within it. It discusses the treatment of this role within the various sources, and compares this both with the experience of the role that such modelling plays in the undergraduate curriculum at Sheffield University, and with the practice in other branches of engineering. The idea is examined that modelling should be treated as a recurring concept within the curriculum, and it is shown that this gives rise to a matrix structure for the software engineering curriculum. The paper discusses how such a structure can be mapped into a conventional hierarchical curriculum model, and the relationships that need to be made explicit in doing so. It describes the practical implications of these results for the structures of degree programmes in software engineering. 
75|1-2||The SEI curriculum modules and their influence: Norm Gibbs' legacy to software engineering education|The Software Engineering Institute (SEI) at Carnegie Mellon University started its first contract with a carte blanche opportunity and generous funding to improve the state of software engineering education. Norm Gibbs, the first Director of Education at the SEI guided efforts in this area. One of his innovations, discussed here, were the “curriculum modules” encapsulating software engineering knowledge. We describe the scope and form of the curriculum modules, together with our personal experiences of developing the prototype modules. We conclude with an informal assessment of how well the original set of SEI curriculum modules match current ideas, both about software engineering education and also about the activities and practices that make up software engineering as a discipline. 
75|1-2||Secure key agreement protocols for three-party against guessing attacks|Key exchange protocol is important for sending secret messages using the session key between two parties. In order to reach the objective, the premise is to generate a session key securely. Encryption key exchange was first proposed to generate a session key with a weak authenticated password against guessing attacks. Next, another authenticated key exchange protocols for three-party, two clients who request the session key and one server who authenticates the user's identity and assist in generating a session key, were proposed. In this paper, we focus on the three-party authenticated key exchange protocol. In addition to analyzing and improving a password-based authenticated key exchange protocol, a new verified-based protocol is also proposed. 
75|1-2||A practical pattern recovery approach based on both structural and behavioral analysis|While the merit of using design patterns is clear for forward engineering, we could also benefit from design pattern recovery in program understanding and reverse engineering. In this paper, we present a practical approach to enlarge the recoverable scope and improve precision ratio of pattern recovery. To specify both structural aspect and behavioral aspect of design patterns, we introduce traditional predicate logic combined with Allen's interval-based temporal logic as our theory foundation. The formal specifications could be conveniently converted into Prolog representations to support pattern recovery. To illustrate how to specify and recover design patterns in our approach, we take one example for each category of design patterns. Moreover, we give a taxonomy of design patterns based on the analysis in our approach to show its applicable scope. To validate our approach, we have developed a tool named PRAssistor and analyzed two well-known open source frameworks. The experiment results show that most of the patterns addressed in our taxonomy have been recovered. Besides larger recoverable scope, the recovery precision of our approach is much higher than others. Furthermore, we consider that our approach and tool could be promisingly extended to support “Debug at Design Level” and “Pattern-Driven Refactoring”. 
75|1-2||Cryptanalysis of XiaâYou group signature scheme|Group signature is a variant of digital signatures, which allows members of a group to sign messages anonymously on behalf of the group. The application of group signatures includes e-voting, e-bidding and e-cash. Recently, Xia and You proposed an identity-based group signature scheme with strong separability. In this paper, however, we find that there are several problems in the Xia–You group signature scheme. This scheme is vulnerable to forgery attacks. Any adversary could easily forge a valid group signature for any message without the knowledge of the secret values of the legal members of the group. Furthermore, the Xia–You group signature scheme does not satisfy nearly all other security requirements of group signatures. The signatures are too long to be computed, stored and transmitted. 
75|1-2||Performance study on implementation of VCR functionality in staggered broadcast video-on-demand systems|Data sharing is a feasible solution to support a large-scale video-on-demand (VoD) system, however, it is still an open issue to provide a full set of continuous interactive functions in such an environment that a single channel is used to serve a group of customers. In this paper, we first investigate the performance of client buffer management and its hybrid use with contingency channels for providing VCR functionality in a staggered broadcast VoD system. Results show that directly combining the two methods may not gain any advantages since the resources are not fully utilized. To tackle this problem, a greedy algorithm is proposed to efficiently manage the contingency channels such that the system performance can be significantly improved by exploiting the property of the staggered broadcast scheme and the use of multiple loaders in the receiver. Simulation results show that the proposed greedy system can efficiently provide a full set of VCR functions with satisfied user performance in a broadcast VoD system. 
75|1-2||Preventing information leakage within workflows that execute among competing organizations|This paper proposes a model for access control within workflows. It is based on access control lists (ACLs) and is named WfACL (ACL-based access control model for workflows). WfACL prevents information leakage within workflows that may execute among competing organizations. Its objective is threefold. First, it prevents an organization that executes a workflow from leaking its information to other organizations. Second, it prevents information leakage among competing organizations. Third, it prevents information leakage within an organization. In addition to achieving the objective, WfACL offers the following features: (a) managing dynamic role association change, (b) managing dynamic role change, (c) avoiding indirect information leakage, (d) detailing the control granularity to roles, and (e) controlling both read and write access. We embedded WfACL in a rule-based workflow language WfACLL and implemented a prototype environment WfACLE. We evaluated WfACL using WfACLL and WfACLE. The evaluation result is also shown in this paper. 
75|1-2||Secure agent computation: X.509 Proxy Certificates in a multi-lingual agent framework|Mobile agent technology presents an attractive alternative to the client–server paradigm for several network and real-time applications. However, for most applications, the lack of a viable agent security model has limited the adoption of the agent paradigm. This paper describes how the security infrastructure for computational Grids using X.509 Proxy Certificates can be extended to facilitate security for mobile agents. Proxy Certificates serve as credentials for Grid applications, and their primary purpose is the temporary delegation of authority. We are exploiting the similarities between Grid applications and mobile agent applications, and motivate the use of Proxy Certificates as credentials for mobile agents. Further, we propose extensions for Proxy Certificates to facilitate the characteristics of mobile agent applications, and present mechanisms that achieve agent-to-host authentication, restriction of agent privileges, and secure delegation of authority during spawning of new agents. 
75|1-2||Wireless protocol testing and validation supported by formal methods. A hands-on report|We apply formal testing and validation techniques and tools to analyse a configuration protocol for a Bluetooth Location Network (BLN). This network is composed by static Bluetooth nodes that establish a spontaneous network at system initialization. Once configured, BLN provides location services for location-aware or context-driven wireless environments, such as m-commerce networks or e-museums. BLN configuration was initially defined in natural language, and had passed some initial tests and simulation-based analysis. Formal methods have provided deeper understanding and discovered unexpected errors that may arise in some failure scenarios. 
75|1-2||A two-level scheduling method: an effective parallelizing technique for uniform nested loops on a DSP multiprocessor|A digital signal processor (DSP), which is a special-purpose microprocessor, is designed to achieve higher performance on DSP applications. Because most DSP applications contain many nested loops and permit a very high degree of parallelism, the DSP multiprocessor has a suitable architecture to execute these applications. Unfortunately, conventional scheduling methods used on DSP multiprocessors allocate only one operation to each DSP every time unit, even if the DSP includes several function units that can operate in parallel. Obviously they cannot achieve full function unit utilization. Hence, in this paper, we propose a two-level scheduling method (TSM) to overcome this common failing. TSM contains two approaches, which integrates unimodular transformations, loop tiling technique, and conventional methods used on single DSP. Besides introducing algorithm, we also use an analytic module to analyze its preliminary performance. Based on our analyses the TSM can achieve shorter execution time and more scalable speedup results. In addition, the TSM causes less memory access and synchronization overheads, which are usually negligible in the DSP multiprocessor architecture. 
75|1-2||Migratable sockets in cluster computing|Optimal utilization of cluster computing is partly dependent upon pre-emptive process migration. However, this migration involves a host of issues, one of them being the transfer of system-dependent resources. We focus on the overhead incurred by migrated processes using sockets. We then describe a solution that we devised and implemented to avoid this overhead through the use of `migratable sockets'. Our studies show that the use of `migratable sockets' considerably improves the execution time of a process using sockets as compared to the execution time of a process that uses standard sockets and thus bears the communication overhead. 
75|1-2||An agent-based inter-application information flow control model|Access control in an application prevents information leakage in the application. The prevention can be achieved by controlling information flows. Many information flow control models have been developed. Since applications may cooperate, controlling information flows among cooperating applications is necessary. Our survey reveals that no existing model offers the control. We thus developed a model to control information flows among cooperating object-oriented applications. In designing the model, we require that cooperating applications communicate with one another through JAVA RMI (remote method invocation). Our model is based on the following considerations: when a RMI occurs, the security level of an argument should not be higher than that of the parameter receiving the argument's value, and the security level of a variable receiving a method return value should not be lower than that of the method return value. The model is agent-based. Moreover, different applications can embed different information flow control models. With this, the proposed model coordinates heterogeneous information flow control models. This paper presents our inter-application information flow control model. 
75|1-2||Cryptanalysis of HwangâYang scheme for controlling access in large partially ordered hierarchies|Recently, Hwang and Yang [J. Syst. Software 67 (2003) 99] proposed a cryptographic key assignment scheme for access control in large partially ordered hierarchies. In this paper, we show that their scheme is insecure against the collusion attack whereby some security classes conspire to derive the secret keys of other leaf security classes, which is not allowed according to the scheme. 
75|1-2||Enhancing Service Location Protocol for efficiency, scalability and advanced discovery|This paper presents three new mechanisms for the Service Location Protocol (SLP): mesh enhancement, preference filters and global attributes. The mesh enhancement simplifies Service Agent (SA) registrations and improves consistency among Directory Agents (DAs) by defining an interaction scheme for DAs and supporting automatic registration distribution among peer DAs. Preference filters facilitate processing of search results (e.g., finding the best match) in SLP servers (DAs and SAs) to reduce the amount of data transferred to the client for saving network bandwidth. Global attributes allow using a single query to search services across multiple types. These mechanisms can improve SLP efficiency and scalability and support advanced discovery such as discovering multi-access-point services and multi-function devices. We expect that these techniques can also be applied to other service discovery systems. 
75|1-2||Revocation of privacy-enhanced public-key certificates|This paper presents a novel protocol for the revocation of privacy-enhanced/anonymous public-key certificates in relation to a protocol for anonymous public-key certificate issuing published previously. Not only can this certificate revocation protocol revoke an anonymous public-key certificate upon a request from its holder, but also automatically revoke any certificate issued directly or indirectly based on the certificate revoked, in an anonymous and accountable manner. In case the private key associated with an anonymous public-key certificate is suspected of having been compromised, the certificate holder can operate the protocol to easily revoke the compromised certificate together with its related ones so as to stop them being abused. The protocol is also assessed with regard to requirements such as accountability and anonymity. 
75|1-2||J2EE support for wireless services|We explore by design and implementation the applicability of J2EE (Java 2 Platform, Enterprise Edition) architecture in supporting wireless services as well as web-based services. This has been demonstrated by implementing a wireless prescription system, which provides wireless services to physicians, and web-based services to pharmacies and administrators. The goal of the wireless prescription system is to improve the efficiency of the healthcare system by reducing the overall time and cost used to create documents and retrieve information. We also present a performance evaluation of the implemented wireless prescription system using latency as the performance metric. In particular, we investigated the overheads associated with different components of the underlying J2EE architecture, which includes the Session Bean, Entity Bean, and the database. We found that the latency overheads introduced by these components account for 3.8–4.3%, 3.9–4.4%, and 91.8% of the total delay experienced inside the J2EE architecture respectively. 
75|1-2||A generic anti-spyware solution by access control list at kernel level|Spyware refers to programs that steal the user information stored in the user’s computer and transmit this information via the Internet to a designated home server without the user being aware of this transmission. Existing anti-spyware solutions are not generic and flexible. These solutions either check for the existence of known spyware or try to block the transmission of the private information at the packet level. In this paper, we propose a more generic and flexible anti-spyware solution by utilizing an access control list in kernel mode of the operating system. The major difference between our approach and the existing approaches is that instead of asking a guard to look for the theft (spyware) or control the exit of the computer (and hence giving the spyware enough time to hide the information to be transmitted), we put a guard besides the treasure (the private information) and carefully control the access to it in the kernel mode. We also show the details of an implementation that realizes our proposed solution. 
75|3|http://www.sciencedirect.com/science/journal/01641212/75/3|Editorial board|
75|3||Adaptive multimedia computing|
75|3||Streaming video delivery over Internet with adaptive end-to-end QoS|Many multimedia applications rely on video streaming techniques. However, video delivery over a Best Effort network such as today's Internet is a challenge. Traffic load in the network changes dynamically and in an unpredictable way causing the resource availability to vary. Providing an application level end-to-end quality of service in such an environment requires network-awareness and ability to adapt. We address the issue of mapping between application-level quality of service for streaming video and network-level quality of service. We show that continuous playback requires a limit on the delay jitter. We tackle the problem of providing end-to-end video quality given that the network does not guarantee limited delay variability. Our approach is unique in a way we do not model network as a black box but investigate what information about the network status is necessary for an application to make adaptation decisions. We rely on simple multi-level ECN-based mechanism to obtain a feedback from the network as well as on end-point observations to determine available bandwidth. Such an approach allows to obtain better user-perceived video quality by providing additional information to properly interpret the arrival rate observed at the end-point. We propose a 3-rate adaptation mechanism for video streaming to illustrate the philosophy of adaptivity based on network awareness, where the network awareness is not limited to observing network reaction to a set of stimuli. 
75|3||Adaptive video transcoding and streaming over wireless channels|In this work, we investigate the problem of bit rate adaptation transcoding for transmitting pre-encoded VBR video over burst-error wireless channels, i.e., channels such that errors tend to occur in clusters during fading periods. In particular, we consider a scenario consisting of packet-based transmission with Automatic Repeat ReQuest (ARQ) error control and a feedback channel. With the acknowledgements received through the feedback channel and a statistical channel model, we have an estimate of the current channel state, and effective channel bandwidth. In this paper, we analyze the constraints of buffer and end-to-end delay, and derive the conditions that the transcoder buffers have to meet for preventing the end decoder buffer from underflowing and overflowing. Furthermore, we also investigate the source characteristics and scene changes of the pre-encoded video stream. Based on the channel constraints and source video characteristics, we propose an adaptive bit rate adaptation algorithm for transcoding and transmitting pre-encoded VBR video stream over wireless channel. Our experimental results demonstrate that, by reusing the source characteristics and scene change information, transcoding high quality video can produce better video picture quality than that produced by directly encoding the uncompressed video at the same low bit rate. Moreover, by controlling the frame bit budget according to the channel conditions and buffer occupancy, the initial startup delay of streaming pre-encoded video can be significantly reduced. 
75|3||An adaptive rate-control streaming mechanism with optimal buffer utilization|In this paper, an end-to-end real-time adaptive protocol for multimedia transmission is presented. The bandwidth is dynamically allocated according to the network status, and the client buffer occupancy and playback requirement. The transmission rate is determined by the quadratic probing algorithm that can obtain the maximal utilization of the client buffer and minimal occupation of the network bandwidth. It is also coupled with a congestion control mechanism that can effectively decrease the packet loss rate during network congestion. We investigate the performance of our quadratic probing algorithm in different congestion levels under both the local area net (LAN) and Internet environments. Performance analysis reveals that our approach is more robust in avoiding overflows and underflows in different network congestion levels, and adapting to the changing network delays. Comparisons are made with the fixed rate approach and the rate by playback requirement approach. The experimental results show that our proposed real-time protocol with the rate adjusting quadratic probing algorithm is efficient in utilizing the network resources and decreasing the packet loss ratios. 
75|3||ATF: an Adaptive Three-layer Framework for inter-stream synchronization of SMIL multimedia presentations|In this paper, we propose an Adaptive Three-layer Framework (ATF) to guarantee the inter-stream synchronization, which is called as the APplication Quality of Service (APQoS) in this paper, of World Wide Web Consortium (W3C) Synchronized Multimedia Integration Language (SMIL) presentation. This ATF framework, which is composed of the Framework Control Layer (FCL), the Quality Guarantee Layer (QGL) and the Media Control Layer (MCL), functions as the middleware between underlying best-effort network protocol stacks and the SMIL client/server applications. The FCL at the SMIL client periodically monitors underlying network bandwidth and delay for the QGL to verify whether the total bandwidth requirements of all concurrent objects to meet the APQoS of the SMIL presentation can be satisfied with the available bandwidth, which is the sum of the monitored bandwidths of all concurrent objects. The QGL executes the novel Intra-SMIL Bandwidth Reallocation (ISBR) scheme to dynamically adjust bandwidth allocations among concurrent SMIL objects, according to their Media Importance (MI) values calculated by the MCL. If the available bandwidth is not enough, the ISBR scheme gracefully degrades the layer-encoded objects that have the smallest MI values and only allocate the necessary bandwidth for their base layer (BL) streams instead. Oppositely, it aggressively upgrades the degraded layer-encoded objects that have the largest MI values and allocate the necessary bandwidth for both the BL and enhancement layer (EL) streams. These bandwidths are then feedback to the remote media servers to modify the transmission rates accordingly. With the quantitative analysis for the SMIL presentation quality, this ATF framework achieves the APQoS with the best SMIL playback quality. 
75|3||An agent based adaptive bandwidth allocation scheme for multimedia applications|Bandwidth allocation for multimedia applications in case of network congestion and failure poses technical challenges due to bursty and delay sensitive nature of the applications. The growth of multimedia services on Internet and the development of agent technology have made us to investigate new techniques for resolving the bandwidth issues in multimedia communications. Agent technology is emerging as a flexible promising solution for network resource management and QoS (Quality of Service) control in a distributed environment. In this paper, we propose an adaptive bandwidth allocation scheme for multimedia applications by deploying the static and mobile agents. It is a run-time allocation scheme that functions at the network nodes. This technique adaptively finds an alternate patchup route for every congested/failed link and reallocates the bandwidth for the affected multimedia applications. The designed method has been tested (analytical and simulation) with various network sizes and conditions. The results are presented to assess the performance and effectiveness of the approach. This work also demonstrates some of the benefits of the agent based schemes in providing flexibility, adaptability, software reusability, and maintainability. 
75|3||Scheduling legacy multimedia applications|Millions of applications have been developed on conventional time-sharing systems. We call those applications legacy applications. Many of them, typically multimedia applications, have Quality of Service (QoS) demands, which are not supported in time-sharing systems. Although many scheduling algorithms and schedulers have been proposed to schedule multimedia applications, it is not feasible to rebuild millions of legacy multimedia applications in a completely new programming model. Moreover, the execution pattern of multimedia applications is difficult to predict.This work presents a legacy application-compatible, adaptation-oriented scheduling framework. The new scheduler is implemented as a Linux loadable module. Thus users can either use the original Linux scheduler or use our scheduler by loading the module. In the new scheduler, users can reserve a default execution rate for legacy multimedia applications, and a rate adjustment mechanism is provided for adaptation. The framework also supports rate-based execution (RBE) and periodic threads that run at a constant rate, and non-real-time threads that have no QoS demand. 
75|3||Contents Volume 75|
76|1|http://www.sciencedirect.com/science/journal/01641212/76/1|Editorial board|
76|1||Contents|
76|1||Editorial|
76|1||Reliability assessment and sensitivity analysis of software reliability growth modeling based on software module structure|Software reliability is an important characteristic for most systems. A number of reliability models have been developed to evaluate the reliability of a software system. The parameters in these software reliability models are usually directly obtained from the field failure data. Due to the dynamic properties of the system and the insufficiency of the failure data, the accurate values of the parameters are hard to determine. Therefore, the sensitivity analysis is often used in this stage to deal with this problem. Sensitivity analysis provides a way to analyzing the impact of the different parameters. In order to assess the reliability of a component-based software, we propose a new approach to analyzing the reliability of the system, based on the reliabilities of the individual components and the architecture of the system. Furthermore, we present the sensitivity analysis on the reliability of a component-based software in order to determine which of the components affects the reliability of the system most. Finally, three general examples are evaluated to validate and show the effectiveness of the proposed approach. 
76|1||Smart debugging software architectural design in SDL|Statistical data show that it is much cheaper to fix software bugs at the early design stage than the late stage of the development process where the final system has already been implemented and integrated together. The use of slicing and execution histories as an aid in software debugging is well established for programming languages like C and C++; however, it is rarely applied in the field of software specification for designs. We propose a solution by applying the technology at source code level to debugging software designs represented in a high-level specification and description language such as SDL. More specifically, we extend execution slice-based heuristics from source code-based debugging to the software design specification level. Suspicious locations in an SDL specification are prioritized based on their likelihood of containing faults. Locations with a higher priority should be examined first rather than those with a lower priority as the former are more suspicious than the latter, i.e., more likely to contain the faults. A debugging tool, SmartDSDL, with user-friendly interfaces was developed to support our method. An experiment is reported to demonstrate the feasibility of using our method to effectively debug an architectural design. 
76|1||A middleware service for secure group communication in mobile ad hoc networks|Secure group communication (SGC) is required in many applications using mobile ad hoc networks (MANETs). Due to the mobility and limited resources of mobile devices used by group members, secure groups in a MANET may be required to be set up and maintained dynamically, and applications requiring SGC may request to change the secure groups according to network and application status. In this paper, a secure group model is presented to describe common characteristics of secure groups based on hierarchical authentication relations among the members in each secure group. Based on this model, a middleware service for SGC is developed to set up and maintain secure groups automatically and support development and runtime of applications using SGC in MANETs. This middleware service is implemented in a context-sensitive middleware RCSM. 
76|1||Facilitating secure ad hoc service discovery in public environments|Securely accessing unfamiliar services in public environments using ad hoc wireless networks is challenging. We present a proxy-based approach that uses alternative network channels to establish a secure trust relationship between communication parties to facilitate ad hoc wireless communications. Based on a service discovery protocol, our models achieve secure, trusted, anonymous, efficient, and economical communications between unfamiliar parties. Our protocol design is formally verified using BAN logic. 
76|1||Checking of models built using a graphically based formal modelling language|RDT is a graphical formal modelling language in which the modeller works by constructing diagrams of the processes in their model which they then join together to form complete systems. Aside from the benefits which accrue as a side effect of building a formal model of a proposed system, these diagrammatic models can be useful as a means of communication between the development team and the users. However one of the greatest benefits of a formal model is that it can be subjected to rigorous examination to ensure that it satisfies properties required of the system.This paper describes the RDT language and a transformation from RDT into Promela code (the input language of the SPIN model-checker) which can be performed automatically and illustrates the use of the technique with an example. 
76|1||Effects of introducing survival behaviours into automated negotiators specified in an environmental and behavioural framework|With the rise of distributed e-commerce in recent years, demand for automated negotiation has increased. In turn, this has engendered a demand for ever more complex algorithms to conduct these negotiations. As the complexity of these algorithms increases, our ability to reason about and predict their behaviour in an ever larger and more diverse negotiation environment decreases. In addition, with the proliferation of internet-based negotiation, any algorithm also has to contend with potential reliability issues in the underlying message-passing infrastructure. These factors can create problems for building these algorithms, which need to incorporate methods for survival as well as negotiation.This paper proposes a simple yet effective framework for integrating survivability into negotiators, so they are better able to withstand imperfections in their environment. An overview of this framework is given, with two examples of how negotiation behaviour can be specified within this framework. Results of an experiment which is based on these negotiation algorithms are provided. These results show how the stability of a negotiation community is affected by incorporating an example survival behaviour into negotiators operating in an environment developed to support this framework. 
76|1||Evolving car designs using model-based automated safety analysis and optimisation techniques|Development processes in the automotive industry need to evolve to address increasing demands for integration of car functions over common networked infrastructures. New processes must address cost and safety concerns and maximize the potential for automation to address the problem of increasing technological complexity. In this paper, we propose a design process in which techniques for semi-automatic safety and reliability analysis of systems models are combined with multi-objective optimisation techniques to assist the gradual development of designs that can meet reliability and safety requirements and maximise profit within pragmatic development cost constraints. The proposed process relies on tools to automate some aspects of the design that we believe could be automated and thus simplified without loss of the creative input brought in the process by designers. 
76|1||An assessment of systems and software engineering scholars and institutions (1999â2003)|This paper presents the findings of a five-year study of the top scholars and institutions in the Systems and Software Engineering field, as measured by the quantity of papers published in the journals of the field. The top scholar is Khaled El Emam of the Canadian National Research Council, and the top institution is Carnegie Mellon University and its Software Engineering Institute.This paper is part of an ongoing study, conducted annually, that identifies the top 15 scholars and institutions in the most recent five-year period. 
76|2|http://www.sciencedirect.com/science/journal/01641212/76/2|Impartial evaluation in software reliability practice|In the past decade, the size scale of software systems and their technical complexity has become much more complicated. Accordingly, quality assessment of software applications has been intensively investigated lately. Among popular software quality metrics, software reliability has been proven to be one of the most useful indices in evaluating software applications. In the literature, statistical usage testing has been widely shown to be effective in estimating software reliability. Essentially, it first transfers the practical operations of a software system into a usage model, which then forms a basis for performing statistical testing and analyzing software reliability. This research investigates the extension of statistical usage testing by considering any prior information or prejudgment on software quality before performing a validation test, and propose the derivation of impartial reliability evaluation that is fair for both the software producer and consumer (end user). A numerical demonstration of validating the correctness of hyper links on a web site via the proposed computation is illustrated and discussed. The suggested mechanism with some prior information will converge much more quickly than other similar reliability models. In addition, the proposed framework also provides the flexibility of taking the practical prejudgment into account. 
76|2||Resource-oriented software quality classification models|Developing high-quality software within the allotted time and budget is a key element for a productive and successful software project. Software quality classification models that provide a risk-based quality estimation, such as fault-prone (fp) and not fault-prone (nfp), have proven their usefulness as software quality assurance techniques. However, their usefulness is largely dependent on the availability of resources for deploying quality improvements to modules predicted as fp. Since every project has its own special needs and specifications, we feel a classification modeling approach based on resource availability is greatly warranted.We propose and demonstrate the use of a resource-based measure, i.e., “Modified Expected Cost of Misclassification” (mecm), for selecting and evaluating classification models. It is an extension of the “Expected Cost of Misclassification” (ecm) measure, which we have previously applied for model-evaluation purposes. The proposed measure facilitates building resource-oriented classification models and overcomes the limitation of ecm, which assumes that enough resources are available to enhance all modules predicted as fp. The primary aspect of mecm is that it penalizes a model, in terms of costs of misclassifications, if the model predicts more number of fp modules than the number that can be enhanced with the available resources. Based on the resources available for improving quality of software modules, a practitioner can use the proposed methodology to select a model that best-suits the projects goals. Hence, the best possible and practical usage of the available resources can be achieved. The application, analysis, and benefits of mecm is shown by developing models using Logistic Regression. It is concluded that the use of mecm is a promising approach for practical software quality improvement. 
76|2||Metadata-driven design of integrated environments for software performance validation|Lifecycle validation of the performance of software products (i.e., the prediction of the product ability to satisfy the user performance requirements) encompasses the production of performance models from CASE documents.The model production activity is a critical, time-consuming and error-prone activity so that lifecycle validation is still not widely accepted and applied. The reason is twofold: the lack of methods for the automatic derivation of software performance models from CASE documents and the lack of environments that implement and integrate such methods.A number of methods for the automatic derivation of software performance models from CASE documents has been already proposed in literature, without however solving the automation problem. This paper instead faces up to such problem, by introducing an integrated and standards-based environment for the automatic derivation and evaluation of queueing-based performance models.The environment is based on the use of standards for metadata exchange (MOF, XMI), to ease the integration of the most common UML-based CASE tools, thus enabling software designers to smoothly introduce performance validation activities into their best development practices. 
76|2||Application of neural networks for software quality prediction using object-oriented metrics|This paper presents the application of neural networks in software quality estimation using object-oriented metrics. In this paper, two kinds of investigation are performed. The first on predicting the number of defects in a class and the second on predicting the number of lines changed per class. Two neural network models are used, they are Ward neural network and General Regression neural network (GRNN). Object-oriented design metrics concerning inheritance related measures, complexity measures, cohesion measures, coupling measures and memory allocation measures are used as the independent variables. GRNN network model is found to predict more accurately than Ward network model. 
76|2||A comparative study of SOAP and DCOM|Simple Object Access Protocol (SOAP) is a recent technology, which aims at replacing traditional methods of remote communications, such as RPC-based Distributed Component Object Model (DCOM). Being designed with a goal of increasing interoperability among wide range of programs and environment, SOAP allows applications written in different languages and deployed on different platforms to communicate with each other over the network. While SOAP offers obvious benefits in the world of interoperability, it comes at a price of performance degradation and additional development efforts required for implementation of features missing from SOAP, such as security and state management. This paper reports the outcome of a comparative study of SOAP and DCOM in terms of features, development effort, and application performance. The results indicate that SOAP performance on a homogeneous platform is consistently worse than that of DCOM. Depending on the operation and the amount of data transferred, SOAP performance degradation can range from minor (e.g. two or three times slower than DCOM) to major (e.g. twenty or more times slower). 
76|2||Design and implementation of an extended relationship semantics in an ODMG-compliant OODBMS|Relationships, in addition to entities, are important in real-world database modeling. In particular, many object oriented database applications including CAD/CAM, CASE and multi-media need to model various and complex relationships, especially the `part–whole' relationship. Without the built-in relationship supports from DBMSs, there is a huge overhead in managing relationships from application development to maintenance, since the relationships should be hard-coded within the application program itself.In this paper, we propose a powerful `part–whole' relationship model, which naturally extends the ODMG-3.0 object database standard. The proposed relationship model can support almost all of the relationship functionalities existing in the contemporary relational database model and the object oriented data model. In order to design and implement this relationship model, we seamlessly extend the ODMG-3.0 relationship using the inheritance concept. Also, we identify several possible run-time anomalies in implementing the relationship and provide solutions for their problems. 
76|2||Performance analysis of software reliability growth models with testing-effort and change-point|In this paper, a scheme for constructing software reliability growth model based on Non-Homogeneous Poisson Process is proposed. The main focus is to provide a method for software reliability modeling, which considers both testing-effort and change-point. In the vast literature, most researchers assume a constant detection rate per fault in deriving their software reliability models. They suppose that all faults have equal probability of being detected during the software testing process, and the rate remains constant over the intervals between fault occurrences. In reality, the fault detection rate strongly depends on the skill of test teams, program size, and software testability. Therefore, it may not be smooth and can be changed. On the other hand, sometimes we have to detect more additional faults in order to reach the desired reliability objective during testing. It is advisable for project managers to purchase new automated test tool, technology or additional manpower. These approaches can provide a conspicuous improvement in software testing and productivity. In this case, the fault detection rate will be changed during the software development process. Therefore, here we incorporate both generalized logistic testing-effort function and change-point parameter into software reliability modeling. New theorems are proposed and software testing data collected from real application are utilized to illustrate the proposed model. Experimental results show that the proposed framework to incorporate both testing-effort and change-point for SRGM has a fairly accurate prediction capability. 
76|2||The task-dependent nature of the maintenance of object-oriented programs|
76|3|http://www.sciencedirect.com/science/journal/01641212/76/3|DPE/PAC: decentralized process engine with product access control|
76|3||Workflow analysis for web publishing using a stage-activity process model|Web publishing has been the core business of ICP (Internet Content Provider) companies. It is also a major functional component in other Internet industry sectors. However, although there have been on-going developments in this area, up to our knowledge, very little work has been done on web publishing process models and methodologies. In this paper we describe a general process model for web publishing based on workflow analysis. Using a case study approach, the existing web publishing methods used by major Hong Kong ICP companies have been evaluated. Based on the evaluation results and our observations, the issues and problems of the existing methods are identified. A general process model, called the stage-activity model, is proposed which facilitates the development of a framework for automating the web publishing process with the incorporation of workflow management. The results of this research contribute to a clear understanding of what challenges ICPs are currently facing and a solution for building an automating web publishing system. The framework can also be used to evaluate and select the web publishing tools in the market. 
76|3||Multi-disk scheduling for time-constrained requests in RAID-0 devices|In this paper, we study the scheduling problem of real-time disk requests in multi-disk systems, such as RAID-0. We first propose a multi-disk scheduling algorithm, called Least-Remaining-Request-Size-First (LRSF), to improve soft real-time performance of I/O systems. LRSF may be integrated with different real-time/non-real-time single-disk scheduling algorithms, such as SATF and SSEDV, adopted by the disks in a multi-disk system. We then extend LRSF by considering the serving requests on-the-way (OTW) to the target request to minimize the starvation problem for requests that need to retrieve a large amount of data. The pre-fetching issue in RAID-0 is also studied to further improve the I/O performance. The performance of the proposed algorithm and schemes is investigated and compared with other disk scheduling algorithms through a series of experiments using both randomly generated workload and realistic workload. 
76|3||Using an expert panel to validate a requirements process improvement model|In this paper we present components of a newly developed software process improvement model that aims to represent key practices in requirements engineering (RE). Our model is developed in response to practitioner needs highlighted in our empirical work with UK software development companies. We have now reached the stage in model development where we need some independent feedback as to how well our model meets our objectives. We perform this validation through involving a group of software process improvement and RE experts in examining our RE model components and completing a detailed questionnaire. A major part of this paper is devoted to explaining our validation methodology. There is very little in the literature that directly relates to how process models have been validated, therefore providing this transparency will benefit both the research community and practitioners. The validation methodology and the model itself contribute towards a better understanding of modelling RE processes. 
76|3||Genetic granular classifiers in modeling software quality|Hyperbox classifiers are one of the most appealing and intuitively transparent classification schemes. As the name itself stipulates, these classifiers are based on a collection of hyperboxes––generic and highly interpretable geometric descriptors of data belonging to a given class. The hyperboxes translate into conditional statements (rules) of the form “if feature1 is in [a, b] and feature2 is in [d, f] and … and featuren is in [w, z] then class Ï” where the intervals ([a, b], … , [w, z]) are the respective edges of the hyperbox. The proposed design process of hyperboxes comprises of two main phases. In the first phase, a collection of “seeds” of the hyperboxes is formed through data clustering (realized by means of the Fuzzy C-Means algorithm, FCM). In the second phase, the hyperboxes are “grown” (expanded) by applying mechanisms of genetic optimization (and genetic algorithm, in particular). We reveal how the underlying geometry of the hyperboxes supports an immediate interpretation of software data concerning software maintenance and dealing with rules describing a number of changes made to software modules and their linkages with various software measures (such as size of code, McCabe cyclomatic complexity, number of comments, number of characters, etc.). 
76|3||A crisscross checking technique for tamper detection in halftone images|As the race for digital technological advancements accelerates, foremost is the troubling concern for better protection for digital data. Unequivocally, detecting for tamper in data poses a great challenge to information security and image processing. In this paper, a crisscross checking technique is proposed for tamper detection in halftone images. First, a host image is divided into N × N blocks. The individual blocks will be scrambled with a random number into code-strings which will be concatenated into rows and columns code-streams. Finally, these code-streams will be applied to the MD5 and RSA algorithms to generate encrypted signed messages that will be hidden in the blocks. For the detection process, the row and column blocks will be checked for these signed messages which will be extracted, decrypted and compared to the rows and columns blocks of the image for dissimilar bits as sign of tamper. The tampered row and column blocks will be crisscrossed and the intersecting blocks will be identified as tampered areas. 
76|3||Implementation of wireless network environments supporting inter access point protocol and dual packet filtering|With advances in wireless technologies, future internetworks will include large numbers of mobile hosts roaming across wireless cells. The goal of this paper is to provide a wireless network communication environment, in which mobile hosts can roam among various access points (wireless cells) and across different subnets. For this purpose, we develop methods to implement and integrate the communication mechanisms of Mobile IP and Inter Access Point Protocol (IAPP). For efficient transmission, we also devise a dual packet filtering technique to filter out unnecessary packet transmission, thereby reducing traffic on wireless networks. Such filtering is especially important because of the limited bandwidth of wireless links. Our work can serve as the reference once IAPP has been approved as standard. 
76|3||Specifying process and measuring progress in terms of information state|This paper suggests an approach to specifying process in such a way that progress can be measured in terms of achievement rather than effort expended. The outcome of each process activity is specified in terms of the status of information items and the relationships between them. Pre-requisites to this approach are the presence of an overall information model, and the ability to manage this model and its instantiation in an information repository. A description is given of how the approach has been implemented for a requirements management process using the DOORS requirements management tool. The work was carried out as part of the EC funded Framework project, SUCSEDE. 
76|3||Cryptanalysis of HuangâChang partially blind signature scheme|Partially blind signature is an extension of blind signature that allows a signer to sign a partially blinded message that include pre-agreed information such as expiry date or collateral conditions in the resulting signatures. It is more useful than blind signature in many applications, such as electronic cash (e-cash) system. Recently, Huang and Chang proposed a new design of efficient partially blind signature scheme. However, in this paper, we show that Huang–Chang partially blind signature scheme is not secure, this scheme does not meet the basic property of a partially blind signature, partial blindness: a malicious user (requester) can prepare a special public information a′ and then remove the original information a from the signer’s signature to obtain a signature with this special information. 
76|3||A new multi-secret images sharing scheme using Largrangeâs interpolation|Secret sharing is to send shares of a secret to several participants, and the hidden secret can be decrypted only by gathering the distributed shares. This paper proposes an efficient secret sharing scheme using Largrange’s interpolation for generalized access structures. Furthermore, the generated shared data for each qualified set is 1/(r − 1) smaller than the original secret image if the corresponding qualified set has r participants. In our sharing process, a sharing circle is constructed first and the shared data is generated from the secret images according to this sharing circle. The properties of the sharing circle not only reduce the size of generated data between two qualified sets but also maintain the security. Thus the actual ratio of the shared data to the original secret images is reduced further. The proposed scheme offers a more efficient and effective way to share multiple secrets. 
76|3||Contents Volume 76|
77|1|http://www.sciencedirect.com/science/journal/01641212/77/1|Parallel and distributed real-time systems|
77|1||Robust scheduling in team-robotics|Mobile robots interact with a dynamically changing, physical environment. All tasks controlling such interactions must be performed reliably and in real-time. Information from the local sensors often is incomplete or inconsistent. Distributed sensor fusion is a technique that enables a team to get a more complete view of the world with a better quality of the provided information. In this paper we address the problem of scheduling the local processing tasks that are part of the overall fusion process. The particular problem to be addressed lies in the unpredictable execution times of these tasks, which do not allow for scheduling using worst-case execution times. The Time-Aware Fault-Tolerant (TAFT) scheduler allows working with expected-case execution times instead, and still achieves a predictable timing behavior. The paper details an efficient scheduling strategy for TAFT based on Earliest Deadline algorithms, formalizing the adopted task model and the underlying scheduling mechanism. Results are presented showing the achieved real-time behavior with an increased acceptance rate, a higher throughput, and a graceful degradation in transient overload situations compared to standard schedulers. Additionally, it describes the implementation of TAFT in the real-time platform that is embedded in our robot team. 
77|1||Reliable event-triggered systems for mechatronic applications|Mechatronic systems most often require hard real-time behaviour of the controlling system. The standard solution for this kind of application is based on the time-triggered approach, and for certain circumstances the schedulability is provable. In contrast, this paper introduces an approach using some hardware enhancements that allow first to substitute the time-triggered system by an event-triggered system but conserving the reliability, second to enhance the event-triggered system by a two-level reaction system while conserving the hard real-time capabilities and third to combine tasks to improve even the worst-case behaviour. This results in a hard-time-but-weak-logic reaction system when computing time is tide but maintains full processing capabilities and therefore exact reaction values for all reactions whenever possible. This meets the goal of creating an event-triggered and reliable system approach. Combining two or more events to more than one combination will improve the theoretical schedulability of the system too, especially in the case when configurable computing elements are used. 
77|1||COSMIC: A real-time event-based middleware for the CAN-bus|The paper describes the event model and the architecture of the COSMIC (COoperating SMart devICes) middleware. Based on the assumption of tiny smart sensors and actuators, COSMIC supports a distributed system of cooperating autonomous devices. COSMIC considers quality of service requirements in the event model and provides an application interface which allows to express the respective temporal and reliability attributes on a high, application related abstraction level. According to the need in most real-time systems, COSMIC supports event channels with different timeliness and reliability classes. Hard real-time event channels are considered to meet all temporal requirements under the specified fault assumptions. The resource requirements for this type of channel are statically assigned by an appropriate reservation scheme. Soft real-time event channels are scheduled by their deadlines, but they are not guaranteed under transient overload conditions. Non-real-time event channels are used for events without any specified timeliness requirements in a best-effort manner. The paper finally presents the layered COSMIC architecture to map the different channel classes to the CAN-Bus. 
77|1||Design and performance of a CAN-based connection-oriented protocol for Real-Time CORBA|The Real-Time CORBA and minimumCORBA specifications are important steps towards defining standard-based middleware which can satisfy real-time requirements in an embedded system. To reach a broad acceptance in the real-time and embedded community, these specifications have to facilitate the utilization of traditional real-time networks for embedded systems. The Controller Area Network (CAN) is one of the most important networks in the field of real-time embedded systems. Consequently, this paper presents a CAN-based connection-oriented communication model and its integration into Real-Time CORBA. In order to make efficient use of the advantages of CAN, we present a new inter-ORB protocol, which uses smaller message headers for CAN and maps the CAN priorities to bands of CORBA priorities. We also present design and implementation details and evaluate the performance of the new inter-ORB protocol. 
77|1||Towards modeling and evaluation of ETCS real-time communication and operation|The future European Train Control System (ETCS) will be based on mobile communication and overcome fixed blocks. It is introduced in order to increase track utilization and interoperability throughout Europe while reducing trackside equipment cost. Data processing on board the train and in radio block centers as well as the radio communication link are crucial factors for the safe and efficient operation. Their real-time behavior under inevitable link failures needs to be modeled and evaluated. The paper presents a stochastic Petri net model of communication failure and recover behavior. A second model for the exchange of location and movement authority data packets between trains and radio block centers is presented and analyzed. Performance evaluation of the model shows the significant impact of packet delays and losses on the reliable operation of high-speed trains. 
77|1||MIP formulation for robust resource allocation in dynamic real-time systems|Real-time systems usually operate in an environment that changes continuously. These changes cause the performance of the system to vary during run time. An allocation of resources in this environment must be robust. Using the amount of load variation that the allocation can accommodate as a measure of robustness, we develop a mathematical formulation for the problem of robust resource allocation. Due to the complexity of the models used to represent the problem, the formulation is non-linear. We propose a linearization technique based on variable substitution to reduce the mathematical formulation to a mixed integer programming formulation, called SMIP. Compared with existing techniques, the search space of SMIP is not restricted. Thus, if a feasible allocation exists, SMIP will always produce an optimal allocation. 
77|1||Fair scheduling of dynamic task systems on multiprocessors|In dynamic real-time task systems, tasks that are subject to deadlines are allowed to join and leave the system. In previous work, Stoica et al. and Baruah et al. presented conditions under which such joins and leaves may occur in fair-scheduled uniprocessor systems without causing missed deadlines. In this paper, we extend their work by considering fair-scheduled multiprocessors. We show that their conditions are sufficient on M processors, under any deadline-based Pfair scheduling algorithm, if the utilization of every subset of M − 1 tasks is at most one. Further, for the general case in which task utilizations are not restricted in this way, we derive sufficient join/leave conditions for the PD2 Pfair algorithm. We also show that, in general, these conditions cannot be improved upon without causing missed deadlines. 
77|2|http://www.sciencedirect.com/science/journal/01641212/77/2|Improved standard FPA methodâresolving problems with upper boundaries in the rating complexity process|The standard Function Point Analysis (FPA) method maps complexity to an ordinal scale such that function types (functions) labelled “high” complexity may have a very different underlying complexity. This deficiency is the subject of this article. An improved FPA method is presented that breaks down a very complex function into an equivalent set of functions with normal complexity. The improved FPA method is not a new method, it is an improvement of the standard FPA method. In the process of defining an improved FPA method, a model was built, which helped us find the connection between the standard FPA method and the Common Software Measurement International Consortium—Full Function Point (COSMIC-FFP) and Mark II Function Point Analysis (MKII FPA) methods. These methods were used to define the improved FPA method algorithm, because they show a bigger functional size for very complex functions. In the end, an evaluation of the improved FPA method was made with the results of the ISBSG Repository Data Disk. 
77|2||An empirical comparison of the dynamic modeling in OML and UML|This paper presents an empirical research for evaluating the semantic comprehension of two standard languages, UML (Unified Modeling Language) versus OML (OPEN Modeling Language), from the perspective of the dynamic modeling. We carried out two controlled experiments using a 2 × 2 crossover design, where the metrics studied were the comprehension time and the total score. We examined the OML and UML interaction diagrams and the statecharts of each language corresponding to the design of a real-time embedded system. The results obtained reveal that the specification of the dynamic behavior using OML is faster to comprehend and easier to interpret than using the UML language, regardless of the dynamic diagram type. 
77|2||Bayesian network based software reliability prediction with an operational profile|This paper uses a Bayesian network to model software reliability prediction with an operational profile. Due to the complexity of software products and development processes, software reliability models need to possess the ability to deal with multiple parameters. A Bayesian network exhibits a strong ability to adapt to problems involving complex variant factors. A special kind of Bayesian network named a Markov Bayesian network has been applied successfully into modeling software reliability prediction. However, the existing research did not pay enough attention to the fact that the failure characteristics of many software systems often depend on the specific operation performed. In this paper, an extended Markov Bayesian network is developed to model software reliability prediction with an operational profile. The extended Markov Bayesian network proposed in the paper is focused on discrete-time failure data. Methods to solve the network are proposed, and an example is used to illustrate the utilization of the model. 
77|2||A family of experiments to validate metrics for software process models|Process modelling is a key activity of software process management and it is the starting point for enacting, evaluating and improving software processes. The current competitive marketplace calls for the continuous improvement of processes and therefore, it is fundamental to have software process models with a high maintainability. In this paper we introduce a set of metrics for software process models and discuss how these can be used as maintainability indicators. In particular, we report the results of a family of experiments that assess relationships between the structural properties, as measured by the defined metrics, of the process models and their maintainability. 
77|2||An empirical investigation of the impact of the object-oriented paradigm on the maintainability of real-world mission-critical software|Empirical evidence for the maintainability of object-oriented systems is far from conclusive, partly due to the lack of representativeness of the subjects and systems used in earlier studies. We empirically examined this issue for mission-critical software that was currently operational and maintained by software professionals. Two functionally equivalent versions of a credit approval system were maintained, one object oriented (OO) and the other non-object oriented (NOO). We found that the OO group took less time to maintain a greater number of software artifacts than its NOO counterpart. This difference held for all phases of the software development life cycle. This result was due to the usefulness of UML for impact analysis of the OO version, which contributed to effective comprehension and communication. Insufficient design specifications for the NOO version led to ambiguity and costly defects in transferring design solutions to development. Also, the encapsulation of the OO version appeared to reduce mental loads for maintenance tasks and resulted in code reuse. On the other hand, the number of files to be managed increased and, thus, dependency management was required for the OO version. Furthermore, despite much tuning, the OO version ran slower than its NOO counterpart. More field studies on software professionals are needed to compare contextual factors such as methods, processes, and maintenance tools. 
77|2||Cost-reliability-optimal release policy for software reliability models incorporating improvements in testing efficiency|Over the past 30 years, many software reliability growth models (SRGMs) have been proposed for estimation of reliability growth of products during software development processes. One of the most important applications of SRGMs is to determine the software release time. Most software developers and managers always want to know the date on which the desired reliability goal will be met. In this paper, we first review a SRGM with generalized logistic testing-effort function and the proposed generalized logistic testing-effort function can be used to describe the actual consumption of resources during the software development process. Secondly, if software developers want to detect more faults in practice, it is advisable to introduce new test techniques, tools, or consultants, etc. Consequently, here we propose a software cost model that can be used to formulate realistic total software cost projects and discuss the optimal release policy based on cost and reliability considering testing effort and efficiency. Some theorems and several numerical illustrations are also presented. Based on the proposed models and methods, we can specifically address the problem of how to decide when to stop testing and when to release software for use. 
77|2||Investigating Web size metrics for early Web cost estimation|This paper’s aim is to bring light to this issue by identifying size metrics and cost drivers for early Web cost estimation based on current practices of several Web Companies worldwide. This is achieved using two surveys and a case study. The first survey (S1) used a search engine to obtain Web project quote forms employed by Web companies worldwide to provide initial quotes on Web development projects. The 133 Web project quote forms gathered data on size metrics, cost factors, contingency and possibly profit metrics. These metrics were organised into categories and ranked. Results indicated that the two most common size metrics used for Web cost estimation were “total number of Web pages” (70%) and “which features/functionality to be provided by the application” (66%).The results of S1 were then validated by a mature Web company that has more than 12 years of experience in Web development and a portfolio of more than 50 Web applications. The analysis was conducted using an interview.Finally, once the case study was finished, a second validation was conducted using a survey (S2) involving local New Zealand Web companies. The results of both validations were used to prepare Web project data entry forms to gather data on Web projects worldwide. After gathering data on 67 real Web projects worldwide, multivariate regression applied to the data confirmed that the number of Web pages and features/functionality provided by the application to be developed were the two most influential effort predictors. 
77|2||On-line prediction of software reliability using an evolutionary connectionist model|An on-line adaptive software reliability prediction model using evolutionary connectionist approach based on multiple-delayed-input single-output architecture is proposed. Based on the currently available software failure time data, genetic algorithm is used to globally optimize the number of the delayed input neurons and the number of neurons in the hidden layer of the neural network architecture. Bayesian regularization is applied to our network training scheme to improve the generalization capability. The corresponding optimized neural network architecture is iteratively and dynamically reconfigured in real-time as new actual failure time data arrives. The performance of our proposed approach has been tested using four real-time control and flight dynamic application data sets. Numerical results show that our proposed approach is robust across different software projects, and has a better performance with respect to next-step-predictability compared to existing neural network model for failure time prediction. 
77|2||Hybrid approaches to product recommendation based on customer lifetime value and purchase preferences|Recommending products to attract customers and meet their needs is important in fiercely competitive environments. Recommender systems have emerged in e-commerce applications to support the recommendation of products. Recently, a weighted RFM-based method (WRFM-based method) has been proposed to provide recommendations based on customer lifetime value, including Recency, Frequency and Monetary. Preference-based collaborative filtering (CF) typically makes recommendations based on the similarities of customer preferences. This study proposes two hybrid methods that exploit the merits of the WRFM-based method and the preference-based CF method to improve the quality of recommendations. Experiments are conducted to evaluate the quality of recommendations provided by the proposed methods, using a data set concerning the hardware retail marketing. The experimental results indicate that the proposed hybrid methods outperform the WRFM-based method and the preference-based CF method. 
77|2||Model driven generation and testing of object-relational mappings|Object-oriented software development as well as relational data storage are leading standards in their respective areas. The persistent storage of objects in relational tables is therefore a topic of major interest. To do so efficiently, a plethora of problems has to be overcome due to the impedance mismatch between the object-oriented and relational paradigms. Nowadays, dedicated object-relational middlewares are frequently used to decouple relational databases from object-oriented applications. Even if this approach shields the developer from the majority of run-time related aspects, the manual mapping of incrementally evolving complex object models to relational tables still remains as an inherently difficult and error-prone task. Therefore, this article focuses on automation support for the model driven generation and testing of object-relational mappings. 
77|3|http://www.sciencedirect.com/science/journal/01641212/77/3|Software reverse engineering|
77|3||Spectral and meta-heuristic algorithms for software clustering|When large software systems are reverse engineered, one of the views that is produced is the system decomposition hierarchy. This hierarchy shows the system’s subsystems, the contents of the subsystems (i.e., modules or other subsystems), and so on. Software clustering tools create the system decomposition automatically or semi-automatically with the aid of the software engineer.The Bunch software clustering tool shows how meta-heuristic search algorithms can be applied to the software clustering problem, successfully. Unfortunately, we do not know how close the solutions produced by Bunch are to the optimal solution. We can only obtain the optimal solution for trivial systems using an exhaustive search.This paper presents evidence that Bunch’s solutions are within a known factor of the optimal solution. We show this by applying spectral methods to the software clustering problem. The advantage of using spectral methods is that the results this technique produces are within a known factor of the optimal solution. Meta-heuristic search methods only guarantee local optimality, which may be far from the global optimum. In this paper, we apply the spectral methods to the software clustering problem and make comparisons to Bunch. We conducted a case study to draw our comparisons and to determine if an efficient clustering algorithm, one that guarantees a near-optimal solution, can be created. 
77|3||A language-independent software renovation framework|One of the undesired effects of software evolution is the proliferation of unused components, which are not used by any application. As a consequence, the size of binaries and libraries tends to grow and system maintainability tends to decrease. At the same time, a major trend of today’s software market is the porting of applications on hand-held devices or, in general, on devices which have a limited amount of available resources. Refactoring and, in particular, the miniaturization of libraries and applications are therefore necessary.We propose a Software Renovation Framework (SRF) and a toolkit covering several aspects of software renovation, such as removing unused objects and code clones, and refactoring existing libraries into smaller more cohesive ones. Refactoring has been implemented in the SRF using a hybrid approach based on hierarchical clustering, on genetic algorithms and hill climbing, also taking into account the developers’ feedback. The SRF aims to monitor software system quality in terms of the identified affecting factors, and to perform renovation activities when necessary. Most of the framework activities are language-independent, do not require any kind of source code parsing, and rely on object module analysis.The SRF has been applied to GRASS, which is a large open source Geographical Information System of about one million LOCs in size. It has significantly improved the software organization, has reduced by about 50% the average number of objects linked by each application, and has consequently also reduced the applications’ memory requirements. 
77|3||ConSUS: a light-weight program conditioner|Program conditioning consists of identifying and removing a set of statements which cannot be executed when a condition of interest holds at some point in a program. It has been applied to problems in maintenance, testing, re-use and re-engineering. All current approaches to program conditioning rely upon both symbolic execution and reasoning about symbolic predicates. The reasoning can be performed by a ‘heavy duty’ theorem prover but this may impose unrealistic performance constraints.This paper reports on a lightweight approach to theorem proving using the FermaT Simplify decision procedure. This is used as a component to ConSUS, a program conditioning system for the Wide Spectrum Language WSL. The paper describes the symbolic execution algorithm used by ConSUS, which prunes as it conditions.The paper also provides empirical evidence that conditioning produces a significant reduction in program size and, although exponential in the worst case, the conditioning system has low degree polynomial behaviour in many cases, thereby making it scalable to unit level applications of program conditioning. 
77|3||Static object trace extraction for programs with pointers|A trace is a record of the execution of a computer program, showing the sequence of operations executed. Dynamic traces are obtained by executing the program and depend upon the input. Static traces, on the other hand, describe potential sequences of operations extracted statically from the source code. Static traces offer the advantage that they do not depend upon input data.This paper describes a new automatic technique to extract static traces for individual stack and heap objects. The extracted static traces can be used in many ways, such as protocol recovery and validation in particular and program understanding in general.In addition, this article describes four case studies we conducted to explore the efficiency of our algorithm, the size of the resulting static traces, and the influence of the underlying points-to analysis on this size. 
77|3||Discovering thread interactions in a concurrent system|Understanding the behavior of a system is a central reverse engineering task, and is crucial for being able to modify, maintain, and improve the system. An often difficult aspect of some system behaviors is concurrency, in particular identifying those areas that exhibit mutual exclusion and those that exhibit synchronization. In this paper we present a technique that builds on our previous work in behavior discovery to find the points in the system that demonstrate mutually exclusive and synchronized behavior. Finding these points in the behavior of the system is an important aid in reverse engineering a complete and correct model of the system. 
77|3||Contents Volume 77|
78|1|http://www.sciencedirect.com/science/journal/01641212/78/1|A novel image watermarking scheme based on support vector regression|In this paper, a novel support vector regression based color image watermarking scheme is proposed. Using the information provided by the reference positions, the support vector regression can be trained at the embedding procedure, and the watermark is adaptively embedded into the blue channel of the host image by considering the human visual system. Thanks to the good learning ability of support vector machine, the watermark can be correctly extracted under several different attacks. Experimental results show that the proposed scheme outperform the Kutter’s method and Yu’s method against different attacks including noise addition, shearing, luminance and contrast enhancement, distortion, etc. Especially when the watermarked image is enhanced in luminance and contrast at rate 70%, our method can extract the watermark with few bit errors. 
78|1||Relevance feedback using adaptive clustering for image similarity retrieval|Research has been devoted in recent years to relevance feedback as an effective solution to improve performance of image similarity search. However, few methods using the relevance feedback are currently available to perform relatively complex queries on large image databases. In the case of complex image queries, images with relevant concepts are often scattered across several visual regions in the feature space. This leads to adapting multiple regions to represent a query in the feature space. Therefore, it is necessary to handle disjunctive queries in the feature space.In this paper, we propose a new adaptive classification and cluster-merging method to find multiple regions and their arbitrary shapes of a complex image query. Our method achieves the same high retrieval quality regardless of the shapes of query regions since the measures used in our method are invariant under linear transformations. Extensive experiments show that the result of our method converges to the user’s true information need fast, and the retrieval quality of our method is about 22% in recall and 20% in precision better than that of the query expansion approach, and about 35% in recall and about 31% in precision better than that of the query point movement approach, in MARS. 
78|1||OPERA: An open-source extensible router architecture for adding new network services and protocols|In this paper, we present the design and implementation of a programmable and extensible router architecture. The proposed architecture not only provides the conventional packet forward/routing functions, but also the flexibility to integrate additional services (or extension) into a router. These extensions are dynamically loadable modules so one can easily deploy new services, such as reliability and security enhancement, onto the router in a dynamic and incremental fashion. To avoid new extensions that may monopolize system resource and degrade the performance of normal packet forwarding/routing function, we propose a novel CPU resource reservation scheme which facilitates the efficient use of resources and increases the stability of extension execution. To illustrate the “extensibility” and “effectiveness” of the proposed architecture, we present the results of a new service, namely, how to perform “Distributed Denial-of-Service (DDoS) attack traceback”. In particular, we illustrate the deployment of the probabilistic marking in performing IP traceback. Note that this approach requires the collaboration of routers so that effective traceback can be performed. Currently, the programmable router platform is released as an open source1 and we believe the system provides an ideal platform for researchers to experiment and to validate new services and protocols. 
78|1||Lightweight multigranularity locking for transaction management in XML database systems|As eXtensible Markup Language (XML) provides a capability for describing data structures, and can accommodate many kinds of semistructured data. The semistructured data format is flexible for changing data structures through insertion and deletion of data elements in mission-critical applications. In the case of concurrently changing such a data format, this flexibility could be endangered by a phantom problem which might lead to inconsistent information flow. For the purpose of developing a concurrency control scheme without the phantom phenomenon, we propose a lightweight multigranularity locking (LWMGL) scheme that is a hybrid mechanism of Tree-based Locking and Multigranularity Locking. The goal of this scheme is to realize locking at the level of precise elements in an XML database while preventing the phantom problems. Since these precise locks could considerably reduce the number of pseudo-conflicts that are regarded as unnecessary locks, they provide high concurrency compared with other concurrency control schemes. In order to realize the LWMGL scheme we also devised a new data model of XML indexed element tables (XIETs) for transferring diverse XML documents. This data model does not only can preserve the XML tree structure in application levels, but also enables execution of the structural change operations as well as the data access operations in parallel. 
78|1||A least upper bound on the fault tolerance of real-time systems|This paper presents a method to deal with the reexecution of tasks in a hard real-time system subject to temporary faults. The set of tasks follows the Liu and Layland model: they are periodic, independent and preemptible. Time is considered to be slotted. The system is said to be k-schedulable if it is schedulable in spite of the fact that in the interval between its release and its deadline, every task admits that k slots are devoted to uses other than its first execution. In this case, the k slots are used to reexecute tasks subject to temporary faults. Since the value of k can be easily determined, a least upper bound on all the possible combinations of faults that the system can tolerate while meeting the hard time-constraints, follows immediately. The method is bandwidth preserving and the expression of the bound is a diophantic inequality relating k, the execution time and the period of each task. The method is compared to methods proposed by other authors to solve the same problem and it is evaluated through extensive simulations performed on random generated sets of tasks. 
78|1||Remarks on WuâHsuâs threshold signature scheme using self-certified public keys|Wu and Hsu proposed a (t, n) threshold signature scheme using self-certified public keys in order to integrate the properties of self-certified public key schemes and threshold signature schemes. Even though their scheme is more efficient when compared to previous works based on the certificate-based public key systems, we find some design defects of their scheme. In this paper, by identifying some concrete instances and analyses we will show that their scheme is not as secure as they claimed. 
78|1||Overcoming the obfuscation of Java programs by identifier renaming|Decompilation is the process of translating object code to source code and is usually the first step towards the reverse-engineering of an application. Many obfuscation techniques and tools have been developed, with the aim of modifying a program, such that its functionalities are preserved, while its understandability is compromised for a human reader or the decompilation is made unsuccessful. Some approaches rely on malicious identifiers renaming, i.e., on the modification of the program identifiers in order to introduce confusion and possibly prevent the decompilation of the code.In this work we introduce a new technique to overcome the obfuscation of Java programs by identifier renaming. Such a technique relies on the intelligent modification of identifiers in Java bytecode.We present a new software tool which implements our technique and allows the processing of an obfuscated program in order to rename the identifiers as required by our technique. Moreover, we show how to use the existing tools to provide a partial implementation of the technique we propose.Finally, we discuss the feasibility of our approach by showing how to contrast the obfuscation techniques based on malicious identifier renaming recently presented in literature. 
78|1||DDH-based group key agreement in a mobile environment|A group key agreement protocol is designed to efficiently implement secure multicast channels for a group of parties communicating over an untrusted, open network by allowing them to agree on a common secret key. In the past decade many problems related to group key agreement have been tackled and solved (diminished if not solved), and recently some constant-round protocols have been proven secure in concrete, realistic setting. However, all forward-secure protocols so far are still too expensive for small mobile devices. In this paper we propose a new constant-round protocol well suited for a mobile environment and prove its security under the decisional Diffie–Hellman assumption. The protocol meets simplicity, efficiency, and all the desired security properties. 
78|1||An information flow control model for C applications based on access control lists|Access control within an application during its execution prevents information leakage. The prevention can be achieved through information flow control. Many information flow control models were developed, which may be based on discretionary access control (DAC), mandatory access control (MAC), label-based approach, and role-based access control (RBAC). Most existing models are for object-oriented systems. Since the procedural C language is still in use heavily, offering a model to control information flows for C applications should be fruitful. Although we identified information flow control models that can be applied to procedural languages, they do not offer the features we need. We thus developed a model to control information flows for C applications. Our model is based on access control lists (ACLs) and named CACL. It offers the following features: (a) controlling both read and write access, (b) preventing indirect information leakage, (c) detailing the control granularity to variables, (d) avoiding improper function call, (e) controlling function call through argument sensitivity, and (f) preventing change of an application when the access rights of the application’s real world users change. This paper presents CACL. 
78|2|http://www.sciencedirect.com/science/journal/01641212/78/2|Automatic detection and correction of programming faults for software applications|Software reliability is an important feature of a good software implementation. However some faults which cause software unreliability are not detected during the development stages, and these faults create unexpected problems for users whenever they arise. At present most of the current techniques detect faults while a software is running. These techniques interrupt the software process when a fault occurs, and require some forms of restart.In this paper Precompiled Fault Detection (PFD) technique is proposed to detect and correct faults before a source code is compiled. The objective of the PFD technique is to increase software reliability without increasing the programmers’ responsibilities. The concepts of “pre-compilation” and “pattern matching” are applied to PFD in order to reduce the risk of significant damage during execution period. This technique can completely eliminate the significant faults in a software and thus, improves software reliability. 
78|2||Bi-directional safety analysis of product lines|As product-line engineering becomes more widespread, more safety-critical software product lines are being built. This paper describes a structured method for performing safety analysis on a software product line, building on standard product-line assets: product-line requirements, architecture, and scenarios. The safety-analysis method is bi-directional in that it combines a forward analysis (from failure modes to effects) with a backward analysis (from hazards to contributing causes). Safety-analysis results are converted to XML files to allow automated consistency checking between the forward and backward analysis results and to support reuse of the safety-analysis results throughout the product line. The paper demonstrates and evaluates the method on a safety-critical product-line subsystem, the Door Control System. Results show that the bi-directional safety-analysis method found both missing and incorrect software safety requirements. Some of the new safety requirements affected all the systems in the product line while others affected only some of the systems in the product line. The results demonstrate that the proposed method can handle the challenges to safety analysis posed by variations within a product line. 
78|2||Software complexity and its impacts in embedded intelligent real-time systems|Applications of intelligent software systems are proliferating. As these systems proliferate, understanding and measuring their complexity becomes vital, especially in safety-critical environments. This paper proposes a model assessing the impacts of complexity for a particular type of intelligent software system, embedded intelligent real-time systems (EIRTS), and answers two research questions. (1) How should the complexity of embedded intelligent real-time systems be measured?, and (2) What are the impacts of differing levels of EIRTS complexity on software, operator and system performance when EIRTS are deployed in a safety-critical large-scale system? The model is tested and operationalized using an operational EIRTS in a safety-critical environment. The results suggest that users significantly prefer simple decision support and user interfaces, even when sophisticated user interfaces and complex decision support capabilities have been embedded in the system. These results have interesting implications for operators using complex EIRTS in safety-critical settings. 
78|2||Efficient online computation of statement coverage|Evaluation of statement coverage is the problem of identifying the statements of a program that execute in one or more runs of a program. The traditional approach for statement coverage tools is to use static code instrumentation. In this paper we present a new approach to dynamically insert and remove instrumentation code to reduce the runtime overhead of statement coverage measurement. We also explore the use of dominator tree information to reduce the number of instrumentation points needed. Our experiments show that our approach reduces runtime overhead by 38–90% compared with purecov, a commercial statement coverage tool. Our tool is fully automated and available for download from the Internet. 
78|2||Two controlled experiments concerning the comparison of pair programming to peer review|This paper reports on two controlled experiments comparing pair programming with single developers who are assisted by an additional anonymous peer code review phase. The experiments were conducted in the summer semester 2002 and 2003 at the University of Karlsruhe with 38 computer science students. Instead of comparing pair programming to solo programming this study aims at finding a technique by which a single developer produces similar program quality as programmer pairs do but with moderate cost.The study has one major finding concerning the cost of the two development methods. Single developers are as costly as programmer pairs, if both programmer pairs and single developers with an additional review phase are forced to produce programs of similar level of correctness. In conclusion, programmer pairs and single developers become interchangeable in terms of development cost. As this paper reports on the results of small development tasks the comparison could not take into account long time benefits of either technique. 
78|2||FNDS: a dialogue-based system for accessing digested financial news|Electronic financial news available on the Internet contains a wealth of information useful for business decision-making. However, as this information is both qualitative and existent in huge volumes, it is very inefficient to digest manually. This paper presents a prototype system called FNDS, which automatically digests financial news by extracting important information from the articles and using this information to fill in pre-defined templates. A unique feature of FNDS is that users can access the extracted information through an interactive dialogue-based interface. This has the advantage that if users do not know exactly what information is required, the system will provide feedback to help them to formulate the information requirement incrementally. 
78|2||What do software practitioners really think about project success: an exploratory study|Understanding what software practitioners value and how they define project success has implications for both practitioner motivation and software development productivity. We conducted a survey to discover some of the components of project outcome (in terms of personal/professional aspects as well as the project as a whole) that practitioners consider important in defining project success. We also investigated some of those components that practitioners perceived were important contributors to success through their impact on the development process. Sixty-six practitioners participated in our study. They considered software projects to be successful if they provide them with intrinsic, internally motivating work in developing software systems that both meet customer/user needs and are easy to use. 
78|2||A framework for assisting the design of effective software process improvement implementation strategies|A number of advances have been made in the development of software process improvement (SPI) standards and models, e.g. Capability Maturity Model (CMM), more recently CMMI, and ISO’s SPICE. However, these advances have not been matched by equal advances in the adoption of these standards and models in software development which hasresulted in limited success for many SPI efforts. The current problem with SPI is not a lack of standards or models, but rather a lack of an effective strategy to successfully implement these standards or models.In this paper we have focused on SPI implementation issues and designed three individual components in order to assist SPI practitioners in the design of effective SPI implementation initiatives. We have pulled together individual components under one SPI implementation framework (SPI-IF) using a bottom-up approach already familiar to many practitioners and researchers. The framework is based on the results drawn from SPI literature and an empirical study we have carried out. In the design of SPI-IF, the concept of critical success factors (CSFs) was used and extended. Thirty-four CSF interviews were conducted with Australian practitioners. In addition, 50 research articles (published experience reports and case studies) were also selected and analysed in order to identify factors that play positive or negative roles in SPI implementation. The SPI-IF provides a very practical structure with which to assess and implement SPI implementation initiatives.In order to evaluate SPI-IF, a practical evaluation scheme was undertaken. The evaluation results show that SPI-IF has potential to assist SPI practitioners in the design of effective SPI implementation initiatives. Thus, we recommend organizations to use SPI-IF in order to effectively design SPI implementation initiatives. 
78|3|http://www.sciencedirect.com/science/journal/01641212/78/3|Announcement: New Section on Software Architecture|
78|3||Pinned demand paging based on the access frequency of video files in video servers|In this paper, we present an ameliorative demand-paging algorithm called PDPAF (i.e., pinned demand paging based on the access frequency of video files), to efficiently utilize the limited buffer space in a VOD (video-on-demand) server. It excludes the limitation of the disk bandwidth, and raises the hit ratio of video pages in the buffer, thereby increasing the total number of concurrent clients. Furthermore, we also propose an admission control algorithm to decide whether a new request can be admitted. Finally, we conduct extensive experiments to compare PDPAF with other algorithms on the average waiting time and the maximal number of concurrent requests, and the simulation results validate the superiority of our approach. 
78|3||Distributed multiple selection algorithm for peer-to-peer systems|This paper presents an efficient distributed multiple selection algorithm designed to select multiple keys simultaneously from different data sets which are distributed to many computers in a peer-to-peer system. The communication time is usually much longer than the computation time and is thus a major criterion for measuring the performance of a distributed algorithm. The objective of this algorithm is to reduce the number of communication messages. The algorithm makes use of statistical knowledge and results in a smaller communication overhead compared with existing algorithms.In this algorithm, each computer will select keys as pivots (candidates for the answers) according to statistical knowledge and transmit them to other computers in the system. Each computer will compare each pivot with key values in its local file and respond by transmitting ranks to the originating computer. The originating computer will calculate the global ranks and verify whether the pivots are the answers. Each computer will broadcast once sequentially in each round. This broadcasting process will be repeated until all answers are found.Complexity analyses are presented to provide theoretical upper bounds of this algorithm. The correctness of the theoretical predication is verified by experiments with 40 computers connected using Web technologies. 
78|3||Worst case constant time priority queue|We present a new data structure of size 3M bits, where M is the size of the universe at hand, for realizing a discrete priority queue. When this data structure is used in combination with a new memory topology it executes all discrete priority queue operations in O(1) worst case time. In doing so we demonstrate how an unconventional, but practically implementable, memory architecture can be employed to sidestep known lower bounds and achieve constant time performance. 
78|3||Adaptive row major order: a new space filling curve for efficient spatial join processing in the transform space|A transform-space index indexes spatial objects represented as points in the transform space. An advantage of a transform-space index is that optimization of spatial join algorithms using these indexes can be more formal. The authors earlier proposed the Transform-Based Spatial Join algorithm that joins two transform-space indexes. It renders global optimization easy with little overhead by utilizing the characteristics of the transform space. In particular, it allows us to globally determine the order of accessing disk pages, which makes a significant impact on the performance of joins. For this purpose, we use various space filling curves. In this paper, we propose a new space filling curve called the adaptive row major order (ARM order). The ARM order adaptively controls the order of accessing pages and significantly reduces the one-pass buffer size (the minimum buffer size required for guaranteeing one disk access per page) and the number of disk accesses for a given buffer size. Through analysis and experiments, we verify excellence of the ARM order when used with the Transform-Based Spatial Join. The Transform-Based Spatial Join with the ARM order always outperforms those with other conventional space filling curves in terms of both measures used: the one-pass buffer size and the number of disk accesses. Specifically, it reduces the one-pass buffer size by up to 25.9 times and the number of disk accesses by up to 2.11 times. We conclude that we achieve these results mainly due to global optimization of the order of accessing disk pages using an adaptive space filling curve. 
78|3||HANet: a framework toward ultimately reliable network services|High availability is becoming an essential part of network services because even a little downtime may lead to a great loss of money. According to previous research, network failure is one of the major causes of system unavailability. In this paper, we propose a framework called HANet for building highly available network services. The main goal of HANet is to allow a server to continue providing services when all its network interfaces to the outside world (i.e., public interfaces) have failed. This is achieved by two techniques. First, a network interface can be backed up not only by other public network interfaces, but also by other inter-server I/O communication interfaces (i.e., private interfaces) such as Ethernet, USB, RS232, etc. Therefore, IP packets can still be transmitted and received via these I/O links, even when all of the public network interfaces have failed. Second, HANet allows a server to take over the packet transmission job of another network-failed server.The benefit of HANet is that a network-failed server will not lose any requests which are being processed. And, it is efficient since no synchronization overhead or replaying process is required. Moreover, it is totally transparent to server applications and clients. To demonstrate the feasibility of HANet, we implemented it in the Linux kernel. According to the performance results, using a private Fast Ethernet interface for data communication leads to only 1% overhead in user-perceived latency when the public Fast Ethernet interface of the server has failed. This indicates that HANet is efficient, and hence is feasible for commercial network services. 
78|3||Contents Volume 78|
79|1|http://www.sciencedirect.com/science/journal/01641212/79/1|Towards the automatic generation of mobile agents for distributed intrusion detection system|The Mobile Agent Intrusion Detection System (MAIDS) is an agent based distributed Intrusion Detection System (IDS). A disciplined requirement engineering process is developed to build MAIDS. The starting point is a high level description of intrusions expressed as Software Fault Trees (SFTs). Then the SFTs are translated to Colored Petri Nets (CPNs) that specify the IDS design. Subsequently, the CPNs are implemented as software intrusion detection agents in the MAIDS agent system. By using SFT and CPN as the theoretical underpinnings, the design and implementation of MAIDS can be verified and the design and implementation errors can be substantially reduced.This paper presents a tool that automatically translates CPNs that specify IDS design into software intrusion detection agents in MAIDS. Together with the translator we have developed to convert SFTs that model intrusions into the CPN for IDS design, this tool can automatically generate intrusion detection software agents from a high level description of intrusions. 
79|1||The knowledge management efficacy of matching information systems development methodologies with application characteristicsâan experimental study|An experimental study was conducted to determine whether appropriately matching methodology type to the characteristics of the business application being developed resulted in more effective knowledge-work processes among team members. Specifically, the experiment compared the use of hypermedia systems development methodologies to that of conventional software engineering methodologies in enabling knowledge work processes during the development of hypermedia-intensive business applications. Results obtained indicate that there is value in effectively matching methodologies to application domain. Based on this finding, there is justification in employing strong-typed methodologies for systems design projects particularly in the cases where the application domain is quite specialized. Our results also suggest that Information Technology (IT) studies that assess methodology influences on the resultant Information System (IS) artifact need to include knowledge and/or cognitive elements and their related theories. Since systems development is knowledge intensive, inclusion of cognitive and knowledge aspects provide a more complete model of how methodologies influence the various aspects of the IS artifact. 
79|1||REDUP: a packet loss recovery scheme for real-time audio streaming over wireless IP networks|Due to the characteristics of (1) smaller bandwidth and (2) unreliable transmission media, real-time audio streaming over wireless networks is not trivial. To have smooth audio streaming over wireless networks, we propose a scheme called REDUP in this paper. Two sending modes, in which redundant information is embedded in each packet, that the REDUP scheme contains are (1) the “redundant” mode and (2) the “duplicated” mode. Let a packet i can contain three audio frames i, i − 1, and i − 2. In the redundant mode, frame i uses a codec of better quality than that for frames i − 1 and i − 2. In the duplicated mode, frames i, i − 1, and i − 2 use the same codec, which has lower quality than that for frame i used in the redundant mode. The “redundant” mode may give better quality of sound but consumes more bandwidth, while the “duplicated” mode gives lower quality of sound but consumes less bandwidth. The two modes are selected depending on the networking situation. Round trip time (RTT) between the mobile host and the mobile gateway is used to determine the networking situation. Two thresholds named “upper-ratio” and “lower-ratio” are set. When the average RTT exceeds the upper-ratio multiplies the maximum RTT, the networking situation is set to congested; when the average RTT is under the lower-ratio multiplies the maximum RTT, the networking situation is set to unloaded. The transmission mode is switched from “redundant” to “duplicated” when the networking situation is determined to be congested. The transmission mode is switched from “duplicated” to “redundant” when the networking situation is determined to be unloaded. In this way, the redundant information of packets is adapted to the networking situation and the bandwidth can be utilized more effectively. 
79|1||FMF: Query adaptive melody retrieval system|Recent progress of computer and network technologies makes it possible to store and retrieve a large volume of multimedia data in many applications. In such applications, efficient indexing scheme is very important for multimedia retrieval. Depending on the media type, multimedia data shows distinct characteristics and requires different approach to handle. In this paper, we propose a fast melody finder (FMF) that can retrieve melodies fast from audio database based on frequently queried tunes. Those tunes are collected from user queries and incrementally updated into index. Considering empirical user request pattern for multimedia data, those tunes will cover significant portion of user requests. FMF represents all the acoustic and common music notational inputs using a well-known string format such as UDR and LSR and uses string matching techniques to find query results. We implemented a prototype system and report on its performance through various experiments. 
79|1||An approach to feature location in distributed systems|This paper describes an approach to the feature location problem for distributed systems, that is, to the problem of locating which code components are important in providing a particular feature for an end user. A feature is located by observing system execution and noting time intervals in which it is active. Traces of execution in intervals with and without the feature are compared. Earlier experience has shown that this analysis is difficult because distributed systems often exhibit stochastic behavior and because time intervals are hard to identify with precision.To get around these difficulties, the paper proposes a definition of time interval based on the causality analysis introduced by Lamport and others. A strict causal interval may be defined, but it must often be extended to capture latent events and to represent the inherent imprecision in time measurement. This extension is modeled using a weighting function which may be customized to the specific circumstances of each study.The end result of the analysis is a component relevance index, denoted pc, which can be used to measure the relevance of a software component to a particular feature. Software engineers may focus their analysis efforts on the top components as ranked according to pc.Two case studies are presented. The first study demonstrates the feasibility of pc by applying our method to a well-defined distributed system. The second study demonstrates the versatility of pc by applying our method to message logs obtained from a large military system. Both studies indicate that the suggested approach could be an effective guide for a software engineer who is maintaining or enhancing a distributed system. 
79|1||Designing state-based systems with entity-life modeling|This article introduces the entity-life modeling (ELM) design approach for multithread software. The article focuses on problems that can be described by state machines with associated activities. We compare ELM with a traditional design approach based on a dataflow model. Using a cruise controller for a car as an example, we show that entity-life modeling is a more direct and less ceremonious approach that produces a much simpler architecture. 
79|1||Mining association rules with multi-dimensional constraints|To improve the effectiveness and efficiency of mining tasks, constraint-based mining enables users to concentrate on mining their interested association rules instead of the complete set of association rules. Previously proposed methods are mainly contributed to handling a single constraint and only consider the constraints which are characterized by a single attribute value. In this paper, we propose an approach to mine association rules with multiple constraints constructed by multi-dimensional attribute values. Our proposed approach basically consists of three phases. First, we collect the frequent items and prune infrequent items according to the Apriori property. Second, we exploit the properties of the given constraints to prune search space or save constraint checking in the conditional databases. Third, for each itemset possible to satisfy the constraint, we generate its conditional database and perform the three phases in the conditional database recursively. Our proposed algorithms can exploit the properties of constraints to prune search space or save constraint checking. Therefore, our proposed algorithm is more efficient than the revised FP-growth and FIC algorithms. 
79|1||Efficient index caching for data dissemination in mobile computing environments|Due to the limited power supply of mobile devices, much research has been done on reducing the power consumption of mobile devices in mobile computing environments. Since supporting indices on the broadcast data items can effectively reduce the power consumption of the mobile clients, most of the existing research on data broadcasting has focused on designing efficient indexing schemes. In this paper, we propose to cache indices on the mobile clients and to use cached indices to facilitate the access of the broadcast data items. To manage the cache usage of the mobile clients, we propose two cache management policies. The lower-level-index-first policy caches the index nodes that are at the lower level of the index tree, while the cut-plane-first policy caches the index nodes that belong to a cut-plane of the index tree. Through experiments, we compare the performance of the two proposed policies with some existing policies in terms of tuning time and access time. The experiments show that index caching significantly reduces the tuning time of a mobile client without increasing its access time. In terms of tuning time, the experiments show that, when the access pattern of a mobile client is not skew, the cut-plane-first policy outperforms the lower-level-index first policy, LRU and LRFU. On the contrary, when the mobile client has a small cache and its access pattern is skew, the lower-level-index-first policy outperforms the cut-plane-first policy, LRU and LRFU. In terms of access time, the lower-level-index first policy outperforms the cut-plane-first policy, LRU and LRFU. 
79|1||A practical framework for eliciting and modeling system dependability requirements: Experience from the NASA high dependability computing project|The dependability of a system is contextually subjective and reflects the particular stakeholder’s needs. In different circumstances, the focus will be on different system properties, e.g., availability, real-time response, ability to avoid catastrophic failures, and prevention of deliberate intrusions, as well as different levels of adherence to such properties. Close involvement from stakeholders is thus crucial during the elicitation and definition of dependability requirements. In this paper, we suggest a practical framework for eliciting and modeling dependability requirements devised to support and improve stakeholders’ participation. The framework is designed around a basic modeling language that analysts and stakeholders can adopt as a common tool for discussing dependability, and adapt for precise (possibly measurable) requirements. An air traffic control system, adopted as testbed within the NASA High Dependability Computing Project, is used as a case study. 
79|1||A method for defining IEEE Std 1471 viewpoints|With the growing impact of information technology the proper understanding of IT-architecture designs is becoming ever more important. Much debate has been going on about how to describe them. In 2000, the IEEE Std 1471 proposed a model of an architecture description and its context.In this paper we propose a lightweight method for modeling architectural information after (part of) the conceptual model of IEEE Std 1471 and defining IEEE Std 1471 viewpoints. The method gives support by outlining in textual form and in diagram form the relation of the concerns of the stakeholders to the architectural information. The definition of viewpoints can then be done with insight from these relations. The method has four steps: (1) creating stakeholder profiles, (2) summarizing internal design documentation, (3) relating the summary to the concerns of the stakeholders, and (4) defining viewpoints.We have conducted a round of discussion and testing in practice in various settings. In this paper we present the feedback we received and propose improvements. 
79|1||Architecture-based software reliability modeling|Reliability is one of the essential quality requirements of software systems, especially for life critical ones. Software reliability modeling provides a means for estimating reliability of software, which facilitates effective decision making for quality control. Existing models either use a black-box approach that is based on test data of the whole system or a white-box approach that uses components’ reliabilities and inter-component interactions. While the black-box approach is applicable to the late phases of software development, the white-box approach can support decisions on early component reuse and later component upgrades and updates. However, the white-box based models have traditionally considered only homogeneous software behaviors. For this reason, we extended the white-box to an architecture-based approach, utilizing the characteristics of architectural styles to capture design information and to realize non-uniform heterogeneous software behaviors. Adhered to the behaviors, a state machine of a discrete-time Markov model is constructed and employed to compute software reliability. Our approach allows reliability to be modeled at architectural design stage to decide components for reuse as well as later testing and maintenance phases for continuous software evolution. In contrast to the black-box approach, the model only needs to retest the influenced portions for a behavioral or structural change, instead of the complete system. This model yields a compelling result after being applied to an industrial real-time component-based financial system. We believe that this architecture-based modeling technique can have a great potential for use to effectively improve software quality. 
79|10|http://www.sciencedirect.com/science/journal/01641212/79/10|Architecting dependable systems|
79|10||Using temporal logic to specify adaptive program semantics|Computer software must dynamically adapt to changing conditions. In order to fully realize the benefit of dynamic adaptation, it must be performed correctly. The correctness of adaptation cannot be properly addressed without precisely specifying the requirements for adaptation. This paper introduces an approach to formally specifying adaptation requirements in temporal logic. We introduce A-LTL, an adaptation-based extension to linear temporal logic, and use this logic to specify three commonly used adaptation semantics. Composition techniques are developed and applied to A-LTL to construct the specification of an adaptive program. We introduce adaptation semantics graphs to visually represent the adaptation semantics, which can also be used to automatically generate specification for adaptive programs. 
79|10||An architectural pattern for non-functional dependability requirements|We address the research question of transforming dependability requirements into corresponding software architecture constructs, by proposing first that dependability needs can be classified into three types of requirements and second, an architectural pattern that allows requirements engineers and architects to map the three types of dependability requirements into three corresponding types of architectural components. The proposed pattern is general enough to work with existing requirements techniques and existing software architectural styles, including enterprise and product-line architectures. 
79|10||Software architecture-based regression testing|Software architectures are becoming centric to the development of quality software systems, being the first concrete model of the software system and the base to guide the implementation of software systems. When architecting dependable systems, in addition to improving system dependability by means of construction (fault-tolerant and redundant mechanisms, for instance), it is also important to evaluate, and thereby confirm, system dependability. There are many different approaches for evaluating system dependability, and testing has been always an important one, being fault removal one of the means to achieve dependable systems.Previous work on software architecture-based testing has shown it is possible to apply conformance testing techniques to yield some confidence on the implemented system conformance to expected, architecture-level, behaviors.This work explores how regression testing can be systematically applied at the software architecture level in order to reduce the cost of retesting modified systems, and also to assess the regression testability of the evolved system. We consider assessing both “low-level” and “high-level” evolution, i.e., whether a slightly modified implementation conforms to the initial architecture, and whether the implementation continues to conform to an evolved architecture. A better understanding on how regression testing can be applied at the software architecture level will help us to assess and identify architecture with higher dependability. 
79|10||Specification of exception flow in software architectures|In recent years, various approaches combining software architectures and exception handling have been proposed for increasing the dependability of software systems. This conforms with the idea supported by some authors that addressing exception handling-related issues since early phases of software development may improve the overall dependability of a system. By systematically designing the mechanism responsible for rendering a system reliable, developers increase the probability of the system being able to avoid catastrophic failures at runtime. This paper addresses the problem of describing how exceptions flow amongst architectural elements. In critical systems where rollback-based mechanisms might not be available, such as systems that interact with mechanical devices, exception handling is an important means for recovering from errors in a forward-based manner. We present a framework, named Aereal, that supports the description and analysis of exceptions that flow between architectural components. Since different architectural styles have different policies for exception flow, Aereal makes it possible to specify rules on how exceptions flow in a given style and to check for violations of these rules. We use a financial application and a control system as case studies to validate the proposed approach. 
79|10||Comparative evaluation of dependability characteristics for peer-to-peer architectural styles by simulation|An important concern for the successful deployment of a dependable system is its quality of service (QoS), which is significantly influenced by its architectural style. We propose the comparative evaluation of architectural styles by simulation. Our approach integrates architectural styles and concrete architectures to enable early design-space exploration in order to predict the QoS of peer-to-peer systems. We illustrate the approach via two case studies where availability of resources and performance of peer-to-peer search methods are evaluated. Based on our experience with these simulation environments, we sketch tool support for simulating architectural changes at runtime. 
79|10||Verification of rectangular hybrid automata models|Formal analysis of hybrid systems is concerned with verifying whether a hybrid system satisfies a desired specification, like avoiding an unsafe region of the state space. For safety, economical and performance reasons, the satisfaction of critical design specifications of hybrid systems must be verified before embarking into their expensive construction and operation. Verification approaches based on system modelling with rectangular hybrid automata can lead to decidable solutions that find an algorithm which searches for the existence of states in the model which can be associated with the design specifications. This search is usually computation time intensive and after the elapse of a significant portion of time the user is unable to judge whether the state of interest is not reachable or the algorithm requires more time to terminate. In this work an algorithm, which alleviates the drawback of state space search, is proposed. Analytic approximations are used to express the time period the automaton stays at each one of its vertices in terms of the number of automaton iterations required to reach a desired state. Solving these relationships, one can find out whether the state of the model is reachable after a finite number of iterations. A case study is also presented that demonstrates the application of the algorithm to the verification of the operating specifications of a chemical reaction process that is controlled by a computer. 
79|10||Automatic code generation from high-level Petri-Nets for model driven systems engineering|One of the main concepts of the model driven architecture framework proposed by the OMG is the transformation of platform independent models into either platform dependent ones or into implementations. The latter needs generators which automatically translate models into code of a chosen target programming language. Nowadays most models in systems engineering are created with the UML as standardized set of notations. However, the UML is still largely undefined from a semantical point of view. Automatic code generation from these models is thus only possible if further semantic definitions are provided. Since Petri-Nets are frequently utilized for these purposes, the automatic generation of code from Petri-Nets is an important topic.Starting from this observation, this article gives an overview of different strategies to generate code from high-level Petri-Nets. One of these strategies is due to its characteristics investigated in more detail. Several improvements of the basic idea are introduced and also the impact of these enhancements in terms of efficient execution of generated code is illustrated with a benchmark. 
79|10||Structural translation from Time Petri Nets to Timed Automata|In this paper, we consider Time Petri Nets (TPN) where time is associated with transitions. We give a formal semantics for TPNs in terms of Timed Transition Systems. Then, we propose a translation from TPNs to Timed Automata (TA) that preserves the behavioral semantics (timed bisimilarity) of the TPNs. For the theory of TPNs this result is twofold: (i) reachability problems and more generally TCTL model-checking are decidable for bounded TPNs; (ii) allowing strict time constraints on transitions for TPNs preserves the results described in (i). The practical applications of the translation are: (i) one can specify a system using both TPNs and Timed Automata and a precise semantics is given to the composition; (ii) one can use existing tools for analyzing timed automata (like Kronos, Uppaal or Cmc) to analyze TPNs. In this paper we describe the new feature of the tool Romeo that implements our translation of TPNs in the Uppaal input format. We also report on experiments carried out on various examples and compare the result of our method to state-of-the-art tool for analyzing TPNs. 
79|10||Semantic component networking: Toward the synergy of static reuse and dynamic clustering of resources in the knowledge grid|Model is a kind of codified knowledge that has been verified in solving problems. Solving a complex problem usually needs a set of models. Using components, the composition of a set of closely related models, could enhance the efficiency of solving problems as components are experience in form of knowledge network. Mappings between components vary with the relevancy of problems. Such mappings can help retrieve appropriate components when solving new problems. This paper investigates the relationship between components to identify these mappings by a set of semantic links and develops relevant rules to form a mechanism for semantically networking and using components. Flexible component reuse can be realized by semantic retrieval and rule reasoning. Extending the notion of component to a general service mechanism, this paper further investigates the organization, reuse and clustering of components in form of Web services and research spaces. The semantic component networking overlay supports intelligent e-Science Knowledge Grid applications by the synergy of static reuse and dynamic clustering of various research spaces and resources on-demand. 
79|11|http://www.sciencedirect.com/science/journal/01641212/79/11|Introduction to the special section on software cybernetics|
79|11||A control-theoretic approach to the management of the software system test phase|A quantitative, adaptive process control technique is described using an industrially validated model of the software system test phase (STP) as the concrete target to be controlled. The technique combines the use of parameter correction and Model Predictive Control to overcome the problems induced by modeling errors, parameter estimation errors, and limits on the resources available for productivity improvement.We present an example of the technique applied to data from the execution of the STP of a commercial software development effort at a large software manufacturer. The example shows that the control technique successfully achieves the schedule and quality objectives despite uncertainty in the estimation of the model parameters. 
79|11||Requirement process establishment and improvement from the viewpoint of cybernetics|As a branch of engineering cybernetics, automatic control theory has been extensively applied to improve products, increase productivity and rationalize management. This paper adapts the principles of automatic control theory to the field of software process improvement. In particular, the work described uses control theory to define a requirement engineering (RE) process control system, its dynamic and steady-state performance, and the steps in designing, analyzing and improving such a system. The work has highlighted the need for process activities relating to measuring elements, including those in feedback compensation and organizational support. The results of this research can be used to guide the establishment and improvement of RE processes, compare different requirement process solutions quantitatively, develop methods for evaluating benefits from process improvements, and structure the application of knowledge about RE. 
79|11||Developing adaptive systems with synchronized architectures|In this paper we show how to specify and refine self-adapting systems, by employing the state-based formalism called action systems. Assuming a fixed set of possible configurations, we use a recently-introduced synchronization operator to achieve separation of concerns between adaptation policy, fault tolerance and steady-state system execution. Using action systems allows us to apply standard refinement techniques, aimed for correct implementations of higher-level models. We illustrate this idea by applying it to the problem of coping with dynamically-varying user requirements and possible fault situations. 
79|11||Monitoring techniques for an online neuro-adaptive controller|The appeal of biologically inspired soft computing systems such as neural networks in complex systems comes from their ability to cope with a changing environment. Unfortunately, adaptability induces uncertainty that limits the applicability of static analysis to such systems. This is particularly true for systems with multiple adaptive components or systems with multiple types of learning operation.This work builds a paradigm of dynamic analysis for a neuro-adaptive controller where different types of learning are to be employed for its online neural networks. We use support vector data description as the novelty detector to detect unforeseen patterns that may cause abrupt system functionality changes. It differentiates transients from failures based on the duration and degree of novelties. Further, for incremental learning, we utilize Lyapunov functions to assess real-time performance of the online neural networks. For quasi-online learning, we define a confidence measure, the validity index, to be associated with each network output. Our study on the NASA F-15 Intelligent Flight Control System demonstrates that our novelty detection tool effectively filters out transients and detects failures; and our light-weight monitoring techniques supply sufficient evidence for an insightful validation. 
79|11||Software project managers and project success: An exploratory study|Traditionally, a project should deliver agreed upon functionality on time and within budget. This study examines the mindset of software development project managers in regard to how they ‘define’ a successful project in order to arrive at a richer perspective of ‘success’ from the perspective of project managers. Senior management and members of the development team can gain a better understanding of the perspective of project managers regarding some of the aspects of their work and the project as a whole. Such understanding can facilitate better communication and cooperation among these groups. We investigated components of the developed system (project) in terms of some of the aspects of the delivered system (outcome/project) in order to place traditional measures of success in context with other organizational/managerial measures that have been suggested in the literature. We also investigated specific work-related items. These items have potential implications for the intrinsic motivation of the project manager. The consensus among the project managers who participated in our study indicated that delivering a system that meets customer/user requirements and works as intended through work that provides a sense quality and personal achievement are important aspects that lead to a project being considered a success. 
79|11||An intelligent early warning system for software quality improvement and project management|There are three major problems with software projects: over budget, behind schedule, and poor quality. It is often too late to correct these problems by the time they are detected in failed software projects. In this paper, we discuss design, implementation, and evaluation of an experimental intelligent software early warning system based on fuzzy logic using an integrated set of software metrics. It consists of the following components: software metrics database, risk knowledge base, intelligent risk assessment, and risk tracing. It helps to assess risks associated with the three problems from perspectives of product, process, and organization in the early phases of the software development process. It is capable of aggregating various partial risk assessment results into an overall risk indicator even if they may be conflicting. In addition, it can be used to analyze a risk by identifying its root causes through its risk tracing utility. 
79|11||A combined specification language and development framework for agent-based application engineering|Software agents are used to solve several complex problems. Software engineering processes and tools are vital to support all the development phases of agent-based systems, and guarantee that these systems are built properly. Indeed, many modeling languages and implementation frameworks are available for system engineers. However, there are few methods that properly combine agent-oriented design and implementation initiatives. In this paper, we propose a development method that goes from the requirements elicitation to the actual implementation of agent systems using a modeling language and an implementation framework. We also present a case study to illustrate the suitability of our approach. 
79|11||Software reliability growth model with change-point and environmental function|This paper presents a SRGM (software reliability growth model) with both change-point and environmental function based on NHPP (non-homogeneous Poisson process). Although a few research projects have been devoted to the change-point problems of SRGMs, consideration of the variation of environmental factors in the existing models during testing time is limited. The proposed model is one of a few NHPP models, which takes environmental factors as a function of testing time. FDR (fault detection rate) is usually used to measure the effectiveness of fault detection by test techniques and test cases. A FDR function after the change-point of the testing is proposed, which is computed from both environmental factors and FDR before the change-point of the testing. A NHPP SRGM with both change-point and environmental function called CE-SRGM is built which integrates the FDR before the change-point of the testing and the proposed FDR function after the change-point of the testing. CE-SRGM is evaluated using two sets of software failure data. The experimental results show that the predictive power of CE-SRGM is better than those of other SRGMs. 
79|11||Identifying use cases in source code|Understanding the behavior of a software system is an important problem in software maintenance. As use cases have been accepted as an effective means for describing behavioral requirements for a software system, it should be helpful for maintainers to acquire the use case model from source code. In this paper, we propose a novel approach to identifying use cases in source code. The central idea of our approach is based on the observation that branch statements are a primary mechanism to separate one use case from another in source code. Following this idea, we design a static representation of software systems through incorporating branch information into the traditional call graph, which is named the Branch-Reserving Call Graph (BRCG). To effectively use this representation for use case identification, branches that do not serve as the separations of use cases should be pruned off in the BRCG. In this paper, we also provide a metric-based heuristic to automate this pruning. From the pruned BRCG, use cases, which may just undergo some modifications from human experts, can be generated through traversing the pruned BRCG. Overall, our method can effectively reduce the intensive human involvement in use case identification. We have also performed two case studies for this method on two shareware systems. The results from the case studies can confirm the effectiveness of our approach. 
79|11||Shortcut method for reliability comparisons in RAID|Given that the reliability of each disk in a disk array during its useful lifetime is given as r = 1 − Ïµ with Ïµ âª 1, we show that the reliability of a RAID disk array tolerating all possible n − 1 disk failures can be specified as R ≈ 1 − anÏµn, where an is the smallest nonzero coefficient in the corresponding asymptotic expansion, e.g., for n-way replication R = 1 − Ïµn. We compare the reliability of several mirrored disk organizations, which provide tradeoffs between reliability and load balancedness (after disk failure) by comparing their a2 values, which can be obtained via a partial reliability analysis taking into account a few disk failures. We next use asymptotic expansions to compare the reliability of hierarchical RAID disk arrays, which combine replication and rotated parity disk arrays (RAID5 and RAID6). Finally, we argue that the mean time to data loss in systems with repair is related to the reliability without repair. As part of this discussion we show how to estimate the mean time to data loss in RAID5 and RAID6 disk arrays without resorting to transient analysis. 
79|11||Packaging experiences for improving testing technique selection|One of the major problems within the software testing area is how to get a suitable set of cases to test a software system. A good set of test cases should assure maximum effectiveness with as few cases as possible. There are now numerous testing techniques available for generating test cases. However, many are never used, while just a few are used over and over again. Testers use little (if any) information about the available techniques, their usefulness and, generally, how suited they are to the project at hand, upon which to base their decision on which testing techniques to use. Using a characterisation schema is one solution for improving testing techniques selection. The schema helps to choose the best-suited techniques for a given project based on relevant information for the purpose of selection, assuring that testers’ selections are systematic. However, a characterisation schema is only part of the solution. We have found that a critical aspect for making a good selection is the availability of the necessary information and the sources of information that have to be consulted to access this information. Any organisation wishing to use characterisation schemas to select SE techniques needs to first address the issue of packaging the information that the schema contains. 
79|11||A semantic-based P2P resource organization model R-Chord|This paper proposes a semantic-based P2P resource organization model R-Chord by incorporating the Resource Space Model (RSM), the P2P Semantic Link Network Model (P2PSLN) and the DHT Chord protocol. Peers provide services with each other according to the content of their resources and the related configuration information. It incorporates the classification semantics and the relational semantics to provide users and applications with a uniform view on distributed resources. Experiments show that, compared to the Chord approach, the R-Chord approach is more flexible to support semantic-based queries and can significantly decrease the average visiting number of and visiting times on peers for answering queries. 
79|11||Automated software clustering: An insight using cluster labels|Clustering techniques have shown promising results for the architecture recovery and re-modularization of legacy software systems. Clusters that are obtained as a result of the clustering process may not be easy to interpret until they are assigned appropriate labels. Automatic labeling of clusters reduces the time required to understand them and can also be used to evaluate the effectiveness of the clustering process, if the assigned labels are meaningful and convey the purpose of each cluster effectively. In this paper, we present a labeling scheme based on identifiers of an entity. As the clustering process proceeds, keywords within identifiers are ranked using two ranking schemes: frequency and inverse frequency. We present experimental results to demonstrate the effectiveness of our labeling approach. A comparison between the ranking schemes reveals the inverse frequency scheme to form more meaningful labels, especially for small clusters. A comparison of clustering results of the complete and weighted combined algorithms based on labels of the clusters produced by them during clustering shows that the latter produces a more understandable cluster hierarchy with easily identifiable software sub-systems. 
79|11||Priority assessment of software process requirements from multiple perspectives|The improvement of software processes involves the collection of software process requirements from various groups of stakeholders – each having their own perception of the process. Often stakeholders specify similar requirements in different ways. This results in the fact that different perceptions are related to each other. An effective technique is needed to incorporate the relationships between various stakeholders’ process requirements. This paper presents a Correlation-Based Priority Assessment framework (CBPA), which prioritizes software process requirements gathered from multiple stakeholders by incorporating inter-perspective relationships of requirements. This research uses a relationship matrix to analyze the impacts between requirements and to facilitate the integration. Higher stakeholder satisfaction is achieved by increasing the priorities of the software process requirements whose satisfaction can increase the satisfaction of other requirements from multiple perspectives. 
79|12|http://www.sciencedirect.com/science/journal/01641212/79/12|Distributed dynamic slicing of Java programs|We propose a novel dynamic slicing technique for distributed Java programs. We first construct the intermediate representation of a distributed Java program in the form of a set of Distributed Program Dependence Graphs (DPDG). We mark and unmark the edges of the DPDG appropriately as and when dependencies arise and cease during run-time. Our algorithm can run parallely on a network of computers, with each node in the network contributing to the dynamic slice in a fully distributed fashion. Our approach does not require any trace files to be maintained. Another advantage of our approach is that a slice is available even before a request for a slice is made. This appreciably reduces the response time of slicing commands. We have implemented the algorithm in a distributed environment. The results obtained from our experiments show promise. 
79|12||Generalized comparison of graph-based ranking algorithms for publications and authors|Citation analysis helps in evaluating the impact of scientific collections (journals and conferences), publications and scholar authors. In this paper we examine known algorithms that are currently used for Link Analysis Ranking, and present their weaknesses over specific examples. We also introduce new alternative methods specifically designed for citation graphs. We use the SCEAS system as a base platform to introduce these new methods and perform a generalized comparison of all methods. We also introduce an aggregate function for the generation of author ranking based on publication ranking. Finally, we try to evaluate the rank results based on the prizes of ‘VLDB 10 Year Award’, ‘SIGMOD Test of Time Award’ and ‘SIGMOD E.F.Codd Innovations Award’. 
79|12||Modelling and simulation of off-chip communication architectures for high-speed packet processors|In this work, we propose a visual, custom-designed, event-driven interconnect simulation framework to evaluate the performance of off-chip multi-processor/memory communications architectures for line cards. The simulator uses the state-of-the-art software design techniques to provide the user with a flexible, robust and comprehensive tool that can evaluate k-ary n-cube based network topologies under non-uniform traffic patterns. The simulator provides full control over essential network parameters and flow control mechanisms such as virtual channels and sub-channeling. We compare three low-dimensional k-ary n-cube based interconnects that can fit into the physical limitations on line cards, where each one of these interconnects has multiple processor-memory configurations. Performance results show that k-ary n-cube architectures perform better than existing interconnects, and they can sustain current line rates and higher. In addition, we provide performance tradeoffs between multiple flow control mechanisms and performance metrics such as throughput, routing accuracy, failure rate and interconnect utilization. 
79|12||Semantic prefetching objects of slower web site pages|The structure of most web sites consists of a composition of web pages that require varying amounts of time to render. Typically, web pages with large amount content (text/images/code) require more time to render than web pages with a smaller aggregate content size. Most users expect this discrepancy among web page rendering times. In this paper, we will describe a semantic link prefetching technique that uses object bundling to expedite the rendering time of slower web pages, at the cost of extending the rendering speed of faster web pages within an established threshold value limit. Study results show that our approach enhances the overall rendering time of slower web pages with imperceptible time extension to other, faster rendering web pages. 
79|12||Component adaptation for event-based application integration using active rules|Enterprise applications integrate information and applications in a distributed, networked environment to develop a global application of interconnected components and applications. This paper presents the design and implementation of the wrappers generated to adapt components to a middleware environment that employs active rules to build an enterprise application using an event-driven approach to integrating distributed black-box components. The components adapted in this implementation are based on the Enterprise JavaBeans component model. An application integrator declares the components to be integrated, and may define additional attributes, relationships, and events to be used in the integration process. The metadata for the components to be integrated is used to adapt and enhance the black-box components through the use of wrappers. The wrappers act as a proxy to the black-box component. The adaptation of the wrappers provides for the storage of additional properties for component instances and for the detection of events within the components to generate event notifications. The wrappers are automatically generated from the component metadata using the Extensible Stylesheet Language. The design and implementation of the wrappers establishes a framework that supports an active approach to component-based software integration. 
79|12||Computing frequent itemsets in parallel using partial support trees|A key process in association rules mining, which has attracted a lot of interest during the last decade, is the discovery of frequent sets of items in a database of transactions. A number of sequential algorithms have been proposed that accomplish this task. On the other hand, only few parallel algorithms have appeared in the literature. In this paper, we study the parallelization of the partial-support-tree approach Goulbourne et al. (2000). Numerical results show that this method is generally competitive, while it is particularly adequate for certain types of datasets. 
79|12||A faster exact schedulability analysis for fixed-priority scheduling|Real-time scheduling for task sets has been studied, and the corresponding schedulability analysis has been developed. Due to the considerable overheads required to precisely analyze the schedulability of a task set (referred to as exact schedulability analysis), the trade-off between precision and efficiency is widely studied. Many efficient but imprecise (i.e., sufficient but not necessary) analyses are discussed in the literature. However, how to precisely and efficiently analyze the schedulability of task sets remains an important issue. The Audsley’s Algorithm was shown to be effective in exact schedulability analysis for task sets under rate-monotonic scheduling (one of the optimal fixed-priority scheduling algorithms). This paper focuses on reducing the runtime overhead of the Audsley’s Algorithm. By properly partitioning a task set into two subsets and differently treating these two subsets during each iteration, the number of iterations required for analyzing the schedulability of the task set can be significantly reduced. The capability of the proposed algorithm was evaluated and compared to related works, which revealed up to a 55.5% saving in the runtime overhead for the Audsley’s Algorithm when the system was under a heavy load. 
79|12||A difference expansion oriented data hiding scheme for restoring the original host images|This paper proposes a lossless data embedding scheme that exploits the difference expansion of the pixels to conceal large amount of message data in a digital image. The proposed scheme takes into consideration the correlation between the pixel and its surrounding pixels to determine the degree of the difference expansion for message data embedding. The performance has been evaluated in terms of image distortion, payload capacity, as well as embedding rate. The experimental results show that the scheme is capable of providing a great payload capacity, and the image quality of the embedded image is better than that of Tian’s and Celik’s schemes for a gray-level image. What is more, for a color image, the proposed scheme outperforms Alattar’s scheme at low PSNR. In addition, the proposed scheme can completely restore the original image after data extraction. 
79|12||An efficient subscription routing algorithm for scalable XML-based publish/subscribe systems|XML-based publish/subscribe systems have gained popularity as one of the newly emerging communication paradigms. As the number of clients has increased, the concept of the scalability has become the most important factor in the distributed information processing. In this paper, we propose an efficient subscription routing algorithm as a part of efforts to achieve scalability, with subscriptions especially expressed in XPath patterns. We first discuss that the homomorphism relationship among XPath patterns can be utilized for routing XPath patterns. Then, we develop a lattice data structure called the partially ordered set of XPath patterns which is abbreviated as POX and its corresponding algorithm for efficiently maintaining homomorphism relationships. Finally, we present performance evaluation results which validate our algorithm with respect to system performance and scalability. 
79|12||Using playing cards to estimate interference in frequency-hopping spread spectrum radio networks|Many mobile ad hoc networks, wireless LANs, cellular systems, and military radio networks use frequency-hopping spread spectrum (FHSS) to provide multiple access to the channel as well as for anti-jamming, interference suppression, and mitigating other channel effects. Estimates of interference between two networks using the same set of frequencies is often assumed to be 1/N where N is the total number of available hopping frequencies. However, this simple model does not incorporate the partial spectral or partial temporal overlap that may occur between the hopping sequences or account for the fact that this overlap may be “stable” which will result in a significantly larger (or smaller) probability of interference.We extend an almost 300 year old probability model developed by the French mathematician Pierre Rémond de Montmort to provide a more accurate estimate of interference between such FHSS networks. This improved playing card model, almost as easy to calculate as the commonly used 1/N model, is more robust because it incorporates the effects due to partial spectral and temporal overlap. Furthermore, since the hopping sequence in each network is independent, our model can easily be used to determine the interference from multiple networks by simply applying it pairwise and combining the results using elementary probability theory. 
79|12||Erratum to âOn the security of the Yen-Guoâs domino signal encryption algorithm (DSEA)â [The Journal of Systems and Software 79 (2006) 253â258]|
79|12||Understanding the past, improving the present, and mapping out the future of software architecture|
79|12||A survey of architecture design rationale|Many claims have been made about the consequences of not documenting design rationale. The general perception is that designers and architects usually do not fully understand the critical role of systematic use and capture of design rationale. However, there is to date little empirical evidence available on what design rationale mean to practitioners, how valuable they consider it, and how they use and document it during the design process. This paper reports a survey of practitioners to probe their perception of the value of design rationale and how they use and document the background knowledge related to their design decisions. Based on 81 valid responses, this study has discovered that practitioners recognize the importance of documenting design rationale and frequently use them to reason about their design choices. However, they have indicated barriers to the use and documentation of design rationale. Based on the findings, we conclude that further research is needed to develop methodology and tool support for design rationale capture and usage. Furthermore, we put forward some specific research questions about design rationale that could be further investigated to benefit industry practice. 
79|2|http://www.sciencedirect.com/science/journal/01641212/79/2|An efficient and secure protocol for sensor network time synchronization|The emerging field of wireless sensor networks offers countless possibilities for achieving large scale monitoring in a distributed environment. These networks of resource constrained nodes require time synchronization for various distributed operations, but traditional protocols have significant overhead and rapidly deplete battery power. This paper addresses the challenges of sensor network engineering by proposing an efficient and secure time synchronization protocol named Tempest. The protocol reduces overhead and conserves power by using passive participation. It allows a node to infer the canonical time by simply overhearing the communication of its neighbors. It also authenticates protocol messages, and uses cross-layer control to manipulate counters in an encryption module to prevent attacks. Its implementation uses only minimal processing and negligible state, while an emphasis on reuse and modularity reduces code size. The protocol is implemented on embedded sensor node hardware, and is shown to substantially reduce overhead while maintaining the synchronization accuracy of recent related work. This reduction in overhead saves valuable energy, extending the lifetime of each sensor node and the lifetime of the sensor network itself. 
79|2||Hotswapping Linux kernel modules|
79|2||Analysis of Sun et al.âs linkability attack on some proxy blind signature schemes|The proxy blind signature scheme allows the designated proxy signer using the proxy secret key to generate a blind signature on behalf of the original signer. Tan et al. presented the DLP-based and ECDLP based blind signature schemes. Awasthi and Lal proposed a improved DLP-based scheme later. Recently, Sun et al. presented linkability attack on Tan et al.’s and Awasthi–Lal’s proxy blind signature schemes respectively. In this paper, we show that Sun et al.’s attack is failed and these schemes are still satisfy the unlinkability property. 
79|2||Efficient evaluation of linear path expressions on large-scale heterogeneous XML documents using information retrieval techniques|We propose XIR-Linear, a method for efficiently evaluating linear path expressions (LPEs) on large-scale heterogeneous XML documents using information retrieval (IR) techniques. LPEs are the primary form of XPath queries, and their evaluation techniques have been researched actively. XPath queries in their general form are partial match queries, and these queries are particularly useful for searching documents of heterogeneous schemas. Thus, XIR-Linear is geared for partial match queries expressed as LPEs. XIR-Linear has its basis on existing methods using relational tables (e.g., XRel, XParent), and drastically improves their efficiency using the inverted index technique. Specifically, it indexes the labels in label paths (i.e., sequences of node labels) like keywords in texts, and finds the label paths matching the LPE far more efficiently than string match used in the existing methods. We demonstrate the efficiency of XIR-Linear by comparing it with XRel and XParent using XML documents crawled from the Internet. The results show that XIR-Linear outperforms XRel and XParent by an order of magnitude with the performance gap widening as database size grows. 
79|2||Shape-based retrieval in time-series databases|The shape-based retrieval is defined as the operation that searches for the (sub)sequences whose shapes are similar to that of a query sequence regardless of their actual element values. In this paper, we propose a similarity model suitable for shape-based retrieval and present an indexing method for supporting the similarity model. The proposed similarity model enables to retrieve similar shapes accurately by providing the combination of multiple shape-preserving transformations such as normalization, moving average, and time warping. Our indexing method stores every distinct subsequence concisely into the disk-based suffix tree for efficient and adaptive query processing. We allow the user to dynamically choose a similarity model suitable for a given application. More specifically, we allow the user to determine the parameter p of the distance function Lp when submitting a query. The result of extensive experiments revealed that our approach not only successfully finds the subsequences whose shapes are similar to a query shape but also significantly outperforms the sequential scan method. 
79|2||Odyssey-Search: A multi-agent system for component information search and retrieval|Component Based Development aims at constructing software through the integration of components by using interfaces and contracts among them. However, these components should be bound to a specific application domain in order to be effectively reused. Domain Engineering and Component Based Engineering are adequate techniques to develop components related to specific domains. A solution for accessing domain information, including components, is to use a software layer that integrates different component information sources. This paper presents OSE, a search and retrieval system that provides heterogeneous/distributed access and storage to domain component information. Ideas drawn from the field of agents, user modeling, hypermedia, and mediation were combined to develop the OSE system. An evolutionary model of the user interests, ontologies and a base of rules are some of the underlying concepts of the system that help users to identify relevant domain information. Hence, the main contribution of OSE is to provide a new approach for accessing software components. Our innovative aspect is to provide flexibility, transparency and accuracy in software component retrieval, by using a multi-agent system approach. A case study conducted in the legislative domain has evidenced the advantages of our system. 
79|2||Shortening retrieval sequences in browsing-based component retrieval using information entropy|Reuse repositories are an essential element in component-based software development (CBSD). Querying-based retrieval and browsing-based retrieval are two main retrieval mechanisms provided in real world reuse repositories, especially web-based repositories. Although browsing-based retrieval is superior to querying-based retrieval in some aspects, the tedious retrieval process is its main drawback, because the browsing-based component retrieval usually involves long retrieval sequences. In this paper, we propose a novel approach to shorten the retrieval sequences in browsing-based component retrieval using information entropy. The basic idea of our approach is to build a navigation model by ranking the features into a tree structure using the components’ indexing information. According to our experimental results on real data, our approach can effectively shorten the average length of retrieval sequences. 
79|2||Polyhedral space generation and memory estimation from interface and memory models of real-time video systems|We present a tool and a methodology for estimating the memory storage requirement for synchronous real-time video processing systems. Typically, a designer will use the feedback information from this estimation to select the most optimal execution order for software processors or space to time mapping for hardware. We propose to start from a conceptual interface and memory model that captures memory usage and data transfers. This high-level modeling is provided as an extension library of SystemC called IMEM. A common polyhedral iteration space is generated from the model, where polytopes are placed using a new placement algorithm based on simple heuristics. This algorithm will ensure maximum freedom of selecting executing order as all negative dependencies are removed to the length of zero. A demonstration is given regarding how the polytopes and dependency vectors can then be used as input to a memory storage estimation tool called STOREQ. 
79|2||Assessment of eco-security in the Knowledge Grid e-science environment|Ecological security is an important basis of the entire human security system, the cornerstone for human survival. Knowing the status of ecological security is crucial for making decisions to avoid ecological disaster. Existing research, both the basic research on ecological security mechanism and information service systems, is still primitive in their abilities to resolve eco-security problems. This paper investigates the eco-security impact factors and identifies eco-security types. Taking urban eco-security as an example, we develop an assessment method including the indicator system and assessment model, and construct an integrated assessment framework based on data integrity, security assessment and security management with the support of the Knowledge Grid e-science environment. The proposed assessment framework enables decision makers to better know the status of eco-security in making policies for achieving sustainability. 
79|2||On the security of the YenâGuoâs domino signal encryption algorithm (DSEA)|Recently, a new domino signal encryption algorithm (DSEA) was proposed for digital signal transmission, especially for digital images and videos. This paper analyzes the security of DSEA, and points out the following weaknesses: (1) its security against the brute-force attack was overestimated; (2) it is not sufficiently secure against ciphertext-only attacks, and only one ciphertext is enough to get some information about the plaintext and to break the value of a sub-key; (3) it is insecure against known/chosen-plaintext attacks, in the sense that the secret key can be recovered from a number of continuous bytes of only one known/chosen plaintext and the corresponding ciphertext. Experimental results are given to show the performance of the proposed attacks, and some countermeasures are discussed to improve DSEA. 
79|2||A scheduling framework for enterprise services|Application-level scheduling is a common process in the enterprise domain, but in order to be productive it should be time-effective, interoperable and reliable. In this paper we present the design and implementation aspects of a modular scheduling solution, which aims to fulfill the above-mentioned requirements. The proposed solution constitutes an integral part of an existing service provisioning platform targeted to the location based services (LBS) domain; however application to other service-oriented architectures is also possible. A careful design process is followed in order to identify the scheduler’s functional subsystems and the relationships between them. The implementation is based on the J2EE framework, thus guaranteeing independence from underlying technologies and applicability in enterprise environments. The high quality and scalability of the scheduler is also validated through an intensive performance analysis. 
79|2||Comparing requirements analysis methods for developing reusable component libraries|Two approaches to requirements modelling are compared—the Domain Theory [Sutcliffe, A.G., 2002. The Domain Theory: Patterns for Knowledge and Software Reuse. Lawrence Erlbaum Associates, Mahwah, NJ.] and Problem Frames [Jackson, M., 2001. Problem Frames: Analysing and Structuring Software Development Problems, Pearson Education, Harlow.]—as a means of domain analysis for creating a reusable library of software components for constructing telemedicine applications. Experience of applying each approach as a domain analysis method to specify abstract components (object system models and Problem Frames) is reported. The two approaches produced detailed specifications although at different levels of abstraction: problems frames were better for monitoring, updating and data integrity requirements whereas the Domain Theory proved more useful for task support and user interface requirements. The lessons learned from using model-based approaches to requirements specification, and their merits for creating consistent specifications for reuse libraries, are discussed. 
79|2||Call for Papers|
79|3|http://www.sciencedirect.com/science/journal/01641212/79/3|Software field failure rate prediction before software deployment|For both in-house development and outsourcing development environments, knowing the field failure rate of an integrated software system prior to field deployment provides guidance for better decision-makings in balancing reliability, time-to-market and development cost. This paper demonstrates a field failure rate prediction methodology that starts with analyzing system test data and field data (of previous releases or products) using software reliability growth models (SRGMs). A typical issue associated with predicting field failure rate based on test data is that potentially the test environment might not match exactly up the field environment. We discuss how to address the mismatch of the operational profiles of the test and filed environments. Two other practical issues in predicting field failure rates include that fault removals in the field are usually non-instantaneous and fixes of certain faults reported in the field can be deferred. Non-instantaneous fault removal and fault fix deferral becomes more realistic as the current software development environment shifts to a new trend of leveraging third-party, off-the-shelf, and semi-custom hardware and software and having the suppliers focus on development of highest-value applications and system integration. In such an environment, removing a fault might require a longer time and fix deferrals of certain faults becomes more possible in particular for the faults whose fixes will result in changes to other software components. In this paper, we illustrate how to incorporate these issues into field failure rate prediction. Confidence intervals of the predicted failure rate are also included to account for variations in the parameter estimation. Sensitivity analyses are conducted to estimate the uncertainties in the field failure rate prediction. 
79|3||Towards human-centred design: Two case studies|Currently much system development is done using a technology-centred approach: automating the functions the technology is able to perform. Human-centred design including a cognitive work analysis seems a promising alternative for systems combining skilled humans and automated support. Carefully selected information technology can support this innovative system development approach.Two correlated case studies assess the merits and limitations of a human-centred approach. To improve human capacity while maintaining or preferably increasing current safety levels automated support is needed. Despite the long-term trend of increasing automated support, the human remains the major contributing factor in accidents and incidents. Combining these two observations substantiates the need for innovative system design. The described results are relevant for other domains relying on human experts supported by complex automated systems. 
79|3||Technology-driven business evolution|Innovating the business processes and supporting software systems of an enterprise requires their preliminary analysis and assessment. In particular, data concerning the performance and costs of activities and processes must be gathered in order to identify candidates for innovation. A critical point is finding a suitable presentation means for the gathered data in order to effectively support decision makers.This paper presents two case studies performed by applying a strategy, named Joint Evolution of business Processes and software Systems (JEPS), for innovating business processes and their supporting systems. JEPS integrates measurement, decision-making, and critiquing techniques for analyzing business processes, identifying activities and software systems to be innovated, and mapping critiques onto specific innovation actions. JEPS is supported by a software environment, named WebEv+, for managing the assessment and evaluation tasks, and the modeling and critiquing of the business processes. The paper discusses two case studies regarding two different organizations in the Local Government domain. The analysis of the organization highlighted improvement goals and technological enhancement needs, and permitted the identification of the business processes and software systems to be evolved. 
79|3||Automated impact analysis of UML models|The use of Unified Modeling Language (UML) analysis/design models on large projects leads to a large number of interdependent UML diagrams. As software systems evolve, UML diagrams undergo changes that address error corrections and changed requirements. Those changes can in turn lead to subsequent changes to other elements in the UML diagrams. Impact analysis is defined as the process of identifying the potential consequences (side-effects) of a change, and estimating what needs to be modified to accomplish that change. In this article, we propose a UML model-based approach to impact analysis that can be applied before implementation of changes, thus allowing early decision-making and change planning. We first verify that the UML diagrams in a design model are consistent. Then the changes between two different versions of UML models are automatically identified according to a change taxonomy. Next, model elements which are directly or indirectly impacted by the changes (i.e., may undergo changes) are determined using formally defined impact analysis rules (defined with the Object Constraint Language). A measure of distance between a changed element and potentially impacted elements is also proposed to prioritize the results of impact analysis according to their likelihood of occurrence. We also present a prototype tool that provides automated support for our impact analysis strategy, and two case studies that validate both the methodology and the tool. Empirical results confirm that distance helps determine the likelihood of change in the code. 
79|3||An empirical study of process-related attributes in segmented software cost-estimation relationships|Parametric software effort estimation models consisting on a single mathematical relationship suffer from poor adjustment and predictive characteristics in cases in which the historical database considered contains data coming from projects of a heterogeneous nature. The segmentation of the input domain according to clusters obtained from the database of historical projects serves as a tool for more realistic models that use several local estimation relationships. Nonetheless, it may be hypothesized that using clustering algorithms without previous consideration of the influence of well-known project attributes misses the opportunity to obtain more realistic segments. In this paper, we describe the results of an empirical study using the ISBSG-8 database and the EM clustering algorithm that studies the influence of the consideration of two process-related attributes as drivers of the clustering process: the use of engineering methodologies and the use of CASE tools. The results provide evidence that such consideration conditions significantly the final model obtained, even though the resulting predictive quality is of a similar magnitude. 
79|3||Validating strategic alignment of organizational IT requirements using goal modeling and problem diagrams|
79|3||An XML environment for scenario based requirements engineering|The pervasive use of scenarios in the development of computer systems and software has motivated the need of formalisms and tools for the description and manipulation of scenarios. To this aim, we present SME (Scenario Model Environment), an XML based tool for scenario-based requirements engineering. The tool enables to exploit the emerging XML technologies in order to offer powerful ways to create, maintain, distribute and use scenarios. We have widely experimented SME through several case studies, ranging from small examples, like a library system, to complex industrial applications. 
79|3||Categorical missing data imputation for software cost estimation by multinomial logistic regression|A common problem in software cost estimation is the manipulation of incomplete or missing data in databases used for the development of prediction models. In such cases, the most popular and simple method of handling missing data is to ignore either the projects or the attributes with missing observations. This technique causes the loss of valuable information and therefore may lead to inaccurate cost estimation models. On the other hand, there are various imputation methods used to estimate the missing values in a data set. These methods are applied mainly on numerical data and produce continuous estimates. However, it is well known that the majority of the cost data sets contain software projects with mostly categorical attributes with many missing values. It is therefore reasonable to use some estimating method producing categorical rather than continuous values. The purpose of this paper is to investigate the possibility of using such a method for estimating categorical missing values in software cost databases. Specifically, the method known as multinomial logistic regression (MLR) is suggested for imputation and is applied on projects of the ISBSG multi-organizational software database. Comparisons of MLR with other techniques for handling missing data, such as listwise deletion (LD), mean imputation (MI), expectation maximization (EM) and regression imputation (RI) under different patterns and percentages of missing data, show the high efficiency of the proposed method. 
79|3||Investigating the impact of usability on software architecture through scenarios: A case study on Web systems|
79|3||Measuring the usability of software components|The last decade marked the first real attempt to turn software development into engineering through the concepts of Component-Based Software Development (CBSD) and Commercial Off-The-Shelf (COTS) components, with the goal of creating high-quality parts that could be joined together to form a functioning system. One of the most critical processes in CBSD is the selection of a set of software components from in-house or external repositories that fulfil some architectural and user-defined requirements. However, there is a lack of quality models and metrics that can help evaluate the quality characteristics of software components during this selection process. This paper presents a set of measures to assess the Usability of software components, and describes the method followed to obtain and validate them. Such a method can be re-used as a pattern for defining and validating measures for further quality characteristics. 
79|3||Call for Papers|
79|4|http://www.sciencedirect.com/science/journal/01641212/79/4|Adaptive schemes for location update generation in execution location-dependent continuous queries|An important feature that is expected to be owned by today’s mobile computing systems is the ability of processing location-dependent continuous queries on moving objects. The result of a location-dependent query depends on the current location of the mobile client which has generated the query as well as the locations of the moving objects on which the query has been issued. When a location-dependent query is specified to be continuous, the result of the query can continuously change. In order to provide accurate and timely query results to a client, the location of the client as well as the locations of moving objects in the system has to be closely monitored. Most of the location generation methods proposed in the literature aim to optimize utilization of the limited wireless bandwidth. The issues of correctness and timeliness of query results reported to clients have been largely ignored. In this paper, we propose an adaptive monitoring method (AMM) and a deadline-driven method (DDM) for managing the locations of moving objects. The aim of our methods is to generate location updates with the consideration of maintaining the correctness of query evaluation results without increasing location update workload. Extensive simulation experiments have been conducted to investigate the performance of the proposed methods as compared to a well-known location update generation method, the plain dead-reckoning (pdr). 
79|4||A method for analysing multimedia protocol performance|Performance analysis derived from a formal specification of a multimedia protocol provides important information on the performance characteristics of a multimedia system before implementation. The major problem of existing performance methodology is the lack of feasibility in time-dependent and functional behaviours, as well as statistical traffic characteristics for distributed multimedia systems. The internationally standardised Formal Description Technique, Estelle, does not have enough expressive power to specify the time-dependent performance characteristics of a multimedia system. To address this limitation of Estelle, we have developed a variant of Estelle, called Time-Estelle. In this paper, we describe a method for analysing the performance of a multimedia protocol specified in Time-Estelle. To aid the analysis, we introduce a traffic model called the Time-interval Performance Indices in order to capture the burstiness and time correlations of realistic multimedia traffic. Thus, performance parameters of multimedia systems, such as the long-term average rate and short-term peak rate allocation, and exploitation of statistical resources utilisation, can be analysed. To demonstrate the soundness of our method, we have applied it to the Resource ReSerVation Protocol and some results are included in this paper. 
79|4||An object-oriented cryptosystem based on two-level reconfigurable computing architecture|Cryptosystem is a system that needs a secure software and hardware environment. The performance of hardware has made great progress year by year. Hence, it is important to change the architecture of cryptosystem and complicate the computation of cryptography algorithm to catch the trend of modern hardware. If the system is built with software then it is not difficult to change the architecture and complicate the computation. However, in this way, the main disadvantage is poor performance. Using hardware to design a cryptosystem is a good choice because it is highly more secure and efficient than software, but it is a fixed system that cannot be changed. So, it is an important topic to implement a flexible, secure, and efficient cryptosystem which can coordinate with novel hardware. In this article, a new method to implement cryptosystems based on hardware design is proposed. It can fast implement a new cryptosystem because it separates system into two levels: one is system architecture and the other is system algorithm. To combine different architectures and algorithms can implement varied cryptosystems. Otherwise, to split architectures and algorithms will lead to improve the security of system. An application of data transfer between stream cipher and block cipher is also implemented in this system and successfully verified the flexibility, security, and efficiency of this proposed method. 
79|4||Efficient and adaptive discovery techniques of Web Services handling large data sets|
79|4||Cryptanalysis of a hybrid authentication protocol for large mobile networks|In this paper we analyse a hybrid authentication protocol due to Chien and Jan, designed for use in large mobile networks. The proposed protocol consists of two sub-protocols, namely the intra-domain authentication protocol and the inter-domain authentication protocol, which are used depending on whether the user and the request service are located in the same domain. We show that both sub-protocols suffer from a number of security vulnerabilities. 
79|4||Improved self-certified group-oriented cryptosystem without a combiner|In 2001, Ghodosi and Saeednia proposed a self-certified group-oriented cryptosystem without a combiner to prevent the Susilo et al.’s attack. However, in this paper we will show that their scheme is still insecure and probably suffers from the conspired attack. To remedy the weakness, an enhanced version is proposed while providing the new functionality to confirm the source of the encrypted message. 
79|4||Managing role relationships in an information flow control model|
79|4||Declarative programming of integrated peer-to-peer and Web based systems: the case of Prolog|Web and peer-to-peer systems have emerged as popular areas in distributed computing, and their integrated usage permits the benefits of both to be exploited. While much work in these areas have utilized the imperative programming paradigm, the need for declarative programming paradigms is increasingly being recognized, not only for the often cited advantages such as a higher level of abstraction and specialized language features, but also to tackle the querying and manipulation of knowledge and reasoning with semantics that will be the mainstay of the proposed next generation of the Web and peer-to-peer computing. This paper presents an approach towards integrative use of the Web and peer-to-peer systems within a declarative programming paradigm. We contend that logic programming can be useful in peer-to-peer computing, especially for querying and representing knowledge shared over peer networks, and for scripting applications that involve sophisticated search behaviour over peer networks. As an example of peer-to-peer querying expressed in a logic programming language, we propose a simple extension of Prolog, which we call LogicPeer, to enable goal evaluation over peers in a peer network. Using LogicPeer, we outline how a peer-to-peer version of a Yahoo-like system can be built and queried, and several other applications that involve decentralized knowledge sharing. We then show how LogicPeer can be used with LogicWeb, a Prolog extension to access Web pages, thereby integrating peer-to-peer querying and Web querying in a common declarative framework. 
79|4||Patterns of conflict among software components|Integrating a system of disparate components to form a single application is still a daunting, high risk task, especially for components with heterogeneous communication expectations. It benefits integration to know explicitly the interoperability conflicts that can arise based on the current application design and the components being considered. However, there is no consistent representation of identified conflicts that also defines strategies to resolve them. Instead, developers use prior experience which may have consequential gaps. In this paper, we formulate a common representation for six major interoperability conflicts that arise through discrepancies or direct mismatches among architectural properties of interacting components. We use an Extender-Translator-Controller (ETC) classification scheme to describe the conflict resolution strategies. Detailing these associations as patterns provides insight into formulating an overall integration architecture design reflecting the solution strategy for developers to codify with respect to all components in the application. This approach moves conflict detection and resolution toward automation, immediately reducing the risk associated with the development of component based integrated systems. 
79|4||Recovering architectural assumptions|During the creation of a software architecture, the architects and stakeholders take a lot of decisions. Many of these decisions can be directly related to functional or quality requirements. Some design decisions, though, are more or less arbitrarily made on the fly because of personal experience, domain knowledge, budget constraints, available expertise, and the like. These decisions, as well as the reasons for those decisions, are often not explicit upfront. They are implicit, and usually remain undocumented. We call them assumptions. There is no accepted way to document assumptions, and the relation between the software architecture and these assumptions easily gets lost, becomes hidden in the girders of the architecture. They are rediscovered at a later stage, when the software evolves and assumptions become invalid or new assumptions contradict earlier ones. In this paper, we develop a method to recover such assumptions from an existing software product. We illustrate the method by applying it to a commercial software product, and show how the results can help assess the evolutionary capabilities of its architecture. 
79|5|http://www.sciencedirect.com/science/journal/01641212/79/5|Editorial|
79|5||A comparison of MC/DC, MUMCUT and several other coverage criteria for logical decisions|Many testing criteria, including condition coverage and decision coverage, are inadequate for software characterised by complex logical decisions, such as those in safety-critical software. In the past decade, more sophisticated testing criteria have been advocated. In particular, compliance of the MC/DC criterion has been mandated in the commercial aviation industry for the approval of airborne software. Recently, the MUMCUT criterion has been proposed as it guarantees the detection of certain faults in logical decisions in disjunctive normal form in which no variable is redundant. This paper compares MC/DC, MUMCUT and several other related coverage criteria for logical decisions by both formal and empirical analysis, focusing on the fault-detecting ability of test sets satisfying these testing criteria. Our results show that MC/DC test sets are effective, but they may still miss some faults that can almost always be detected by test sets satisfying the MUMCUT criterion. 
79|5||On the statistical properties of testing effectiveness measures|
79|5||Automatic goal-oriented classification of failure behaviors for testing XML-based multimedia software applications: An experimental case study|When testing multimedia software applications, we need to overcome important issues such as the forbidding size of the input domains, great difficulties in repeating non-deterministic test outcomes, and the test oracle problem. A statistical testing methodology is proposed. It applies pattern classification techniques enhanced with the notion of test dimensions. Test dimensions are orthogonal properties of associated test cases. Temporal properties are being studied in the experimentation in this paper. For each test dimension, a pattern classifier is trained on the normal and abnormal behaviors. A type of failure is said to be classified if it is recognized by the classifier. Test cases can then be analyzed by the failure pattern recognizers. Experiments show that some test dimensions are more effective than others in failure identification and classification. 
79|5||ASM-based design of data warehouses and on-line analytical processing systems|On-line analytical processing (OLAP) systems deal with analytical tasks in businesses. As these tasks do not depend on the latest updates by transactions, it is assumed that the data used in OLAP systems are kept in a data warehouse, which separates the input from operational databases from the outputs to dialogue interfaces for OLAP. Data Warehouses and OLAP systems are a promising area for the application of Abstract State Machines (ASMs). In this article a layered ground model specification for data warehouses and OLAP system is presented that is based explicitly on the fundamental idea of separating input from operational databases from output to OLAP systems. On this basis we start defining formal refinement rules for such systems. As these refinement rules are formally correct they enable a formal method for the high-quality design of data warehouses and OLAP systems that can be applied without knowing mathematical details of the ASM formalism. Furthermore, we discuss pragmatic guidelines for the application of such rules. 
79|5||Semantic errors in SQL queries: A quite complete list|We investigate classes of SQL queries which are syntactically correct, but certainly not intended, no matter for which task the query was written. For instance, queries that are contradictory, i.e. always return the empty set, are obviously not intended. However, current database management systems (DBMS) execute such queries without any warning. In this paper, we give an extensive list of conditions that are strong indications of semantic errors. Of course, questions like the satisfiability are in general undecidable, but a significant subset of SQL queries can actually be checked. We believe that future DBMS will perform such checks and that the generated warnings will help to develop application programs with fewer bugs in less time. 
79|5||Yet shorter warmup by combining no-state-loss and MRRL for sampled LRU cache simulation|Sampling is a well known technique for speeding up time-consuming architectural simulations. An important issue with sampling is the hardware state at the beginning of each sampling unit. This paper presents a highly accurate and highly efficient warmup method for sampled cache simulation by combining ‘no-state-loss (NSL)’ and ‘memory reference reuse latency (MRRL)’. Our combined warmup scheme MRRL–NSL achieves the same accuracy for sampled LRU cache simulation as MRRL with a two orders of magnitude shorter warmup. Compared to NSL, MRRL–NSL has a factor 2–6 shorter warmup while inducing a small absolute miss rate error of 0.1%. 
79|5||Optimal resource allocation for cost and reliability of modular software systems in the testing phase|Reliability is one of the most important quality attributes of commercial software since it quantifies software failures during the development process. In order to increase the reliability, we should have a comprehensive test plan that ensures all requirements are included and tested. In practice, software testing must be completed within a limited time and project managers should know how to allocate the specified testing-resources among all the modules. In this paper, we present an optimal resource allocation problem in modular software systems during testing phase. The main purpose is to minimize the cost of software development when the fixed amount of testing-effort and a desired reliability objective are given. An elaborated optimization algorithm based on the Lagrange multiplier method is proposed and numerical examples are illustrated. Moreover, sensitivity analysis is also conducted. We analyze the sensitivity of parameters of proposed software reliability growth models and show the results in detail. The experimental results greatly help us to identify the contributions of each selected parameter and its weight. The proposed algorithm and method can facilitate the allocation of limited testing-resource efficiently and thus the desired reliability objective during software module testing can be better achieved. 
79|5||Effective fair allocation using smart market label auction with CSLF and CR-CSFQ|Core-stateless mechanisms achieve better scalability by reducing the complexity of fair queuing, which usually needs to maintain states, manage buffers, and perform flow scheduling on a per-flow basis. In this paper, we propose two core-stateless fair bandwidth allocation schemes. Both schemes do not need to maintain per-flow state. Packet is labeled using smart market model according to the characteristics of the flow to which the packet belongs. No matter TCP or UDP flows can get their fair share rate by the proposed schemes. The first scheme is called core-stateless labeling fairness (CSLF). In CSLF scheme, packets are labeled only once at the entrance of the network and estimators of number of active flows based on Bloom filter are employed at the core routers. The estimation can be used to provide a fair rate to perform auction. Packets of a flow whose rate exceeds the estimated fair share rate are dropped at a congested router. The second scheme, called congestion-responsive-CSFQ (CR-CSFQ), is one extension from CSFQ. Congestion-responsive flows should get a different treatment from non-responsive flows at each core router. Fairness can be achieved by one extra smart market label in each packet. Through simulations, CSLF and CR-CSFQ are shown to achieve fair allocation effectively. 
79|5||Adaptive data dissemination schemes for location-aware mobile services|Broadcasting is the natural method of propagating information in wireless links, which guarantees scalability in the case of bulk data transfers. It is particularly attractive for resource limited mobile clients in asymmetric communications. To facilitate power saving via wireless data broadcast, index information is typically broadcast along with the data. By first accessing the broadcast index, the mobile client is able to predict the arrival time of the desired data. However, it suffers from the drawback that the client has to wait and tune for an index segment, in order to conserve battery power consumption. In location-aware mobile services (LAMSs), it is important to reduce the query response time, since a late query response may contain out-of-date information. This paper proposes a new broadcast-based spatial query processing method, called BBS designed to support NN query processing. In the BBS, broadcasted data objects are sorted sequentially based on their locations, and the server broadcasts the location dependent data along with an index segment. In this method, since the data objects broadcasted by the server are sequentially ordered based on their location, it is not necessary for the client to wait for an index segment, if it has already identified the desired data items before the associated index segment has arrived. The performance of this scheme is investigated in relation to various environmental variables, such as the distributions of the data objects, the average speed of the clients and the size of the service area. 
79|5||Comparison of performance of Web services, WS-Security, RMI, and RMIâSSL|This article analyses two most commonly used distributed models in Java: Web services and RMI (Remote Method Invocation). The paper focuses on regular (unsecured) as well as on secured variants, WS-Security and RMI–SSL. The most important functional differences are identified and the performance on two operating systems (Windows and Linux) is compared. Sources of performance differences related to the architecture and implementation are identified. The overheads related to the usage of security and the influences of JCE (Java Cryptography Extension) security providers on the performance of secured remote invocations are identified. Finally, the impact of distributed models on design and implementation of distributed applications is identified and guidelines for improving distributed application performance in design and implementation stage are provided. The paper contributes to the understanding of functional and performance related differences between Web services and RMI and their secure variants, WS-Security and RMI–SSL. 
79|5||An agent based synchronization scheme for multimedia applications|Synchronization of multimedia streams is one of the important issue in multimedia communications. In this paper, we propose an adaptive synchronization agency for synchronization of streams by using an agent based approach. The synchronization agency triggers one of the three synchronization mechanisms, point synchronization or real-time continuous or adaptive synchronization in order to adapt to the run-time and life-time presentation requirements of an application. The scheme or agency employs static and mobile agents for the following purpose: to estimate the network delays in real-time based on sustainable stream loss, to compute the skew, to monitor the loss and estimate the playout times of the presentation units. We have experimentally evaluated the scheme by using IBM Aglets and verified its functioning in terms of synchronization loss and mean buffering delays. The benefits of this agent based scheme are: asynchronous and autonomous delay estimation, flexibility, adaptability, software re-usability and maintainability. 
79|5||Performance evaluation of peer-to-peer Web caching systems|Peer-to-peer Web caching has attracted a great attention from the research community recently, and is one of the potential peer-to-peer applications. In this paper, we systematically examine the three orthogonal dimensions to design a peer-to-peer Web caching system, including the caching algorithm, the document lookup algorithm, and the peer granularity. In addition to the traditional URL-based caching algorithm, we also evaluate the content-based caching algorithm for both dynamic and static Web content. Four different document lookup algorithms are compared and evaluated in the context of four different peer granularities, i.e., host level, organization level, building level, and centralized. A detailed simulation, using the traces collected at a medium size education institution, is conducted for the purpose of performance evaluation. Finally, several implications derived from this analysis are also discussed. 
79|5||Empirical assessment of using stereotypes to improve comprehension of UML models: A set of experiments|Stereotypes were introduced into the Unified Modeling Language to provide means of customizing this general purpose modeling language for its usage in specific application domains. The primary role of stereotypes is to brand an existing model element with specific semantics, but stereotypes can also be used to provide means of a secondary classification of modeling elements. This paper elaborates on the influence of stereotypes on the comprehension of models. The paper describes a set of controlled experiments performed in academia and industry which were aimed at evaluating the role of stereotypes in improving comprehension of UML models. The results of the experiments show that stereotypes play a significant role in the comprehension of models and the improvement achieved both by students and industry professionals. 
79|5||Book review: H. Zhuge, The Knowledge Grid, World Scientific Publishing Co., Singapore, 2004|
79|6|http://www.sciencedirect.com/science/journal/01641212/79/6|Software reliability forecasting by support vector machines with simulated annealing algorithms|Support vector machines (SVMs) have been successfully employed to solve non-linear regression and time series problems. However, SVMs have rarely been applied to forecasting software reliability. This investigation elucidates the feasibility of the use of SVMs to forecast software reliability. Simulated annealing algorithms (SA) are used to select the parameters of an SVM model. Numerical examples taken from the existing literature are used to demonstrate the performance of software reliability forecasting. The experimental results reveal that the SVM model with simulated annealing algorithms (SVMSA) results in better predictions than the other methods. Hence, the proposed model is a valid and promising alternative for forecasting software reliability. 
79|6||Constraint based structural testing criteria|
79|6||A petri-net-based synthesis methodology for use-case-driven system design|In use-case-driven system design, one need to derive from a set of use cases an integrated system design that meets at least two objectives. First, the system design must be consistent with the use cases in the sense that the functionalities of the use cases are exactly reflected. Second, the system design must be correct in the sense that the system is free from errors such as deadlock and capacity overflow. The latter is especially important for systems involving some concurrent, sequential, asynchronous, competitive and coordinated processes, such as manufacturing systems. In this paper, we propose a synthesis methodology for use-case-driven system design whereby both objectives are satisfied. In our methodology, use cases are specified as labelled Petri nets, in which the conditions, events and their causal relationships are explicitly represented. A system design is then derived by synthesising these nets into a single consistent whole. Our methodology has two distinctive features, namely, a formal specification of use cases and a rigorous and systematic process for deriving a system design whose correctness and consistency can be optimally achieved. We specifically show its application to manufacturing system design. Promising results are obtained. 
79|6||An efficient interprocedural dynamic slicing method|We present an efficient interprocedural dynamic slicing algorithm for structured programs. We first propose an intraprocedural dynamic slicing algorithm, and subsequently extend it to handle interprocedural calls. Our intraprocedural dynamic slicing algorithm uses control dependence graph as the intermediate program representation, and computes precise dynamic slices. The interprocedural dynamic slicing algorithm uses a collection of control dependence graphs (one for each procedure) as the intermediate program representation, and computes precise dynamic slices. We show that our proposed interprocedural dynamic slicing algorithm is more efficient than the existing dynamic slicing algorithms. We also discuss how our algorithm can be extended to efficiently handle recursion, composite data structures and pointers. 
79|6||Maintainability of the kernels of open-source operating systems: A comparison of Linux with FreeBSD, NetBSD, and OpenBSD|
79|6||An assessment of systems and software engineering scholars and institutions (2000â2004)|This paper presents the findings of a five-year study of the top scholars and institutions in the Systems and Software Engineering field, as measured by the quantity of papers published in the journals of the field in 2000–2004. The top scholar is Hai Zhuge of the Chinese Academy of Sciences, and the top institution is Korea Advanced Institute of Science and Technology. This paper is part of an ongoing study, conducted annually, that identifies the top 15 scholars and institutions in the most recent five-year period. 
79|6||Automatic generation of test cases from Boolean specifications using the MUMCUT strategy|
79|6||A field study of the Wheelâa usability engineering process model|Interactive system developers are increasingly including usability engineering as an integral part of interactive system development. With recognition of the importance of usability come attempts to structure this new aspect of overall system development, leading to a variety of processes and methodologies. Unfortunately, these processes often lack flexibility, customizability, completeness, and breadth of coverage. This paper describes our development of a meta process or process model that we call the Wheel. This innovative approach to creating and tailoring usability engineering processes addresses these shortcomings, and describes an evaluation of its application in a real-world commercial development environment.The Wheel process model for usability engineering is not a process itself, but instead, it provides a general framework into which developers can fit specific existing or new techniques, methods, or activities to apply “best usability practices”. It grew out of our examination, adaptation, and extension of several existing usability engineering and software methodologies. The methods that most strongly guided creation of the Wheel were the LUCID framework of interaction design, the Star life cycle of usability engineering, and the waterfall and spiral models of software engineering. The resulting process model assumes the form of a sequence of distinct cycles (each of which produces a product form), allowing developers to focus on each cycle separately. Each cycle has the same four activity types: Analyze, Design, Implement, and Evaluate. Each activity type in a cycle is instantiated using an existing usability engineering technique.Working with an Internet technology company in northern Virginia under grant sponsorship from the Virginia Center for Innovative Technology (a research and development incubator for the Commonwealth of Virginia), we instantiated the Wheel process model and used it to develop a Web-based device management system. The process model performed remarkably well for this development environment, overcoming the tight constraints of budget and schedule cuts to produce an excellent process instance that resulted in a demonstration prototype of the company’s target system. 
79|6||QoS analysis for component-based embedded software: Model and methodology|Component-based development (CBD) techniques have been widely used to enhance the productivity and reduce the cost for software systems development. However, applying CBD techniques to embedded software development faces additional challenges. For embedded systems, it is crucial to consider the quality of service (QoS) attributes, such as timeliness, memory limitations, output precision, and battery constraints. Frequently, multiple components implementing the same functionality with different QoS properties (measurements in terms of QoS attributes) can be used to compose a system. Also, software components may have parameters that can be configured to satisfy different QoS requirements. Composition analysis, which is used to determine the most suitable component selections and parameter settings to best satisfy the system QoS requirement, is very important in embedded software development process. In this paper, we present a model and the methodologies to facilitate composition analysis. We define QoS requirements as constraints and objectives. Composition analysis is performed based on the QoS properties and requirements to find solutions (component selections and parameter settings) that can optimize the QoS objectives while satisfying the QoS constraints. We use a multi-objective concept to model the composition analysis problem and use an evolutionary algorithm to determine the Pareto-optimal solutions efficiently. 
79|6||Feature analysis for architectural evaluation methods|Given the complexity of today’s software systems, Software Architecture is a topic that has recently increased in significance. It is consider as a key element in the design and development of systems and has the ability to promote/punish some quality characteristics. Quality is related mainly to the non-functional requirements expected of the system.Using appropriate methods for Architectural Evaluation can help to reduce the risk of low quality Architecture. The objective of this research is to conduct a Features Analysis for three Architectural Evaluation Methods applied to a single case. This resulted in two significant contributions; the aspects which must be present in an architectural evaluation method which is translated in a set of 49 metrics, grouped into features and sub-features to enable the evaluation of a particular method. Different methods can be compared on the basis of these features; however, the choice of method will depend largely on the requirements of the evaluating team. The second contribution pinpoints the strengths of the methods evaluated. The methods studied were: Design and Use of Software Architectures, Architecture Tradeoff Analysis Method, and Architectural Evaluation Method. The evaluation method employed for this research was Features Analysis Case Study. 
79|7|http://www.sciencedirect.com/science/journal/01641212/79/7|Guest Editorial|
79|7||Effective program debugging based on execution slices and inter-block data dependency|Localizing a fault in program debugging is a complex and time-consuming process. In this paper we present a novel approach using execution slices and inter-block data dependency to effectively identify the locations of program faults. An execution slice with respect to a given test case is the set of code executed by this test, and two blocks are data dependent if one block contains a definition that is used by another block or vice versa. Not only can our approach reduce the search domain for program debugging, but also prioritize suspicious locations in the reduced domain based on their likelihood of containing faults. More specifically, we use multiple execution slices to prioritize the likelihood of a piece of code containing a specific fault. In addition, the likelihood also depends on whether this piece of code is data dependent on other suspicious code. A debugging tool, DESiD, was developed to support our method. A case study that shows the effectiveness of our method in locating faults on an application developed for the European Space Agency is also reported. 
79|7||Assigning tasks in a 24-h software development model|With the advent of globalization and the Internet, the concept of global software development is gaining ground. The global development model opens up the possibility of 24-h software development by effectively utilizing the time zone differences. To harness the potential of the 24-h software development model for reducing the overall development time, a key issue is the allocation of project tasks to the resources in the distributed team. In this paper, we examine this issue of task allocation in order to minimize the completion time of a project. We discuss a model for distributed team across time zones and propose a task allocation algorithm for the same. We apply the approach on tasks of a few synthetic projects and two real projects and show that there is a potential to reduce the project duration as well as improve the resource utilization through 24-h development. 
79|7||An empirical study of groupware support for distributed software architecture evaluation process|Software architecture evaluation is an effective means of addressing quality related issues early in the software development lifecycle. Scenario-based approaches to evaluate architecture usually involve a large number of stakeholders, who need to be collocated for face-to-face evaluation meetings. Collocating a large number of stakeholders is an expensive and time-consuming exercise, which may prove to be a hurdle in the wide-spread adoption of disciplined architectural evaluation practices. Drawing upon the successful introduction of groupware applications to support geographically distributed teams in software inspection, and requirements engineering disciplines, we propose the concept of distributed architectural evaluation using Internet-based collaborative technologies. This paper presents a pilot study used to assess the viability of a larger experiment intended to investigate the feasibility of groupware support for distributed software architecture evaluation. In addition, the results of the pilot study provide some preliminary findings on the viability of groupware-supported software architectural evaluation process. 
79|7||Goal and scenario based domain requirements analysis environment|Identifying and representing domain requirements among products in a product family are crucial activities for a successful software reuse. The domain requirements should be not only identified based on the business goal, which drives marketing plan, product plan, and differences among products, but also represented as familiar notations in order to support developing a particular product in the product family. Thus, our proposal is to identify the domain requirements through goals and scenarios, and represent them as variable use cases for a product family. Especially, for identification of the domain requirements, we propose four abstraction levels of requirements in a product family, and goal and scenario modeling. For representation of them, variable use case model is suggested, and also the use case transfer rules are proposed so as to bridge the gap between the identification and representation activity. The paper illustrates the application of the approach within a supporting tool using the HIS (Home Integration System) example. 
79|7||MUDABlue: An automatic categorization system for Open Source repositories|Open Source communities typically use a software repository to archive various software projects with their source code, mailing list discussions, documentation, bug reports, and so forth. For example, SourceForge currently hosts over seventy thousand Open Source software systems. Because of the size of the rich information content, such repositories offer numerous opportunities for sharing information among projects. For example, one would like to know a set of projects that are related or similar to each other, so that the project groups can collaborate and share their work. With thousands of projects in typical repositories, however, manually locating related projects can be difficult. Hence, we propose MUDABlue, a tool that automatically categorizes software systems. MUDABlue has three major aspects: (1) it relies on no other information than the source code, (2) it determines category sets automatically, and (3) it allows a software system to be a member of multiple categories. MUDABlue has a Web interface to visualize determined categories, which eases browsing a software repository. We show the effectiveness of MUDABlue’s categorization capability by comparing its generated categories with that of some other existing research tools. 
79|7||A unified model for the implementation of both ISO 9001:2000 and CMMI by ISO-certified organizations|ISO 9001 is a standard for quality management systems and CMMI is a model for process improvement. If an ISO-certified organization wishes to improve its processes continuously, implementing CMMI would be a good choice, as it provides more detailed practices for process improvement than the ISO standards. However, there are two issues that need to be resolved when an ISO-certified organization implements CMMI. First, it is not easy to identify any reusable parts of the ISO standards, and it would be advantageous to be able to reuse selected portions of the ISO standards during CMMI adoption in order to use existing resources to their best advantage. Second, it is difficult for an ISO-certified organization to implement CMMI in a straightforward, easy manner because of the differences in the language, structure, and details of the two sets of documents. In this paper, we present our unified model for ISO 9001:2000 and CMMI that resolves these two issues. Our model would be an extremely useful tool for ISO-certified organizations that plan to implement CMMI. 
79|7||Accessing embedded program in untestable mobile environment: Experience of a trustworthiness approach|Comparing actual output with the expected output of some controlled input is a fundamental principle of program correctness testing. However, in some situations, the input is uncontrollable or even undetectable during testing, it is impossible to decide the expected output or the test oracle. Or in some situations, the output may be indeterministic or unpredictable, hence it is impossible to compare the output with the expected one. We encountered these problems and could not conduct normal program testing when we developed programs to extract network data from various mobile stations in the Mobile Location Estimation System project. Instead we analyzed the program output and challenge it against the intrinsic properties, the environment, the program itself, and the application results to find evidence that the output is suitable to be used for the planned purposes. In short, we accessed the trustworthiness of the programs. In the development of our mobile software, it was common that different programs of the same specification had to be developed for mobile stations of different models. These different implementations provided another source of reference for trustworthiness assessment. Our experience of applying the trustworthiness approach to developing software for extracting network data from mobile stations is reported in this paper. 
79|7||Automatic generation of document semantics for the e-science Knowledge Grid|
79|7||An empirical study of XML data management in business information systems|Due the popularity of XML, an increasingly large amount of business transactions encoded in XML have been exchanged on-line. Currently there are two approaches to process and manage these XML data. One is to store them in relational databases, and the other is to store them in recently-developed native XML databases. There is no conclusion yet as to which approach suits better for contemporary business information systems. Also, the effectiveness of native XML databases used in daily operational systems has not been completely investigated. Therefore, in this paper, we provide: (1) a complete and systematic survey of the current development and challenges of processing XML data in relational and native XML databases, (2) a useful benchmark for IT practitioners who need to process XML data effectively, (3) experimental results and detailed analysis which reveal several interesting tips that can be helpful to XML document designers, and (4) a conclusion, based on the findings of using native XML databases in EDI processes, that it is practical to use native XML databases for daily operations although our experimental results showed that relational database systems outperform native XML databases in processing XML data. 
79|7||Results from introducing component-level test automation and Test-Driven Development|For many software development organizations it is of crucial importance to reduce development costs while still maintaining high product quality. Since testing commonly constitutes a significant part of the development time, one way to increase efficiency is to find more faults early when they are cheaper to pinpoint and remove. This paper presents empirical results from introducing a concept for early fault detection. That is, an alternative approach to Test-Driven Development which was applied on a component level instead of on a class/method level. The selected method for evaluating the result of introducing the concept was based on an existing method for fault-based process assessment and was proven practically useful for evaluating fault reducing improvements. The evaluation was made on two industrial projects and on different features within a project that only implemented the concept partly. The evaluation result demonstrated improvements regarding decreased fault rates and Return On Investment (ROI), e.g. the total project cost became about 5–6% less already in the first two studied projects. 
79|7||Methodology for customer relationship management|Customer relationship management (CRM) is a customer-focused business strategy that dynamically integrates sales, marketing and customer care service in order to create and add value for the company and its customers.This change towards a customer-focused strategy is leading to a strong demand for CRM solutions by companies. However, in spite of companies’ interest in this new management model, many CRM implementations fail. One of the main reasons for this lack of success is that the existing methodologies being used to approach a CRM project are not adequate, since they do not satisfactorily integrate and complement the strategic and technological aspects of CRM.This paper describes a formal methodology for directing the process of developing and implementing a CRM System that considers and integrates various aspects, such as defining a customer strategy, re-engineering customer-oriented business processes, human resources management, the computer system, management of change and continuous improvement. 
79|7||Prototyping mediators to project performance: Learning and interaction|The prototyping approach has been considered to be a more effective systems development methodology than the traditional systems development life cycle approach. Prototyping provides a framework for meaningful social interaction between system users and developers. This research examines the social contextual factors that determine the performance of information system projects and addresses the social perspectives of system development methodologies within the prototyping development framework. This study suggests that the full mediators of the prototyping approach were organizational technology learning and user–IS interaction effectiveness. Organizations should use prototyping in situations where these mediators are important to the development process and are not achieved through other organizational practice. 
79|8|http://www.sciencedirect.com/science/journal/01641212/79/8|Location-aware multimedia proxy handoff over the IPv6 mobile network environment|In a server-proxy-client 3-tier networking architecture that is executed in the mobile network, proxies should be dynamically assigned to serve mobile hosts according to geographical dependency and the network situation. The goal of proxy handoff is to allow a mobile host still can receive packets from the corresponding server while its serving proxy is switched from the current one to another one. Once proxy handoff occurs, a proper proxy should be selected based on the load balance concern and the network situation concern. In this paper, a 3-tier Multimedia Mobile Transmission Platform, called MMTP, is proposed to solve the proxy handoff problem in the IPv6-based mobile network. A proxy handoff scheme based on the application-layer anycasting technique is proposed in MMTP. A proper proxy is selected from a number of candidate proxies by using the application-layer anycast. An experimental environment based on MMTP is also built to analyze the performance metrics of MMTP, including load balance among proxies and handoff latency. 
79|8||A tunable hybrid memory allocator|Dynamic memory management can make up to 60% of total program execution time. Object oriented languages such as C++ can use 20 times more memory than procedural languages like C. Bad memory management causes severe waste of memory, several times that actually needed, in programs. It can also cause degradation in performance. Many widely used allocators waste memory and/or CPU time. Since computer memory is an expensive and limited resource its efficient utilization is necessary. There cannot exist a memory allocator that will deliver best performance and least memory consumption for all programs and therefore easily tunable allocators are required. General purpose allocators that come with operating systems give less than optimal performance or memory consumption. An allocator with a few tunable parameters can be tailored to a program’s needs for optimal performance and memory consumption. Our tunable hybrid allocator design shows 11–54% better performance and nearly equal memory consumption when compared to the well known Doug Lea allocator in seven benchmark programs. 
79|8||On the efficiency and performance evaluation of the bandwidth clustering scheme for adaptive and reliable resource allocation|A variety of disciplines have recently advocated the use of self-adaptive and auto-configuration methods, including biodynamics, cybernetics and computer modeling. Of these methods, one which exhibits numerous powerful features that are desirable in communication systems is adaptive swarm based intelligence. Swarm-based self-configuration does not require the need of external help, supervision or control. The stochastic nature of random events adversely affects the complexity of optimization tasks. This work proposes a bandwidth clustering scheme suited for this network resource allocation problem. Bandwidth clustering is used in a swarm-based active network environment where active packets continuously communicate with active nodes by using the Split Agent Routing Technique (SART). This mechanism enables the adaptation of the system to new conditions (bandwidth reservation/capacity allocation), as well as the passing of additional information to neighboring nodes in which the information is held in transmitted packets. Paths are clustered with respect to different levels of bandwidth in order to enable capacity allocation and bandwidth reservation on demand, for any requested traffic. The performance, reliability and adaptivity degree of the proposed scheme is thoroughly examined for different traffic measures, as well as the corresponding QoS offered (in terms of the end-to-end delay, available bandwidth and probability of packet loss). This scheme offers a decentralized and non-path-oriented way to efficiently increase the overall network utilization, enabling an equal share of network resources at the same time. 
79|8||On past-time indexing of moving objects|Tracking of mobile objects trajectories is one of many modern applications supported by Spatiotemporal databases. Within the context of this application, queries about the present, future or past positions of the objects need to be answered. Several indexing methods have been proposed to efficiently handle such spatiotemporal queries. In the current paper, we propose a method for indexing the historic (past) positions of moving objects called XBR-tree, a quadtree-like technique that is able to handle both timestamp and window queries. Moreover, we compare experimentally this with other methods proposed in the literature for the same purpose. In particular, we compare XBR-trees with PMR-trees, structures also related to quadtrees and MV3R-trees, R-tree based structures. 
79|8||The implementation of an extensible system for comparison and visualization of class ordering methodologies|In this paper we present the design and implementation of a system that exploits well-known design patterns to facilitate construction of an extensible system for comparison and visualization of ordering methodologies for class-based testing of C++ applications. Using our implementation, we present a comparative study and evaluation of two advanced ordering methodologies: the edge based approach by Briand et al., and the Class Ordering System (COS) introduced in this paper. We compare two variations of the approach by Briand and three variations of the COS system and draw conclusions about the number of edges removed, and therefore the number of stubs that must be constructed, using each approach. We also compare the run-time efficiency of each approach and raise some interesting questions about edge type considerations for removal in the presence of cycles in the ORD. Using the design patterns together with the dot tool from the Graphviz package, we incorporate visualization of the ORD and the edge removals into our system. We present details and graphical visualization of the edge removal process. 
79|8||Timeslot-sharing algorithm with a dynamic grouping for WDM broadcast-and-select star networks|The all-to-all transmission schedule is suitable for Wavelength division multiplexing (WDM) broadcast-and-select star networks under uniform traffic patterns. The performance suffers degradation under non-uniform traffic conditions. The timeslot-sharing algorithm with a dynamic grouping is proposed to decrease the degradation under the non-uniform traffic matrices. According to a given traffic matrix, the algorithm decomposes the given traffic matrix into a transmission matrix and a residual matrix. The algorithm aims to obtain a transmission matrix with most entries and to approach a uniform traffic matrix. Also, its complexity is evaluated. In the simulations, the system with the proposed algorithm provides a superior performance based on throughput, packet delay and node’s buffer size. The results show that the algorithm is an efficient solution to improve the performance under uniform and non-uniform traffic distributions. 
79|8||Reversible index-domain information hiding scheme based on side-match vector quantization|Information hiding has become an interesting topic that receives more and more attention. Recently, many hiding techniques were proposed to directly conceal secret information on an image. However, for convenience and efficiency, images are usually stored and compressed by lossy or lossless compression mechanisms in indices format. The hidden information might be erased or cancelled when the stego image is lossy compressed. Hence, this paper proposes an information hiding scheme based on side-match vector quantization (SMVQ), which conceals the secret information on the indices of the SMVQ compressed images. The proposed scheme not only can embed information in the indices of the compressed image with low image distortion, but also can recover the original indices to reconstruct the SMVQ compressed image. As the experimental results indicated, the proposed scheme indeed outperforms other schemes in terms of image quality, hiding capacity, and compression rate. 
79|8||An image size unconstrained ownership identification scheme for gray-level and color ownership statements based on sampling methods|This paper describes an ownership identification method with gray-level or color ownership statements. The proposed scheme uses the theories and properties of sampling distribution of means to achieve the requirements of robustness and security. Besides, the sampling method also provides that the ownership statements can be of any size regardless of the size of the original image. Since our method does not really insert the ownership statements into the host image, the original image will not be altered and the rightful ownership can be identified without resorting to the original image. Moreover, our method also allows plural ownership statements to be registered for a single host image without causing any damage to other hidden ownership statements. Finally, experimental results will show the robustness of our scheme for gray-level and color ownership statements against several common attacks. 
79|8||Formal specification applied to multiuser distributed services: Experiences in collaborative t-learning|The development of multiuser and distributed software systems faces the difficulty to program the applications correctly, in a way that guarantees the desired interaction among the users. Motivated by experiences with collaborative t-learning services (i.e. multiuser educational services over Interactive TV), this paper presents a solution to that problem, based on supplementing visual development with formal specification techniques. As a contribution to the development of interactive systems, a software process is introduced that helps defining the separate and the conjoint behavior of different users, incrementally and using highly-accessible formalisms. 
79|8||Error resilient locally adaptive data compression|A data compression scheme that exploits locality of reference has been proposed by Bentley et al. in 1986 [Bentley, J.L., Sleator, D.D., Tarjan, R.E., Wei, V.K., 1986. A locally adaptive data compression scheme. Commun. ACM 29 (4), 320–330]. The scheme is based on a self-organizing move-to-front word list. However, a single error occurred on transmission channel could cause the lists of the encoder and decoder to differ, which could corrupt all reference data that follows. To reduce the impact of losing synchronization, a new list structure for the locally adaptive data compression scheme is proposed in this paper. From our analysis and experiments, the proposed scheme can enhance error resiliency of locally adaptive data compression while preserving high compression performance. 
79|8||An efficient key-management scheme for hierarchical access control based on elliptic curve cryptosystem|The elliptic curve cryptosystem is considered to be the strongest public-key cryptosystem known today and is preferred over the RSA cryptosystem because the key length for secure RSA has increased over recent years, and this has put a heavier processing load on its applications. An efficient key management and derivation scheme based on the elliptic curve cryptosystem is proposed in this paper to solve the hierarchical access control problem. Each class in the hierarchy is allowed to select its own secret key. The problem of efficiently adding or deleting classes can be solved without the necessity of regenerating keys for all the users in the hierarchy, as was the case in previous schemes. The scheme is shown much more efficiently and flexibly than the schemes proposed previously. 
79|8||An efficient free-list submesh allocation scheme for two-dimensional mesh-connected multicomputers|This paper presents an efficient free-list submesh allocation scheme for two-dimensional mesh-connected systems. The scheme maintains an unordered list of free submeshes. For allocation, it selects the first free submesh that has at least the same size as the request, and when the selected submesh is larger than the request the part actually allocated is one that has the largest number of busy neighbors and mesh boundary processors. Whereas the best previously proposed schemes have time complexities that are either quadratic or cubic in the number of free or allocated submeshes, the time complexity of the proposed scheme is linear in the number of free submeshes. To evaluate the effectiveness of the scheme, its system performance in terms of parameters such as the average job turnaround time was compared to that of promising previously proposed schemes. Simulation results show that the scheme performs at least as well as these schemes, yet it has the lowest time complexity. 
79|8||AVDL: A highly adaptable architecture view description language|Architectural views are rapidly gaining a momentum as a vehicle to document and analyze software architectures. Despite their popularity, there is no dedicated language flexible enough to support the specifications of an unbound variety of views including those preexisting and needing to be newly created on demand. In this paper, we propose a novel view description language intended for specifying any arbitrary views, using a uniform set of conventions for constructing views and how to use them. The highly adaptable nature of the new language results from its built-in mechanisms to define different types of views in a systematic and repeatable manner. 
79|8||The essential components of software architecture design and analysis|Architecture analysis and design methods such as ATAM, QAW, ADD and CBAM have enjoyed modest success and are being adopted by many companies as part of their standard software development processes. They are used in the lifecycle, as a means of understanding business goals and stakeholders concerns, mapping these onto an architectural representation, and assessing the risks associated with this mapping. These methods have evolved a set of shared component techniques. In this paper we show how these techniques can be combined in countless ways to create needs-specific methods in an agile way. We demonstrate the generality of these techniques by describing a new architecture improvement method called APTIA (Analytic Principles and Tools for the Improvement of Architectures). APTIA almost entirely reuses pre-existing techniques but in a new combination, with new goals and results. We exemplify APTIA’s use in improving the architecture of a commercial information system. 
79|9|http://www.sciencedirect.com/science/journal/01641212/79/9|Selected papers from the fourth Source Code Analysis and Manipulation (SCAM 2004) Workshop|
79|9||An empirical study into class testability|In this paper we investigate factors of the testability of object-oriented software systems. The starting point is given by a study of the literature to obtain both an initial model of testability and existing object-oriented metrics related to testability. Subsequently, these metrics are evaluated by means of five case studies of commercial and open source Java systems for which JUnit test cases exist. The goal of this paper is to identify and evaluate a set of metrics that can be used to assess the testability of the classes of a Java system. 
79|9||Beyond source code: The importance of other artifacts in software development (a case study)|Current software systems contain increasingly more elements that have not usually been considered in software engineering research and studies. Source artifacts, understood as the source components needed to obtain a binary, ready to use version of a program, comprise in many systems more than just the elements written in a programming language (source code). Especially when we move apart from systems-programming and enter the realm of end-user applications, we find files for documentation, interface specifications, internationalization and localization modules and multimedia data. All of them are source artifacts in the sense that developers work directly with them, and that applications are built automatically using them as input. This paper discusses the differences and relationships between source code (usually written in a programming language) and these other files, by analyzing the KDE software versioning repository (with about 6,800,000 commits and 450,000 files). A comprehensive study of those files, and their evolution in time, is performed, looking for patterns and trying to infer from them the related behaviors of developers with different profiles, from where we conclude that studying those ‘other’ source artifacts can provide a great deal of insight on a software system. 
79|9||Effects of context on program slicing|Whether context-sensitive program analysis is more effective than context-insensitive analysis is an ongoing discussion. There is evidence that context-sensitivity matters in complex analyses like pointer analysis or program slicing. Empirical data shows that context-sensitive program slicing is more precise and under some circumstances even faster than context-insensitive program slicing. This article will add to the discussion by examining if the context itself matters, i.e. if a given context leads to more precise slices for that context. Based on some experiments, we will show that this is strongly dependent on the structure of the programs.The presented experiments require backward slices to return to call sites specified by an abstract call stack. Such call stacks can be seen as a poor man’s dynamic slicing: For a concrete execution, the call stack is captured, and static slices are restricted to the captured stack. The experiments show that for recursive programs there is a large increase in precision of the restricted form of slicing compared to the unrestricted traditional slicing.The same experiments also show that a large part (more than half) of an average slice is due to called procedures. 
79|9||Program restructuring using clustering techniques|Program restructuring is a key method for improving the quality of ill-structured programs, thereby increasing the understandability and reducing the maintenance cost. It is a challenging task and a great deal of research is still ongoing. This paper presents an approach to program restructuring inside of a function based on clustering techniques with cohesion as the major concern. Clustering has been widely used to group related entities together. The approach focuses on automated support for identifying ill-structured or low-cohesive functions and providing heuristic advice in both the development and evolution phases. A new similarity measure is defined and studied intensively specifically from the function perspective. A comparative study on three different hierarchical agglomerative clustering algorithms is also conducted. The best algorithm is applied to restructuring of functions of a real industrial system. The empirical observations show that the heuristic advice provided by the approach can help software designers make better decision of why and how to restructure a program. Specific source code level software metrics are presented to demonstrate the value of the approach. 
79|9||Efficient reversal of the intraprocedural flow of control in adjoint computations|Numerical simulations of physical, chemical, and economical processes play an increasingly important role in modern science and engineering. The implementation of mathematical models of real-world applications on a computer facilitates both the speed and the depth of our understanding of the behavior of the respective systems. Derivative models of the computer programs are required to make the transition from pure simulation to the highly desirable optimization of the numerical models with respect to a potentially very large number of input parameters. Such models can be generated automatically from the given numerical program by a source transformation technique known as automatic differentiation. Reversal of the control flow is especially important for the generation of adjoint derivative models. We describe an approach to the control flow reversal of structured programs motivated by the significant weakness of approaches that do not exploit the result of control-flow analysis. Our approach is used to automatically generate adjoint code for numerical programs by semantic source transformation. After a short introduction to applications and the implementation tool set, we motivate the proposed approach with a simple example. We present a novel preaccumulation algorithm for local Jacobian matrices at the level of basic blocks. The main part of the paper covers the reversal of structured control flow graphs. First we show the algorithmic steps for simple branches and loops. We give a detailed algorithm for the reversal of arbitrary combinations of loops and branches based only on the structural information in a general control flow graph. Dependencies between computations and their enclosing control flow constructs can lead to inefficient adjoint code. We formulate a set of conditions that allows for considerable efficiency gains in the adjoint code while permitting a reasonably simple modification of the reversal algorithms for specially designated control flow subgraphs. We present a sensitivity computation of an oceanographic application that illustrates the benefits of this modified approach. 
79|9||How agile are industrial software development practices?|Representatives from the agile development movement claim that agile ways of developing software are more fitting to what is actually needed in industrial software development. If this is so, successful industrial software development should already exhibit agile characteristics. This article therefore aims to examine whether that is the case. It presents an analysis of interviews with software developers from five different companies. We asked about concrete projects, both about the project models and the methods used, but also about the real situation in their daily work. Based on the interviews, we describe and then analyze their development practices. The analysis shows that the software providers we interviewed have more agile practices than they might themselves be aware of. However, plans and more formal development models also are well established. The conclusions answer the question posed in the title: It all depends! It depends on which of the different principles you take to judge agility. And it depends on the characteristics not only of the company but also of the individual project. 
79|9||An integration of fault detection and correction processes in software reliability analysis|Software reliability is defined as the probability of failure-free software operation for a specified period of time in a specified environment and is widely recognized as one of the most significant aspects of software quality. Over the past 30 years, many software reliability growth models (SRGMs) have been proposed and they can greatly help us to estimate some important measures such as the mean time to failure, the number of remaining faults, defect levels, and the failure intensity, etc. Besides, SRGMs can also help to determine person power needed to support the desired reliability requirements. However, from our studies, most of SRGMs only focus on describing the behavior of fault detection process and assume that faults are fixed immediately upon detection. In fact, this assumption may not be realistic. Thus, in this paper, we will propose a general framework for modeling the software fault detection and correction processes. We will also show that the proposed approaches cover a number of well-known SRGMs. Two numerical examples based on two real software failure data sets are presented and discussed in detail. 
79|9||Automatic generation of assumptions for modular verification of software specifications|Model checking is a powerful automated technique mainly used for the verification of properties of reactive systems. In practice, model checkers are limited due to the state explosion problem. Modular verification based on the assume-guarantee paradigm mitigates this problem using a “divide and conquer” technique. Unfortunately, this approach is not automated, for the reason that the user must specify the environment model. In this paper, a novel technique is presented for automatically generating component assumptions based on the behaviour of the environment (the remainder of components of the systems). In the first phase, the environment of the component is computed using state space exploration techniques, and then the assumptions are generated as association rules of the component environment interface. This approach presents a number of advantages. Firstly, user assistance to specify assumptions is not necessary and assumption discharge is avoided. Secondly, the component assumptions are more restrictive and real, and therefore reduce the resources needed by the model checker. The technique is applied to the specification of a steam boiler system. 
79|9||A formal representation of functional size measurement methods|Estimating software size is a difficult task that requires a methodological approach. Many different methods that exist today use distinct abstractions to depict a software system. The gap between abstractions becomes even greater with object-oriented artifacts developed in unified modeling language (UML). In this paper, a formal foundation for the representation of functional size measurement (FSM) methods is presented. The generalized abstraction of the software system (GASS) is then used to formalize different functional measurement methods, namely the FPA, MK II FPA and COSMIC-FFP. The same model is also used for object-oriented projects where UML artifacts are mapped into the GASS form. The algorithms in symbolic code for those UML diagrams that are crucial for size estimation are also given. The mappings defined in this paper enable diverse FSM methods to be supported in estimation tools, the automation of counting steps and a higher-level of independence from the FSM method, since the software abstraction is written in a generalized form. Both improvements are crucial for the practical use of FSM methods. 
80|1|http://www.sciencedirect.com/science/journal/01641212/80/1|Modeling the evolution of operating systems: An empirical study|In this paper, we report on an empirical experiment where we observe, record and analyze the evolution of selected operating systems over the past decades, and derive a statistical model that captures relevant evolutionary laws. This model is derived by quantifying relevant attributes of operating systems, including intrinsic technical factors and time-dependent environmental factors. We use this model to understand past evolution and learn to predict future evolution, not only of individual operating systems, but also of operating systems features. We combine the insights gained from this study with insights gained from other similar empirical experiments to attempt to derive evolutionary laws for software technology trends. 
80|1||Evaluating software project portfolio risks|As in any other business, software development organizations try to maximize their profits and minimize their risks. The risks represent uncertain events and conditions that may prevent enterprises from attaining their goals, turning risk management into a major concern, not only for project managers but also for executive officers involved with strategic objectives. In this sense, economical concepts can greatly support Software Engineers in the effort to better quantify the uncertainties of either a single project or even a project portfolio.In this paper, we present a technique for evaluating risk levels in software projects through analogies with economic concepts. This technique allows a manager to estimate the probability distribution of earnings and losses incurred by an organization in relation to its software project portfolio. This approach has been calibrated by data collected in an empirical study, which has been planned and accomplished to provide information about the relative importance of risk factors in software projects. A usage example of such an approach is presented. Finally, we introduce a case tool specially built to support the application of the proposed techniques. 
80|1||The maintenance and evolution of resource-constrained embedded systems created using design patterns|Most previous work on pattern-based software development has focused on the process of system creation rather than on the post-creation project phases (such as maintenance and evolution). In the study reported in this paper, we present the results from a short series of empirical studies in which we examined techniques for exchanging patterns used in an embedded design after the project had been completed. When exchanging patterns at this time, our aim was to identify the implementation of the pattern of interest in the system code and then substitute a suitable version of the replacement pattern. Findings are presented both from two small test projects, and from a more realistic case study. The results obtained suggest that this approach has considerable potential. 
80|1||An empirical analysis of risk components and performance on software projects|Risk management and performance enhancement have always been the focus of software project management studies. The present paper shows the findings from an empirical study based on 115 software projects on analyzing the probability of occurrence and impact of the six dimensions comprising 27 software risks on project performance. The MANOVA analysis revealed that the probability of occurrence and composite impact have significant differences on six risk dimensions. Moreover, it indicated that no association between the probability of occurrence and composite impact among the six risk dimensions exists and hence, it is a crucial consideration for project managers when deciding the suitable risk management strategy. A pattern analysis of risks across high, medium, and low-performance software projects also showed that (1) the “requirement” risk dimension is the primary area among the six risk dimensions regardless of whether the project performance belongs to high, medium, or low; (2) for medium-performance software projects, project managers, aside from giving importance to “requirement risk”, must also continually monitor and control the “planning and control” and the “project complexity” risks so that the project performance can be improved; and, (3) improper management of the “team”, “requirement”, and “planning and control” risks are the primary factors contributing to a low-performance project. 
80|1||A new imputation method for small software project data sets|Effort prediction is a very important issue for software project management. Historical project data sets are frequently used to support such prediction. But missing data are often contained in these data sets and this makes prediction more difficult. One common practice is to ignore the cases with missing data, but this makes the originally small software project database even smaller and can further decrease the accuracy of prediction. The alternative is missing data imputation. There are many imputation methods. Software data sets are frequently characterised by their small size but unfortunately sophisticated imputation methods prefer larger data sets. For this reason we explore using simple methods to impute missing data in small project effort data sets. We propose a class mean imputation (CMI) method based on the k-NN hot deck imputation method (MINI) to impute both continuous and nominal missing data in small data sets. We use an incremental approach to increase the variance of population. To evaluate MINI (and k-NN and CMI methods as benchmarks) we use data sets with 50 cases and 100 cases sampled from a larger industrial data set with 10%, 15%, 20% and 30% missing data percentages respectively. We also simulate Missing Completely at Random (MCAR) and Missing at Random (MAR) missingness mechanisms. The results suggest that the MINI method outperforms both CMI and the k-NN methods. We conclude that this new imputation technique can be used to impute missing values in small data sets. 
80|1||Identifying and characterizing change-prone classes in two large-scale open-source products|Developing and maintaining open-source software has become an important source of profit for many companies. Change-prone classes in open-source products increase project costs by requiring developers to spend effort and time. Identifying and characterizing change-prone classes can enable developers to focus timely preventive actions, for example, peer-reviews and inspections, on the classes with similar characteristics in the future releases or products. In this study, we collected a set of static metrics and change data at class level from two open-source projects, KOffice and Mozilla. Using these data, we first tested and validated Pareto’s Law which implies that a great majority (around 80%) of change is rooted in a small proportion (around 20%) of classes. Then, we identified and characterized the change-prone classes in the two products by producing tree-based models. In addition, using tree-based models, we suggested a prioritization strategy to use project resources for focused preventive actions in an efficient manner. Our empirical results showed that this strategy was effective for prioritization purposes. This study should provide useful guidance to practitioners involved in development and maintenance of large-scale open-source products. 
80|1||Experimental use of code delta, code churn, and rate of change to understand software product line evolution|This research is a longitudinal study of change processes. It links changes in the product line architecture of a large telecommunications equipment supplier with the company’s customers, inner context, and eight line card products over six-year period. There are three important time related constructs in this study: the time it takes to develop a new product line release; the frequency in which a metric is collected; and the frequency at which financial results and metrics related to the customer layer are collected and made available. Data collection has been organized by product release. The original goal of this research is to study the economic impact of market reposition on the product line and identify metrics that can be used to records changes in product line. We later look at the product line evolution vis-à-vis the changes in the products that form the product line. Our results show that there is no relationship between the size of the code added to the product line and the number of designers required to develop and test it; and there is a positive relationship between designer turnover and impact of change. 
80|1||Interprocedural side-effect analysis for incomplete object-oriented software modules|
80|1||A general model of software architecture design derived from five industrial approaches|We compare five industrial software architecture design methods and we extract from their commonalities a general software architecture design approach. Using this general approach, we compare across the five methods the artifacts and activities they use or recommend, and we pinpoint similarities and differences. Once we get beyond the great variance in terminology and description, we find that the five approaches have a lot in common and match more or less the “ideal” pattern we introduced. From the ideal pattern we derive an evaluation grid that can be used for further method comparisons. 
80|1||Using Bayesian belief networks for change impact analysis in architecture design|Research into design rationale in the past has focused on argumentation-based design deliberations. These approaches cannot be used to support change impact analysis effectively because the dependency between design elements and decisions are not well represented and cannot be quantified. Without such knowledge, designers and architects cannot easily assess how changing requirements and design decisions may affect the system. In this article, we introduce the Architecture Rationale and Element Linkage (AREL) model to represent the causal relationships between architecture design elements and decisions. We apply Bayesian Belief Networks (BBN) to AREL, to capture the probabilistic causal relationships between design elements and decisions. We employ three different BBN-based reasoning methods to analyse design change impact: predictive reasoning, diagnostic reasoning and combined reasoning. We illustrate the application of the BBN modelling and change impact analysis methods by using a partial design of a real-world cheque image processing system. To support its implementation, we have developed a practical, integrated tool set for the architects to use. 
80|10|http://www.sciencedirect.com/science/journal/01641212/80/10|Methodology of security engineering for industrial security management systems|
80|10||IT compliance of industrial information systems: Technology management and industrial engineering perspective|IT compliance is one of the hottest issues in IT and technology management fields. The purpose of this paper is to provide a common framework for IT compliance. First, a review on the compliance age is provided. Second, the characteristics of business records communicated via industrial information systems are described shortly. Finally, an IT compliance framework is suggested. The framework of this paper is not proven by industrial studies because the spread of IT compliance and the industrial response to it are still under progress. Future works should prove the practical value of this framework and should include technical studies. 
80|10||Archetypal behavior in computer security|The purpose of this study is to understand observed behavior and to diagnose and find solutions to issues encountered in organizational computer security using a systemic approach, namely system archetypes. In this paper we show the feasibility of archetypes application and the benefits of simulation. We developed a model and simulation of some aspects of security based on system dynamics principles. The system dynamics simulation model can be used in support of decision-making, training, and teaching regarding the mitigation of computer security risks. In this paper, we combine two archetypes and show the computer security relevance of such combinations. Presented are instances of the archetypes “Escalation”, in which an organization must continuously increase its efforts to counter additional attacker effort; and “Limits to Growth”, in which the gains from an organization’s security efforts plateau or decline due to its limited capacity for security-related tasks. We describe a scenario where these archetypes (individually and combined) can help in diagnosis and understanding, and present simulation of “what-if” scenarios suggesting how an organization might remedy these problems and maximize its gains from security efforts. 
80|10||Managing information security in a business network of machinery maintenance services business â Enterprise architecture as a coordination tool|Today, technologies enable easy access to information across organizational boundaries, also to systems of partners in business networks. This raises, however, several complex research questions on privacy, information security and trust. The study reported here provides motivation and a roadmap for approaching integrated security management solutions in a business network of partners with heterogeneous information and communication technologies (ICT): Systems, platforms, infrastructures as well as security policies. Enterprise architecture (EA) is proposed as a means for comprehensive and coordinated planning and management of corporate ICT and the security infrastructure. The EA approach is proposed as a pre-requisite for transparent and secure inter-organizational information exchange and business process support crossing corporate boundaries. This study provides an example of security architecture planning based on EA, which aligns the development of technological solutions with the business goals. The EA approach combines the planning of business and ICT developments. The alignment provides arguments for cohesive identity and access management (IAM) in a business network. A case study with Metso Paper, Inc., the leading manufacturer of paper machinery and related services, exemplifies the EA-based security architecture planning and specification. 
80|10||A software-based trust framework for distributed industrial management systems|One of the major problems in industrial security management is that most organizations or enterprises do not provide adequate guidelines or well-defined policy with respect to trust management, and trust is still an afterthought in most security engineering projects. With the increase of handheld devices, managers of business organizations tend to use handheld devices to access the information systems. However, the connection or access to an information system requires appropriate level of trust. In this paper, we present a flexible, manageable, and configurable software-based trust framework for the handheld devices of mangers to access distributed information systems. The presented framework minimizes the effects of malicious recommendations related to the trust from other devices or infrastructures. The framework allows managers to customize trust-related settings depending on network environments in an effort to create a more secure and functional network. To cope with the organizational structure of a large enterprise, within this framework, handheld devices of managers are broken down into different categories based upon available resources and desired security functionalities. The framework is implemented and applied to build a number of trust sensitive applications such as health care. 
80|10||Common defects in information security management system of Korean companies|To reduce the possible trials and errors while promoting the establishment and certification of the information security management system (ISMS) by enterprises is the purpose of this paper. To satisfy this purpose, this study presents the defects by item found during the certification process of the ISMS of a number of enterprises by government certification agency in Korea. As a result, by analyzing the derived defects, this paper has outlined the issues to be attended to among enterprises at each stage of the establishment of the ISMS. Furthermore, this study presents a reference model for conducting a self assessment, so that companies may be able to self verify the completeness of their establishment of the ISMS. The case study is also provided to prove the practical value of this study. 
80|10||Active and passive techniques for group size estimation in large-scale and dynamic distributed systems|This paper presents two solutions to a distributed statistic collection problem, called Group Size Estimation. These algorithms are intended for large-scale and dynamic distributed systems such as Grids, peer-to-peer overlays, etc. Each algorithm estimates (both in a one-shot and continuous manner) the number of non-faulty processes present in the global group. The first active scheme samples receipt times of gossip messages, while the second passive scheme calculates the density of process identifiers when hashed to a real interval. Our analysis, trace-driven simulation and deployment on a 33-node Linux cluster study and compare the latencies, scalability, and accuracy of these schemes. 
80|10||Collocation optimizations in an aspect-oriented middleware system|In distributed object-oriented systems, there are situations where client and server objects are deployed in the same address space. In such scenarios, it is possible to dispatch remote calls without having to transverse the infrastructure provided by the underlying communication middleware system and thus without incurring the overhead of using a networking loopback interface. Such optimizations are called collocation optimizations. In this paper we describe an implementation of collocation that is centered on aspect-oriented programming abstractions. This implementation provides high degrees of modularization, configurability and adaptability than current object-oriented support to collocation. The paper also presents results about the performance gains derived from the optimization proposed. 
80|10||A hierarchical key management scheme for secure group communications in mobile ad hoc networks|A mobile ad hoc network (MANET) is a kind of wireless communication infrastructure that does not have base stations or routers. Each node acts as a router and is responsible for dynamically discovering other nodes it can directly communicate with. However, when a message without encryption is sent out through a general tunnel, it may be maliciously attacked. In this paper, we propose a hierarchical key management scheme (HKMS) for secure group communications in MANETs. For the sake of security, we encrypt a packet twice. Due to the frequent changes of the topology of a MANET, we also discuss group maintenance in this paper. Finally, we conducted the security and performance analysis to compare the proposed scheme with Tseng et al.’s [Tseng, Y.-M., Yang, C.-C., Liao, D.-R., 2007. A secure group communication protocol for ad hoc wireless networks. In: Advances in Wireless Ad Hoc and Sensor Networks and Mobile Computing. Book Series Signal and Communication Technology. Springer] and Steiner et al.’s [Steiner, M., Tsudik, G., Waidner, M., 1998. CLIQUES: a new approach to group key agreement. In: Proceedings of the 18th IEEE International Conference on Distributed Computing System. Amsterdam, Netherlands, pp. 380–387] schemes. 
80|10||Energy efficient strategies for object tracking in sensor networks: A data mining approach|In recent years, a number of studies have been done on object tracking sensor networks (OTSNs) due to the wide applications. One important research issue in OTSNs is the energy saving strategy in considering the limited power of sensor nodes. The past studies on energy saving in OTSNs considered the object’s movement behavior as randomness. In some real applications, however, the object movement behavior is often based on certain underlying events instead of randomness completely. In this paper, we propose a novel data mining algorithm named TMP-Mine with a special data structure named TMP-Tree for efficiently discovering the temporal movement patterns of objects in sensor networks. To our best knowledge, this is the first work on mining the movement patterns associated with time intervals in OTSNs. Moreover, we propose novel location prediction strategies that utilize the discovered temporal movement patterns so as to reduce the prediction errors for energy savings. Through empirical evaluation on various simulation conditions and real dataset, TMP-Mine and the proposed prediction strategies are shown to deliver excellent performance in terms of scalability, accuracy and energy efficiency. 
80|10||Building intrusion pattern miner for Snort network intrusion detection system|In this paper, we enhance the functionalities of Snort network-based intrusion detection system to automatically generate patterns of misuse from attack data, and the ability of detecting sequential intrusion behaviors. To that, we implement an intrusion pattern discovery module which applies data mining technique to extract single intrusion patterns and sequential intrusion patterns from a collection of attack packets, and then converts the patterns to Snort detection rules for on-line intrusion detection. In order to detect sequential intrusion behavior, the Snort detection engine is accompanied with our intrusion behavior detection engine. Intrusion behavior detection engine will create an alert when a series of incoming packets match the signatures representing sequential intrusion scenarios. 
80|10||Discrete-time performance analysis of a congestion control mechanism based on RED under multi-class bursty and correlated traffic|Internet traffic congestion control using queue thresholds is a well known and effective mechanism. This motivates the stochastic analysis of a discrete-time queueing systems for the performance evaluation of the active queue management (AQM) based congestion control mechanism called Random Early Detection (RED) with bursty and correlated traffic using a two-state Markov-Modulated Bernoulli arrival process (MMBP-2) as the traffic source. A two-dimensional discrete-time Markov chain is introduced to model the RED mechanism for two traffic classes where each dimension corresponds to a traffic class with its own parameters. This mechanism takes into account the reduction of incoming traffic arrival rate due to packets dropped probabilistically with the drop probability increasing linearly with system contents. The stochastic analysis of the queue considered could be of interest for the performance evaluation of the RED mechanism for the multi-class traffic with short range dependent (SRD) traffic characteristics. The performance metrics including mean system occupancy, mean packet delay, packet loss probability and system throughput are computed from the analytical model for a dropping policy which is a function of the thresholds and maximum drop probability. Typical numerical results are included to illustrate the credibility of the proposed mechanism in the context of external bursty and correlated traffic. These results clearly demonstrate how different threshold settings can provide different trade-offs between loss probability and delay to suit different service requirements. The effects on various performance measures of changes in the input parameters and of burstiness and correlations exhibited by the arrival process are also presented. The model would be applicable to high-speed networks which use slotted protocols. 
80|10||SQUIRE: Sequential pattern mining with quantities|Discovering sequential patterns is an important problem for many applications. Existing algorithms find qualitative sequential patterns in the sense that only items are included in the patterns. However, for many applications, such as business and scientific applications, quantitative attributes are often recorded in the data, which are ignored by existing algorithms. Quantity information included in the mined sequential patterns can provide useful insight to the users.In this paper, we consider the problem of mining sequential patterns with quantities. We demonstrate that naive extensions to existing algorithms for sequential patterns are inefficient, as they may enumerate the search space blindly. To alleviate the situation, we propose hash filtering and quantity sampling techniques that significantly improve the performance of the naive extensions. Experimental results confirm that compared with the naive extensions, these schemes not only improve the execution time substantially but also show better scalability for sequential patterns with quantities. 
80|10||Efficiency study of the information flow mechanism enabling interworking of heterogeneous wireless systems|The efficient co-operation of heterogeneous wireless technologies is directly relevant to the 4G wireless systems. Key building block of such an environment is the signaling protocol that allows the bidirectional flow of information between network and user devices. This provides the motivation behind this paper that initially presents key aspects of such a signalling protocol in a ‘Beyond 3G system’ (B3G) or otherwise known as a composite radio network. On the basis of the described architecture of the B3G system, simple simulation models for capturing the dynamics of the message exchanges are discussed. The simulation study allows the derivation of both qualitative and quantitative results, that provide insight on key characteristics of the composite environment. 
80|11|http://www.sciencedirect.com/science/journal/01641212/80/11|Composing pattern-based components and verifying correctness|Designing large software systems out of reusable components has become increasingly popular. Although liberal composition of reusable components saves time and expense, many experiments indicate that people will pay for this (liberal composition) sooner or later, sometimes paying even a higher price than the savings obtained from reusing components. Thus, we advocate that more rigorous analysis methods to check the correctness of component composition would allow combination problems to be detected early in the development process so that people can save the considerable effort of fixing errors downstream. In this paper we describe a rigorous method for component composition that can be used to solve combination and integration problems at the (architectural) design phase of the software development lifecycle. In addition, we introduce the notion of composition pattern in order to promote the reuse of composition solutions to solve routine component composition problems. Once a composition pattern is proven correct, its instances can be used in a particular application without further proof. In this way, our proposed method involves reusing compositions as well as reusing components. We illustrate our approach through an example related to the composition of design patterns as design components. Structural and behavioral correctness proofs about the composition of design patterns are provided. Case studies are also presented to show the applications of the composition patterns. 
80|11||Inconsistency of expert judgment-based estimates of software development effort|Expert judgment-based effort estimation of software development work is partly based on non-mechanical and unconscious processes. For this reason, a certain degree of intra-person inconsistency is expected, i.e., the same information presented to the same individual at different occasions sometimes lead to different effort estimates. In this paper, we report from an experiment where seven experienced software professionals estimated the same sixty software development tasks over a period of three months. Six of the sixty tasks were estimated twice. We found a high degree of inconsistency in the software professionals’ effort estimates. The mean difference of the effort estimates of the same task by the same estimator was as much as 71%. The correlation between the corresponding estimates was 0.7. Highly inconsistent effort estimates will, on average, be inaccurate and difficult to learn from. It is consequently important to focus estimation process improvement on consistency issues and thereby contribute to reduced budget-overruns, improved time-to-market, and better quality software. 
80|11||Modelling software development methodologies: A conceptual foundation|Current modelling approaches often purport to be based on a strong theoretical underpinning but, in fact, contain many ill-defined concepts or even contradictions leading to potential misinterpretation. Although much modelling in object-oriented contexts is focussed on the use of the Unified Modelling Language (UML), this paper presents a technology-agnostic approach that analyses the basic concepts of structural models and modelling in software engineering, using an approach based on representation theory. We examine the different kinds of interpretive mappings (either isotypical, prototypical or metatypical) that are required in order to trace model entities back to the SUS (subject under study) entities that they represent. The difference between forward- and backward-looking models is also explained, as are issues relating to the appropriate definition of modelling languages in general based on representation theory. The need for product and process integration in methodologies is then addressed, leading to the conclusion that a mesh of verbal plus nominal nodes is necessary in any methodology metamodel. Finally, the need for a common, cross-cutting modelling infrastructure is established, and a solution proposed in the form of an ontologically universal modelling language, OOLang. Examples of the application of these theoretical analyses to the suite of OMG products (particularly SPEM, UML and MOF) are given throughout, with the hope that awareness of the importance of a better modelling infrastructure can be developed. 
80|11||A component composition model providing dynamic, flexible, and hierarchical composition of components for supporting software evolution|Component composition is one of the practical and effective approaches for supporting software evolution. However, existing component composition techniques need to be complemented by advanced features which address various sophisticated composition issues. In this paper, we introduce a set of features that supports and manages dynamic as well as flexible composition of components in a controlled way. We also propose a component composition model that supports these features. The proposed model enables dynamic, flexible, and hierarchical composition of components by providing and manipulating dedicated composition information, which in turn increases reusability of components and capabilities for supporting software evolution. To show the benefits of our model concretely, we provide a Hotel Reservation System case study. The experimental results show that our model supports software evolution effectively and provides efficient and modular structures, refactoring, and collaboration-level extensions as well. 
80|11||Exploiting agents for modelling and simulation of coverage control protocols in large sensor networks|A sensor network is composed of low-cost, low-power nodes densely deployable over a (possibly in-hospitable) territory in order to monitor the state of the environment, e.g. temperature, sound, radiation and so forth. Sensors have the ability to self-organize into an interconnected network and to cooperate for collecting, aggregating and disseminating information to end users. Major challenges in dealing with sensor networks are the strong limitations imposed by finite onboard power capacity. This paper proposes a lightweight actor infrastructure that is well-suited to modelling and simulation of complex sensor networks and, more in general, of multi-agent systems. This infrastructure is exploited for designing and implementing an efficient actor-based distributed simulation model for studying specific aspects of large wireless sensor networks. The paper proposes and compares the performances of two protocols for the coverage control problem that achieve their objective as an emergent property. In particular, one of the two protocols adopts a novel approach based on an evolutionary game. Distributed simulation of the achieved actor-based models is characterized by good execution performances witnessed by reported experimental results. 
80|11||Virtual knowledge service marketâFor effective knowledge flow within knowledge grid|A knowledge service consists of systematic knowledge and the mechanism of using knowledge to perform a task. The supply of knowledge services forms a knowledge service layer over the knowledge flow network formed by free knowledge sharing. To stimulate the supply of knowledge services, this paper proposes a virtual knowledge service market by establishing reward and reputation mechanisms. Simulations demonstrate that a team with the market mechanism performs better than those without it. The virtual knowledge service market provides an experimental platform for exploring the rules of knowledge service such as the impact of individual behavior on states of individual and team as well as the change of the states. 
80|11||Performance evaluation of UML design with Stochastic Well-formed Nets|The paper presents a method to compute performance metrics (response time, sojourn time, throughput) on Unified Modeling Language design. The method starts with UML design annotated according to the UML Profile for Schedulability, Performance and Time. The UML design is transformed into a performance model where to compute the referred metrics. Being the performance model a Stochastic Well-formed Net, the method is enabled to analyze systems where the object identities are relevant as well as those where they are not. A complete case study reveals how to apply the method and its usefulness. 
80|11||TFRP: An efficient microaggregation algorithm for statistical disclosure control|Recently, the issue of statistic disclosure control (SDC) has attracted much attention. SDC is a very important part of data security dealing with the protection of databases. Microaggregation for SDC techniques is widely used to protect confidentiality in statistical databases released for public use. The basic problem of microaggregation is that similar records are clustered into groups, and each group contains at least k records to prevent disclosure of individual information, where k is a pre-defined security threshold. For a certain k, an optimal multivariable microaggregation has the lowest information loss. The minimum information loss is an NP-hard problem. Existing fixed-size techniques can obtain a low information loss with O(n2) or O(n3/k) time complexity. To improve the execution time and lower information loss, this study proposes the Two Fixed Reference Points (TFRP) method, a two-phase algorithm for microaggregation. In the first phase, TFRP employs the pre-computing and median-of-medians techniques to efficiently shorten its running time to O(n2/k). To decrease information loss in the second phase, TFRP generates variable-size groups by removing the lower homogenous groups. Experimental results reveal that the proposed method is significantly faster than the Diameter and the Centroid methods. Running on several test datasets, TFRP also significantly reduces information loss, particularly in sparse datasets with a large k. 
80|11||Safety analysis of software product lines using state-based modeling|The difficulty of managing variations and their potential interactions across an entire product line currently hinders safety analysis in safety-critical, software product lines. The work described here contributes to a solution by integrating product-line safety analysis with model-based development. This approach provides a structured way to construct state-based models of a product line having significant, safety-related variations and to systematically explore the relationships between behavioral variations and potential hazardous states through scenario-guided executions of the state model over the variations. The paper uses a product line of safety-critical medical devices to demonstrate and evaluate the technique and results. 
80|11||Six years of evaluating software architectures in student projects|Software architecture evaluations are an important decision support tool when developing software systems. It is thus important that they are conducted professionally and that the results are of high quality. In order to improve the quality, it is necessary for the participants to gain experience in conducting software architecture evaluations. In this article we present guidelines based on six years of experience in software architecture evaluations. Although we primarily focus on our experiences on software architecture evaluation in student projects, we have also applied the same method in industry with similar experiences. 
80|11||RDL: A language for framework instantiation representation|Reusing software artifacts for system development is showing increasing promise as an approach to reducing the time and effort involved in building new systems, and to improving the software development process and the quality of its outcome. However, software reuse has an associated steep learning curve, since practitioners must become familiar with a third party rationale for representing and implementing reusable assets. For this reason, enabling a systematic approach to the reuse process by making software reuse tasks explicit, allowing software frameworks to be instantiated using pre-defined primitive and complex reuse operations, and supporting the reuse process in a (semi-)automated way become crucial goals. In this paper, we present a systematic reuse approach and the Reuse Description Language (RDL), a language designed to specify object-oriented framework instantiation processes, and an RDL execution environment, which is the tool support for definition and execution of reuse processes and framework instantiations that lead to domain-specific applications. We illustrate our approach using DTFrame, a framework for creating drawing editors. 
80|11||Open standards, open formats, and open source|The paper proposes some comments and reflections on the notion of “openness” and on how it relates to three important topics: open standards, open formats, and open source. Often, these terms are considered equivalent and/or mutually implicated: “open source is the only way to enforce and exploit open standards”. This position is misleading, as it increases the confusion about this complex and extremely critical topic.The paper clarifies the basic terms and concepts. This is instrumental to suggest a number of actions and practices aiming at promoting and defending openness in modern ICT products and services. 
80|12|http://www.sciencedirect.com/science/journal/01641212/80/12|Special issue: International Conference on Pervasive Services (ICPS 2006)|
80|12||COCOA: COnversation-based service COmposition in pervAsive computing environments with QoS support|Pervasive computing environments are populated with networked services, i.e., autonomous software entities, providing a number of functionalities. One of the most challenging objectives to be achieved within these environments is to assist users in realizing tasks that integrate on the fly functionalities of the networked services opportunely according to the current pervasive environment. Towards this purpose, we present COCOA, a solution for COnversation-based service COmposition in pervAsive computing environments with QoS support. COCOA provides COCOA-L, an OWL-S based language for the semantic, QoS-aware specification of services and tasks, which further allows the specification of services and tasks conversations. Moreover, COCOA provides two mechanisms: COCOA-SD for the QoS-aware semantic service discovery and COCOA-CI for the QoS-aware integration of service conversations towards the realization of the user task’s conversation. The distinctive feature of COCOA is the ability of integrating on the fly the conversations of networked services to realize the conversation of the user task, by further meeting the QoS requirements of user tasks. Thereby, COCOA allows the dynamic realization of user tasks according to the specifics of the pervasive computing environment in terms of available services and by enforcing valid service consumption. 
80|12||The DYNAMOS approach to support context-aware service provisioning in mobile environments|To efficiently make use of information and services available in ubiquitous environments, mobile users need novel means for locating relevant content, where relevance has a user-specific definition. In the DYNAMOS project, we have investigated a hybrid approach that enhances context-aware service provisioning with peer-to-peer social functionalities. We have designed and implemented a system platform and application prototype running on smart phones to support this novel conception of service provisioning. To assess the feasibility of our approach in a real-world scenario, we conducted field trials in which the research subject was a community of recreational boaters. 
80|12||A comprehensive approach to model and use context for adapting applications in pervasive environments|With an increasing diversity of pervasive computing devices integrated in our surroundings and an increasing mobility of users, it will be important for computer systems and applications to be context-aware. Lots of works have already been done in this direction on how to capture context data and how to carry it to the application. Among the remaining challenges are to create the intelligence to analyze the context information and deduce the meaning out of it, and to integrate it into adaptable applications. Our work focuses on these challenges by defining generic context storage and processing model and by studying its impact on the application core. We propose a reusable context ontology model that is based on two levels: a generic level and a domain specific level. We propose a generic adaptation framework to guarantee adaptation of applications to the context in a pervasive computing environment. We also introduce a comprehensive adaptation approach that involves content adaptation and presentation adaptation inline with the adaptation of the core services of applications. Our case study shows that the context model and the application adaptation strategies provide promising service architecture. 
80|12||Situational computing: An innovative architecture with imprecise reasoning|
80|12||Granular best match algorithm for context-aware computing systems|In order to be context-aware, a system or application should adapt its behaviour according to current context, acquired by various context provision mechanisms. After acquiring current context, this information should be matched against the previously defined context sets. In this paper, a granular best match algorithm dealing with the subjective, fuzzy, multi-granular and multi-dimensional characteristics of contextual information is introduced. The CAPRA – Context-Aware Personal Reminder Agent tool is used to show the applicability of the new context matching algorithm. The obtained outputs showed that proposed algorithm produces the results which are more sensitive to the user’s intention, and more adaptive to the aforementioned characteristics of the contextual information than the traditional exact match method. 
80|12||A service provisioning system for distributed personalization with private data protection|
80|12||Enabling run-time composition and support for heterogeneous pervasive multi-agent systems|User needs-driven and computer-supported development of pervasive heterogeneous and dynamic multi-agent systems remains a great challenge for agent research community. This paper presents an innovative approach to composing, validating and supporting multi-agent systems at run-time. Multi-agent systems (MASs) can and should be assembled quasi-automatically and dynamically based on high-level user specifications which are transformed into a shared and common goal–mission. Dynamically generating agents could also be supported as a pervasive service. Heterogeneity of MASs refers to diverse functionality and constituency of the system which include mobile as well as host associated software agents. This paper proposes and demonstrates on-demand and just-in-time agent composition approach which is combined with run-time support for MASs. Run-time support is based on mission cost-efficiency and shared objectives which enable termination, generation, injection and replacement of software agents as the mission evolves at run-time. We present the formal underpinning of our approach and describe the prototype tool – called eHermes, which has been implemented using available agent platforms. Analysis and results of evaluating eHermes are presented and discussed. 
80|12||Round-Eye: A system for tracking nearest surrounders in moving object environments|This paper presents “Round-Eye”, a system for tracking nearest surrounding objects (or nearest surrounders) in moving object environments. This system provides a platform for surveillance applications. The core part of this system is continuous nearest surrounder (NS) query that maintains views of the nearest objects at distinct angles from query points. This query differs from conventional spatial queries such as range queries and nearest neighbor queries as NS query considers both distance and angular aspects of objects with respect to a query point at the same time. In our system framework, a centralized server is dedicated (1) to collect location updates of both objects and queries, (2) to determine which NS queries are invalidated in presence of object/query location changes and corresponding result changes if any, and (3) to refresh the affected query answers. To enhance the system performance in terms of processing time and network bandwidth consumption, we propose various techniques, namely, safe region, partial query reevaluation, and incremental query result update. Through simulations, we evaluate our system with the proposed techniques over a wide range of settings. 
80|2|http://www.sciencedirect.com/science/journal/01641212/80/2|Providing fault-tolerant authentication and authorization in wireless mobile IP networks|In wireless Mobile IP systems, the authentications and authorizations are performed by AAA (Authentication, Authorization, and Accounting) servers. An AAA server associates with a mobility agent to form an administrative domain. If an AAA server fails, all the mobile nodes (MNs) within the corresponding domain (failure-effected MNs) are unable to execute data services since their authentications and authorizations cannot be performed by the faulty AAA server. To tolerate the failure of the AAA server, this paper presents an efficient fault-tolerant approach. Once a failure is detected in an AAA server of an administrative domain, the proposed approach utilizes the AAA servers in other administrative domains to virtually generate a backup AAA server. To further reduce the fault-tolerant cost, the proposed approach additionally uses two techniques: preservation and tracking to assist the generation of the backup AAA server. Due to introducing these two techniques, the proposed approach does not need to retrieve the AAA records of failure-effected MNs while performing fault tolerance. Finally, we use M/G/c/c queuing model to analyze the effectiveness of the proposed approach over previous approaches. The analytical results are also validated by simulations. 
80|2||ID-based restrictive partially blind signatures and applications|Restrictive blind signatures allow a recipient to receive a blind signature on a message not known to the signer but the choice of message is restricted and must conform to certain rules. Partially blind signatures allow a signer to explicitly include necessary information (expiration date, collateral conditions, or whatever) in the resulting signatures under some agreement with receiver. Restrictive partially blind signatures incorporate the advantages of these two blind signatures. The existing restrictive partially blind signature scheme was constructed under certificate-based (CA-based) public key systems. In this paper we follow Brand’s construction to propose the first identity-based (ID-based) restrictive blind signature scheme from bilinear pairings. Furthermore, we first propose an ID-based restrictive partially blind signature scheme, which is provably secure in the random oracle model. As an application, we use the proposed signature scheme to build an untraceable off-line electronic cash system followed the Brand’s construction. 
80|2||Improvement of Yang et al.âs threshold proxy signature scheme|Since the first (t, n) threshold proxy signature scheme was proposed, the threshold proxy signature has enjoyed a considerable amount of interest from the cryptographic research community. In 2001, Hsu et al. proposed a non-repudiable threshold proxy signature scheme with known signer, but the efficiency is rather low and a system authority (SA) is also required in this scheme. Recently, to overcome these shortcomings, Yang et al. proposed an improvement of Hsu et al.’s scheme that is very efficient and without employing a SA. However, in this paper, we shows that Yang et al.’s scheme is not secure against the warrant attack. That is, the adversary can replace the warrant of the proxy signature. To resist this attack, based on Yang et al.’s scheme, we propose a new and more efficient scheme without a secure channel. 
80|2||Worm-IT â A wormhole-based intrusion-tolerant group communication system|This paper presents Worm-IT, a new intrusion-tolerant group communication system with a membership service and a view-synchronous atomic multicast primitive. The system is intrusion-tolerant in the sense that it behaves correctly even if some nodes are corrupted and become malicious. It is based on a novel approach that enhances the environment with a special secure distributed component used by the protocols to execute securely a few crucial operations. Using this approach, we manage to bring together two important features: Worm-IT tolerates the maximum number of malicious members possible; it does not have to detect the failure of primary-members, a problem in previous intrusion-tolerant group communication systems. 
80|2||GSR: A global seek-optimizing real-time disk-scheduling algorithm|Earliest-deadline-first (EDF) is good for scheduling real-time tasks in order to meet timing constraint. However, it is not good enough for scheduling real-time disk tasks to achieve high disk throughput. In contrast, although SCAN can maximize disk throughput, its schedule results may violate real-time requirements. Thus, during the past few years, various approaches were proposed to combine EDF and SCAN (e.g., SCAN-EDF and RG-SCAN) to resolve the real-time disk-scheduling problem. However, in previous schemes, real-time tasks can only be rescheduled by SCAN within a local group. Such restriction limited the obtained data throughput. In this paper, we proposed a new globally rescheduling scheme for real-time disk scheduling. First, we formulate the relations between the EDF schedule and the SCAN schedule of input tasks as EDF-to-SCAN mapping (ESM). Then, on the basis of ESM, we propose a new real-time disk-scheduling algorithm: globally seek-optimizing rescheduling (GSR) scheme. Different from previous approaches, a task in GSR may be rescheduled to anywhere in the input schedule to optimize data throughput. Owing to such a globally rescheduling characteristic, GSR obtains a higher disk throughput than previous approaches. Furthermore, we also extend the GSR to serve fairly non-real-time tasks. Experiments show that given 15 real-time tasks, our data throughput is 1.1 times that of RG-SCAN. In addition, in a mixed workload, compared with RG-SCAN, our GSR achieves over 7% improvement in data throughput and 33% improvement in average response time. 
80|2||New results on non-perfect sharing of multiple secrets|A non-perfect secret sharing scheme is a method to distribute a secret among a set of participants, in such a way that some qualified subsets of participants, pooling together their information, can reconstruct the secret, whereas, other subsets of participants may have some information about the secret. In this journal, Feng et al. (2005) [A new multi-secret image sharing scheme using Lagrange’s interpolation. The Journal of Systems and Software, 76 (3), 327–339] recently considered the situation in which there are many secrets to be shared among a set of participants, in such a way that each qualified subset of participant can reconstruct a different secret. They proposed a polynomial-based construction using as a main tool a particular sequence of participants, called a sharing-circle, and having the feature that the smaller the length of the sharing-circle, the smaller the size of the shares distributed to participants. They proposed a simple recursive algorithm to find a minimal length sharing-circle. Since their algorithm is exponential-time, they left the task of finding a better one as an open problem.In this paper we first answer their question, showing that a polynomial-time algorithm computing a minimal-length sharing-circle is unlikely to exist. Indeed, we prove that the problem of finding a sharing-circle having minimal length is NP-hard. Afterwards, we propose a construction which is simpler than the one of Feng et al. and that does not require the participants to perform polynomial interpolation in order to reconstruct the secrets. Moreover, our scheme distributes shares having a smaller size. 
80|2||Timed verification of the reliable adaptive multicast protocol|The uses of timed parameters in formalisms are important for providing realistic descriptions of distributed multimedia systems. We have developed Time-Estelle, an extended Estelle which is capable of doing so. Correct operations of this type of systems have stringent requirements for synchronisation of different entities or media data residing in a number of nodes possibly located very remotely from each other. Verification of formal specifications for such systems with time taken into consideration has been a subject of research. We have developed a method of verifying Time-Estelle specifications; it involves translating Time-Estelle specifications to Communicating Time Petri Nets which can then be verified by using the automated tool ORIS, with the dynamic behaviours of Estelle modules all supported. Using this verification method, this paper describes a timed verification of the Reliable Adaptive Multicast Protocol formally specified in Time-Estelle, and presents the results of the verification. Its contribution is that it represents a success in the use of a method in verifying a real-life protocol with timed properties specified formally. 
80|2||A case study in re-engineering to enforce architectural control flow and data sharing|Without rigorous software development and maintenance, software tends to lose its original architectural structure and become difficult to understand and modify. ArchJava, a recently proposed programming language which embeds a component-and-connector architectural specification within Java implementation code, offers the promise of preventing the loss of architectural structure. AliasJava, which can be used in conjunction with ArchJava, is an annotation system that extends Java to express how data is confined within, passed among, or shared between components and objects in a software system.We describe a case study in which we incrementally re-engineer an existing Java implementation to obtain an implementation which enforces the architectural control flow and data sharing. Building on results from similar case studies, we chose an application consisting of over 16,000 source lines of Java code and over 90 classes. We describe our process, the detailed steps involved (some of which can be automated), as well as some lessons learned and perceived limitations with the languages, techniques and tools we used. 
80|2||MDABench: Customized benchmark generation using MDA|This paper describes an approach for generating customized benchmark suites from a software architecture description following a Model Driven Architecture (MDA) approach. The benchmark generation and performance data capture tool implementation (MDABench) is based on widely used open source MDA frameworks. The benchmark application is modeled in UML and generated by taking advantage of the existing community-maintained code generation “cartridges” so that current component technology can be exploited. We have also tailored the UML 2.0 Testing Profile so architects can model the performance testing and data collection architecture in a standards compatible way. We then extended the MDA framework to generate a load testing suite and automatic performance measurement infrastructure. This greatly reduces the effort and expertise needed for benchmarking with complex component and Web service technologies while being fully MDA standard compatible. The approach complements current model-based performance prediction and analysis methods by generating the benchmark application from the same application architecture that the performance models are derived from. We illustrate the approach using two case studies based on Enterprise JavaBean component technology and Web services. 
80|2||Call for Papers:âSoftware Process and Product Measurementâ|
80|3|http://www.sciencedirect.com/science/journal/01641212/80/3|REBNITAâ05â1st International Workshop on Requirements Engineering for Business Need and IT Alignment|
80|3||Managing requirements for a US$1bn IT-based business transformation: New approaches and challenges|Managing uncertainty and complexity in IT based business transformation presents new requirements engineering (RE) challenges where requirements are not known up-front and business outcomes have to be delivered over time in dynamic alignment with market developments. Responding to these challenges necessitates new RE techniques that go beyond the traditional goals of completeness, correctness and consistency and focuses instead on business needs and IT alignment. Using the case of a major IT based business transformation in a large retail bank, this paper outlines new approaches being applied in practice and identifies areas for further RE research. 
80|3||Eliciting Web application requirements â an industrial case study|A small variety of methods and techniques are presented in the literature as solutions to manage requirements elicitation for Web applications. However, the existing state of the art is lacking research regarding practical functioning solutions that would match Web application characteristics. The main concern for this paper is how requirements for Web applications can be elicited. The Viewpoint-Oriented Requirements Definition method (VORD) is chosen for eliciting and formulating Web application requirements in an industrial case study. VORD is helpful because it allows structuring of requirements around viewpoints and formulating very detailed requirements specifications. Requirements were understandable to the client with minimal explanation but failed to capture the business vision, strategy, and daily business operations, and could not anticipate the changes in the business process as a consequence of introducing the Web application within the organisation. The paper concludes by a discussion of how to adapt and extend VORD to suit Web applications. 
80|3||FBCM: Strategy modeling method for the validation of software requirements|The Balanced Scorecard has attracted great attention as a modeling technique for enterprise management strategy. However, the strategy model is inadequate for examining the validity of software requirements, because methods to evaluate strategy models have yet to be developed. Therefore, this paper proposes a fact based collaboration modeling methodology (FBCM). Based on the results of field observations and data of business processes, it is possible to develop enterprise strategy models from the viewpoints of collaboration between organizations. This paper describes the basic concepts and procedures of the methodology. The methodology has been evaluated via a case study of developing an SCM strategy of a Japanese automobile enterprise. The research project was conducted for seven months to develop a strategy for a complete car logistic process in which five different departments of the company are involved. The results show the effectiveness of the proposed methodology. 
80|3||Requirements change: Fears dictate the must haves; desires the wonât haves|We attempt to contribute to a general theory of requirements change from a goal-oriented and viewpoints-driven angle. To practitioners, this knowledge is relevant to anticipate changes in certain types of requirements, which may shorten the project’s timeline, reduce costs, and increase product quality. Initially, we followed the common assumptions that what should be on a system is demanded by goals to achieve and what should not be on a system is demanded by goal states to avoid. However, requirements engineering of a diversity of systems (capacity and warehouse management, COTS PCs, and a Braille mouse) revealed that must requirements are predicted by goals to avoid (!) and won’t requirements by goals to approach (!). Expectations about the positive or negative impact (valence) of requirements on goals played a moderating role. We unfold the gradual discovery of this “goals-to-requirements chiasm” (CHI-effect or Ï-effect), claiming that variability in agreement to positive or negative requirements is predicted by goals of opposite polarity. We found that whether the Ï-effect occurred or not, depended on the alignment of stakeholder viewpoints on goals and requirements. Comments from practitioners are included.Categories & Subject DescriptorsH.1.2 [Models and Principles]: User/Machine Systems–Human information processing; K.6.3 [Management of Computing and Information Systems]: Software Management–Software development.General TermsRequirements Engineering, Human Factors, Theory. 
80|3||SEAL: A secure communication library for building dynamic group key agreement applications|We present the SEcure communicAtion Library (SEAL) [source can be downloaded from: http://www.cse.cuhk.edu.hk/~cslui/ANSRlab/software/SEAL/], a Linux-based C language application programming interface (API) library that implements secure group key agreement algorithms that allow a communication group to periodically renew a common secret group key for secure and private communication. The group key agreement protocols satisfy several important characteristics: distributed property (i.e., no centralized key server is needed), collaborative property (i.e., every group member contributes to the group key), and dynamic property (i.e., group members can join or leave the group without impairing the efficiency of the group key generation). Using SEAL, we developed a testing tool termed Gauger to evaluate the performance of the group key agreement algorithms in both wired and wireless LANs according to different levels of membership dynamics. We show that our implementation achieves robustness when there are group members leaving the communication group in the middle of a rekeying operation. We also developed a secure chat-room application termed Chatter to illustrate the usage of SEAL. Our SEAL implementation demonstrates the effectiveness of group key agreement in real network settings. 
80|3||Opportunistic prioritised clustering framework for improving OODBMS performance|In object oriented database management systems, clustering has proven to be one of the most effective performance enhancement techniques. Existing clustering algorithms are mainly static, that is re-clustering the object base when the database is off-line. However, this type of re-clustering cannot be used when 24-h database access is required. In such situations dynamic clustering is necessary, since it can re-cluster the object base while the database is in operation. We find that most existing dynamic clustering algorithms do not address the following important points: the use of opportunism to impose the smallest I/O footprint for re-organisation; the re-use of prior research on static clustering algorithms; and the prioritisation of re-clustering so that the worst clustered pages are re-clustered first. Our main achievement in this paper is to create the Opportunistic Prioritised Clustering Framework (OPCF). The framework allows any static clustering algorithm to be made dynamic. Most importantly it allows the created algorithm to have the properties of I/O opportunism and clustering prioritisation which are missing in most existing dynamic clustering algorithms. We have used OPCF to make the static clustering algorithms “Graph Partitioning” and “Probability Ranking Principle” into dynamic algorithms. In our simulation study we found these algorithms outperformed two existing highly competitive dynamic algorithms in a variety of situations. 
80|3||Self-certified signature scheme from pairings|To overcome key escrow problems and secure channel problems that seem to be inherent to identity-based cryptography, here we propose a self-certified signature scheme (SCS) from pairings on elliptic curves. We also give a security model, and further provide a security proof in random oracle model. The scheme incorporates the advantages of self-certified public keys and pairings. Besides, it can also provide an implicit as well as mandatory verification of public keys. 
80|3||The design and evaluation of path matching schemes on compressed control flow traces|A control flow trace captures the complete sequence of dynamically executed basic blocks and function calls. It is usually of very large size and therefore commonly stored in compressed format. On the other hand, control flow traces are frequently queried to assist program analysis and optimization, e.g. finding frequently executed subpaths that may be optimized. In this paper, we identify path interruption and path context problems in querying an intraprocedural path over control flow traces. While algorithms that perform pattern matching on compressed strings have been proposed, solving new challenges requires the extension of traditional algorithms. We design and evaluate four path matching schemes including those that match in the compressed data directly and those that match after decompression. In addition, simple indices are also designed to improve matching performance. Our experimental results show that these schemes are practical and can be adapted to environments with different hardware settings and path matching requests. 
80|3||An effective and efficient code generation algorithm for uniform loops on non-orthogonal DSP architecture|To meet ever-increasing demands for higher performance and lower power consumption, many high-end digital signal processors (DSPs) commonly employ non-orthogonal architecture. This architecture typically is characterized by irregular data paths, heterogeneous registers, and multiple memory banks. Moreover, sufficient compiler support is obviously important to harvest its benefits. However, usual compilation techniques do not adapt well to non-orthogonal architectures and the compiler design becomes much more difficult due to the complexity of these architectures. The entire code generation process for non-orthogonal architecture must include several phases. In this paper, we extend our previous study to propose a code generation algorithm Rotation Scheduling with Spill Codes Avoiding (RSSA), which is suitable for various DSPs with similar architectural features. As well as introducing detailed principles and algorithms of RSSA, we select several DSP applications and evaluate it under Motorola DSP56000 architectures. The evaluation results clearly demonstrate the effectiveness of RSSA, which can obtain scheduling results with minimum length and fewer spill codes compared to related work. In addition, in order to study the influence of different number of resources on the scheduling result, we also define a hypothetical machine model to represent a scalable non-orthogonal DSP architecture. After evaluating RSSA on various target architectures, we find that adding additional accumulators is the most efficient way to reduce spill codes. Meanwhile, for instruction-level parallelism exploration, numbers of data ALUs and accumulators have to be concurrently increased. Furthermore, based on our analysis, RSSA is not only effective but also quite efficient compared to related studies. 
80|3||A novel data hiding scheme for color images using a BSP tree|In this paper, we propose a novel data hiding technique for color images using a BSP (Binary Space Partitioning) tree. First, we treat the RGB values at each pixel as a three-dimensional (3D) virtual point in the XYZ coordinates and a bounding volume is employed to enclose them. Using predefined termination criteria, we construct a BSP tree by recursively decomposing the bounding volume into voxels containing one or several 3D virtual points. The voxels are then further categorized into eight subspaces, each of which is numbered and represented as three-digit binary characters. In the embedding process, we first traverse the BSP tree, locating a leaf voxel; then we embed every three bits of the secret message into the points inside the leaf voxel. This is realized by translating a point’s current position to the corresponding numbered subspace. Finally, we transform the data-embedded 3D points to the stego color image. Our technique is a blind extraction scheme, where embedded messages can be extracted without the aid of the original cover image. It achieves high data capacity, equivalent to at least three times the number of pixels in the cover image. The stego image causes insignificant visual distortion under this high data capacity embedding scheme. In addition, we can take advantage of the properties of tree data structure to improve the security of the embedding process, making it difficult to extract the secret message without the secret key provided. Finally, when we adaptively modify the thresholds used to construct the BSP tree, our technique can be robust against attacks including image cropping, pixel value perturbation, and pixel reordering. But, the scheme is not robust against image compression, blurring, scaling, sharpening, and rotation attacks. 
80|4|http://www.sciencedirect.com/science/journal/01641212/80/4|Editorial|
80|4||Ensuring system performance for cluster and single server systems|A new approach that is useful in identifying and eliminating performance degradation occurring in aging software is proposed. A customer-affecting metric is used to initiate the restoration of such a system to full capacity. A case study is described in which, by simulating an industrial software system, we are able to show that by monitoring a customer-affecting metric and frequently comparing its degradation to the performance objective, we can ensure system stability at a very low cost. 
80|4||Model-based system reconfiguration for dynamic performance management|Recently, growing attention focused on run-time management of Quality of Service (QoS) of complex software systems. In this context, system reconfiguration is considered a useful technique to manage QoS. Several reconfiguration approaches to performance management exist that help systems to maintain performance requirements at run time. However, many of them use prefixed strategies that are in general coded in the application or in the reconfiguration framework.In this work we propose a framework to manage performance of software systems at run time based on monitoring and model-based performance evaluation. The approach makes use of software architectures as abstractions of the managed system to avoid unnecessary details that can heavily affect the model evaluation in terms of complexity and resolution time. 
80|4||Interaction tree algorithms to extract effective architecture and layered performance models from traces|Models of software architecture and software performance both depend on identifying and describing the interactions between the components, during typical responses. This work identifies the components and interactions that are active during a tracing experiment, hence the name “effective architecture” and also derives layered performance models. The System Architecture and Model Extraction Technique (SAMEtech) described here overcomes a weakness of previous work with “angio traces” in two ways. It only requires standard trace formats (rather than a custom format which captures causality) and it uses a simpler algorithm which scales up linearly for very large traces. It accepts some limitations: components must not have internal parallelism with forking and joining of the flow of execution. SAMEtech uses pattern matching based on “interaction trees” for detecting various types of interactions (asynchronous, blocking synchronous, nested synchronous, and forwarding). With this information it builds architecture and performance models. 
80|4||Quantifying software performance, reliability and security: An architecture-based approach|With component-based systems becoming popular and handling diverse and critical applications, the need for their thorough evaluation has become very important. In this paper we propose an architecture-based unified hierarchical model for software performance, reliability, security and cache behavior prediction. We employ discrete time Markov chains (DTMCs) to model software systems and provide expressions for predicting the overall behavior of the system based on its architecture as well as the characteristics of individual components. This approach also facilitates the identification of various bottlenecks. We illustrate its use through some case studies and also provide expressions to perform sensitivity analysis. 
80|4||Efficient performance models for layered server systems with replicated servers and parallel behaviour|Capacity planning for large computer systems may require very large performance models, which are difficult or slow to solve. Layered queueing models solved by mean value analysis can be scaled to dozens of servers and hundreds of service classes, with large class populations, but this may not be enough. A common feature of planning models for large systems is structural repetition expressed through replicated subsystems, which can provide both scalability and reliability, and this replication can be exploited to scale the solution technique. A model has recently been described for symmetrically replicated layered servers, and their integration into the system, with a mean-value solution approximation. However, parallelism is often combined with replication; high-availability systems use parallel data-update operations on redundant replicas, to enhance reliability, and grid systems use parallel computations for scalability. This work extends the replicated layered server model to systems with parallel execution paths. Different servers may be replicated to different degrees, with different relationships between them. The solution time is insensitive to the number of replicas of each replicated server, so systems with thousands or even millions of servers can be modelled efficiently. 
80|4||Filling the gap between design and performance/reliability models of component-based systems: A model-driven approach|To facilitate the use of non-functional analysis results in the selection and assembly of components for component-based systems, automatic prediction tools should be devised, to predict some overall quality attribute of the application without requiring extensive knowledge of analysis methodologies to the application designer. To achieve this goal, a key idea is to define a model transformation that takes as input some “design-oriented” model of the component assembly and produces as a result an “analysis-oriented” model that lends itself to the application of some analysis methodology. However, to actually devise such a transformation, we must face both the heterogeneous design level notations for component-based systems, and the variety of non-functional attributes and related analysis methodologies we could be interested in. To tackle these problems, we define a model-driven transformation framework, centered around a kernel language whose aim is to capture the relevant information for the analysis of non-functional attributes of component-based systems, with a focus on performance and reliability. Using this kernel language as a bridge between design-oriented and analysis-oriented notations we reduce the burden of defining a variety of direct transformations from the former to the latter to the less complex problem of defining transformations to/from the kernel language. The proposed kernel language is defined within the MOF (Meta-Object Facility) framework, to allow the exploitation of MOF-based model transformation facilities. 
80|4||Defect prevention in software processes: An action-based approach|In addition to degrading the quality of software products, software defects also require additional efforts in rewriting software and jeopardize the success of software projects. Software defects should be prevented to reduce the variance of projects and increase the stability of the software process. Factors causing defects vary according to the different attributes of a project, including the experience of the developers, the product complexity, the development tools and the schedule. The most significant challenge for a project manager is to identify actions that may incur defects before the action is performed. Actions performed in different projects may yield different results, which are hard to predict in advance. To alleviate this problem, this study proposes an Action-Based Defect Prevention (ABDP) approach, which applies the classification and Feature Subset Selection (FSS) technologies to project data during execution.Accurately predicting actions that cause many defects by mining records of performed actions is a challenging task due to the rarity of such actions. To address this problem, the under-sampling is applied to the data set to increase the precision of predictions for subsequence actions. To demonstrate the efficiency of this approach, it is applied to a business project, revealing that under-sampling with FSS successfully predicts the problematic actions during project execution. The main advantage utilizing ABDP is that the actions likely to produce defects can be predicted prior to their execution. The detected actions not only provide the information to avoid possible defects, but also facilitate the software process improvement. 
80|4||Lessons from applying the systematic literature review process within the software engineering domain|A consequence of the growing number of empirical studies in software engineering is the need to adopt systematic approaches to assessing and aggregating research outcomes in order to provide a balanced and objective summary of research evidence for a particular topic. The paper reports experiences with applying one such approach, the practice of systematic literature review, to the published studies relevant to topics within the software engineering domain. The systematic literature review process is summarised, a number of reviews being undertaken by the authors and others are described and some lessons about the applicability of this practice to software engineering are extracted.The basic systematic literature review process seems appropriate to software engineering and the preparation and validation of a review protocol in advance of a review activity is especially valuable. The paper highlights areas where some adaptation of the process to accommodate the domain-specific characteristics of software engineering is needed as well as areas where improvements to current software engineering infrastructure and practices would enhance its applicability. In particular, infrastructure support provided by software engineering indexing databases is inadequate. Also, the quality of abstracts is poor; it is usually not possible to judge the relevance of a study from a review of the abstract alone. 
80|4||The WhenâWhoâHow analysis of defects for improving the quality control process|Most large software products have elaborate quality control processes involving many tasks performed by different groups using a variety of techniques. The defects found are generally recorded in a database which is used for tracking and prioritizing defects. However, this defect data also provides a wealth of information which can be analyzed for improving the process. In this paper, we describe the when–who–how approach for analyzing defect data to gain a better understanding of the quality control process and identify improvement opportunities. At the component level, the analysis provides the capability to assess strength of dependency between components, and new ways to study correlation between early and late defects. We also discuss the experience of applying this approach to defect data from an earlier version of Windows, and the improvement opportunities it revealed. 
80|4||Modeling and analysis of software aging and software failure|Many studies reported that system suffered from outages more due to software faults than hardware faults. Recently, the phenomenon of “software aging”, which was caused by aging-related faults, is observed in many software systems. Software aging, characterized by progressive performance degradation, is mainly caused by exhaustion of the operating system resources, such as memory leaking, unreleased-file locks, data corruption, etc. This paper mainly focuses on the modeling and analysis of software aging and software failure. A stochastic time series decomposition algorithm based on robust locally weighted regression (Loess) is presented to separate the exhaustion of system resource from the resource usage, from which aging trend is estimated. Then the model of software aging and software failure process is constructed. Experiments on a practical server system verify the effectiveness of the algorithm presented in this paper, and the two-stage failure process is also confirmed for the first time in the history of research on software aging. The conclusions drawn from this paper will greatly benefit the application of software rejuvenation technique, that is, it makes it easy to determine when to perform software rejuvenation, which is a key issue in implementation of software rejuvenation. The results for the server system employing different rejuvenation policies show that software performance can be effectively improved. 
80|4||Software development risk and project performance measurement: Evidence in Korea|
80|4||Neural-network-based approaches for software reliability estimation using dynamic weighted combinational models|Software reliability is the probability of failure-free software operation for a specified period of time in a specified environment. During the last three decades, many software reliability growth models (SRGMs) have been proposed and analyzed for measuring software reliability growth. SRGMs are mathematical models that represent software failures as a random process and can be used to evaluate development status during testing. However, most of SRGMs depend on some assumptions or distributions. In this paper, we propose an artificial neural-network-based approach for software reliability estimation and modeling. We first explain the neural networks from the mathematical viewpoints of software reliability modeling. We will show how to apply neural network to predict software reliability by designing different elements of neural networks. Furthermore, we will use the neural network approach to build a dynamic weighted combinational model (DWCM). The applicability of proposed model is demonstrated through real software failure data sets. The results obtained from the experiments show that the proposed model has a fairly accurate prediction capability. 
80|4||Software quality and IS project performance improvements from software development process maturity and IS implementation strategies|
80|4||The adjusted analogy-based software effort estimation based on similarity distances|Analogy-based estimation is a widely adopted problem solving method that has been evaluated and confirmed in software effort or cost estimation domains. The similarity measures between pairs of projects play a critical role in the analogy-based software effort estimation models. Such a model calculates a distance between the software project being estimated and each of the historical software projects, and then retrieves the most similar project for generating an effort estimate. Although there exist numerous analogy-based software effort estimation models in literature, little theoretical or experimental works have been reported on the method of deriving an effort estimate from the adjustment of the reused effort based on the similarity distance. The present paper investigates the effect on the improvement of estimation accuracy in analogy-based estimations when the genetic algorithm method is adopted to adjust reused effort based on the similarity distances between pairs of projects. The empirical results show that applying a suitable linear model to adjust the analogy-based estimations is a feasible approach to improving the accuracy of software effort estimates. It also demonstrates that the proposed model is comparable with those obtained when using other effort estimation methods. 
80|5|http://www.sciencedirect.com/science/journal/01641212/80/5|Guest Editorial|
80|5||Component-based hardware/software co-verification for building trustworthy embedded systems|
80|5||The SAVE approach to component-based development of vehicular systems|The component-based strategy aims at managing complexity, shortening time-to-market, and reducing maintenance requirements by building systems with existing components. The full potential of this strategy has not yet been demonstrated for embedded software, mainly because of specific requirements in the domain, e.g., those related to timing, dependability, and resource consumption.We present SaveCCT – a component technology intended for vehicular systems, show the applicability of SaveCCT in the engineering process, and demonstrate its suitability for vehicular systems in an industrial case-study. Our experiments indicate that SaveCCT provides appropriate expressiveness, resource efficiency, analysis and verification support for component-based development of vehicular software. 
80|5||The design and performance of component middleware for QoS-enabled deployment and configuration of DRE systems|Quality of Service (QoS)-enabled component middleware can help reduce the complexity of deploying and configuring QoS aspects, such as priorities and rates of invocation. Few empirical studies have been conducted, however, to guide developers of distributed real-time and embedded (DRE) systems in choosing among alternative designs and performance optimizations. Moreover, few empirical studies have been conducted to examine the performance and flexibility trade-offs between standards-based and domain-specific DRE middleware solutions.This paper makes three key contributions to research on QoS-enabled component middleware for DRE systems. First, it describes optimizations applied to an implementation of the OMG’s Deployment and Configuration (D&C) of Components specification that enable performance trade-offs between QoS aspects of DRE systems. Second, it compares the performance of several dynamic and static configuration mechanisms to help guide the selection of suitable configuration mechanisms based on specific DRE system requirements. Third, it compares the performance of our static standards-based approach to an avionics domain-specific approach. Our results show that these optimizations (1) provide developers improved control over key trade-offs between flexibility and performance at different stages of the DRE system lifecycle, (2) enhance trustworthiness of component-based DRE systems by supporting greater customization of how they are configured to meet specific requirements of each application, and (3) offer greater flexibility at a reasonable performance cost, compared to a domain-specific approach. 
80|5||Early quality prediction of component-based systems â A generic framework|Component-based software engineering is currently an emerging technology used to develop complex embedded systems. These embedded systems need to fulfil requirements regarding quality attributes such as safety, reliability, availability, maintainability, performance, security and temporal correctness. Since quality problems should be identified and tackled early in the development process, there is a rising need to predict and evaluate these properties in the architecture design phase. This paper describes a generic framework for predicting quality properties based on component-based architectures, which is derived from a comprehensive study of recent architecture evaluation methods. This generic framework defines common aspects between the different evaluation methods and enables the improvement of evaluation methods for specific quality properties, by transferring knowledge from one quality domain to the other. Thus, this paper can help to create better evaluation methods in the future. 
80|5||CAmkES: A component model for secure microkernel-based embedded systems|Component-based software engineering promises to provide structure and reusability to embedded-systems software. At the same time, microkernel-based operating systems are being used to increase the reliability and trustworthiness of embedded systems. Since the microkernel approach to designing systems is partially based on the componentisation of system services, component-based software engineering is a particularly attractive approach to developing microkernel-based systems. While a number of widely used component architectures already exist, they are generally targeted at enterprise computing rather than embedded systems. Due to the unique characteristics of embedded systems, a component architecture for embedded systems must have low overhead, be able to address relevant non-functional issues, and be flexible to accommodate application specific requirements. In this paper we introduce a component architecture aimed at the development of microkernel-based embedded systems. The key characteristics of the architecture are that it has a minimal, low-overhead, core but is highly modular and therefore flexible and extensible. We have implemented a prototype of this architecture and confirm that it has very low overhead and is suitable for implementing both system-level and application level services. 
80|5||Intrusion detection aware component-based systems: A specification-based framework|Component-Based Software Engineering (CBSE) increases the reusability of software and hence decreases software development time and cost. Unfortunately, developing components for maximum reusability and acquiring third party components invite many security related concerns. The security related issues are more crucial for embedded and real-time systems. Currently, many approaches are proposed to aid the development and evaluation of secure components. However, it is well known among practitioners that, like any other software entities, components cannot be completely secure. This fact leads us to incorporate intrusion detection facilities to equip components with mechanisms to discover intrusions against components. In this paper, we present a framework for developing components with intrusion detection capabilities. This framework uses UMLintr, a UML profile for intrusion specifications. The profile allows developers to specify intrusion scenarios using UML diagrams. Specifying intrusion scenarios using the same language that is used for specifying software behavior eliminates the need for separate languages for describing intrusions. Other software specification languages can be easily adopted into this framework. The outcome of this framework are components equipped with intrusion detectors. Based on UMLintr, a prototype is built and used to generate signatures for some intrusions included in the benchmark DARPA attack datasets. Furthermore, we describe an Intrusion Detection System (IDS) which uses these signatures to detect component intrusions. 
80|5||A skewed distributed indexing for skewed access patterns on the wireless broadcast|Data broadcast is an efficient way to disseminate information to a large number of mobile clients in the wireless environment. Adding an index data organization to the broadcast file can save client power consumption with little increase in client waiting time. The existing index technologies only consider equal access probabilities of data items. However, in real-life applications, some data items may be more popular than others; that is, access patterns of clients are skewed. In this paper, we propose a skewed distributed indexing, SDI, which considers the access probabilities of data items and the replication of index nodes. The proposed algorithm traverses an index tree to determine whether an index node should be replicated by considering the access probability of its child node. In our experimental results, we have shown that our proposed algorithm outperforms the variant-fanout index tree and the distributed indexing. 
80|5||Task allocation for maximizing reliability of a distributed system using hybrid particle swarm optimization|In a distributed computing system, a number of program modules may need to be allocated to different processors such that the reliability of executing successfully these modules is maximized and the constraints with limited resources are satisfied. The problem of finding an optimal task allocation with maximum system reliability has been shown to be NP-hard; thus, existing approaches to finding exact solutions are limited to the use in problems of small size. This paper presents a hybrid particle swarm optimization (HPSO) algorithm for finding the near-optimal task allocation within reasonable time. The experimental results show that the HPSO is robust against different problem size, task interaction density, and network topology. The proposed method is also more effective and efficient than a genetic algorithm for the test-cases studied. The convergence and the worst-case characteristics of the HPSO are addressed using both theoretical and empirical analysis. 
80|5||Serfs: Dynamically-bound parameterized components|
80|5||Taking advantages of a disadvantage: Digital forensics and steganography using document metadata|All the information contained in a plain-text document are visible to everybody. On the other hand, compound documents using opaque formats, like Microsoft Compound Document File Format, may contain undisclosed data such as authors name, organizational information of users involved, previously deleted text, machine related information, and much more. Those information could be exploited by third party for illegal purposes. Computer users are unaware of the problem and, even though the Internet offers several tools to clean hidden data from documents, they are not widespread. Furthermore, there is only one paper about this problem in scientific literature, but there is no detailed analysis.In this paper we fill the gap, analyzing the problem with its causes and then we show how to take advantage of this issue: we show how hidden data may be extracted to gain evidence in forensic environment where even a small piece of information may be relevant and we also introduce a new stegosystem especially designed for Microsoft Office documents. We developed FTA, a tool to improve forensic analysis of Microsoft Office documents, and StegOlè, another tool that implements a new stegosystem for Microsoft Office documents. This is the first scientific paper to address the problem from both a steganographic and a forensic point of view. 
80|5||Distributed service provision using open APIs-based middleware: âOSA/Parlay vs. JAINâ performance evaluation study|Demand for value added services creates significant revenue opportunities for operators that decide to open their networks to external service providers. This process is practically feasible only if standardized interfaces to core network operations are utilized. This way the cost and complexity of the development and integration process is reduced, as well as the service administration overhead, which is distributed among service providers and does not burden the network operator. Most existing solutions for standardized access to core network operations follow the middleware paradigm that encapsulates network operations and provides open interfaces to outside service providers. The main concern about this approach has always been the performance impact of the additional architectural layers on top of the core network architecture. Different middleware technologies such as distributed object technologies and messaging middleware have been proposed and applied, all with varying performance results. The scope of this paper is to investigate and compare the performance impact inflicted by two such solutions encapsulating network call control functions. The first one is an OSA/Parlay based call control implementation with CORBA and RMI serving as underlying distributed object technologies and the second one is a JAIN call control implementation, again with an RMI underlying messaging framework. All performance measurements are taken in the context of a real-time “call forwarding” service translated from a corresponding VoIP SIP-based implementation. 
80|5||Exploiting idle cycles to execute data mining applications on clusters of PCs|In this paper we present and evaluate Inhambu, a distributed object-oriented system that supports the execution of data mining applications on clusters of PCs and workstations. This system provides a resource management layer, built on the top of Java/RMI, that supports the execution of the data mining tool called Weka. We evaluate the performance of Inhambu by means of several experiments in homogeneous, heterogeneous and non-dedicated clusters. The obtained results are compared with those achieved by a similar system named Weka-Parallel. Inhambu outperforms its counterpart for coarse grain applications, mainly for heterogeneous and non-dedicated clusters. Also, our system provides additional advantages such as application checkpointing, support for dynamic aggregation of hosts to the cluster, automatic restarting of failed tasks, and a more effective usage of the cluster. Therefore, Inhambu is a promising tool for efficiently executing real-world data mining applications. The software is delivered at the project’s web site available at http://incubadora.fapesp.br/projects/inhambu/. 
80|5||Security problems with improper implementations of improved FEA-M|
80|6|http://www.sciencedirect.com/science/journal/01641212/80/6|Web error classification and analysis for reliability improvement|In this paper, we adapt an existing defect classification and analysis framework, orthogonal defect classification (ODC), to analyze web errors and identify problematic areas for focused reliability improvement. Based on information extracted from existing web server logs, web errors are classified according to their response code, file type, referrer type, agent type, and observation time. We also introduce an analysis procedure to identify high-risk/high-leverage sub-classes of problems and consolidate analysis results to recommend appropriate followup actions. Results applying our approach to the www.seas.smu.edu and www.kde.org web sites are included to demonstrate its applicability and effectiveness. 
80|6||Assessing the validity of one-part software reliability models using likelihood ratio and early detection tests|To achieve higher accuracy in software failure rate estimates in a field environment, software reliability models that were fit from failure data collected during the test interval are often refit using the combined test interval and field interval data. The validity of this analysis depends on the test and field environments being compatible with respect to the manner in which the software is used. In this paper, we formulate the hypothesis of compatible test and field environments in terms of a statistical hypothesis and develop an appropriate test procedure. The test procedure is illustrated by applying it to a real-life software project. 
80|6||Modeling software testing costs and risks using fuzzy logic paradigm|The overall lifecycle cost associated with product failures exceeds 10% of yearly corporations’ turnover. A major factor contributing to this loss is ineffective performance of software and systems Verification, Validation and Testing (VVT). Given these realities, we proposed a set of quantitative probabilistic models for estimating costs and risks stemming from carrying out any given VVT strategy [Engel, A., Barad, M., 2003. A methodology for modeling VVT risks and costs. Systems Engineering Journal 6 (3), 135–151, Wiley InterScience, Online ISSN: 1520-6858, Print ISSN: 1098-1241]. We also demonstrated that quality costs in software-intensive projects are likely to consume as much as 60% of the development budget. Finally, we showed that project cost and duration could be reduced by optimizing the VVT strategy, yielding about 10–15% reduction in development costs and project schedule [Engel, A., Shachar, S., 2006. Measuring and optimizing systems’ quality costs and project duration. Systems Engineering Journal 9 (3), 259–280].A key problem associated with such cost and time estimates is that input data are imprecise by nature. Certain parameters are better captured using tuple structures (e.g. Minimum, Most-likely and Maximum values). Other parameters can be better encapsulated using linguistic terms such as “High” or “Low”. This paper extends the above research by modeling the problem using the fuzzy logic paradigm. We estimate the quality cost occurring during the development of software for an avionic suite in a fighter aircraft and demonstrate that applying fuzzy logic methodology yields results comparable to estimations based on models using the probabilistic paradigm (less than 4% differences in each of the five VVT cost categories). 
80|6||Institutionalization of software product line: An empirical investigation of key organizational factors|A good fit between the person and the organization is essential in a better organizational performance. This is even more crucial in case of institutionalization of a software product line practice within an organization. Employees’ participation, organizational behavior and management contemplation play a vital role in successfully institutionalizing software product lines in a company. Organizational dimension has been weighted as one of the critical dimensions in software product line theory and practice. A comprehensive empirical investigation to study the impact of some organizational factors on the performance of software product line practice is presented in this work. This is the first study to empirically investigate and demonstrate the relationships between some of the key organizational factors and software product line performance of an organization. The results of this investigation provide empirical evidence and further support the theoretical foundations that in order to institutionalize software product lines within an organization, organizational factors play an important role. 
80|6||The impacts of software product management|The success of any product depends on the skills and competence of the product manager. This article evaluates the relevance of good product management on the success of a product. The empirical study is supported by data from 178 industry projects from telecommunication industry over a period of three years throughout which the product management role and competency was defined, deployed and improved. The behaviors and project performance before and after strengthening the product management discipline are compared. We found that with increasing institutionalization of a consistent and empowered product management role, the success rate of projects in terms of schedule predictability, quality and project duration improves. To allow better transfer of achieved results to other settings, the article provides concrete guidelines about key success factors for good product management. 
80|6||Control and data flow structural testing criteria for aspect-oriented programs|Although it is claimed that, among other features, aspect-oriented programming (AOP) increases understandability and eases the maintenance burden, this technology cannot provide correctness by itself, and thus it also requires the use of systematic verification, validation and testing (VV&T) approaches. With the purpose of producing high quality software, many approaches to apply structural testing criteria for the unit testing of procedural and object-oriented (OO) programs have been proposed. Nevertheless, until now, few works have addressed the application of such criteria to test aspect-oriented programs. In this paper we define a family of control flow and data flow based testing criteria for aspect-oriented programs inspired by the implementation strategy adopted by AspectJ – an aspect-oriented extension of the Java language – and extending a previous work proposed for Java programs. We propose the derivation of a control and data flow model for aspect-oriented programs based upon the static analysis of the object code (the Java bytecode) resulted from the compilation/weaving process. Using this model, called aspect-oriented def-use graph (AODUAODU), traditional and also aspect-oriented testing criteria are defined (called Control and Data Flow Structural Testing Criteria for Aspect-Oriented Programs – CDSTC-AOP). The main idea is that composition of aspect-oriented programs leads to new crosscutting interfaces in several modules of the system, which must be considered for coverage during structural testing. The implementation of a prototype tool – the JaBUTi/AJ tool – to support the proposed criteria and model is presented along with an example. Also, theoretical and practical questions regarding the CDSTC-AOP criteria are discussed. 
80|6||An exploratory study of why organizations do not adopt CMMI|This paper explores why organizations do not adopt CMMI (Capability Maturity Model Integration), by analysing two months of sales data collected by an Australian company selling CMMI appraisal and improvement services. The most frequent reasons given by organizations were: the organization was small; the services were too costly, the organization had no time, and the organization was using another SPI approach. Overall, we found small organizations not adopting CMMI tend to say that adopting it would be infeasible, but do not say it would be unbeneficial. We comment on the significance of our findings and research method for SPI research. 
80|6||Understanding failure response in service discovery systems|Service discovery systems enable distributed components to find each other without prior arrangement, to express capabilities and needs, to aggregate into useful compositions, and to detect and adapt to changes. First-generation discovery systems can be categorized based on one of three underlying architectures and on choice of behaviors for discovery, monitoring, and recovery. This paper reports a series of investigations into the robustness of designs that underlie selected service discovery systems. The paper presents a set of experimental methods for analysis of robustness in discovery systems under increasing failure intensity. These methods yield quantitative measures for effectiveness, responsiveness, and efficiency. Using these methods, we characterize robustness of alternate service discovery architectures and discuss benefits and costs of various system configurations. Overall, we find that first-generation service discovery systems can be robust under difficult failure environments. This work contributes to better understanding of failure behavior in existing discovery systems, allowing potential users to configure deployments to obtain the best achievable robustness at the least available cost. The work also contributes to design improvements for next-generation service discovery systems. 
80|6||A rationale-based architecture model for design traceability and reasoning|Large systems often have a long life-span and comprise many intricately related elements. The verification and maintenance of these systems require a good understanding of their architecture design. Design rationale can support such understanding but it is often undocumented or unstructured. The absence of design rationale makes it much more difficult to detect inconsistencies, omissions and conflicts in an architecture design. We address these issues by introducing a rationale-based architecture model that incorporates design rationale, design objects and their relationships. This model provides reasoning support to explain why design objects exist and what assumptions and constraints they depend on. Based on this model, we apply traceability techniques for change impact analysis and root-cause analysis, thereby allowing software architects to better understand and reason about an architecture design. In order to align closely with industry practices, we choose to represent the rationale-based architecture model in UML. We have implemented a tool-set to support the capture and the automated tracing of the model. As a case study, we apply this approach to an real-world electronic payment system. 
80|7|http://www.sciencedirect.com/science/journal/01641212/80/7|Guest Editorâs introduction|
80|7||FC-ORB: A robust distributed real-time embedded middleware with end-to-end utilization control|A key challenge for distributed real-time and embedded (DRE) middleware is maintaining both system reliability and desired real-time performance in unpredictable environments where system workload and resources may fluctuate significantly. This paper presents FC-ORB, a real-time Object Request Broker (ORB) middleware that employs end-to-end utilization control to handle fluctuations in application workload and system resources. The contributions of this paper are three-fold. First, we present a novel utilization control service that enforces desired CPU utilization bounds on multiple processors by adapting the rates of end-to-end tasks within user-specified ranges. Second, we describe a set of middleware-level mechanisms designed to support end-to-end tasks and distributed multi-processor utilization control in a real-time ORB. Finally, we present extensive experimental results on a Linux testbed. Our results demonstrate that our middleware can maintain desired utilizations in face of uncertainties and variations in task execution times, resource contentions from external workloads, and permanent processor failure. FC-ORB demonstrates that the integration of utilization control, end-to-end scheduling, and fault-tolerance mechanisms in DRE middleware is a promising approach for enhancing the robustness of DRE applications in unpredictable environments. 
80|7||Hierarchical resource allocation for robust in-home video streaming|High quality video streaming puts high demands on network and processor resources. The bandwidth of the communication medium and the timely arrival of the frames necessitate a tight resource allocation. Given the dynamic environment where videos are started and stopped and electro-magnetic perturbations affect the bandwidth of the wireless medium, a framework is needed that reacts timely to the changes in network load and network operational conditions. This paper describes a hierarchical framework, which can handle the dynamic network resource allocation in a timely manner. 
80|7||Resource management for real-time tasks in mobile robotics|Coordinated behavior of mobile robots is an important emerging application area. Different coordinated behaviors can be achieved by assigning sets of control tasks, or strategies, to robots in a team. These control tasks must be scheduled either locally on the robot or distributed across the team. An application may have many control strategies to dynamically choose from, although some may not be feasible, given limited resource and time availability. Thus, dynamic feasibility checking becomes important as the coordination between robots and the tasks that need to be performed evolves with time. This paper presents an on-line algorithm for finding a feasible strategy given a functionally equivalent set of strategies for achieving an application’s goals.We present two algorithms for feasibility improvement. Both consider communication cost and utilization bound to make resource allocation and scheduling decisions. Extensive experimental results show the effectiveness of the approaches, especially in resource-tight environments. We also demonstrate the application of our approach to real world scenarios involving teams of robots and show how feasibility analysis also allows the prediction of the scalability of the solution to large robot teams. 
80|7||Adaptive network QoS in layer-3/layer-2 networks as a middleware service for mission-critical applications|
80|7||A multi-layered resource management framework for dynamic resource management in enterprise DRE systems|Enterprise distributed real-time and embedded (DRE) systems can benefit from dynamic management of computing and networking resources to optimize and reconfigure system resources at runtime in response to changing mission needs and/or other situations, such as failures or system overload. This paper provides two contributions to the study of dynamic resource management (DRM) for enterprise DRE systems. First, we describe a standards-based multi-layered resource management (ARMS MLRM) architecture that provides DRM capabilities to enterprise DRE systems. Second, we show the results of experiments evaluating our ARMS MLRM architecture in the context of a representative enterprise DRE system for shipboard computing. 
80|7||Feedback control-based dynamic resource management in distributed real-time systems|The resource management in distributed real-time systems becomes increasingly unpredictable with the proliferation of data-driven applications. Therefore, it is inefficient to allocate the resources statically to handle a set of highly dynamic tasks whose resource requirements (e.g., execution time) are unknown a prior. In this paper, we build a distributed real-time system based on the control theory, focusing on the computational resource management. Specifically, this work makes three important contributions. First, it allows the designer to specify the desired temporal behavior of system adaptation, such as the speed of convergence. This is in contrast to previous literature, specifying only steady-state metrics, e.g. the deadline miss ratio. Second, unlike QoS optimization approaches, our solution meets performance guarantees with no accurate knowledge of task execution parameters – a key advantage in a poorly modeled environment. Last, in contrast to ad hoc algorithms based on intuition and testing, we rigorously prove that our approach not only has excellent steady state behavior, but also meets stability, overshoot, and settling time requirements. 
80|7||Characterizing robustness in dynamic real-time systems|The problem of robust task allocation is motivated by the need to deploy real-time systems in dynamic operational environments. Existing robust allocation approaches employ coarse robustness metrics, which can result in poor allocations. This paper proposes a metric that accurately characterizes a system’s robustness within feasible allocation regions. An allocation algorithm is provided to find allocations that are both feasible and robust; the robustness as measured by the metric is shown to have theoretical bounds. Experiments demonstrate that the algorithm produces good and scalable performance compared with several heuristic algorithms. 
80|7||Verification of instrumentation techniques for resource management of real-time systems|Dynamic resource management is an effective way to ensure that real-time systems work correctly in unpredictable environments. For mission-critical systems, dynamic resource managers (RMs) must be verified. Every RM depends on instruments to report the state of the system and its resources, so verified instrumentation is foundational to the larger goal of verified RMs.In this paper we address the verification of instrumentation. We identify the following instruments as necessary for any RM: event times; task execution time; task deadline overrun; resource accessibility; and overall CPU utilization. To assure the core instrumentation functionality, we argue that it is required to establish bounds for the following properties: precision, uncertainty, resource usage, timeliness and intrusiveness. As a case study, we illustrate the detailed verification techniques and experimental results for the event-time instrumentation. 
80|7||Identity based proxy multi-signature|As a variation of ordinary digital signature scheme, a proxy signature scheme enables a proxy signer to sign messages on behalf of the original signer. Proxy multi-signature is an extension of the basic proxy signature primitive, and permits two or more entities to delegate their signing capabilities to the same other entity. Combining proxy multi-signature with identity based cryptography, in this paper, we define security notions for identity based proxy multi-signature schemes. We also construct a concrete identity based proxy multi-signature scheme which is provably secure in the random oracle model under the computational Diffie–Hellman assumption over pairing-friendly groups. Furthermore, the new scheme is very simple and efficient computationally. It has the property that the size of a proxy multi-signature is independent of the number of the original signers. 
80|7||Resource allocation in network processors for network intrusion prevention systems|Networking applications with high memory access overhead gradually exploit network processors that feature multiple hardware multithreaded processor cores along with a versatile memory hierarchy. Given rich hardware resources, however, the performance depends on whether those resources are properly allocated. In this work, we develop an NIPS (Network Intrusion Prevention System) edge gateway over the Intel IXP2400 by characterizing/mapping the processing stages onto hardware components. The impact and strategy of resource allocation are also investigated through internal and external benchmarks. Important conclusions include: (1) the system throughput is influenced mostly by the total number of threads, namely I × J, where I and J represent the numbers of processors and threads per processor, respectively, as long as the processors are not fully utilized, (2) given an application, algorithm and hardware specification, an appropriate (I, J) for packet inspection can be derived and (3) the effectiveness of multiple memory banks for tackling the SRAM bottleneck is affected considerably by the algorithms adopted. 
80|7||Application-specific garbage collection|Prior work, including our own, shows that application performance in garbage collected languages is highly dependent upon the application behavior and on underlying resource availability. We show that given a wide range of diverse garbage collection (GC) algorithms, no single system performs best across programs and heap sizes.We present a Java Virtual Machine extension for dynamic and automatic switching between diverse, widely used GCs for application-specific garbage collection selection. We describe annotation-guided, and automatic GC switching. We also describe a novel extension to extant on-stack replacement (OSR) mechanisms for aggressive GC specialization that is readily amenable to compiler optimization. 
80|7||Novel image copy detection with rotating tolerance|
80|7||Improvements of image sharing with steganography and authentication|Recently, Lin and Tsai proposed an image secret sharing scheme with steganography and authentication to prevent participants from the incidental or intentional provision of a false stego-image (an image containing the hidden secret image). However, dishonest participants can easily manipulate the stego-image for successful authentication but cannot recover the secret image, i.e., compromise the steganography. In this paper, we present a scheme to improve authentication ability that prevents dishonest participants from cheating. The proposed scheme also defines the arrangement of embedded bits to improve the quality of stego-image. Furthermore, by means of the Galois Field GF(28), we improve the scheme to a lossless version without additional pixels. 
80|7||Efficient self-tuning spin-locks using competitive analysis|Reactive spin-lock algorithms that can automatically adapt to contention variation on the lock have received great attention in the field of multiprocessor synchronization since they can help applications achieve good performance in all possible contention conditions. However, in existing reactive spin-locks the reaction relies on (i) some fixed experimentally tuned thresholds, which may get frequently outdated in dynamic environments like multiprogramming/multiprocessor systems, or (ii) known probability distributions of inputs.This paper presents a new reactive spin-lock algorithm that is completely self-tuning, which means no experimentally tuned parameter nor probability distribution of inputs are needed. The new spin-lock is built on both synchronization structures of applications and online algorithmic techniques. Our experiments, which use the Spark98 kernels and the SPLASH-2 applications as application benchmarks, on a multiprocessor machine SGI Origin2000 and an Intel Xeon workstation have showed that the new self-tuned spin-lock performs as well as the best of hand-tuned spin-lock representatives in a wide range of contention levels. 
80|7||A communication-efficient and fault-tolerant conference-key agreement protocol with forward secrecy|A conference-key establishment protocol allows participants to construct a common session key that is used to encrypt/decrypt transmitted messages among the participants over an open channel. There are two kinds of conference-key establishment protocols: conference-key distribution and conference-key agreement. In a conference-key distribution protocol, a trusted or elected entity is responsible for generating and distributing the conference key. A conference-key agreement protocol involves all participants cooperatively establishing a conference key. This article designs a secure conference-key agreement protocol with constant round number and message size. Under the decision Diffie–Hellman problem assumption, the resulting protocol is demonstrated to be secure against passive adversaries. Under the random oracle model, the proposed protocol is demonstrated to be provable secure against impersonator attacks and withstand known-key attacks. Compared to previously proposed protocols with round-efficiency, the proposed protocol requires a constant message size for each participant. Furthermore, the proposed protocol possesses both fault tolerance and forward secrecy, while previously proposed protocols with round-efficiency lack one or both properties. 
80|7||A context-aware cache structure for mobile computing environments|This paper proposes a cache management method that maintains a mobile terminal’s cache content by prefetching data items with maximum benefit and evicting cache data entries with minimum benefit. The data item benefit is evaluated based on the user’s query context which is defined as a set of constraints (predicates) that define both the movement pattern and the information context requested by the mobile user. A context-aware cache is formed and maintained using a set of neighboring locations (called the prime list) that are restricted by the validity of the data fetched from the server. Simulation results show that the proposed strategy, using different levels of granularity, can greatly improve system performance in terms of the cache hit ratio. 
80|7||An empirical study of the bad smells and class error probability in the post-release object-oriented system evolution|Bad smells are used as a means to identify problematic classes in object-oriented systems for refactoring. The belief that the bad smells are linked with problematic classes is largely based on previous metric research results. Although there is a plethora of empirical studies linking software metrics to errors and error proneness of classes in object-oriented systems, the link between the bad smells and class error probability in the evolution of object-oriented systems after the systems are released has not been explored. There has been no empirical evidence linking the bad smells with class error probability so far. This paper presents the results from an empirical study that investigated the relationship between the bad smells and class error probability in three error-severity levels in an industrial-strength open source system. Our research, which was conducted in the context of the post-release system evolution process, showed that some bad smells were positively associated with the class error probability in the three error-severity levels. This finding supports the use of bad smells as a systematic method to identify and refactor problematic classes in this specific context. 
80|7||Logic synthesis for PAL-based CPLD-s based on two-stage decomposition|A PAL-based (PAL – Programmable Array Logic) logic block is the core of a great majority of contemporary CPLD (Complex Programmable Logic Device) circuits. The purpose of the paper is to present a novel method of two-stage decomposition dedicated for PAL-based CPLD-s. The key point of the algorithm lies in sequential search for a decomposition providing feasibility of implementation of the free block in one PAL-based logic block containing a limited number of product terms. The proposed method is an alternative to the classical approach, based on two-level minimisation of separate single-output functions. An original method of determining the row multiplicity of the partition matrix is presented. For this purpose a new concept of graph is proposed – the Row Incompatibility and Complement Graph. An appropriate algorithm of the Row Incompatibility and Complement Graph colouring is presented. On the basis of row multiplicity evaluated for individual partitionings, the partitioning which provides minimisation of the bound block is chosen. Results of the experiments, which are also presented, prove that the proposed method leads to significant reduction of chip area in relation to the classical approach, especially for CPLD structures, that consist of PAL-based blocks containing 2i (a power of 2) product terms. The proposed method was also compared with decomposition algorithms presented in another works. The results lead to a conclusion, that the proposed two-stage PAL decomposition is especially attractive with respect to the number of logic levels obtained. 
80|7||Synchronization modeling and its application for SMIL2.0 presentations|A novel synchronization model namely Extended Real-Time Synchronization Model (E-RTSM) for modeling SMIL2.0 temporal behaviors is proposed in this paper. E-RTSM deals with event-based/non-deterministic synchronization as well as schedule-based synchronization in SMIL2.0. Converting of the temporal relationship of a SMIL2.0 document to E-RTSM is presented. Moreover, design of the E-RTSM-based data-retrieving engine for SMIL2.0 presentations is also proposed in the paper. The data-retrieving engine estimates the worst-case playback time of each object at the parsing stage and applying an error compensation mechanism at run-time to adjust the estimated playback time as well as the schedule of the fetching requests for data retrieval. Performance measurements from the real implementation of the E-RTSM-based data-retrieving engine for SMIL2.0 presentations have demonstrated the efficiency of the proposed technique. 
80|7||A framework for the static verification of api calls|A number of tools can statically check program code to identify commonly encountered bug patterns. At the same time, programs are increasingly relying on external apis for performing the bulk of their work: the bug-prone program logic is being fleshed-out, and many errors involve tricky subroutine calls to the constantly growing set of external libraries. Extending the static analysis tools to cover the available apis is an approach that replicates scarce human effort across different tools and does not scale. Instead, we propose moving the static api call verification code into the api implementation, and distributing the verification code together with the library proper. We have designed a framework for providing static verification code together with Java classes, and have extended the FindBugs static analysis tool to check the corresponding method invocations. To validate our approach we wrote verification tests for 100 different methods, and ran FindBugs on 6.9 million method invocations on what amounts to about 13 million lines of production-quality code. In the set of 55 thousand method invocations that could potentially be statically verified our approach identified 800 probable errors. 
80|7||An evaluation of the middlewareâs impact on the performance of object oriented distributed systems|In this paper, we present a performance analysis of the response time of a client–server e-banking application running over three different enterprise middleware platforms, namely HTTPServlets, RMI and Web services with JAX-RPC. We conducted performance testing with the purpose to reveal the specific characteristics of each middleware technology and the impact that they infer on the distributed application’s performance. A server node running the three J2EE platforms was benchmarked over a wide array of intranet usage patterns. A statistical analysis of the collected data led into conclusions regarding the benefits of each middleware technology. The simulation framework can be further extended to become a testing tool able to differentiate on various service demand classes as an input in distributed applications so as to offer first-cut validated results concerning the systems’ performance. 
80|8|http://www.sciencedirect.com/science/journal/01641212/80/8|Reflections on the influences of the COCOMO, spiral and the Win-Win models on software project and risk management|
80|8||Impact and contributions of MBASE on software engineering graduate courses|As the founding Director of the Center for Software Engineering, Professor Barry Boehm developed courses that have greatly impacted the education of software engineering students. Through the use of the MBASE framework and complementary tools, students have been able to obtain real-life software development experience without leaving campus. Project team clients and the universities have also benefited. This paper provides evidence on the impact of Dr. Boehm’s frameworks on courses at two universities, and identifies major contributions to software engineering education and practice. 
80|8||Making every student a winner: The WinWin approach in software engineering education|This paper shows how Theory-W and the WinWin requirements negotiation approach are used in software engineering education at several universities in the US, Europe, and Asia. We briefly describe Theory-W, the WinWin negotiation model, available processes, and tool support. We then discuss how students can benefit from WinWin in their software engineering education. We explore different options for teaching the approach and present concrete examples and experiences from the different universities. 
80|8||The influence of COCOMO on software engineering education and training|As the discipline of software engineering has matured, COCOMO (constructive cost model) has evolved, both in response to and as a leading indicator of changes in software engineering methods and techniques. This paper traces the evolution of the COCOMO cost estimation models as they have evolved from 1981 to 2005. In particular, COCOMO 81, Ada COCOMO, and COCOMO II are presented. COCOMO has been, and continues to be a vehicle for introducing and illustrating software engineering methods and techniques. Emphasis is placed on the role COCOMO models have played, and continue to play, in software engineering education and training. 
80|8||Reflections on 10 years of sponsored senior design projects: Students winâclients win!|Undergraduate computer science degree programs often provide an opportunity for students to experience real software projects as a part of their programs of study. These experiences frequently reside in a course in which students form software development teams, are assigned to a project offered by a corporate sponsor and devote one or two semesters to the task of making progress on the project. In an ideal model, faculty mentor student teams who, in turn, behave as subcontractors or consultants to the sponsor. Students work for a grade, not directly for the sponsor as a true subcontractor would. In the ideal model, students demonstrate what they have learned about software engineering process, as well as their ability to implement programmed solutions. Student teams provide progress reports, both oral and written, and directly experience many of the challenges and successes of true software engineering professionals. This paper reports on one such program after 10 years of operation. The technologies and software development processes of student projects are summarized and presented as an informal survey. Student response is discussed in terms of software systems they produced and how they went about producing them. The maturation of these students as software engineering professionals is also discussed. 
80|8||Experience teaching Barry Boehmâs techniques in industrial and academic settings|
80|8||Leadership by example: A perspective on the influence of Barry Boehm|Over the course of the past 10 years of working with Dr. Boehm on various projects has both influenced the software engineering program at Mississippi State University as well as provided growth opportunities and expansion of the MSU ABET accredited software engineering undergraduate degree program. Looking back over the key interactions with him, it is apparent that he leads (and influences) by his example, his work ethic, and his intellect in the software engineering field. This paper provides insights into his specific influences though collaborative work with another university. 
80|8||Statistical models vs. expert estimation for fault prediction in modified code â an industrial case study|Statistical fault prediction models and expert estimations are two popular methods for deciding where to focus the fault detection efforts when the fault detection budget is limited. In this paper, we present a study in which we empirically compare the accuracy of fault prediction offered by statistical prediction models with the accuracy of expert estimations. The study is performed in an industrial setting. We invited eleven experts that are involved in the development of two large telecommunication systems. Our statistical prediction models are built on historical data describing one release of one of those systems. We compare the performance of these statistical fault prediction models with the performance of our experts when predicting faults in the latest releases of both systems. We show that the statistical methods clearly outperform the expert estimations. As the main reason for the superiority of the statistical models we see their ability to cope with large datasets. This makes it possible for statistical models to perform reliable predictions for all components in the system. This also enables prediction at a more fine-grain level, e.g., at the class instead of at the component level. We show that such a prediction is better both from the theoretical and from the practical perspective. 
80|8||Fine-grain analysis of common coupling and its application to a Linux case study|Common coupling (sharing global variables across modules) is widely accepted as a measure of software quality and maintainability; a low level of common coupling is necessary (but not sufficient) to ensure maintainability. But when the global variables in question are large multi-field data structures, one must decide whether to consider such data structures as single units, or examine each of their fields individually. We explore this issue by re-analyzing a case study based on the Linux operating system. We determine the common coupling at the level of granularity of the component fields of large, complex data structures, rather than at the level of the data structures themselves, as in previous work. We claim that this is the appropriate level of analysis based on how such data structures are used in practice, and also that such a study is required due to concern that coarse-grained analysis leads to false coupling. We find that, for this case study, the granularity does not have a decisive effect on the results. In particular, our results for coupling based on individual fields are similar in spirit to the results reported previously (by others) based on using complete data structures. In both cases, the coupling indicates that the system kernel is vulnerable to modifications in peripheral modules of the system. 
80|8||Verification method of dataflow algorithms in high-level synthesis|This paper presents a formal verification algorithm using the Petri Net theory to detect design errors for high-level synthesis of dataflow algorithms. Typically, given a dataflow algorithm and a set of architectural constraints, the high-level synthesis performs algorithmic transformation and produces the optimal scheduling. How to verify the correctness of high-level synthesis becomes a key issue before mapping the synthesis results onto a silicon. Many tools exist for RTL (Register Transfer Level) design, but few for high-level synthesis. Instead of applying Boolean algebra, this paper adopts the Petri Net theory to verify the correctness of the synthesis result, because the Petri Net model has the nature of dataflow algorithms. Herein, we propose three approaches to realize the Petri Net based formal verification algorithm and conclude the best one who outperforms the others in terms of processing speed and resource usage. 
80|8||An automated approach to specification animation for validation|Formal specification has been increasingly adopted for the development of software systems of the highest integrity. However, the readability of specifications for large-scale and complex systems can be so poor that even the developers may not easily understand whether their specifications define the “intended behaviors”. In this paper, we describe a software tool that supports the animation of specifications by simulating their functional scenarios using the Message Sequence Chart (MSC). The tool extracts automatically functional scenarios from a specification and generates a message sequence chart for each of them for a syntactic level analysis. The tool can also execute a functional scenario with test cases for a semantic level analysis if all the processes involved in the scenario are defined using explicit specifications. With the tool support the animation of a specification can be carried out incrementally to assist its user to review the adequacy of the specification. We present a case study applying the tool to animate a formal specification for a library system and evaluate its result. 
80|8||Practical experience of eliciting classes from use case descriptions|In moving from requirements analysis to design, use cases are often recommended as the starting point for the derivation of classes. However, exactly how classes are to be found within the use case is not entirely obvious. Typical approaches suggest a simple noun/verb search or brainstorming. Recent work is moving towards an interrogation of the use case diagram as a means of validation and of the description (and scenario) to elicit objects in the problem domain. This paper presents a set of Elicitation Questions that enables the interrogation of descriptions from the perspectives of specification, software architecture and design. This qualitative ‘interrogation’ teases out design issues. The Elicitation Questions were trialled through application to a real industrial project at a financial services company. Feedback from practitioners shows that the Elicitation Questions are important in raising design and testing issues from the use case descriptions but the organisational culture in how software is developed would impact its uptake. 
80|8||Communication support for systems engineering â process modelling and animation with APRIL|The most important task in the early stages of systems engineering is the building of models which capture the relevant knowledge of a given application domain. A working communication with domain experts who possess this knowledge is crucial, since misunderstandings almost always lead to expensive system redesigns in later development stages. In this context, especially the modelling of systems behaviour is a challenging problem. While formally based languages in this area are often too difficult to understand for domain experts, more informal languages frequently lack animation support for dynamic process visualizations. Out of this, an easy to understand and semi-formal visual modelling language which allows for process animations is needed in order to improve communication in systems engineering. If the use of such a language leads to an earlier identification of conceptualization flaws, the overall costs of systems development may be significantly reduced.As an attempt into this direction, the APRIL process modelling language is introduced in this article together with two complementary animation concepts as well as the prototype of a supporting tool. 
80|8||Evaluating performances of pair designing in industry|Pair programming has attracted an increasing interest from practitioners and researchers: there is initial empirical evidence that it has positive effects on quality and overall delivery time, as demonstrated by several controlled experiments. The practice does not only regard coding, since it can be applied to any other phase of the software process: analysis, design, and testing. Because of the asymmetry between design and coding, applying pair programming to the design phase might not produce the same benefits as those it produces in the development phase. In this paper, we report the findings of a controlled experiment on pair programming, applied to the design phase and performed in a software company. The results of the experiment suggest that pair programming slows down the task, yet improves quality. Furthermore we compare our results with those of a previous exploratory experiment involving students, and we demonstrate how the outcomes exhibit very similar trends. 
80|8||Adaptive software testing with fixed-memory feedback|Adaptive software testing is the counterpart of adaptive control in software testing. It means that software testing strategy should be adjusted on-line by using the testing data collected during software testing as our understanding of the software under test is improved. In this paper we propose a new strategy of adaptive software testing in the context of software cybernetics. This new strategy employs fixed-memory feedback for on-line parameter estimations and is intended to circumvent the drawbacks of the assumption that all remaining defects are equally detectable at constant rate and to reduce the underlying computational complexity of on-line parameter estimations. A comprehensive case study with the Space program demonstrates that the new adaptive testing strategy can really work in practice and may noticeably outperform the purely-random testing strategy and the random-partition testing strategy (or collectively, the random testing strategies) in terms of the number of tests used to detect and remove a given number of defects in a single process of software testing and the corresponding standard deviation. In addition, the case study shows that the input domain of the software under test should be partitioned non-evenly for the adaptive testing strategy. 
80|8||Predicting object-oriented software maintainability using multivariate adaptive regression splines|Accurate software metrics-based maintainability prediction can not only enable developers to better identify the determinants of software quality and thus help them improve design or coding, it can also provide managers with useful information to help them plan the use of valuable resources. In this paper, we employ a novel exploratory modeling technique, multiple adaptive regression splines (MARS), to build software maintainability prediction models using the metric data collected from two different object-oriented systems. The prediction accuracy of the MARS models are evaluated and compared using multivariate linear regression models, artificial neural network models, regression tree models, and support vector models. The results suggest that for one system MARS can predict maintainability more accurately than the other four typical modeling techniques, and that for the other system MARS is as accurate as the best modeling technique. 
80|8||The design and implementation of an application program interface for securing XML documents|
80|8||Towards efficient web engineering approaches through flexible process models|After more than a decade of web developments and some deafening fiascos, it has become clear that it is not possible to face the development of large scale web systems without following a systematic and well-defined process to guarantee quality, measurability, maintainability and reusability. There are a number of hypermedia/web engineering methods that provide mechanisms to specify the product requirements, including those concerning structure, navigation, interaction, presentation and access. But apart from product requirements there are also process requirements which in the web arena are constantly changing. Hence, to be put in practice without disturbing the project goals nor compromising its success, methods have to rely on empirical and flexible process models that can be easily adapted to fit process requirements. Little attention has been paid to the process model in most hypermedia/web methods which usually apply a classical iterative process based on the use of prototypes that are tested with users. In this paper we describe how we did apply the ADM web engineering method following a flexible star life cycle in the context of a specific web project highlighting the main benefits of this approach. In particular we will describe the different cycles applied in the ARCE project, illustrating the application of a usability engineering life cycle in a real case. 
80|8||Pounamu: A meta-tool for exploratory domain-specific visual language tool development|Domain-specific visual language tools have become important in many domains of software engineering and end user development. However building such tools is very challenging with a need for multiple views of information and multi-user support, the ability for users to change tool diagram and meta-model specifications while in use, and a need for an open architecture for tool integration. We describe Pounamu, a meta-tool for realising such visual design environments. We describe the motivation for Pounamu, its architecture and implementation and illustrate examples of domain-specific visual language tools that we have developed with Pounamu. 
80|8||Model-based user interface engineering with design patterns|The main idea surrounding model-based UI (User Interface) development is to identify useful abstractions that highlight the core aspects and properties of an interactive system and its design. These abstractions are instantiated and iteratively transformed at different level to create a concrete user interface. However, certain limitations prevent UI developers from adopting model-based approaches for UI engineering. One such limitation is the lack of reusability of best design practices and knowledge within such approaches. With a view to fostering reuse in the instantiation and transformation of models, we introduce patterns as building blocks, which can be first used to construct different models and then instantiated into concrete UI artefacts. In particular, we will demonstrate how different kinds of patterns can be used as modules for establishing task, dialog, presentation and layout models. Starting from an outline of the general process of pattern application, an interface for combining patterns and a possible formalization are suggested. The Task Pattern Wizard, an XML/XUL-based tool for selecting, adapting and applying patterns to task models, will be presented. In addition, an extended example will illustrate the intimate complicity of several patterns and the proposed model-driven approach. 
80|9|http://www.sciencedirect.com/science/journal/01641212/80/9|Introduction to special section on Evaluation and Assessment in Software Engineering EASE06|
80|9||Experiences using systematic review guidelines|Systematic review is a method to identify, assess and analyse published primary studies to investigate research questions. We critique recently published guidelines for performing systematic reviews on software engineering, and comment on systematic review generally with respect to our experience conducting one. Overall we recommend the guidelines. We recommend researchers clearly and narrowly define research questions to reduce overall effort, and to improve selection and data extraction. We suggest that “complementary” research questions can help clarify the main questions and define selection criteria. We show our project timeline, and discuss possibilities for automating and increasing the acceptance of systematic review. 
80|9||Establishing and maintaining trust in software outsourcing relationships: An empirical investigation|Our research objective is to understand software outsourcing practitioners’ perceptions of the role of trust in managing client–vendor relationships and the factors that are critical to trust in off-shore software outsourcing relationships. Participants were 12 Vietnamese software development practitioners developing software for Far Eastern, European, and American clients. They identified that cultural understanding, creditability, capabilities, and personal visits are important factors in gaining the initial trust of a client, while cultural understanding, communication strategies, contract conformance, and timely delivery are vital factors in maintaining that trust. We contrast Vietnamese and Indian practitioners’ views on factors affecting trust relationships. 
80|9||Ranking reusability of software components using coupling metrics|This paper provides an account of new static measures of coupling developed to assess the reusability of Java components retrieved from the internet by a search engine. These measures differ from the majority of established metrics in three respects: they take account of indirect coupling, they reflect the degree to which two classes are coupled, and they take account of the functional complexity of classes. An empirical comparison of the new measures with six established coupling metrics is described. The new measures are shown to be consistently superior at ranking components according to their reusability. 
80|9||Do programmer pairs make different mistakes than solo programmers?|Objective: Comparison of program defects caused by programmer pairs and solo developers.Design: Analysis of programs developed during two counter balanced experiments.Setting: Programming lab at University.Experimental units: 42 programs developed by computer science students participating in an extreme programming lab course.Main outcome measures: Programmer pairs make as many algorithmic mistakes but fewer expression mistakes than solo programmers.Results: The second result is significant on the 5% level.Conclusions: For simple problems, pair programming seems to lead to fewer mistakes than solo programming. 
80|9||Characteristics of software engineers with optimistic predictions|This paper examines the degree to which level of optimism in software engineers’ predictions is related to optimism on previous predictions, general level of optimism (explanatory style, life orientation and self-assessed optimism), development skill, confidence in the accuracy of their own predictions, and ability to recall effort used on previous tasks. Results from four experiments suggest that more optimistic software engineers are characterized by more optimistic previous predictions, higher confidence in the accuracy of their own predictions, lower development skills, poorer ability or willingness to recall effort on previous tasks, and higher optimism scores. However, a substantial part of the variation in the level of optimism seems to be random. 
80|9||SPICE in retrospect: Developing a standard for process assessment|The SPICE Project was established in 1993 to support the development, validation and transition into use of an International Standard for software process assessment. Its efforts have resulted in the publication of a five-part Standard for Process Assessment, ISO/IEC 15504. This paper reviews the evolution of the Standard, and reflects on the parallel achievements of the SPICE Project and the standardisation effort in advancing the state of the art in process assessment and improvement. 
80|9||A three-tier knowledge management scheme for software engineering support and innovation|To ensure smooth and successful transition of software innovations to enterprise systems, it is critical to maintain proper levels of knowledge about the system configuration, the operational environment, and the technology in both existing and new systems. We present a three-tier knowledge management scheme through a systematic planning of actions spanning the transition processes in levels from conceptual exploration to prototype development, experimentation, and product evaluation. The three-tier scheme is an integrated effort for bridging the development and operation communities, maintaining stability to the operational performance, and adapting swiftly to software technology innovations. The scheme combines experiences of academic researches and industrial practitioners to provide necessary technical expertise and qualifications for knowledge management in software engineering support (SES) processes. 
80|9||Analysing the impact of usability on software design|This paper analyses what implications usability has for software development, paying special attention to the impact of this quality attribute on design. In this context, the aim is twofold. On the one hand, we intend to empirically corroborate that software design and usability are really related. This would mean that this, like other quality attributes, would need to be dealt with no later than at design time to develop usable software at a reasonable cost. On the other hand, we present a possible quantification, calculated from a number of real applications, of the effect of incorporating certain usability features at design time. 
80|9||Empirical study of the effects of open source adoption on software development economics|In this paper, we present the results of empirical study of the effects of open source software (OSS) components reuse on software development economics. Specifically, we examined three economic factors – cost, productivity, and quality. This study started with an extensive literature review followed by an exploratory study conducted through interviews with 18 senior project/quality managers, and senior software developers. Then, the result of the literature review and the exploratory study was used to formulate research model, hypotheses, and survey questionnaire. Software intensive companies from Canada and the US were targeted for this study. The period of study was between September 2004 and March 2006. Our findings show that there are strong significant statistical correlations between the factors of OSS components reuse and software development economics. The conclusion from this study shows that software organizations can achieve some economic gains in terms of software development productivity and product quality if they implement OSS components reuse adoption in a systematic way. A big lesson learned in this study is that OSS components are of highest quality and that open source community is not setting a bad example (contrary to some opinion) so far as ‘good practices’ are concerned. 
80|9||A software fault tree key node metric|Analysis of software fault trees exposes failure events that can impact safety within safety-critical software product lines. This paper presents a software fault tree key node safety metric for measuring software safety within product lines. Fault tree structures impacting the metric’s composition are provided, and the mathematical basis for the metric is defined. The metric is applied to an embedded control system as well as to a series of experiments expected to either improve or degrade system safety. The effectiveness of the metric is analyzed, and lessons learned during the application of the metric are discussed. 
80|9||Comprehension strategies and difficulties in maintaining object-oriented systems: An explorative study|Program comprehension is a major time-consuming activity in software maintenance. Understanding the underlying mechanisms of program comprehension is therefore necessary for improving software maintenance. It has been argued that acquiring knowledge of how a program works before modifying it (the systematic strategy) is unrealistic in larger programs. The goal of the experiment presented in this paper is to explore this claim. The experiment examines strategies for program comprehension and cognitive difficulties of developers who maintain an unfamiliar object-oriented system. The subjects were 38 students in their third or fourth year of study in computer science. They used a professional Java tool to perform several maintenance tasks on a medium-size Java application system in a 6-h long experiment. The results showed that the subjects who applied the systematic strategy were more likely to produce correct solutions. Two major groups of difficulties were related to the comprehension of the application structure, namely to the understanding of GUI implementation and OO comprehension and programming. Acquisition of strategic knowledge might improve program comprehension in software maintenance. 
80|9||Efficient approach for restructuring multiple inheritance hierarchies|This paper discusses the restructuring of inheritance hierarchies of classes and introduces a method of restructuring multiple hierarchies of class inheritance, which removes duplicated methods and creates inheritance hierarchies without overridden methods while preserving the behavior of objects. This paper formulates a restructuring problem for 0–1 integer programming and presents a network-based solution method, which uses a distance parameter between every pair of characteristics for similarity metric. This paper presents basic theorems for clustering characteristics and defining of inheritance hierarchy. We create inheritance hierarchies based on the rules for a definition of class relationship. The method is analyzed and compared with the existing method. 
80|9||A new research agenda for tool integration|This article highlights tool integration within software engineering environments. Tool integration concerns the techniques used to form coalitions of tools that provide an environment supporting some, or all, activities within a software engineering process. These techniques have been used to create environments that attempt to address aspects of software development, with varying success. This article provides a timely analysis and review of many of the significant projects in the field and, combined with evidence collected from industry, concludes by proposing an empirical manifesto for future research, where we see the need for work to justify tool integration efforts in terms of relevant socio-economic indicators. 
80|9||Comments on âA Semantic Web Primerâ, Grigoris Antoniou, Frank Van Harmelen. The MIT Press, Cambridge, Massachusetts, London, England (2004)|
volume|issue|url|title|abstract
81|-|http://www.sciencedirect.com/science/journal/01641212/81|Introduction to the Special Issue|
81|-||Using planning poker for combining expert estimates in software projects|When producing estimates in software projects, expert opinions are frequently combined. However, it is poorly understood whether, when, and how to combine expert estimates. In order to study the effects of a combination technique called planning poker, the technique was introduced in a software project for half of the tasks. The tasks estimated with planning poker provided: (1) group consensus estimates that were less optimistic than the statistical combination (mean) of individual estimates for the same tasks, and (2) group consensus estimates that were more accurate than the statistical combination of individual estimates for the same tasks. For tasks in the same project, individual experts who estimated a set of control tasks achieved estimation accuracy similar to that achieved by estimators who estimated tasks using planning poker. Moreover, for both planning poker and the control group, measures of the median estimation bias indicated that both groups had unbiased estimates, because the typical estimated task was perfectly on target. A code analysis revealed that for tasks estimated with planning poker, more effort was expended due to the complexity of the changes to be made, possibly caused by the information provided in group discussions. 
81|-||Risk and risk management in software projects: A reassessment|Controlling risk in software projects is considered to be a major contributor to project success. This paper reconsiders the status of risk and risk management in the literature and practice. The analysis is supported by a study of risk practices in government agencies in an Australian State, contributing to a gap in research in the public sector. It is found that risk is narrowly conceived in research, and risk management is under-performed in practice. The findings challenge some conventional conceptions of risk management and project management. For example, it was found that software projects do not conform to a uniform structure, as assumed in much of the literature. This introduces variations in the risk and project management challenges they face. Findings also suggest that formal project management is neither necessary nor sufficient for project success. It is concluded that risk management research lags the needs of practice, and risk management as practiced lags the prescriptions of research. Implications and directions for future research and practice are discussed. 
81|-||The architecture of an event correlation service for adaptive middleware-based applications|Loosely coupled component communication driven by events is a key mechanism for building middleware-based applications that must achieve reliable qualities of service in an adaptive manner. In such a system, events that encapsulate state snapshots of a running system are generated by monitoring components. Hence, an event correlation service is necessary for correlating monitored events from multiple sources. The requirements for the event correlation raise two challenges: to seamlessly integrate event correlation services with other services and applications; and to provide reliable event management with minimal delay. This paper describes our experience in the design and implementation of an event correlation service. The design encompasses an event correlator and an event proxy that are integrated with an architecture for adaptive middleware components. The implementation utilizes the common-based event (CBE) specification and stateful Web service technologies to support the deployment of the event correlation service in a distributed architecture. We evaluate the performance of the overall solution in a test bed and present the results in terms of the trade-off between the flexibility and the performance overhead of the architecture. 
81|-||Distributing test cases more evenly in adaptive random testing|Adaptive random testing (ART) has recently been proposed to enhance the failure-detection capability of random testing. In ART, test cases are not only randomly generated, but also evenly spread over the input domain. Various ART algorithms have been developed to evenly spread test cases in different ways. Previous studies have shown that some ART algorithms prefer to select test cases from the edge part of the input domain rather than from the centre part, that is, inputs do not have equal chance to be selected as test cases. Since we do not know where the failure-causing inputs are prior to testing, it is not desirable for inputs to have different chances of being selected as test cases. Therefore, in this paper, we investigate how to enhance some ART algorithms by offsetting the edge preference, and propose a new family of ART algorithms. A series of simulations have been conducted and it is shown that these new algorithms not only select test cases more evenly, but also have better failure detection capabilities. 
81|-||Timed Behavior Trees for Failure Mode and Effects Analysis of time-critical systems|Behavior Trees are a graphical notation used for formalising functional requirements, and have been successfully applied to several industrial case studies. However, the standard notation does not support the concept of time, and consequently its application is limited to non-real-time systems. To overcome this limitation we extend the notation to timed Behavior Trees. We provide an operational semantics which is based on timed automata, and thus serves as a formal basis for the translation of timed Behavior Trees into the input notation of the timed model checker UPPAAL. System-level timing properties of a Behavior Tree model can then be automatically verified using UPPAAL. Based on the notational extensions with model checking support, we introduce timed Failure Mode and Effects Analysis, a process for identifying cause-consequence relationships between component failures and system hazards in real-time safety critical systems. 
81|-||Using social networking and semantic web technology in software engineering â Use cases, patterns, and a case study|
81|-||A component- and push-based architectural style for ajax applications|A new breed of web application, dubbed ajax, is emerging in response to a limited degree of interactivity in large-grain stateless Web interactions. At the heart of this new approach lies a single page interaction model that facilitates rich interactivity. Also push-based solutions from the distributed systems are being adopted on the web for ajax applications. The field is, however, characterized by the lack of a coherent and precisely described set of architectural concepts. As a consequence, it is rather difficult to understand, assess, and compare the existing approaches. We have studied and experimented with several ajax frameworks trying to understand their architectural properties. In this paper, we summarize four of these frameworks and examine their properties and introduce the spiar architectural style which captures the essence of ajax applications. We describe the guiding software engineering principles and the constraints chosen to induce the desired properties. The style emphasizes user interface component development, intermediary delta-communication between client/server components, and push-based event notification of state changes through the components, to improve a number of properties such as user interactivity, user-perceived latency, data coherence, and ease of development. In addition, we use the concepts and principles to discuss various open issues in ajax frameworks and application development. 
81|-||Synthesis of decentralized and concurrent adaptors for correctly assembling distributed component-based systems|Building a distributed system from third-party components introduces a set of problems, mainly related to compatibility and communication. Our existing approach to solve such problems is to build a centralized adaptor which restricts the system’s behavior to exhibit only deadlock-free and desired interactions. However, in a distributed environment such an approach is not always suitable. In this paper, we show how to automatically generate a distributed adaptor for a set of black-box components. First, by taking into account a specification of the interaction behavior of each component, we synthesize a behavioral model for a centralized glue adaptor. Second, from the synthesized adaptor model and a specification of the desired behavior that must be enforced, we generate one local adaptor for each component. The local adaptors cooperatively behave as the centralized one restricted with respect to the specified desired interactions. 
81|-||An architectural approach to the correct and automatic assembly of evolving component-based systems|Software components are specified, designed and implemented with the intention to be reused, and they are assembled in various contexts in order to produce a multitude of software systems. However, in the practice of software development, this ideal scenario is often unrealistic. This is mainly due to the lack of an automatic and efficient support to predict properties of the assembly code by only assuming a limited knowledge of the properties of single components. Moreover, to make effective the component-based vision, the assembly code should evolve when things change, i.e., the properties guaranteed by the assembly, before a change occurs, must hold also after the change. Glue code synthesis approaches technically permit one to construct an assembly of components that guarantees specific properties but, practically, they may suffer from the state-space explosion phenomenon.In this paper, we propose a Software Architecture (SA) based approach in which the usage of the system SA and of SA verification techniques allows the system assembler to design architectural components whose interaction is verified with respect to the specified properties. By exploiting this validation, the system assembler can perform code synthesis by only focusing on each single architectural component, hence refining it as an assembly of actual components which respect the architectural component observable behaviour. In this way code synthesis is performed locally on each architectural component, instead of globally on the whole system interactions, hence reducing the state-space explosion phenomenon.The approach can be equally well applied to efficiently manage the whole reconfiguration of the system when one or more components need to be updated, still maintaining the required properties. The specified and verified system SA is used as starting point for the derivation of glue adaptors that are required to apply changes in the composed system. The approach is firstly illustrated over an explanatory example and is then applied and validated over a real-world industrial case study. 
81|-||Execution trace analysis through massive sequence and circular bundle views|An important part of many software maintenance tasks is to gain a sufficient level of understanding of the system at hand. The use of dynamic information to aid in this software understanding process is a common practice nowadays. A major issue in this context is scalability: due to the vast amounts of information, it is a very difficult task to successfully navigate through the dynamic data contained in execution traces without getting lost.In this paper, we propose the use of two novel trace visualization techniques based on the massive sequence and circular bundle view, which both reflect a strong emphasis on scalability. These techniques have been implemented in a tool called Extravis. By means of distinct usage scenarios that were conducted on three different software systems, we show how our approach is applicable in three typical program comprehension tasks: trace exploration, feature location, and top-down analysis with domain knowledge. 
81|-||Analyzing clusters of class characteristics in OO applications|The transition from Java 1.4 to Java 1.5 has provided the programmer with more flexibility due to the inclusion of several new language constructs, such as parameterized types. This transition is expected to increase the number of class clusters exhibiting different combinations of class characteristics. In this paper we investigate how the number and distribution of clusters are expected to change during this transition. We present the results of an empirical study were we analyzed applications written in both Java 1.4 and 1.5. In addition, we show how the variability of the combinations of class characteristics may affect the testing of class members. 
81|-||An empirical study of the relationship between the concepts expressed in source code and dependence|Programs express domain-level concepts in their source code. It might be expected that such concepts would have a degree of semantic cohesion. This cohesion ought to manifest itself in the dependence between statements all of which contribute to the computation of the same concept. This paper addresses a set of research questions that capture this informal observation. It presents the results of experiments on 10 programs that explore the relationship between domain-level concepts and dependence in source code. The results show that code associated with concepts has a greater degree of coherence, with tighter dependence. This finding has positive implications for the analysis of concepts as it provides an approach to decompose a program into smaller executable units, each of which captures the behaviour of the program with respect to a domain-level concept. 
81|-||OCL2Trigger: Deriving active mechanisms for relational databases using Model-Driven Architecture|
81|-||A self-stabilizing autonomic recoverer for eventual Byzantine software|We suggest modeling software package flaws (bugs) by assuming eventual Byzantine behavior of the package. We assume that if a program is started in a predefined initial state, it will exhibit legal behavior for a period of time but will eventually become Byzantine. We assume that this behavior pattern can be attributed to the fact that the manufacturer had performed sufficient package tests for limited time scenarios. Restarts are useful for recovering such systems. We suggest a general, yet practical, framework and paradigm for the monitoring and restarting of systems where the framework and paradigm are based on a theoretical foundation. An autonomic recoverer that monitors and initiates system recovery is proposed. It is designed to handle a task, given specific task requirements in the form of predicates and actions. A directed acyclic graph subsystem hierarchical structure is used by a consistency monitoring procedure for achieving a gracious recovery. The existence and correct functionality of the autonomic recovery is guaranteed by the use of a self-stabilizing kernel resident (anchor) process. The autonomic recoverer uses a new scheme for liveness assurance via on-line monitoring that complements known schemes for on-line safety assurance. 
81|-||Security analysis of the full-round DDO-64 block cipher|DDO-64 is a 64-bit Feistel-like block cipher based on data-dependent operations (DDOs). It is composed of 8 rounds and uses a 128-bit key. There are two versions of DDO-64, named DDO-64V1 and DDO-64V2, according to the key schedule. They were designed under an attempt for improving the security and performance of DDP-based ciphers. In this paper, however, we show that like most of the existing DDP-based ciphers, DDO-64V1 and DDO-64V2 are also vulnerable to related-key attacks. The attack on DDO-64V1 requires 235.5 related-key chosen plaintexts and 263.5 encryptions while the attack on DDO-64V2 only needs 8 related-key chosen plaintexts and 231 encryptions; our attacks are both mainly due to their simple key schedules and structural weaknesses. These works are the first known cryptanalytic results on DDO-64V1 and DDO-64V2 so far. 
81|-||Characterization of the evolution of a news Web site|The Web has become a ubiquitous tool for distributing knowledge and information and for conducting businesses. To exploit the huge potential of the Web as a global information repository, it is necessary to understand its dynamics. These issues are particularly important for news Web sites as they are expected to provide fresh information on current world events to a potentially large user population. This paper presents an experimental study aimed at characterizing and modeling the evolution of a news Web site. We focused on the MSNBC Web site as it is a good representative of its category in terms of structure, news coverage and popularity. Specifically, we analyzed how often and to what extent the content of this site changed and we identified models describing its dynamics. The study has shown that the rate of page creations and updates was characterized by some well defined patterns that varied as a function of time of day and day of week. On the contrary, the content of individual pages changed to a different extent. Most updates involved a very small fraction of their content, whereas very few were more extensive and spread over the whole page. By taking into accounts all these aspects, we derived analytical models able to accurately capture and reproduce the evolution of the news Web site. 
81|-||Data access in distributed simulations of multi-agent systems|Distributed simulation has emerged as an important instrument for studying large-scale complex systems. Such systems inherently consist of a large number of components, which operate in a large shared state space interacting with it in highly dynamic and unpredictable ways. Optimising access to the shared state space is crucial for achieving efficient simulation executions. Data accesses may take two forms: locating data according to a set of attribute value ranges (range query) or locating a particular state variable from the given identifier (ID query and update). This paper proposes two alternative routing approaches, namely the address-based approach, which locates data according to their address information, and the range-based approach, whose operation is based on looking up attribute value range information along the paths to the destinations. The two algorithms are discussed and analysed in the context of PDES-MAS, a framework for the distributed simulation of multi-agent systems, which uses a hierarchical infrastructure to manage the shared state space. The paper introduces a generic meta-simulation framework which is used to perform a quantitative comparative analysis of the proposed algorithms under various circumstances. 
81|-||Can k-NN imputation improve the performance of C4.5 with small software project data sets? A comparative evaluation|Missing data is a widespread problem that can affect the ability to use data to construct effective prediction systems. We investigate a common machine learning technique that can tolerate missing values, namely C4.5, to predict cost using six real world software project databases. We analyze the predictive performance after using the k-NN missing data imputation technique to see if it is better to tolerate missing data or to try to impute missing values and then apply the C4.5 algorithm. For the investigation, we simulated three missingness mechanisms, three missing data patterns, and five missing data percentages. We found that the k-NN imputation can improve the prediction accuracy of C4.5. At the same time, both C4.5 and k-NN are little affected by the missingness mechanism, but that the missing data pattern and the missing data percentage have a strong negative impact upon prediction (or imputation) accuracy particularly if the missing data percentage exceeds 40%. 
81|-||Exploiting synergies between semantic reasoning and personalization strategies in intelligent recommender systems: A case study|Current recommender systems attempt to identify appealing items for a user by applying syntactic matching techniques, which suffer from significant limitations that reduce the quality of the offered suggestions. To overcome this drawback, we have developed a domain-independent personalization strategy that borrows reasoning techniques from the Semantic Web, elaborating recommendations based on the semantic relationships inferred between the user’s preferences and the available items. Our reasoning-based approach improves the quality of the suggestions offered by the current personalization approaches, and greatly reduces their most severe limitations. To validate these claims, we have carried out a case study in the Digital TV field, in which our strategy selects TV programs interesting for the viewers from among the myriad of contents available in the digital streams. Our experimental evaluation compares the traditional approaches with our proposal in terms of both the number of TV programs suggested, and the users’ perception of the recommendations. Finally, we discuss concerns related to computational feasibility and scalability of our approach. 
81|-||The mechanisms of project management of software development|The changing environments of software development such as component-based, distributed and outsourced software development require matching changes by project managers to monitor, control and coordinate their projects. While the objectives of project management may be well established, the mechanisms with which those objectives are achieved are less well known. An empirical study was undertaken to investigate which mechanisms were used by practising project managers to monitor, control and coordinate software development projects.First, the types of mechanisms are discussed so that the mechanisms can be classified usefully. Then, the design of the empirical study is described. The data were collected through structured interview, which provided both quantitative and qualitative data. The data are analysed for each mechanism separately and the findings presented. The study found that project managers use multiple mechanisms to achieve project management objectives and use the same mechanism to serve multiple objectives. Further research is suggested to investigate project management from the opposite orientation, that is, which objectives are served by specific project management mechanisms. 
81|-||A receiver-centric rate control scheme for layered video streams in the Internet|We present a new end-to-end protocol, namely Dynamic Video Rate Control (DVRC), which operates on top of UDP and enables the adaptive delivery of layered video streams over the Internet. The protocol optimizes the performance on video delivery with concern to friendliness with interfering traffic. DVRC enables a closed-loop control between server and client, where the receiver detects the state of congestion, determines the proper transmission rate, and eventually opts for the optimal number of layers that should be delivered according to this rate. The protocol relies on a hybrid Additive Increase Additive Decrease (AIAD)/Additive Increase Multiplicative Decrease (AIMD) algorithm (namely AIAMD) that manages to differentiate congestive and non-congestive loss by utilizing history in its control rules. AIAMD combines the most desirable features of AIAD and AIMD, reacting gently to random loss and more aggressively to congestion and adapting effectively to the dynamics of the network. Therefore, DVRC enables the desired smoothness for video streaming applications and at the same time avoids significant damage during congestion. Exploring DVRC’s potential through extensive simulations, we identify notable gains in terms of bandwidth utilization and smooth video delivery. Furthermore, our results indicate that the protocol allocates a well-balanced amount of network resources maintaining friendliness with corporate TCP connections. 
81|-||What do software architects really do?|To be successful, a software architect—or a software architecture team, collectively—must strike a delicate balance between an external focus—both outwards: Listening to customers, users, watching technology, developing a long-term vision, and inwards: driving the development teams—and an internal, reflective focus: spending time to make the right design choices, validating them, and documenting them. Teams that stray too far away from this metastable equilibrium fall into some traps that we describe as antipatterns of software architecture teams. 
81|1|http://www.sciencedirect.com/science/journal/01641212/81/1|Software performance tuning of software product family architectures: Two case studies in the real-time embedded systems domain|Software performance is an important non-functional quality attribute and software performance evaluation is an essential activity in the software development process. Especially in embedded real-time systems, software design and evaluation are driven by the needs to optimize the limited resources, to respect time deadlines and, at the same time, to produce the best experience for end-users. Software product family architectures add additional requirements to the evaluation process. In this case, the evaluation includes the analysis of the optimizations and tradeoffs for the whole products in the family. Performance evaluation of software product family architectures requires knowledge and a clear understanding of different domains: software architecture assessments, software performance and software product family architecture. We have used a scenario-driven approach to evaluate performance and dynamic memory management efficiency in one Nokia software product family architecture. In this paper we present two case studies. Furthermore, we discuss the implications and tradeoffs of software performance against evolvability and maintenability in software product family architectures. 
81|1||Description templates for agent-oriented patterns|A number of agent-oriented patterns have been proposed to share agent-oriented software engineering experiences. However, most of the existing descriptions of these patterns are incomprehensive or inconsistent with descriptions of similar patterns by other authors. We seek to improve the communication and comprehension of agent-oriented patterns by improving their descriptions. This paper presents a pattern template structure for defining agent-oriented pattern templates. We used the template structure to define eight agent-oriented pattern templates. We describe the elements of these eight templates. We illustrate the expressiveness of these templates by using three of them to describe three existing agent-oriented patterns. 
81|1||DRAMA: A framework for domain requirements analysis and modeling architectures in software product lines|One of the benefits of software product line approach is to improve time-to-market. The changes in market needs cause software requirements to be flexible in product lines. Whenever software requirements are changed, software architecture should be evolved to correspond with them. Therefore, domain architecture should be designed based on domain requirements. It is essential that there is traceability between requirements and architecture, and that the structure of architecture is derived from quality requirements. The purpose of this paper is to provide a framework for modeling domain architecture based on domain requirements within product lines. In particular, we focus on the traceable relationship between requirements and architectural structures. Our framework consists of processes, methods, and a supporting tool. It uses four basic concepts, namely, goal based domain requirements analysis, Analytical Hierarchy Process, Matrix technique, and architecture styles. Our approach is illustrated using HIS (Home Integration System) product line. Finally, industrial examples are used to validate DRAMA. 
81|1||Dynamic interval-based labeling scheme for efficient XML query and update processing|XML data can be represented by a tree or graph structure and XML query processing requires the information of structural relationships among nodes. The basic structural relationships are parent–child and ancestor–descendant, and finding all occurrences of these basic structural relationships in an XML data is clearly a core operation in XML query processing. Several node labeling schemes have been suggested to support the determination of ancestor–descendant or parent–child structural relationships simply by comparing the labels of nodes. However, the previous node labeling schemes have some disadvantages, such as a large number of nodes that need to be relabeled in the case of an insertion of XML data, huge space requirements for node labels, and inefficient processing of structural joins. In this paper, we propose the nested tree structure that eliminates the disadvantages and takes advantage of the previous node labeling schemes. The nested tree structure makes it possible to use the dynamic interval-based labeling scheme, which supports XML data updates with almost no node relabeling as well as efficient structural join processing. Experimental results show that our approach is efficient in handling updates with the interval-based labeling scheme and also significantly improves the performance of the structural join processing compared with recent methods. 
81|1||Utilizing venation features for efficient leaf image retrieval|
81|1||DR-TCP: Downloadable and reconfigurable TCP|Advances in communication technology allow a variety of new network environments and services available very rapidly. Appearance of various network environments tends to enable a user with a mobile terminal to access among different network simultaneously. However, since new network environment affects performance of communication protocols, terminal systems should provide adaptation schemes for the protocols in order to keep the quality of network performance high. A possible solution is to make the protocol reconfigurable to be adapted to current network environment. Unfortunately, because most existing network systems are implemented monolithically, they cannot support protocol reconfiguration dynamically at runtime.This paper proposes a new reconfigurable model that enables TCP functions to be adapted whenever network environment is changed. The proposed scheme also supports binary-level protocol upgrade for extensibility by downloading new TCP variants which the terminal does not have for new network environment, and it is more suitable for mobile hand-held devices than existing source-level solution. Our model is based on a recursive state machine. We re-implement TCP Reno from scratch using our proposed model. The new implementation of TCP Reno is named DR-TCP. To demonstrate the effectiveness of DR-TCP, dynamic reconfiguration is performed over Internet, which successfully converts DR-TCP to TCP Westwood at runtime. 
81|1||Under storage constraints of epidemic backup node selection using HyMIS architecture for data replication in mobile peer-to-peer networks|The attainment of high reliability and availability is very difficult to be achieved in very complex wireless infrastructureless networks. Reliability concept describes essentially the transmission characteristics of infrastructureless networks, such as packet loss probability, packet duplication, data misinsertion, and corruption of packets. Some other metrics nowadays, aggregate and contribute to the aggravation of end to end reliability. In this work a new scheme for end to end reliable file/resource sharing is studied, among mobile peer-to-peer users. The proposed scheme uses the Hybrid Mobile Infostation System (HyMIS) to maintain and enhance the reliability of file/resource sharing process among wireless devices. Under various storage constraints the epidemic backup node selection is adopted, merging the advantages of epidemic file dissemination through purely mobile Infostations, using the HyMIS architecture. Examination through simulation is performed, taking into account many newly introduced storage metrics, for the performance evaluation of the proposed scheme. These storage metrics are tuned into certain bounded values to enable high packet delivery ratio. Results show that this scheme under certain storage requirements offer a reliable and robust solution for sharing resources of any capacity in dynamic mobile peer-to-peer wireless environments. 
81|1||Bidder-anonymous English auction scheme with privacy and public verifiability|This work studies the English auction protocol, which comprises three interactive parties—the Registration Manager, the Auction Manager and the Bidder. The registration manager confirms and authenticates the identities of bidders; the auction manager issues the bidding rights and maintains order in holding the auction. The proposed scheme provides the following security features—anonymity, traceability, no framing, unforgeability, non-repudiation, fairness, public verifiability, non-linkability among various auction rounds, linkability within a single auction round, bidding efficiency, single registration, and easy revocation. The scheme developed herein can effectively reduce the load on the registration and auction managers by requiring the end server to derive the key. It also eliminates the need for bidders to download the auction key and the auction certificate. Hence, the time complexity of processing data is clearly reduced and the best interests of the bidders can be achieved. Accordingly, the scheme is consistent with the actual practice of online transactions. 
81|1||A parametrized algorithm that implements sequential, causal, and cache memory consistencies|In this paper, we present an algorithm that can be used to implement sequential, causal, or cache consistency in distributed shared memory (DSM) systems. For this purpose it includes a parameter that allows us to choose the consistency model to be implemented. If all processes run the algorithm with the same value in this parameter, the corresponding consistency is achieved. (Additionally, the algorithm tolerates that processes use certain combination of parameter values.) This characteristic allows a concrete consistency model to be chosen, but implements it with the more efficient algorithm in each case (depending of the requirements of the applications). Additionally, as far as we know, this is the first algorithm proposed that implements cache coherence.In our algorithm, all the read and write operations are executed locally when implementing causal and cache consistency (i.e., they are fast). It is known that no sequential algorithm has only fast memory operations. In our algorithm, however, all the write operations and some read operations are fast when implementing sequential consistency. The algorithm uses propagation and full replication, where the values written by a process are propagated to the rest of the processes. It works in a cyclic turn fashion, with each process of the DSM system, broadcasting one message in turn. The values written by the process are sent in the message (instead of sending one message for each write operation): However, unnecessary values are excluded. All this permits the amount of message traffic owing to the algorithm to be controlled. 
81|1||Solving a real-time allocation problem with constraint programming|In this paper, we present an original approach (CPRTA for “Constraint Programming for solving Real-Time Allocation”) based on constraint programming to solve a static allocation problem of hard real-time tasks. This problem consists in assigning periodic tasks to distributed processors in the context of fixed priority preemptive scheduling. CPRTA is built on dynamic constraint programming together with a learning method to find a feasible processor allocation under constraints. Two efficient new approaches are proposed and validated with experimental results. Moreover, CPRTA exhibits very interesting properties. It is complete (if a problem has no solution, the algorithm is able to prove it); it is non-parametric (it does not require specific tuning) thus allowing a large diversity of models to be easily considered. Finally, thanks to its capacity to explain failures, it offers attractive perspectives for guiding the architectural design process. 
81|1||A high quality steganographic method with pixel-value differencing and modulus function|In this paper, we shall propose a new image steganographic technique capable of producing a secret-embedded image that is totally indistinguishable from the original image by the human eye. In addition, our new method avoids the falling-off-boundary problem by using pixel-value differencing and the modulus function. First, we derive a difference value from two consecutive pixels by utilizing the pixel-value differencing technique (PVD). The hiding capacity of the two consecutive pixels depends on the difference value. In other words, the smoother area is, the less secret data can be hidden; on the contrary, the more edges an area has, the more secret data can be embedded. This way, the stego-image quality degradation is more imperceptible to the human eye. Second, the remainder of the two consecutive pixels can be computed by using the modulus operation, and then secret data can be embedded into the two pixels by modifying their remainder. In our scheme, there is an optimal approach to alter the remainder so as to greatly reduce the image distortion caused by the hiding of the secret data. The values of the two consecutive pixels are scarcely changed after the embedding of the secret message by the proposed optimal alteration algorithm. Experimental results have also demonstrated that the proposed scheme is secure against the RS detection attack. 
81|10|http://www.sciencedirect.com/science/journal/01641212/81/10|Message from the guest editors|
81|10||An agent-based metric for quality of services over wireless networks|In a wireless LAN environment, clients tend to associate with the nearest access point (AP) which usually provides the strongest signal. However, this does not guarantee that users will receive the best quality of service (QoS) if the population sharing the network capacity were not considered. In other words, within the same access point, the more the population is, the less bandwidth each user will share, and the worse the quality of service will be. In this paper, we proposed an anticipative agent assistance (AAA) which is an agent-based metric for evaluating and managing the resource information of the wireless access points, computing the potential AP list, and providing clients with resource information of APs. We also propose a novel QoS feedback mechanism which allows users to promptly adjust the service quality with AAA according to the throughput and delay requirements. We evaluate the performance of our proposed method using the ns-2 simulator. Numerical results show that AAA achieves: (1) reduce the transmission delay, (2) increase the throughput, (3) improve the network utilization, (4) accommodate more users to access the networks, and (5) achieve load-balancing. Our metric is implementation feasible in various IEEE WLAN environments. 
81|10||CVM â A communication virtual machine|The convergence of data, voice, and multimedia communication over digital networks, coupled with continuous improvement in network capacity and reliability has resulted in a proliferation of communication technologies. Unfortunately, despite these new developments, there is no easy way to build new application-specific communication services. The stovepipe approach used today for building new communication services results in rigid technology, limited utility, lengthy and costly development cycle, and difficulty in integration. In this paper, we introduce communication virtual machine (CVM) that supports rapid conception, specification, and automatic realization of new application-specific communication services through a user-centric, model-driven approach. We present the concept, architecture, modeling language, prototypical design, and implementation of CVM in the context of a healthcare application. 
81|10||Specification, decomposition and agent synthesis for situation-aware service-based systems|Service-based systems are distributed computing systems with the major advantage of enabling rapid composition of distributed applications, such as collaborative research and development, e-business, health care, military applications and homeland security, regardless of the programming languages and platforms used in developing and running various components of the applications. In dynamic service-oriented computing environment, situation awareness (SAW) is needed for system monitoring, adaptive service coordination and flexible security policy enforcement. To greatly reduce the development effort of SAW capability in service-based systems and effectively support runtime system adaptation, it is necessary to automate the development of reusable and autonomous software components, called SAW agents, for situation-aware service-based systems. In this paper, a logic-based approach to declaratively specifying SAW requirements, decomposing SAW specifications for efficient distributed situation analysis, and automated synthesis of SAW agents is presented. This approach is based on AS3 calculus and logic, and our declarative model for SAW. Evaluation results of our approach are also presented. 
81|10||A backtracking search tool for constructing combinatorial test suites|Combinatorial testing is an important testing method. It requires the test cases to cover various combinations of parameters of the system under test. The test generation problem for combinatorial testing can be modeled as constructing a matrix which has certain properties. This paper first discusses two combinatorial testing criteria: covering array and orthogonal array, and then proposes a backtracking search algorithm to construct matrices satisfying them. Several search heuristics and symmetry breaking techniques are used to reduce the search time. This paper also introduces some techniques to generate large covering array instances from smaller ones. All the techniques have been implemented in a tool called EXACT (EXhaustive seArch of Combinatorial Test suites). A new optimal covering array is found by this tool. 
81|10||Software engineering article types: An analysis of the literature|The software engineering (SE) community has recently recognized that the field lacks well-established research paradigms and clear guidance on how to write good research reports. With no comprehensive guide to the different article types in the field, article writing and reviewing heavily depends on the expertise and the understanding of the individual SE actors.In this work, we classify and describe the article types published in SE with an emphasis on what is required for publication in journals and conference proceedings. Theoretically, we consider article types as genres, because we assume that each type of article has a specific function and a particular communicative purpose within the community, which the members of the community can recognize. We draw on written sources available, i.e. the instructions to authors/reviewers of major SE journals, the calls for papers of major SE conferences, and previous research published on the topic.Despite the fragmentation and limitations of the sources studied, we are able to propose a classification of different SE article types. Such classification helps in guiding the reader through the SE literature, and in making the researcher reflect on directions for improvements. 
81|10||XML security â A comparative literature review|Since the turn of the millenium, working groups of the W3C have been concentrating on the development of XML-based security standards, which are paraphrased as XML security. XML security consists of three recommendations: XML (digital) signature, XML encryption and XML key management specification (XKMS), all of them published by the W3C.By means of a review of the available literature the authors draw several conclusions about the status quo of XML security. Furthermore, the current state and focuses of research as well as the existing challenges are derived. Trends to different application areas – e.g. use of XML security for mobile computing – are also outlined. Based on this information the analyzed results are discussed and a future outlook is predicted. 
81|10||Facilitating software extension with design patterns and Aspect-Oriented Programming|Software products, especially large applications, need to continuously evolve, in order to adapt to the changing environment and updated requirements. With both the producer and the customer unwilling to replace the existing application with a completely new one, adoption of design constructs and techniques which facilitate the application extension is a major design issue. In the current work we investigate the behavior of an object-oriented software application at a specific extension scenario, following three implementation alternatives with regards to a certain design problem relevant to the extension. The first alternative follows a simplistic solution, the second makes use of a design pattern and the third applies Aspect-Oriented Programming techniques to implement the same pattern. An assessment of the three alternatives is attempted, both on a qualitative and a quantitative level, by identifying the additional design implications needed to perform the extension and evaluating the effect of the extension on several quality attributes of the application. 
81|10||Early quality monitoring in the development of real-time reactive systems|The increasing trend toward complex software systems has highlighted the need to incorporate quality requirements earlier in the development cycle. We propose a new methodology for monitoring quality in the earliest phases of real-time reactive system (RTRS) development. The targeted quality characteristics are functional complexity, performance, reliability, architectural complexity, maintainability, and test coverage. All these characteristics should be continuously monitored throughout the RTRS development cycle, to provide decision support and detect the first signs of low or decreasing quality as the system design evolves. The ultimate goal of this methodology is to assist developers in dealing with complex user requirements and ensure that the formal development process yields a high-quality application. Each aspect of quality monitoring is formalized mathematically and illustrated using a train–gate–controller case study. 
81|10||A framework for QoS-aware binding and re-binding of composite web services|QoS-aware dynamic binding of composite services provides the capability of binding each service invocation in a composition to a service chosen among a set of functionally equivalent ones to achieve a QoS goal, for example minimizing the response time while limiting the price under a maximum value.This paper proposes a QoS-aware binding approach based on Genetic Algorithms. The approach includes a feature for early run-time re-binding whenever the actual QoS deviates from initial estimates, or when a service is not available. The approach has been implemented in a framework and empirically assessed through two different service compositions. 
81|10||An incremental analysis for resource conflicts to workflow specifications|
81|10||Evolution support mechanisms for software product line process|
81|11|http://www.sciencedirect.com/science/journal/01641212/81/11|Systematic Component-Oriented development with Axiomatic Design|A form of design guidance is offered through a process that applies Axiomatic Design Theory to Component-Orientation. Axiomatic Design has been proposed by Do and Suh for object-oriented software development. Our approach leverages divide-and-conquer and find-integrate techniques that support service-based development as an alternative to developing code from scratch. Using this process, missing or conflicting components can be identified, and missing components can be defined. The effectiveness of our proposed system is demonstrated through an example based on High Level Architecture (HLA) simulations. Our Component-Oriented approach utilizing axiomatic design theory has been adapted to HLA Federation Development and Execution Process (FEDEP). As one of the Component-Oriented approaches proposed, FEDEP is able to obtain interoperability and reusability of available components, namely federates. 
81|11||An e-contracting reference architecture|Business-to-business e-contracting aims at automating the contracting process between companies. It improves the efficiency of the contracting process and enables the introduction of new business models that can be supported by companies. For the development of an e-contracting system, an architecture is required that describes the system components and the communication channels between them. This paper presents a reference architecture for the development of e-contracting systems. The architecture is designed on the basis of a requirement analysis of e-contracting systems. Established architectural principles are used in its design. The architecture can serve as a foundation in the analysis and design of concrete architectures of e-contracting systems. Furthermore, it can be used as a standardization model that facilitates system integration and communication of ideas. Its value for both software architects and business professionals makes it an important tool in the analysis and implementation of e-contracting systems. 
81|11||Reconciling usability and interactive system architecture using patterns|Traditional interactive system architectures such as MVC [Goldberg, A., 1984. Smaltalk-80: The Interactive Programming Environment, Addison-Wesley Publ.] and PAC [Coutaz, J., 1987. PAC, an implementation model for dialog design. In: Interact’87, Sttutgart, September 1987, pp. 431–436; Coutaz, J., 1990. Architecture models for interactive software: faillures and trends. In: Cockton, G. (Ed.), Engineering for Human–Computer Interaction, Elsevier Science Publ., pp. 137–153.] decompose the system into subsystems that are relatively independent, thereby allowing the design work to be partitioned between the user interfaces and underlying functionalities. Such architectures extend the independence assumption to usability, approaching the design of the user interface as a subsystem that can designed and tested independently from the underlying functionality. This Cartesian dichotomy can be fallacious, as functionalities buried in the application’s logic can sometimes affect the usability of the system. Our investigations model the relationships between internal software attributes and externally visible usability factors. We propose a pattern-based approach for dealing with these relationships. We conclude by discussing how these patterns can lead to a methodological framework for improving interactive system architectures, and how these patterns can support the integration of usability in the software design process. 
81|11||Software development cost estimation using wavelet neural networks|Software development has become an essential investment for many organizations. Software engineering practitioners have become more and more concerned about accurately predicting the cost and quality of software product under development. Accurate estimates are desired but no model has proved to be successful at effectively and consistently predicting software development cost. In this paper, we propose the use of wavelet neural network (WNN) to forecast the software development effort. We used two types of WNN with Morlet function and Gaussian function as transfer function and also proposed threshold acceptance training algorithm for wavelet neural network (TAWNN). The effectiveness of the WNN variants is compared with other techniques such as multilayer perceptron (MLP), radial basis function network (RBFN), multiple linear regression (MLR), dynamic evolving neuro-fuzzy inference system (DENFIS) and support vector machine (SVM) in terms of the error measure which is mean magnitude relative error (MMRE) obtained on Canadian financial (CF) dataset and IBM data processing services (IBMDPS) dataset. Based on the experiments conducted, it is observed that the WNN-Morlet for CF dataset and WNN-Gaussian for IBMDPS outperformed all the other techniques. Also, TAWNN outperformed all other techniques except WNN. 
81|11||The effectiveness of software metrics in identifying error-prone classes in post-release software evolution process|Many empirical studies have found that software metrics can predict class error proneness and the prediction can be used to accurately group error-prone classes. Recent empirical studies have used open source systems. These studies, however, focused on the relationship between software metrics and class error proneness during the development phase of software projects. Whether software metrics can still predict class error proneness in a system’s post-release evolution is still a question to be answered. This study examined three releases of the Eclipse project and found that although some metrics can still predict class error proneness in three error-severity categories, the accuracy of the prediction decreased from release to release. Furthermore, we found that the prediction cannot be used to build a metrics model to identify error-prone classes with acceptable accuracy. These findings suggest that as a system evolves, the use of some commonly used metrics to identify which classes are more prone to errors becomes increasingly difficult and we should seek alternative methods (to the metric-prediction models) to locate error-prone classes if we want high accuracy. 
81|11||Automatic, evolutionary test data generation for dynamic software testing|This paper proposes a dynamic test data generation framework based on genetic algorithms. The framework houses a Program Analyser and a Test Case Generator, which intercommunicate to automatically generate test cases. The Program Analyser extracts statements and variables, isolates code paths and creates control flow graphs. The Test Case Generator utilises two optimisation algorithms, the Batch-Optimistic (BO) and the Close-Up (CU), and produces a near to optimum set of test cases with respect to the edge/condition coverage criterion. The efficacy of the proposed approach is assessed on a number of programs and the empirical results indicate that its performance is significantly better compared to existing dynamic test data generation methods. 
81|11||A framework to support the evaluation, adoption and improvement of agile methods in practice|Agile methods are often seen as providing ways to avoid overheads typically perceived as being imposed by traditional software development environments. However, few organizations are psychologically or technically able to take on an agile approach rapidly and effectively. Here, we describe a number of approaches to assist in such a transition. The Agile Software Solution Framework (ASSF) provides an overall context for the exploration of agile methods, knowledge and governance and contains an Agile Toolkit for quantifying part of the agile process. These link to the business aspects of software development so that the business value and agile process are well aligned. Finally, we describe how these theories are applied in practice with two industry case studies using the Agile Adoption and Improvement Model (AAIM). 
81|11||A pilot study to compare programming effort for two parallel programming models|ContextWriting software for the current generation of parallel systems requires significant programmer effort, and the community is seeking alternatives that reduce effort while still achieving good performance.ObjectiveMeasure the effect of parallel programming models (message-passing vs. PRAM-like) on programmer effort.Design, setting, and subjectsOne group of subjects implemented sparse-matrix dense-vector multiplication using message-passing (MPI), and a second group solved the same problem using a PRAM-like model (XMTC). The subjects were students in two graduate-level classes: one class was taught MPI and the other was taught XMTC.Main outcome measuresDevelopment time, program correctness.ResultsMean XMTC development time was 4.8 h less than mean MPI development time (95% confidence interval, 2.0–7.7), a 46% reduction. XMTC programs were more likely to be correct, but the difference in correctness rates was not statistically significant (p = .16).ConclusionsXMTC solutions for this particular problem required less effort than MPI equivalents, but further studies are necessary which examine different types of problems and different levels of programmer experience. 
81|11||Benchmarking temporal database models with interval-based and temporal element-based timestamping|Starting from mid 1980s, there has been a debate about what data model is most appropriate for temporal databases. A fundamental choice one has to make is whether to use intervals of time or temporal elements to timestamp objects and events with the periods of validity. The advantage of using interval timestamps is that Start and End columns can be added to relations for treating them within the framework of classical databases, leading to quick implementation. Temporal elements are finite unions of intervals. The advantage of temporal elements is that timestamps become implicitly associated with values, tuples, and relations. Furthermore, since temporal elements, by design, are closed under set theoretical operations such as union, intersection and complementation, they lead to query languages that are natural. Here, we investigate the ease of use as well as system performance for the two approaches to help settle the debate. 
81|11||Segmented software cost estimation models based on fuzzy clustering|Parametric software cost estimation models are based on mathematical relations, obtained from the study of historical software projects databases, that intend to be useful to estimate the effort and time required to develop a software product. Those databases often integrate data coming from projects of a heterogeneous nature. This entails that it is difficult to obtain a reasonably reliable single parametric model for the range of diverging project sizes and characteristics. A solution proposed elsewhere for that problem was the use of segmented models in which several models combined into a single one contribute to the estimates depending on the concrete characteristic of the inputs. However, a second problem arises with the use of segmented models, since the belonging of concrete projects to segments or clusters is subject to a degree of fuzziness, i.e. a given project can be considered to belong to several segments with different degrees.This paper reports the first exploration of a possible solution for both problems together, using a segmented model based on fuzzy clusters of the project space. The use of fuzzy clustering allows obtaining different mathematical models for each cluster and also allows the items of a project database to contribute to more than one cluster, while preserving constant time execution of the estimation process. The results of an evaluation of a concrete model using the ISBSG 8 project database are reported, yielding better figures of adjustment than its crisp counterpart. 
81|11||Analysis of ID-based restrictive partially blind signatures and applications|Partially blind signatures and restrictive blind signatures are two important techniques in electronic cash systems and voting systems. Restrictive partially blind signatures incorporate the advantages of these two blind signatures. Recently, Chen–Zhang–Liu first proposed an ID-based restrictive partially blind signature from bilinear pairings (Chen, X.F., Zhang, F.G., Liu, S.L., 2007. ID-based restrictive partially blind signatures and applications. The Journal of Systems and Software 80 (2), 164–171). However, in this paper, we show that Chen–Zhang–Liu’s scheme has a security weakness. Their scheme does not satisfy the property of restrictiveness as they claimed, and an account-holder cannot be revealed when double-spending happens. 
81|11||The influence of organizational culture on the adoption of extreme programming|
81|11||A model for software rework reduction through a combination of anomaly metrics|Analysis of anomalies reported during testing of a project can tell a lot about how well the processes and products work. Still, organizations rarely use anomaly reports for more than progress tracking although projects commonly spend a significant part of the development time on finding and correcting faults. This paper presents an anomaly metrics model that organizations can use for identifying improvements in the development process, i.e. to reduce the cost and lead-time spent on rework-related activities and to improve the quality of the delivered product. The model is the result of a four year research project performed at Ericsson. 
81|11||Real-time scheduling of batch systems using Petri nets and linear logic|This paper presents an approach to model, design and verify scenarios of real-time systems used in the scheduling and global coordination of batch systems. The initial requirements of a system specified with sequence diagrams are translated into a single p-time Petri net model representing the global behavior of the system. For the Petri net fragments involved in conflicts, symbolic production and consumption dates assigned to tokens are calculated based on the sequent calculus of linear logic. These dates are then used for off-line conflict resolution within a token player algorithm used for scenario verification of real-time specifications and which can be seen as a simulation tool for UML interaction diagrams. 
81|11||Avoiding semantic and temporal gaps in developing software intensive systems|Development of software intensive systems (systems) in practice involves a series of self-contained phases for the lifecycle of a system. Semantic and temporal gaps, which occur among phases and among developer disciplines within and across phases, hinder the ongoing development of a system because of the interdependencies among phases and among disciplines. Such gaps are magnified among systems that are developed at different times by different development teams, which may limit reuse of artifacts of systems development and interoperability among the systems. This article discusses such gaps and a systems development process for avoiding them. 
81|11||Multi-layer survivable routing mechanism in GMPLS based optical networks|The structure of IP/MPLS over optical networks has become the trend for new generation backbone transport networks since the technology of general multi-protocol label switching (GMPLS) can realize the seamless converage between IP and optical networks. At the same time, due to the high-speed transmission rate of each wavelength in optical networks, survivability is a very important issue and has been studied all through these years. Therefore, in new generation GMPLS based networks, it is very necessary to investigate the multi-layer survivable algorithms. In this paper, we comprehensively review the existing survivable algorithms in multi-layer networks and analyze the shortages of current researches. Based on previous studies, we prospect challenges and propose new solutions of designing efficient survivable algorithms to well guide the future work of researchers in multi-layer networks. 
81|11||Application of redundant computation in program debugging|Programmers spend most of their time and resources in localizing program defects. On the other hand, they commit many errors by manipulating dynamic data improperly, which may produce dynamic memory problems, such as dangling pointer, memory leaks, and inaccessible objects. Dangling pointers can occur when a function returns a pointer to an automatic variable, or when trying to access a deleted object. Inaccessible objects occur when a pointer is assigned to point to another object, leaving the original object inaccessible, either by using the new operator or regular assignment operator. Memory leaks occur when a dynamic data is allocated but never de-allocated. The existence of such dynamic memory problems causes the programs to behave incorrectly. Improper usage of dynamic data is a common defect that is easy to commit, but is difficult to diagnose and discover. In this paper, we propose a dynamic approach that detects different types of program defects including those that occur as a result of misusing the dynamic data in computer programs. Our approach uses the notion of redundant computation to identify the suspicious locations in the program that may contain defects. Redundant computation is an execution of a program statement(s) that does not contribute to the program output. The notion of redundant computation is introduced as a potential indicator of defects in programs. We investigate the application of redundant computation in debugging programs. The detection of redundant computation indicates deficiency that may represent a bug in the program. The results of the experiment show that, the redundant computation detection can help the debuggers to localize the source(s) of the program defects. 
81|11||Bytecode fault injection for Java software|
81|11||Design and implementation of an efficient web cluster with content-based request distribution and file caching|We have implemented an efficient and scalable web cluster named LVS-CAD/FC (i.e. LVS with Content-Aware Dispatching and File Caching). In LVS-CAD/FC, a kernel-level one-way content-aware web switch based on TCP Rebuilding is implemented to examine and distribute the HTTP requests from clients to web servers, and the fast Multiple TCP Rebuilding is implemented to efficiently support persistent connection. Besides, a file-based web cache stores a small set of the most frequently accessed web files in server RAM to reduce disk I/Os and a light-weight redirect method is developed to efficiently redirect requests to this cache. In this paper, we have further proposed new policies related to content-based workload-aware request distribution, in which the web switch considers the content of requests and workload characterization in request dispatching. In particular, web files with more access frequencies would be duplicated in more servers’ file-based caches, such that hot web files can be served by more servers. Our goals are to improve cluster performance by obtaining better memory utilization and increasing the cache hit rates while achieving load balancing among servers. Experimental results of practical implementation on Linux show that LVS-CAD/FC is efficient and scales well. Besides, LVS-CAD/FC with the proposed policies can achieve 66.89% better performance than the Linux Virtual Server with a content-blind web switch. 
81|11||Local variable access behavior of a hardware-translation based Java virtual machine|Hardware bytecode translation is a technique to improve the performance of the Java virtual machine (JVM), especially on the portable devices for which the overhead of dynamic compilation is significant. However, since the translation is done on a single bytecode basis, a naive implementation of the JVM generates frequent memory accesses for local variables which can be not only a performance bottleneck but also an obstacle for instruction folding. A solution to this problem is to add a small register file to the data path of the microprocessor which is dedicated for storing local variables. However, the effectiveness of such a local variable register file depends on the size and the local variable access behavior of the applications.In this paper, we analyze the local variable access behavior of various Java applications. In particular, we will investigate the fraction of local variable accesses that are covered by the register file of a varying size, which determines the chip area overhead and the operation speed. We also evaluate the effectiveness of the sliding register window for parameter passing in context of JVM and on-the-fly optimization of local variable to register file mapping.With two types of exceptions, a 16-entry register file achieves coverages of up to 98%. The first type of exception is represented by the SAXON XSLT processor for which the effect of cold miss is significant. Adding the sliding window feature to the register file for parameter passing turns 6.2–13.3% of total accesses from miss to hit to the register file for the SAXON with XSLTMark. The second type of exception is represented by the FFT, which accesses more than 16 local variables for most of method invocations. In this case, on-the-fly profiling is effective. The hit ratio of a 16-entry register file for the FFT is increased from 44% to 83% by an array of 8-bit counters. 
81|11||The Java 5 generics compromise orthogonality to keep compatibility|In response to a long-lasting anticipation by the Java community, version 1.5 of the Java 2 platform – referred to as Java 5 – introduced generic types and methods to the Java language. The Java 5 generics are a significant enhancement to the language expressivity because they allow straightforward composition of new generic classes from existing ones while reducing the need for a plethora of type casts. While the Java 5 generics are expressive, the chosen implementation method, type erasure, has triggered undesirable orthogonality violations. This paper identifies six cases of orthogonality violations in the Java 5 generics and demonstrates how these violations are mandated by the use of type erasure. The paper also compares the Java 5 cases of orthogonality violations to compatible cases in C# 2 and NextGen 2 and analyzes the tradeoffs in the three approaches. The conclusion is that Java 5 users face new challenges: a number of generic type expressions are forbidden, while others that are allowed are left unchecked by the compiler. 
81|11||Quality-of-service oriented web service composition algorithm and planning architecture|In the next few decades, it is expected that web services will proliferate, many web services will offer the same services, and the clients will demand more value added and informative services rather than those offered by single, isolated web services. As the result, the problem of synthesizing web services of high quality will be raised as a prominent issue. The clients will face the trouble of choosing or creating composition plans, among numerous possible plans, that satisfy their quality-of-service (QoS) requirements. Typical QoS properties associated with a web service are the execution cost and time, availability, successful execution rate, reputation, and usage frequency. In engineering perspective, generating the composition plan that fulfills a client’s QoS requirement is a time-consuming optimization problem. To resolve the problem in a timely manner, we propose a constraint satisfaction based web service composition algorithm that combines tabu search and simulated annealing meta-heuristics. As an implementation framework of the algorithm, we suggest a QoS-oriented web service composition planning architecture. The architecture maintains expert made composition schemas in a service category and assists the client as pure user to choose the one he/she wants to use. The main modules of the architecture are composition broker and execution plan optimizer. With the aid of the UDDI server, the composition broker discovers candidate outsourced web services for each atomic process of the selected schema and gathers QoS information on the web services. After that, the execution plan optimizer runs the web service composition algorithm in order to generate a QoS-oriented composition plan. The performance of the algorithm was tested in a simulated environment. 
81|11||An efficient nonuniform index in the wireless broadcast environments|Data broadcast is an efficient dissemination method to deliver information to mobile clients through the wireless channel. It allows a huge number of the mobile clients simultaneously access data in the wireless environments. In real-life applications, more popular data may be frequently accessed by clients than less popular ones. Under such scenarios, Acharya et al.’s Broadcast Disks algorithm (BD) allocates more popular data appeared more times in a broadcast period than less popular ones, i.e., the nonuniform broadcast, and provides a good performance on reducing client waiting time. However, mobile devices should constantly tune in to the wireless broadcast channel to examine data, consuming a lot of energy. Using index technologies on the broadcast file can reduce a lot of energy consumption of the mobile devices without significantly increasing client waiting time. In this paper, we propose an efficient nonuniform index called the skewed index, SI, over BD. The proposed algorithm builds an index tree according to skewed access patterns of clients, and allocates index nodes for the popular data more times than those for the less popular ones in a broadcast cycle. From our experimental study, we have shown that our proposed algorithm outperforms the flexible index and the flexible distributed index. 
81|2|http://www.sciencedirect.com/science/journal/01641212/81/2|Editorial|
81|2||Traffic-aware stress testing of distributed real-time systems based on UML models using genetic algorithms|This paper presents a model-driven, stress test methodology aimed at increasing chances of discovering faults related to network traffic in distributed real-time systems (DRTS). The technique uses the UML 2.0 model of the distributed system under test, augmented with timing information, and is based on an analysis of the control flow in sequence diagrams. It yields stress test requirements that are made of specific control flow paths along with time values indicating when to trigger them. The technique considers different types of arrival patterns (e.g., periodic) for real-time events (common to DRTSs), and generates test requirements which comply with such timing constraints. Though different variants of our stress testing technique already exist (that stress different aspects of a distributed system), they share a large amount of common concepts and we therefore focus here on one variant that is designed to stress test the system at a time instant when data traffic on a network is maximal. Our technique uses genetic algorithms to find test requirements which lead to maximum possible traffic-aware stress in a system under test. Using a real-world DRTS specification, we design and implement a prototype DRTS and describe, for that particular system, how the stress test cases are derived and executed using our methodology. The stress test results indicate that the technique is significantly more effective at detecting network traffic-related faults when compared to test cases based on an operational profile. 
81|2||Applying machine learning to software fault-proneness prediction|The importance of software testing to quality assurance cannot be overemphasized. The estimation of a module’s fault-proneness is important for minimizing cost and improving the effectiveness of the software testing process. Unfortunately, no general technique for estimating software fault-proneness is available. The observed correlation between some software metrics and fault-proneness has resulted in a variety of predictive models based on multiple metrics. Much work has concentrated on how to select the software metrics that are most likely to indicate fault-proneness. In this paper, we propose the use of machine learning for this purpose. Specifically, given historical data on software metric values and number of reported errors, an Artificial Neural Network (ANN) is trained. Then, in order to determine the importance of each software metric in predicting fault-proneness, a sensitivity analysis is performed on the trained ANN. The software metrics that are deemed to be the most critical are then used as the basis of an ANN-based predictive model of a continuous measure of fault-proneness. We also view fault-proneness prediction as a binary classification task (i.e., a module can either contain errors or be error-free) and use Support Vector Machines (SVM) as a state-of-the-art classification method. We perform a comparative experimental study of the effectiveness of ANNs and SVMs on a data set obtained from NASA’s Metrics Data Program data repository. 
81|2||Experiments with test case prioritization using relevant slices|Software testing and retesting occurs continuously during the software development lifecycle to detect errors as early as possible and to gain confidence that changes to software do not introduce defects. Once developed, test suites are reused and updated frequently, and their sizes grow as software evolves. Due to time and resource constraints, an important goal during regression testing of software is to prioritize the execution of test cases in a suite so as to improve the chances of increasing the rate of fault detection. Prior techniques for test case prioritization are based on the total number of coverage requirements exercised by the test cases. In this paper, we present a new approach to prioritize test cases that takes into account the coverage requirements present in the relevant slices of the outputs of test cases. We have implemented three different heuristics based on our relevant slicing based approach to prioritize test cases and conducted experiments to compare the effectiveness of our techniques with those of the traditional techniques that only account for the total requirement coverage. Our detailed experimental study and results provide interesting insights into the effectiveness of using relevant slices for test case prioritization in terms of ability to achieve high rate of fault detection. 
81|2||Testing input validation in Web applications through automated model recovery|
81|2||A relation-based method combining functional and structural testing for test case generation|Specification-based (or functional) testing enables us to detect errors in the implementation of functions defined in specifications, but since specifications are often incomplete in practice for some reasons (e.g., lack of ideas, no time to write), it is unlikely to be sufficient for testing all parts of corresponding programs. On the other hand, implementation-based (or structural) testing focuses on the examination of program structures, which allows us to test all parts of the programs, but may not be effective to show whether the programs properly implement the corresponding specifications. To perform a comprehensive testing of a program in practice, it is important to adopt both specification-based and implementation-based testing. In this paper we describe a relation-based test method that combines the specification-based and the implementation-based testing approaches. We establish a set of relations for test case generation, illustrate how the method is used with an example, and investigate the effectiveness and weakness of the method through an experiment on testing a software tool system. 
81|2||An empirical, path-oriented approach to software analysis and testing|Error flow analysis and testing techniques focus on the introduction of errors through code faults into data states of an executing program, and their subsequent cancellation or propagation to output. The goals and limitations of several error flow techniques are discussed, including mutation analysis, fault-based testing, PIE analysis, and dynamic impact analysis. The attributes desired of a good error flow technique are proposed, and a model called dynamic error flow analysis (DEFA) is described that embodies many of these attributes. A testing strategy is proposed that uses DEFA information to select an optimal set of test paths and to quantify the results of successful testing. An experiment is presented that illustrates this testing strategy. In this experiment, the proposed testing strategy outperforms mutation testing in catching arbitrary data state errors. 
81|2||A search-based framework for automatic testing of MATLAB/Simulink models|Search-based test-data generation has proved successful for code-level testing but almost no search-based work has been carried out at higher levels of abstraction. In this paper the application of such approaches at the higher levels of abstraction offered by MATLAB/Simulink models is investigated and a wide-ranging framework for test-data generation and management is presented. Model-level analogues of code-level structural coverage criteria are presented and search-based approaches to achieving them are described. The paper also describes the first search-based approach to the generation of mutant-killing test data, addressing a fundamental limitation of mutation testing. Some problems remain whatever the level of abstraction considered. In particular, complexity introduced by the presence of persistent state when generating test sequences is as much a challenge at the Simulink model level as it has been found to be at the code level. The framework addresses this problem. Finally, a flexible approach to test sub-set extraction is presented, allowing testing resources to be deployed effectively and efficiently. 
81|2||Automated generation of test suites from formal specifications of real-time reactive systems|Real-time reactive systems are among the most difficult systems to test because of their size and complex time-dependent functionality. The number of test experiments for such systems is very large, if not infinite. Often such systems arise in safety-critical contexts. Hence, such systems require a rigorous analysis and thorough testing before they are deployed. This paper addresses test case generation methods and a metric-based test case selection algorithm for sufficient testing of real-time reactive systems. The methods are rigorous, and based on the formal specifications of the system and its fault models. The test generation and execution of algorithms are implemented in TROMLAB, a formal framework for developing real-time reactive systems. The methods are applied to the formal specification of the Train–Gate–Controller (TGC) example, a bench-mark case study in the real-time systems community. A brief description of the experimental results obtained on the case study is given. 
81|3|http://www.sciencedirect.com/science/journal/01641212/81/3|Editorial â Outgoing Editor-in-Chief|
81|3||Editorial â Taking over|
81|3||Editorial|
81|3||Process pipeline scheduling|
81|3||An adaptive in-network aggregation operator for query processing in wireless sensor networks|A wireless sensor network (WSN) is composed of tens or hundreds of spatially distributed autonomous nodes, called sensors. Sensors are devices used to collect data from the environment related to the detection or measurement of physical phenomena. In fact, a WSN consists of groups of sensors where each group is responsible for providing information about one or more physical phenomena (e.g., group for collecting temperature data). Sensors are limited in power, computational capacity, and memory. Therefore, a query engine and query operators for processing queries in WSNs should be able to handle resource limitations such as memory and battery life. Adaptability has been explored as an alternative approach when dealing with these conditions. Adaptive query operators (algorithms) can adjust their behavior in response to specific events that take place during data processing. In this paper, we propose an adaptive in-network aggregation operator for query processing in sensor nodes of a WSN, called ADAGA (ADaptive AGgregation Algorithm for sensor networks). The ADAGA adapts its behavior according to memory and energy usage by dynamically adjusting data-collection and data-sending time intervals. ADAGA can correctly aggregate data in WSNs with packet replication. Moreover, ADAGA is able to predict non-performed detection values by analyzing collected values. Thus, ADAGA is able to produce results as close as possible to real results (obtained when no resource constraint is faced). The results obtained through experiments prove the efficiency of ADAGA. 
81|3||Accelerating k-medoid-based algorithms through metric access methods|Scalable data mining algorithms have become crucial to efficiently support KDD processes on large databases. In this paper, we address the task of scaling up k-medoid-based algorithms through the utilization of metric access methods, allowing clustering algorithms to be executed by database management systems in a fraction of the time usually required by the traditional approaches. We also present an optimization strategy that can be applied as an additional step of the proposed algorithm in order to achieve better clustering solutions. Experimental results based on several datasets, including synthetic and real ones, show that the proposed algorithm can reduce the number of distance calculations by a factor of more than three thousand times when compared to existing algorithms, while producing clusters of equivalent quality. 
81|3||An investigation of artificial neural networks based prediction systems in software project management|A critical issue in software project management is the accurate estimation of size, effort, resources, cost, and time spent in the development process. Underestimates may lead to time pressures that may compromise full functional development and the software testing process. Likewise, overestimates can result in noncompetitive budgets. In this paper, artificial neural network and stepwise regression based predictive models are investigated, aiming at offering alternative methods for those who do not believe in estimation models. The results presented in this paper compare the performance of both methods and indicate that these techniques are competitive with the APF, SLIM, and COCOMO methods. 
81|3||Using ontologies and Web services for content adaptation in Ubiquitous Computing|The diversity of small mobile devices and networks enabling users to access the Internet expands every day. In this highly dynamic environment of Ubiquitous Computing, current programming paradigms do not offer the flexibility needed for software reuse. To improve this flexibility, this paper proposes the use of ontologies and Web services, within a framework of components for the content adaptation domain, to facilitate the development of software based on reuse. A case study illustrates the use of the proposed solution. 
81|3||XMobile: A MB-UID environment for semi-automatic generation of adaptive applications for mobile devices|Ubiquitous Computing promises seamless access to information anytime, anywhere with different and heterogeneous devices. This kind of environment imposes new challenges to software development. For example, information and user interface should be adapted according to contextual characteristics such as user, environment, and access device. In case of device adaptation, the development challenge is related to the heterogeneity of the devices, which requires software engineers to create different versions for each type of device and every platform. This paper proposes a MB-UID (model-based user interface development) approach for semi-automatic generation of adaptive applications for mobile devices. An environment, called XMobile, offers a device-independent user interface framework and a code generation tool for providing fast development of multi-platform and adaptive applications according to device and platform features. A case study is also presented to illustrate how the environment can be used for constructing an application for heterogeneous devices with different network connectivity modes. 
81|3||Software Engineering Using RATionale|
81|3||Proactive and reactive multi-dimensional histogram maintenance for selectivity estimation|Many state-of-the-art selectivity estimation methods use query feedback to maintain histogram buckets, thereby using the limited memory efficiently. However, they are “reactive” in nature, that is, they update the histogram based on queries that have come to the system in the past for evaluation. In some applications, future occurrences of certain queries may be predicted and a “proactive” approach can bring much needed performance gain, especially when combined with the reactive approach. For these applications, this paper provides a method that builds customized proactive histograms based on query prediction and mergers them into reactive histograms when the predicted future arrives. Thus, the method is called the proactive and reactive histogram (PRHist). Two factors affect the usefulness of the proactive histograms and are dealt with during the merge process: the first is the predictability of queries and the second is the extent of data updates. PRHist adjusts itself to be more reactive or more proactive depending on these two factors. Through extensive experiments using both real and synthetic data and query sets, this paper shows that in most cases, PRHist outperforms STHoles, the state-of-the-art reactive method, even when only a small portion of the queries are predictable and a significant portion of data is updated. 
81|3||Understanding knowledge sharing activities in free/open source software projects: An empirical study|Free/Open Source Software (F/OSS) projects are people-oriented and knowledge intensive software development environments. Many researchers focused on mailing lists to study coding activities of software developers. How expert software developers interact with each other and with non-developers in the use of community products have received little attention. This paper discusses the altruistic sharing of knowledge between knowledge providers and knowledge seekers in the Developer and User mailing lists of the Debian project. We analyze the posting and replying activities of the participants by counting the number of email messages they posted to the lists and the number of replies they made to questions others posted. We found out that participants interact and share their knowledge a lot, their positing activity is fairly highly correlated with their replying activity, the characteristics of posting and replying activities are different for different kinds of lists, and the knowledge sharing activity of self-organizing Free/Open Source communities could best be explained in terms of what we called “Fractal Cubic Distribution” rather than the power-law distribution mostly reported in the literature. The paper also proposes what could be researched in knowledge sharing activities in F/OSS projects mailing list and for what purpose. The research findings add to our understanding of knowledge sharing activities in F/OSS projects. 
81|3||Examining the significance of high-level programming features in source code author classification|The use of Source Code Author Profiles (SCAP) represents a new, highly accurate approach to source code authorship identification that is, unlike previous methods, language independent. While accuracy is clearly a crucial requirement of any author identification method, in cases of litigation regarding authorship, plagiarism, and so on, there is also a need to know why it is claimed that a piece of code is written by a particular author. What is it about that piece of code that suggests a particular author? What features in the code make one author more likely than another? In this study, we describe a means of identifying the high-level features that contribute to source code authorship identification using as a tool the SCAP method. A variety of features are considered for Java and Common Lisp and the importance of each feature in determining authorship is measured through a sequence of experiments in which we remove one feature at a time. The results show that, for these programs, comments, layout features and package-related naming influence classification accuracy whereas user-defined naming, an obvious programmer related feature, does not appear to influence accuracy. A comparison is also made between the relative feature contributions in programs written in the two languages. 
81|4|http://www.sciencedirect.com/science/journal/01641212/81/4|Guest Editorâs Introduction: 10th Conference on Software Maintenance and Reengineering|
81|4||A wrapping approach for migrating legacy system interactive functionalities to Service Oriented Architectures|Software systems modernisation using Service Oriented Architectures (SOAs) and Web Services represents a valuable option for extending the lifetime of mission-critical legacy systems.This paper presents a black-box modernisation approach for exposing interactive functionalities of legacy systems as Services. The problem of transforming the original user interface of the system into the request/response interface of a SOA is solved by a wrapper that is able to interact with the system on behalf of the user. The wrapper behaviour is defined in the form of Finite State Machines retrievable by black-box reverse engineering of the human–computer interface.The paper describes our wrapper-based migration process and discusses the results of case studies showing process effectiveness and quality of resulting services. 
81|4||Dynamic object process graphs|A trace is a record of the execution of a computer program, showing the sequence of operations executed. A trace may be obtained through static or dynamic analysis. An object trace contains only those operations that relate to a particular object.Traces can be very large for longer system executions. Moreover, they lack structure because they do not show the control dependencies and completely unfold loops. Object process graphs are a finite concise description of dynamic object traces. They offer the advantage of representing control dependencies and loops explicitly.This article describes a new technique to extract object process graphs through dynamic analysis and discusses several applications, in particular program understanding and protocol recovery. A case study is described that illustrates and demonstrates use and feasibility of the technique. Finally, statically and dynamically derived object process graphs are compared. 
81|4||Search-based refactoring for software maintenance|The high cost of software maintenance could be reduced by automatically improving the design of object-oriented programs without altering their behaviour. We have constructed a software tool capable of refactoring object-oriented programs to conform more closely to a given design quality model, by formulating the task as a search problem in the space of alternative designs. This novel approach is validated by two case studies, where programs are automatically refactored to increase flexibility, reusability and understandability as defined by a contemporary quality model. Both local and simulated annealing searches were found to be effective in this task. 
81|4||Model-driven migration of supervisory machine control architectures|
81|4||Documenting after the fact: Recovering architectural design decisions|Software architecture documentation helps people in understanding the software architecture of a system. In practice, software architectures are often documented after the fact, i.e. they are maintained or created after most of the design decisions have been made and implemented. To keep the architecture documentation up-to-date an architect needs to recover and describe these decisions.This paper presents ADDRA, an approach an architect can use for recovering architectural design decisions after the fact. ADDRA uses architectural deltas to provide the architect with clues about these design decisions. This allows the architect to systematically recover and document relevant architectural design decisions. The recovered architectural design decisions improve the documentation of the architecture, which increases traceability, communication, and general understanding of a system. 
81|4||Software architecture reliability analysis using failure scenarios|With the increasing size and complexity of software in embedded systems, software has now become a primary threat for the reliability. Several mature conventional reliability engineering techniques exist in literature but traditionally these have primarily addressed failures in hardware components and usually assume the availability of a running system. Software architecture analysis methods aim to analyze the quality of software-intensive system early at the software architecture design level and before a system is implemented. We propose a Software Architecture Reliability Analysis Approach (SARAH) that benefits from mature reliability engineering techniques and scenario-based software architecture analysis to provide an early software reliability analysis at the architecture design level. SARAH defines the notion of failure scenario model that is based on the Failure Modes and Effects Analysis method (FMEA) in the reliability engineering domain. The failure scenario model is applied to represent so-called failure scenarios that are utilized to derive fault tree sets (FTS). Fault tree sets are utilized to provide a severity analysis for the overall software architecture and the individual architectural elements. Despite conventional reliability analysis techniques which prioritize failures based on criteria such as safety concerns, in SARAH failure scenarios are prioritized based on severity from the end-user perspective. SARAH results in a failure analysis report that can be utilized to identify architectural tactics for improving the reliability of the software architecture. The approach is illustrated using an industrial case for analyzing reliability of the software architecture of the next release of a Digital TV. 
81|4||Software reliability prediction by soft computing techniques|In this paper, ensemble models are developed to accurately forecast software reliability. Various statistical (multiple linear regression and multivariate adaptive regression splines) and intelligent techniques (backpropagation trained neural network, dynamic evolving neuro–fuzzy inference system and TreeNet) constitute the ensembles presented. Three linear ensembles and one non-linear ensemble are designed and tested. Based on the experiments performed on the software reliability data obtained from literature, it is observed that the non-linear ensemble outperformed all the other ensembles and also the constituent statistical and intelligent techniques. 
81|4||Industrial validation of COVAMOF|COVAMOF is a variability management framework for product families that was developed to reduce the number of iterations required during product derivation and to reduce the dependency on experts. In this paper, we present the results of an experiment with COVAMOF in industry. The results show that with COVAMOF, engineers that are not involved in the product family were now capable of deriving the products in 100% of the cases, compared to 29% of the cases without COVAMOF. For experts, the use of COVAMOF reduced the number of iterations by 42%, and the total derivation time by 38%. 
81|5|http://www.sciencedirect.com/science/journal/01641212/81/5|Special Issue on Software Process and Product Measurement|
81|5||Measuring where it matters: Determining starting points for metrics collection|Defining useful metrics to measure the goals of a software organisation is difficult. Defining useful metrics to measure the causes of the (failure) to fulfil those organisational goals is even more difficult, as the diversity of potential causes makes their measurement illusive. In this article, we describe a method to select useful software metrics based on findings from qualitative research. In a case study, we apply this method to a previously conducted study of project post-mortem reviews to assess the validity of our prior claims. For this we collected data on 109 new software projects in the organisation in which we conducted the previous case study. 
81|5||Comparing cost prediction models by resampling techniques|The accurate software cost prediction is a research topic that has attracted much of the interest of the software engineering community during the latest decades. A large part of the research efforts involves the development of statistical models based on historical data. Since there are a lot of models that can be fitted to certain data, a crucial issue is the selection of the most efficient prediction model. Most often this selection is based on comparisons of various accuracy measures that are functions of the model’s relative errors. However, the usual practice is to consider as the most accurate prediction model the one providing the best accuracy measure without testing if this superiority is in fact statistically significant. This policy can lead to unstable and erroneous conclusions since a small change in the data is able to turn over the best model selection. On the other hand, the accuracy measures used in practice are statistics with unknown probability distributions, making the testing of any hypothesis, by the traditional parametric methods, problematic. In this paper, the use of statistical simulation tools is proposed in order to test the significance of the difference between the accuracy of two prediction methods: regression and estimation by analogy. The statistical simulation procedures involve permutation tests and bootstrap techniques for the construction of confidence intervals for the difference of measures. Four known datasets are used for experimentation in order to validate the results and make comparisons between the simulation methods and the traditional parametric and non-parametric procedures. 
81|5||A framework for the design and verification of software measurement methods|At the core of any engineering discipline is the use of measures, based on ISO standards or on widely recognized conventions, for the development and analysis of the artifacts produced by engineers. In the software domain, many alternatives have been proposed to measure the same attributes, but there is no consensus on a framework for how to analyze or choose among these measures. Furthermore, there is often not even a consensus on the characteristics of the attributes to be measured.In this paper, a framework is proposed for a software measurement life cycle with a particular focus on the design phase of a software measure. The framework includes definitions of the verification criteria that can be used to understand the stages of software measurement design. This framework also integrates the different perspectives of existing measurement approaches. In addition to inputs from the software measurement literature the framework integrates the concepts and vocabulary of metrology. This metrological approach provides a clear definition of the concepts, as well as the activities and products, related to measurement. The aim is to give an integrated view, involving the practical side and the theoretical side, as well as the basic underlying concepts of measurement. 
81|5||Predicting defect-prone software modules using support vector machines|Effective prediction of defect-prone software modules can enable software developers to focus quality assurance activities and allocate effort and resources more efficiently. Support vector machines (SVM) have been successfully applied for solving both classification and regression problems in many applications. This paper evaluates the capability of SVM in predicting defect-prone software modules and compares its prediction performance against eight statistical and machine learning models in the context of four NASA datasets. The results indicate that the prediction performance of SVM is generally better than, or at least, is competitive against the compared models. 
81|5||On the conversion between IFPUG and COSMIC software functional size units: A theoretical and empirical study|Since 1984 the International Function Point Users Group (IFPUG) has produced and maintained a set of standards and technical documents about a functional size measurement methods, known as IFPUG, based on Albrecht function points. On the other hand, in 1998, the Common Software Measurement International Consortium (COSMIC) proposed an improved measurement method known as full function points (FFP). Both the IFPUG and the COSMIC methods both measure functional size of software, but produce different results. In this paper, we propose a model to convert functional size measures obtained with the IFPUG method to the corresponding COSMIC measures. We also present the validation of the model using 33 software projects measured with both methods. This approach may be beneficial to companies using both methods or migrating to COSMIC such that past data in IFPUG can be considered for future estimates using COSMIC and as a validation procedure. 
81|5||Cross-company vs. single-company web effort models using the Tukutuku database: An extended study|In 2004 [Kitchenham, B.A., Mendes, E., 2004a. Software productivity measurement using multiple size measures. IEEE Transactions on Software Engineering 30 (12), 1023–1035, Kitchenham, B.A., Mendes, E., 2004b. A comparison of cross-company and single-company effort estimation models for web applications. In: Proceedings Evaluation and Assessment in Software Engineering (EASE’ 04), pp. 47–55] (S1) investigated, using data on 63 Web projects, to what extent a cross-company cost model could be successfully employed to estimate development effort for single-company Web projects. Their effort models were built using Forward Stepwise Regression (SWR) and they found that cross-company predictions were significantly worse than single-company predictions. This study S1 was extended by Mendes and Kitchenham [Mendes, E., Kitchenham, B.A., 2004. Further comparison of cross-company and within company effort estimation models for web applications. In: Proceedings International Software Metrics Symposium (METRICS’04), Chicago, Illinois, September 11–17th, 2004. IEEE Computer Society, pp. 348–357] (S2), who used SWR and Case-based reasoning (CBR), and data on 67 Web projects from the Tukutuku database. They built two cross-company and one single-company models and found that both SWR cross-company models and CBR cross-company data provided predictions significantly worse than single-company predictions. Since 2004 another 83 projects were volunteered to the Tukutuku database, and recently used by Mendes et al. [Mendes, E., Di Martino, S., Ferrucci, F., Gravino, C., in press. Effort estimation: How valuable is it for a web company to use a cross-company data set, compared to using its own single-company data set? In: Proceedings of International World Wide Web Conference (WWW’07), Banff, Canada, 8–12 May] (S3), who partially replicated Mendes and Kitchenham’s study (S2), using SWR and CBR. They corroborated some of S2’s findings (SWR cross-company model and the CBR cross-company data provided predictions significantly worse than single-company predictions) however they replicated only part of S2. The objective of this paper (S4) is therefore to extend Mendes et al.’s work and fully replicate S2. We used the same dataset used in S3, and our results corroborated most of those obtained in S2. The main difference between S2 and our study was that one of our SWR cross-company models showed significantly similar predictions to the single-company model, which contradicts the findings from S2. 
81|5||A comprehensive empirical evaluation of missing value imputation in noisy software measurement data|The handling of missing values is a topic of growing interest in the software quality modeling domain. Data values may be absent from a dataset for numerous reasons, for example, the inability to measure certain attributes. As software engineering datasets are sometimes small in size, discarding observations (or program modules) with incomplete data is usually not desirable. Deleting data from a dataset can result in a significant loss of potentially valuable information. This is especially true when the missing data is located in an attribute that measures the quality of the program module, such as the number of faults observed in the program module during testing and after release. We present a comprehensive experimental analysis of five commonly used imputation techniques. This work also considers three different mechanisms governing the distribution of missing values in a dataset, and examines the impact of noise on the imputation process. To our knowledge, this is the first study to thoroughly evaluate the relationship between data quality and imputation. Further, our work is unique in that it employs a software engineering expert to oversee the evaluation of all of the procedures and to ensure that the results are not inadvertently influenced by poor quality data. Based on a comprehensive set of carefully controlled experiments, we conclude that Bayesian multiple imputation and regression imputation are the most effective techniques, while mean imputation performs extremely poorly. Although a preliminary evaluation has been conducted using Bayesian multiple imputation in the empirical software engineering domain, this is the first work to provide a thorough and detailed analysis of this technique. Our studies also demonstrate conclusively that the presence of noisy data has a dramatic impact on the effectiveness of imputation techniques. 
81|5||UM-RTCOM: An analyzable component model for real-time distributed systems|
81|5||Integrating a software architecture-centric method into object-oriented analysis and design|The choice of methodology for the development of the architecture for software systems has a direct effect on the suitability of that architecture. If the development process is driven by the user’s functional requirements, we would expect the architecture to appropriately reflect those requirements. We would also expect other aspects not captured in the functional specification to be absent from the architecture. The same phenomenon is true in development approaches that stress the importance of systemic quality attributes or other non-functional requirements; those requirements are prominent in the resulting architecture, while other requirement types not stressed by the approach are absent. In other words, the final architecture reflects the focus of the development approach. An ideal approach, therefore, is one that incorporates all goals, expectations, and requirements: both business and technical. To accomplish this we have incorporated, into a single architectural development process, generalized Object-Oriented Analysis and Design (OOAD) methodologies with the software architecture-centric method, the Quality Attribute Workshop (QAW) and Attribute Driven Design (ADD). OOAD, while relatively intuitive, focuses heavily on functional requirements and has the benefit of semantic closeness to the problem domain making it an intuitive process with comprehendible results. Architecture-centric approaches, on the other hand, provide explicit and methodical guidance to an architect in creating systems with desirable qualities and goals. They provide minimal guidance in determining fine-grained architecture, however. The integrated approach described in this paper maximizes the benefits of the respective processes while eliminating their flaws and was applied in a eight university, global development research project with great success. A case study from that experiment is included here to demonstrate the method. 
81|5||Teaching disciplined software development|Discipline is an essential prerequisite for the development of large and complex software-intensive systems. However, discipline is also important on the level of individual development activities. A major challenge for teaching disciplined software development is to enable students to experience the benefits of discipline and to overcome the gap between real professional scenarios and scenarios used in software engineering university courses. Students often do not have the chance to internalize what disciplined software development means at both the individual and collaborative level. Therefore, students often feel overwhelmed by the complexity of disciplined development and later on tend to avoid applying the underlying principles. The Personal Software Process (PSP) and the Team Software Process (TSP) are tools designed to help software engineers control, manage, and improve the way they work at both the individual and collaborative level. Both tools have been considered effective means for introducing discipline into the conscience of professional developers. In this paper, we address the meaning of disciplined software development, its benefits, and the challenges of teaching it. We present a quantitative study that demonstrates the benefits of disciplined software development on the individual level and provides further experience and recommendations with PSP and TSP as teaching tools. 
81|5||Extracting entity-relationship diagram from a table-based legacy database|Current database reverse engineering researches presume that the information regarding semantics of attributes, primary keys, and foreign keys in database tables is complete. However, this may not be the case. In a recent DBRE effort to derive a data model from a table-based database system, we find the data content of many attributes are not related to their names at all. In this paper, we present a process that extracts an extended entity-relationship diagram from a table-based database with little descriptions for the fields in its tables and no description for keys. The primary inputs of our approach are system display forms, table schema and data instance. We utilize screen displays to construct form instances. Secondly, code analysis and data analysis involving comparisons of fields and decomposition of fields are applied to extract attribute semantics from forms and table schemas, followed by the determination of primary keys, foreign keys and constraints of the database system. In the final step of conceptualization, with the processes of table mergence and relationship identification, an extended ER diagram is successfully extracted in a case study. 
81|5||Investigating software process in practice: A grounded theory perspective|
81|5||EASY: Efficient semAntic Service discoverY in pervasive computing environments with QoS and context support|Pervasive computing environments are populated with networked software and hardware resources providing various functionalities that are abstracted, thanks to the Service Oriented Architecture paradigm, as services. Within these environments, service discovery enabled by service discovery protocols (SDPs) is a critical functionality for establishing ad hoc associations between service providers and service requesters. Furthermore, the dynamics, the openness and the user-centric vision aimed at by the pervasive computing paradigm call for solutions that enable rich, semantic, context- and QoS-aware service discovery. Although the semantic Web paradigm envisions to achieve such support, current solutions are hardly deployable in the pervasive environment due to the costly underlying semantic reasoning with ontologies. In this article, we present EASY to support efficient, semantic, context- and QoS-aware service discovery on top of existing SDPs. EASY provides EASY-L, a language for semantic specification of functional and non-functional service properties, as well as EASY-M, a corresponding set of conformance relations. Furthermore, EASY provides solutions to efficiently assess conformance between service capabilities. These solutions are based on an efficient encoding technique, as well as on an efficient organization of service repositories (caches), which enables both fast service advertising and discovery. Experimental results show that the deployment of EASY on top of an existing SDP, namely Ariadne, enhancing it only with slight changes to EASY-Ariadne, enables rich semantic, context- and QoS-aware service discovery, which furthermore performs better than the classical, rigid, syntactic matching, and improves the scalability of Ariadne. 
81|5||A Web services-based framework for building componentized digital libraries|We present a new Web services-based framework for building componentized digital libraries (DLs). We particularly demonstrate how traditional RDBMS technology can be easily deployed to support several common digital library services. Configuration and customization of the framework to build specialized systems is supported by a wizard-like tool which is based on a generic metamodel for DLs. Such a tool implements a workflow process that segments the DL design tasks into well-defined steps and drives the designer along these steps. Both the framework and the configuration tool are evaluated in terms of several performance and usability criteria. Our experimental evaluation demonstrates the feasibility and superior performance of our framework, as well as the effectiveness of the wizard tool for setting up DLs. 
81|5||Mining software repositories for comprehensible software fault prediction models|Software managers are routinely confronted with software projects that contain errors or inconsistencies and exceed budget and time limits. By mining software repositories with comprehensible data mining techniques, predictive models can be induced that offer software managers the insights they need to tackle these quality and budgeting problems in an efficient way. This paper deals with the role that the Ant Colony Optimization (ACO)-based classification technique AntMiner+ can play as a comprehensible data mining technique to predict erroneous software modules. In an empirical comparison on three real-world public datasets, the rule-based models produced by AntMiner+ are shown to achieve a predictive accuracy that is competitive to that of the models induced by several other included classification techniques, such as C4.5, logistic regression and support vector machines. In addition, we will argue that the intuitiveness and comprehensibility of the AntMiner+ models can be considered superior to the latter models. 
81|6|http://www.sciencedirect.com/science/journal/01641212/81/6|Agile Product Line Engineering - List of reviewers|
81|6||Editorial|
81|6||Process fusion: An industrial case study on agile software product line engineering|This paper presents a case study of a software product company that has successfully integrated practices from software product line engineering and agile software development. We show how practices from the two fields support the company’s strategic and tactical ambitions, respectively. We also discuss how the company integrates strategic, tactical and operational processes to optimize collaboration and consequently improve its ability to meet market needs, opportunities and challenges. The findings from this study are relevant to software product companies seeking ways to balance agility and product management. The findings also contribute to research on industrializing software engineering. 
81|6||A product-line architecture for web service-based visual composition of web applications|
81|6||Agile product line planning: A collaborative approach and a case study|Agile methods and product line engineering (PLE) have both proven successful in increasing customer satisfaction and decreasing time to market under certain conditions. Key characteristics of agile methods are lean and highly iterative development with a strong emphasis on stakeholder involvement. PLE leverages reuse through systematic approaches such as variability modeling or product derivation. Integrating agile approaches with product line engineering is an interesting proposition which – not surprisingly – entails several challenges: Product lines (PL) rely on complex plans and models to ensure their long-term evolution while agile methods emphasize simplicity and short-term value-creation for customers. When incorporating agility in product line engineering, it is thus essential to define carefully how agile principles can support particular PLE processes. For instance, the processes of defining and setting up a product line (domain engineering) and deriving products (application engineering) differ significantly in practices and focus with implications on the suitability of agile principles. This paper presents practical experiences of adopting agile principles in product line planning (a domain engineering activity). ThinkLets, i.e., collaborative practices from the area of collaboration engineering, are the building blocks of the presented approach as they codify agile principles such as stakeholder involvement, rapid feedback, or value-based prioritization. We discuss how our approach balances agility and the intrinsic needs of product line planning. A case study carried out with an industrial partner indicates that the approach is practicable, usable, and useful. 
81|6||Automated error analysis for the agilization of feature modeling|Software Product Lines (SPL) and agile methods share the common goal of rapidly developing high-quality software. Although they follow different approaches to achieve it, some synergies can be found between them by (i) applying agile techniques to SPL activities so SPL development becomes more agile; and (ii) tailoring agile methodologies to support the development of SPL. Both options require an intensive use of feature models, which are usually strongly affected by changes on requirements. Changing large-scale feature models as a consequence of changes on requirements is a well-known error-prone activity. Since one of the objectives of agile methods is a rapid response to changes in requirements, it is essential an automated error analysis support in order to make SPL development more agile and to produce error-free feature models.As a contribution to find the intended synergies, this article sets the basis to provide an automated support to feature model error analysis by means of a framework which is organized in three levels: a feature model level, where the problem of error treatment is described; a diagnosis level, where an abstract solution that relies on Reiter’s theory of diagnosis is proposed; and an implementation level, where the abstract solution is implemented by using Constraint Satisfaction Problems (CSP).To show an application of our proposal, a real case study is presented where the Feature-Driven Development (FDD) methodology is adapted to develop an SPL. Current proposals on error analysis are also studied and a comparison among them and our proposal is provided. Lastly, the support of new kinds of errors and different implementation levels for the proposed framework are proposed as the focus of our future work. 
81|6||What do software practitioners really think about project success: A cross-cultural comparison|Due to the increasing globalization of software development we are interested to discover if there exist significant cultural differences in practitioners’ definition of a successful software project. This study presents the results of a survey in which Chilean software practitioners’ perceptions of project success are compared with previous research with US practitioners. Responses from both groups of practitioners indicate that there is a relationship between team-work and success; our results also indicate that there are similar perceptions related to the importance of job satisfaction and project success. However, Chilean responses suggest that if a practitioner is allowed too much freedom within the work environment, job stress results; this in turn is reflected in increasing demands for both job satisfaction and good environmental conditions. This may indicate the potential for the attribution of failure to conditions outside the team, thus preventing a search for problematic team issues and technical problems. In contrast, the data suggests peer control inside the US teams indicating a less stressful environment. 
81|6||The influence of checklists and roles on software practitioner risk perception and decision-making|
81|6||Setting checkpoints in legacy code to improve fault-tolerance|In this paper we describe the results of a study of the insertion of checkpoints within a legacy software system in the aerospace domain. The purpose of the checkpoints was to improve program fault-tolerance during program execution by rolling back system control to a saved state from which program execution can continue. The study used novice programmers for the determination of where the checkpoints were to be added. The focus was on the programmer’s understanding of the code, since this affected how the checkpoints were placed. The results should provide guidance to those interested in improving the fault-tolerance of legacy software systems, especially those written in older, nearly obsolescent programming languages. 
81|6||An XML-based methodology for parametric temporal database model implementation|Parametric data model is one of dimensional data models. It defines attributes as functions, modeling a real world object into a single tuple in a database. Such one-to-one correspondence between an object in the real world and a tuple provides various advantages in modeling dimensional data, avoiding self-joins which frequently appear in temporal data models which fragment an object into multiple tuples. Despite its modeling advantages, it is impractical to implement the parametric data model on top of conventional database systems because of the data model’s variable attribute sizes. However, such implementation challenge can be resolved by XML because XML is flexible for data boundaries. In this paper, we present an XML-based implementation methodology for the parametric temporal data model. In our implementation, we develop our own XML storage called CanStoreX (Canonical Storage for XML) and build the temporal database system on top of the CanStoreX. 
81|6||Predictive accuracy comparison of fuzzy models for software development effort of small programs|Regression analysis to generate predictive equations for software development effort estimation has recently been complemented by analyses using less common methods such as fuzzy logic models. On the other hand, unless engineers have the capabilities provided by personal training, they cannot properly support their teams or consistently and reliably produce quality products. In this paper, an investigation aimed to compare personal Fuzzy Logic Models (FLM) with a Linear Regression Model (LRM) is presented. The evaluation criteria were based mainly upon the magnitude of error relative to the estimate (MER) as well as to the mean of MER (MMER). One hundred five small programs were developed by thirty programmers. From these programs, three FLM were generated to estimate the effort in the development of twenty programs by seven programmers. Both the verification and validation of the models were made. Results show a slightly better predictive accuracy amongst FLM and LRM for estimating the development effort at personal level when small programs are developed. 
81|6||A survey study of critical success factors in agile software projects|While software is so important for all facets of the modern world, software development itself is not a perfect process. Agile software engineering methods have recently emerged as a new and different way of developing software as compared to the traditional methodologies. However, their success has mostly been anecdotal, and research in this subject is still scant in the academic circles. This research study was a survey study on the critical success factors of Agile software development projects using quantitative approach.Based on existing literature, a preliminary list of potential critical success factors of Agile projects were identified and compiled. Subsequently, reliability analysis and factor analysis were conducted to consolidate this preliminary list into a final set of 12 possible critical success factors for each of the four project success categories – Quality, Scope, Time, and Cost.A survey was conducted among Agile professionals, gathering survey data from 109 Agile projects from 25 countries across the world. Multiple regression techniques were used, both at the full regression model and at the optimized regression model via the stepwise screening procedure. The results revealed that only 10 out of 48 hypotheses were supported, identifying three critical success factors for Agile software development projects: (a) Delivery Strategy, (b) Agile Software Engineering Techniques, and (c) Team Capability.Limitations of the study are discussed together with interpretations for practitioners. To ensure success of their projects, managers are urged to focus on choosing a high-caliber team, practicing Agile engineering techniques and following Agile-style delivery strategy. 
81|6||A practitionerâs guide to light weight software process assessment and improvement planning|Software process improvement (SPI) is challenging, particularly for small and medium sized enterprises. Most existing SPI frameworks are either too expensive to deploy, or do not take an organizations’ specific needs into consideration. There is a need for light weight SPI frameworks that enable practitioners to base improvement efforts on the issues that are the most critical for the specific organization.This paper presents a step-by-step guide to process assessment and improvement planning using improvement framework utilizing light weight assessment and improvement planning (iFLAP), aimed at practitioners undertaking SPI initiatives. In addition to the guide itself the industrial application of iFLAP is shown through two industrial cases. iFLAP is a packaged improvement framework, containing both assessment and improvement planning capabilities, explicitly developed to be light weight in nature. Assessment is performed by eliciting improvements issues based on the organization’s experience and knowledge. The findings are validated through triangulation utilizing multiple data sources. iFLAP actively involves practitioners in prioritizing improvement issues and identifying dependencies between them in order to package improvements, and thus establish a, for the organization, realistic improvement plan. The two cases of iFLAP application in industry are presented together with lessons learned in order to exemplify actual use of the framework as well as challenges encountered. 
81|6||Software reuse: The Brazilian industry scenario|
81|6||A strategic analysis for successful open source software utilization based on a structural equation model|Commercial software companies face many challenges when competing in today’s fast moving and competitive industry environment. Recently, the use of open source software (OSS) has been proposed as a possible way to address those challenges. OSS provides many benefits, including high-quality software and substantial profits. Nevertheless, OSS has not been effectively utilized in real business. The purpose of this paper is to find what affects the utilization of OSS. For this study, we propose a structural equation model (SEM) to analyze the relationships between the quality factors based on ISO/IEC 9126 and OSS utilization. In addition, we suggest an open source software utilization index (OSSUI) based on the proposed SEM. The results provide us with the controllable feedback information to improve user (programmer) satisfaction during OSS utilization. 
81|6||Enhancing and measuring the predictive capabilities of testing-effort dependent software reliability models|
81|6||Active ERP implementation management: A Real Options perspective|Although enterprise resources planning (ERP) implementation has been one of the most significant challenges of the last decade, it comes with a surprisingly high failure rate due to its high risk nature. The risks of ERP implementation, which involve both technical and social uncertainties, must to be effectively managed. Traditional ERP practices address the implementation of ERP as a static process. Such practices focus on structure, not on ERP as something that will meet the needs of a changing organization. As a result, many relevant uncertainties that cannot be predefined are not accommodated, and cause the implementation fail in the form of project delay and cost overruns. The objective of this paper is to propose an active ERP implementation management perspective to manage ERP risks based on the Real Options (RO) theory, which addresses uncertainties over time, resolves uncertainties in changing environments that cannot be predefined. By actively managing ERP implementation, managers can improve their flexibility, take appropriate action to respond to the often-changing ERP environment, and achieve a more successful ERP implementation. 
81|6||An analysis of research topics in software engineering â 2006|This paper is the first in a new annual series whose goal is to answer the following question: what are the active research focuses within the field of software engineering? We considered 7 top journals and 7 top international conferences in software engineering and examined all the 691 papers published in these journals or presented at these conferences in 2006. Consequently, we have a number of findings.(1)Seventy-three percent of journal papers focus on 20% of subject indexes in software engineering, including Testing and Debugging (D.2.5), Management (D.2.9), and Software/Program Verification (D.2.4).(2)Eighty-nine percent of conference papers focus on 20% of subject indexes in software engineering, including Software/Program Verification (D.2.4), Testing and Debugging (D.2.5), and Design Tools and Techniques (D.2.2).(3)Seventy-seven percent of journal/conference papers focus on 20% of subject indexes in software engineering, including Testing and Debugging (D.2.5), Software/Program Verification (D.2.4), and Management (D.2.9).(4)The average number of references cited by a journal paper is about 33, whereas this number becomes around 24 for a conference paper. 
81|6||An assessment of systems and software engineering scholars and institutions (2001â2005)|
81|6||The Web Resource Space Model, H. Zhuge. Springer (2007)|
81|7|http://www.sciencedirect.com/science/journal/01641212/81/7|MUSEMBLE: A novel music retrieval system with automatic voice query transcription and reformulation|
81|7||Availability-based noncontiguous processor allocation policies for 2D mesh-connected multicomputers|Various contiguous and noncontiguous processor allocation policies have been proposed for mesh-connected multicomputers. Contiguous allocation suffers from high external processor fragmentation because it requires that the processors allocated to a parallel job be contiguous and have the same topology as the multicomputer. The goal of lifting the contiguity condition in noncontiguous allocation is reducing processor fragmentation. However, this can increase the communication overhead because the distances traversed by messages can be longer, and messages from different jobs can interfere with each other by competing for communication resources. The extra communication overhead depends on how the allocation request is partitioned and mapped to free processors. In this paper, we investigate a new class of noncontiguous allocation schemes for two-dimensional mesh-connected multicomputers. These schemes are different from previous ones in that request partitioning is based on the submeshes available for allocation. The available submeshes selected for allocation to a job are such that a high degree of contiguity among their processors is achieved. The proposed policies are compared to previous noncontiguous policies using detailed simulations, where several common communication patterns are considered. The results show that the proposed policies can reduce the communication overhead and improve performance substantially. 
81|7||Improving the schedulability of soft real-time open dynamic systems: The inheritor is actually a debtor|This paper presents the Clearing Fund Protocol, a three layered protocol designed to schedule soft real-time sets of precedence related tasks with shared resources. These sets are processed in an open dynamic environment. Open because new applications may enter the system at any time and dynamic because the schedulability is tested on-line as tasks request admission. Top-down, the three layers are the Clearing Fund, the Bandwidth Inheritance and two versions of the Constant Bandwidth Server algorithms. Bandwidth Inheritance applies a priority inheritance mechanism to the Constant Bandwidth Server. However, a serious drawback is its unfairness. In fact, a task executing in a server can potentially steal the bandwidth of another server without paying any penalty. The main idea of the Clearing Fund Algorithm is to keep track of processor-time debts contracted by lower priority tasks that block higher priority ones and are executed in the higher priority servers by having inherited the higher priority. The proposed algorithm reduces the undesirable effects of those priority inversions because the blocked task can finish its execution in its own server or in the server of the blocking task, whichever has the nearest deadline. If demanded, debts are paid back in that way. Inheritors are therefore debtors. Moreover, at certain instants in time, all existing debts may be waived and the servers are reset making a clear restart of the system. The Clearing Fund Protocol showed definite better performances when evaluated by simulations against Bandwidth Inheritance, the protocol it tries to improve. 
81|7||An efficient algorithm for mining temporal high utility itemsets from data streams|Utility of an itemset is considered as the value of this itemset, and utility mining aims at identifying the itemsets with high utilities. The temporal high utility itemsets are the itemsets whose support is larger than a pre-specified threshold in current time window of the data stream. Discovery of temporal high utility itemsets is an important process for mining interesting patterns like association rules from data streams. In this paper, we propose a novel method, namely THUI (Temporal High Utility Itemsets)-Mine, for mining temporal high utility itemsets from data streams efficiently and effectively. To the best of our knowledge, this is the first work on mining temporal high utility itemsets from data streams. The novel contribution of THUI-Mine is that it can effectively identify the temporal high utility itemsets by generating fewer candidate itemsets such that the execution time can be reduced substantially in mining all high utility itemsets in data streams. In this way, the process of discovering all temporal high utility itemsets under all time windows of data streams can be achieved effectively with less memory space and execution time. This meets the critical requirements on time and space efficiency for mining data streams. Through experimental evaluation, THUI-Mine is shown to significantly outperform other existing methods like Two-Phase algorithm under various experimental conditions. 
81|7||Adaptive watermark mechanism for rightful ownership protection|Watermarking is used to protect the integrity and copyright of images. Conventional copyright protection mechanisms; however, are not robust enough or require complex computations to embed the watermark into the host image. In this article, we propose an adaptive copyright protection scheme without the use of discrete cosine transformation (DCT) and discrete wavelet transformation (DWT). This novel approach allows image owners to adjust the strength of watermarks through a threshold, so that the robustness of the watermark can be enhanced. Moreover, our scheme can resist various signal processing operations (such as blurring, JPEG compression, and noising) and geometric transformations (such as cropping, rotation, and scaling). The experimental results show that our scheme outperforms related works in most cases. Specifically, our scheme preserves the data lossless requirement, so it is suitable for medical and artistic images. 
81|7||Cryptanalysis of the RCES/RSES image encryption scheme|Recently, a chaos-based image encryption scheme called RCES (also called RSES) was proposed. This paper analyses the security of RCES, and points out that it is insecure against the known/chosen-plaintext attacks: the number of required known/chosen plain-images is only one or two to succeed an attack. In addition, the security of RCES against the brute-force attack was overestimated. Both theoretical and experimental analyses are given to show the performance of the suggested known/chosen-plaintext attacks. The insecurity of RCES is due to its special design, which makes it a typical example of insecure image encryption schemes. A number of lessons are drawn from the reported cryptanalysis of RCES, consequently suggesting some common principles for ensuring a high level of security of an image encryption scheme. 
81|7||Resource management using multiple feedback loops in soft real-time distributed object systems|
81|7||Assessment of high-integrity embedded automotive control systems using hardware in the loop simulation|Sensor-based driver assistance systems often have a safety-related role in modern automotive designs. In this paper we argue that the current generation of “Hardware in the Loop” (HIL) simulators have limitations which restrict the extent to which testing of such systems can be carried out, with the consequence that it is more difficult to make informed decisions regarding the impact of new technologies and control methods on vehicle safety and performance prior to system deployment. In order to begin to address this problem, this paper presents a novel, low-cost and flexible HIL simulator. An overview of the simulator is provided, followed by detailed descriptions of the models that are employed. The effectiveness of the simulator is then illustrated using a case study, in which we examine the performance and safety integrity of eight different designs of a representative distributed embedded control system (a throttle- and brake-by-wire system with adaptive cruise control capability). It is concluded that the proposed HIL simulator provides a highly effective and low-cost test environment for assessing and comparing new automotive control system implementations. 
81|7||An efficient iconic indexing strategy for image rotation and reflection in image databases|Spatial relationships are important issues for similarity-based retrieval in many image database applications. With the popularity of digital cameras and the related image processing software, a sequence of images are often rotated or flipped. That is, those images are transformed in the rotation orientation or the reflection direction. However, many iconic indexing strategies based on symbolic projection are sensitive to rotation or reflection. Therefore, these strategies may miss the qualified images, when the query is issued in the orientation different from the orientation of the database images. To solve this problem, some researchers proposed a function to map the spatial relationship to its transformed one. However, this mapping consists of several conditional statements, which is time-consuming. Thus, in this paper, we propose an efficient iconic indexing strategy, in which we carefully assign a unique bit pattern to each spatial relationship and record the spatial information based on the bit patterns in a matrix. Without generating the rotated or flipped image, we can directly derive the index of the rotated or flipped image from the index of the original one by bit operations and matrix manipulation. In our performance study, we analyze the time complexity of our proposed strategy and show the efficiency of our proposed strategy according to the simulation results. Moreover, we implement a prototype to validate our proposed strategy. 
81|7||A language for high-level description of adaptive web systems|Adaptive Web systems (AWS) are Web-based systems that can adapt their features such as, presentation, content, and structure, based on users’ behaviour and preferences, device capabilities, and environment attributes. A framework was developed in our research group to provide the necessary components and protocols for the development of adaptive Web systems; however, there were several issues and shortcomings (e.g. low productivity, lack of verification mechanisms, etc.) in using the framework that inspired the development of a domain-specific language for the framework. This paper focuses on the proposal, design, and implementation of AWL, the Adaptive Web Language, which is used to develop adaptive Web systems within our framework. Not only does AWL address the existing issues in the framework, but it also offers mechanisms to increase software quality attributes, especially, reusability. An example application named PENS (a personalized e-News system) is explained and implemented in AWL. AWL has been designed based on the analysis of the adaptive Web domain, having taken into account the principles of reuse-based software engineering (product-lines), domain-specific languages, and aspect-oriented programming. Specially, a novel design decision, inspired by aspect-oriented programming paradigm, allows separate specification of presentation features in an application from its adaptation features. The AWL’s design decisions and their benefits are explained. 
81|7||Improved certificate-based encryption in the standard model|Certificate-based encryption has been recently proposed as a means to simplify the certificate management inherent to traditional public key encryption. In this paper, we present an efficient certificate-based encryption scheme which is fully secure in the standard model. Our construction is more efficient (in terms of computational cost and ciphertext size) than any of the previous constructions known without random oracles. 
81|7||Recursive protocol for group-oriented authentication with key distribution|The authors propose a recursive protocol for group-oriented authentication with key exchange, in which a group of n entities can authenticate with each other and share a group session key. The proposed protocol has the following characteristics: First, it requires O(n) rounds of messages, O(log n) completion time, O(log n) waiting time, and O(n log n) communication overhead in average for the completion of the recursion. Second, it not only meets the five principles suggested by Diffie et al. [Diffie, W., van Oorschot, P.C., Wiener, M.J., 1992. Authentication and authenticated key exchange. Designs, Codes, and Cryptography 2 (2), 107–125] on the design of a secure key exchange protocol, but also achieves the properties of nondisclosure, independency, and integrity addressed by Janson and Tsudik [Janson, P., Tsudik, G., 1995. Secure and minimal protocols for authenticated key distribution. Computer Communications 18 (9), 645–653] for the authentication of the group session key. Third, we describe the beliefs of trustworthy entities involved in our authentication protocol and the evolution of these beliefs as a consequence of communication by using BAN logic. Finally, it is practical and efficient, because only one-way hash function and exclusive-or (XOR) operations are used in implementation. 
81|7||A pairing SW implementation for Smart-Cards|
81|7||Why and how can human-related measures support software development processes?|
81|8|http://www.sciencedirect.com/science/journal/01641212/81/8|Systematic approaches to understanding and evaluating design trade-offs|The use of trade-off analysis as part of optimising designs has been an emerging technique for a number of years. However, only recently has much work been done with respect to systematically deriving the understanding of the system problem to be optimised and using this information as part of the design process. As systems have become larger and more complex then a need has arisen for suitable approaches. The system problem consists of design choices, measures for individual values related to quality attributes and weights to balance the relative importance of each individual quality attribute. In this paper, a method is presented for establishing an understanding of a system problem using the goal structuring notation (GSN). The motivation for this work is borne out of experience working on embedded systems in the context of critical systems where the cost of change can be large and the impact of design errors potentially catastrophic. A particular focus is deriving an understanding of the problem so that different solutions can be assessed quantitatively, which allows more definitive choices to be made. A secondary benefit is it also enables design using heuristic search approaches which is another area of our research. The overall approach is demonstrated through a case study which is a task allocation problem. 
81|8||A pattern language for designing e-business architecture|The pattern language for e-business provides a holistic support for developing software architectures for the e-business domain. The pattern language contains four related pattern categories: Business Patterns, Integration Patterns, Application Patterns, and Runtime Patterns. These pattern categories organise an e-business architecture into three layers—business interaction, application infrastructure and middleware infrastructure—and provide reusable design solutions to these layers in a top–down decomposition fashion. Business and Integration Patterns partition the business interaction layer into a set of subsystems; Application Patterns provide a high-level application infrastructure for these subsystems and separate business abstractions from their software solutions; Runtime Patterns then define a middleware infrastructure for the subsystems and shield design solutions from their implementations. The paper describes, demonstrates and evaluates this pattern language. 
81|8||A work product pool approach to methodology specification and enactment|Software development methodologies advocated and used today, whether traditional and plan-based or contemporary and agile, usually focus on process steps i.e. they start with requirements and iteratively describe what steps are necessary to move to the next stage or phase, until the software application is delivered to the end user. Such a process-oriented view of methodologies, based on the metaphor that human organizations are “machines” that “execute” processes, often results in methodologies that are too rigid and hard to follow, and most often than not end up being ignored or bypassed. Our proposal here is that, since the ultimate aim of software development is to provide a software product, software development methodologies should be described in terms of the intermediate products that are necessary to reach such a final product, plus the needed micro-processes that, as necessary evils, will be required to produce the appropriate work products from other, previously created ones. Using this product-oriented approach, software development methodologies can be specified that are, at least, as flexible as lightweight, agile approaches and, at the same time, as powerful and scalable as plan-oriented ones. 
81|8||Role engineering: From design to evolution of security schemes|This paper presents a methodology to design the RBAC (Role-Based Access Control) scheme during the design phase of an Information System. Two actors, the component developer and the security administrator, will cooperate to define and set up the minimal set of roles in agreement with the application constraints and the organization constraints that guarantee the global security policy of an enterprise. In order to maintain the global coherence of the existing access control scheme, an algorithm is proposed to detect the possible inconsistencies before the integration of a new component in the Information System. 
81|8||Quantitative risk-based security prediction for component-based systems with explicitly modeled attack profiles|Systems and software architects require quantitative dependability evaluations, which allow them to compare the effect of their design decisions on dependability properties. For security, however, quantitative evaluations have proven difficult, especially for component-based systems. In this paper, we present a risk-based approach that creates modular attack trees for each component in the system. These modular attack trees are specified as parametric constraints, which allow quantifying the probability of security breaches that occur due to internal component vulnerabilities as well as vulnerabilities in the component’s deployment environment. In the second case, attack probabilities are passed between system components as appropriate to model attacks that exploit vulnerabilities in multiple system components. The probability of a successful attack is determined with respect to a set of attack profiles that are chosen to represent potential attackers and corresponding environmental conditions. Based on these attack probabilities and the structure of the modular attack trees, risk measures can be estimated for the complete system and compared with the tolerable risk demanded by stakeholders. The practicability of this approach is demonstrated with an example that evaluates the confidentiality of a distributed document management system. 
81|8||Client-side selection of replicated web services: An empirical assessment|Replicating web services over physically distributed servers can offer client applications a number of QoS benefits, including higher availability and reduced response time. However, selecting the “best” service replica to invoke at the client-side is not a trivial task, as this requires taking into account factors such as local and external network conditions, and the servers’ current workload. This paper presents an empirical assessment of five representative client-side service selection policies for accessing replicated web services. The assessment measured the response time obtained with each of the five policies, at two different client configurations, when accessing a world-wide replicated service with four replicas located in three continents. The assessment’s results were analyzed both quantitatively and qualitatively. In essence, the results show that, in addition to the QoS levels provided by the external network and the remote servers, characteristics of the local client environment can have a significant impact on the performance of some of the policies investigated. In this regard, the paper presents a set of guidelines to help application developers in identifying a server selection policy that best suits a particular service replication scenario. 
81|8||XML-based agent communication, migration and computation in mobile agent systems|This article presents the research work that exploits using XML (Extensible Markup Language) to represent different types of information in mobile agent systems, including agent communication messages, mobile agent messages, and other system information. The goal of the research is to build a programmable information base in mobile agent systems through XML representations. The research not only studies using XML in binary agent system space such as representing agent communication messages and mobile agent messages, but also explores interpretive XML data processing to avoid the need of an interface layer between script mobile agents and system data represented in XML. These XML-based information representations have been implemented in Mobile-C, a FIPA (The Foundation for Intelligent Physical Agents) compliant mobile agent platform. Mobile-C uses FIPA ACL (Agent Communication Language) messages for both inter-agent communication and inter-platform migration. Using FIPA ACL messages for agent migration in FIPA compliant agent systems simplifies agent platform, reduces development effort, and easily achieves inter-platform migration through well-designed communication mechanisms provided in the system. The ability of interpretive XML data processing allows mobile agents in Mobile-C directly accessing XML data information without the need of an extra interface layer. 
81|8||Document recommendation for knowledge sharing in personal folder environments|Sharing sustainable and valuable knowledge among knowledge workers is a fundamental aspect of knowledge management. In organizations, knowledge workers usually have personal folders in which they organize and store needed codified knowledge (textual documents) in categories. In such personal folder environments, providing knowledge workers with needed knowledge from other workers’ folders is important because it increases the workers’ productivity and the possibility of reusing and sharing knowledge. Conventional recommendation methods can be used to recommend relevant documents to workers; however, those methods recommend knowledge items without considering whether the items are assigned to the appropriate category in the target user’s personal folders. In this paper, we propose novel document recommendation methods, including content-based filtering and categorization, collaborative filtering and categorization, and hybrid methods, which integrate text categorization techniques, to recommend documents to target worker’s personalized categories. Our experiment results show that the hybrid methods outperform the pure content-based and the collaborative filtering and categorization methods. The proposed methods not only proactively notify knowledge workers about relevant documents held by their peers, but also facilitate push-mode knowledge sharing. 
81|8||Cronus: A platform for parallel code generation based on computational geometry methods|This paper describes Cronus, a platform for parallelizing general nested loops. General nested loops contain complex loop bodies (assignments, conditionals, repetitions) and exhibit uniform loop-carried dependencies. The novelty of Cronus is twofold: (1) it determines the optimal scheduling hyperplane using the QuickHull algorithm, which is more efficient than previously used methods, and (2) it implements a simple and efficient dynamic rule (successive dynamic scheduling) for the runtime scheduling of the loop iterations along the optimal hyperplane. This scheduling policy enhances data locality and improves the makespan. Cronus provides an efficient runtime library, specifically designed for communication minimization, that performs better than more generic systems, such as Berkeley UPC. Its performance was evaluated through extensive testing. Three representative case studies are examined: the Floyd–Steinberg dithering algorithm, the Transitive Closure algorithm, and the FSBM motion estimation algorithm. The experimental results corroborate the efficiency of the parallel code. The tests show speedup ranging from 1.18 (out of the ideal 4) to 12.29 (out of the ideal 16) on distributed-systems and 3.60 (out of 4) to 15.79 (out of 16) on shared-memory systems. Cronus outperforms UPC by 5–95% depending on the test case. 
81|8||An experimental study of adaptive testing for software reliability assessment|Adaptive testing is a new form of software testing that is based on the feedback and adaptive control principle and can be treated as the software testing counterpart of adaptive control. Our previous work has shown that adaptive testing can be formulated and guided in theory to minimize the variance of an unbiased software reliability estimator and to achieve optimal software reliability assessment. In this paper, we present an experimental study of adaptive testing for software reliability assessment, where the adaptive testing strategy, the random testing strategy and the operational profile based testing strategy were applied to the Space program in four experiments. The experimental results demonstrate that the adaptive testing strategy can really work in practice and may noticeably outperform the other two. Therefore, the adaptive testing strategy can serve as a preferable alternative to the random testing strategy and the operational profile based testing strategy if high confidence in the reliability estimates is required or the real-world operational profile of the software under test cannot be accurately identified. 
81|8||A quantitative approach for evaluating the quality of design patterns|In recent years, the influence of design patterns on software quality has attracted an increasing attention in the area of software engineering, as design patterns encapsulate valuable knowledge to resolve design problems, and more importantly to improve design quality. As the paradigm continues to increase in popularity, a systematic and objective approach to verify the design of a pattern is increasingly important. The intent session in a design pattern indicates the problem the design pattern wants to resolve, and the solution session describes the structural model for the problem. When the problem in the intent is a quality problem, the structure model should provide a solution to improve the relevant quality. In this work we provide an approach, based on object-oriented quality model, to validate if a design pattern is well-designed, i.e., it answers the question of the proposed structural model really resolves the quality problems described in the intent. We propose a validation approach to help pattern developers check if a design pattern is well-designed. In addition, a quantitative method is proposed to measure the effectiveness of the quality improvement of a design pattern that pattern users can determine which design patterns are applicable to meet their functional and quality requirements. 
81|9|http://www.sciencedirect.com/science/journal/01641212/81/9|Editorial|
81|9||Analysis of architecture evaluation data|The output of 18 software architecture evaluations is analyzed. The goal of the analysis is to find patterns in the important quality attributes and risk themes identified in the evaluations. The major results are•A categorization of risk themes.•The observation that twice as many risk themes are risks of “omission” as are risks of “commission”.•A failure to find a relationship between the business and mission goals of a system and the risk themes from an evaluation of that system.•A failure to find a correlation between the domain of a system being evaluated and the important quality attributes for that system.•A wide diversity of names used for various quality attributes.The results of this investigation have application to practitioners by suggesting activities on which developers should put greater focus. They also have application to researchers by suggesting further areas of investigation. 
81|9||Architectural knowledge discovery with latent semantic analysis: Constructing a reading guide for software product audits|Architectural knowledge is reflected in various artifacts of a software product. In a software product audit this architectural knowledge needs to be uncovered and its effects assessed in order to evaluate the quality of the software product. A particular problem is to find and comprehend the architectural knowledge that resides in the software product documentation. In this article, we discuss how the use of a technique called Latent Semantic Analysis can guide auditors through the documentation to the architectural knowledge they need. We validate the use of Latent Semantic Analysis for discovering architectural knowledge by comparing the resulting vector-space model with the mental model of documentation that auditors possess. 
81|9||Software architecting without requirements knowledge and experience: What are the repercussions?|Whereas the relationship between Requirements Engineering and Software Architecture (SA) has been studied increasingly in recent years in terms of methods, notations, representations, tools, development paradigms and project experiences, that in terms of the human agents conducting these processes has not been explored scientifically. This paper describes the impact of requirements knowledge and experience (RKE) on software architecting tasks. Specifically, it describes an exploratory, empirical study involving 15 architecting teams, approximately evenly split between those teams with RKE and those without. Each team developed its own system architecture from the same given set of requirements in the banking domain. The subjects were all final year undergraduate or graduate students enrolled in a university-level course on software architectures. The overall results of this study suggest that architects with RKE develop higher-quality software architectures than those without, and that they have fewer architecture-development problems than did the architects without RKE. This paper identifies specific areas of both architecture design as well as the architecture-development process where the differences manifest between the RKE and non-RKE architects. The paper also describes the possible implications of the findings on the areas of hiring and training, pedagogy, and technology. The empirical study was carried out using the “mixed methods” approach, involving both quantitative and qualitative aspects of the investigation. A bi-product of this study is an architectural assessment instrument (included in the Appendix) for quantitative analysis of the quality of a software architecture. This paper also describes some new threads for future work. 
81|9||Conflict detection and resolution for workflows constrained by resources and non-determined durations|The correctness of a workflow specification is critical for the automation of business processes. Therefore, errors in the specification should be detected and corrected at build-time. In this paper, we present a conflict verification and resolution approach for a kind of workflow constrained by resources and non-determined duration based on Petri net. In this kind of workflow, there are two timing functions for each activity to present the minimum and maximum duration of each activity, and the implementations of some activities require resources. Based on the Petri net model obtained, the earliest time to start each activity can be calculated and the key activities influencing the implementation of the workflow can be determined, with which the resource consistency between activities can be verified. Key-activity and waiting-short priority strategies are adopted to remove the resource conflicts between activities, which can ensure that most of the subsequent activities start as early as possible and that the whole workflow be finished in a shorter time. Through experiments, it is proved that the proposed removal strategy for resource conflicts is better than other strategies. 
81|9||An embedding technique based upon block prediction|This paper presents a novel data hiding scheme for VQ compression images. This scheme first uses SMVQ prediction to classify encoding blocks into different types, then uses different codebooks and encoding strategies to perform encoding and data hiding simultaneously. In using SMVQ prediction, no extra data is required to identify the combination of encoding strategies and codebook, which helps improve compression performance. Furthermore, the proposed scheme adaptively combines VQ and SMVQ encoding characteristics to provide higher image quality of stego-images while size of the hidden payload remains the same. Experimental results show that the proposed scheme indeed outperforms other previously proposed schemes in image quality of the stego-images and compression performance. 
81|9||Designing and evaluating interleaving decompressing and virus scanning in a stream-based mail proxy|A storage-based anti-virus access gateway is not scalable because it stores the entire mail under processing. This work designs and evaluates a stream-based mail proxy constructed from several open-source packages. This proxy processes mail in segments, and interleaves MIME parsing, decoding, decompressing and virus scanning. It is seven times faster than the storage-based one on forwarding, three times faster on virus scanning, and twice as faster on decompressing plus virus scanning. This proxy can keep nearly constant memory usage and work without disks, while the storage-based one requires memory and disk space proportional to the number of clients and the mail size. 
81|9||A new region filtering and region weighting approach to relevance feedback in content-based image retrieval|A new region filtering and region weighting method, which filters out unnecessary regions from images and learns region importance from the region size and the spatial location of regions in an image, is proposed based on region representations. It weights the regions optimally and improves the performance of the region-based retrieval system based on relevance feedback. Due to the semantic gap between the low level feature representation and the high level concept in a query image, semantically relevant images may exhibit very different visual characteristics, and may be scattered in several clusters in the feature space. Our main goal is finding semantically related clusters and their weights to reduce this semantic gap. Experimental results demonstrate the efficiency and effectiveness of the proposed region filtering and weighting method in comparison with the area percentage method and region frequency weighted by inverse image frequency method, respectively. 
81|9||Web proxy cache replacement scheme based on back-propagation neural network|Web proxy caches are used to reduce the strain of contemporary web traffic on web servers and network bandwidth providers. In this research, a novel approach to web proxy cache replacement which utilizes neural networks for replacement decisions is developed and analyzed. Neural networks are trained to classify cacheable objects from real world data sets using information known to be important in web proxy caching, such as frequency and recency. Correct classification ratios between 0.85 and 0.88 are obtained both for data used for training and data not used for training. Our approach is compared with Least Recently Used (LRU), Least Frequently Used (LFU) and the optimal case which always rates an object with the number of future requests. Performance is evaluated in simulation for various neural network structures and cache conditions. The final neural networks achieve hit rates that are 86.60% of the optimal in the worst case and 100% of the optimal in the best case. Byte-hit rates are 93.36% of the optimal in the worst case and 99.92% of the optimal in the best case. We examine the input-to-output mappings of individual neural networks and analyze the resulting caching strategy with respect to specific cache conditions. 
81|9||The MPEG-7 Multimedia Database System (MPEG-7 MMDB)|Broadly used Database Management Systems (DBMS) propose multimedia extensions, like Oracle’s Multimedia (formerly interMedia). However, these extensions lack means for managing the requirements of multimedia data in terms of semantic meaningful querying, advanced indexing, content modeling and multimedia programming libraries.In this context, this paper presents the MPEG-7 Multimedia DataBase System (MPEG-7 MMDB). The innovative parts of our system are our metadata model for multimedia content relying on the XML-based MPEG-7 standard, a new indexing and querying system for MPEG-7, the query optimizer and the supporting internal and external application libraries.The resulting system, extending Oracle 10g, is verified and demonstrated by the use of two real multimedia applications in the field of audio recognition and image retrieval. 
81|9||A round- and computation-efficient three-party authenticated key exchange protocol|In three-party authenticated key exchange protocols, each client shares a secret only with a trusted server with assists in generating a session key used for securely sending messages between two communication clients. Compared with two-party authenticated key exchange protocols where each pair of parties must share a secret with each other, a three-party protocol does not cause any key management problem for the parties. In the literature, mainly there exist three issues in three-party authenticated key exchange protocols are discussed that need to be further improved: (1) to reduce latency, communication steps in the protocol should be as parallel as possible; (2) as the existence of a security-sensitive table on the server side may cause the server to become compromised, the table should be removed; (3) resources required for computation should be as few as possible to avoid the protocol to become an efficiency bottleneck. In various applications over networks, a quick response is required especially by light-weight clients in the mobile e-commerce. In this paper, a round- and computation-efficient three-party authenticated key exchange protocol is proposed which fulfils all of the above mentioned requirements. 
81|9||Virtualization-based autonomic resource management for multi-tier Web applications in shared data center|As large data centers emerge, which host multiple Web applications, it is critical to isolate different application environments for security reasons and to provision shared resources effectively and efficiently to meet different service quality targets at minimum operational cost. To address this problem, we developed a novel architecture of resource management framework for multi-tier applications based on virtualization mechanisms. Key techniques presented in this paper include (1) establishment of the analytic performance model which employs probabilistic analysis and overload management to deal with non-equilibrium states; (2) a general formulation of the resource management problem which can be solved by incorporating both deterministic and stochastic optimizing algorithms; (3) deployment of virtual servers to partition resource at a much finer level; and (4) investigation of the impact of the failure rate to examine the effect of application isolation. Simulation experiments comparing three resource allocation schemes demonstrate the advantage of our dynamic approach in providing differentiated service qualities, preserving QoS levels in failure scenarios and also improving the overall performance while reducing the resource usage cost. 
81|9||The consistency among facilitating factors and ERP implementation success: A holistic view of fit|Traditionally, various ERP implementation factors have been deemed critical to success within diverse business environments. The interaction relationships among these ERP implementation success factors, however, have been overlooked. The objective of this study is to explore the interaction patterns among the ERP implementation success factors from a covariation (co-alignment) perspective. We conceptualize the “consistency” among the factors that facilitate ERP implementation and evaluate them in terms of their positive impact on successful ERP implementation. The results from a field survey of 90 Taiwanese manufacturing firms show that the “consistency” among these facilitating factors of ERP implementation had a significant positive impact on ERP implementation success. The factors examined in this study include vendor support, consultant competence, ERP project team member competence, ERP project manager leadership, top management support, and user support. Implications for managers and researchers conclude this study. 
82|1|http://www.sciencedirect.com/science/journal/01641212/82/1|Guest editorial|
82|1||The Palladio component model for model-driven performance prediction|One aim of component-based software engineering (CBSE) is to enable the prediction of extra-functional properties, such as performance and reliability, utilising a well-defined composition theory. Nowadays, such theories and their accompanying prediction methods are still in a maturation stage. Several factors influencing extra-functional properties need additional research to be understood. A special problem in CBSE stems from its specific development process: Software components should be specified and implemented independently from their later context to enable reuse. Thus, extra-functional properties of components need to be specified in a parametric way to take different influencing factors like the hardware platform or the usage profile into account. Our approach uses the Palladio component model (PCM) to specify component-based software architectures in a parametric way. This model offers direct support of the CBSE development process by dividing the model creation among the developer roles. This paper presents our model and a simulation tool based on it, which is capable of making performance predictions. Within a case study, we show that the resulting prediction accuracy is sufficient to support the evaluation of architectural design decisions. 
82|1||Achieving efficiency, quality of service and robustness in multi-organizational Grids|Scalability, flexibility, quality of service provisioning, efficiency and robustness are the desired characteristics of most computing systems. Although the emerging Grid computing paradigm is scalable and flexible, achieving both efficiency and quality of service provisioning in Grids is a challenging task but is necessary for the wide adoption of Grids. Grid middleware should also be robust to uncertainties such as those in user-estimated runtimes of Grid applications. In this paper, we present a complete middleware framework for Grids that achieves user satisfaction by providing QoS guarantees for Grid applications, cost effectiveness by efficiently utilizing resources and robustness by intelligently handling uncertain runtimes of applications. 
82|1||An overhead and resource contention aware analytical model for overloaded Web servers|A Web server, when overloaded, shows a severe degradation of goodput initially, with the eventual settling of goodput as load increases further. Traditional performance models have failed to capture this behavior. In this paper, we propose an analytical model, which is a two-stage and layered queuing model of the Web server, which is able to reproduce this behavior. We do this by explicitly modelling the overhead processing, the user abandonment and retry behavior, and the contention for resources, for the FIFO and LIFO queuing disciplines. We show that LIFO provides better goodput in most overload situations. We compare our model predictions with experimental results from a test bed and find that our results match well with measurements. 
82|1||Performance analysis of security aspects by weaving scenarios extracted from UML models|Aspect-oriented modeling (AOM) allows software designers to describe features that address pervasive concerns separately as aspects, and to systematically incorporate the features into a design model using model composition techniques. The goal of this paper is to analyze the performance effects of different security features that may be represented as aspect models. This is part of a larger research effort to integrate methodologies and tools for the analysis of security and performance properties early in the software development process. In this paper, we describe an extension to the AOM approach that provides support for performance analysis. We use the performance analysis techniques developed previously in the PUMA project, which take as input UML models annotated with the standard UML Profile for Schedulability, Performance and Time (SPT), and transform them first into Core Scenario Model (CSM), and then into different performance models. The composition of the aspects with the primary (base) model is performed at the CSM level. A new formal definition of CSM properties and operations is described as a foundation for scenario-based weaving. The proposed approach is illustrated with an example that utilizes two standards, TPC-W and SSL. 
82|1||A practical approach for performance-driven UML modelling of handheld devices â A case study|In this article, we present a performance engineering enhanced modelling methodology for designing embedded devices and describe the experiences we have gained in applying this methodology during the design of a DVB-H enabled handheld device. The methodology uses UML 2.0 to model the system following a strict separation of architectural and behavioural aspects of the system. For this purpose we employ the new composite structure diagram and show its advantages over already established approaches. This methodology specially aims on an easy application by non performance experts. From the model, a multiclass queueing network is generated for the analysis of the system performance. The configuration of hardware resources and resource demands is done using the standard SPT Profile which is extended where necessary. This makes queueing theory accessible to system designers even if they are not familiar with the underlying mathematics. In this way the acceptance of developers to use performance engineering in their daily work is increased. Special attention has been put on an easy evaluation of design alternatives. We describe our implementation and its seamless integration into a UML 2.0 CASE tool. 
82|1||Developing reusable simulation core code for networking: The grid resource discovery example|In this work, first, we present a grid resource discovery protocol that discovers computing resources without the need for resource brokers to track existing resource providers. The protocol uses a scoring mechanism to aggregate and rank resource provider assets and Internet router data tables (called grid routing tables) for storage and retrieval of the assets. Then, we discuss the simulation framework used to model the protocol and the results of the experimentation. The simulator utilizes a simulation engine core that can be reused for other network protocol simulators considering time management, event distribution, and a simulated network infrastructure. The techniques for constructing the simulation core code using C++/CLR are also presented in this paper. 
82|1||A security policy language for wireless sensor networks|Authenticated computer system users are only authorized to access certain data within the system. In the future, wireless sensor networks (WSNs) will need to restrict access to data as well. To date, WSN security has largely been based on encryption and authentication schemes. The WSN Authorization Specification Language (WASL) is a mechanism-independent composable WSN policy language that can specify arbitrary and composable security policies that are able to span and integrate multiple WSN policies. Using WASL, a multi-level security policy for a 1000 node network requires only 60 bytes of memory per node. 
82|1||A model of domain-polymorph component for heterogeneous system design|Heterogeneous systems mix different technical domains such as signal processing, analog and digital electronics, software, telecommunication protocols, etc. Heterogeneous systems are composed of subsystems that are designed using different models of computation (MoC). These MoCs are the laws that govern the interactions of the components of a subsystem. The design of heterogeneous systems includes the design of each part of the system according to its specific MoC, and the connection of the parts in order to build the model representing the system. Indeed, this model allows the MoCs that govern different parts of system to coexist and interact.To be able to use a component which is specified according to a given MoC, under other, different MoCs, we can use either a hierarchical or a non-hierarchical approach, or we can build domain-specific components (DSC). However, these solutions present several disadvantages. This paper presents a new model of component, called domain-polymorph component (DPC). Such a component is atomic and is able to execute its core behavior, specified under a given MoC, under different host MoCs. This approach is not a competitor to the approaches above but is complementary. 
82|1||Model-based performance analysis using block coverage measurements|The primary advantage of model-based performance analysis is its ability to facilitate sensitivity and predictive analysis, in addition to providing an estimate of the application performance. To conduct model-based analysis, it is necessary to build a performance model of an application which represents the application structure in terms of the interactions among its components, using an appropriate modeling paradigm. While several research efforts have been devoted to the development of the theoretical aspects of model-based analysis, its practical applicability has been limited despite the advantages it offers. This limited practical applicability is due to the lack of techniques available to estimate the parameters of the performance model of the application. Since the model parameters cannot be estimated in a realistic manner, the results obtained from model-based analysis may not be accurate.In this paper, we present an empirical approach in which profile data in the form of block coverage measurements is used to parameterize the performance model of an application. We illustrate the approach using a network routing simulator called Maryland routing simulator (MaRS). Validation of the performance estimate of MaRS obtained from the performance model parameterized using our approach demonstrates the viability of our approach. We then illustrate how the model could be used for predictive performance analysis using two scenarios. By the virtue of using code coverage measurements to parameterize a performance model, we integrate two mature, yet independent research areas, namely, software testing and model-based performance analysis. 
82|1||CoRAL: A transparent fault-tolerant web service|The Web is increasingly used for critical applications and services. We present a client-transparent mechanism, called CoRAL, that provides high reliability and availability for Web service. CoRAL provides fault tolerance even for requests being processed at the time of server failure. The scheme does not require deterministic servers and can thus handle dynamic content. CoRAL actively replicates the TCP connection state while maintaining logs of HTTP requests and replies. In the event of a primary server failure, active client connections fail over to a spare, where their processing continues seamlessly. We describe key aspects of the design and implementation as well as several performance optimizations. Measurements of system overhead, failover performance, and preliminary validation using fault injection are presented. 
82|1||An anomaly prevention approach for real-time task scheduling|This research responds to practical requirements in the porting of embedded software over platforms and the well-known multiprocessor anomaly. In particular, we consider the task scheduling problem when the system configuration changes. With mutual-exclusive resource accessing, we show that new violations of the timing constraints of tasks might occur even when a more powerful processor or device is adopted. The concept of scheduler stability and rules are then proposed to prevent scheduling anomaly from occurring in task executions that might be involved with task synchronization or I/O access. Finally, we explore policies for bounding the duration of scheduling anomalies. 
82|1||Mining temporal interval relational rules from temporal data|Temporal data mining is still one of important research topic since there are application areas that need knowledge from temporal data such as sequential patterns, similar time sequences, cyclic and temporal association rules, and so on. Although there are many studies for temporal data mining, they do not deal with discovering knowledge from temporal interval data such as patient histories, purchaser histories, and web logs etc. We propose a new temporal data mining technique that can extract temporal interval relation rules from temporal interval data by using Allen’s theory: a preprocessing algorithm designed for the generalization of temporal interval data and a temporal relation algorithm for mining temporal relation rules from the generalized temporal interval data. This technique can provide more useful knowledge in comparison with conventional data mining techniques. 
82|1||A scoped approach to traceability management|
82|10|http://www.sciencedirect.com/science/journal/01641212/82/10|Introduction|
82|10||Understanding the effects of requirements volatility in software engineering by using analytical modeling and software process simulation|This paper introduces an executable system dynamics simulation model developed to help project managers comprehend the complex impacts related to requirements volatility on a software development project. The simulator extends previous research and adds research results from an empirical survey, including over 50 new parameters derived from the associated survey data, to a base model. The paper discusses detailed results from two cases that show significant cost, schedule, and quality impacts as a result of requirements volatility. The simulator can be used as an effective tool to demonstrate the complex set of factor relationships and effects related to requirements volatility. 
82|10||Experience on knowledge-based software engineering: A logic-based requirements language and its industrial applications|A formal requirements specification language plays an important role in software development. Not only can such language be used for stating requirements specification, but also can be used in many phases of software development life cycle. The FRORL project started from constructing a language with a solid logical foundation and further expanded to research in verification, validation, requirements analysis, debugging, and transformation. Research in this project aided in some industrial applications in which a code generation tool produced software for embedded systems. This article reports the experiences gained from this project and states the value of research in knowledge-based software engineering. 
82|10||Dynamically reconfigurable hardwareâsoftware architecture for partitioning networking functions on the SoC platform|We present an issue of the dynamically reconfigurable hardware–software architecture which allows for partitioning networking functions on a SoC (System on Chip) platform. We address this issue as a partition problem of implementing network protocol functions into dynamically reconfigurable hardware and software modules. Such a partitioning technique can improve the co-design productivity of hardware and software modules. Practically, the proposed partitioning technique, which is called the ITC (Inter-Task Communication) technique incorporating the RT-IJC2 (Real-Time Inter-Job Communication Channel), makes it possible to resolve the issue of partitioning networking functions into hardware and software modules on the SoC platform. Additionally, the proposed partitioning technique can support the modularity and reuse of complex network protocol functions, enabling a higher level of abstraction of future network protocol specifications onto the SoC platform. Especially, the RT-IJC2 allows for more complex data transfers between hardware and software tasks as well as provides real-time data processing simultaneously for given application-specific real-time requirements. We conduct a variety of experiments to illustrate the application and efficiency of the proposed technique after implementing it on a commercial SoC platform based on the Altera’s Excalibur including the ARM922T core and up to 1 million gates of programmable logic. 
82|10||Detecting artifact anomalies in business process specifications with a formal model|Many business process analysis models have been proposed, however there are few discussions for artifact usages in workflow specifications. A well-structured business process with sufficient resources might fail or yield unexpected results dynamically due to inaccurate artifact specification, e.g. an inconsistency between artifact and control flow, or contradictions between artifact operations. This paper, based on our previous work, presents a model for describing the input/output of a workflow process and analyzes the artifact usages upon the model. This work identifies and formulates thirteen cases of artifact usage anomalies affecting process execution and categorizes the cases into three types. Moreover, the methods for detecting these anomalies with time complexities O(n2), less than O(n3) in previous methods, are presented. Besides, the paper uses an example to demonstrate the processing of them. 
82|10||Challenge and solutions of NAT traversal for ubiquitous and pervasive applications on the Internet|Network Address Translator (NAT) has brought up many changes and opportunities to the Internet. How do the ubiquitous and pervasive applications coexist with NAT and interoperate with each other? In this article, we discuss the essence of NAT sensitive applications as well as the challenge and response for various NAT traversal solutions. All questions pointed to redesign a new NAT framework with a major change to accommodate NAT problems all at once. We introduce a novel next generation NAT (NATng) framework, which consists of a Bi-directional NAT (BNAT) and a Domain Name System Application Level Gateway (DNS_ALG) with a Border Network Address Translator Control Protocol (BNATCP) function to control all BNATs. The above components coordinate and provide bidirectional access capability between intranet and Internet, so all private hosts can be addressed via Fully Qualified Domain Name (FQDN). Logistically, NATng extends the IPv4 address space from 232to248 or even more. It features high potential to solve the problems for ubiquitous and pervasive applications which may encounter IPv4 address exhaustion on the current Internet. 
82|10||Modeling and verification of real-time embedded systems with urgency|Real-time embedded systems are often designed with different types of urgencies such as delayable or eager, that are modeled by several urgency variants of the timed automata model. However, most model checkers do not support such urgency semantics, except for the IF toolset that model checks timed automata with urgency against observers. This work proposes an Urgent Timed Automata (UTA) model with zone-based urgency semantics that gives the same model checking results as absolute urgency semantics of other existing urgency variants of the timed automata model, including timed automata with deadlines and timed automata with urgent transitions. A necessary and sufficient condition, called complete urgency, is formulated and proved for avoiding zone partitioning so that the system state graphs are simpler and model checking is faster. A novel zone capping method is proposed that is time-reactive, preserves complete urgency, satisfies all deadlines, and does not need zone partitioning. The proposed verification methods were implemented in the SGM CTL model checker and applied to real-time and embedded systems. Several experiments, comparing the state space sizes produced by SGM with that by the IF toolset, show that SGM produces much smaller state-spaces. 
82|10||Tactics based approach for integrating non-functional requirements in object-oriented analysis and design|Non-Functional Requirements (NFRs) are rarely treated as “first-class” elements in software development as Functional Requirements (FRs) are. Often NFRs are stated informally and incorporated in the final software as an after-thought. We leverage existing research work for the treatment of NFRs to propose an approach that enables to systematically analyze and design NFRs in parallel with FRs. Our approach premises on the importance of focusing on tactics (the specific mechanisms used to fulfill NFRs) as opposed to focusing on NFRs themselves. The advantages of our approach include filling the gap between NFRs elicitation and NFRs implementation, systematically treating NFRs through grouping of tactics so that tactics in the same group can be addressed uniformly, remedying some shortcomings in existing work (by prioritizing NFRs and analyzing tradeoff among NFRs), and integration of FRs and NFRs by treating them as first-class entities. 
82|10||Design and implementation of S-MARKS: A secure middleware for pervasive computing applications|
82|10||Case study on distributed and fault tolerant system modeling based on timed automata|This article presents the modeling of a distributed fault-tolerant real-time application by timed automata. The application under consideration consists of several processors communicating via a Controller Area Network (CAN); each processor executes an application that consists of fault-tolerant tasks running on top of an operating system (e.g. OSEK/VDX compliant) and using inter-task synchronization primitives. For such a system, a model checking tool (e.g. UPPAAL) can be used to verify the complex time and logical properties formalized as safety or bounded liveness properties (e.g. end-to-end response time considering an occurrence of a fault). The proposed model reduces the size of the state-space by sharing clocks measuring the execution time of the tasks. 
82|10||Developing platform specific model for MPSoC architecture from UML-based embedded software models|In this paper, we describe a technique to design UML-based software models for MPSoC architecture, which focuses on the development of the platform specific model of embedded software. To develop the platform specific model, we define a process for the design of UML-based software model and suggest an algorithm with precise actions to map the model to MPSoC architecture. In order to support our design process, we implemented our approach in an integrated tool. Using the tool, we applied our design technique to a target system. We believe that our technique provides several benefits such as improving parallelism of tasks and fast-and-valid mapping of software models to hardware architecture. 
82|10||Flexible coordinator design for modeling resource sharing in multi-agent systems|One approach to modeling multi-agent systems (MASs) is to employ a method that defines components which describe the local behavior of individual agents, as well as a special component, called a coordinator. The coordinator component coordinates the resource sharing behavior among the agents. The agent models define a set of local plans, and the combination of local plans and a coordinator defines a system’s global plan. Although earlier work has provided the base functionality needed to synthesize inter-agent resource sharing behavior for a global, conflict-free MAS environment, the lack of coordination flexibility limits the modeling capability at both the local plan level and the global plan level. In this paper, we describe a flexible design method that supports a range of coordinator components. The method defines four levels of coordination and an associated four-step coordinator generation process, which allows for the design of coordinators with increasing capabilities for handling complexity associated with resource coordination. Colored Petri net based simulation is used to analyze various properties that derive from different coordinators and synthesis of a reduced coordinator component is discussed for cases that involve homogeneous agents. 
82|10||A method to build information systems engineering process metamodels|Several process metamodels exist. Each of them presents a different viewpoint of the same information systems engineering process. However, there are no existing correspondences between them. We propose a method to build unified, fitted and multi-viewpoint process metamodels for information systems engineering. Our method is based on a process domain metamodel that contains the main concepts of information systems engineering process field. This process domain metamodel helps selecting the needed metamodel concepts for a particular situational context. Our method is also based on patterns to refine the process metamodel. The process metamodel can then be instantiated according to the organisation’s needs. The resulting method is represented as a pattern system. 
82|10||A high stego-image quality steganographic scheme with reversibility and high payload using multiple embedding strategy|Tian’s method is a breakthrough reversible data embedding scheme with high embedding capacity measured by bits per pixel (bpp) and good visual quality measured by peak signal-to-noise ratio (PSNR). However, the embedding capacity and visual quality of this method can be significantly improved. Thus, we propose a simple reversible steganographic scheme in spatial domain for digital images by using the multiple embedding strategy. The proposed method horizontally and vertically embeds one secret bit into one cover pixel pair. The experimental results show that the proposed reversible steganographic method achieves good visual quality and high embedding capacity. Specifically, with the one-layer embedding, the proposed method can obtain the embedding capacity of more than 0.5 bpp and the PSNR value greater than 54 dB for all test images. Especially, with the five-layer embedding, the proposed method has the embedding capacity of more than 2 bpp and the PSNR value higher than 52 dB for all test images. Therefore, the proposed method surpasses many existing reversible data embedding methods in terms of visual quality and embedding capacity. 
82|11|http://www.sciencedirect.com/science/journal/01641212/82/11|TAIC PART 2007 and Mutation 2007 special issue editorial|
82|11||Modelling dynamic memory management in constraint-based testing|Constraint-based testing (CBT) is the process of generating test cases against a testing objective by using constraint solving techniques. When programs contain dynamic memory allocation and loops, constraint reasoning becomes challenging as new variables and new constraints should be created during the test data generation process. In this paper, we address this problem by proposing a new constraint model of C programs based on operators that model dynamic memory management. These operators apply powerful deduction rules on abstract states of the memory enhancing the constraint reasoning process. This allows to automatically generate test data respecting complex coverage objectives. We illustrate our approach on a well-known difficult example program that contains dynamic memory allocation/deallocation, structures and loops. We describe our implementation and provide preliminary experimental results on this example that show the highly deductive potential of the approach. 
82|11||Evolutionary testing of software with function-assigned flags|Evolutionary structural testing, an approach to automatically generate relevant unit test data, encounters difficulties when the software being tested contains boolean variables. This issue, known as the flag problem, has been studied by many researchers. However, previous work does not address the issue of function-assigned flags which constitutes a special type of flag problem that often occurs in the context of object-orientation. This paper elaborates on a new approach to the flag problem that can also handle function-assigned flags while being applicable to the conventional flag problem, as well. It relies on a code transformation that leads to an improved fitness landscape which provides better guidance to the evolutionary search. We present seven case studies including a fitness landscape analysis and experimental results. The results show that the suggested code transformation improves evolutionary structural testing in the presence of function-assigned flags. 
82|11||A practical evaluation of spectrum-based fault localization|Spectrum-based fault localization (SFL) shortens the test–diagnose–repair cycle by reducing the debugging effort. As a light-weight automated diagnosis technique it can easily be integrated with existing testing schemes. Since SFL is based on discovering statistical coincidences between system failures and the activity of the different parts of a system, its diagnostic accuracy is inherently limited. Using a common benchmark consisting of the Siemens set and the space program, we investigate this diagnostic accuracy as a function of several parameters (such as quality and quantity of the program spectra collected during the execution of the system), some of which directly relate to test design. Our results indicate that the superior performance of a particular similarity coefficient, used to analyze the program spectra, is largely independent of test design. Furthermore, near-optimal diagnostic accuracy (exonerating over 80% of the blocks of code on average) is already obtained for low-quality error observations and limited numbers of test cases. In addition to establishing these results in the controlled environment of our benchmark set, we show that SFL can effectively be applied in the context of embedded software development in an industrial environment. 
82|11||Increasing diversity: Natural language measures for software fault prediction|
82|11||Mutation testing from probabilistic and stochastic finite state machines|Specification mutation involves mutating a specification, and for each mutation a test is derived that distinguishes the behaviours of the mutated and original specifications. This approach has been applied with finite state machine based models. This paper extends mutation testing to finite state machine models that contain non-functional properties. The paper describes several ways of mutating a finite state machine with probabilities (PFSM) or stochastic time (PSFSM) attached to its transitions and shows how we can generate test sequences that distinguish between such a model and its mutants. Testing then involves applying each test sequence multiple times, observing the resultant behaviours and using results from statistical sampling theory in order to compare the observed frequency and execution time of each output sequence with that expected. 
82|11||Should software testers use mutation analysis to augment a test set?|Mutation testing has historically been used to assess the fault-finding effectiveness of a test suite or other verification technique. Mutation analysis, rather, entails augmenting a test suite to detect all killable mutants. Concerns about the time efficiency of mutation analysis may prohibit its widespread, practical use. The goal of our research is to assess the effectiveness of the mutation analysis process when used by software testers to augment a test suite to obtain higher statement coverage scores. We conducted two empirical studies and have shown that mutation analysis can be used by software testers to effectively produce new test cases and to improve statement coverage scores in a feasible amount of time. Additionally, we find that our user study participants view mutation analysis as an effective but relatively expensive technique for writing new test cases. Finally, we have shown that the choice of mutation tool and operator set can play an important role in determining how efficient mutation analysis is for producing new test cases. 
82|11||Reversible data hiding for high quality images using modification of prediction errors|In this paper, a reversible data hiding scheme based on modification of prediction errors (MPE) is proposed. For the existing histogram-shifting based reversible data hiding techniques, though the distortion caused by embedding is low, the embedding capacity is limited by the frequency of the most frequent pixel. To remedy this problem, the proposed method modifies the histogram of prediction errors to prepare vacant positions for data embedding. The PSNR of the stego image produced by MPE is guaranteed to be above 48 dB, while the embedding capacity is, on average, almost five times higher than that of the well-known Ni et al. techniques with the same PSNR. Besides, MPE not only has the capability to control the capacity-PSNR, where fewer data bits need less error modification, but also can be applied to images with flat histogram. Experimental results indicate that MPE, which innovatively exploits the modification of prediction errors, outperforms the prior works not only in terms of larger payload, but also in terms of stego image quality. 
82|11||DOM tree browsing of a very large XML document: Design and implementation|Browsing the DOM tree of an XML document is an act of following the links among the nodes of the DOM tree to find some desired nodes without any knowledge for search. When the structure of the XML document is not known to a user, browsing is the basic operation performed for referring the contents of the XML document. If the size of the XML document is very large, however, using a general-purpose XML parser for browsing the DOM tree of the XML document to access arbitrary node may suffer from the lack of memory space for constructing the large DOM tree. To alleviate this problem, we suggest a method to browse the DOM tree of a very large XML document by splitting the XML document into n small XML documents and generating sequentially the DOM tree of each of those small n XML documents. For later reference, the information of some nodes accessed from the DOM tree already generated has been also kept using the concept of their virtual nodes. With our suggested approach, the memory space necessary for browsing the DOM tree of a very large XML document is reduced such that it can be managed by a personal computer. 
82|11||A wireless sensor system for validation of real-time automatic calibration of groundwater transport models|In this paper, we present the use of a wireless sensor network in a lab for subsurface contaminant plume monitoring with the objective of automatic calibration of groundwater transport models. A tank configured to simulate an aquifer was used as a testbed, and a 2D model was created based on the setup. To simulate a contaminant plume, an ion tracer was injected into the tank. Sensor probes capable of detecting the plume were buried inside the tank, and wireless motes used to take readings from the sensors and relay data to a base station. More importantly, a run-time fault detection and diagnosis for abnormal sensor readings is designed and integrated into the data acquisition system. Further, an adaptive data collection technique is integrated that is able to provide evidence about the effectiveness of the groundwater transport model in use. Results from the tracer tests are presented, as well as lessons gained. 
82|11||Identifying some important success factors in adopting agile software development practices|Agile software development (ASD) is an emerging approach in software engineering, initially advocated by a group of 17 software professionals who practice a set of “lightweight” methods, and share a common set of values of software development. In this paper, we advance the state-of-the-art of the research in this area by conducting a survey-based ex-post-facto study for identifying factors from the perspective of the ASD practitioners that will influence the success of projects that adopt ASD practices. In this paper, we describe a hypothetical success factors framework we developed to address our research question, the hypotheses we conjectured, the research methodology, the data analysis techniques we used to validate the hypotheses, and the results we obtained from data analysis. The study was conducted using an unprecedentedly large-scale survey-based methodology, consisting of respondents who practice ASD and who had experience practicing plan-driven software development in the past. The study indicates that nine of the 14 hypothesized factors have statistically significant relationship with “Success”. The important success factors that were found are: customer satisfaction, customer collaboration, customer commitment, decision time, corporate culture, control, personal characteristics, societal culture, and training and learning. 
82|11||Discovery of architectural layers and measurement of layering violations in source code|The layers architectural pattern has been widely adopted by the developer community in order to build large software systems. In reality, as the system evolves over time, rarely does the system remain conformed to the intended layers pattern, causing a significant degradation of the system maintainability. As a part of re-factoring such a system, practitioners often undertake a mostly manual exercise to discover the intended layers and organize the modules into these layers. In this paper, we present a method for semi-automatically detecting layers in the system and propose a quantitative measurement to compute the amount of non-conformance of the system from the set of layered design principles. We have applied the layer detection method and the non-conformance measurement on a set of open source and proprietary enterprise applications. 
82|11||A robust DWT-based copyright verification scheme with Fuzzy ART|
82|11||Practical design of a proxy agent to facilitate adaptive video streaming service across wired/wireless networks|Thanks to the growing of the wireless networks, the video streaming application becomes a ubiquitous joyful service. In a wireless communication network environment, the service traffic spans across the wired and wireless domains. In this article, we propose a practical design of a proxy agent – SPONGE (Stream Pooler Over a Network Graded Environment) sitting between the wireless User Equipments (UEs) and the video streaming server to facilitate the adaptive video streaming service across wired/wireless networks. To make the wireless streaming service more efficient, an input video session would be encoded as multiple qualities of video streams so that UEs with a similar receiving condition can share streams with the same service quality via SPONGE. SPONGE can alleviate the direct load on the original stream broadcasting server. Meanwhile, it can make each UE get an adaptive streaming service according to the network conditions of the UE by a reduced network condition feedback latency. Our theoretical analysis and simulation results show that SPONGE can help wireless streaming users get a smooth and better playback quality by a quick and accurate reaction to the network condition. 
82|12|http://www.sciencedirect.com/science/journal/01641212/82/12|Full mobile agent interoperability in an IEEE-FIPA context|The existence of heterogeneous mobile agent systems hinders the interoperability of mobile agents. Several solutions exist, but they are limited in some aspects. This article proposes a full interoperability solution, in the context of the IEEE-FIPA agent standards, composed of three parts. The first part is a simple language-independent agent interface that enables agents to visit locations with different types of middlewares. The second part is a set of design models for the middlewares to support agents developed for different programming languages and architectures. And the third part is a method based on agents with multiple codes and a common agent data encoding mechanism to enable interoperability between middlewares that do not support the same programming languages. Furthermore two agent interoperability implementations, and its corresponding performance comparison, carried out over the JADE and AgentScape agent middlewares are presented. 
82|12||New enhancements to the SOCKS communication network security protocol: Schemes and performance evaluation|In this paper we propose two new enhancements to the SOCKS protocol in the areas of IP multicasting and UDP tunneling. Most network firewalls deployed at the entrance to a private network block multicast traffic. This is because of potential security threats inherent with IP multicast. Multicasting is the backbone of many Internet technologies like voice and video conferencing, real time gaming, multimedia streaming, and online stock quotes, among others. There is a need to be able to safely and securely allow multicast streams to enter into and leave a protected enterprise network. Securing multicast streams is challenging. It poses many architectural issues. The SOCKS protocol is typically implemented in a network firewall as an application-layer gateway. Our first enhancement in the area of IP multicast to the SOCKS protocol is to enable the application of security and access control policies and safely allow multicast traffic to enter into the boundaries of a protected enterprise network. The second enhancement we propose is to allow the establishment of a tunnel between two protected networks that have SOCKS based firewalls to transport UDP datagrams. 
82|12||Methodology evaluation framework for dynamic evolution in composition-based distributed applications|Dynamic evolution can be used to upgrade distributed applications without shutdown and restart as a way of improving service levels while minimising the loss of business revenue caused by the downtime. An evaluation framework assessing the level of support offered by existing methodologies in composition-based application (e.g. component-based and service-oriented) development is proposed. It was developed by an analysis of the literature and existing methodologies together with a refinement based on a survey of experienced practitioners and researchers. The use of the framework is demonstrated by applying it to twelve methodologies to assess their support for dynamic evolution. 
82|12||A high capacity reversible data hiding scheme with edge prediction and difference expansion|To enhance the embedding capacity of a reversible data hiding system, in this paper, a novel multiple-base lossless scheme based on JPEG-LS pixel value prediction and reversible difference expansion will be presented. The proposed scheme employs a pixel value prediction mechanism to decrease the distortion caused by the hiding of the secret data. In general, the prediction error value tends to be much smaller in smooth areas than in edge areas, and more secret data embedded in smooth areas still meets better stego-image quality. The multiple-base notational system, on the other hand, is applied to increase the payload of the image. With the system, the payload of each pixel, determined by the complexity of its neighboring pixels, can be very different. In addition, the cover image processed by the proposed scheme can be fully recovered without any distortion. Experimental results, as shown in this paper, have demonstrated that the proposed method is capable of hiding more secret data while keeping the stego-image quality degradation imperceptible. 
82|12||Constructing attribute weights from computer audit data for effective intrusion detection|Attributes construction and selection from audit data is the first and very important step for anomaly intrusion detection. In this paper, we present several cross frequency attribute weights to model user and program behaviors for anomaly intrusion detection. The frequency attribute weights include plain term frequency (TF) and various forms of term frequency-inverse document frequency (tfidf), referred to as Ltfidf, Mtfidf and LOGtfidf. Nearest Neighbor (NN) and k-NN methods with Euclidean and Cosine distance measures as well as principal component analysis (PCA) and Chi-square test method based on these frequency attribute weights are used for anomaly detection. Extensive experiments are performed based on command data from Schonlau et al. The testing results show that the LOGtfidf weight gives better detection performance compared with plain frequency and other types of weights. By using the LOGtfidf weight, the simple NN method and PCA method achieve the better masquerade detection results than the other 7 methods in the literature while the Chi-square test consistently returns the worst results. The PCA method is suitable for fast intrusion detection because of its capability of reducing data dimensionality while NN and k-NN methods are suitable for detection of a small data set because of its no need of training process. A HTTP log data set collected in a real environment and the sendmail system call data from University of New Mexico (UNM) are used as well and the results also demonstrate the effectiveness of the LOGtfidf weight for anomaly intrusion detection. 
82|12||A systematic review of domain analysis solutions for product lines|Domain analysis is crucial and central to software product line engineering (SPLE) as it is one of the main instruments to decide what to include in a product and how it should fit in to the overall software product line. For this reason many domain analysis solutions have been proposed both by researchers and industry practitioners. Domain analysis comprises various modeling and scoping activities. This paper presents a systematic review of all the domain analysis solutions presented until 2007. The goal of the review is to analyze the level of industrial application and/or empirical validation of the proposed solutions with the purpose of mapping maturity in terms of industrial application, as well as to what extent proposed solutions might have been evaluated in terms of usability and usefulness. The finding of this review indicates that, although many new domain analysis solutions for software product lines have been proposed over the years, the absence of qualitative and quantitative results from empirical application and/or validation makes it hard to evaluate the potential of proposed solutions with respect to their usability and/or usefulness for industry adoption. The detailed results of the systematic review can be used by individual researchers to see large gaps in research that give opportunities for future work, and from a general research perspective lessons can be learned from the absence of validation as well as from good examples presented. From an industry practitioner view, the results can be used to gauge as to what extent solutions have been applied and/or validated and in what manner, both valuable as input prior to industry adoption of a domain analysis solution. 
82|12||MHS: A distributed metadata management strategy|This paper proposes a novel distributed metadata management strategy to efficiently handle different metadata workloads. It can deliver high performance and scalable metadata service through four techniques, including directory conversion metadata, mimic hierarchical directory structure, flexible partition methods targeted different kinds of metadata of diverse characteristics, and the application of database to metadata backend. Using micro-benchmarks and a prototype system, we firstly demonstrate the performance superiority of our strategy compared to Lazy Hybrid, and then present the detailed performance results and analysis of our strategy on different MDS scales. 
82|12||An energy-efficient mobile transaction processing method using random back-off in wireless broadcast environments|Broadcast is widely accepted as an efficient technique for disseminating data to a large number of mobile clients over a single or multiple channels. Due to the limited uplink bandwidth from mobile clients to server, conventional concurrency control methods cannot be directly applied. There has been many researches on concurrency control methods for wireless broadcast environments. However, they are mostly for read-only transactions or do not consider exploiting cache. They also suffer from the repetitive aborts and restarts of mobile transactions when the access patterns of mobile transactions are skewed. In this paper, we propose a new optimistic concurrency control method suitable for mobile broadcast environments. To prevent the repetitive aborts and restarts of mobile transactions, we propose a random back-off technique. To exploit the cache on mobile clients, our method keeps the read data set of mobile transactions and prefetches those data items when the mobile transactions are restarted. As other existing optimistic concurrency control methods for mobile broadcast environments does, it works for both read-only and update transactions. Read-only transactions are validated and locally committed without using any uplink bandwidth. Update transactions are validated with forward and backward validation, and committed after final validation consuming a small amount of uplink bandwidth. Our performance analysis shows that it significantly decreases uplink and downlink bandwidth usage compared to other existing methods. 
82|12||Integrating knowledge flow mining and collaborative filtering to support document recommendation|Knowledge is a critical resource that organizations use to gain and maintain competitive advantages. In the constantly changing business environment, organizations must exploit effective and efficient methods of preserving, sharing and reusing knowledge in order to help knowledge workers find task-relevant information. Hence, an important issue is how to discover and model the knowledge flow (KF) of workers from their historical work records. The objectives of a knowledge flow model are to understand knowledge workers’ task-needs and the ways they reference documents, and then provide adaptive knowledge support. This work proposes hybrid recommendation methods based on the knowledge flow model, which integrates KF mining, sequential rule mining and collaborative filtering techniques to recommend codified knowledge. These KF-based recommendation methods involve two phases: a KF mining phase and a KF-based recommendation phase. The KF mining phase identifies each worker’s knowledge flow by analyzing his/her knowledge referencing behavior (information needs), while the KF-based recommendation phase utilizes the proposed hybrid methods to proactively provide relevant codified knowledge for the worker. Therefore, the proposed methods use workers’ preferences for codified knowledge as well as their knowledge referencing behavior to predict their topics of interest and recommend task-related knowledge. Using data collected from a research institute laboratory, experiments are conducted to evaluate the performance of the proposed hybrid methods and compare them with the traditional CF method. The results of experiments demonstrate that utilizing the document preferences and knowledge referencing behavior of workers can effectively improve the quality of recommendations and facilitate efficient knowledge sharing. 
82|12||UWIS: An assessment methodology for usability of web-based information systems|A methodology for usability assessment and design of web-based information systems (UWIS) is proposed. It combines web-based service quality and usability dimensions of information systems. Checklist items with the highest and the lowest contribution to the usability performance of a web-based information system can be specified by UWIS. A case study by a student information system at Fatih University is carried out to validate the methodology. UWIS reveals a strong relationship between quality and usability which is assumed to exist by many researchers but not experimentally analyzed yet. This study depicts a strong relevance between web-based service quality and usability of web-based information systems. UWIS methodology can be used for designing more usable and higher quality web-based information systems. 
82|12||A holistic approach to managing software change impact|Change is inevitable in the software product lifecycle. When a software change occurs, all of the stakeholders and related artifacts should be considered in determining the success of the change action in a collaborative development environment such as JAD (joint application development). In this regard, current implementation-based or homogeneous impact analyses are insufficient; therefore, this paper presents a holistic approach to change impact analysis in handling not only software contents but also other items such as requirements, documents and data. This approach characterizes product contents and relates heterogeneous items by using attributes and linkages. It also uses an object-oriented propagation mechanism to handle dynamic looping in determining the impact of changes. A prototype, EPIC, was built to realize this approach and these concepts. A walkthrough example is provided in order to verify the work of the proposed approach. An empirical study is presented to discuss the benefits of the proposed approach and the application of EPIC in a software company. Lessons learned from the case study and improvement issues of the proposed approach and the tool are also discussed. 
82|12||Tool support for the rapid composition, analysis and implementation of reactive services|We present the integrated set of tools Arctis for the rapid development of reactive services. In our method, services are composed of collaborative building blocks that encapsulate behavioral patterns expressed as UML 2.0 collaborations and activities. Due to our underlying semantics in temporal logic, building blocks as well as their compositions can be transformed into formulas and model checked incrementally in order to guarantee that important system properties are kept. The process of model checking is fully automated. Error traces are presented to the users as easily understandable animations, so that no expertise in temporal logic is needed. In addition, the results of model checking are analyzed, so that in some cases automated diagnoses and fixes can be provided as well. The formal semantics also enables the correct, automatic synthesis of the activities to state machines which form the input of our code generators. Thus, the collaborative models can be fully automatically transformed into executable Java code. We present the development of a mobile treasure hunt system to exemplify the method and the tools. 
82|12||Approach to designing bribery-free and coercion-free electronic voting scheme|Electronic voting has been in development for more than 20 years, during which it has produced outstanding results both in theory and in practice. However, bribery and coercion remain an open problem, as there is still no suitable manner to prevent or fight them. Publications emphasizing practicality has not been able to achieve effective protection, probably due to their overtly simple protection method, while publications emphasizing theories are difficult to put into practice due to the complicated protection method devised by them. Thus, how to design a scheme that can flawlessly prevent problems of bribery and coercion as well as put into practice easily becomes a significant issue. In this paper, we suggest that designers apply two indispensable design components, invisible channel and biometrics receipts, to design a prevention e-voting scheme, and also to introduce several feasible technology to help with its implementation. Followingly, a prevention electronic voting scheme that matches our ideal is proposed. We expect this study to arouse the interest of more researchers regarding the subject. 
82|12||A real options approach for evaluation and justification of a hospital information system|Nowadays healthcare organizations globally recognize the importance of investing in information technologies to improve the quality of care delivery and reduce costs. The key drivers of healthcare sector such as continuously improving healthcare standards and insurance systems have introduced new requirements for hospitals, which in return provided a solid ground for decision-makers to consider implementing hospital information systems that are customized and improved versions of enterprise resource planning (ERP) systems designed according to the needs of the healthcare sector. The conventional discounted cash flow methods ignore the value of managerial and strategic flexibility inherent in these investments, which is crucial for justification of the investment decision. This study introduces a real options-based methodology which overcomes the limitations of traditional valuation methods and enables decision-makers to value an ERP system investment incorporating multiple options. The option valuation model developed in this study extends the binomial lattice framework to model a hospital information system (HIS) investment opportunity with compound options. The potential application of the proposed model is illustrated through evaluation of a real-world HIS investment. 
82|12||Communication cost effective scheduling policies of nonclairvoyant jobs with load balancing in a grid|Effective load distribution is of great importance at grids, which are complex heterogeneous distributed systems. In this paper we study site allocation scheduling of nonclairvoyant jobs in a 2-level heterogeneous grid architecture. Three scheduling policies at grid level which utilize site load information are examined. The aim is the reduction of site load information traffic, while at the same time mean response time of jobs and fairness in utilization between the heterogeneous sites are of great interest. A simulation model is used to evaluate performance under various conditions. Simulation results show that considerable decrement in site load information traffic and utilization fairness can be achieved at the expense of a slight increase in response time. 
82|12||Platform-independent modeling and prediction of application resource usage characteristics|Application resource usage models can be used in the decision making process for ensuring quality-of-service as well as for capacity planning, apart from their general use in performance modeling, optimization, and systems management. Current solutions for modeling application resource usage tend to address parts of the problem by either focusing on a specific application, or a specific platform, or on a small subset of system resources. We propose a simple and flexible approach for modeling application resource usage in a platform-independent manner that enables the prediction of application resource usage on unseen platforms. The technique proposed is application agnostic, requiring no modification to the application (binary or source) and no knowledge of application-semantics. We implement a Linux-based prototype and evaluate it using four different workloads including real-world applications and benchmarks. Our experiments reveal prediction errors that are bound within 6–24% of the observed for these workloads when using the proposed approach. 
82|2|http://www.sciencedirect.com/science/journal/01641212/82/2|Expressing and organizing real-time specification patterns via temporal logics|Formal specification models provide support for the formal verification and validation of the system behaviour. This advantage is typically paid in terms of effort and time spent in learning and using formal methods and tools. The introduction and usage of patterns have a double impact. They stand for examples on how to cover classical problems with formal methods in many different notations, so that the user can shorten the time to understand if a formal method can be used to meet his purpose and how it can be used. Furthermore, they are used for shortening the specification time, by reusing and composing different patterns to cover the specification, thus producing more understandable specifications which refer to commonly known patterns. For these reasons, both interests in and usage of patterns are growing and a higher number of proposals for patterns and pattern classification/organization has appeared in the literature. This paper reports a review of the state of the art for real-time specification patterns, so as to organize them in a unified way, while providing some new patterns which complete the unified model. The proposed organization is based on some relationships among patterns as demonstrated in the paper. During the presentation the patterns have been formalized in TILCO-X, whereas in appendix a list of patterns with formalizations in several different logics such as TILCO, LTL, CTL, GIL, QRE, MTL, TCTL and RTGIL, is provided disguised as links to the locations where such formalizations can be recovered and/or are directly reported, if found not accessible in the literature; this allows the reader to have a detailed view of all the classified patterns, including the ones already added. Furthermore, an example has been proposed to highlight the usefulness of the new identified patterns completing the unified model. 
82|2||Protecting mobile agents from external replay attacks|This paper presents a protocol for the protection of mobile agents against external replay attacks. This kind of attacks are performed by malicious platforms when dispatching an agent multiple times to a remote host, thus making it reexecute part of its itinerary. Current proposals aiming to address this problem are based on storing agent identifiers, or trip markers, inside agent platforms, so that future reexecutions can be detected and prevented. The problem of these solutions is that they do not allow the agent to perform legal migrations to the same platform several times. The aim of this paper is to address these issues by presenting a novel solution based on authorisation entities, which allow the agent to be reexecuted on the same platform a number of times determined at runtime. The proposed protocol is secure under the assumption that authorisation entities are trusted. 
82|2||Frameworks for designing and implementing dependable systems using Coordinated Atomic Actions: A comparative study|
82|2||Development of a team measure for tacit knowledge in software development teams|In this paper we operationally define and measure tacit knowledge at the team-level in the software development domain. Through a series of three empirical studies we developed and validated the team tacit knowledge measure (TTKM) for software developers. In the first study, initial scale items were developed using the repertory grid technique and content analysis. In Study 2, supplied repertory grids were administered to novices and experts to establish differential items, and Study 3 validated the TTKM on a sample of 48 industrial software development teams. In developing the TTKM we explored the relationships between tacit knowledge, explicit job knowledge and social interaction and their effect on team performance as measured by efficiency and effectiveness. In addition we assess the implications for managing software development teams and increasing team performance through social interaction. 
82|2||A study of project selection and feature weighting for analogy based software cost estimation|A number of software cost estimation methods have been presented in literature over the past decades. Analogy based estimation (ABE), which is essentially a case based reasoning (CBR) approach, is one of the most popular techniques. In order to improve the performance of ABE, many previous studies proposed effective approaches to optimize the weights of the project features (feature weighting) in its similarity function. However, ABE is still criticized for the low prediction accuracy, the large memory requirement, and the expensive computation cost. To alleviate these drawbacks, in this paper we propose the project selection technique for ABE (PSABE) which reduces the whole project base into a small subset that consist only of representative projects. Moreover, PSABE is combined with the feature weighting to form FWPSABE for a further improvement of ABE. The proposed methods are validated on four datasets (two real-world sets and two artificial sets) and compared with conventional ABE, feature weighted ABE (FWABE), and machine learning methods. The promising results indicate that project selection technique could significantly improve analogy based models for software cost estimation. 
82|2||A family of experiments to evaluate a functional size measurement procedure for Web applications|The objective of this paper is to empirically evaluate OOmFPWeb, a functional size measurement procedure for Web applications. We analyzed four data sets from a family of experiments conducted in Spain, Argentina and Austria. Results showed that OOmFPWeb is efficient when compared to current industry practices. OOmFPWeb produced reproducible functional size measurements and was perceived as easy to use and useful by the study participants, who also expressed their intention to use OOmFPWeb in the future. The analysis further supports the validity and reliability of the technology acceptance model (TAM)-based evaluation instrument used in the study. 
82|2||A novel identity-based strong designated verifier signature scheme|Unlike ordinary digital signatures, a designated verifier signature scheme makes it possible for a signer to convince a designated verifier that she has signed a message in such a way that the designated verifier cannot transfer the signature to a third party. In a strong designated verifier signature scheme, no third party can even verify the validity of a designated verifier signature, since the designated verifier’s private key is required in the verifying phase. Firstly, this paper proposes the model of identity-based strong designated verifier signature scheme based on bilinear pairings by combining identity-based cryptosystem with the designated verifier signature scheme, and then, provides one concrete strong identity-based designated verifier signature scheme, which has short size of signature, low communication and computational cost. We provide security proofs for our scheme. 
82|2||Adaptive disk scheduling with workload-dependent anticipation intervals|Anticipatory scheduling (AS) of I/O requests has become a viable choice for block-device schedulers in open-source OS-kernels as prior work has established its superiority over traditional disk-scheduling policies. An AS-scheduler selectively stalls the block-device right after servicing a request in hope that a new request for a nearby sector will be soon posted. Clearly, this decision may introduce delays if the anticipated I/O does not arrive on time. In this paper, we build on the success of the AS and propose an approach that minimizes the overhead of unsuccessful anticipations. Our suggested approach termed workload-dependent anticipation scheduling (WAS), determines the length of every anticipation period in an on-line fashion in order to reduce penalties by taking into account the evolving spatio-temporal characteristics of running processes as well as properties of the underlying computing system. We harvest the spatio-temporal features of individual processes and employ a system-wide process classification scheme that is re-calibrated on the fly. The resulting classification enables the disk scheduler to make informed decisions and vary the anticipation interval accordingly, on a per-process basis. We have implemented and incorporated WAS into the current Linux kernel. Through experimentation with a wide range of diverse workloads, we demonstrate WAS benefits and establish reduction of penalties over other AS-scheduler implementations. 
82|2||Specifying behavioral semantics of UML diagrams through graph transformations|The Unified Modeling Language (UML) has been widely accepted as a standard for modeling software systems from various perspectives. The intuitive notations of UML diagrams greatly improve the communication among developers. However, the lack of a formal semantics makes it difficult to automate analysis and verification. This paper offers a graphical yet formal approach to specifying the behavioral semantics of statechart diagrams using graph transformation techniques. It supports many advanced features of statecharts, such as composite states, firing priority, history, junction, and choice. In our approach, a graph grammar is derived automatically from a state machine to summarize the hierarchy of states. Based on the graph grammar, the execution of a set of non-conflict state transitions is interpreted by a sequence of graph transformations. This facilitates verifying a design model against system requirements. To demonstrate our approach, we present a case study on a toll-gate system. 
82|2||Comparative evaluation of contiguous allocation strategies on 3D mesh multicomputers|The performance of contiguous allocation strategies can be significantly affected by the type of the distribution adopted for job execution times. In this paper, the performance of the existing contiguous allocation strategies for 3D mesh multicomputers is re-visited in the context of heavy-tailed distributions (e.g., a Bounded Pareto distribution). The strategies are evaluated and compared using simulation experiments for both First-Come-First-Served (FCFS) and Shortest-Service-Demand (SSD) scheduling strategies under a variety of system loads and system sizes. The results show that the performance of the allocation strategies degrades considerably when job execution times follow a heavy-tailed distribution. Moreover, SSD copes much better than FCFS scheduling strategy in the presence of heavy-tailed job execution times. The results also reveal that allocation strategies that employ a list of allocated sub-meshes for both allocation and de-allocation exhibit low allocation overhead, and maintain good system performance in terms of average turnaround time and mean system utilization. 
82|2||Quality attribute tradeoff through adaptive architectures at runtime|Quality attributes, e.g. performance and reliability, become more and more important for the development of software systems. One of the critical issues on quality assurance is how to make good enough tradeoffs between quality attributes that interfere with each other. Some architecture-based quality design and analysis methods are proposed to make tradeoffs at design time. However, many quality attributes depend on runtime contexts; it may be difficult and even impossible to make tradeoffs between them at design time. In this paper, we use an adaptive architecture model to capture candidate strategies for different quality attributes; the tradeoff, i.e. which strategies are more appropriate and thus applied, is postponed to runtime. The contribution of our approach is threefold. First, it makes use of existing architecture-based quality design and analysis methods to identify why and where quality attribute tradeoffs are necessary. Second, a traditional architecture description language is extended to support the modeling of an adaptive architecture, which records strategies for different quality attributes under different conditions. Third, a reflective middleware is used to monitor the runtime system, collect required information to determine appropriate strategies, and adapt the application’s architecture to achieve expected quality attributes. This approach is demonstrated on J2EE. 
82|2||Exception handling refactorings: Directed by goals and driven by bug fixing|
82|2||Differential fault analysis on the contracting UFN structure, with application to SMS4 and MacGuffin|The contracting unbalanced Feistel networks (UFN) is a particular structure in the block ciphers, where the “left half” and the “right half” are not of equal size, and the size of the domain of one half is larger than that of the range. This paper studies the security of the contracting UFN structure against differential fault analysis (DFA). We propose two basic byte-oriented fault models and two corresponding attacking methods. Then we implement the attack on two instances of the contracting UFN structure, the block ciphers SMS4 and MacGuffin. The experiments require 20 and 4 faulty ciphertexts to recover the 128-bit secret key of SMS4 in the two fault models, respectively. Under similar hypothesis, MacGuffin is breakable with 355 and 165 faulty ciphertexts, respectively. So our work not only builds up a general model of DFA on the contracting UFN structure and ciphers, but also provides a new reference for fault analysis on other block ciphers. 
82|2||A mobile agent platform for distributed network and systems management|The mobile agent (MA) technology has been proposed for the management of networks and distributed systems as an answer to the scalability problems of the centralized paradigm. Management tasks may be assigned to an agent, which delegates and executes management logic in a distributed and autonomous fashion. MA-based management has been a subject of intense research in the past few years, reflected on the proliferation of MA platforms (MAPs) expressly oriented to distributed management. However, most of these platforms impose considerable burden on network and system resources and also lack of essential functionality, such as security mechanisms, fault tolerance, strategies for building network-aware MA itineraries and support for user-friendly customization of MA-based management tasks. In this paper, we discuss the design considerations and implementation details of a complete MAP research prototype that sufficiently addresses all the aforementioned issues. Our MAP has been implemented in Java and optimized for network and systems management applications. The paper also presents the evaluation results of our prototype in real and simulated networking environments. 
82|3|http://www.sciencedirect.com/science/journal/01641212/82/3|An update to experimental models for validating computer technology|In 1998 a survey was published on the extent to which software engineering papers validate the claims made in those papers. The survey looked at publications in 1985, 1990 and 1995. This current paper updates that survey with data from 2000 to 2005. The basic conclusion is that the situation is improving. One earlier complaint that access to data repositories was difficult is becoming less prevalent and the percentage of papers including validation is increasing. 
82|3||A new algorithm with segment protection and load balancing for single-link failure in multicasting survivable networks|In this paper, we investigate the problem of failure tolerated multicast requests in survivable networks and propose a new heuristic algorithm called segment protection with load balancing (SPLB) to address the single-link failure. In order to obtain better performances, in SPLB first we consider the techniques of cross-sharing and self-sharing to improve the resource utilization ratio, second we propose a segment protection routing algorithm to overcome the trap problem, and third we design a load balancing method to reduce the blocking probability. Compared with conventional algorithm, SPLB performs better performances. Simulation results meet our expectation. 
82|3||Component-based software version management based on a Component-Interface Dependency Matrix|As much component-based software is developed, a software configuration management (SCM) tool for component-based software is necessary. In this paper, we propose a version management mechanism for impact analysis while components are upgraded. We separately version the components and interfaces based on a Component-Interface Dependency Matrix (CIDM), and analyze impacts according to their dependency relationship. The result of our simulation shows that CIDM is capable of managing large numbers of components without impedance mismatch. In addition, in a well-designed software system, using CIDM to analyze impacts can save resources in the software development phase. 
82|3||Classification and evaluation of timed running schemas for workflow based on process mining|The system running logs of a workflow contain much information about the behavior and logical structure between activities. In this paper, a mining approach is proposed to discover the structural and temporal model for a workflow from its timed running logs. The mining results are represented in the formalized form of Petri nets extended with two timing factors that allows validation or verification the actual behaviors, especially the temporal constraints between activities. According to the reachability graph of the extended Petri net model mined, all running schemas of a workflow can be generated, which defines the temporal constraints between running activities. By calculating the earliest and latest start time of each activity, the earliest starting and latest existing time of each state in the running schema can be determined. Based on the temporal relations between the timing factors of each running state, the running schemas can be classified into six classes. The effects of the six classes of running schemas on the implementation of the whole workflow are evaluated so as to obtain the best one that can ensure the workflow is finished in the shortest time. The standards for the ideal, reliable and favorable running schemas and their existence conditions are discussed, which can be used to evaluate the running logs and control the future running of a workflow. 
82|3||A HDWT-based reversible data hiding method|This paper presents a reversible data hiding method which provides a high payload and a high stego-image quality. The proposed method transforms a spatial domain cover image into a frequency domain image using the Haar digital wavelet transform (HDWT) method, compresses the coefficients of the high frequency band by the Huffman (or arithmetic) coding method, and then embeds the compression data and the secret data in the high frequency band. Since the high frequency band incorporates less energy than other bands of an image, it can be exploited to carry secret data. Furthermore, the proposed method utilizes the Huffman (or arithmetic) coding method to recover the cover image without any distortion. The proposed method is simple and the experimental results show that the designed method can give a high hiding capacity with a high quality of stego-image. 
82|3||PAT: A pattern classification approach to automatic reference oracles for the testing of mesh simplification programs|Graphics applications often need to manipulate numerous graphical objects stored as polygonal models. Mesh simplification is an approach to vary the levels of visual details as appropriate, thereby improving on the overall performance of the applications. Different mesh simplification algorithms may cater for different needs, producing diversified types of simplified polygonal model as a result. Testing mesh simplification implementations is essential to assure the quality of the graphics applications. However, it is very difficult to determine the oracles (or expected outcomes) of mesh simplification for the verification of test results.A reference model is an implementation closely related to the program under test. Is it possible to use such reference models as pseudo-oracles for testing mesh simplification programs? If so, how effective are they?This paper presents a fault-based pattern classification methodology called PAT, to address the questions. In PAT, we train the C4.5 classifier using black-box features of samples from a reference model and its fault-based versions, in order to test samples from the subject program. We evaluate PAT using four implementations of mesh simplification algorithms as reference models applied to 44 open-source three-dimensional polygonal models. Empirical results reveal that the use of a reference model as a pseudo-oracle is effective for testing the implementations of resembling mesh simplification algorithms. However, the results also show a tradeoff: When compared with a simple reference model, the use of a resembling but sophisticated reference model is more effective and accurate but less robust. 
82|3||Managing requirements specifications for product lines â An approach and industry case study|Software product line development has emerged as a leading approach for software reuse. This paper describes an approach to manage natural-language requirements specifications in a software product line context. Variability in such product line specifications is modeled and managed using a feature model. The proposed approach has been introduced in the Swedish defense industry. We present a multiple-case study covering two different product lines with in total eight product instances. These were compared to experiences from previous projects in the organization employing clone-and-own reuse. We conclude that the proposed product line approach performs better than clone-and-own reuse of requirements specifications in this particular industrial context. 
82|3||Reengineering for service oriented architectures: A strategic decision model for integration versus migration|Service Oriented Architecture (SOA) is a popular paradigm at present because it provides a standards-based conceptual framework for flexible and adaptable enterprise wide systems. This implies that most present systems need to be reengineered to become SOA compliant. However, SOA reengineering projects raise serious strategic as well as technical questions that require management oversight. This paper, based on practical experience with SOA projects, presents a decision model for SOA reengineering projects that combines strategic and technical factors with cost-benefit analysis for integration versus migration decisions. The paper identifies the key issues that need to be addressed in enterprise application reengineering projects for SOA, examines the strategic alternatives, explains how the alternatives can be evaluated based on architectural and cost-benefit considerations and illustrates the main ideas through a detailed case study. 
82|3||An empirical approach to evaluating dependency locality in hierarchically structured software systems|In software development, especially component-based software development, dependency locality states that relevant software components should be at shorter distances than irrelevant components. This principle is used together with modularity and hierarchy to guide the design of large-scale complex software systems. In previous work, dependency locality and its correlation with design quality were studied by statically measuring the interactions between software components. This paper presents an empirical approach to evaluating the hierarchical structure of software systems through mining their revision history. Two metrics, spatial distance and temporal distance, are adapted to measure the dependencies between software components. The correlation of spatial distance and temporal distance between software components represents a factor that influences system design quality. More specially, a well designed system hierarchy should have a significant positive correlation while a non-significant positive correlation or a negative correlation would signify design flaws. In an application of this approach, we use Mantel test to study the dependency locality of six software systems from Apache projects. 
82|3||A real-time network simulation infrastructure based on OpenVPN|We present an open and flexible software infrastructure that embeds physical hosts in a simulated network. In real-time network simulation, where real-world implementations of distributed applications and network services can run together with the network simulator that operates in real-time, real network packets are injected into the simulation system and subject to the simulated network conditions computed as a result of both real and virtual traffic traversing the network and competing for network resources. Our real-time simulation infrastructure has been implemented based on Open Virtual Private Network (OpenVPN), modified and customized to bridges traffic between the physical hosts and the simulated network. We identify the performance advantages and limitations of our approach via a set of experiments. We also present two interesting application scenarios to show the capabilities of the real-time simulation infrastructure. 
82|3||Autonomic QoS control in enterprise Grid environments using online simulation|As Grid Computing increasingly enters the commercial domain, performance and quality of service (QoS) issues are becoming a major concern. The inherent complexity, heterogeneity and dynamics of Grid computing environments pose some challenges in managing their capacity to ensure that QoS requirements are continuously met. In this paper, a comprehensive framework for autonomic QoS control in enterprise Grid environments using online simulation is proposed. This paper presents a novel methodology for designing autonomic QoS-aware resource managers that have the capability to predict the performance of the Grid components they manage and allocate resources in such a way that service level agreements are honored. Support for advanced features such as autonomic workload characterization on-the-fly, dynamic deployment of Grid servers on demand, as well as dynamic system reconfiguration after a server failure is provided. The goal is to make the Grid middleware self-configurable and adaptable to changes in the system environment and workload. The approach is subjected to an extensive experimental evaluation in the context of a real-world Grid environment and its effectiveness, practicality and performance are demonstrated. 
82|3||An efficient XML encoding and labeling method for query processing and updating on dynamic XML data|In this paper, we propose an efficient encoding and labeling scheme for XML, called EXEL, which is a variant of the region labeling scheme using ordinal and insert-friendly bit strings. We devise a binary encoding method to generate the ordinal bit strings, and an algorithm to make a new bit string inserted between bit strings without any influences on the order of preexisting bit strings. These binary encoding method and bit string insertion algorithm are the bases of the efficient query processing and the complete avoidance of re-labeling for updates. We present query processing and update processing methods based on EXEL. In addition, the Stack-Tree-Desc algorithm is used for an efficient structural join, and the String B-tree indexing is utilized to improve the join performance. Finally, the experimental results show that EXEL enables complete avoidance of re-labeling for updates while providing fairly reasonable query processing performance. 
82|3||Search-order coding method with indicator-elimination property|Vector quantization (VQ) is a widely used technique for many applications especially for lossy image compression. Since VQ significantly reduces the size of a digital image, it can save the costs of storage space and image delivery. Search-order coding (SOC) was proposed for improving the performance of VQ in terms of compression rate. However, SOC requires extra data (i.e. indicators) to indicate source of codewords so the compression rate may be affected. To overcome such a drawback, in this paper, a search-order coding with the indicator-elimination property was proposed by using a technique of reversible data hiding. The proposed method is the first one using such a concept of data hiding to achieve a better compression rate of SOC. From experimental results, the performance of the SOC method can be successfully improved by the proposed indicator eliminated search-order coding method in terms of compression rate. In addition, compared with other relevant schemes, the proposed method is also more flexible than some existing schemes. 
82|3||A lossy 3D wavelet transform for high-quality compression of medical video|In this paper, we present a lossy compression scheme based on the application of the 3D fast wavelet transform to code medical video. This type of video has special features, such as its representation in gray scale, its very few interframe variations, and the quality requirements of the reconstructed images. These characteristics as well as the social impact of the desired applications demand a design and implementation of coding schemes especially oriented to exploit them. We analyze different parameters of the codification process, such as the utilization of different wavelets functions, the number of steps the wavelet function is applied to, the way the thresholds are chosen, and the selected methods in the quantization and entropy encoder. In order to enhance our original encoder, we propose several improvements in the entropy encoder: 3D-conscious run-length, hexadecimal coding and the application of arithmetic coding instead of Huffman. Our coder achieves a good trade-off between compression ratio and quality of the reconstructed video. We have also compared our scheme with MPEG-2 and EZW, obtaining better compression ratios up to 119% and 46%, respectively for the same PSNR. 
82|3||Gompertz software reliability model: Estimation algorithm and empirical validation|
82|3||On the similarity between requirements and architecture|Many would agree that there is a relationship between requirements engineering and software architecture. However, there have always been different opinions about the exact nature of this relationship. Nevertheless, all arguments have been based on one overarching notion: that of requirements as problem description and software architecture as the structure of a software system that solves that problem, with components and connectors as the main elements.Recent developments in the software architecture field show a change in how software architecture is perceived. There is a shift from viewing architecture as only structure to a broader view of ‘architectural knowledge’ that emphasizes the treatment of architectural design decisions as first-class entities. From this emerging perspective we argue that there is no fundamental distinction between architectural decisions and architecturally significant requirements. This new view on the intrinsic relation between architecture and requirements allows us to identify areas in which closer cooperation between the architecture and requirements engineering communities would bring advantages for both. 
82|4|http://www.sciencedirect.com/science/journal/01641212/82/4|Tilte page|
82|4||Contents|
82|4||Software engineering challenges of the âNetâ generation|
82|4||Using Wikis to support the Net Generation in improving knowledge acquisition in capstone projects|Students have to cope with new technologies, changing environments, and conflicting changes in capstone projects. They often lack practical experience, which might lead to failing to achieve a project’s learning goals. In addition, the Net Generation students put new requirements upon software engineering education because they are digitally literate, always connected to the Internet and their social networks. They react fast and multitask, prefer an experimental working approach, are communicative, and need personalized learning and working environments. Reusing experiences from other students provides a first step towards building up practical knowledge and implementing experiential learning in higher education. In order to further improve knowledge acquisition during experience reuse, we present an approach based on Web 2.0 technologies that generates so-called learning spaces. This approach automatically enriches experiences with additional learning content and contextual information. To evaluate our approach, we conducted a controlled experiment, which showed a statistically significant improvement for knowledge acquisition of 204% compared to conventional experience descriptions. From a technical perspective, the approach provides a good basis for future applications that support learning at the workplace in academia and industry for the Net Generation. 
82|4||Engaging the net generation with evidence-based software engineering through a community-driven web database|
82|4||Software engineering education: How far weâve come and how far we have to go|
82|4||Single development project|An often-cited problem in undergraduate software engineering courses states that some topics are difficult to teach in a university setting and, although laboratory work is a useful supplement to the lectures, it is difficult to make projects realistic and relevant. In recognition of this problem, and based on our past experience, we started preparing a new course by examining the pedagogies and curricular aspects of software engineering that are important for the Net Generation of software engineers.The course project described in this paper concentrates on those aspects that can be dealt with effectively within the environment, i.e., the software lifecycle, system interdependences, teamwork, and realistic yet manageable project dynamics, all supported by various means of communication. The workload per students must be balanced with their lack of knowledge and skills, so that their unpreparedness to deal with complex issues does not abate their motivation.The approach was tested on six large projects over the period of one semester. We believe that the results reflect the students’ strong interest and commitment, and demonstrate their ability to stay focused and work at a level that is well above the obvious. 
82|4||Discovering vulnerabilities in control system humanâmachine interface software|
82|4||Optimization methodology of dynamic data structures based on genetic algorithms for multimedia embedded systems|
82|4||Mining frequent patterns in image databases with 9D-SPA representation|
82|4||Real-time task scheduling by multiobjective genetic algorithm|Real-time tasks are characterized by computational activities with timing constraints and classified into two categories: a hard real-time task and a soft real-time task. In hard real-time tasks, tardiness can be catastrophic. The goal of hard real-time tasks scheduling algorithms is to meet all tasks’ deadlines, in other words, to keep the feasibility of scheduling through admission control. However, in the case of soft real-time tasks, slight violation of deadlines is not so critical.In this paper, we propose a new scheduling algorithm for soft real-time tasks using multiobjective genetic algorithm (moGA) on multiprocessors system. It is assumed that tasks have precedence relations among them and are executed on homogeneous multiprocessor environment.The objective of the proposed scheduling algorithm is to minimize the total tardiness and total number of processors used. For these objectives, this paper combines adaptive weight approach (AWA) that utilizes some useful information from the current population to readjust weights for obtaining a search pressure toward a positive ideal point. The effectiveness of the proposed algorithm is shown through simulation studies. 
82|4||An event-driven high level model for the specification of laws in open multi-agent systems|
82|4||An identity based universal designated verifier signature scheme secure in the standard model|
82|4||Reducing software requirement perception gaps through coordination mechanisms|
82|4||Early fire detection method in video for vessels|
82|4||Using aspect orientation in legacy environments for reverse engineering using dynamic analysisâAn industrial experience report|
82|4||A new relevance feedback technique for iconic image retrieval based on spatial relationships|
82|4||Energy-efficient real-time object tracking in multi-level sensor networks by mining and predicting movement patterns|
82|4||A belief-theoretic framework for the collaborative development and integration of para-consistent conceptual models|
82|4||Use of an adaptable quality model approach in a production support environment|
82|5|http://www.sciencedirect.com/science/journal/01641212/82/5|Identifying exogenous drivers and evolutionary stages in FLOSS projects|
82|5||Refining component description by leveraging user query logs|
82|5||Dynamic SLAs management in service oriented environments|
82|5||Searching for similar trajectories in spatial networks|
82|5||Design of DL-based certificateless digital signatures|
82|5||Improvement of identity-based proxy multi-signature scheme|
82|5||The relation of requirements uncertainty and stakeholder perception gaps to project management performance|
82|5||Dynamic Web Service discovery architecture based on a novel peer based overlay network|
82|5||An index management using CHC-cluster for flash memory databases|
82|5||Evaluating two ways of calculating priorities in requirements hierarchies â An experiment on hierarchical cumulative voting|When developing large-scale software systems, there is often a large amount of requirements present, and they often reside on several hierarchical levels. In most cases, not all stated requirements can be implemented into the product due to different constraints, and the requirements must hence be prioritized. As requirements on different abstraction levels shall not be compared, prioritization techniques that are able to handle multi-level prioritization are needed. Different such techniques exist, but they seem to result in unfair comparisons when a hierarchy is unbalanced. In this paper, an empirical experiment is presented where an approach that compensate for this challenge is evaluated. The results indicate that some form of compensation is preferred, and that the subjects’ preference is not influenced by the amount of information given. 
82|5||Determining factors that affect long-term evolution in scientific application software|
82|5||A static API birthmark for Windows binary executables|
82|5||Improving reliability of cooperative concurrent systems with exception flow analysis|
82|5||Autonomous mobile agent routing for efficient server resource allocation|
82|5||An entropy-based algorithm for data elimination in time-driven software instrumentation|
82|6|http://www.sciencedirect.com/science/journal/01641212/82/6|Lightweight query-based analysis of workflow process dependencies|
82|6||A delegation-based approach for the unanticipated dynamic evolution of distributed objects|Large information systems are typically distributed and cater to several client programs, with different needs. Traditional approaches to software development and deployment cannot handle situations where (i) the needs of one client application evolve over time, diverging from the needs of others, and (ii) when the server application cannot be shutdown for maintenance. In this paper, we propose an experimental framework for the unanticipated dynamic evolution of distributed objects that enables us to: (i) extend the behavior of distributed objects during run-time, requiring no shutdown, and (ii) offer different functionalities to different applications simultaneously. In our approach, new client programs can invoke behavioral extensions to server objects that are visible only to them, while legacy applications may continue to use the non-extended versions of the server. Our approach has the advantage of: (i) requiring no changes to the host programming language or to the virtual machine, and (ii) providing a transparent programming model to the developer. In this paper, we describe the problem of unanticipated dynamic evolution of distributed objects, the principles underlying our approach, and our prototype implementations for Java and C#. We conclude by discussing related work, and the extent to which our approach can be used to support industrial strength unanticipated evolution. 
82|6||CUDL language semantics: Updating FDB data|
82|6||An efficient void resolution method for geographic routing in wireless sensor networks|Geographic routing is an attractive choice for routing data in wireless sensor networks because of lightweight and scalable characteristics. Most geographic routing approaches combine a greedy forwarding scheme and a void resolution method to detour a void area that has no active sensor. The previous solutions, including the well-known GPSR protocol, commonly use the right-hand rule for void resolution. However, the detour path produced by the right-hand rule is not energy-efficient in many cases. In this paper, we propose a new void resolution method, called void resolution-forwarding, which can overcome voids in the sensor network energy-efficiently. It exploits the quadrant-level right-hand rule to select the next hop for the current node during circumventing a void area. We show by experiments that the proposed method is efficient and scalable with respect to the various voids and network density and that it outperforms the GPSR protocol significantly. 
82|6||Efficient self-certified proxy CAE scheme and its variants|Elaborating on the merits of proxy signature schemes and convertible authenticated encryption (CAE) schemes, we adopt self-certified public key systems to construct efficient proxy CAE schemes enabling an authorized proxy signer to generate an authenticated ciphertext on behalf of the original signer. To satisfy the requirement of confidentiality, only the designated recipient is capable of decrypting the ciphertext and verifying the proxy signature. A significant advantage of the proposed schemes is that the proxy signature conversion process takes no extra cost, i.e., when the case of a later dispute over repudiation occurs, the designated recipient can easily reveal the ordinary proxy signature for the public arbitration. If needed, the designated recipient can also convince anyone that he is the real recipient. In addition, integrating with self-certified public key systems, our schemes can earn more computational efficiency, since authenticating the public key and verifying the proxy signature can be simultaneously carried out within one-step. 
82|6||An empirical analysis of the impact of software development problem factors on software maintainability|Many problem factors in the software development phase affect the maintainability of the delivered software systems. Therefore, understanding software development problem factors can help in not only reducing the incidence of project failure but can also ensure software maintainability. This study focuses on those software development problem factors which may possibly affect software maintainability. Twenty-five problem factors were classified into five dimensions; a questionnaire was designed and 137 software projects were surveyed. A K-means cluster analysis was performed to classify the projects into three groups of low, medium and high maintainability projects. For projects which had a higher level of severity of problem factors, the influence on software maintainability becomes more obvious. The influence of software process improvement (SPI) on project problems and the associated software maintainability was also examined in this study. Results suggest that SPI can help reduce the level of severity of the documentation quality and process management problems, and is only likely to enhance software maintainability to a medium level. Finally, the top 10 list of higher-severity software development problem factors was identified, and implications were discussed. 
82|6||Software engineering technology innovation â Turning research results into industrial success|This paper deals with the innovation of software engineering technologies. These technologies are methods and tools for conducting software development and maintenance. We consider innovation as a process consisting of two phases, being technology creation and technology transfer. In this paper, we focus mainly on the transfer phase.Technology transfer is of mutual interest to both academia and industry. Transfer is important for academia, because it shows the industrial relevance of their research results to, for example, their sponsoring authorities. Showing the industrial applicability of research results is sometimes referred to as valorization of research. Nowadays, valorization is often required by research funding bodies. The transfer is important for industries, because innovating their development processes and their products is supportive in gaining a competitive edge or remaining competitive in their business. We describe the technology transfer phase by means of three activities: technology evaluation, technology engineering, and technology embedding. The technology evaluation activity is perceived as the main gate between the technology creation phase and the technology transfer phase. With two case studies, originating from the Dutch high-tech systems industry, we will illustrate the activities in the transfer phase.In addition to the process we will also define the main roles in a software engineering technology innovation, namely: the technology provider (academic research, industrial research and technology vendor) and the technology receiver (industrial development). With those roles we also address the issues concerning the ownership of technologies. 
82|6||Design and implementation of a Byzantine fault tolerance framework for Web services|
82|6||An improved lossless data hiding scheme based on image VQ-index residual value coding|
82|6||Extending path summary and region encoding for efficient structural query processing in native XML databases|Optimizing query processing is always a challenging task in the XML database community. Current state-of-the-art approaches focus mainly on simple query. Yet, as the usage of XML shifts towards the data-oriented paradigm, more and more complex query processing needs to be supported. In this paper, we present TwigX-Guide, a hybrid system, which takes advantage of the beautiful features of path summary in DataGuide and region encoding in TwigStack to improve complex query processing. Experimental results indicate that TwigX-Guide can process complex queries on an average 38% better than the TwigStack algorithm, 31% better than TwigINLAB, 11% better than TwigStackList and about 9% better than TwigStackXB in terms of execution time. 
82|6||Using phrases as features in email classification|In this paper, we report our experience on the use of phrases as basic features in the email classification problem. We performed extensive empirical evaluation using our large email collections and tested with three text classification algorithms, namely, a naive Bayes classifier and two k-NN classifiers using TF–IDF weighting and resemblance respectively. The investigation includes studies on the effect of phrase size, the size of local and global sampling, the neighbourhood size, and various methods to improve the classification accuracy. We determined suitable settings for various parameters of the classifiers and performed a comparison among the classifiers with their best settings. Our result shows that no classifier dominates the others in terms of classification accuracy. Also, we made a number of observations on the special characteristics of emails. In particular, we observed that public emails are easier to classify than private ones. 
82|6||Measuring e-business dependability: The employee perspective|
82|7|http://www.sciencedirect.com/science/journal/01641212/82/7|nAIT: A source analysis and instrumentation framework for nesC|
82|7||HIPaG: An energy-efficient in-network join for distributed condition tables in sensor networks|
82|7||Grid enabled MRP process improvement under distributed database environment|The material requirement planning (MRP) process is crucial when software packages, like enterprise resource planning (ERP) software, are used in the production planning for manufacturing enterprises to ensure that appropriate quantities of raw materials and subassemblies are provided at the right time. Whereas little attention has been paid to the architectural aspects of MRP process in academic studies, in practice, reports are often made of its time consuming characteristics due to intensive interactions with databases and difficulty in real time processing. This paper proposes a grid enabled MRP process in a distributed database environment and demonstrates the performance improvement of the proposed process by a simulation study. 
82|7||Period sensitivity analysis and DâP domain feasibility region in dynamic priority systems|
82|7||An efficient approach for distributed dynamic channel allocation with queues for real-time and non-real-time traffic in cellular networks|We are witnessing these days a rapid growth of mobile users. Therefore, frequency spectrum must be efficiently utilized, as available frequency spectrum is limited. This paper proposes a channel allocation scheme with efficient bandwidth reservation, which initially reserves some channels for handoff calls, and later reserves the channels dynamically, based on the user mobility. The direction of user mobility may not be straight always, but the user may also go left, right or backwards. Thus, QoS can be improved, if the channel reservation is made based upon the user mobility and the location of the user. We devise here a new algorithm that deals with multiple traffic systems by modifying the existing DDCA algorithm [Krishna, P.V., Iyengar, N.Ch.S.N., 2008. Optimal channel allocation algorithm with efficient channel reservation for cellular networks. International Journal of Communication Networks and Distributed Systems 1 (1), 33–51]. This algorithm reserves more channels for hot cells, less number of channels for cold cells and an average number of channels for the medium cells. Furthermore, we maintain queues for all types of calls. We model the system by a three-dimensional Markov Chain and compute the QoS parameters in terms of the blocking probability of originating calls and the dropping probability of handoff calls. The results indicate that the proposed channel allocation scheme exhibits better performance by considering the above mentioned user mobility, type of cells, and maintaining of the queues for various traffic sources. In addition, it can be observed that our approach reduces the dropping probability by using reservation factor. 
82|7||Functional metamodels for systems and software|The modeling, analysis and design of systems is generally based on many formalisms to describe discrete and/or continuous behaviors, and to map these descriptions into a specific platform. In this context, the article proposes the concept of functional metamodeling to capture, then to integrate modeling languages. The concept offers an alternative to standard Model Driven Engineering (MDE) and is well adapted to mathematical descriptions such as the ones found in system modeling. As an application, a set of functional metamodels is proposed for dataflows (usable to model continuous behaviors), state-transition systems (usable to model discrete behaviors) and a metamodel for actions (to model interactions with a target platform and concurrent execution). A model of a control architecture for a legged robot is proposed as an application of these modeling languages. 
82|7||Building problem-solving environments with the Arches framework|
82|7||A scalable publish/subscribe system for large mobile ad hoc networks|
82|7||Security weakness of Tsengâs fault-tolerant conference-key agreement protocol|
82|7||Fair anonymous rewarding based on electronic cash|
82|7||Design pattern recovery through visual language parsing and source code analysis|In this paper we propose an approach for recovering structural design patterns from object-oriented source code. The recovery process is organized in two phases. In the first phase, the design pattern instances are identified at a coarse-grained level by considering the design structure only and exploiting a parsing technique used for visual language recognition. Then, the identified candidate patterns are validated by a fine-grained source code analysis phase. The recognition process is supported by a tool, namely design pattern recovery environment, which allowed us to assess the retrieval effectiveness of the proposed approach on six public-domain programs and libraries. 
82|8|http://www.sciencedirect.com/science/journal/01641212/82/8|Design decisions and design rationale in software architecture|
82|8||Visualization and comparison of architecture rationale with semantic web technologies|
82|8||Quality-driven architecture development using architectural tactics|
82|8||Enriching software architecture documentation|
82|8||Managing architectural decision models with dependency relations, integrity constraints, and production rules|Software architects consider capturing and sharing architectural decisions increasingly important; many tacit dependencies exist in this architectural knowledge. Architectural decision modeling makes these dependencies explicit and serves as a foundation for knowledge management tools. In practice, however, text templates and informal rich pictures rather than models are used to capture the knowledge; a formal definition of model entities and their relations is missing in the current state of the art. In this paper, we propose such a formal definition of architectural decision models as directed acyclic graphs with several types of nodes and edges. In our models, architectural decision topic groups, issues, alternatives, and outcomes form trees of nodes connected by edges expressing containment and refinement, decomposition, and triggers dependencies, as well as logical relations such as (in)compatibility of alternatives. The formalization can be used to verify integrity constraints and to organize the decision making process; production rules and dependency patterns can be defined. A reusable architectural decision model supporting service-oriented architecture design demonstrates how we use these concepts. We also present tool support and give a quantitative evaluation. 
82|8||Selecting highly optimal architectural feature sets with Filtered Cartesian Flattening|Feature modeling is a common method used to capture the variability in a configurable application. A key challenge developers face when using a feature model is determining how to select a set of features for a variant that simultaneously satisfy a series of resource constraints. This paper presents an approximation technique for selecting highly optimal feature sets while adhering to resource limits. The paper provides the following contributions to configuring application variants from feature models: (1) we provide a polynomial time approximation algorithm for selecting a highly optimal set of features that adheres to a set of resource constraints, (2) we show how this algorithm can incorporate complex configuration constraints; and (3) we present empirical results showing that the approximation algorithm can be used to derive feature sets that are more than 90%+ optimal. 
82|8||Context-aware service engineering: A survey|Context constitutes an essential part of service behaviour, especially when interaction with end-users is involved. As observed from the literature, context handling in service engineering has been during recent years a field of intense research, which has produced several interesting approaches. In this paper, we present research efforts that attempt mainly to decouple context handling from the service logic. We enumerate all context management categories, but focus on the most appropriate for service engineering, namely source code level, model-driven and message interception, taking also into account the fact that these have not been dealt with in detail in other surveys. A representative example is used to illustrate more precisely how these approaches can be used. Finally, all three categories are compared based on a number of criteria. 
82|8||FAST: Flash-aware external sorting for mobile database systems|
82|8||On the design of an global intrusion tolerance network architecture against the internet catastrophes|
82|8||WSDL and UDDI extensions for version support in web services|Versioning is an important aspect of web service development, which has not been adequately addressed so far. In this article, we propose extensions to WSDL and UDDI to support versioning of web service interfaces at development-time and run-time. We address service-level and operation-level versioning, service endpoint mapping, and version sequencing. We also propose annotation extensions for developing versioned web services in Java. We have tested the proposed solution for versioning in two real-world environments and identified considerable improvements in service development and maintenance efficiency, improved service reuse, and simplified governance. 
82|8||A stepwise optimization algorithm of clustered streaming media servers|
82|8||A reversible information hiding scheme using leftâright and upâdown chinese character representation|Techniques for text data hiding are different from image data hiding, video data hiding and audio data hiding. To break through the difficulty of text data hiding, Sun, Lou and Huang proposed a novel Chinese text data hiding scheme called the L-R scheme. In the L-R scheme, Sun et al. embedded secrets into Chinese characters that can be divided into left and right components. This paper describes how our proposed scheme extends the component concept to incorporate the up and down components of Chinese characters rather than the left and right components only, to significantly enhance hiding capacity. In addition, this paper adds a reversible function to Sun et al.’s L-R scheme to make it possible for receivers to obtain the original cover text and use it repeatedly for later transmission of secrets after the initial hidden secrets have been extracted. Finally, the extended scheme simplifies the extracting procedure and efficiently reduces the memory required on the receiver side during the secret extracting phase by using a new comparison method. Experimental results confirm the improved functions offered by the proposed scheme. 
82|8||An assessment of systems and software engineering scholars and institutions (2002â2006)|
82|9|http://www.sciencedirect.com/science/journal/01641212/82/9|Editorial|
82|9||Resource prioritization of code optimization techniques for program synthesis of wireless sensor network applications|
82|9||Exploring alternatives for transition verification|
82|9||Issues in using model checkers for test case generation|
82|9||Adaptive random testing based on distribution metrics|Random testing (RT) is a fundamental software testing technique. Adaptive random testing (ART), an enhancement of RT, generally uses fewer test cases than RT to detect the first failure. ART generates test cases in a random manner, together with additional test case selection criteria to enforce that the executed test cases are evenly spread over the input domain. Some studies have been conducted to measure how evenly an ART algorithm can spread its test cases with respect to some distribution metrics. These studies observed that there exists a correlation between the failure detection capability and the evenness of test case distribution. Inspired by this observation, we aim to study whether failure detection capability of ART can be enhanced by using distribution metrics as criteria for the test case selection process. Our simulations and empirical results show that the newly proposed algorithms not only improve the evenness of test case distribution, but also enhance the failure detection capability of ART. 
82|9||Fault-tolerant design for wide-area Mobile IPv6 networks|
82|9||Design and implementation of MLC NAND flash-based DBMS for mobile devices|
82|9||Incremental integrity checking of UML/OCL conceptual schemas|
82|9||A comparison of issues and advantages in agile and incremental development between state of the art and an industrial case|Recent empirical studies have been conducted identifying a number of issues and advantages of incremental and agile methods. However, the majority of studies focused on one model (Extreme Programming) and small projects. To draw more general conclusions we conduct a case study in large-scale development identifying issues and advantages, and compare the results with previous empirical studies on the topic. The principle results are that (1) the case study and literature agree on the benefits while new issues arise when using agile in large-scale and (2) an empirical research framework is needed to make agile studies comparable. 
82|9||One-step t-fault diagnosis for hypermesh optical interconnection multiprocessor systems|To maintain high reliability and availability, system-level diagnosis should be considered for the multiprocessor systems. The self-diagnosis problem of hypermesh, emerging potential optical interconnection networks for multiprocessor systems, is solved in this paper. We derive that the precise one-step diagnosability of kn-hypermesh is n(k − 1). Based on the principle of cycle decomposition, a one-step t-fault diagnosis algorithm for kn-hypermesh which runs in O(knn(k − 1)) time also is described. 
82|9||An efficient three-party authenticated key exchange protocol using elliptic curve cryptography for mobile-commerce environments|
82|9||Trading decryption for speeding encryption in Rebalanced-RSA|In 1982, Quisquater and Couvreur proposed an RSA variant, called RSA-CRT, based on the Chinese Remainder Theorem to speed up RSA decryption. In 1990, Wiener suggested another RSA variant, called Rebalanced-RSA, which further speeds up RSA decryption by shifting decryption costs to encryption costs. However, this approach essentially maximizes the encryption time since the public exponent e is generally about the same order of magnitude as the RSA modulus. In this paper, we introduce two variants of Rebalanced-RSA in which the public exponent e is much smaller than the modulus, thus reducing the encryption costs, while still maintaining low decryption costs. For a 1024-bit RSA modulus, our first variant (Scheme A) offers encryption times that are at least 2.6 times faster than that in the original Rebalanced-RSA, while the second variant (Scheme B) offers encryption times at least 3 times faster. In both variants, the decrease in encryption costs is obtained at the expense of slightly increased decryption costs and increased key generation costs. Thus, the variants proposed here are best suited for applications which require low costs in encryption and decryption. 
82|9||A survey on security in JXTA applications|
82|9||Multimedia Internet Rekeying for secure session mobility in ubiquitous mobile networks|Session mobility is one of new critical issues in the ubiquitous mobile networking environment. Session mobility provides a user changing its ongoing multimedia session, e.g., Voice-over-Internet Protocol (VoIP), from the currently using device to another by adapting user’s demand. In session Initial Protocol (SIP)-based multimedia services supporting session mobility, SIP serves as a signaling control protocol to negotiate session control, whereas media is transmitted using Real-time Transport Protocol (RTP). For securing multimedia sessions, Multimedia Internet Keying (MIKEY) is embedded in SIP signaling to negotiate security parameters for Secure RTP (SRTP), whereas SRTP is used to protect media stream. Since session mobility allows an ongoing multimedia session to be transferred from one device to another, a new security problem is raised, i.e., sensitive parameters may remain in the previous device when the ongoing multimedia session has been transferred to the current device. Unfortunately, current MIKEY cannot bear the aforementioned security problem in session mobility. Therefore, we propose Multimedia Internet Rekeying (MIRKEY) for session mobility in the ubiquitous mobile networking environment. Although MIKEY can be executed again to carry out the rekeying of the session key and Crypto Session bundle (CSB) update, the sensitive parameters still remain in previous devices. MIRKEY contains a SBK to bind the participated user and multimedia session. Besides, SBK can persist in rekeying based on the key chain whenever a multimedia session is transferred to other devices. As a result, SBK is operative only in the specific device. As a result, MIRKEY can solve the newly raised security problem in session mobility. Furthermore, we verify MIRKEY using Burrows–Abadi–Needham (BAN) logic and realize it in the implemented ubiquitous multimedia service platform (UMSP). 
82|9||Understanding developer and manager perceptions of function points and source lines of code|Although function points (FP) are considered superior to source lines of code (SLOC) for estimating software size and monitoring developer productivity, practitioners still commonly use SLOC. One reason for this is that individuals who fill different roles on a development team, such as managers and developers, may perceive the benefits of FP differently. We conducted a survey to determine whether a perception gap exists between managers and developers for FP and SLOC across several desirable properties of software measures. Results suggest managers and developers perceive the benefits of FP differently and indicate that developers better understand the benefits of using FP than managers. 
82|9||Empirical analysis of biometric technology adoption and acceptance in Botswana|There has been a slight surge in the study of technology adoption in developing countries. However, little attention has been paid to the adoption of biometric security systems. This paper reports a study that analyzed the adoption of biometric technology in a developing country from an institutional point of view. The results show that job positions (managerial and operational) could influence perceptions of innovation characteristics (especially ease of use and usefulness) in the decision to adopt biometrics. However, the unified organizational analyses indicate that ease of use, communication, size and type of organizations have significant impacts on the decision to adopt biometrics. 
83|1|http://www.sciencedirect.com/science/journal/01641212/83/1|Editorial for the JSS Top Scholar Special Issue|
83|1||A systematic and comprehensive investigation of methods to build and evaluate fault prediction models|This paper describes a study performed in an industrial setting that attempts to build predictive models to identify parts of a Java system with a high fault probability. The system under consideration is constantly evolving as several releases a year are shipped to customers. Developers usually have limited resources for their testing and would like to devote extra resources to faulty system parts. The main research focus of this paper is to systematically assess three aspects on how to build and evaluate fault-proneness models in the context of this large Java legacy system development project: (1) compare many data mining and machine learning techniques to build fault-proneness models, (2) assess the impact of using different metric sets such as source code structural measures and change/fault history (process measures), and (3) compare several alternative ways of assessing the performance of the models, in terms of (i) confusion matrix criteria such as accuracy and precision/recall, (ii) ranking ability, using the receiver operating characteristic area (ROC), and (iii) our proposed cost-effectiveness measure (CE).The results of the study indicate that the choice of fault-proneness modeling technique has limited impact on the resulting classification accuracy or cost-effectiveness. There is however large differences between the individual metric sets in terms of cost-effectiveness, and although the process measures are among the most expensive ones to collect, including them as candidate measures significantly improves the prediction models compared with models that only include structural measures and/or their deltas between releases – both in terms of ROC area and in terms of CE. Further, we observe that what is considered the best model is highly dependent on the criteria that are used to evaluate and compare the models. And the regular confusion matrix criteria, although popular, are not clearly related to the problem at hand, namely the cost-effectiveness of using fault-proneness prediction models to focus verification efforts to deliver software with less faults at less cost. 
83|1||Multi-faceted quality and defect measurement for web software and source contents|In this paper, we examine external failures and internal faults traceable to web software and source contents. We develop related defect and quality measurements based on different perspectives of customers, users, information or service hosts, maintainers, developers, integrators, and managers. These measurements can help web information and service providers with their quality assessment and improvement activities to meet the quality expectations of their customers and users. The different usages of our measurement framework by different stakeholders of web sites and web applications are also outlined and discussed. The data sources include existing web server logs and statistics reports, defect repositories from web application development and maintenance activities, and source files. We applied our approach to four diverse websites: one educational website, one open source software project website, one online catalog showroom for a small company, and one e-Commerce website for a large company. The results demonstrated the viability and effectiveness of our approach. 
83|1||The effects of request formats on judgment-based effort estimation|
83|1||Whatâs up with software metrics? â A preliminary mapping study|
83|1||Software project management anti-patterns|This paper explores the area of bad practices, namely anti-patterns, and their consequences in software project management (SPM). The paper surveys the multitude of anti-patterns that have been reported and documented up to now and stresses the need for tools to formally represent SPM anti-patterns, proposing specific formalisms for such purpose, namely Bayesian Belief Networks, Ontologies and Social Networks. It is also explained how the Web can provide an opportunity for capturing, storing, disseminating and ultimately avoiding SPM anti-patterns. As a consequence, anti-patterns may provide an excellent tool for educating active and future software managers. Finally, conclusions and future research trends are given. 
83|1||Adaptive Random Testing: The ART of test case diversity|Random testing is not only a useful testing technique in itself, but also plays a core role in many other testing methods. Hence, any significant improvement to random testing has an impact throughout the software testing community. Recently, Adaptive Random Testing (ART) was proposed as an effective alternative to random testing. This paper presents a synthesis of the most important research results related to ART. In the course of our research and through further reflection, we have realised how the techniques and concepts of ART can be applied in a much broader context, which we present here. We believe such ideas can be applied in a variety of areas of software testing, and even beyond software testing. Amongst these ideas, we particularly note the fundamental role of diversity in test case selection strategies. We hope this paper serves to provoke further discussions and investigations of these ideas. 
83|1||From integration to composition: On the impact of software product lines, global development and ecosystems|Three trends accelerate the increase in complexity of large-scale software development, i.e. software product lines, global development and software ecosystems. For the case study companies we studied, these trends caused several problems, which are organized around architecture, process and organization, and the problems are related to the efficiency and effectiveness of software development as these companies used too integration-centric approaches. We present five approaches to software development, organized from integration-centric to composition-oriented and describe the areas of applicability. 
83|1||An extended XACML model to ensure secure information access for web services|
83|1||An intelligent query processing for distributed ontologies|
83|1||Assessing the impact of global variables on program dependence and dependence clusters|
83|1||McTorrent: Using multiple communication channels for efficient bulk data dissemination in wireless sensor networks|This paper presents McTorrent, a reliable bulk data dissemination protocol for sensor networks. The protocol is designed to take advantage of multiple radio channels to reduce packet collisions and improve the latency of large object dissemination. We evaluated the performance of McTorrent via detailed simulations and experiments based upon an implementation on the TinyOS platform. Our results show that in comparison to Deluge, the de facto network reprogramming protocol for TinyOS, McTorrent significantly reduces the number of packet transmissions and the amount of time required to propagate a large data object through a sensor network. 
83|1||Multi-layer bus minimization for SoC|
83|1||An empirical investigation of architectural prototyping|
83|1||Semantic oriented ontology cohesion metrics for ontology-based systems|
83|1||CCA2 secure (hierarchical) identity-based parallel key-insulated encryption without random oracles|
83|1||DoS-resistant ID-based password authentication scheme using smart cards|
83|10|http://www.sciencedirect.com/science/journal/01641212/83/10|Varied PVD + LSB evading detection programs to spatial domain in data embedding systems|Image steganographic schemes based on the least-significant-bit (LSB) replacement method own the character of high capacity and good quality, but they are detected easily by some programs. Pixel-value differencing (PVD) approaches are one kind of methods to pass some program detections, but PVD approaches usually provide lower capacities and larger distortion. Accordingly, the combined method of PVD and LSB replacement was proposed to raise the capacity and the quality of PVD approaches over the past literatures. In this paper, not only we contribute a new exploration in spatial domain to benefiting both the capacity and the fidelity on the basis of varied LSB + PVD approaches, but also the risk of the RS-steganalysis detection program is evaded. Furthermore, proof works are conducted to proclaim the correctness of the general LSB + PVD method. Following up, the varied LSB + PVD approach is therefore applied to our scheme. 
83|10||Performance Evaluation of Fast Handover in Mobile IPv6 Based on Link-Layer Information|
83|10||A model checker for WS-CDL|
83|10||Using Scrum to guide the execution of software process improvement in small organizations|For software process improvement – SPI – there are few small organizations using models that guide the management and deployment of their improvement initiatives. This is largely because a lot of these models do not consider the special characteristics of small businesses, nor the appropriate strategies for deploying an SPI initiative in this type of organization. It should also be noted that the models which direct improvement implementation for small settings do not present an explicit process with which to organize and guide the internal work of the employees involved in the implementation of the improvement opportunities. In this paper we propose a lightweight process, which takes into account appropriate strategies for this type of organization. Our proposal, known as a “Lightweight process to incorporate improvements”, uses the philosophy of the Scrum agile method, aiming to give detailed guidelines for supporting the management and performance of the incorporation of improvement opportunities within processes and their putting into practice in small companies. We have applied the proposed process in two small companies by means of the case study research method, and from the initial results, we have observed that it is indeed suitable for small businesses. 
83|10||A novel global harmony search algorithm for task assignment problem|The objective of task assignment problem (TAP) is to minimize the sum of interprocessor communication and task processing costs for a distributed system which subjects to several resource constraints. We use a novel global harmony search algorithm (NGHS) to solve this problem, and the NGHS algorithm has demonstrated higher efficiency than the improved harmony search algorithm (IHS) on finding the near optimal task assignment. We also devise a new method called normalized penalty function method to tradeo® the costs and the constraints. A large number of experiments show that our algorithm performs well on finding the near optimal task assignment, and it is a viable approach for the task assignment problem. 
83|10||A novel DRM framework for peer-to-peer music content delivery|
83|10||Survey of data management and analysis in disaster situations|The area of disaster management receives increasing attention from multiple disciplines of research. A key role of computer scientists has been in devising ways to manage and analyze the data produced in disaster management situations.In this paper we make an effort to survey and organize the current knowledge in the management and analysis of data in disaster situations, as well as present the challenges and future research directions. Our findings come as a result of a thorough bibliography survey as well as our hands-on experiences from building a Business Continuity Information Network (BCIN) with the collaboration with the Miami-Dade county emergency management office. We organize our findings across the following Computer Science disciplines: data integration and ingestion, information extraction, information retrieval, information filtering, data mining and decision support. We conclude by presenting specific research directions. 
83|10||Image watermarking with a directed periodic pattern to embed multibit messages resilient to print-scan and compound attacks|
83|10||Software development team flexibility antecedents|
83|10||How do architecture patterns and tactics interact? A model and annotation|Software architecture designers inevitably work with both architecture patterns and tactics. Architecture patterns describe the high-level structure and behavior of software systems as the solution to multiple system requirements, whereas tactics are design decisions that improve individual quality attribute concerns. Tactics that are implemented in existing architectures can have significant impact on the architecture patterns in the system. Similarly, tactics that are selected during initial architecture design significantly impact the architecture of the system to be designed: which patterns to use, and how they must be changed to accommodate the tactics. However, little is understood about how patterns and tactics interact. In this paper, we develop a model for the interaction of patterns and tactics that enables software architects to annotate architecture diagrams with information about the tactics used and their impact on the overall structure. This model is based on our in-depth analysis of the types of interactions involved, and we show several examples of how the model can be used to annotate different kinds of architecture diagrams. We illustrate the model and annotation by showing examples taken from real systems, and describe how the annotation was used in architecture reviews. Tactics and patterns are known architectural concepts; this work provides more specific and in-depth understanding of how they interact. Its other key contribution is that it explores the larger problem of understanding the relation between strategic decisions and how they need to be tailored in light of more tactical decisions. 
83|10||A comprehensive engineering framework for guaranteeing component compatibility|
83|10||A software integration approach for designing and assessing dependable embedded systems|
83|10||Multi-party covert communication with steganography and quantum secret sharing|
83|10||Using quad smoothness to efficiently control capacityâdistortion of reversible data hiding|One of the main uses of data hiding is to protect secret messages being transmitted on the Internet. Reversible data hiding can fully recover the original host image after extracting the secret message. It is especially suitable for applications where, after extracting the secret message, the quality of the recovered host image cannot be compromised, such as for medical or military image data.Many difference-expansion-based (DE-based) reversible data hiding methods have made use of a threshold value to control the stego-image's quality. Usually repeated trial and error is required to find a relatively good threshold with acceptable capacity–distortion behavior. This paper introduces a scheme that does not require a threshold value, such as is used in Alattar's quad-based reversible data hiding. It applies a prediction of quad of quads smoothness to determine the embedding sequence. The proposed scheme is shown to perform better than other DE-based schemes. Results showed that it has the ability of maintaining embedding quality at all capacity levels, especially when the embedding capacity is at low to medium levels. 
83|10||Security and privacy issues in the Portable Document Format|The Portable Document Format (PDF) was developed by Adobe in the early nineties and today it is the de-facto standard for electronic document exchange. It allows reliable reproductions of published materials on any platform and it is used by many governmental and educational institutions, as well as companies and individuals. PDF documents are also credited with being more secure than other document formats such as Microsoft Compound Document File Format or Rich Text Format.This paper investigates the Portable Document Format and shows that it is not immune from some privacy related issues that affect other popular document formats. From a PDF document, it is possible to retrieve any text or object previously deleted or modified, extract user information and perform some actions that may be used to violate user privacy. There are several applications of such an issue. One of them is relevant to the scientific community and it pertains to the ability to overcome the blind review process of a paper, revealing information related to the anonymous referee (e.g., the IP address of the referee). 
83|10||A Superscalar software architecture model for Multi-Core Processors (MCPs)|
83|10||Automated assembly of Internet-scale software systems involving autonomous agents|
83|10||LESSON: A system for lecture notes searching and sharing over Internet|
83|10||Embedding capacity raising in reversible data hiding based on prediction of difference expansion|Most of the proposed methods of reversible data hiding based on difference expansion require location maps to recover cover images. Although the location map can be compressed by a lossless compression algorithm, this lowers embedding capacity and increases computational cost during the procedures of embedding and extracting. The study presents an adaptive reversible data scheme based on the prediction of difference expansion. Since each cover pixel generally resembles its surrounding pixels, most of the difference values between the cover pixels and their corresponding predictive pixels are small; therefore, the proposed scheme gains from embedding capacity by taking full advantage of the large quantities of smaller difference values where secret data can be embedded. The proposed scheme offers several advantages, namely, (1) the location map is no more required, (2) the embedding capacity can be adjusted depending on the practical applications, and (3) the high embedding capacity with minimal visual distortion can be achieved. Moreover, the experimental results demonstrate that the proposed scheme yields high embedding capacity by comparing the related schemes that are proposed recently. 
83|10||An ID-based aggregate signature scheme with constant pairing computations|
83|10||Coordination implications of software architecture in a global software development project|
83|10||Towards a general purpose architecture for UI generation|
83|10||Actual vs. perceived effect of software engineering practices in the Italian industry|
83|10||Efficient utilization of elliptic curve cryptosystem for hierarchical access control|
83|10||Online discovery of Heart Rate Variability patterns in mobile healthcare services|Recent years, advances in day-to-day wearable sensors have led to the development of low powered physiological sensor platforms, which can be integrated in body area networks, a new enabling technology for real-time health monitoring. The bottleneck in health state awareness is the algorithm that has to interpret the sensor data. Nowadays Coronary Heart Disease (CHD) is still the leading cause of death. Many classification techniques such as decision tree and neural networks proposed for an early detection of individual at risk for CHD are not able to continuously detect heart state based on sensor data stream. In this study, we propose an online three-layer neural network to recognize Heart Rate Variability (HRV) patterns related to CHD risk in consideration of daily activities. ECG sensor data is preprocessed using Poincaré plot encoding. Incremental learning is utilized to train the network with new data without forgetting the previously learned patterns. The algorithm is named Poincaré-based HRV patterns discovering Incremental Artificial neural Network (PHIAN). When a sample is presented, the nodes in the hidden layer of PHIAN compete for determining the node with the highest similarity to the input. Error variables associated with the neuron units are used as criteria for new node insertion in hopes of allowing the network to learn new patterns and reducing classification error. However, the node insertion has to be stopped in the overlapping decision areas. We suppose that the overlaps between classes have lower probability than the centric part of the classes. Therefore, after a period of learning we remove the nodes with no neighbor. Plus, the error probability density is taken into account instead of input probability density. Finally, the predictive capability of PHIAN is compared with three previous classification models, namely Self-Organizing Map (SOM), Growing Neural Gas (GNG), and Multilayer Perceptron (MLP) in terms of classification error and network structure. The results show that PHIAN outperforms the existing techniques. Our proposed model can be efficiently applied to early detection of abnormal conditions and prevent the abnormal becoming serious. 
83|10||A multicriteria approach for risks assessment in ERP maintenance|
83|10||The relation between EA effectiveness and stakeholder satisfaction|Enterprise Architecture (EA) is increasingly being used by large organizations to get a grip on the complexity of their business processes, information systems and technical infrastructure. Although seen as an important instrument to help solve major organizational problems, effectively applying EA seems no easy task. Active participation of EA stakeholders is one of the main critical success factors for EA. This participation depends on the degree in which EA helps stakeholders achieve their individual goals. A highly related topic is effectiveness of EA, the degree in which EA helps to achieve the collective goals of the organization. In this article we present our work regarding EA stakeholder satisfaction and EA effectiveness, and compare these two topics. We found that, regarding EA, the individual goals of stakeholders map quite well onto the collective goals of the organization. In a case study we conducted, we found that the organization is primarily concerned with the final results of EA, while individual stakeholders also worry about the way the architects operate. 
83|10||Generating blogs out of product catalogues: An MDE approach|
83|10||Dependency-aware maintenance for highly available service-oriented grid|
83|10||Modular analysis and modelling of risk scenarios with dependencies|
83|11|http://www.sciencedirect.com/science/journal/01641212/83/11|Interplay between usability and software development|
83|11||Work-domain knowledge in usability evaluation: Experiences with Cooperative Usability Testing|
83|11||Exploring the benefits of the combination of a software architecture analysis and a usability evaluation of a mobile application|Designing easy to use mobile applications is a difficult task. In order to optimize the development of a usable mobile application, it is necessary to consider the mobile usage context for the design and the evaluation of the user–system interaction of a mobile application. In our research we designed a method that aligns the inspection method “Software ArchitecTure analysis of Usability Requirements realizatioN” SATURN and a mobile usability evaluation in the form of a user test. We propose to use mobile context factors and thus requirements as a common basis for both inspection and user test. After conducting both analysis and user test, the results described as usability problems are mapped and discussed. The mobile context factors identified define and describe the usage context of a mobile application. We exemplify and apply our approach in a case study. This allows us to show how our method can be used to identify more usability problems than with each method separately. Additionally, we could confirm the validity and identified the severity of usability problems found by both methods. Our work presents how a combination of both methods allows to address usability issues in a more holistic way. We argue that the increased quantity and quality of results can lead to a reduction of the number of iterations required in early stages of an iterative software development process. 
83|11||Measuring effectiveness of HCI integration in software development processes|Integrating human–computer interaction (HCI) activities in software engineering (SE) processes is an often-expressed desire. Two metrics to demonstrate the impact of integrating HCI activities in SE processes are proposed. Usability Goals Achievement Metric (UGAM) is a product metric that measures the extent to which the design of a product achieves its user-experience goals. Index of Integration (IoI) is a process metric that measures the extent of integration of the HCI activities in the SE process. Both the metrics have an organizational perspective and can be applied to a wide range of products and projects. An attempt has been made to keep the metrics easy to use in the industrial context. While the two metrics were proposed mainly to establish a correlation between the two and thereby demonstrate the effectiveness of integration of HCI in SE processes, several other applications seem likely. The two metrics were evaluated in three independent studies: a classroom-based evaluation with two groups of students, a qualitative feedback from three industry projects, and a quantitative evaluation using 61 industry projects. The metrics were found to be useful, easy to use, and helpful in making the process more systematic. Our studies showed that the two metrics correlate well with each other and that IoI is a good predictor of UGAM. Regression analysis showed that IoI has a somewhat greater effect on UGAM in projects that use the agile process model than the waterfall process and in the projects that are executed as a contracted software development service than in the projects in product companies. UGAM also correlated well with the traditional usability evaluations. 
83|11||SPI success factors within product usability evaluation|
83|11||An encoding method for both image compression and data lossless information hiding|
83|11||CLPL: Providing software infrastructure for the systematic and effective construction of complex collaborative learning systems|
83|11||Monetary pricing of software development risks: A method and empirical illustration|
83|11||Composition of architectural models: Empirical analysis and language support|
83|11||Software Process Improvement as organizational change: A metaphorical analysis of the literature|Software Process Improvement (SPI) typically involves rather complex organizational changes. Acknowledging that managers can approach these changes in quite different ways, this paper addresses the following question: what perspectives do the research literature offer on SPI as organizational change and how is this knowledge presented and published? To answer this question, we analyzed SPI research publications with a main emphasis on organizational change using Gareth Morgan's organizational metaphors (1996) as analytical lenses. In addition, we characterized each article along the following dimensions: knowledge orientation (normative versus descriptive), theoretical emphasis (high versus low), main audience (practitioner versus academic), geographical origin (Scandinavia, the Americas, Europe, or the Asia-Pacific), and publication level (high versus low ranked journal). The review demonstrates that the literature as a whole is firmly grounded in both theory and practice, it appropriately targets both practitioner and academic audiences, and Scandinavian and American researchers are the main contributors.However, the distribution of articles across Morgan's metaphors is uneven and reveals knowledge gaps that present new avenues for research. The current literature offers important insights into organizational change in SPI when viewed through machine, organism, flux and transformation, and brain metaphors. Practitioners may use these articles to guide SPI initiatives. In contrast, the impact of culture, dominance, psychic prison, and politics in SPI has only received scant attention. We argue that these perspectives could offer important additional insights into the challenges involved in managing SPI. Researchers are therefore advised to engage in new SPI research based on one or more of these perspectives. Overall, the paper offers research directions and management lessons, and it provides a roadmap to help identify insights and specific articles related to SPI as organizational change. 
83|11||A new real time disk-scheduling method based on GSR algorithm|
83|11||Task allocation for maximizing reliability of distributed computing systems using honeybee mating optimization|This paper deals with the problem of task allocation (i.e., to which processor should each task of an application be assigned) in heterogeneous distributed computing systems with the goal of maximizing the system reliability. The problem of finding an optimal task allocation is known to be NP-hard in the strong sense. We propose a new swarm intelligence technique based on the honeybee mating optimization (HBMO) algorithm for this problem. The HBMO based approach combines the power of simulated annealing, genetic algorithms with a fast problem specific local search heuristic to find the best possible solution within a reasonable computation time. We study the performance of the algorithm over a wide range of parameters such as the number of tasks, the number of processors, the ratio of average communication time to average computation time, and task interaction density of applications. The effectiveness and efficiency of our algorithm are demonstrated by comparing it with recently proposed task allocation algorithms for maximizing system reliability available in the literature. 
83|11||Software engineering projects may fail before they are started: Post-mortem analysis of five cancelled projects|
83|11||An ant swarm-inspired energy-aware routing protocol for wireless ad-hoc networks|Primitive routing protocols for ad-hoc networks are “power hungry” and can therefore consume considerable amount of the limited amount of battery power resident in the nodes. Thus, routing in ad-hoc networks is very much energy-constrained. Continuous drainage of energy degrades battery performance as well. If a battery is allowed to intermittently remain in an idle state, it recovers some of its lost charge due to the charge recovery effect, which, in turn, results in prolonged battery life.In this paper, we use the ideas of naturally occurring ants’ foraging behavior (Dorigo and Stuetzle, 2004) [1] and based on those ideas, we design an energy-aware routing protocol, which not only incorporates the effect of power consumption in routing a packet, but also exploits the multi-path transmission properties of ant swarms and, hence, increases the battery life of a node. The efficiency of the protocol with respect to some of the existing ones has been established through simulations. It has been observed that the energy consumed in the network, the energy per packet in the case of EAAR are 60% less compared to MMBCR and the packets lost is only around 12% of what we have in AODV, in mobility scenarios. 
83|11||A web personalizing technique using adaptive data structures: The case of bursts in web visits|
83|11||Software architecture awareness in long-term software product evolution|Software architecture has been established in software engineering for almost 40 years. When developing and evolving software products, architecture is expected to be even more relevant compared to contract development. However, the research results seem not to have influenced the development practice around software products very much. The architecture often only exists implicitly in discussions that accompany the development. Nonetheless many of the software products have been used for over 10, or even 20 years. How do development teams manage to accommodate changing needs and at the same time maintain the quality of the product? In order to answer this question, grounded theory study based on 15 semi-structured interviews was conducted in order to find out about the wide spectrum of architecture practices in software product developing organisations. Our results indicate that a chief architect or central developer acts as a ‘walking architecture’ devising changes and discussing local designs while at the same time updating his own knowledge about problematic aspects that need to be addressed. Architecture documentation and representations might not be used, especially if they replace the feedback from on-going developments into the ‘architecturing’ practices. Referring to results from Computer Supported Cooperative Work, we discuss how explicating the existing architecture needs to be complemented by social protocols to support the communication and knowledge sharing processes of the ‘walking architecture’. 
83|11||HSP: A solution against heap sprays|
83|11||Vertical partitioning for flash and HDD database systems|
83|11||A replicated survey of software testing practices in the Canadian province of Alberta: What has changed from 2004 to 2009?|
83|11||Perturbation-based user-input-validation testing of web applications|User-input-validation (UIV) is the first barricade that protects web applications from application-level attacks. Most UIV test tools cannot detect semantics-related vulnerabilities in validators, such as filling a five-digit number to a field that accepts a year. To address this issue, we propose a new approach to generate test inputs for UIV based on the analysis of client-side information. In particular, we use input-field information to generate valid inputs, and then perturb valid inputs to generate invalid test inputs. We conducted an empirical study to evaluate our approach. The empirical result shows that, in comparison to existing vulnerability scanners, our approach is more effective than existing vulnerability scanners in finding semantics-related vulnerabilities of UIV for web applications. 
83|11||A replicated and refined empirical study of the use of friends in C++ software|The friend mechanism is widely used in C++ software even though the potential benefits of its use are disputed and little is known about when, where and why it is employed in practice. Furthermore, there is limited empirical analysis of its impact in object-oriented software, with only one study (Counsell and Newson, 2000) reported at journal level.This paper aims to add to the empirical evidence of friendship’s impact by replicating Counsell and Newson (2000)’s original study. The study’s design is refined to improve the construct validity of the evaluation and a larger cohort of systems is used to improve the generalisability of the results. The findings suggest that classes involved in friendship are coupling hotspots and that there is no link between inheritance and friendship, contrary to the findings presented in Counsell and Newson (2000). The findings also suggest that the use of friends in a class is independent of the number of hidden members in a class. 
83|11||Seeing eye to eye? An exploratory study of free open source software usersâ perceptions|This study develops a typology that permits the classification of free open source software (FOSS) users into market segments. Based on the typology the nature and extent of perception differences between core FOSS users and FOSS users in other market segments is examined. Significant perception differences are observed between users in different market segments. Consequently, the potential barriers to FOSS adoption are identified and recommendations that may drive greater FOSS adoption are proposed. 
83|11||Provably secure authenticated key exchange protocol under the CDH assumption|
83|11||Bad news reporting on troubled IT projects: Reassessing the mediating role of responsibility in the basic whistleblowing model|
83|11||A pattern-based prediction: An empirical approach to predict end-to-end network latency|Understanding latency in network-based applications has received considerable attention to provide consistent and acceptable levels of services. This paper presents an empirical approach, a pattern-based prediction method, to predict end-to-end network latency. The key idea of the approach is to utilize past history of latency and their variation patterns in latency predictions. After some preliminary study on simple numerical prediction models we examine the effectiveness of the proposed method with real latency data and various definitions of network stability. Our results show that the pattern-based method outperforms any single numerical model obtaining an overall prediction accuracy of 86.2%. 
83|11||Development of Java based RFID application programmable interface for heterogeneous RFID system|Developing RFID based applications is a painstakingly difficult endeavor. The difficulties include non-standard software and hardware peripherals from vendors, interoperability problems between different operating systems as well as lack of expertise in terms of low-level programming for RFID (i.e. steep learning curve). In order to address these difficulties, a reusable RFIDTM API (RFID Tracking & Monitoring Application Programmable Interface) for heterogeneous RFID system has been designed and implemented. The API has been successfully employed in a number of application prototypes including tracking of inventories as well as human/object tracking and tagging. Here, the module has been tested on a number of different types and configuration of active and passive readers including that LF and UHF Readers. 
83|11||Adaptive ridge regression system for software cost estimating on multi-collinear datasets|
83|11||A property based specification formalism classification|
83|11||Disciplined and free-spirited: âTime-out behaviourâ at the Agile conference|
83|12|http://www.sciencedirect.com/science/journal/01641212/83/12|TAIC-PART 2009 â Testing: Academic & Industrial Conference â Practice And Research Techniques: Special Section Editorial|
83|12||Two case studies in grammar-based test generation|
83|12||An empirical investigation into branch coverage for C programs using CUTE and AUSTIN|Automated test data generation has remained a topic of considerable interest for several decades because it lies at the heart of attempts to automate the process of Software Testing. This paper reports the results of an empirical study using the dynamic symbolic-execution tool, CUTE, and a search based tool, AUSTIN on five non-trivial open source applications. The aim is to provide practitioners with an assessment of what can be achieved by existing techniques with little or no specialist knowledge and to provide researchers with baseline data against which to measure subsequent work. To achieve this, each tool is applied ‘as is’, with neither additional tuning nor supporting harnesses and with no adjustments applied to the subject programs under test. The mere fact that these tools can be applied ‘out of the box’ in this manner reflects the growing maturity of Automated test data generation. However, as might be expected, the study reveals opportunities for improvement and suggests ways to hybridize these two approaches that have hitherto been developed entirely independently. 
83|12||Bottom-up reuse for multi-level testing|
83|12||Efficient multi-objective higher order mutation testing with genetic programming|It is said 90% of faults that survive manufacturer’s testing procedures are complex. That is, the corresponding bug fix contains multiple changes. Higher order mutation testing is used to study defect interactions and their impact on software testing for fault finding. We adopt a multi-objective Pareto optimal approach using Monte Carlo sampling, genetic algorithms and genetic programming to search for higher order mutants which are both hard-to-kill and realistic. The space of complex faults (higher order mutants) is much larger than that of traditional first order mutations which correspond to simple faults, nevertheless search based approaches make this scalable. The problems of non-determinism and efficiency are overcome. Easy to detect faults may become harder to detect when they interact and impossible to detect single faults may be brought to light when code contains two such faults. We use strong typing and BNF grammars in search based mutation testing to find examples of both in ancient heavily optimised every day C code. 
83|12||A robust and flexible digital rights management system for home networks|
83|12||An exploratory study of architectural effects on requirements decisions|
83|12||Component Point: A system-level size measure for Component-Based Software Systems|
83|12||Accelerated collection of sensor data by mobility-enabled topology ranks|We study the problem of fast and energy-efficient data collection of sensory data using a mobile sink, in wireless sensor networks in which both the sensors and the sink move. Motivated by relevant applications, we focus on dynamic sensory mobility and heterogeneous sensor placement. Our approach basically suggests to exploit the sensor motion to adaptively propagate information based on local conditions (such as high placement concentrations), so that the sink gradually “learns” the network and accordingly optimizes its motion. Compared to relevant solutions in the state of the art (such as the blind random walk, biased walks, and even optimized deterministic sink mobility), our method significantly reduces latency (the improvement ranges from 40% for uniform placements, to 800% for heterogeneous ones), while also improving the success rate and keeping the energy dissipation at very satisfactory levels. 
83|12||Code analyzer for an online course management system|The online course management system (OCMS) assists online instruction in various aspects, including testing, course discussion, assignment submission, and assignment grading. This paper proposes a plagiarism detection system whose design is integrated with an OCMS. Online assignment submission is prone to easy plagiarism, which can seriously influence the quality of learning. In the past, plagiarism was detected manually, making it very time-consuming. This research thus focuses on developing a system involving code standardization, textual analysis, structural analysis, and variable analysis for evaluating and comparing programming codes. An agent system serves as a daemon to analyze the program codes for OCMS. For textual analysis, the Fingerprinting Algorithm was used for text comparison. Structurally, a formal algebraic expression and a dynamic control structure tree (DCS Tree) were utilized to rebuild and evaluate the program structure. For variables, not only the relevant information for each variable was recorded, but also the programming structure was analyzed where the variables are positioned. By applying a similarity measuring method, a similarity value was produced for each program in the three aspects mentioned above. This research implements an Online Detection Plagiarism System (ODPS) providing a web-based user interface. This system can be applied independently for assignment analysis of Java programs. After three comparison experiments with other researches, the results demonstrated the ODPS has many advantages and good performance. Meanwhile, a combined approach is proven that it is better than a single approach for source codes of various styles. 
83|12||Novel segmentation algorithm in segmenting medical images|
83|12||Decision support for moving from a single product to a product portfolio in evolving software systems|
83|12||Enhancing middleware support for architecture-based development through compositional weaving of styles|
83|12||A perfect maze based steganographic method|
83|12||Design and realization of ad-hoc VoIP with embedded p-SIP server|
83|12||Two robust remote user authentication protocols using smart cards|With the rapid growth of electronic commerce and enormous demand from variants of Internet based applications, strong privacy protection and robust system security have become essential requirements for an authentication scheme or universal access control mechanism. In order to reduce implementation complexity and achieve computation efficiency, design issues for efficient and secure password based remote user authentication scheme have been extensively investigated by research community in these two decades. Recently, two well-designed password based authentication schemes using smart cards are introduced by Hsiang and Shih (2009) and Wang et al. (2009), respectively. Hsiang et al. proposed a static ID based authentication protocol and Wang et al. presented a dynamic ID based authentication scheme. The authors of both schemes claimed that their protocol delivers important security features and system functionalities, such as mutual authentication, data security, no verification table implementation, freedom on password selection, resistance against ID-theft attack, replay attack and insider attack, as well as computation efficiency. However, these two schemes still have much space for security enhancement. In this paper, we first demonstrate a series of vulnerabilities on these two schemes. Then, two enhanced protocols with corresponding remedies are proposed to eliminate all identified security flaws in both schemes. 
83|12||Consistent query answers from virtually integrated XML data|
83|12||Temperature-aware task scheduling algorithm for soft real-time multi-core systems|
83|12||Domain-specific language modelling with UML profiles by decoupling abstract and concrete syntaxes|
83|12||Fault coverage of Constrained Random Test Selection for access control: A formal analysis|
83|12||A uniform random test data generator for path testing|Path-oriented Random Testing (PRT) aims at generating a uniformly spread out sequence of random test data that execute a single control flow path within a program. The main challenge of PRT lies in its ability to build efficiently such a test suite in order to minimize the number of rejects (test data that execute another control flow path). We address this problem with an original divide-and-conquer approach based on constraint reasoning over finite domains, a well-recognized Constraint Programming technique. Our approach first derives path conditions by using backward symbolic execution and computes a tight over-approximation of their associated subdomain by using constraint propagation and constraint refutation. Second, a uniform random test data generator is extracted from this approximated subdomain. We implemented this approach and got experimental results that show the practical benefits of PRT based on constraint reasoning. On average, we got a two-order magnitude CPU time improvement over standard Random Testing on a set of paths extracted from classical benchmark programs. 
83|12||Effective processing of continuous group-by aggregate queries in sensor networks|Aggregate queries are one of the most important queries in sensor networks. Especially, group-by aggregate queries can be used in various sensor network applications such as tracking, monitoring, and event detection. However, most research has focused on aggregate queries without a group-by clause.In this paper, we propose a framework, called the G-Framework, to effectively process continuous group-by aggregate queries in the environment where sensors are grouped by the geographical location. In the G-Framework, we can perform energy effective data aggregate processing and dissemination using two-dimensional Haar wavelets. Also, to process continuous group-by aggregate queries with a HAVING clause, we divide data collection into two phases. We send only non-filtered data in the first collection phase, and send data requested by the leader node in the second collection phase. Experimental results show that the G-Framework can process continuous group-by aggregate queries effectively in terms of energy consumption. 
83|12||Unreliable transport protocol using congestion control for high-speed networks|Currently there is no control for the real-time traffic of multimedia applications using UDP (User Datagram Protocol) in high-speed networks. Therefore, although a number of high-speed TCP (Transmission Control Protocol) protocols have been developed for gigabit-speed (or faster) links, the real-time traffic could also congest the network and result in unfairness and throughput degradation of TCP traffic. In this paper, a new unreliable transport protocol, FAST DCCP, is presented for the real-time traffic in high-speed networks. FAST DCCP is based on the DCCP protocol and adopts the FAST scheme to realize congestion control. Some modifications have been made to the mechanisms inherited from DCCP so as to let the proposed protocol can efficiently operate under a large size window. In addition, an enhanced protocol, EEFAST DCCP, using the measurements of one-way delay to dynamically adjust the window size is also proposed to improve the throughput of FAST DCCP with the effect of reverse traffic. Simulation results show that FAST DCCP not only can satisfy the requirements of real-time data delivery, but also perform well in bandwidth utilization and fairness in high-speed networks. Meanwhile, EEFAST DCCP is able to effectively conquer the throughput degradation caused by the reverse traffic. 
83|12||A local variance-controlled reversible data hiding method using prediction and histogram-shifting|The stego image quality produced by the histogram-shifting based reversible data hiding technique is high; however, it often suffers from lower embedding capacity compared to other types of reversible data hiding techniques. In 2009, Tsai et al. solved this problem by exploiting the similarity of neighboring pixels to construct a histogram of prediction errors; data embedding is done by shifting the error histogram. However, Tsai et al.’s method does not fully exploit the correlation of the neighboring pixels. In this paper, a set of basic pixels is employed to improve the prediction accuracy, thereby increasing the payload. To further improve the image quality, a threshold is used to select only low-variance blocks to join the embedding process. According to the experimental results, the proposed method provides a better or comparable stego image quality than Tsai et al.’s method and other existing reversible data hiding methods under the same payload. 
83|12||Erratum to âMeans-ends and whole-part traceability analysis of safety requirementsâ [J. Syst. Software 83 (2010) 1612â1621]|
83|12||Corrigendum to âA modeling approach on the TelosB WSN platform power consumptionâ [J. Syst. Software 83 (2010) 1355â1363]|
83|2|http://www.sciencedirect.com/science/journal/01641212/83/2|Computer software and applications|
83|2||Fault localization through evaluation sequences|
83|2||A family of code coverage-based heuristics for effective fault localization|
83|2||Formal specification of the variants and behavioural features of design patterns|
83|2||Measuring behavioral dependency for improving change-proneness prediction in UML-based design models|
83|2||Embedded architecture description language|In the state-of-the-art hardware/software (HW/SW) co-design of embedded systems, there is a lack of sufficient support for architectural specifications across HW/SW boundaries. Such an architectural specification ought to capture both hardware and software components and their interactions, and facilitate effective design exploitation of HW/SW trade-offs and scalable HW/SW co-verification. In this paper, we present the embedded architecture description language (EADL). EADL is based on a component model for embedded systems that unifies hardware and software components. EADL does not dictate execution and interface semantics of hardware and software components while supporting flexible platform-oriented semantics instantiation. EADL supports concise representation of embedded system architectures and also formulation of architectural patterns of embedded systems. Besides facilitating design reuse, architectural patterns also facilitate verification reuse via association of property templates with these patterns. Effectiveness of EADL has been demonstrated by its successful application in integrating component-based co-design, co-simulation, co-verification, and co-synthesis. 
83|2||Design, analysis, and deployment of omnipresent Formal Trust Model (FTM) with trust bootstrapping for pervasive environments|The rapid decrease in the size of mobile devices, coupled with an increase in capability, has enabled a swift proliferation of small and very capable devices into our daily lives. With such a prevalence of pervasive computing, the interaction among portable devices needs to be continuous and invisible to device users. As these devices become better connected, collaboration among them will play a vital role in sharing resources in an ad-hoc manner. The sharing of resources works as a facilitator for pervasive devices. However, this ad hoc interaction among devices provides the potential for security breaches. Trust can fight against such security violations by restricting malicious nodes from participating in interactions. Therefore, we need a unified trust relationship model between entities, which captures both the needs of the traditional computing world and the world of pervasive computing where the continuum of trust is based on identity, physical context or a combination of both. Here, we present a context specific and reputation-based trust model along with a brief survey of trust models suitable for peer-to-peer and ad-hoc environments. This paper presents a multi-hop recommendation protocol and a flexible behavioral model to handle interactions. One other contribution of this paper is the integration of an initial trust model; this model categorizes services or contexts in different security levels based on their security needs, and these security needs are considered in trust bootstrapping. The other major contribution of this paper is a simple method of handling malicious recommendations. This paper also illustrates the implementation and evaluation of our proposed formal trust model. 
83|2||EDGES: Efficient data gathering in sensor networks using temporal and spatial correlations|In this paper, we present an approximate data gathering technique, called EDGES, for sensor networks that utilizes temporal and spatial correlations. The goal of EDGES is to efficiently obtain the sensor reading within a certain error bound. To do this, EDGES utilizes the multiple model Kalman filter, which is for the non-linear data distribution, as an approximation approach. The use of the Kalman filter allows EDGES to predict the future value using a single previous sensor reading in contrast to the other statistical models such as the linear regression and multivariate Gaussian. In order to extend the lifetime of networks, EDGES utilizes the spatial correlation. In EDGES, we group spatially close sensors as a cluster. Since a cluster header in a network acts as a sensor and router, a cluster header wastes its energy severely to send its own reading and/or data coming from its children. Thus, we devise a redistribution method which distributes the energy consumption of a cluster header using the spatial correlation. In some previous works, the fixed routing topology is used or the roles of nodes are decided at the base station and this information propagates through the whole network. But, in EDGES, the change of a cluster is notified to a small portion of the network. Our experimental results over randomly generated sensor networks with synthetic and real data sets demonstrate the efficiency of EDGES. 
83|2||Verification and validation of declarative model-to-model transformations through invariants|
83|2||Class movement and re-location: An empirical study of Java inheritance evolution|
83|2||A cocktail protocol with the Authentication and Key Agreement on the UMTS|
83|2||An evaluation of timed scenario notations|
83|3|http://www.sciencedirect.com/science/journal/01641212/83/3|Reviewers are a sparse and precious resource|
83|3||A comparative study of architecture knowledge management tools|Recent research suggests that architectural knowledge, such as design decisions, is important and should be recorded alongside the architecture description. Different approaches have emerged to support such architectural knowledge (AK) management activities. However, there are different notions of and emphasis on what and how architectural activities should be supported. This is reflected in the design and implementation of existing AK tools. To understand the current status of software architecture knowledge engineering and future research trends, this paper compares five architectural knowledge management tools and the support they provide in the architecture life-cycle. The comparison is based on an evaluation framework defined by a set of 10 criteria. The results of the comparison provide insights into the current focus of architectural knowledge management support, their advantages, deficiencies, and conformance to the current architectural description standard. Based on the outcome of this comparison a research agenda is proposed for future work on AK tools. 
83|3||Timed Property Sequence Chart|
83|3||Identification of refactoring opportunities introducing polymorphism|Polymorphism is one of the most important features offered by object-oriented programming languages, since it allows to extend/modify the behavior of a class without altering its source code, in accordance to the Open/Closed Principle. However, there is a lack of methods and tools for the identification of places in the code of an existing system that could benefit from the employment of polymorphism. In this paper we propose a technique that extracts refactoring suggestions introducing polymorphism. The approach ensures the behavior preservation of the code and the applicability of the refactoring suggestions based on the examination of a set of preconditions. 
83|3||Summary queries for frequent itemsets mining|
83|3||P/S-CoM: Building correct by design Publish/Subscribe architectural styles with safe reconfiguration|We present P/S-CoM, a formal approach supporting the correct modeling of Publish/Subscribe architectural styles and safe reconfiguration of dynamic architectures for event-based communication. We elaborate a set of patterns and we define the corresponding composition rules to build correct by design Publish/Subscribe styles. The defined patterns and rules respect the principle of information dissemination guaranteeing that the produced information reaches all the subscribed consumers. The patterns are modeled as graphs and the semantics of each pattern and each rule is specified formally in Z notations. We implement these specifications under the Z-Eves theorem prover which we use to prove specification consistency. The Z specification of the designed architectural style is also built by composition by applying the composition rules coded in Z. We consider the interconnection topology between event dispatchers as well as the subscription model using elementary refinements of the style specification. Moreover, we model the reconfiguration of Publish/Subscribe architecture via guarded graph-rewriting rules whose body specifies the structural constraints and whose guards define the pre- and post-conditions ensuring in this way the preservation of stylistic constraints. Similarly, we interpret reconfiguration rules in Z notations, and we implement these rules under Z-Eves for proving that all reconfigurations are style preserving. This results in a unified formal approach which handles both the static and the dynamic aspects of Publish/Subscribe software architectures. 
83|3||A fast and progressive algorithm for skyline queries with totally- and partially-ordered domains|
83|3||SALSA: QoS-aware load balancing for autonomous service brokering|
83|3||Survivable ATM mesh networks: Techniques and performance evaluation|
83|3||A modern approach to multiagent development|
83|3||The Linux kernel as a case study in software evolution|We use 810 versions of the Linux kernel, released over a period of 14 years, to characterize the system’s evolution, using Lehman’s laws of software evolution as a basis. We investigate different possible interpretations of these laws, as reflected by different metrics that can be used to quantify them. For example, system growth has traditionally been quantified using lines of code or number of functions, but functional growth of an operating system like Linux can also be quantified using the number of system calls. In addition we use the availability of the source code to track metrics, such as McCabe’s cyclomatic complexity, that have not been tracked across so many versions previously. We find that the data supports several of Lehman’s laws, mainly those concerned with growth and with the stability of the process. We also make some novel observations, e.g. that the average complexity of functions is decreasing with time, but this is mainly due to the addition of many small functions. 
83|3||Modeling and managing the variability of Web service-based systems|
83|3||Progressive sharing for a secret image|
83|3||Thank you_list of reviewers|
83|4|http://www.sciencedirect.com/science/journal/01641212/83/4|Maintaining and checking parity in highly available Scalable Distributed Data Structures|Access to data stored in distributed main memory is much faster than access to local disks. Highly available, Scalable Distributed Data Structures (SDDS) utilize this fast access. They counteract the effects of failed or unavailable nodes by storing data redundantly. Since main memory per node is limited, they generate this redundancy by storing parity data calculated with erasure correcting codes instead of using replication. We present here a way to maintain parity that is about 10 times faster than using the traditional 2PC scheme. We also present a scheme that can diagnose a mismatch between parity and user data with very little network traffic. 
83|4||A practical distinguisher for the Shannon cipher|
83|4||ATTEST: ATTributes-based Extendable STorage|
83|4||Agent-oriented software patterns for rapid and affordable robot programming|Robotic systems are often quite complex to develop; they are huge, heavily constrained from the non-functional point of view and they implement challenging algorithms. The lack of integrated methods with reuse approaches leads robotic developers to reinvent the wheel each time a new project starts. This paper proposes to reuse the experience done when building robotic applications, by catching it into design patterns. These represent a general mean for (i) reusing proved solutions increasing the final quality, (ii) communicating the knowledge about a domain and (iii) reducing the development time and effort. Despite of this generality, the proposed repository of patterns is specific for multi-agent robotic systems. These patterns are documented by a set of design diagrams and the corresponding implementing code is obtained through a series of automatic transformations. Some patterns extracted from an existing and freely available repository are presented. The paper also discusses an experimental set-up based on the construction of a complete robotic application obtained by composing some highly reusable patterns. 
83|4||An empirical examination of application frameworks success based on technology acceptance model|
83|4||Power optimization for dynamic configuration in heterogeneous web server clusters|
83|4||Pseudo software: A mediating instrument for modeling software requirements|
83|4||Redirection based recovery for MPLS network systems|
83|4||Visual comparison of software cost estimation models by regression error characteristic analysis|The well-balanced management of a software project is a critical task accomplished at the early stages of the development process. Due to this requirement, a wide variety of prediction methods has been introduced in order to identify the best strategy for software cost estimation. The selection of the best technique is usually based on measures of error whereas in more recent studies researchers use formal statistical procedures. The former approach can lead to unstable and erroneous results due to the existence of outlying points whereas the latter cannot be easily presented to non-experts and has to be carried out by an expert with statistical background. In this paper, we introduce the regression error characteristic (REC) analysis, a powerful visualization tool with interesting geometrical properties, in order to validate and compare different prediction models easily, by a simple inspection of a graph. Moreover, we propose a formal framework covering different aspects of the estimation process such as the calibration of the prediction methodology, the identification of factors that affect the error, the investigation of errors on certain ranges of the actual cost and the examination of the distribution of the cost for certain errors. Application of REC analysis to the ISBSG10 dataset for comparing estimation by analogy and linear regression illustrates the benefits and the significant information obtained. 
83|4||RO-cash: An efficient and practical recoverable pre-paid offline e-cash scheme using bilinear pairings|
83|4||Design and analysis of GUI test-case prioritization using weight-based methods|Testing the correctness of a GUI-based application is more complex than the conventional code-based application. In addition to testing the underlying codes of the GUI application, the space of possible combinations of events with a large GUI-input sequence also requires creating numerous test cases to confirm the adequacy of the GUI testing. Running all GUI test cases and then fixing all found bugs may be time-consuming and delaying the project completion. Hence, it is important to advance the test cases that uncover the most faults as fast as possible in the testing process. Test-case prioritization has been proposed and used in recent years because it can improve the rate of fault detection during the testing phase. However, few studies have discussed the problem of GUI test-case prioritization. In this paper, we propose a weighted-event flow graph for solving the non-weighted GUI test case and ranking GUI test cases based on weight scores. The weighted scores can either be ranked from high to low or be ordered by dynamic adjusted scores. Finally, three experiments are performed, and experimental results show that the adjusted-weight method can obtain a better fault-detection rate. 
83|4||On the ability of complexity metrics to predict fault-prone classes in object-oriented systems|
83|4||Petri net modeling and deadlock analysis of parallel manufacturing processes with shared-resources|Multiple resource-sharing is a common situation in parallel and complex manufacturing processes and may lead to deadlock states. To alleviate this issue, this paper presents the method of modeling parallel processing flows, sharing limited number of resources, in flexible manufacturing systems (FMSs). A new class of Petri net called parallel process net with resources (PPNRs) is introduced for modeling such FMSs. PPNRs have the capacity to model the more complex resource-sharing among parallel manufacturing processes. Furthermore, this paper presents the simple diagnostic and remedial procedures for deadlocks in PPNRs. The proposed technique for deadlock detection and recovery is based on transition vectors which have the power of determining the structural aspects as well as the process flow condition in PPNRs. Moreover, the proposed technique for dealing with deadlocks is not a siphon-based thus the large-scale PPNRs for real-life FMSs can be tackled. Finally, the proposed method of modeling and deadlock analysis in the FMS having parallel processing is demonstrated by a practical example. 
83|4||Using hybrid algorithm for Pareto efficient multi-objective test suite minimisation|
83|4||An improved impossible differential cryptanalysis of Zodiac|
83|4||Corrigendum to âA wireless sensor system for validation of real-time automatic calibration of groundwater transport modelsâ [J. Syst. Software 82 (2009) 1859â1868]|
83|5|http://www.sciencedirect.com/science/journal/01641212/83/5|Stability assessment of aspect-oriented software architectures: A quantitative study|Design of stable software architectures has increasingly been a deep challenge to software developers due to the high volatility of their concerns and respective design decisions. Architecture stability is the ability of the high-level design units to sustain their modularity properties and not succumb to modifications. Architectural aspects are new modularity units aimed at improving design stability through the modularization of otherwise crosscutting concerns. However, there is no empirical knowledge about the positive and negative influences of aspectual decompositions on architecture stability. This paper presents an exploratory analysis of the influence exerted by aspect-oriented composition mechanisms in the stability of architectural modules addressing typical crosscutting concerns, such as error handling and security. Our investigation encompassed a comparative analysis of aspectual and non-aspectual decompositions based on different architectural styles applied to an evolving multi-agent software architecture. In particular, we assessed various facets of components’ and compositions’ stability through such alternative designs of the same multi-agent system using conventional quantitative indicators. We have also investigated the key characteristics of aspectual decompositions that led to (in)stabilities being observed in the target architectural options. The evaluation focused upon a number of architecturally-relevant changes that are typically performed through real-life maintenance tasks. 
83|5||A classification and comparison of model checking software architecture techniques|Software architecture specifications are used for many different purposes, such as documenting architectural decisions, predicting architectural qualities before the system is implemented, and guiding the design and coding process. In these contexts, assessing the architectural model as early as possible becomes a relevant challenge. Various analysis techniques have been proposed for testing, model checking, and evaluating performance based on architectural models. Among them, model checking is an exhaustive and automatic verification technique, used to verify whether an architectural specification conforms to expected properties. While model checking is being extensively applied to software architectures, little work has been done to comprehensively enumerate and classify these different techniques.The goal of this paper is to investigate the state-of-the-art in model checking software architectures. For this purpose, we first define the main activities in a model checking software architecture process. Then, we define a classification and comparison framework and compare model checking software architecture techniques according to it. 
83|5||Fast convergence to network fairness|Most AQM algorithms, such as RED, assure fairness through randomness in congestion notification. However, randomness results in fair allocation of network resources only when time limitations are not considered. This is not compatible with the current Internet, where traffic oscillations are frequent and the demand for fair treatment is rather urgent, due to short duration of most applications. Given the short duration of most modern Internet applications, fast convergence to fairness is necessitated. In this paper, we use fairness as the major criterion to adjust traffic and present a corresponding algorithm of active queue management, which is called Explicit Global Congestion Notifier (EGCN). EGCN notifies flows almost simultaneously about incipient congestion by marking packets arriving at the router’s queue, when the load in the network increases and buffer overflow is expected. This is a new approach compared with the random notification policy of RED or ECN. EGCN distributes the burden to adjust backward to more flows and consequently allows for smoother window adjustments. We elaborate on the properties of system-wide response in terms of fairness, smoothness and efficiency. Simulation results demonstrate a clear-cut advantage of the proposed scheme. 
83|5||Trapdoor security in a searchable public-key encryption scheme with a designated tester|We study a secure searchable public-key encryption scheme with a designated tester (dPEKS). The contributions of this paper are threefold. First, we enhance the existing security model to incorporate the realistic abilities of dPEKS attackers. Second, we introduce the concept of “trapdoor indistinguishability” and show that trapdoor indistinguishability is a sufficient condition for thwarting keyword-guessing attacks. This answers the open problem of how to construct PEKS (dPEKS) schemes that are provably secure against keyword-guessing attacks. Finally, we propose a dPEKS scheme that is secure in the enhanced security model. The scheme is the first dPEKS scheme that is secure against keyword-guessing attacks. 
83|5||Compiler-assisted leakage-aware loop scheduling for embedded VLIW DSP processors|
83|5||Defining and controlling the heterogeneity of a cluster: The Wrekavoc tool|
83|5||Measuring design complexity of semantic web ontologies|
83|5||A family of languages for architecture constraint specification|During software development, architecture decisions should be documented so that quality attributes guaranteed by these decisions and required in the software specification could be persisted. An important part of these architectural decisions is often formalized using constraint languages which differ from one stage to another in the development process. In this paper, we present a family of architectural constraint languages, called ACL. Each member of this family, called a profile, can be used to formalize architectural decisions at a given stage of the development process. An ACL profile is composed of a core constraint language, which is shared with the other profiles, and a MOF architecture metamodel. In addition to this family of languages, this paper introduces a transformation-based interpretation method of profiles and its associated tool. 
83|5||A novel data hiding scheme based on modulus function|Four criteria are generally used to evaluate the performance of data hiding scheme: the embedding capacity, the visual quality of the stego-image, the security, and the complexity of the data-embedding algorithm. However, data hiding schemes seldom take all these factors into consideration. This paper proposes a novel data hiding scheme that uses a simple modulus function to address all the performance criteria listed above. According to the input secret keys, the encoder and decoder use the same set-generation functions Hr() and Hc() to first generate two sets Kr and Kc. A variant Cartesian product is then created using Kr and Kc. Each cover pixel then forms a pixel group with its neighboring pixels by exploiting an efficient modulus function; the secret data are then embedded or extracted via a mapping process between the variant of the Cartesian product and each pixel group. The proposed scheme offers several advantages, namely (1) the embedding capacity can be scaled, (2) a good visual quality of the stego-image can be achieved, (3) the computational cost of embedding or extracting the secret data is low and requires little memory space, (4) secret keys are used to protect the secret data and (5) the problem of overflow or underflow does not occur, regardless of the nature of the cover pixels.We tested the performance of the proposed scheme by comparing it with Mielikainen’s and Zhang and Wang’s schemes for gray-scale images. The experimental results showed that our proposed scheme outperforms Mielikainen’s in three respects, namely scalable embedding capacity, embedding rate, and level of security. Our data hiding scheme also achieved a higher embedding capacity than Zhang and Wang’s. The proposed scheme can easily be applied to both gray-scale and color images. Analyses of its performance showed that our proposed scheme outperforms Tsai and Wang’s in terms of its time complexity and memory space requirement. 
83|5||Differential fault analysis on Camellia|
83|5||A simple, least-time, and energy-efficient routing protocol with one-level data aggregation for wireless sensor networks|The area of wireless sensor networks (WSN) is currently attractive in the research community area due to its applications in diverse fields such as defense security, civilian applications and medical research. Routing is a serious issue in WSN due to the use of computationally-constrained and resource-constrained micro-sensors. These constraints prohibit the deployment of traditional routing protocols designed for other ad hoc wireless networks. Any routing protocol designed for use in WSN should be reliable, energy-efficient and should increase the lifetime of the network. We propose a simple, least-time, energy-efficient routing protocol with one-level data aggregation that ensures increased life time for the network. The proposed protocol was compared with popular ad hoc and sensor network routing protocols, viz., AODV (35 and 12), DSR (Johnson et al., 2001), DSDV (Perkins and Bhagwat, 1994), DD (Intanagonwiwat et al., 2000) and MCF (Ye et al., 2001). It was observed that the proposed protocol outperformed them in throughput, latency, average energy consumption and average network lifetime. The proposed protocol uses absolute time and node energy as the criteria for routing, this ensures reliability and congestion avoidance. 
83|5||A novel user-participating authentication scheme|
83|5||A symbolic fault-prediction model based on multiobjective particle swarm optimization|
83|6|http://www.sciencedirect.com/science/journal/01641212/83/6|Special Issue on Software Architecture and Mobility|
83|6||Software architecture and mobility: A roadmap|Modern software-intensive systems are characterized not only by the movement of data, as has been the case in traditional distributed systems, but also by the movement of users, devices, and code. Developing effective, efficient, and dependable systems in the mobile setting is challenging. Existing architectural principles need to be adapted and novel architectural paradigms devised. In this paper, we give an overview of the intersection of the areas of software architecture and mobility. We consider mobility from two related perspectives: (1) mobile software, which represents the computing functionality designed to migrate across hardware devices at runtime and execute on mobile hardware platforms, and (2) mobile systems, which are computing applications that include mobile software and hardware elements. We study the advances in both these areas, highlight representative existing solutions, and identify several remaining research challenges. 
83|6||Commentary on âSoftware architectures and mobility: A Roadmapâ|
83|6||Computer supported cooperative work and âSoftware architectures and mobility: A Roadmapâ|
83|6||Multi-layer faults in the architectures of mobile, context-aware adaptive applications|
83|6||Dealing with variability in context-aware mobile software|
83|6||Ambient-PRISMA: Ambients in mobile aspect-oriented software architecture|This work presents an approach called Ambient-PRISMA for modelling and developing distributed and mobile applications. Ambient-PRISMA enriches an aspect-oriented software architectural approach called PRISMA with the ambient concept from Ambient Calculus. Ambients are introduced in PRISMA as specialized kinds of connectors that offer mobility services to architectural elements (components and connectors) and are able to coordinate a boundary, which models the notion of location. Mobility of architectural elements is supported by reconfiguring the software architecture. This paper presents a metamodel that introduces ambients to design aspect-oriented software architectural models for mobile systems. The design of models is performed using an Aspect-Oriented Architecture Description Language. A middleware called Ambient-PRISMANET which maps the metamodel to .NET technology and supports the distributed runtime environment needed for executing mobile applications is also presented. In addition, a CASE Tool which allows users to specify the aspect-oriented architectural models in a graphical way and generate .NET code is provided. In this way, we explain how Ambient-PRISMA follows Model Driven Engineering. An example of an auction system is used throughout the article to illustrate the work. 
83|6||Promoting the development of secure mobile agent applications|
83|6||An architecture-driven software mobility framework|Software architecture has been shown to provide an appropriate level of granularity for assessing a software system’s quality attributes (e.g., performance and dependability). Similarly, previous research has adopted an architecture-centric approach to reasoning about and managing the run-time adaptation of software systems. For mobile and pervasive software systems, which are known to be innately dynamic and unpredictable, the ability to assess a system’s quality attributes and manage its dynamic run-time behavior is especially important. In the past, researchers have argued that a software architecture-based approach can be instrumental in facilitating mobile computing. In this paper, we present an integrated architecture-driven framework for modeling, analysis, implementation, deployment, and run-time migration of software systems executing on distributed, mobile, heterogeneous computing platforms. In particular, we describe the framework’s support for dealing with the challenges posed by both logical and physical mobility. We also provide an overview of our experience with applying the framework to a family of distributed mobile robotics systems. This experience has verified our envisioned benefits of the approach, and has helped us to identify several avenues of future work. 
83|6||A novel XML keyword query approach using entity subtree|
83|6||Scheduling multiple task graphs with end-to-end deadlines in distributed real-time systems utilizing imprecise computations|
83|6||Robust lossless image watermarking based on Î±-trimmed mean algorithm and support vector machine|This paper presents a robust lossless watermarking technique, based on Î±-trimmed mean algorithm and support vector machine (SVM), for image authentication. SVM is trained to memorize relationship between the watermark and the image-dependent watermark other than embedding watermark into the host image. While needing to authenticate the ownership of the image, the trained SVM is used to recover the watermark and then the recovered watermark is compared with the original watermark to determine the ownership. Meanwhile, the robustness can be enhanced using Î±-trimmed mean operator against attacks. Experimental results demonstrate that the technique not only possesses the robustness to resist on image-manipulation attacks under consideration but also, in average, is superior to other existing methods being considered in the paper. 
83|6||Efficient evaluation of query rewriting plan over materialized XML view|
83|6||Selection of strategies in judgment-based effort estimation|
83|6||Software metadata: Systematic characterization of the memory behaviour of dynamic applications|
83|7|http://www.sciencedirect.com/science/journal/01641212/83/7|Editorial for the JSS SPLC 2008 Special Issue|
83|7||Automating the construction of domain-specific modeling languages for object-oriented frameworks|
83|7||Automated diagnosis of feature model configurations|
83|7||Structuring the modeling space and supporting evolution in software product line engineering|The scale and complexity of product lines means that it is practically infeasible to develop a single model of the entire system, regardless of the languages or notations used. The dynamic nature of real-world systems means that product line models need to evolve continuously to meet new customer requirements and to reflect changes of product line artifacts. To address these challenges, product line engineers need to apply different strategies for structuring the modeling space to ease the creation and maintenance of models. This paper presents an approach that aims at reducing the maintenance effort by organizing product lines as a set of interrelated model fragments defining the variability of particular parts of the system. We provide support to semi-automatically merge fragments into complete product line models. We also provide support to automatically detect inconsistencies between product line artifacts and the models representing these artifacts after changes. Furthermore, our approach supports the co-evolution of models and their respective meta-models. We discuss strategies for structuring the modeling space and show the usefulness of our approach using real-world examples from our ongoing industry collaboration. 
83|7||A feature-oriented approach for developing reusable product line assets of service-based systems|
83|7||Handling over-fitting in test cost-sensitive decision tree learning by feature selection, smoothing and pruning|Cost-sensitive learning algorithms are typically designed for minimizing the total cost when multiple costs are taken into account. Like other learning algorithms, cost-sensitive learning algorithms must face a significant challenge, over-fitting, in an applied context of cost-sensitive learning. Specifically speaking, they can generate good results on training data but normally do not produce an optimal model when applied to unseen data in real world applications. It is called data over-fitting. This paper deals with the issue of data over-fitting by designing three simple and efficient strategies, feature selection, smoothing and threshold pruning, against the TCSDT (test cost-sensitive decision tree) method. The feature selection approach is used to pre-process the data set before applying the TCSDT algorithm. The smoothing and threshold pruning are used in a TCSDT algorithm before calculating the class probability estimate for each decision tree leaf. To evaluate our approaches, we conduct extensive experiments on the selected UCI data sets across different cost ratios, and on a real world data set, KDD-98 with real misclassification cost. The experimental results show that our algorithms outperform both the original TCSDT and other competing algorithms on reducing data over-fitting. 
83|7||A service-based architecture for dynamically reconfigurable workflows|
83|7||A catalogue of component connectors to support development with reuse|
83|7||TALISMAN MDE: Mixing MDE principles|
83|7||Robust fuzzy CPU utilization control for dynamic workloads|
83|7||Agile monitoring using the line of balance|There is a need to collect, measure, and present progress information in all projects, and Agile projects are no exception. In this article, the authors show how the line of balance, a relatively obscure indicator, can be used to gain insights into the progress of projects not provided by burn down charts or cumulative flow diagrams, two of the most common indicators used to track and report progress in Agile projects. The authors also propose to replace the original plan-based control point lead-time calculations with dynamic information extracted from a version control system and introduce the concept of the ideal plan to measure progress relative to both, end of iteration milestones and project completion date. 
83|7||Analyzing architectural styles|
83|7||A novel adaptive steganography based on local complexity and human vision sensitivity|This paper presents a novel adaptive steganographic scheme that is capable of both preventing visual degradation and providing a large embedding capacity. The embedding capacity of each pixel is dynamically determined by the local complexity of the cover image, allowing us to maintain good visual quality as well as embedding a large amount of secret messages. We classify pixels into three levels based on the variance of the local complexity of the cover image. When determining which level of local complexity a pixel should belong to, we take human vision sensitivity into consideration. This ensures that the visual artifacts appeared in the stego image are imperceptible, and the difference between the original and stego image is indistinguishable by the human visual system. The pixel classification assures that the embedding capacity offered by a cover image is bounded by the embedding capacity imposed on three levels that are distinguished by two boundary thresholds values. This allows us to derive a combination ratio of the maximal embedding capacity encountered with at each level. Consequently, our scheme is capable of determining two threshold values according to the desired demand of the embedding capacity requested by the user. Experimental results demonstrated that our adaptive steganographic algorithm produces insignificant visual distortion due to the hidden message. It provides high embedding capacity superior to that offered by a number of existing schemes. Our algorithm can resist the RS steganalysis attack, and it is statistically invisible for the attack of histogram comparison. The proposed scheme is simple, efficient and feasible for adaptive steganographic applications. 
83|7||Bioinformatics algorithm development for Grid environments|A Grid environment can be viewed as a virtual computing architecture that provides the ability to perform higher throughput computing by taking advantage of many computers geographically dispersed and connected by a network. Bioinformatics applications stand to gain in such a distributed environment in terms of increased availability, reliability and efficiency of computational resources. There is already considerable research in progress toward applying parallel computing techniques on bioinformatics methods, such as multiple sequence alignment, gene expression analysis and phylogenetic studies. In order to cope with the dimensionality issue, most machine learning methods either focus on specific groups of proteins or reduce the size of the original data set and/or the number of attributes involved. Grid computing could potentially provide an alternative solution to this problem, by combining multiple approaches in a seamless way. In this paper we introduce a unifying methodology coupling the strengths of the Grid with the specific needs and constraints of the major bioinformatics approaches. We also present a tool that implements this process and allows researchers to assess the computational needs for a specific task and optimize the allocation of available resources for its efficient completion. 
83|7||The effect of testing location on usability testing performance, participant stress levels, and subjective testing experience|The effect of testing location on usability test elements such as stress levels and user experience is not clear. A comparison between traditional lab testing and synchronous remote testing was conducted. The present study investigated two groups of users in remote and traditional settings. Within each group participants completed two tasks, a simple task and a complex task. The dependent measures were task time taken, number of critical incidents reported, and user-reported anxiety score. Task times differed significantly between the physical location condition; this difference was not meaningful for real world application, and likely introduced by overhead regarding synchronous remote testing methods. Critical incident reporting counts did not differ in any condition. No significant differences were found in user reported stress levels. Subjective assessments of the study and interface also did not differ significantly. Study findings suggest a similar user testing experience exists for remote and traditional laboratory usability testing. 
83|7||A weighted common structure based clustering technique for XML documents|
83|7||Software process improvement through the Lean Measurement (SPI-LEAM) method|
83|7||A cusum change-point detection algorithm for non-stationary sequences with application to data network surveillance|We adapt the classic cusum change-point detection algorithm to handle non-stationary sequences that are typical with network surveillance applications. The proposed algorithm uses a defined timeslot structure to take into account time varying distributions, and uses historical samples of observations within each timeslot to facilitate a nonparametric methodology. Our proposed solution includes an on-line screening feature that fully automates the implementation of the algorithm and eliminates the need for manual oversight up until the point where root cause analysis begins. 
83|8|http://www.sciencedirect.com/science/journal/01641212/83/8|Special issue: Performance evaluation and optimization of ubiquitous computing and networked systems|
83|8||Improving routing protocol performance in delay tolerant networks using extended information|
83|8||Performance analysis of opportunistic broadcast for delay-tolerant wireless sensor networks|This paper investigates a class of mobile wireless sensor networks that are unconnected most of the times; we refer to them as delay–tolerant wireless sensor networks (DTWSN). These networks inherit their characteristics from both delay tolerant networks (DTN) and traditional wireless sensor networks. After introducing DTWSNs, three main problems in the design space of these networks are discussed: routing, data gathering, and neighbor discovery. A general protocol is proposed for DTWSNs based on opportunistic broadcasting in delay-tolerant networks with radio device on–off periods. Three performance measures are defined in the study: the energy for sending queries to ask for data from possible neighbors (querying energy), data transfer energy, and absorption time (delay). A simple yet accurate approximation for the data-transfer energy is proposed. An analytic model is provided to evaluate the querying energy per contact (epc). Simulation results for the data propagation delay show that the querying energy per contact measure obtained from the analytic model is proportional to the product of the querying energy and the delay. A practical rule of thumb for an optimal query interval in terms of delay and energy is derived from different parts of the study. 
83|8||GLBM: A new QoS aware multicast scheme for wireless mesh networks|
83|8||Communication modeling of multicast in all-port wormhole-routed NoCs|
83|8||On the performance of real-time multi-item request scheduling in data broadcast environments|
83|8||Performance evaluation of bag of gangs scheduling in a heterogeneous distributed system|
83|8||A modeling approach on the TelosB WSN platform power consumption|Substantial characteristics of wireless sensor networks, such as autonomy and miniature size, are achieved at the expense of restricted energy resources. Optimal resource management is thus among the most important challenges in WSNs development and its success requires accurate and practical models based on detailed insight concerning the factors contributing to the overall power consumption of a WSN mote. To achieve such awareness, that will enable models development, appropriate measuring test-beds and methodologies are needed, facilitating reliable and accurate power consumption measurements of critical functionalities.To cover the need for energy models that precisely define the power consumption behavior of WSN hardware platforms, this paper contributes with a measuring methodology including three steps: the design and implementation of a measuring system for a wide range of power consumption thresholds, the identification, isolation and measurement of elementary functionalities of a WSN platform with respect to their contribution to the overall mote power consumption, and the extraction of valuable conclusions based on the respective measurements resulting in the composition of a practical, yet accurate power consumption model. 
83|8||Design and evaluation of a novel MAC layer handoff protocol for IEEE 802.11 wireless networks|
83|8||Signal strength based routing for power saving in mobile ad hoc networks|The transmission bandwidth between two nodes in mobile ad hoc networks is important in terms of power consumption. However, the bandwidth between two nodes is always treated the same, regardless of what the distance is between the two nodes. If a node equips a GPS device to determine the distance between two nodes, the hardware cost and the power consumption increase. In this paper, we propose using a bandwidth-based power-aware routing protocol with signal detection instead of using GPS devices to determine the distance. In our proposed routing protocol, we use the received signal variation to predict the transmission bandwidth and the lifetime of a link. Accordingly, the possible amount of data that can be transmitted and the remaining power of nodes in the path after data transmission can be predicted. By predicting the possible amount of data that can be transmitted and the remaining power of nodes after data transmission, we can design a bandwidth-based power-aware routing protocol that has power efficiency and that prolongs network lifetime. In our simulation, we compare our proposed routing protocol with two signal-based routing protocols, SSA and ABR, and a power-aware routing protocol, MMBCR, in terms of the throughput, the average transmission bandwidth, the number of rerouting paths, the path lifetime, the power consumed when a byte is transmitted, and the network lifetime (the ratio of active nodes). 
83|8||Architecture analysis of enterprise systems modifiability â Models, analysis, and validation|
83|8||Handling communications in process algebraic architectural description languages: Modeling, verification, and implementation|
83|8||A new approach for componentâs port modeling in software architecture|The component’s interaction points with the external world play a fundamental role in the specification of an application’s architecture. Current software architecture approaches consider an interaction point as an atomic element in the specification of interconnections, despite the complexity of its structure and the attached behavior. It is not possible in current component models to deal separately with an element of an interaction point when such an element is needed alone for specifying a specific logic. To support such logic and the specification of a wide range of early ideas in the process of elaborating a software system, the Integrated Approach to Software Architecture (IASA) uses an interaction point model which provides facilities to manipulate any structural or behavioral element defining an interaction point. In addition, such facilities represent the fundamental foundation of the native support by IASA of Aspect Oriented Software Architectures (AOSA) specifications. 
83|8||A differential cryptanalysis of YenâChenâWu multimedia cryptography system|
83|8||A distributed platform for personalized advertising in digital interactive TV environments|
83|8||Image watermarking method in multiwavelet domain based on support vector machines|A novel image watermarking method in multiwavelet domain based on support vector machines (SVMs) is proposed in this paper. The special frequency band and property of image in multiwavelet domain are employed for the watermarking algorithm. After performed single-level multiwavelet decomposition on each image block of an image, a mean value modulation method, which modulates mean value relationship of multiwavelet coefficients in two approximation sub-bands, is used for carrying watermark embedding. The mean value modulation method can more effectively reduce image distortion than that of conventional single coefficient. At watermark detector, SVMs is used to learn the mean value relationship. Due to good learning ability of SVMs, watermark can be correctly extracted under several different attacks. The experimental results show proposed algorithm is robust to common attacks such as JPEG, low-pass filtering, noise addition, rotation and scaling, etc. 
83|8||Requirement-based approach for groupware environments design|
83|8||Quality adaptive end-to-end packet scheduling to avoid playout interruptions in Internet video streaming systems|
83|8||Historical index structure for reducing insertion and search cost in LBS|
83|8||Per-flow optimal service selection for Web services based processes|
83|8||Intrusion detection for mobile devices using the knowledge-based, temporal abstraction method|In this paper, a new approach for detecting previously unencountered malware targeting mobile device is proposed. In the proposed approach, time-stamped security data is continuously monitored within the target mobile device (i.e., smartphones, PDAs) and then processed by the knowledge-based temporal abstraction (KBTA) methodology. Using KBTA, continuously measured data (e.g., the number of sent SMSs) and events (e.g., software installation) are integrated with a mobile device security domain knowledge-base (i.e., an ontology for abstracting meaningful patterns from raw, time-oriented security data), to create higher level, time-oriented concepts and patterns, also known as temporal abstractions. Automatically-generated temporal abstractions are then monitored to detect suspicious temporal patterns and to issue an alert. These patterns are compatible with a set of predefined classes of malware as defined by a security expert (or the owner) employing a set of time and value constraints. The goal is to identify malicious behavior that other defensive technologies (e.g., antivirus or firewall) failed to detect. Since the abstraction derivation process is complex, the KBTA method was adapted for mobile devices that are limited in resources (i.e., CPU, memory, battery). To evaluate the proposed modified KBTA method a lightweight host-based intrusion detection system (HIDS), combined with central management capabilities for Android-based mobile phones, was developed. Evaluation results demonstrated the effectiveness of the new approach in detecting malicious applications on mobile devices (detection rate above 94% in most scenarios) and the feasibility of running such a system on mobile devices (CPU consumption was 3% on average). 
83|8||A distributed server architecture supporting dynamic resource provisioning for BPM-oriented workflow management systems|
83|9|http://www.sciencedirect.com/science/journal/01641212/83/9|Introduction to the special issue|
83|9||In Memoriam: Dr. Chandra Kintala|
83|9||Memory leak analysis of mission-critical middleware|
83|9||Methods and opportunities for rejuvenation in aging distributed software systems|
83|9||Analysis of service availability for time-triggered rejuvenation policies|
83|9||Comprehensive evaluation of aperiodic checkpointing and rejuvenation schemes in operational software system|
83|9||Software for protection system of VR-1 training reactor|
83|9||Means-ends and whole-part traceability analysis of safety requirements|
83|9||Quantifying security risk level from CVSS estimates of frequency and impact|
84|1|http://www.sciencedirect.com/science/journal/01641212/84/1|Special issue on the information networking and services|
84|1||A topology control protocol based on eligibility and efficiency metrics|The question of fairness in wireless sensor networks is not studied very well. It is not unusual to observe in the literature fairness traded for low latency or reliability. However, a disproportional use of some critical nodes as relaying nodes can cause premature network fragmentation. This paper investigates fairness in multi-hop wireless sensor networks and proposes a topology control protocol that enables nodes to exhaust their energy fairly. Moreover, it demonstrates that whereas the number of neighboring nodes with which a node should cooperate depends on the density of the network, increasing this number beyond a certain amount does not contribute to network connectivity. 
84|1||An analytical model of broadcast in QoS-aware wormhole-routed NoCs|
84|1||An experimental study of peer behavior in a pure P2P network|
84|1||Fault-tolerant flocking for a group of autonomous mobile robots|
84|1||Performance analysis of an integrated scheduling scheme in the presence of bursty MMPP traffic|
84|1||A comparative study on simulation vs. real time deployment in wireless sensor networks|Increasing deployment density and shrinking size of wireless sensor nodes requires small equipped battery size. This means emerging wireless sensor nodes must compete for efficient energy utilization. Medium Access Control (MAC) protocols play a vital role in energy consumption of sensor node as it controls the radio activities. Customized or open source simulators play an important role to measure the performance effectiveness of MAC protocols based on the fact that they are flexible, reduce experimental overhead and cost. Nevertheless, these benefits come at the cost of results accuracy. In this paper, we investigate differences of the behaviour of our agent based S-MAC protocols in real deployment compared to the results produced using our custom based simulator, which ignores the lower layers effects such as packet collision and overhearing. We use network simulator 2 (ns2), an open source simulator, which provides a complete protocol stack. We further try to find and explain the rationale of the variance of results produced by real deployment and that of simulators. 
84|1||Using Grid services to parallelize IBM's Generic Log Adapter|
84|1||Double-layered schema integration of heterogeneous XML sources|Schema integration aims to create a mediated schema as a unified representation of existing heterogeneous sources sharing a common application domain. These sources have been increasingly written in XML due to its versatility and expressive power. Unfortunately, these sources often use different elements and structures to express the same concepts and relations, thus causing substantial semantic and structural conflicts. Such a challenge impedes the creation of high-quality mediated schemas and has not been adequately addressed by existing integration methods. In this paper, we propose a novel method, named XINTOR, for automating the integration of heterogeneous schemas. Given a set of XML sources and a set of correspondences between the source schemas, our method aims to create a complete and minimal mediated schema: it completely captures all of the concepts and relations in the sources without duplication, provided that the concepts do not overlap. Our contributions are fourfold. First, we resolve structural conflicts inherent in the source schemas. Second, we introduce a new statistics-based measure, called path cohesion, for selecting concepts and relations to be a part of the mediated schema. The path cohesion is statistically computed based on multiple path quality dimensions such as average path length and path frequency. Third, we resolve semantic conflicts by augmenting the semantics of similar concepts with context-dependent information. Finally, we propose a novel double-layered mediated schema to retain a wider range of concepts and relations than existing mediated schemas, which are at best either complete or minimal, but not both. Performed on both real and synthetic datasets, our experimental results show that XINTOR outperforms existing methods with respect to (i) the mediated-schema quality using precision, recall, F-measure, and schema minimality; and (ii) the execution performance based on execution time and scale-up performance. 
84|1||A formal approach for the specification and verification of trustworthy component-based systems|Software systems are increasingly becoming ubiquitous affecting the way we experience the world. Embedded software systems, especially those used in smart devices, have become an essential constituent of the technological infrastructure of modern societies. Such systems, in order to be trusted in society, must be proved to be trustworthy. Trustworthiness is a composite non-functional property that implies safety, timeliness, security, availability, and reliability. This paper presents a formal approach for the development of trustworthy component-based systems. The approach involves a formal component model for the specification of component’s structure, functional, and non-functional (trustworthiness) properties, a model transformation technique for the automatic generation of component behavior using the specified structure and restricted by the specified properties, and a unified formal verification method for safety, security, reliability and availability properties using model checking. 
84|1||High capacity data hiding schemes for medical images based on difference expansion|
84|1||Efficient key management for preserving HIPAA regulations|The protection of patients’ health information is a very important issue in the information age. Health Insurance Portability and Accountability Act (HIPAA) of privacy and security regulations are two crucial provisions in the protection of healthcare privacy, especially electronic medical information. For the quality and efficiency of the electronic services, it is necessary to construct better performance for the user and the trusted party. Based on elliptic curve cryptography (ECC) and complying with HIPAA regulations, this article presents an efficient key management scheme to facilitate inter-operations among the applied cryptographic mechanisms. In addition, the proposed scheme can achieve the complete functionality which includes: (1) a dictionary of key tables is not required for users and other units; (2) users can freely choose their own passwords; (3) users can freely update their passwords after the registration phase; (4) the computational cost is very low for users and the trusted center or server; (5) users are able to access their individual medical information through the authorization process; (6) case of consent exceptions intended to facilitate emergency applications or other possible exceptions can also be dealt with easier. 
84|1||Identity-based strong designated verifier signature revisited|
84|1||Effective rank aggregation for metasearching|Nowadays, mashup services and especially metasearch engines play an increasingly important role on the Web. Most of users use them directly or indirectly to access and aggregate information from more than one data sources. Similarly to the rest of the search systems, the effectiveness of a metasearch engine is mainly determined by the quality of the results it returns in response to user queries. Since these services do not maintain their own document index, they exploit multiple search engines using a rank aggregation method in order to classify the collected results. However, the rank aggregation methods which have been proposed until now, utilize a very limited set of parameters regarding these results, such as the total number of the exploited resources and the rankings they receive from each individual resource. In this paper we present QuadRank, a new rank aggregation method, which takes into consideration additional information regarding the query terms, the collected results and the data correlated to each of these results (title, textual snippet, URL, individual ranking and others). We have implemented and tested QuadRank in a real-world metasearch engine, QuadSearch, a system developed as a testbed for algorithms related to the wide problem of metasearching. The name QuadSearch is related to the current number of the exploited engines (four). We have exhaustively tested QuadRank for both effectiveness and efficiency in the real-world search environment of QuadSearch and also, using a task from the recent TREC-2009 conference. The results we present in our experiments reveal that in most cases QuadRank outperformed all component engines, another metasearch engine (Dogpile) and two successful rank aggregation methods, Borda Count and the Outranking Approach. 
84|1||Firmsâ involvement in Open Source projects: A trade-off between software structural quality and popularity|
84|1||An assessment of systems and software engineering scholars and institutions (2003â2007 and 2004â2008)|
84|10|http://www.sciencedirect.com/science/journal/01641212/84/10|A geographic routing hybrid approach for void resolution in wireless sensor networks|Geographic routing is one of the most suitable routing strategies for large scale wireless sensor networks due to its low overhead and high scalability features. A geographic routing scheme usually combines a geographic greedy forwarding with a recovery mechanism to solve the local minima problem. Solutions proposed in the literature commonly combine greedy forwarding with the well known face routing for achieving this goal. However, the average path length in number of hops produced by face routing could be much worse than the optimal topological path in most realistic scenarios. In this paper, we propose a new intermediate procedure between the geographic greedy mode and the recovery mode in order to improve routing efficiency in number of hops, without network overhead. It exploits the optimal topological route to base stations, obtained by beacon messages, as a resource to find better routes than the ones created by face routing. We show by simulations that the proposed hybrid approach leads to a significant improvement of routing performance when applied to combined greedy-face routing algorithms. 
84|10||Adaptable Decentralized Service Oriented Architecture|
84|10||Formal analysis of an electronic voting system: An experience report|We have seen that several currently deployed e-voting systems share critical failures in their design and implementation that render their technical and procedural controls insufficient to guarantee trustworthy voting. The application of formal methods would greatly help to better address problems associated with assurance against requirements and standards. More specifically, it would help to thoroughly specify and analyze the underlying assumptions and security specific properties, and it would improve the trustworthiness of the final systems. In this article, we show how such techniques can be used to model and reason about the security of one of the currently deployed e-voting systems in the U.S.A named ES&S. We used the ASTRAL language to specify the voting process of ES&S machines and the critical security requirements for the system. Proof obligations that verify that the specified system meets the critical requirements were automatically generated by the ASTRAL Software Development Environment (SDE). The PVS interactive theorem prover was then used to apply the appropriate proof strategies and discharge the proof obligations. We also believe that besides analyzing the system against its requirements, it is equally important to perform an analysis under malicious circumstances where the execution model is augmented with attack behaviors. Thus, we extend the formal specification of the system by specifying attacks that have been shown to successfully compromise the system, and we then repeat the formal verification. This is helpful in detecting missing requirements or unwarranted assumptions about the specification of the system. In addition, this allows one to sketch countermeasure strategies to be used when the system behaves differently than it should and to build confidence about the system under development. Finally, we acknowledge the main problem that arises in e-voting system specification and verification: modeling attacks is very difficult because the different types of attack often cut across the structure of the original behavior models, thus making (incremental or compositional) verification very difficult. 
84|10||New and efficient knowledge discovery of partial periodic patterns with multiple minimum supports|
84|10||Confidential deniable authentication using promised signcryption|
84|10||An efficient CRT-RSA algorithm secure against power and fault attacks|
84|10||An aspect-oriented reference architecture for Software Engineering Environments|Reusable and evolvable Software Engineering Environments (SEEs) are essential to software production and have increasingly become a need. In another perspective, software architectures and reference architectures have played a significant role in determining the success of software systems. In this paper we present a reference architecture for SEEs, named RefASSET, which is based on concepts coming from the aspect-oriented approach. This architecture is specialized to the software testing domain and the development of tools for that domain is discussed. This and other case studies have pointed out that the use of aspects in RefASSET provides a better Separation of Concerns, resulting in reusable and evolvable SEEs. 
84|10||A meet-in-the-middle attack on reduced-round ARIA|
84|10||On software verification for sensor nodes|
84|10||Boosting adaptivity of fault-tolerant scheduling for real-time tasks with service requirements on clusters|
84|10||Provably secure and efficient authentication techniques for the global mobility network|
84|10||A general (k, n) scalable secret image sharing scheme with the smooth scalability|
84|10||Exploiting social networks to provide privacy in personalized web search|Web search engines (WSE) have become an essential tool for searching information on the Internet. In order to provide personalized search results for the users, WSEs store all the queries which have been submitted by the users and the search results which they have selected. The AOL scandal in 2006 proved that this information contains personally identifiable information which represents a privacy threat for the users who have generated it. In this way, AOL released a file containing twenty million queries made by 658,000 persons and several of those users were successfully tracked. In this paper, we propose a P2P protocol that exploits social networks in order to protect the privacy of the users from the profiling mechanisms of the WSEs. The proposed scheme has been designed considering the presence of users who do not follow the protocol (i.e., adversaries). In order to evaluate the privacy of the users, we have designed a new measure (the profile exposure level (PEL)). Finally, we have used the AOL’s file in order to simulate the behavior of our scheme with real queries which have been generated by real users. Our tests show that our scheme is usable in practice and that it preserves the privacy of the users even in the presence of adversaries. 
84|10||A grid-based coverage approach for target tracking in hybrid sensor networks|Most existing work on the coverage problem of wireless sensor networks focuses on improving the coverage of the whole sensing field. In target tracking, the interested coverage area is the emerging region of a motorized target, not the whole sensing field. As the motorized target moves, the emerging region is also dynamically changed. In this paper, we propose a grid-based and distributed approach for providing large coverage for a motorized target in a hybrid sensor network. The large coverage is achieved by moving mobile sensor nodes in the network. To minimize total movement cost, the proposed approach needs to solve the following problems: the minimum number of mobile sensor nodes used for healing coverage holes and the best matching between mobile sensor nodes and coverage holes. In the proposed approach, the above two problems are first transformed into the modified circle covering and minimum cost flow problems, respectively. Then, two polynomial-time algorithms are presented to efficiently solve these two modified graph problems, respectively. Finally, we perform simulation experiments to show the effectiveness of proposed approach in providing the coverage for a motorized target in a hybrid sensor network. 
84|10||Identification of extract method refactoring opportunities for the decomposition of methods|
84|10||Improving security of q-SDH based digital signatures|
84|10||Policy-based Awareness Management (PAM): Case study of a wireless communication system at a hospital|The present paper evaluates the use of software agents to identify relevance of information, called awareness. This evaluation is based on existing policies and scenarios in the context of wireless communication of a hospital in Norway. The study is to address the lack of literature for experimental studies on a method to employ software agents for awareness identification. Research in computer supported cooperative work indicates the significant contributions of software agents to assist individuals. There are bodies of work that show awareness provides the means for software agents in which effective cooperation can take place. In addition, the role of the methods to identify awareness is emphasized in the literature of both computer supported cooperative work and software agents. This paper explains a step-wise process, called Policy-based Awareness Management, which allows agents to use policies as a source to identify awareness and thus change their behaviors accordingly. The contribution of this method is based on the concepts proposed by the logic of general awareness. The present study applies Directory Enabled Networks-next generation as the policy structure for the method. The paper evaluates the process via its application to identify the relevance of information in wireless communication scenarios in a hospital. The present study conducts observations, interviews and discussions on the wireless communication system of the hospital to identify the different scenarios happening in the system. The paper presents a set of simulations on these scenarios and concludes that the method is effective and cost-efficient. 
84|10||Engineering the authoring of usable service front ends|
84|11|http://www.sciencedirect.com/science/journal/01641212/84/11|Mobile applications: Status and trends|
84|11||The âAlways Best Packet Switchingâ architecture for SIP-based mobile multimedia services|
84|11||Mobiiscape: Middleware support for scalable mobility pattern monitoring of moving objects in a large-scale city|
84|11||Dynamic deployment and quality adaptation for mobile augmented reality applications|
84|11||State of the art of frameworks and middleware for facilitating mobile and ubiquitous learning development|The emergence of mobile and ubiquitous technologies as important tools to complement formal learning has been accompanied by a growing interest in their educational benefits and applications. Mobile devices can be used to promote learning anywhere and anytime, to foster social learning and knowledge sharing, or to visualize augmented reality applications for learning purposes. However, the development of these applications is difficult for many researchers because it requires understanding many different protocols; dealing with distributed schemas, processes, platforms, and services; learning new programming languages; and interacting with different hardware sensors and drivers. For that reason, the use of frameworks and middleware that encapsulate part of this complexity appears to be fundamental to the further development of mobile learning projects. This study analyzes the state of the art of frameworks and middleware devoted to simplifying the development of mobile and ubiquitous learning applications. The results can be useful to many researchers involved in the development of projects using these technologies by providing an overview of the features implemented in each of these frameworks. 
84|11||A more efficient and secure ID-based remote mutual authentication with key agreement scheme for mobile devices on elliptic curve cryptosystem|Recently, Yang and Chang proposed an identity-based remote login scheme using elliptic curve cryptography for the users of mobile devices. We have analyzed the security aspects of the Yang and Chang's scheme and identified some security flaws. Also two improvements of the Yang and Chang's scheme have been proposed recently, however, it has been found that the schemes have similar security flaws as in the Yang and Chang's scheme. In order to remove the security pitfalls of the Yang and Chang and the subsequent schemes, we proposed an enhanced remote user mutual authentication scheme that uses elliptic curve cryptography and identity-based cryptosystem with three-way challenge-response handshake technique. It supports flawless mutual authentication of participants, agreement of session key and the leaked key revocation capability. In addition, the proposed scheme possesses low power consumption, low computation cost and better security attributes. As a result, the proposed scheme seems to be more practical and suitable for mobile users for secure Internet banking, online shopping, online voting, etc. 
84|11||A secure energy-efficient m-banking application for mobile devices|
84|11||Meetings through the cloud: Privacy-preserving scheduling on mobile devices|
84|11||A survey on privacy in mobile participatory sensing applications|The presence of multimodal sensors on current mobile phones enables a broad range of novel mobile applications. Environmental and user-centric sensor data of unprecedented quantity and quality can be captured and reported by a possible user base of billions of mobile phone subscribers worldwide. The strong focus on the collection of detailed sensor data may however compromise user privacy in various regards, e.g., by tracking a user’s current location. In this survey, we identify the sensing modalities used in current participatory sensing applications, and assess the threats to user privacy when personal information is sensed and disclosed. We outline how privacy aspects are addressed in existing sensing applications, and determine the adequacy of the solutions under real-world conditions. Finally, we present countermeasures from related research fields, and discuss their applicability in participatory sensing scenarios. Based on our findings, we identify open issues and outline possible solutions to guarantee user privacy in participatory sensing. 
84|11||Towards privacy-enhanced mobile communitiesâArchitecture, concepts and user trials|With the advent of mobile broadband technologies and capable mobile devices, social communities become a ubiquitous environment for people to stay in contact and share information with friends and fellows. This provides new opportunities for communities and their providers (e.g. regarding advertising) but also implies new question regarding the privacy and trust of their users. We argue that a balance needs to be found between these (partially) diverging interests and motivate, why a new approach to identity management and users privacy is necessary in this context. Based on requirements retrieved by real-life communities, we describe an architecture including privacy enhancing concepts and advanced privacy respecting advertising, which addresses such requirements. We further describe the architectures’ prototypical implementation, and present for the first time evaluation results based on user trials with two different mobile communities. 
84|11||Implementing collaborative learning activities in the classroom supported by one-to-one mobile computing: A design-based process|
84|11||Mobile applications in an aging society: Status and trends|Today, many countries, including several European states, the USA, and Japan, are aging; both the number and the percentage of elderly people are increasing. To create a cohesive and inclusive intergenerational society, technological products and services must be adapted to the needs and preferences of these citizens. Mobile phones are promising tools to improve the quality of life for the elderly. This work presents a review of the status of mobile functionalities and applications that can satisfy the requirements and needs of older people and improve their quality of life. This analysis of the state of the art enables us to identify the strengths and weaknesses of the current systems as well as discover trends and promising future lines of research. This paper outlines several needs that should be met to improve the quality of research in this area. This work provides a basis for researchers, designers, and mobile phone service providers to think about the existing needs of the elderly, the developing trends in the field and the opportunities that mobile applications offer to improve the quality of life of the elderly and to support a cohesive and inclusive society. 
84|11||Implementing multiplayer pervasive installations based on mobile sensing devices: Field experience and user evaluation from a public showcase|
84|11||Measuring air quality in city areas by vehicular wireless sensor networks|
84|11||Score optimization and template updating in a biometric technique for authentication in mobiles based on gestures|This article focuses on the evaluation of a biometric technique based on the performance of an identifying gesture by holding a telephone with an embedded accelerometer in his/her hand. The acceleration signals obtained when users perform gestures are analyzed following a mathematical method based on global sequence alignment. In this article, eight different scores are proposed and evaluated in order to quantify the differences between gestures, obtaining an optimal EER result of 3.42% when analyzing a random set of 40 users of a database made up of 80 users with real attempts of falsification. Moreover, a temporal study of the technique is presented leeding to the need to update the template to adapt the manner in which users modify how they perform their identifying gesture over time. Six updating schemes have been assessed within a database of 22 users repeating their identifying gesture in 20 sessions over 4 months, concluding that the more often the template is updated the better and more stable performance the technique presents. 
84|11||Status and trends of mobile-health applications for iOS devices: A developer's perspective|Modern smart mobile devices offer media-rich and context-aware features that are highly useful for electronic-health (e-health) applications. It is therefore not surprising that these devices have gained acceptance as target devices for e-health applications, turning them into m-health (mobile-health) apps. In particular, many e-health application developers have chosen Apple's iOS mobile devices such as iPad, iPhone, or iPod Touch as the target device to provide more convenient and richer user experience, as evidenced by the rapidly increasing number of m-health apps in Apple's App Store. In this paper, the top two hundred of such apps from the App Store were examined from a developer's perspective to provide a focused overview of the status and trends of iOS m-health apps and an analysis of related technology, architecture, and user interface design issues. The top 200 apps were classified into different groups according to their purposes, functions, and user satisfaction. It was shown that although the biggest group of apps was medical information reference apps that were delivered from or related to medical articles, websites, or journals, mobile users disproportionally favored tracking tools. It was clear that m-health apps still had plenty of room to grow to take full advantage of unique mobile platform features and truly fulfill their potential. In particular, introduction of two- or three-dimensional visualization and context-awareness could further enhance m-health app's usability and utility. This paper aims to serve as a reference point and guide for developers and practitioners interested in using iOS as a platform for m-health applications, particular from the technical point of view. 
84|12|http://www.sciencedirect.com/science/journal/01641212/84/12|A feature-based approach for modeling role-based access control systems|
84|12||An automated approach to reducing test suites for testing retargeted C compilers for embedded systems|
84|12||Examining the influences of external expertise and in-house computer/IT knowledge on ERP system success|External expertise and adequate levels of internal computer skills and knowledge are essential factors that can contribute to the success of complex information technology (IT) systems, including enterprise resource planning (ERP). Studies examining the effects of external expertise and in-house or internal computer/IT knowledge on the success of ERP packages are rare. This present study was designed to fill this gap in research. A relevant research model was developed to test fifteen (15) hypothesized paths or relationships among the study's variables. Data was collected in a cross-sectional field survey of 109 firms in two European countries. The partial least squares (PLS) technique was used for data analysis. The PLS results supported eleven (11) out of the fifteen (15) hypotheses. Essentially, this research's results confirmed that external expertise (an exogenous factor) and internal computer/IT knowledge (endogenous factors) are pertinent to success enhancement of ERP system success for adopting organizations. The implications of the findings for both practice and research are discussed, and possible areas of future research identified. 
84|12||Is there convergence in the field of UI generation?|
84|12||A fast, GPU based, dictionary attack to OpenPGP secret keyrings|
84|12||Modelling and analysis of pipelined circuit switching in interconnection networks with bursty traffic and hot-spot destinations|
84|12||Trust-based minimum cost opportunistic routing for Ad hoc networks|Recently, opportunistic routing has received much attention as a new design direction. It can exploit the wireless broadcast and more highly reliable opportunistic forwarding, so as to substantially increase the throughput of network. Due to dynamic topology, distributed collaboration, limited bandwidth and computing ability, the absence of enough physical protection in Ad hoc networks, opportunistic routing is vulnerable to attacks by malicious nodes. In order to alleviate the malicious behaviors, we incorporate the concept of trust to Ad hoc networks, build a simple trust model to evaluate neighbors’ forwarding behavior and apply this model to opportunistic routing for Ad hoc networks. A new trusted opportunistic forwarding model is proposed by choosing the trusted and highest priority candidate forwarder, then a trusted minimum cost routing algorithm (MCOR) is formally formulated, the correctness and effectiveness of this algorithm from theoretical analysis are also approved. Finally, MCOR algorithm is verified by simulation using nsclick software and compared its performance with the classic protocols: ExOR, TAODV and Watchdog-DSR. The simulation results show that MCOR scheme can detect and mitigate node misbehaviors. Furthermore, MCOR scheme outperforms the other protocols in terms of: throughput, delay, Expected ETX, security-gains and cost of routing. 
84|12||Evaluating the impacts of dynamic reconfiguration on the QoS of running systems|
84|12||Scalable and efficient web services composition based on a relational database|
84|12||Performance evaluation of noncontiguous allocation algorithms for 2D mesh interconnection networks|
84|12||A variable strength interaction test suites generation strategy using Particle Swarm Optimization|
84|12||Communication-efficient leader election in crashârecovery systems|This work addresses the leader election problem in partially synchronous distributed systems where processes can crash and recover. More precisely, it focuses on implementing the Omega failure detector class, which provides a leader election functionality, in the crash–recovery failure model. The concepts of communication efficiency and near-efficiency for an algorithm implementing Omega are defined. Depending on the use or not of stable storage, the property satisfied by unstable processes, i.e., those that crash and recover infinitely often, varies. Two algorithms implementing Omega are presented. In the first algorithm, which is communication-efficient and uses stable storage, eventually and permanently unstable processes agree on the leader with correct processes. In the second algorithm, which is near-communication-efficient and does not use stable storage, processes start their execution with no leader in order to avoid the disagreement among unstable processes, that will agree on the leader with correct processes after receiving a first message from the leader. 
84|12||A high quality image sharing with steganography and adaptive authentication scheme|
84|12||A genetic algorithm for optimized feature selection with resource constraints in software product lines|Software product line (SPL) engineering is a software engineering approach to building configurable software systems. SPLs commonly use a feature model to capture and document the commonalities and variabilities of the underlying software system. A key challenge when using a feature model to derive a new SPL configuration is determining how to find an optimized feature selection that minimizes or maximizes an objective function, such as total cost, subject to resource constraints. To help address the challenges of optimizing feature selection in the face of resource constraints, this paper presents an approach that uses G enetic A lgorithms for optimized FE ature S election (GAFES) in SPLs. Our empirical results show that GAFES can produce solutions with 86–97% of the optimality of other automated feature selection algorithms and in 45–99% less time than existing exact and heuristic feature selection techniques. 
84|12||Evolutionary generation of test data for many paths coverage based on grouping|Path-oriented test data generation is an important issue of software testing, but the efficiency of existing methods needs to be further improved. We focus on the problem of generating test data for many paths coverage, and present a method of evolutionary generation of test data for many paths coverage based on grouping. First, target paths are divided into several groups according to their similarities, and each group forms a sub-optimization problem, which transforms a complicated optimization problem into several simpler sub-optimization problems; then a domain-based fitness is designed when genetic algorithms are employed to solve these problems; finally, these sub-optimization problems are simplified along with the process of generating test data, hence improving the efficiency of generating test data. Having analyzed the performance of our method theoretically, we apply it in some typical programs under test, and compare it with some previous methods. The experimental results show that our method has advantage in the number of evaluations and uncovered target paths. 
84|12||ReuseToolâAn extensible tool support for object-oriented framework reuse|
84|12||Checking enforcement of integrity constraints in database applications based on code patterns|Integrity constraints (including key, referential and domain constraints) are unique features of database applications. Integrity constraints are crucial for ensuring accuracy and consistency of data in a database. It is important to perform integrity constraint enforcement (ICE) at the application level to reduce the risk of database corruption. We have conducted an empirical analysis of open-source PHP database applications and found that ICE does not receive enough attention in real-world programming practice. We propose an approach for automatic detection of ICE violations at the application level based on identification of code patterns. We define four patterns that characterize the structures of code implementing integrity constraint enforcement. Violations of these patterns indicate the missing of integrity constraint enforcement. Our work contributes to quality improvement of database applications. Our work also demonstrates that it is feasible to effectively identify bugs or problematic code by mining code patterns in a specific domain/application area. 
84|12||An empirical investigation on the reusability of design patterns and software packages|Nowadays open-source software communities are thriving. Successful open-source projects are competitive and the amount of source code that is freely available offers great reuse opportunities to software developers. Thus, it is expected that several requirements can be implemented based on open source software reuse. Additionally, design patterns, i.e. well-known solution to common design problems, are introduced as elements of reuse. This study attempts to empirically investigate the reusability of design patterns, classes and software packages. Thus, the results can help developers to identify the most beneficial starting points for white box reuse, which is quite popular among open source communities. In order to achieve this goal we conducted a case study on one hundred (100) open source projects. More specifically, we identified 27,461 classes that participate in design patterns and compared the reusability of each of these classes with the reusability of the pattern and the package that this class belongs to. In more than 40% of the cases investigated, design pattern based class selection, offers the most reusable starting point for white-box reuse. However there are several cases when package based selection might be preferable. The results suggest that each pattern has different level of reusability. 
84|12||Delegatable secret handshake scheme|
84|12||Interactive conditional proxy re-encryption with fine grain policy|Conditional proxy re-encryption (C-PRE) allows a semi-trusted proxy to convert a ciphertext satisfying one conditional set by sender into an encryption of the same message intended for a different recipient than the one that was originally intended to. In ISC 2009, Weng, Yang, Tang, Deng, and Bao proposed an efficient CCA secure C-PRE scheme, and left an open problem on how to construct CCA-secure C-PRE schemes supporting “OR” and “AND” gates over conditions. In this paper, we made the first attempt in constructing C-PRE schemes with richer policy, and hence addressing the problem raised by Weng et al. Nevertheless, our scheme is an interactive scheme. The ‘interactive setting’ used in our scheme refers to the case where the re-encryption key generation algorithm requires the involvement of the private key of the delegator and delegatee. As a consequence, we call our new cryptographic primitive as interactive conditional proxy re-encryption with fine grain policy (ICPRE-FG). This notion basically enhances the notion of PRE by enabling the features from the attribute-based encryption (ABE). That means, our ICPRE-FG has been constructed from a careful combination of the existing PRE and ABE techniques. In an ICPRE-FG system, each ciphertext is labeled by the delegator with a set of descriptive conditions and each re-encryption key is associated with an access tree that specifies which type of ciphertexts the key can re-encrypt. We formalize the security model of ICPRE-FG, and then we present a new and efficient construction of ICPRE-FG scheme with CCA-security under the well-studied assumption in the random oracle model. 
84|12||Tensor Field Model for higher-order information retrieval|
84|12||Design of a Java spatial extension for relational databases|Jaspa (Java Spatial) is a novel spatial extension for relational database management systems (RDBMSs). It is the result of a research project that aims to accomplish two goals: firstly, to fill the absence in the Free and Open Source Software (FOSS) world of a solid Java-based alternative to PostGIS, the leading spatial extension written in C. Secondly, to exploit the advantages of Java and the Java geospatial libraries over C in terms of portability and easiness to extend. Java programming for geospatial purposes is a flowering field and similar solutions to Jaspa have recently appeared, but none of them can equate with PostGIS due to lack of functionalities. Jaspa currently implements almost all PostGIS functionality. The next step will be the enrichment of the software with more sophisticated features: storage of spatial data in a topological structure within the RDBMS, cluster tolerance and geodetic functionalities. Jaspa is being developed at the Department of Cartographic Engineering, Geodesy and Photogrammetry of the Universidad Politécnica de Valencia and it has been published under an Open Source license on the OSOR.eu repository. This paper has been written by its creators with the aim of introducing users to its main capabilities. 
84|12||Zero-laxity based real-time multiprocessor scheduling|
84|12||Understanding the relevance of micro-structures for design patterns detection|One important issue concerning design patterns in reverse engineering is their detection to support program comprehension, design recovery, system (re-)documentation, and software evolution. The objectives of this paper are to identify and analyze different types of building blocks of design patterns and to evaluate if the detection of these building blocks (called micro-structures) is relevant for the detection of occurrences of the design patterns. This analysis is useful to understand how the different types of micro-structures can be combined to better comprehend design patterns and to improve their detection. To achieve the objectives, the paper provides a description of different micro-structures, an analysis of their relevance in different design motifs, and a statistical analysis on the number and types of micro-structures present in different design patterns. Finally, we investigate if the detection of some design patterns can be performed only through the detection of a combined set of micro-structures, or other techniques should be exploited. 
84|12||P2P-based multidimensional indexing methods: A survey|
84|12||How do we measure and improve the quality of a hierarchical ontology?|
84|12||Adjusting Fuzzy Similarity Functions for use with standard data mining tools|
84|2|http://www.sciencedirect.com/science/journal/01641212/84/2|Organizational structures supported by agent-oriented methodologies|
84|2||Toward architecture-based context-aware deployment and adaptation|
84|2||One-time signature scheme from syndrome decoding over generic error-correcting codes|
84|2||A context-aware reflective middleware framework for distributed real-time and embedded systems|
84|2||Cryptanalysis of an (hierarchical) identity based parallel key-insulated encryption scheme|
84|2||Adjusting transport segmentation policy of DTN Bundle Protocol under synergy with lower layers|We assess Delay-Tolerant Network (DTN) performance in space under the scope of adjusting protocol data unit (PDU) size at various layers. We quantify the importance of combinatively adjusting size of DTN bundles, transport packets, and link frames. Through simulations, our paper reveals trade-offs that involve file delivery time, transmission effort of sending nodes, and memory resources release rate. Based on our findings, we propose a transport adaptation scheme that dynamically adjusts DTN bundle and transport packet size by means of heuristic search. To our knowledge, this is the first study to examine transport segmentation policy and interaction among various layers of the DTN protocol stack. 
84|2||Antecedents to IT personnel's intentions to leave: A systematic literature review|
84|2||Taxonomy and classification of automatic monitoring of program security vulnerability exploitations|Software applications (programs) are implemented in a wide variety of languages and run on different execution environments. Programs contain vulnerabilities which can be detected before their deployment. Nevertheless, there exist some program vulnerabilities, which do not surface until a program is operational. No matter how much effort has been put during the development phases, building large vulnerability-free programs has proven extremely difficult in practice. Given that, it is very important to have a tool that can be used for online monitoring of programs in the operational stage. The tool can help to mitigate the consequences of some vulnerability exploitations, by early detection of attacks at runtime. Currently, many monitoring approaches have been proposed and applied in practice. However, there is no classification of these approaches to understand their common characteristics and limitations. In this paper, we present a taxonomy and classification of the state of the art approaches employed for monitoring program vulnerability exploitations (or attacks). We first classify the existing approaches based on a set of characteristics which are common in online attack detection approaches. Then, we present a taxonomy by classifying the approaches based on monitoring aspects that primarily differentiate among the approaches. We also discuss open issues and future research direction in the area of program vulnerability exploitation monitoring. The study will enable practitioners and researchers to differentiate among existing monitoring approaches. It will provide a guideline to consider the desired characteristics while developing monitoring approaches. 
84|2||Analogy-based software effort estimation using Fuzzy numbers|BackgroundEarly stage software effort estimation is a crucial task for project bedding and feasibility studies. Since collected data during the early stages of a software development lifecycle is always imprecise and uncertain, it is very hard to deliver accurate estimates. Analogy-based estimation, which is one of the popular estimation methods, is rarely used during the early stage of a project because of uncertainty associated with attribute measurement and data availability.AimsWe have integrated analogy-based estimation with Fuzzy numbers in order to improve the performance of software project effort estimation during the early stages of a software development lifecycle, using all available early data. Particularly, this paper proposes a new software project similarity measure and a new adaptation technique based on Fuzzy numbers.MethodEmpirical evaluations with Jack-knifing procedure have been carried out using five benchmark data sets of software projects, namely, ISBSG, Desharnais, Kemerer, Albrecht and COCOMO, and results are reported. The results are compared to those obtained by methods employed in the literature using case-based reasoning and stepwise regression.ResultsIn all data sets the empirical evaluations have shown that the proposed similarity measure and adaptation techniques method were able to significantly improve the performance of analogy-based estimation during the early stages of software development. The results have also shown that the proposed method outperforms some well know estimation techniques such as case-based reasoning and stepwise regression.ConclusionsIt is concluded that the proposed estimation model could form a useful approach for early stage estimation especially when data is almost uncertain. 
84|2||Key activities for product derivation in software product lines|
84|2||Bridging metamodels and ontologies in software engineering|Over the last several years, metamodels and ontologies have been developed in parallel isolation. Ontological thinking, largely from the research field of artificial intelligence, has been increasingly investigated by software engineering researchers, more familiar with the idea of a metamodel. Here, we investigate the literature on both metamodelling and ontologies in order to identify ways in which they can be made compatible and linked in such a way as to benefit both communities and create a contribution to a coherent underpinning theory for software engineering. Analysis of a large number of theoretical and semi-theoretical approaches using as a framework a multi-level modelling construct identifies strengths, weaknesses, incompatibilities and inconsistencies within the extant literature. A metamodel deals with conceptual definitions while an ontology deals with real-world descriptors of business entities and is thus better named “domain ontology”. A specific kind of ontology (foundational or high-level) provides “metalevel” concepts for the domain ontologies. In other words, a foundational ontology may be used at the same abstraction level as a metamodel and a domain ontology at the same abstraction level as a (design) model, with each pair linked via an appropriate semantic mapping. 
84|2||Distributed adaptive top-k monitoring in wireless sensor networks|
84|2||User requirements modeling and analysis of software-intensive systems|The increasing complexity of software systems makes Requirements Engineering activities both more important and more difficult. This article is about user requirements development, mainly the activities of documenting and analyzing user requirements for software-intensive systems. These are modeling activities that are useful for further Requirements Engineering activities. Current techniques for requirements modeling present a number of problems and limitations. Based on these shortcomings, a list of requirements for requirements modeling languages is proposed. The proposal of this article is to show how some extensions to SysML diagrams and tables can fulfill most of these requirements. The approach is illustrated by a list of user requirements for a Road Traffic Management System. 
84|3|http://www.sciencedirect.com/science/journal/01641212/84/3|Medical image security and EPR hiding using Shamir's secret sharing scheme|
84|3||A novel statistical time-series pattern based interval forecasting strategy for activity durations in workflow systems|
84|3||Typical Virtual Appliances: An optimized mechanism for virtual appliances provisioning and management|
84|3||Huffman-code strategies to improve MFCVQ-based reversible data hiding for VQ indexes|
84|3||Identifying Extract Class refactoring opportunities using structural and semantic cohesion measures|
84|3||A systematic literature review of software quality cost research|Software quality costs have not received as much attention from the research community as other economic aspects of software development. Over the last three decades, a number of articles on this topic have appeared in a range of journals, but comprehensive overviews of this body of research are not available.For the detailed review of software quality cost research presented in this article, we collect 87 articles published between 1980 and 2009 in 60 leading computing journals. We study the distribution of these articles across research disciplines and journals as well as over time. Moreover, we identify the predominant researchers in the software quality cost domain and the related research clusters. We also classify the articles according to three properties, namely, research topic, research scope, and research approach. This categorization enables us to identify aspects emphasized by previous research on software quality costs and to point out promising future research directions. Our review shows that prevention costs have gained the least attention, in spite of their big cost impact. It also reveals that only one article has targeted multiple companies. Further, we observe that many articles do not empirically validate their findings. This is especially true for those articles dealing with an entire firm. 
84|3||Adaptive reversible image watermarking scheme|
84|3||Mining significant factors affecting the adoption of SaaS using the rough set approach|Despite that Software as a Service (SaaS) seems to be the most tempting solution among different types of cloud services, yet it has not been adopted to-date with as much alacrity as was originally expected. A variety of factors may influence the adoption of SaaS solutions. The objective of this study is thus to explore the significant factors affecting the adoption of SaaS for vendors and enterprise users. An analytical framework is proposed containing two approaches—Technology Acceptance Model (TAM) and Rough Set Theory (RST). An empirical study on the IT/MIS enterprises in Taiwan is carried out. The results have revealed a considerable amount of meaningful information, which not only facilitates the SaaS vendors to grasp users’ needs and concerns about SaaS adoption, but also helps the managers to introduce effective marketing strategies and actions to promote the growth of SaaS market. Based on the findings, some managerial implications are discussed. 
84|3||A comparison of deterministic and probabilistic methods for indoor localization|
84|3||Missing data imputation by utilizing information within incomplete instances|This paper proposes to utilize information within incomplete instances (instances with missing values) when estimating missing values. Accordingly, a simple and efficient nonparametric iterative imputation algorithm, called the NIIA method, is designed for iteratively imputing missing target values. The NIIA method imputes each missing value several times until the algorithm converges. In the first iteration, all the complete instances are used to estimate missing values. The information within incomplete instances is utilized since the second imputation iteration. We conduct some experiments for evaluating the efficiency, and demonstrate: (1) the utilization of information within incomplete instances is of benefit to easily capture the distribution of a dataset; and (2) the NIIA method outperforms the existing methods in accuracy, and this advantage is clearly highlighted when datasets have a high missing ratio. 
84|3||Oblivious transfer with timed-release receiverâs privacy|
84|3||Resource discovery in a Grid system: Directing requests to trustworthy virtual organizations based on global trust values|
84|3||Software engineering education: A study on conducting collaborative senior project development|
84|3||A novel general framework for automatic and cost-effective handling of recoverable temporal violations in scientific workflow systems|Due to the complex nature of scientific workflow environments, temporal violations often take place and may severely reduce the timeliness of the execution's results. To handle temporal violations in an automatic and cost-effective fashion, two interdependent fundamental issues viz. the definition of fine-grained recoverable temporal violations and the design of light-weight effective exception handling strategies need to be resolved. However, most existing works study them separately without defining a comprehensive framework. To address such a problem, with the probability based temporal consistency model which defines the range of recoverable temporal violations, a novel general automatic and cost-effective exception handling framework is proposed in this paper where fine-grained temporal violations are defined based on the empirical function for the capability lower bounds of the exception handling strategies. To serve as a representative case study, a concrete example exception handling framework which consists of three levels of fine-grained temporal violations and their corresponding exception handling strategies is presented. The effectiveness of the example framework is evaluated by large scale simulation experiments conducted in the SwinDeW-G scientific grid workflow system. The experimental results demonstrate that the example framework can significantly reduce the overall average violation rates of local temporal constraints and global temporal constraints to 0.127% and 0.167% respectively. 
84|3||Self-adapting workflow reconfiguration|
84|3||Publisher's note|
84|3||Publisher's note|
84|4|http://www.sciencedirect.com/science/journal/01641212/84/4|Special issue on the best papers of QSIC 2009|
84|4||An approach to analyzing the software process change impact using process slicing and simulation|
84|4||Testing and validating machine learning classifiers by metamorphic testing|
84|4||BDTEX: A GQM-based Bayesian approach for the detection of antipatterns|
84|4||Simultaneous debugging of software faults|
84|4||On the estimation of adequate test set size using fault failure rates|Test set size in terms of the number of test cases is an important consideration when testing software systems. Using too few test cases might result in poor fault detection and using too many might be very expensive and suffer from redundancy. We define the failure rate of a program as the fraction of test cases in an available test pool that result in execution failure on that program. This paper investigates the relationship between failure rates and the number of test cases required to detect the faults. Our experiments based on 11 sets of C programs suggest that an accurate estimation of failure rates of potential fault(s) in a program can provide a reliable estimate of adequate test set size with respect to fault detection and should therefore be one of the factors kept in mind during test set construction. Furthermore, the model proposed herein is fairly robust to incorrect estimations in failure rates and can still provide good predictive quality. Experiments are also performed to observe the relationship between multiple faults present in the same program using the concept of a failure rate. When predicting the effectiveness against a program with multiple faults, results indicate that not knowing the number of faults in the program is not a significant concern, as the predictive quality is typically not affected adversely. 
84|4||XML-manipulating test case prioritization for XML-manipulating services|A web service may evolve autonomously, making peer web services in the same service composition uncertain as to whether the evolved behaviors are compatible with its original collaborative agreement. Although peer services may wish to conduct regression testing to verify the agreed collaboration, the source code of the former service may be inaccessible to them. Owing to the black-box nature of peer services, traditional code-based approaches to regression testing are inapplicable. In addition, traditional techniques assume that a regression test suite for verifying a web service is available. The location to store a regression test suite is also a problem. On the other hand, we note that the rich interface specifications of a web service provide peer services with a means to formulate black-box testing strategies. In this paper, we provide a strategy for black-box service-oriented testing. We also formulate new test case prioritization strategies using tags embedded in XML messages to reorder regression test cases, and reveal how the test cases use the interface specifications of web services. We experimentally evaluate the effectiveness of these black-box strategies in revealing regression faults in modified WS-BPEL programs. The results show that the new techniques can have a high chance of outperforming random ordering. Moreover, our experiment shows that prioritizing test cases based on WSDL tag coverage can achieve a smaller variance than that based on the number of tags in XML messages in regression test cases, even though their overall fault detection rates are similar. 
84|4||Selection of third party software in Off-The-Shelf-based software developmentâAn interview study with industrial practitioners|The success of software development using third party components highly depends on the ability to select a suitable component for the intended application. The evidence shows that there is limited knowledge about current industrial OTS selection practices. As a result, there is often a gap between theory and practice, and the proposed methods for supporting selection are rarely adopted in the industrial practice. This paper's goal is to investigate the actual industrial practice of component selection in order to provide an initial empirical basis that allows the reconciliation of research and industrial endeavors. The study consisted of semi-structured interviews with 23 employees from 20 different software-intensive companies that mostly develop web information system applications. It provides qualitative information that help to further understand these practices, and emphasize some aspects that have been overlooked by researchers. For instance, although the literature claims that component repositories are important for locating reusable components; these are hardly used in industrial practice. Instead, other resources that have not received considerable attention are used with this aim. Practices and potential market niches for software-intensive companies have been also identified. The results are valuable from both the research and the industrial perspectives as they provide a basis for formulating well-substantiated hypotheses and more effective improvement strategies. 
84|4||A policy-based publish/subscribe middleware for sense-and-react applications|
84|4||Bringing white-box testing to Service Oriented Architectures through a Service Oriented Approach|
84|4||A data hiding scheme using the varieties of pixel-value differencing in multimedia images|In this paper, a capacity promoting technique is proposed for embedding data in an image using pixel-value differencing (PVD). The PVD scheme embeds data by changing the difference value between two adjacent pixels so that more data is embedded into two pixels located in the edge area, than in the smooth area. In order to increase the embedding capacity, a new approach is proposed in this paper by searching edge area more flexibly. Instead of processing a pair of pixels at a time as proposed by Wu and Tsai, two pairs of pixels in a block are processed at the same time. In addition, we proposed a pixel-value shifting scheme to further increase the chances for embedding data. Our scheme exploits the edge areas more efficiently, thus leading to an increase in embedding capacity as shown by experimental results compared to Wu and Tsai's method. Also, the embedding result of our scheme passes the Fridrich et al.’s detection. Besides, according to the distribution of difference values, more practical range partitions are suggested for improving capacity. 
84|4||Secure key management scheme for dynamic hierarchical access control based on ECC|An access control mechanism in a user hierarchy is used to provide the management of sensitive information for authorized users. The users and their own information can be organized into a number of disjoint sets of security classes according to their responsibilities. Each security class in a user hierarchy is assigned an encryption key and can derive the encryption keys of all lower security classes according to predefined partially ordered relation. In 2006, Jeng and Wang proposed an efficient key management scheme based on elliptic curve cryptosystems. This paper, however, pointed out that Jeng–Wang scheme is vulnerable to the so-called compromising attack that the secret keys of some security classes can be compromised by any adversary if some public information modified. We further proposed a secure key management scheme based on elliptic curve cryptosystems to eliminate the pointed out the security leak and provide better security requirements. As compared with Jeng and Wang's scheme (Jeng and Wang, 2006), the proposed scheme has the following properties. (i) It is simple to execute the key generation and key derivation phases. (ii) It is easily to address dynamic access control when a security class is added into or deleted from the hierarchy. (iii) It is secure against some potential attacks. (iv) The required storage of the public/secret parameters is constant. 
84|4||Factors influencing clients in the selection of offshore software outsourcing vendors: An exploratory study using a systematic literature review|ContextOffshore software development outsourcing is a modern business strategy for developing high quality software at low cost.ObjectiveThe objective of this research paper is to identify and analyse factors that are important in terms of the competitiveness of vendor organisations in attracting outsourcing projects.MethodWe performed a systematic literature review (SLR) by applying our customised search strings which were derived from our research questions. We performed all the SLR steps, such as the protocol development, initial selection, final selection, quality assessment, data extraction and data synthesis.ResultsWe have identified factors such as cost-saving, skilled human resource, appropriate infrastructure, quality of product and services, efficient outsourcing relationships management, and an organisation's track record of successful projects which are generally considered important by the outsourcing clients. Our results indicate that appropriate infrastructure, cost-saving, and skilled human resource are common in three continents, namely Asia, North America and Europe. We identified appropriate infrastructure, cost-saving, and quality of products and services as being common in three types of organisations (small, medium and large). We have also identified four factors-appropriate infrastructure, cost-saving, quality of products and services, and skilled human resource as being common in the two decades (1990–1999 and 2000–mid 2008).ConclusionsCost-saving should not be considered as the driving factor in the selection process of software development outsourcing vendors. Vendors should rather address other factors in order to compete in the OSDO business, such as skilled human resource, appropriate infrastructure and quality of products and services. 
84|4||Supporting real-time supply chain decisions based on RFID data streams|
84|5|http://www.sciencedirect.com/science/journal/01641212/84/5|Supporting runtime software architecture: A bidirectional-transformation-based approach|
84|5||Real-time perceptual watermarking architectures for video broadcasting|Existing secure embedded systems are primarily cryptography based. However, for effective Digital Rights Management (DRM) of multimedia in the framework of embedded systems, both watermarking and cryptography are necessary. In this paper, a watermarking algorithm and corresponding VLSI architectures are presented that will insert a broadcaster's logo into video streams in real-time to facilitate copyrighted video broadcasting and Internet protocol television (IP-TV). The VLSI architecture is prototyped using a hardware description language (HDL) and when realized in silicon can be deployed in any multimedia producing consumer electronics equipment to enable real-time DRM right at the source. The watermark is inserted into the video stream before MPEG-4 compression, resulting in simplified hardware requirements and superior video quality. The watermarking processing is performed in the frequency (DCT) domain. The system is initially simulated and validated in MATLAB/Simulink® and subsequently prototyped on an Altera® Cyclone-II FPGA using VHDL. Its maximum throughput is 43 frames/s at a clock speed of 100 MHz which makes it suitable for emerging real-time digital video broadcasting applications such as IP-TV. The watermarked video is of high quality, with an average Peak-Signal-to-Noise Ratio (PSNR) of 21.8 dB and an average Root-Mean-Square Error (RMSE) of 20.6. 
84|5||A model of job satisfaction for collaborative development processes|Modern software development relies on collaborative work as a means for sharing knowledge, distributing tasks and responsibilities, reducing risk of failures, and increasing the overall quality of the software product. Such objectives are achieved with a continuous share of the programmers’ daily working life that inevitably influences the programmers’ job satisfaction. One of the major challenges in process management is to determine the causes of this satisfaction. Traditional research models job satisfaction with social aspects of collaborative work like communication, work sustainability, and work environment.This study reflects on existing models of job satisfaction in collaborative environments, creates one for modern software development processes, and validates it with a retrospective comparative survey run on a sample of 108 respondents. In addition, the work investigates the impact on job satisfaction and its model of the agile practice of Pair Programming that pushes job sharing to the extreme. With this intent, the questionnaire also collected feedback from pair programmers whose responses were used for a comparative analysis. The results demonstrate that Pair Programming has actually a strong positive effect on satisfaction, work sustainability, and communication. 
84|5||Shape analysis for power signal cryptanalysis on secure components|This paper presents an application of pattern recognition techniques in reverse engineering for smart cards. The aim of the study is to design algorithms based on shape classification and to determine instructions executed on a chip as well as processed data sets. Information is extracted from the power consumption in order to recover secret information. Then geometrical features are determined and a syntactic analysis is achieved in order to recover secret algorithms and data. Some examples are given showing how code execution can be reversed on a recent secure component. These examples are essentially focused on instruction recovery but the algorithms also work on data recovery or on a combination of both instruction and data recovery. 
84|5||Periphery deployment for wireless sensor systems with guaranteed coverage percentage|
84|5||The optimization of success probability for software projects using genetic algorithms|
84|5||Advanced quality prediction model for software architectural knowledge sharing|In the field of software architecture, a paradigm shift is occurring from describing the outcome of architecting process to describing the Architectural Knowledge (AK) created and used during architecting. Many AK models have been defined to represent domain concepts and their relationships, and they can be used for sharing and reusing AK across organizations, especially in geographically distributed contexts. However, different AK domain models can represent concepts that are different, thereby making effective AK sharing challenging. In order to understand the mapping quality from one AK model to another when more than one AK model coexists, AK sharing quality prediction based on the concept differences across AK models is necessary. Previous works in this area lack validation in the actual practice of AK sharing. In this paper, we carry out validation using four AK sharing case studies. We also improve the previous prediction models. We developed a new advanced mapping quality prediction model, this model (i) improves the prediction accuracy of the recall rate of AK sharing quality; (ii) provides a better balance between prediction effort and accuracy for AK sharing quality. 
84|5||Secret image sharing with authentication-chaining and dynamic embedding|
84|5||Dynamic adaptation of response-time models for QoS management in autonomic systems|
84|5||Exploring implicit parallelism in class diagrams|
84|5||Reliability-driven deployment optimization for embedded systems|One of the crucial aspects that influence reliability of embedded systems is the deployment of software components to hardware nodes. If the hardware architecture is designed prior to the customized software architecture, which is often the case in product-line manufacturing (e.g. in the automotive domain), the system architect needs to resolve a nontrivial task of finding a (near-)optimal deployment balancing the reliabilities of individual services implemented on the software level.In this paper, we introduce an approach to automate this task. As distinct to related approaches, which typically stay with quantification of reliability for a specific deployment, we target multi-criteria optimization and provide the architect with near-optimal (non-dominated) deployment alternatives with respect to service reliabilities. Toward this goal, we annotate the software and hardware architecture with necessary reliability-relevant attributes, design a method to quantify the quality of individual deployment alternatives, and implement the approach employing an evolutionary algorithm. 
84|5||A design pattern coupling role and component concepts: Application to medical software|
84|5||Load and storage balanced posting file partitioning for parallel information retrieval|Many recent major search engines on Internet use a large-scale cluster to store a large database and cope with high query arrival rate. To design a large scale parallel information retrieval system, both performance and storage cost has to be taken into integrated consideration. Moreover, a quantitative method to design the cluster in systematical way is required. This paper proposes posting file partitioning algorithm for these requirements. The partitioning follows the partition-by-document-ID principle to eliminate communication overhead. The kernel of the partitioning is a data allocation algorithm to allocate variable-sized data items for both load and storage balancing. The data allocation algorithm is proven to satisfy a load balancing constraint with asymptotical 1-optimal storage cost. A probability model is established such that query processing throughput can be calculated from keyword popularities and data allocation result. With these results, we show a quantitative method to design a cluster systematically. This research provides a systematical approach to large-scale information retrieval system design. This approach has the following features: (1) the differences to ideal load balancing and storage balancing are negligible in real-world application. (2) Both load balancing and storage balancing can be taken into integrated consideration without conflicting. (3) The data allocation algorithm is capable to deal with data items of variable-sizes and variable loads. An algorithm having all these features together is never achieved before and is the key factor for achieving load and storage balanced workstation cluster in a real-world environment. 
84|6|http://www.sciencedirect.com/science/journal/01641212/84/6|Non-parametric statistical fault localization|Fault localization is a major activity in program debugging. To automate this time-consuming task, many existing fault-localization techniques compare passed executions and failed executions, and suggest suspicious program elements, such as predicates or statements, to facilitate the identification of faults. To do that, these techniques propose statistical models and use hypothesis testing methods to test the similarity or dissimilarity of proposed program features between passed and failed executions. Furthermore, when applying their models, these techniques presume that the feature spectra come from populations with specific distributions. The accuracy of using a model to describe feature spectra is related to and may be affected by the underlying distribution of the feature spectra, and the use of a (sound) model on inapplicable circumstances to describe real-life feature spectra may lower the effectiveness of these fault-localization techniques. In this paper, we make use of hypothesis testing methods as the core concept in developing a predicate-based fault-localization framework. We report a controlled experiment to compare, within our framework, the efficacy, scalability, and efficiency of applying three categories of hypothesis testing methods, namely, standard non-parametric hypothesis testing methods, standard parametric hypothesis testing methods, and debugging-specific parametric testing methods. We also conduct a case study to compare the effectiveness of the winner of these three categories with the effectiveness of 33 existing statement-level fault-localization techniques. The experimental results show that the use of non-parametric hypothesis testing methods in our proposed predicate-based fault-localization model is the most promising. 
84|6||An efficient shuffling based eVoting scheme|
84|6||A novel image watermarking in redistributed invariant wavelet domain|
84|6||A web search-centric approach to recommender systems with URLs as minimal user contexts|
84|6||A synergistic model-driven approach for persistence modeling with UML|
84|6||Software product roadmapping in a volatile business environment|
84|6||Function point measurement from Web application source code based on screen transitions and database accesses|A function point (FP) is a unit of measurement that expresses the degree of functionality that an information system provides to a user. Many software organizations use FPs to estimate the effort required for software development. However, it is essential that the definition of 1 FP be based on the software development experience of the organization. In the present study, we propose a method by which to automatically extract data and transaction functions from Web applications under several conditions using static analysis. The proposed method is based on the International Function Point Users Group (IFPUG) method and has been developed as an FP measurement tool. We applied the proposed method to several Web applications and examined the difference between FP counts obtained by the tool and those obtained by a certified FP specialist (CFPS). The results reveal that the numbers of data and transaction functions extracted by the tool is approximately the same as the numbers of data and transaction functions extracted by the specialist. 
84|6||Task assignment in heterogeneous computing systems using an effective iterated greedy algorithm|A fundamental issue affecting the performance of a parallel application running on a heterogeneous computing system is the assignment of tasks to the processors in the system. The task assignment problem for more than three processors is known to be NP-hard, and therefore satisfactory suboptimal solutions obtainable in an acceptable amount of time are generally sought. This paper proposes a simple and effective iterative greedy algorithm to deal with the problem with goal of minimizing the total sum of execution and communication costs. The main idea in this algorithm is to improve the quality of the assignment in an iterative manner using results from previous iterations. The algorithm first uses a constructive heuristic to find an initial assignment and iteratively improves it in a greedy way. Through simulations over a wide range of parameters, we have demonstrated the effectiveness of our algorithm by comparing it with recent competing task assignment algorithms in the literature. 
84|6||A syntactic approach to twig-query matching on XML streams|Query matching on XML streams is challenging work for querying efficiency when the amount of queried stream data is huge and the data can be streamed in continuously. In this paper, the method Syntactic Twig-Query Matching (STQM) is proposed to process queries on an XML stream and return the query results continuously and immediately. STQM matches twig queries on the XML stream in a syntactic manner by using a lexical analyzer and a parser, both of which are built from our lexical-rules and grammar-rules generators according to the user's queries and document schema, respectively. For query matching, the lexical analyzer scans the incoming XML stream and the parser recognizes XML structures for retrieving every twig-query result from the XML stream. Moreover, STQM obtains query results without a post-phase for excluding false positives, which are common in many streaming query methods. Through the experimental results, we found that STQM matches the twig query efficiently and also has good scalability both in the queried data size and the branch degree of the twig query. The proposed method takes less execution time than that of a sequence-based approach, which is widely accepted as a proper solution to the XML stream query. 
84|6||A framework for developing home automation systems: From requirements to code|This article presents an integrated framework for the development of home automation systems following the model-driven approach. By executing model transformations the environment allows developers to generate executable code for specific platforms. The tools presented in this work help developers to model home automation systems by means of a domain specific language which is later transformed into code for home automation specific platforms. These transformations have been defined by means of graph grammars and template engines extended with traceability capabilities. Our framework also allows the models to be reused for different applications since a catalogue of requirements is provided. This framework enables the development of home automation applications with techniques for improving the quality of both the process and the models obtained. In order to evaluate the benefits of the approach, we conducted a survey among developers that used the framework. The analysis of the outcome of this survey shows which conditions should be fulfilled in order to increase reusability. 
84|6||An energy-efficient MAC protocol with downlink traffic scheduling strategy in IEEE 802.11 infrastructure WLANs|
84|6||Managing crosscutting concerns in component based systems using a model driven development approach|In the last few years, Model-Driven Development (MDD), Aspect-Oriented Software Development (AOSD), and Component-Based Software Development (CBSD) have become interesting alternatives for the design and construction of complex distributed applications. Although these methodological approaches share the principle of separation of concerns and their further integration as key factors to obtaining high-quality and evolvable large software systems, they usually each address this principle from their own particular perspective. In the present work, we combine Component-Based and Aspect-Oriented Software Developments in a Model Driven software process targeted at the development of complex systems. This process constitutes an enhancement of the separation of concerns by allowing the isolation of crosscutting concerns in both Platform Independent and Platform Specific models. Following a pure MDD philosophy, a set of model transformations are used to generate the system, from preliminary models to the final source code for the Corba Component Model platform. A twofold empirical analysis was used to evaluate the approach’s benefits in terms of two internal quality attributes: modularity and complexity. Conclusions were drawn from this evaluation regarding other quality attributes correlated with these two – stability, changeability, error-proneness, and reusability. An Eclipse plug-in was developed to drive the development of the entire system from early modeling to late deployment stages. 
84|6||The reliability estimation, prediction and measuring of component-based software|Reliability is a key driver of safety-critical systems such as health-care systems and traffic controllers. It is also one of the most important quality attributes of the systems embedded into our surroundings, e.g. sensor networks that produce information for business processes. Therefore, the design decisions that have a great impact on the reliability of a software system, i.e. architecture and components, need to be thoroughly evaluated. This paper addresses software reliability evaluation during the design and implementation phases; it provides a coherent approach by combining both predicted and measured reliability values with heuristic estimates in order to facilitate a smooth reliability evaluation process. The approach contributes by integrating the component-level reliability evaluation activities (i.e. the heuristic reliability estimation, model-based reliability prediction and model-based reliability measuring of components) and the system-level reliability prediction activity to support the incremental and iterative development of reliable component-based software systems. The use of the developed reliability evaluation approach with the supporting tool chain is illustrated by a case study. The paper concludes with a summary of lessons learnt from the case studies. 
84|7|http://www.sciencedirect.com/science/journal/01641212/84/7|Reusable software components for accelerator-based clusters|
84|7||Scalable, statistical storage allocation for extensible inverted file construction|
84|7||Is my model right? Let me ask the expert|Defining a domain model is a costly and error-prone process. It requires that the knowledge possessed by domain experts be suitably captured by modeling experts. Eliciting what is in the domain expert’s mind and expressing it using a modeling language involve substantial human effort. In the process, conceptual errors may be introduced that are hard to detect without a suitable validation methodology. This paper proposes an approach to support such validation, by reducing the knowledge gap that separates modeling experts and domain experts. While our methodology still requires the domain expert’s judgement, it partially automates the validation process by generating a set of yes/no questions from the model. Answers differing from expected ones point to elements in the model which require further consideration and can be used to guide the dialogue between domain experts and modeling experts. Our methodology was implemented as a tool and was applied to a real case study, within the IPERMOB project. 
84|7||Model-driven development of industrial process control applications|
84|7||Procedural security analysis: A methodological approach|This article introduces what we call procedural security analysis, an approach that allows for a systematic security assessment of (business) processes. The approach is based on explicit reasoning on asset flows and is implemented by building formal models to describe the nominal procedures under analysis, by injecting possible threat-actions of such models, and by assuming that any combination of threats can be possible in all steps into such models. We use the NuSMV input language to encode the asset flows, which are amenable for formal analysis. This allows us to understand how the switch to a new technological solution changes the requirements of an organization, with the ultimate goal of defining the new processes that ensure a sufficient level of security.We have applied the technique to a real-world electronic voting system named ProVotE to analyze the procedures used during and after elections. Such analyses are essential to identify the limits of the current procedures (i.e., conditions under which attacks are undetectable) and to identify the hypotheses that can guarantee reasonably secure electronic elections. Additionally, the results of the analyses can be a step forward to devise a set of requirements, to be applied both at the organizational level and on the (software) systems to make them more secure. 
84|7||A benchmarking environment for performance evaluation of tree-based rekeying algorithms|
84|7||Dynamic deployment of context-aware access control policies for constrained security devices|Securing the access to a server, guaranteeing a certain level of protection over an encrypted communication channel, executing particular counter measures when attacks are detected are examples of security requirements. Such requirements are identified based on organizational purposes and expectations in terms of resource access and availability and also on system vulnerabilities and threats. All these requirements belong to the so-called security policy. Deploying the policy means enforcing, i.e., configuring, those security components and mechanisms so that the system behavior be finally the one specified by the policy. The deployment issue becomes more difficult as the growing organizational requirements and expectations generally leave behind the integration of new security functionalities in the information system: the information system will not always embed the necessary security functionalities for the proper deployment of contextual security requirements. To overcome this issue, our solution is based on a central entity approach which takes in charge unmanaged contextual requirements and dynamically redeploys the policy when context changes are detected by this central entity.We also present an improvement over the OrBAC (Organization-Based Access Control) model. Up to now, a controller based on a contextual OrBAC policy is passive, in the sense that it assumes policy evaluation triggered by access requests. Therefore, it does not allow reasoning about policy state evolution when actions occur. The modifications introduced by our work overcome this limitation and provide a proactive version of the model by integrating concepts from action specification languages. 
84|7||Simulation-based analysis of middleware service impact on system reliability: Experiment on Java application server|
84|7||An algorithm for capturing variables dependences in test suites|
84|7||Impossible differential cryptanalysis of 13-round CLEFIA-128|Block cipher plays an important role in the domain of information security. CLEFIA is a 128-bit block cipher proposed by SONY Corporation in FSE 2007. Using the previous 9-round impossible differentials, the redundancy in the key schedule and the early-abort technique, we present the first successful impossible differential cryptanalysis of 13-round CLEFIA-128 in this paper. The data and time complexities of the attack with the whitening layers are 2119.4 and 2125.52, respectively. And for the attack without the whitening layers, more relationships among the subkeys can be used, thus the data and time complexities are reduced to 2111.3 and 2117.5, respectively. As far as we know, the presented results are the best compared with the previously published cryptanalytic results on reduced-round CLEFIA-128. 
84|7||Threshold visual secret sharing by random grids|A new visual secret sharing (VSS) approach by random grids (RG-based VSS), proposed by Kafri and Keren (1987), has drawn close attention recently. With almost all advantages of visual cryptography-based VSS, RG-based VSS benefits more from keeping the same size of secret images without the problem of pixel expansion from which VC-based VSS suffer. In this paper, a threshold RG-based VSS scheme aiming at providing the wide-use version is presented. This is the first effort to develop the technique. The experimental results and theoretically analysis in visual quality and security show that the proposed scheme performs well. 
84|7||Parsed use case descriptions as a basis for object-oriented class model generation|Object-oriented analysis and design has become a major approach in the design of software systems. Recent developments in CASE tools provide help in documenting the analysis and design stages and in detecting incompleteness and inconsistency in analysis. However, these tools do not contribute to the initial and difficult stage of the analysis process of identifying the objects/classes, attributes and relationships used to model the problem domain. This paper presents a tool, Class-Gen, which can partially automate the identification of objects/classes from natural language requirement specifications for object identification. Use case descriptions (UCDs) provide the input to Class-Gen which parses and analyzes the text written in English. A parsed use case description (PUCD) is generated which is then used as the basis for the construction of an initial UML class model representing object classes and relationships identified in the requirements. PUCDs enable the extraction of nouns, verbs, adjectives and adverbs from traditional UCDs for the identification process. Finally Class-Gen allows the initial class model to be refined manually. Class-Gen has been evaluated against a collection of unseen requirements. The results of the evaluation are encouraging as they demonstrate the potential for such tools to assist with the software development process. 
84|7||Optimized QoS-aware replica placement heuristics and applications in astronomy data grid|
84|7||An empirical study of software architecturesâ effect on product quality|
84|7||APDL: A reference XML schema for process-centered definition of RFID solutions|Despite the proliferation of RFID systems and applications, there is still no easy way to develop, integrate and deploy non-trivial RFID solutions. Indeed, the latter comprise various middleware modules (e.g., data collection and filtering, generation of business events, integration with enterprise applications), which must be deployed and configured independently. In this paper we introduce APDL (AspireRFID Process Description Language), an XML based specification for describing and configuring RFID solutions. Using APDL one can minimize the steps and effort required to integrate and configure an RFID solution, since it unifies all the configuration parameters and steps comprising an RFID deployment. APDL supports several configuration parameters defined in the scope of the EPCglobal architecture and related standards. However, it extends beyond the EPCglobal architecture, to a wider class of RFID solutions. Furthermore, APDL is amendable by visual tools, which obviates the need to carry out low-level programming tasks in order to deploy an RFID solution. These tools are also presented and evaluated in the paper. 
84|8|http://www.sciencedirect.com/science/journal/01641212/84/8|BSN: An automatic generation algorithm of social network data|
84|8||The effects of scheduling, workload type and consolidation scenarios on virtual machine performance and their prediction through optimized artificial neural networks|The aim of this paper is to study and predict the effect of a number of critical parameters on the performance of virtual machines (VMs). These parameters include allocation percentages, real-time scheduling decisions and co-placement of VMs when these are deployed concurrently on the same physical node, as dictated by the server consolidation trend and the recent advances in the Cloud computing systems. Different combinations of VM workload types are investigated in relation to the aforementioned factors in order to find the optimal allocation strategies. What is more, different levels of memory sharing are applied, based on the coupling of VMs to cores on a multi-core architecture. For all the aforementioned cases, the effect on the score of specific benchmarks running inside the VMs is measured. Finally, a black box method based on genetically optimized artificial neural networks is inserted in order to investigate the degradation prediction ability a priori of the execution and is compared to the linear regression method. 
84|8||Developing an efficient query system for encrypted XML documents|
84|8||An information presentation method based on tree-like super entity component|Information systems are increasingly oriented in the direction of large-scale integration due to the explosion of multi-source information. It is therefore important to discuss how to reasonably organize and present information from multiple structures and sources on the same information system platform. In this study, we propose a 3C (Components, Connections, Container) component model by combining white-box and black-box methods, design a tree-like super entity based on the model, present its construction and related algorithm, and take a tree-like super entity as the information organization method for multi-level entities. In order to represent structural, semi-structural and non-structural data on the same information system platform, an information presentation method based on an editable e-book component has been developed by combining the tree-like super entity component, QQ-style menu and 1/K switch connection component, which has been successfully applied in the Flood Protection Project Information System of the Yangtze River in China. 
84|8||A family of implementation-friendly BN elliptic curves|For the last decade, elliptic curve cryptography has gained increasing interest in industry and in the academic community. This is especially due to the high level of security it provides with relatively small keys and to its ability to create very efficient and multifunctional cryptographic schemes by means of bilinear pairings. Pairings require pairing-friendly elliptic curves and among the possible choices, Barreto–Naehrig (BN) curves arguably constitute one of the most versatile families.In this paper, we further expand the potential of the BN curve family. We describe BN curves that are not only computationally very simple to generate, but also specially suitable for efficient implementation on a very broad range of scenarios. We also present implementation results of the optimal ate pairing using such a curve defined over a 254-bit prime field. 
84|8||An approach to process continuous location-dependent queries on moving objects with support for location granules|
84|8||Deriving business processes with service level agreements from early requirements|
84|8||On a security model of conjunctive keyword search over encrypted relational database|
84|8||Priority scheduling of requests to web portals|
84|8||An overview on test generation from functional requirements|
84|8||Framework for evaluation and selection of the software packages: A hybrid knowledge based system approach|Evaluation and selection of the software packages is complicated and time consuming decision making process. Selection of inappropriate software package can turn out to be costly and adversely affects business processes and functioning of the organization. In this paper we describe (i) generic methodology for software selection, (ii) software evaluation criteria, and (iii) hybrid knowledge based system (HKBS) approach to assist decision makers in evaluation and selection of the software packages. The proposed HKBS approach employs an integrated rule based and case based reasoning techniques. Rule based reasoning is used to capture user needs of the software package and formulate a problem case. Case based reasoning is used to retrieve and compare candidate software packages with the user needs of the package. This paper also evaluates and compares HKBS approach with the widely used existing software evaluation techniques such as analytic hierarchy process (AHP) and weighted scoring method (WSM). 
84|8||A robust digital audio watermarking scheme using wavelet moment invariance|It is a challenging work to design a robust audio watermarking scheme against various attacks. Wavelet moment invariances are new features combining the moment invariant features and the wavelet features, and they have some excellent characteristics, such as the ability to capture local information, robustness against common signal processing, and the linear relationship between a signal and its wavelet moments etc. Based on wavelet moment and synchronization code, we propose a new digital audio watermarking algorithm with good auditory quality and reasonable resistance against most attacks in this paper. Firstly, the origin digital audio is segmented and then each segment is cut into two parts. Secondly, with the spatial watermarking technique, synchronization code is embedded into the statistics average value of audio samples in the first part. And then, map 1D digital audio signal in the second part into 2D form, and calculate its wavelet moments. Finally, the watermark bit is embedded into the average value of modulus of the low-order wavelet moments. Meanwhile combining the two adjacent synchronization code searching technology, the algorithm can extract the watermark without the help from the origin digital audio signal. Simulation results show that the proposed watermarking scheme is not only inaudible and robust against common signals processing such as MP3 compression, noise addition, resampling, and re-quantization etc., but also robust against the desynchronization attacks such as random cropping, amplitude variation, pitch shifting, and jittering etc. 
84|9|http://www.sciencedirect.com/science/journal/01641212/84/9|Guest Editors Introduction to the Special Issue|
84|9||The lonesome architect|Although the benefits are well-known and undisputed, sharing architectural knowledge is not something architects automatically do. In an attempt to better understand what architects really do and what kind of support they need for sharing knowledge, we have conducted large-scale survey research. The results of our study indicate that architects can be characterized as rather lonesome decision makers who mainly consume, but neglect documenting and actively sharing architectural knowledge. Acknowledging this nature of architects suggests ways to develop more effective support for architectural knowledge sharing. 
84|9||Composing enterprise mashup components and services using architecture integration patterns|Enterprise mashups leverage various source of information to compose new situational applications. The architecture of such applications must address integration issues: it needs to deal with heterogeneous local and/or public data sources, and build value-added applications on existing corporate IT systems. In this paper, we leverage enterprise architecture integration patterns to compose reusable mashup components. We present a service oriented architecture that addresses reusability and integration needs for building enterprise mashup applications. Key techniques to customize this architecture are developed for mashups with themed data on location maps. The usage of this architecture is illustrated by a property valuation application derived from a real-world scenario. We demonstrate and discuss how this state-of-the-art architecture design method can be applied to enhance the design and development of emerging enterprise mashups. 
84|9||Defining and documenting execution viewpoints for a large and complex software-intensive system|
84|9||A secure fragile watermarking scheme based on chaos-and-hamming code|
84|9||Strongly unforgeable proxy signature scheme secure in the standard model|
84|9||Enterprise architecture patterns for business process support analysis|The field of enterprise architectures lacks architecture patterns that would support analysis of a given enterprise architecture, comparison of different enterprise architecture solutions and provide guidelines for development of a target enterprise architecture based on the analysis of existing enterprise architecture. In this paper, we focus on business process support analysis using information derived from enterprise architecture description. We give a systematic overview of important aspects. We establish and formally define foundational enterprise architecture patterns for business process support analysis. They are implementation independent and enable more efficient qualitative architecture analysis of business process support, which is the basis for achieving more optimal business operation. We have defined the patterns using the standard enterprise architecture language – ArchiMate. They are formalized in a way that enables their implementation in enterprise architecture tools. This is an important characteristic that allows for efficient work by automatic detection of different, more or less suitable, architecture structures. We have derived the patterns based on real-world enterprise architecture descriptions and have used and verified them in enterprise architecture analysis and planning projects for four large organizations. The enterprise architecture analysis patterns address an important research issue in the field of enterprise architectures that has so far not been systematically researched. 
84|9||FeGC: An efficient garbage collection scheme for flash memory based storage systems|NAND flash memory is a promising storage media that provides low-power consumption, high density, high performance, and shock resistance. Due to these versatile features, NAND flash memory is anticipated to be used as storage in enterprise-scale systems as well as small embedded devices. However, unlike traditional hard disks, flash memory should perform garbage collection that consists of a series of erase operations. The erase operation is time-consuming and it usually degrades the performance of storage systems seriously. Moreover, the number of erase operations allowed to each flash memory block is limited. This paper presents a new garbage collection scheme for flash memory based storage systems that focuses on reducing garbage collection overhead, and improving the endurance of flash memory. The scheme also reduces the energy consumption of storage systems significantly. Trace-driven simulations show that the proposed scheme performs better than various existing garbage collection schemes in terms of the garbage collection time, the number of erase operations, the energy consumption, and the endurance of flash memory. 
84|9||Enhancing grid-density based clustering for high dimensional data|We propose an enhanced grid-density based approach for clustering high dimensional data. Our technique takes objects (or points) as atomic units in which the size requirement to cells is waived without losing clustering accuracy. For efficiency, a new partitioning is developed to make the number of cells smoothly adjustable; a concept of the ith-order neighbors is defined for avoiding considering the exponential number of neighboring cells; and a novel density compensation is proposed for improving the clustering accuracy and quality. We experimentally evaluate our approach and demonstrate that our algorithm significantly improves the clustering accuracy and quality. 
84|9||Practitioner perceptions of Open Source software in the embedded systems area|There is a growing body of research to show that, with the advent of so-called professional Open Source, attitudes within many organisations towards adopting Open Source software have changed. However, there have been conflicting reports on the extent to which this is true of the embedded software systems sector—a large sector in Europe. This paper reports on attitudes towards Open Source software within that sector. Our results show a high level of acceptance of Open Source products with large, well established communities, and not only at the level of the operating system. Control over the software is seen as fundamentally important. Other key perceptions with Open Source are an easing of long-term maintenance problems and ready availability of support. The classical strengths of Open Source, namely mass inspection, ease of conducting trials, longevity and source code access for debugging, were at the forefront of thinking. However, there was an acknowledgement that more guidelines are needed for assessing Open Source software and incorporating it into products. 
84|9||A multi-purpose digital image watermarking using fractal block coding|In this paper, a new multi-purpose watermarking technique is presented which satisfies both verification and authentication purposes simultaneously by embedding a binary watermark into the image. The proposed method uses a special type of fractal block coding with a local search region with contrast scaling and mean of range block as its parameters. It also utilizes Fuzzy C-Mean clustering to specify the watermark bits. To overcome the high computational complexity of fractal coding, a new simple coding method is also presented which improves the robustness of the watermarking and decreases the run time of fractal block coding in the watermarking procedure. To measure the fragility and robustness of the method to signal distortions such as JPEG compression, median filter, and additive noise, some experiments were employed. The experimental results showed that the proposed method has provided a sensitive authentication and a reliable verification. 
84|9||Revealing bullying patterns in multi-agent systems|
85|1|http://www.sciencedirect.com/science/journal/01641212/85/1|Special Issue on Dynamic Analysis and Testing of Embedded Software|
85|1||InRob: An approach for testing interoperability and robustness of real-time embedded software|
85|1||Test coverage optimization for large code problems|Software developers frequently conduct regression testing on a series of major, minor, or bug-fix software or firmware releases. However, retesting all test cases for each release is time-consuming. For example, it takes about 36 test-bed-days to thoroughly exercise a test suite made up of 2320 test cases for the MPLS testing area that contains 57,758 functions in Cisco IOS. The cost is infeasible for a series of regression testing on the MPLS area. Thus, the test suite needs to be reduced intelligently, not just randomly, and its fault detection capability must be kept as much as possible. The mode of safe regression test selection approach is adopted for seeking a subset of modification-traversing test cases to substitute for fault-revealing test cases. The algorithms, CW-NumMin, CW-CostMin, and CW-CostCov-B, apply the safe-mode approach in selecting test cases for achieving full-modified function coverage. It is assumed that modified functions are fault-prone, and the fault distribution of the testing area is Pareto-like. Moreover, we also assume that once a subject program is getting more mature, its fault concentration will become stronger. Only function coverage criterion is adopted because of the scalability of a software system with large code. The metrics of test’s function reachability and function’s test intensity are defined in this study for algorithms. Both CW-CovMax and CW-CostMin algorithms are not safe-mode, but the approaches they use still attempt to obtain a test suite with a maximal amount of function coverage under certain constraints, i.e. the effective-confidence level and time restriction. We conclude that the most effective algorithm in this study can significantly reduce the cost (time) of regression testing on the MPLS testing area to 1.10%, on the average. Approaches proposed here can be effectively and efficiently applied to the regression testing on bug-fix releases of a software system with large code, especially to the releases having very few modified functions with low test intensities. 
85|1||Lightweight embedded software performance analysis method by kernel hack and its industrial field study|
85|1||Automatic testing environment for multi-core embedded softwareâATEMES|
85|1||Erratum to âPerformance evaluation of fast handover in mobile IPv6 based on link layer informationsâ [J. Syst. Softw. 83 (2010) 1644â1650]|
85|1||A power efficiency routing and maintenance protocol in wireless multi-hop networks|In wireless multi-hop networks, selecting a path that has a high transmission bandwidth or a high delivery rate of packets can reduce power consumption and shorten transmission delay during data transmission. There are two factors that influence the transmission bandwidth: the signal strength of the received packets and contentions in the contention-based MAC layer. These two factors may cause more power to be consumed during data transmission. We analyze these two factors and propose a power-aware routing protocol called MTPCR. MTPCR discovers the desired routing path that has reduced power consumption during data transmission. In addition to finding a desired path to reduce power consumption, MTPCR also takes into account the situations in which the transmission bandwidth of the routing path may decrease, resulting in much power consumption during data transmission because of the mobility of nodes in a network. MTPCR is thus useful in a network: it analyzes power consumption during data transmission with the help of neighboring nodes, and it uses a path maintenance mechanism to maintain good path bandwidth. The density of nodes in a network is used to determine when to activate the path maintenance mechanism in order to reduce the overhead of this mechanism. With the proposed path maintenance mechanism, power consumption during data transmission can be efficiently reduced, as well as the number of path breakages. In our simulation, we compared our proposed routing protocol, MTPCR, with the following protocols: two classical routing protocols, AODV and DSR; two power-aware routing protocols, MMBCR and xMBCR; and one multiple path routing protocol, PAMP. The comparisons are made in terms of throughput of the routing path, power consumption in path discovery, power consumption in data transmission, and network lifetime. 
85|1||Self-tuning of disk inputâoutput in operating systems|
85|1||Keyword clustering for user interest profiling refinement within paper recommender systems|
85|1||SimFuzz: Test case similarity directed deep fuzzing|
85|1||Dependability analysis in the Ambient Assisted Living Domain: An exploratory case study|
85|1||Controlling software architecture erosion: A survey|Software architectures capture the most significant properties and design constraints of software systems. Thus, modifications to a system that violate its architectural principles can degrade system performance and shorten its useful lifetime. As the potential frequency and scale of software adaptations increase to meet rapidly changing requirements and business conditions, controlling such architecture erosion becomes an important concern for software architects and developers. This paper presents a survey of techniques and technologies that have been proposed over the years either to prevent architecture erosion or to detect and restore architectures that have been eroded. These approaches, which include tools, techniques and processes, are primarily classified into three generic categories that attempt to minimise, prevent and repair architecture erosion. Within these broad categories, each approach is further broken down reflecting the high-level strategies adopted to tackle erosion. These are: process-oriented architecture conformance, architecture evolution management, architecture design enforcement, architecture to implementation linkage, self-adaptation and architecture restoration techniques consisting of recovery, discovery and reconciliation. Some of these strategies contain sub-categories under which survey results are presented.We discuss the merits and weaknesses of each strategy and argue that no single strategy can address the problem of erosion. Further, we explore the possibility of combining strategies and present a case for further work in developing a holistic framework for controlling architecture erosion. 
85|1||DyDAP: A dynamic data aggregation scheme for privacy aware wireless sensor networks|End-to-end data aggregation, without degrading sensing accuracy, is a very relevant issue in wireless sensor networks (WSN) that can prevent network congestion to occur. Moreover, privacy management requires that anonymity and data integrity are preserved in such networks. Unfortunately, no integrated solutions have been proposed so far, able to tackle both issues in a unified and general environment. To bridge this gap, in this paper we present an approach for dynamic secure end-to-end data aggregation with privacy function, named DyDAP. It has been designed starting from a UML model that encompasses the most important building blocks of a privacy-aware WSN, including aggregation policies. Furthermore, it introduces an original aggregation algorithm that, using a discrete-time control loop, is able to dynamically handle in-network data fusion to reduce the communication load. The performance of the proposed scheme has been verified using computer simulations, showing that DyDAP avoids network congestion and therefore improves WSN estimation accuracy while, at the same time, guaranteeing anonymity and data integrity. 
85|1||Using compressed index structures for processing moving objects in large spatio-temporal databases|
85|1||The changing industry structure of software development for consumer electronics and its consequences for software architectures|During the last decade the structure of the consumer electronics industry has been changing profoundly. Current consumer electronics products are built using components from a large variety of specialized firms, whereas previously each product was developed by a single, vertically integrated company. Taking a software development perspective, we analyze the transition in the consumer electronics industry using case studies from digital televisions and mobile phones. We introduce a model consisting of five industry structure types and describe the forces that govern the transition between types and we describe the consequences for software architectures.We conclude that, at this point in time, software supply chains are the dominant industry structure for developing consumer electronics products. This is because the modularization of the architecture is limited, due to the lack of industry-wide standards and because resource constrained devices require variants of supplied software that are optimized for different hardware configurations. Due to these characteristics open ecosystems have not been widely adopted. The model and forces can serve the decision making process for individual companies that consider the transition to a different type of industry structure as well as provide a framework for researchers studying the software-intensive industries. 
85|1||Appraisal and reporting of security assurance at operational systems level|
85|1||ID-based proxy signature scheme with message recovery|
85|10|http://www.sciencedirect.com/science/journal/01641212/85/10|Guest editorsâ introduction to the special issue on automated software evolution|
85|10||Using Pig as a data preparation language for large-scale mining software repositories studies: An experience report|The Mining Software Repositories (MSR) field analyzes software repository data to uncover knowledge and assist development of ever growing, complex systems. However, existing approaches and platforms for MSR analysis face many challenges when performing large-scale MSR studies. Such approaches and platforms rarely scale easily out of the box. Instead, they often require custom scaling tricks and designs that are costly to maintain and that are not reusable for other types of analysis. We believe that the web community has faced many of these software engineering scaling challenges before, as web analyses have to cope with the enormous growth of web data. In this paper, we report on our experience in using a web-scale platform (i.e., Pig) as a data preparation language to aid large-scale MSR studies. Through three case studies, we carefully validate the use of this web platform to prepare (i.e., Extract, Transform, and Load, ETL) data for further analysis. Despite several limitations, we still encourage MSR researchers to leverage Pig in their large-scale studies because of Pig's scalability and flexibility. Our experience report will help other researchers who want to scale their analyses. 
85|10||Towards automated traceability maintenance|Traceability relations support stakeholders in understanding the dependencies between artifacts created during the development of a software system and thus enable many development-related tasks. To ensure that the anticipated benefits of these tasks can be realized, it is necessary to have an up-to-date set of traceability relations between the established artifacts. This goal requires the creation of traceability relations during the initial development process. Furthermore, the goal also requires the maintenance of traceability relations over time as the software system evolves in order to prevent their decay. In this paper, an approach is discussed that supports the (semi-) automated update of traceability relations between requirements, analysis and design models of software systems expressed in the UML. This is made possible by analyzing change events that have been captured while working within a third-party UML modeling tool. Within the captured flow of events, development activities comprised of several events are recognized. These are matched with predefined rules that direct the update of impacted traceability relations. The overall approach is supported by a prototype tool and empirical results on the effectiveness of tool-supported traceability maintenance are provided. 
85|10||Dependency solving: A separate concern in component evolution management|Maintenance of component-based software platforms often has to face rapid evolution of software components. Component dependencies, conflicts, and package managers with dependency solving capabilities are the key ingredients of prevalent software maintenance technologies that have been proposed to keep software installations synchronized with evolving component repositories. We review state-of-the-art package managers and their ability to keep up with evolution at the current growth rate of popular component-based platforms, and conclude that their dependency solving abilities are not up to the task.We show that the complexity of the underlying upgrade planning problem is NP-complete even for seemingly simple component models, and argue that the principal source of complexity lies in multiple available versions of components. We then discuss the need of expressive languages for user preferences, which makes the problem even more challenging.We propose to establish dependency solving as a separate concern from other upgrade aspects, and present CUDF as a formalism to describe upgrade scenarios. By analyzing the result of an international dependency solving competition, we provide evidence that the proposed approach is viable. 
85|10||Identification and application of Extract Class refactorings in object-oriented systems|Refactoring is recognized as an essential practice in the context of evolutionary and agile software development. Recognizing the importance of the practice, modern IDEs provide some support for low-level refactorings. A notable exception in the list of supported refactorings is the “Extract Class” refactoring, which is conceived to simplify large, complex, unwieldy and less cohesive classes.In this work, we describe a method and a tool, implemented as an Eclipse plugin, designed to fulfill exactly this need. Our method involves three steps: (a) recognition of Extract Class opportunities, (b) ranking of the identified opportunities in terms of the improvement each one is anticipated to bring about to the system design, and (c) fully automated application of the refactoring chosen by the developer. The first step relies on an agglomerative clustering algorithm, which identifies cohesive sets of class members within the system classes. The second step relies on the Entity Placement metric as a measure of design quality. Through a set of experiments we have shown that the tool is able to identify and extract new classes that developers recognize as “coherent concepts” and improve the design quality of the underlying system. 
85|10||Model-driven support for product line evolution on feature level|Software Product Lines (SPL) are an engineering technique to efficiently derive a set of similar products from a set of shared assets. In particular in conjunction with model-driven engineering, SPL engineering promises high productivity benefits. There is however, a lack of support for systematic management of SPL evolution, which is an important success factor as a product line often represents a long term investment. In this article, we present a model-driven approach for managing SPL evolution on feature level. To reduce complexity we use model fragments to cluster related elements. The relationships between these fragments are specified using feature model concepts itself leading to a specific kind of feature model called EvoFM. A configuration of EvoFM represents an evolution step and can be transformed to a concrete instance of the product line (i.e., a feature model for the corresponding point in time). Similarly, automatic transformations allow the derivation of an EvoFM from a given set of feature models. This enables retrospective analysis of historic evolution and serves as a starting point for introduction of EvoFM, e.g., to plan future evolution steps. 
85|10||Automated, highly-accurate, bug assignment using machine learning and tossing graphs|
85|10||On the relationship between comment update practices and Software Bugs|When changing source code, developers sometimes update the associated comments of the code (a consistent update), while at other times they do not (an inconsistent update). Similarly, developers sometimes only update a comment without its associated code (an inconsistent update). The relationship of such comment update practices and software bugs has never been explored empirically. While some (in)consistent updates might be harmless, software engineering folklore warns of the risks of inconsistent updates between code and comments, because these updates are likely to lead to out-of-date comments, which in turn might mislead developers and cause the introduction of bugs in the future. In this paper, we study comment update practices in three large open-source systems written in C (FreeBSD and PostgreSQL) and Java (Eclipse). We find that these practices can better explain and predict future bugs than other indicators like the number of prior bugs or changes. Our findings suggest that inconsistent changes are not necessarily correlated with more bugs. Instead, a change in which a function and its comment are suddenly updated inconsistently, whereas they are usually updated consistently (or vice versa), is risky (high probability of introducing a bug) and should be reviewed carefully by practitioners. 
85|10||Towards automated debugging in software evolution: Evaluating delta debugging on real regression bugs from the developersâ perspectives|Delta debugging has been proposed to isolate failure-inducing changes when regressions occur. In this work, we focus on evaluating delta debugging in practical settings from developers’ perspectives. A collection of real regressions taken from medium-sized open source programs is used in our evaluation. Towards automated debugging in software evolution, a tool based on delta debugging is created and both the limitations and costs are discussed.We have evaluated two variants of delta debugging. Different from successful isolation in Zeller's initial studies, the results in our experiments vary wildly. Two thirds of isolated changes in studied programs provide direct or indirect clues in locating regression bugs. The remaining results are superfluous changes or even wrong isolations. In the case of wrong isolations, the isolated changes cause the same behaviour of the regression but are failure-irrelevant. Moreover, the hierarchical variant does not yield definite improvements in terms of the efficiency and accuracy. 
85|10||Preserving knowledge in software projects|
85|10||Convex optimization framework for intermediate deadline assignment in soft and hard real-time distributed systems|
85|10||Architecture-driven reliability optimization with uncertain model parameters|
85|10||The influence of SPI on business success in software SMEs: An empirical study|
85|10||A framework for model-driven development of information systems: Technical decisions and lessons learned|In recent years, the impact of the model-driven engineering (MDE) paradigm has resulted in the advent of a number of model-based methodological proposals that leverage the use of models at any stage of the development cycle. Apart from promoting the role of models, MDE is notable for leveraging the level of automation along the development process. For this to be achieved there is a need of supporting frameworks, tools or environments. This way, while accompanying any methodological proposal of the corresponding technical support has been traditionally recognized as a good practice, it becomes a mandatory requirement in MDE contexts. To address this task, this work presents in a systematic and reasoned way the set of methodological and technical decisions that drove the specification of M2DAT, a technical solution for model-driven development of Information Systems and its reference implementation: M2DAT-DB, a DSL toolkit for model-driven development of modern DB schemas. The objective of this work is to put forward the conclusions and decisions derived from the experience of the authors when designing and building such framework. As a result, this work will help not only MDE practitioners, but also SE practitioners wishing to bring the advantages of MDE to their fields of interest. 
85|10||A compression-based text steganography method|
85|10||High capacity reversible data hiding scheme based upon discrete cosine transformation|In this paper, we propose a reversible data hiding scheme based on the varieties of coefficients of discrete cosine transformation of an image. Cover images are decomposed into several different frequencies, and the high-frequency parts are embedded with secret data. We use integer mapping to implement our 2-dimensional discrete cosine transformation. Thus, the image recovered from the modified coefficients can be transformed back to the correct data-hidden coefficients. Since the distribution of 2-dimensional DCT coefficients looks close to Gaussian distribution centralized at zero, it is a natural candidate for embedding secret data using the histogram shifting approach. Thus, our approach shifts the positive coefficients around zero to the right and the negative coefficients around zero to the left in order to leave a space to hide the secret data. The experimental comparisons show that, compared to Chang et al. and Lin et al.'s method, the embedding capacity and quality of the stego-image of the proposed method is a great improvement. 
85|10||An experimental comparison of different real-time schedulers on multicore systems|
85|10||3 + 1 Challenges for the future of universities|
85|11|http://www.sciencedirect.com/science/journal/01641212/85/11|Strong fuzzy c-means in medical image data analysis|
85|11||Ontology driven bee's foraging approach based self adaptive online recommendation system|Online recommendation system is the modern software system used in all the e-commerce sites to capture the user intent and recommend the web pages that contain user expected information. The important challenges for such a system must include a need of being self-adaptive because the needs for online users may change dynamically. Classifier plays a very important role to improve the overall system accuracy. Here, we proposed the Ontology driven bee's foraging approach (ODBFA) that accurately classify the current user activity to any of the navigation profiles and predict the navigations that most likely to be visited by online users.Our proposed ODBFA method uses the Honey bee foraging behaviour in selecting the more profitable navigation profile for the current user activity. This approach makes the system self adaptive by capturing the changing needs of online user with the help of ontological framework comprising of ontology based similarity comparison and scoring algorithm. This approach effectively outperforms the other methods in achieving accurate classification and prediction of future navigation for the current online user. 
85|11||Improved results on impossible differential cryptanalysis of reduced-round Camellia-192/256|
85|11||Cross-layer end-to-end label switching protocol for WiMAXâMPLS heterogeneous networks|The integration of WiMAX networks and multi-protocol label switching (MPLS) networks, called WiMPLS networks, is the trend for nomadic Internet access in the fourth generation (4G) wireless networks. The base station (BS) in such heterogeneous networks will play the role of bridge and router between the IEEE 802.16 subscriber stations (SSs) and MPLS networks. However, there is no such integrated solution so far and the switching efficiency of the BS should be considered as well. This paper, therefore, adopts a cross-layer fashion (from network layer to MAC layer) to design the end-to-end label switching protocol (ELSP) for filling this gap. ELSP provides the mechanism of end-to-end (SS-to-SS) and layer 2 switching transfer for switching performance enhancement by assigning the SS with the MPLS labels (M-labels). The M-label can be carried by the IEEE 802.16e extended subheader within the MAC protocol data unit (MPDU), which is fully compliant with the IEEE 802.16e standard. The security issue caused by M-label usage is also concerned and solved in this paper. This paper also reveals an extra advantage that the switching delay of the BS achieved by ELSP can be as low as hardware-accelerated IP lookup mechanism, e.g., ternary content addressable memory (TCAM). Simulation results show that ELSP efficiently improves the end-to-end transfer delay as well as the throughput for WiMPLS heterogeneous networks. 
85|11||A variable-length model for masquerade detection|
85|11||JCSI: A tool for checking secure information flow in Java Card applications|
85|11||Interpretation problems related to the use of regression models to decide on economy of scale in software development|
85|11||From Teleo-Reactive specifications to architectural components: A model-driven approach|
85|11||Performance analysis of SCOOP programs|
85|11||Grouping target paths for evolutionary generation of test data in parallel|
85|11||Nearest neighbor selection for iteratively kNN imputation|Existing kNN imputation methods for dealing with missing data are designed according to Minkowski distance or its variants, and have been shown to be generally efficient for numerical variables (features, or attributes). To deal with heterogeneous (i.e., mixed-attributes) data, we propose a novel kNN (k nearest neighbor) imputation method to iteratively imputing missing data, named GkNN (gray kNN) imputation. GkNN selects k nearest neighbors for each missing datum via calculating the gray distance between the missing datum and all the training data rather than traditional distance metric methods, such as Euclidean distance. Such a distance metric can deal with both numerical and categorical attributes. For achieving the better effectiveness, GkNN regards all the imputed instances (i.e., the missing data been imputed) as observed data, which with complete instances (instances without missing values) together to iteratively impute other missing data. We experimentally evaluate the proposed approach, and demonstrate that the gray distance is much better than the Minkowski distance at both capturing the proximity relationship (or nearness) of two instances and dealing with mixed attributes. Moreover, experimental results also show that the GkNN algorithm is much more efficient than existent kNN imputation methods. 
85|11||An endurance solution for solid state drives with cache|
85|11||Percolation-based routing in the Internet|
85|11||Software architecture evolution through evolvability analysis|Software evolvability is a multifaceted quality attribute that describes a software system's ability to easily accommodate future changes. It is a fundamental characteristic for the efficient implementation of strategic decisions, and the increasing economic value of software. For long life systems, there is a need to address evolvability explicitly during the entire software lifecycle in order to prolong the productive lifetime of software systems. However, designing and evolving software architectures are the challenging task. To improve the ability to understand and systematically analyze the evolution of software system architectures, in this paper, we describe software architecture evolution characterization, and propose an architecture evolvability analysis process that provides replicable techniques for performing activities to aim at understanding and supporting software architecture evolution. The activities are embedded in: (i) the application of a software evolvability model; (ii) a structured qualitative method for analyzing evolvability at the architectural level; and (iii) a quantitative evolvability analysis method with explicit and quantitative treatment of stakeholders’ evolvability concerns and the impact of potential architectural solutions on evolvability. The qualitative and quantitative assessments manifested in the evolvability analysis process have been applied in two large-scale industrial software systems at ABB and Ericsson, with experiences and reflections described. 
85|11||Optimizing virtual machines using hybrid virtualization|Minimizing virtualization overhead and improving the reliability of virtual machines are challenging when establishing virtual machine cluster. Paravirtualization and hardware-assisted virtualization are two mainstream solutions for modern system virtualization. Hardware-assisted virtualization is superior in CPU and memory virtualization and becoming the leading solution, yet paravirtualization is still valuable in some aspects as it is capable of shortening the disposal path of I/O virtualization. Thus we propose the hybrid virtualization which runs the paravirtualized guest in the hardware-assisted virtual machine container to take advantage of both. Experiment results indicate that our hybrid solution outweighs origin paravirtualization by nearly 30% in memory intensive test and 50% in microbenchmarks. Meanwhile, compared with the origin hardware-assisted virtual machine, hybrid guest owns over 16% improvement in I/O intensive workloads. 
85|11||Managing data dependencies in service compositions|Composing services into service-based systems requires the design of coordination logic, which describes all service interactions realizing the composition. Coordination can be defined as the management of dependencies; in a services context we can discriminate between ‘control flow’ that manages sequence dependencies and ‘data flow’ for managing data dependencies. Current research fails to address the management of data dependencies in a systematic way and mostly treats it as subordinate to sequence dependencies. In this article a ‘data flow’ pattern language is presented that provides a systematic way of designing the data flow aspects of a coordination scenario, orthogonally to the way in which the control flow is designed. Starting from a set of fundamental and basic building blocks, each data dependency will yield a data flow design that takes a set of design criteria (e.g. loose coupling, data confidentiality, etc.) into account. The pattern language is evaluated in three ways. First, it is shown that every potential coordination scenario for managing a data dependency can be composed by the set of patterns. Second, the pattern language was applied in a real-life insurance case to show how it can guide the design of complex data flows. Third, the patterns were implemented in a tool that provides configurable model-to-code transformations for automatically generating BPEL coordination scenarios. In this tool both the data flow and control flow can be designed separately using different sets of patterns. 
85|11||AIOLOS: Middleware for improving mobile application performance through cyber foraging|
85|11||Masquerade attacks based on user's profile|
85|11||Execution of natural language requirements using State Machines synthesised from Behavior Trees|This paper defines a transformation from Behavior Tree models to UML state machines. Behavior Trees are a graphical modelling notation for capturing and formalising dynamic system behaviour described in natural language requirements. But state machines are more widely used in software development, and are required for use with many tools, such as test case generators. Combining the two approaches provides a formal path from natural language requirements to an executable model of the system. This in turn facilitates requirements validation and transition to model-driven software development methods. The approach is demonstrated by defining a mapping from Behavior Trees to UML state machines using the ATLAS Transformation Language (ATL) in the Eclipse Modeling Framework. A security-alarm system case study is used to illustrate the use of Behavior Trees and execution to debug requirements. 
85|11||Authentication of images for 3D cameras: Reversibly embedding information using intelligent approaches|
85|12|http://www.sciencedirect.com/science/journal/01641212/85/12|Introduction to the special issue on state of the art in engineering self-adaptive systems|
85|12||An evaluation of multi-model self-managing control schemes for adaptive performance management of software systems|
85|12||Self-control of the time complexity of a constraint satisfaction problem solver program|This paper presents the self-controlling software paradigm and reports on its use to control the branch and bound based constraint satisfaction problem solving algorithm. In this paradigm, an algorithm is first conceptualized as a dynamical system and then a feedback control loop is added to control its behavior. The loop includes a Quality of Service component that assesses the performance of the algorithm during its run time and a controller that adjusts the parameters of the algorithm in order to achieve the control goal. Although other approaches – generally termed as “self-*” – make use of control loops, this use is limited to the structure of the software system, rather than to its behavior and its dynamics. This paper advocates the analysis of dynamics of any program with control loops. The self-controlling software paradigm is evaluated on two different NP-hard constraint satisfaction and optimization problems. The results of the evaluation show an improvement in the performance due to the added control loop for both of the tested constraint satisfaction problems. 
85|12||Self-tuning of software systems through dynamic quality tradeoff and value-based feedback control loop|Quality requirements of a software system cannot be optimally met, especially when it is running in an uncertain and changing environment. In principle, a controller at runtime can monitor the change impact on quality requirements of the system, update the expectations and priorities from the environment, and take reasonable actions to improve the overall satisfaction. In practice, however, existing controllers are mostly designed for tuning low-level performance indicators instead of high-level requirements. By maintaining a live goal model to represent runtime requirements and linking the overall satisfaction of quality requirements to an indicator of earned business value, we propose a control-theoretic self-tuning method that can dynamically tune the preferences of different quality requirements, and can autonomously make tradeoff decisions through our Preference-Based Goal Reasoning procedure. The reasoning procedure results in an optimal configuration of the variation points by selecting the right alternative of OR-decomposed goals and such a configuration is mapped onto corresponding system architecture reconfigurations. The effectiveness of our self-tuning method is evaluated by earned business value, comparing our results with those obtained using static and ad hoc methods. 
85|12||Achieving dynamic adaptation via management and interpretation of runtime models|
85|12||Specification and monitoring of data-centric temporal properties for service-based systems|Service-based systems operate in a very dynamic environment. To guarantee functional and non-functional objective at runtime, an adaptation mechanism is usually expected to monitor software changes, make appropriate decisions, and act accordingly. However, existing runtime monitoring solutions consider only the constraints on the sequence of messages exchanged between partner services and ignore the actual data contents inside the messages. As a result, it is difficult to monitor some dynamic properties such as how message data of interest is processed between different participants. To address this issue, we propose an efficient, non-intrusive online monitoring approach to dynamically analyze data-centric properties for service-oriented applications involving multiple participants. By introducing Par-BCL – a Parametric Behavior Constraint Language for Web services – to define monitoring parameters, various data-centric temporal behavior properties for Web services can be specified and monitored. This approach broadens the monitored patterns to include not only message exchange orders, but also data contents bound to the parameters. To reduce runtime overhead, we statically analyze the monitored properties and combine two different indexing mechanisms to optimize monitoring. The experiments show that our solution is efficient and promising. 
85|12||Adaptive application offloading using distributed abstract class graphs in mobile environments|Self-adaptation of software has been used as a mechanism to address complexity and constraint in mobile and pervasive computing environments. Adaptive offloading is a software adaptation mechanism in which an application dynamically distributes portions of itself to remote devices to achieve context specific optimizations. The feasibility of using adaptive offloading in pervasive environments is determined by the computational efficiency of adaptation algorithms and the efficacy of their decisions. However, existing state-of-the-art approaches incur overheads from storing, updating and partitioning complete application graphs on each device, which limits their utility and scalability in resource constrained mobile environments. Hence, this paper presents a novel distributed approach to application representation in which each device maintains a graph consisting only of components in its memory space, while maintaining abstraction elements for components in remote devices. This approach removes the need to store and update complete application graphs on each device and reduces the cost of partitioning an application during adaptation. In addition, an extension to an existing application graph partitioning heuristic is proposed to utilize this representation approach. An evaluation involving computationally heavy open-source applications adapting in a heterogeneous collaboration showed that the new approach reduced graph update network cost by 100%, collaboration-wide memory cost by between 37% and 50%, power usage by between 63% and 93%, and adaptation time by between 19.47% and 98%, while improving efficacy of adaptation by 12% and 34% for two of the considered applications. 
85|12||HPobSAM for modeling and analyzing IT Ecosystems â Through a case study|The next generation of software systems includes systems composed of a large number of distributed, decentralized, autonomous, interacting, cooperating, organically grown, heterogeneous, and continually evolving subsystems, which we call IT Ecosystems. Clearly, we need novel models and approaches to design and develop such systems which can tackle the long-term evolution and complexity problems. In this paper, our framework to model IT Ecosystems is a combination of centralized control (top-down) and self-organizing (bottom-up) approach. We use a flexible formal model, HPobSAM, that supports both behavioral and structural adaptation/evolution. We use a detailed, close to real-life, case study of a smart airport to show how we can use HPobSAM in modeling, analyzing and developing an IT Ecosystem. We provide an executable formal specification of the model in Maude, and use LTL model checking and bounded state space search provided by Maude to analyze the model. We develop a prototype of our case study designed by HPobSAM using Java and Ponder2. Due to the complexity of the model, we cannot check all properties at design time using Maude. We propose a new approach for run-time verification of our case study, and check different types of properties which we could not verify using model checking. As our model uses dynamic policies to control the behavior of systems which can be modified at runtime, it provides us a suitable capability to react to the property violation by modification of policies. 
85|12||LossEstimate: Distributed failure estimation in wireless networks|
85|12||QoS and energy management with Petri nets: A self-adaptive framework|Energy use is becoming a key design consideration in computing infrastructures and services. In this paper we focus on service-based applications and we propose an adaptation framework that can be used to reduce power consumption according to the observed workload. The adaptation guarantees a trade-off between energy consumption and system performance. The approach is based on the principle of proportional energy consumption obtained by scaling down energy for unused resources, considering both the number of servers switched on and their operating frequencies. Stochastic Petri nets are proposed for the modeling of the framework concerns, their analyses give results about the trade-offs. The application of the approach to a simple case study shows its usefulness and practical applicability. Finally, different types of workloads are analyzed with validation purposes. 
85|12||Adam: Identifying defects in context-aware adaptation|
85|12||Analysing monitoring and switching problems for adaptive systems|In the field of pervasive and ubiquitous computing, context-aware adaptive systems need to monitor changes in their environment in order to detect violations of requirements and switch their behaviour in order to continue satisfying requirements. In a complex and rapidly changing environment, identifying what to monitor and deciding when and how to switch behaviours effectively is difficult and error prone. The goal of our research is to provide systematic and, where possible, automated support for the software engineer developing such adaptive systems.In this paper, we investigate the necessary and sufficient conditions for both monitoring and switching in order to adapt the system behaviours as the problem context varies. Necessary and sufficient conditions provide complementary safeguards to ensure that not too much and not too little monitoring and switching are carried out. Our approach encodes monitoring and switching problems into propositional logic constraints in order for these conditions to be analysed automatically using a standard SAT solver.We demonstrate our approach by analysing a mobile phone system problem. We analysed requirements violations caused by changes in the system's operating environment. By providing necessary and sufficient monitoring and switching capabilities to the system, particular requirements violations were avoided. 
85|12||A development framework and methodology for self-adapting applications in ubiquitous computing environments|Today software is the main enabler of many of the appliances and devices omnipresent in our daily life and important for our well being and work satisfaction. It is expected that the software works as intended, and that the software always and everywhere provides us with the best possible utility. This paper discusses the motivation, technical approach, and innovative results of the MUSIC project. MUSIC provides a comprehensive software development framework for applications that operate in ubiquitous and dynamic computing environments and adapt to context changes. Context is understood as any information about the user needs and operating environment which vary dynamically and have an impact on design choices. MUSIC supports several adaptation mechanisms and offers a model-driven application development approach supported by a sophisticated middleware that facilitates the dynamic and automatic adaptation of applications and services based on a clear separation of business logic, context awareness and adaptation concerns. The main contribution of this paper is a holistic, coherent presentation of the motivation, design, implementation, and evaluation of the MUSIC development framework and methodology. 
85|12||Stitch: A language for architecture-based self-adaptation|Requirements for high availability in computing systems today demand that systems be self-adaptive to maintain expected qualities-of-service in the presence of system faults, variable environmental conditions, and changing user requirements. Autonomic computing tackles the challenge of automating tasks that humans would otherwise have to perform to achieve this goal. However, existing approaches to autonomic computing lack the ability to capture routine human repair tasks in a way that takes into account the business context humans use in selecting an appropriate form of adaptation, while dealing with timing delays and uncertainties in outcome of repair actions. In this article, we present Stitch, a language for representing repair strategies within the context of an architecture-based self-adaptation framework. Stitch supports the explicit representation of repair decision trees together with the ability to express business objectives, allowing a self-adaptive system to select a strategy that has optimal utility in a given context, even in the presence of potential timing delays and outcome uncertainty. 
85|12||A semantic translation method for data communication protocols|Protocol translation is a method for transforming pieces of information from a source protocol into relevant target protocol formats in order to communicate between heterogeneous legacy systems in interoperability environments. There are some existing protocol translation technologies for network protocols and simple data messages, but these have semantic information problems such as information distortion or incorrect information translations when applied to semantic messages. To deal with these problems, we propose a method that translates messages to semantically equivalent messages. We devised the method to be practical in implementing semantic information-sensitive gateway systems. We conducted a comparative experiment with the translation approach developed in the defense domain. We found that our semantic translation method provides more accurate and efficient message translations in interoperability environments. 
85|12||Side channel analysis attacks using AM demodulation on commercial smart cards with SEED|We investigate statistical side channel analysis attacks on the SEED block cipher implemented in two commercial smart cards used in a real-world electronic payment system. The first one is a contact-only card and the second one is a combination card. Both cards have no masking scheme at algorithm level and the combination card supports only hiding techniques in hardware level. Our results show that an unprotected implementation of SEED allows one to recover the secret key with low number of power or electromagnetic traces. Moreover, this paper clearly confirms that, although hiding countermeasures such as random current and random noise may increase the number of power traces needed for a successful attack, it is difficult to provide sufficient resistance to side channel attacks for itself. We believe that our results in this research will also be beneficial to the analysis and protection of other algorithms and commercial smart cards. 
85|12||Grindstone4Spam: An optimization toolkit for boosting e-mail classification|Resulting from the huge expansion of Internet usage, the problem of unsolicited commercial e-mail (UCE) has grown astronomically. Although a good number of successful content-based anti-spam filters are available, their current utilization in real scenarios is still a long way off. In this context, the SpamAssassin filter offers a rule-based framework that can be easily used as a powerful integration and deployment tool for the fast development of new anti-spam strategies. This paper presents Grindstone4Spam, a publicly available optimization toolkit for boosting SpamAssassin performance. Its applicability has been verified by comparing its results with those obtained by the default SpamAssassin software as well as four well-known anti-spam filtering techniques such as Naïve Bayes, Flexible Bayes, Adaboost and Support Vector Machines in two different case studies. The performance of the proposed alternative clearly outperforms existing approaches working in a cost-sensitive scenario. 
85|12||Technology flexibility as enabler of robust application development in community source: The case of Kuali and Sakai|
85|2|http://www.sciencedirect.com/science/journal/01641212/85/2|Special issue with selected papers from the 23rd Brazilian Symposium on Software Engineering|
85|2||Towards understanding the underlying structure of motivational factors for software engineers to guide the definition of motivational programs|
85|2||Applying and evaluating concern-sensitive design heuristics|
85|2||Identifying thresholds for object-oriented software metrics|Despite the importance of software metrics and the large number of proposed metrics, they have not been widely applied in industry yet. One reason might be that, for most metrics, the range of expected values, i.e., reference values are not known. This paper presents results of a study on the structure of a large collection of open-source programs developed in Java, of varying sizes and from different application domains. The aim of this work is the definition of thresholds for a set of object-oriented software metrics, namely: LCOM, DIT, coupling factor, afferent couplings, number of public methods, and number of public fields. We carried out an experiment to evaluate the practical use of the proposed thresholds. The results of this evaluation indicate that the proposed thresholds can support the identification of classes which violate design principles, as well as the identification of well-designed classes. The method used in this study to derive software metrics thresholds can be applied to other software metrics in order to find their reference values. 
85|2||Automating the product derivation process of multi-agent systems product lines|
85|2||To lock, or not to lock: That is the question|
85|2||A novel color image encryption algorithm based on DNA sequence operation and hyper-chaotic system|A new color image encryption algorithm based on DNA (Deoxyribonucleic acid) sequence addition operation is presented. Firstly, three DNA sequence matrices are obtained by encoding the original color image which can be converted into three matrices R, G and B. Secondly, we use the chaotic sequences generated by Chen's hyper-chaotic maps to scramble the locations of elements from three DNA sequence matrices, and then divide three DNA sequence matrices into some equal blocks respectively. Thirdly, we add these blocks by using DNA sequence addition operation and Chen's hyper-chaotic maps. At last, by decoding the DNA sequence matrices and recombining the three channels R, G and B, we get the encrypted color image. The simulation results and security analysis show that our algorithm not only has good encryption effect, but also has the ability of resisting exhaustive attack, statistical attack and differential attack. 
85|2||A hybrid channel assignment strategy to QoS support of video-streaming over multi-channel ad hoc networks|
85|2||Lossless data hiding in JPEG bitstream|This paper proposes a method of embedding secret data into JPEG bitstream by Huffman code mapping. Although JPEG defined 162 different variable length code (VLC) for AC coefficients, many codes are not used during image compression. According to the statistical results of VLC usage in a cover, we map the unused codes to used codes. The relationships of code mapping are performed by modifying the Huffman values defined in the file header. During data hiding, we replace the codes appearing in bitstream by the mapped codes according to the secret bits. The proposed embedding method preserves the image with no quality distortion and provides more embedding capacity. 
85|2||An efficient short certificate-based signature scheme|
85|2||Towards developing consistent misuse case models|
85|2||Provably secure three-party password authenticated key exchange protocol in the standard model|
85|2||Reconciling software development models: A quasi-systematic review|PurposeThe purpose of this paper is to characterize reconciliation among the plan-driven, agile, and free/open source software models of software development.Design/methodology/approachAn automated quasi-systematic review identified 42 papers, which were then analyzed.FindingsThe main findings are: there exist distinct – organization, group and process – levels of reconciliation; few studies deal with reconciliation among the three models of development; a significant amount of work addresses reconciliation between plan-driven and agile development; several large organizations (such as Microsoft, Motorola, and Philips) are interested in trying to combine these models; and reconciliation among software development models is still an open issue, since it is an emerging area and research on most proposals is at an early stage.Research limitationsAutomated searches may not capture relevant papers in publications that are not indexed. Other data sources not amenable to execution of the protocol were not used. Data extraction was performed by only one researcher, which may increase the risk of threats to internal validity.ImplicationsThis characterization is important for practitioners wanting to be current with the state of research. This review will also assist the scientific community working with software development processes to build a common understanding of the challenges that must be faced, and to identify areas where research is lacking. Finally, the results will be useful to software industry that is calling for solutions in this area.Originality/valueThere is no other systematic review on this subject, and reconciliation among software development models is an emerging area. This study helps to identify and consolidate the work done so far and to guide future research. The conclusions are an important step towards expanding the body of knowledge in the field. 
85|2||Concept vector for semantic similarity and relatedness based on WordNet structure|
85|2||Intrusion-resilient identity-based signature: Security definition and construction|
85|2||Reversible data hiding of high payload using local edge sensing prediction|
85|2||Collision-based flexible image encryption algorithm|
85|2||Design patterns selection: An automatic two-phase method|Over many years of research and practices in software development, hundreds of software design patterns have been invented and published. Now, a question which naturally arises is how software developers select the right design patterns from all relevant patterns to solve design problems in the software design phase. To address this issue, in this paper, we propose a two-phase method to select a right design pattern. The proposed method is based on a text classification approach that aims to show an appropriate way to suggest the right design pattern(s) to developers for solving each given design problem. There are two advantages of the proposed method in comparison to previous works. First, there is no need for semi-formal specifications of design patterns and second, the suitable design patterns are suggested with their degree of similarity to the design problem. To evaluate the proposed method, we apply it on real problems and several case studies. The experimental results show that the proposed method is promising and effective. 
85|2||CONFIDDENT: A model-driven consistent and non-redundant layer-3 firewall ACL design, development and maintenance framework|
85|3|http://www.sciencedirect.com/science/journal/01641212/85/3|Novel approaches in the design and implementation of system/software architectures|
85|3||Changing attitudes towards the generation of architectural models|
85|3||A proposal to detect errors in Enterprise Application Integration solutions|Enterprise Application Integration (EAI) solutions comprise a set of specific-purpose processes that implement exogenous message workflows. The goal is to keep a number of applications’ data in synchrony or to develop new functionality on top of them. Such solutions are prone to errors because they are highly distributed and usually involve applications that were not designed with integration concerns in mind. This has motivated many authors to work on provisioning EAI solutions with fault-tolerance capabilities. In this article we analyse EAI solutions from two orthogonal perspectives: viewpoint (orchestration versus choreography) and execution model (process- versus task-based model). A review of the literature shows that current proposals are bound to a specific viewpoint or execution model or have important limitations. To address the problem, we have devised an error monitor that can be used to provision EAI solutions with fault-tolerance capabilities. Our theoretical analysis proves that the algorithms we use are computationally tractable, and our experimental results prove that they are efficient enough to be used in situations in which the workload is very high. 
85|3||Enabling correct design and formal analysis of Ambient Assisted Living systems|Ambient Assisted Living (AAL) systems intend to provide services that enable people with specific needs to live an independent and safe life. Emergency treatment services are critical, time-constrained, and require compliance to numerous non-functional (or quality) requirements. In conventional approaches, often, non-functional requirements are kept outside the modeling scope and as such, their verification is also overlooked. For this reason, the specification and verification of Non-functional requirements (NFR) in this kind of services is a key issue. This paper presents a verification approach based on timed traces semantics and a methodology based on UML-RT models (MEDISTAM-RT) to check the fulfillment of non-functional requirements, such as timeliness and safety (deadlock freeness), and to assure the correct functioning of the AAL systems. We validate this approach by its application to an Emergency Assistance System for monitoring people suffering from cardiac alteration with syncope. 
85|3||A reusable structural design for mobile collaborative applications|Architecting mobile collaborative applications has always been a challenge for designers. However, counting on a structural design as a reference can help developers to reduce risks and efforts involved in system design. This article presents a reusable architecture which helps modeling the communication and coordination services required by mobile collaborative applications to support collaboration among users. This architecture has been used as a basis for the design of several mobile systems. Two of them are presented in this article to show the applicability of the proposal to real world collaborative systems. 
85|3||Deriving detailed design models from an aspect-oriented ADL using MDD|
85|3||Towards supporting the software architecture life cycle|
85|3||Empirical findings on team size and productivity in software development|The size of software project teams has been considered to be a driver of project productivity. Although there is a large literature on this, new publicly available software repositories allow us to empirically perform further research. In this paper we analyse the relationships between productivity, team size and other project variables using the International Software Benchmarking Standards Group (ISBSG) repository. To do so, we apply statistical approaches to a preprocessed subset of the ISBSG repository to facilitate the study. The results show some expected correlations between productivity, effort and time as well as corroborating some other beliefs concerning team size and productivity. In addition, this study concludes that in order to apply statistical or data mining techniques to these type of repositories extensive preprocessing of the data needs to be performed due to ambiguities, wrongly recorded values, missing values, unbalanced datasets, etc. Such preprocessing is a difficult and error prone activity that would need further guidance and information that is not always provided in the repository. 
85|3||Wireless sensor network-based fire detection, alarming, monitoring and prevention system for Bord-and-Pillar coal mines|Fire is a major concern for those who work in underground coal mines. Coal mine fire can occur at any time and often results in partial or total evacuation of mine personnel and could result in the loss of lives. Therefore, having a warning system that is capable of detecting fire and generating an alarm is important. In this paper, we present the response time of a proposed system for detecting fire hazard in a Bord-and-Pillar coal mine panel (Hustrulid and Bullock, 2001). It uses wireless sensor networks (WSNs), and can be used to detect the exact fire location and spreading direction, and also provide the fire prevention system to stop the spread of fire to save the natural resources and the mining personnel from fire. The proposed system is capable of early detection of fire and generating alarm in case of emergencies. The performance of the proposed system has been evaluated through rigorous simulations. Simulation results show that the average network delay varies almost linearly with the increasing the number of hops. 
85|3||A lightweight framework for describing software practices|
85|3||Making sense of business process descriptions: An experimental comparison of graphical and textual notations|How effective is a notation in conveying the writer's intent correctly? This paper identifies understandability of design notations as an important aspect which calls for an experimental comparison. We compare the success of university students in interpreting business process descriptions, for an established graphical notation (BPMN) and for an alternative textual notation (based on written use-cases). Because a design must be read by diverse communities, including technically trained professionals such as developers and business analysts, as well as end-users and stakeholders from a wider business setting, we used different types of participants in our experiment. Specifically, we included those who had formal training in process description, and others who had not. Our experiments showed significant increases by both groups in their understanding of the process from reading the textual model. This was not so for the graphical model, where only the trained readers showed significant increases. This finding points at the value of educating readers of graphical descriptions in that particular notation when they become exposed to such models in their daily work. 
85|3||Automatic execution of business process models: Exploiting the benefits of Model-driven Engineering approaches|
85|3||A history-based cost-cognizant test case prioritization technique in regression testing|Software testing is typically used to verify whether the developed software product meets its requirements. From the result of software testing, developers can make an assessment about the quality or the acceptability of developed software. It is noted that during testing, the test case is a pair of input and expected output, and a number of test cases will be executed either sequentially or randomly. The techniques of test case prioritization usually schedule test cases for regression testing in an order that attempts to increase the effectiveness. However, the cost of test cases and the severity of faults are usually varied. In this paper, we propose a method of cost-cognizant test case prioritization based on the use of historical records. We gather the historical records from the latest regression testing and then propose a genetic algorithm to determine the most effective order. Some controlled experiments are performed to evaluate the effectiveness of our proposed method. Evaluation results indicate that our proposed method has improved the fault detection effectiveness. It can also been found that prioritizing test cases based on their historical information can provide high test effectiveness during testing. 
85|3||An efficient RSA-based certificateless signature scheme|Until now, the only known construction of certificateless signature scheme is mainly based on the rather new and untested assumptions related to bilinear maps. But the implementations of pairings are more time-consuming than exponentiation operator in a RSA group. As an industry standard cryptographic algorithm, RSA is widely applied in real-life scenarios and provides many interfaces for the applied software. However, to the best of our knowledge, there does not exist RSA-based certificateless signature scheme. To overcome this problem, we present a RSA-based construction of certificateless signature scheme in the paper. And the scheme is shown to be secure in the random oracles model. The security of the scheme is closely related to the RSA problem and the discrete logarithm problem. 
85|3||Further observation on proxy re-encryption with keyword search|Recently Shao et al. proposed an interesting cryptographic primitive called proxy re-encryption with keyword search (PRES). The main novelty is simultaneously realizing the functionality of proxy re-encryption and keyword search in one primitive. In this paper, we further extend their research by introducing a new primitive: constrained single-hop unidirectional proxy re-encryption supporting conjunctive keywords search (CPRE-CKS). Our results are as following: (1) In Shao's PRES scheme, the proxy can re-encrypt all the second level ciphertext. While in our CPRE-CKS proposal, the proxy can only re-encrypt those second level ciphertexts which contain the corresponding keywords. (2) We give the definition and security model for CPRE-CKS, and propose a concrete scheme and prove its security. (3) On the way to construct a secure CPRE-CKS scheme, we found a flaw in the security proof of Hwang et al.'s public key encryption with conjunctive keyword search (PECK) scheme proposed in Pairing’07. 
85|3||Achieving key privacy without losing CCA security in proxy re-encryption|
85|3||Organizational adoption of open source software|Organizations and individuals can use open source software (OSS) for free, they can study its internal workings, and they can even fix it or modify it to make it suit their particular needs. These attributes make OSS an enticing technological choice for a company. Unfortunately, because most enterprises view technology as a proprietary differentiating element of their operation, little is known about the extent of OSS adoption in industry and the key drivers behind adoption decisions. In this article we examine factors and behaviors associated with the adoption of OSS and provide empirical findings through data gathered from the US Fortune-1000 companies. The data come from each company's web browsing and serving activities, gathered by sifting through more than 278 million web server log records and analyzing the results of thousands of network probes. We show that the adoption of OSS in large US companies is significant and is increasing over time through a low-churn transition, advancing from applications to platforms. Its adoption is a pragmatic decision influenced by network effects. It is likelier in larger organizations and those with many less productive employees, and is associated with IT and knowledge-intensive work and operating efficiencies. 
85|3||Evolution and change management of XML-based systems|
85|3||Fully CCA2 secure identity-based broadcast encryption with black-box accountable authority|
85|3||Bridging the gap between requirements and design: An approach based on Problem Frames and SysML|The relation between the requirements specification and the design has been widely investigated with the aim to bridge the gap between the two artifacts. The goal is to find effective mechanisms to generate the system design starting from the analysis and specification of the requirements.This paper contributes to this research stream with an approach to create early design models from requirement artifacts. The approach weaves together the analysis and design phases favoring a tight collaboration between analysts and designers. It is based on Problem Frames, decomposition and re-composition patterns and supported by the System Modeling Language. The proposed solution has the potentiality of easing the development, shortening the development cycle and reducing the associated cost.The proposed design generation guidelines have been implemented as ATLAS Transformation Language rules in a model-based transformation process. The entire approach is model driven, allowing for the generation of the design model through transformations applied to the requirements model. The design model is automatically generated through the application of the transformation rules described in the paper. The proposed rules are fairly general and can be applied to any analysis model built according to the proposed analysis guidelines. The transformation process can be easily re-implemented using any suitable modeling tool that includes the ATLAS Transformation Language interpretation engine. 
85|3||A dynamic layout of sliding window for frequent itemset mining over data streams|
85|3||Gateway-oriented password-authenticated key exchange protocol in the standard model|
85|4|http://www.sciencedirect.com/science/journal/01641212/85/4|Decision tree classifiers sensitive to heterogeneous costs|
85|4||Rolling-horizon scheduling for energy constrained distributed real-time embedded systems|
85|4||A documentation framework for architecture decisions|In this paper, we introduce a documentation framework for architecture decisions. This framework consists of four viewpoint definitions using the conventions of ISO/IEC/IEEE 42010, the new international standard for the description of system and software architectures. The four viewpoints, a Decision Detail viewpoint, a Decision Relationship viewpoint, a Decision Chronology viewpoint, and a Decision Stakeholder Involvement viewpoint satisfy several stakeholder concerns related to architecture decision management.With the exception of the Decision Stakeholder Involvement viewpoint, the framework was evaluated in an industrial case study. The results are promising, as they show that decision views can be created with reasonable effort while satisfying many of the stakeholder concerns in decision documentation. 
85|4||Data management for component-based embedded real-time systems: The database proxy approach|
85|4||Formally based semi-automatic implementation of an open security protocol|
85|4||The novel bilateral â Diffusion image encryption algorithm with dynamical compound chaos|
85|4||Perpetual development: A model of the Linux kernel life cycle|
85|4||An efficient and secure multi-server authentication scheme with key agreement|Remote user authentication is used to validate the legitimacy of a remote log-in user. Due to the rapid growth of computer networks, many network environments have been becoming multi-server based. Recently, much research has been focused on proposing remote password authentication schemes based on smart cards for securing multi-server environments. Each of these schemes used either a nonce or a timestamp technique to prevent the replay attack. However, using the nonce technique to withstand the replay attack is potentially susceptible to the man-in-the-middle attack. Alternatively, when employing the timestamp method to secure remote password authentication, it will require the cost of implementing clock synchronization. In order to solve the above two issues, this paper proposes a self-verified timestamp technique to help the smart-card-based authentication scheme not only effectively achieve password-authenticated key agreement but also avoid the difficulty of implementing clock synchronization in multi-server environments. A secure authenticated key agreement should accomplish both mutual authentication and session key establishment. Therefore, in this paper we further give the formal proof on the execution of the proposed authenticated key agreement scheme. 
85|4||Intelligent reversible watermarking in integer wavelet domain for medical images|The prime requirement of reversible watermarking scheme is that the system should be able to restore the cover work to its original state after extracting the hidden information. Reversible watermarking approaches, therefore, have wide applications in medical and defense imagery. In this paper, an intelligent reversible watermarking approach GA-RevWM for medical images is proposed. GA-RevWM is based on the concept of block-based embedding using genetic algorithm (GA) and integer wavelet transform (IWT). GA based intelligent threshold selection scheme is applied to improve the imperceptibility for a fixed payload or vice versa. The experimental results show that GA-RevWM provides significant improvement in terms of imperceptibility for a desired level of payload against the existing approaches. 
85|4||Factors affecting the success of Open Source Software|With the rapid rise in the use of Open Source Software (OSS) in all types of applications, it is important to know which factors can lead to OSS success. OSS projects evolve and transform over time; therefore success must be examined longitudinally over a period of time. In this research, we examine two measures of project success: project popularity and developer activity, of 283 OSS projects over a span of 3 years, in order to observe changes over time. A comprehensive research model of OSS success is developed which includes both extrinsic and intrinsic attributes. Results show that while many of the hypothesized relationships are supported, there were marked differences in some of the relationships at different points in time lending support to the notion that different factors need to be emphasized as the OSS project unfolds over time. 
85|4||High performance dynamic voltage/frequency scaling algorithm for real-time dynamic load management|
85|4||Free and Open Source Software versus Internet content filtering and censorship: A case study|
85|4||Debugging applications created by a Domain Specific Language: The IPAC case|
85|4||Attribute-based strong designated-verifier signature scheme|
85|4||EClass: An execution classification approach to improving the energy-efficiency of software via machine learning|Energy efficiency at the software level has gained much attention in the past decade. This paper presents a performance-aware frequency assignment algorithm for reducing processor energy consumption using Dynamic Voltage and Frequency Scaling (DVFS). Existing energy-saving techniques often rely on simplified predictions or domain knowledge to extract energy savings for specialized software (such as multimedia or mobile applications) or hardware (such as NPU or sensor nodes). We present an innovative framework, known as EClass, for general-purpose DVFS processors by recognizing short and repetitive utilization patterns efficiently using machine learning. Our algorithm is lightweight and can save up to 52.9% of the energy consumption compared with the classical PAST algorithm. It achieves an average savings of 9.1% when compared with an existing online learning algorithm that also utilizes the statistics from the current execution only. We have simulated the algorithms on a cycle-accurate power simulator. Experimental results show that EClass can effectively save energy for real life applications that exhibit mixed CPU utilization patterns during executions. Our research challenges an assumption among previous work in the research community that a simple and efficient heuristic should be used to adjust the processor frequency online. Our empirical result shows that the use of an advanced algorithm such as machine learning can not only compensate for the energy needed to run such an algorithm, but also outperforms prior techniques based on the above assumption. 
85|4||A fast algorithm for Huffman decoding based on a recursion Huffman tree|This paper focuses on the time efficiency of Huffman decoding. In this paper, we utilize numerical interpretation to speed up the decoding process. The proposed algorithm firstly transforms the given Huffman tree into a recursion Huffman tree. Then, with the help of the recursion Huffman tree, the algorithm has the possibility to decode more than one symbol at a time if the minimum code length is less than or equal to half of the width of the processing unit. When the minimum code length is larger than the half of the width of the processing unit, the proposed method can still increase the average symbols decoded in one table access (thus speeding up the decoding time). In fact, the experimental results of the test files show that the average number of decoded symbols at one time for the proposed method ranges from 1.91 to 2.13 when the processing unit is 10. The experimental comparisons show that, compared to the conventional binary tree search method and the level-compressed Huffman decoding method, the decoding time of the proposed method is a great improvement. 
85|4||Improved preimage attack on one-block MD4|MD4 is a hash function designed by Rivest in 1990. The design philosophy of many important hash functions, such as MD5, SHA-1 and SHA-2, originated from that of MD4. We propose an improved preimage attack on one-block MD4 with the time complexity 295 MD4 compression function operations, as compared to the 21071 complexity of the previous attack by Aoki et al. (SAC 2008). The attack is based on previous methods, but introduces new techniques. We also use the same techniques to improve the pseudo-preimage and preimage attacks on Extended MD4 with 225.2 and 212.6 improvement factor, as compared to previous attacks by Sasaki et al. (ACISP 2009). 
85|4||Handling timing constraints violations in soft real-time applications as exceptions|
85|4||Modularity analysis of use case implementations|
85|5|http://www.sciencedirect.com/science/journal/01641212/85/5|A Self-adaptive hierarchical monitoring mechanism for Clouds|While Cloud computing offers the potential to dramatically reduce the cost of software services through the commoditization of IT assets and on-demand usage patterns, one has to consider that Future Internet applications raise the need for environments that can facilitate real-time and interactivity and thus pose specific requirements to the underlying infrastructure. The latter, should be able to efficiently adapt resource provisioning to the dynamic Quality of Service (QoS) demands of such applications. To this direction, in this paper we present a monitoring system that facilitates on-the-fly self-configuration in terms of both the monitoring time intervals and the monitoring parameters. The proposed approach forms a multi-layered monitoring framework for measuring QoS at both application and infrastructure levels targeting trigger events for runtime adaptability of resource provisioning estimation and decision making. Besides, we demonstrate the operation of the implemented mechanism and evaluate its effectiveness using a real-world application scenario, namely Film Post Production. 
85|5||The impact of accounting for special methods in the measurement of object-oriented class cohesion on refactoring and fault prediction activities|
85|5||DRMFS: A file system layer for transparent access semantics of DRM-protected contents|In many digital rights management (DRM) schemes, only a specialized application can decode DRM-protected contents. This restriction is harmful to users because they want to use their purchased digital contents with their preferred applications. To relax this restriction, DRM technology should provide transparent access semantics of DRM-protected contents to authorized applications. Some previous schemes achieve limited transparent access semantics but have efficiency and applicability problems. In this paper, we propose a DRM control scheme at the file system layer (DRMFS) that achieves transparent access semantics of DRM-protected contents with efficiency, applicability, and portability. Since DRMFS is working at the file system layer, any authorized application can access DRM-protected content in the same way as using general files. To implement a prototype of DRMFS, we use the Filesystem in Userspace (FUSE) library that is a well known library used to develop user level file systems. We explain details of the implementation and evaluate its performance. The evaluation results show that DRMFS has acceptable overheads. 
85|5||Noisy data elimination using mutual k-nearest neighbor for classification mining|
85|5||UniSpaCh: A text-based data hiding method using Unicode space characters|This paper proposes a text-based data hiding method to insert external information into Microsoft Word document. First, the drawback of low embedding efficiency in the existing text-based data hiding methods is addressed, and a simple attack, DASH, is proposed to reveal the information inserted by the existing text-based data hiding methods. Then, a new data hiding method, UniSpaCh, is proposed to counter DASH. The characteristics of Unicode space characters with respect to embedding efficiency and DASH are analyzed, and the selected Unicode space characters are inserted into inter-sentence, inter-word, end-of-line and inter-paragraph spacings to encode external information while improving embedding efficiency and imperceptivity of the embedded information. UniSpaCh is also reversible where the embedded information can be removed to completely reconstruct the original Microsoft Word document. Experiments were carried out to verify the performance of UniSpaCh as well as comparing it to the existing space-manipulating data hiding methods. Results suggest that UniSpaCh offers higher embedding efficiency while exhibiting higher imperceptivity of white space manipulation when compared to the existing methods considered. In the best case scenario, UniSpaCh produces output document of size almost 9 times smaller than that of the existing method. 
85|5||Efficient audit service outsourcing for data integrity in clouds|Cloud-based outsourced storage relieves the client's burden for storage management and maintenance by providing a comparably low-cost, scalable, location-independent platform. However, the fact that clients no longer have physical possession of data indicates that they are facing a potentially formidable risk for missing or corrupted data. To avoid the security risks, audit services are critical to ensure the integrity and availability of outsourced data and to achieve digital forensics and credibility on cloud computing. Provable data possession (PDP), which is a cryptographic technique for verifying the integrity of data without retrieving it at an untrusted server, can be used to realize audit services.In this paper, profiting from the interactive zero-knowledge proof system, we address the construction of an interactive PDP protocol to prevent the fraudulence of prover (soundness property) and the leakage of verified data (zero-knowledge property). We prove that our construction holds these properties based on the computation Diffie–Hellman assumption and the rewindable black-box knowledge extractor. We also propose an efficient mechanism with respect to probabilistic queries and periodic verification to reduce the audit costs per verification and implement abnormal detection timely. In addition, we present an efficient method for selecting an optimal parameter value to minimize computational overheads of cloud audit services. Our experimental results demonstrate the effectiveness of our approach. 
85|5||Coopetitive relationships in cross-functional software development teams: How to model and measure?|Understanding simultaneous cooperative and competitive (coopetitive) dynamics in cross-functional software development teams is fundamental to the success of software development process. The recent coopetition research is, however, hampered by a lack of conceptual focus, and the corresponding inconsistent treatment of the constructs associated with cross-functional coopetitive relationships. This study conceptualizes and operationalizes the multi-dimensional construct of cross-functional coopetition, and then presents an instrument for measuring this construct. Cross-functional coopetition is conceptualized with 5 distinct and independent constructs; 3 of them are related to cross-functional cooperation, and 2 are associated with cross-functional competition. The data collected from 115 software development project managers in Australia confirms the applicability of the constructs and their measures. This study contributes to the extant literature by providing a consensus on the conceptualization of cross-functional coopetitive behaviors, particularly in multi-party software development teams. The conceptual basis for cross-functional coopetition and its instrument will aid researchers and project managers interested in understanding coopetition in cross-functional collaborative contexts. Research and practical implications are discussed. 
85|5||Sonata: Flexible connections between interaction and business spaces|
85|5||Random grid-based visual secret sharing for general access structures with cheat-preventing ability|Conventional visual secret sharing (VSS) encodes a secret image into shares which are m times as big as the secrets. m is called pixel expansion. Random grid (RG) is an approach to solve pixel expansion problem. However, the existing VSS methods using RGs are confined to (2,n),(n,n) and (k,n). In this paper, RG-based VSS schemes for general access structures are proposed. The proposed algorithms can encode one secret image into n random grids while qualified sets can recover the secret visually. Furthermore, a cheating immune method is also presented to provided extra ability of cheat-preventing for RG-based VSS. Experimental results demonstrate that both the RG-based VSS for general access structures and cheating immune method are effective. More complicated sharing strategies can be implemented. 
85|5||An adaptive model-free resource and power management approach for multi-tier cloud environments|With the development of cloud environments serving as a unified infrastructure, the resource management and energy consumption issues become more important in the operations of such systems. In this paper, we investigate adaptive model-free approaches for resource allocation and energy management under time-varying workloads and heterogeneous multi-tier applications. Specifically, we make use of measurable metrics, including throughput, rejection amount, queuing state, and so on, to design resource adjustment schemes and to make control decisions adaptively. The ultimate objective is to guarantee the summarized revenue of the resource provider while saving energy and operational costs. To validate the effectiveness, performance evaluation experiments are performed in a simulated environment, with realistic workloads considered. Results show that with the combination of long-term adaptation and short-term adaptation, the fluctuation of unpredictable workloads can be captured, and thus the total revenue can be preserved while balancing the power consumption as needed. Furthermore, the proposed approach can achieve better effect and efficiency than the model-based approaches in dealing with real-world workloads. 
85|5||Efficient and robust probabilistic guarantees for real-time tasks|
85|5||A graphical-based password keystroke dynamic authentication system for touch screen handheld mobile devices|
85|5||Data embedding using pixel value differencing and diamond encoding with multiple-base notational system|We propose a new data hiding method that adaptively embeds data into pixel pairs using the diamond encoding (DE) technique. Because the human eyes tolerate more changes in edge and texture areas than in smooth areas, and pixel pairs in these areas often possess larger differences, the method exploits pixel value differences (PVD) to estimate the base of digits to be embedded into pixel pairs. Pixel pairs with larger differences are embedded with digits in larger base than those pixel pairs with smaller differences to maximize the payload and image quality. Two sophisticated pixel pair adjustment processes are provided to maintain the division consistency and to eliminate the overflow/underflow problem. Experimental results reveal that the proposed method offers better embedding performance compared to prior PVD-based works in terms of payload and image quality. 
85|5||Improving test efficiency through system test prioritization|
85|5||Thresholds for error probability measures of business process models|
85|5||A loss recovery approach for reliable application layer multicast|
85|5||A signature-based Grid index design for main-memory RFID database applications|A large-scale RFID application often requires a highly efficient database system in data processing. This research is motivated by the strong demand for an efficient index structure design for main-memory database systems of RFID applications. In this paper, a signature-based Grid index structure is proposed for efficient data queries and storage. An efficient methodology is proposed to locate duplicates and to execute batch deletions and range queries based on application domain knowhow. The capability of the design is implemented in an open source main-memory database system H2 and evaluated by realistic workloads of RFID applications. 
85|6|http://www.sciencedirect.com/science/journal/01641212/85/6|A decade of agile methodologies: Towards explaining agile software development|Ever since the agile manifesto was created in 2001, the research community has devoted a great deal of attention to agile software development. This article examines publications and citations to illustrate how the research on agile has progressed in the 10 years following the articulation of the manifesto. Specifically, we delineate the conceptual structure underlying agile scholarship by performing an analysis of authors who have made notable contributions to the field. Further, we summarize prior research and introduce contributions in this special issue on agile software development. We conclude by discussing directions for future research and urging agile researchers to embrace a theory-based approach in their scholarship. 
85|6||Coordination in co-located agile software development projects|Agile software development provides a way to organise the complex task of multi-participant software development while accommodating constant project change. Agile software development is well accepted in the practitioner community but there is little understanding of how such projects achieve effective coordination, which is known to be critical in successful software projects. A theoretical model of coordination in the agile software development context is presented based on empirical data from three cases of co-located agile software development. Many practices in these projects act as coordination mechanisms, which together form a coordination strategy. Coordination strategy in this context has three components: synchronisation, structure, and boundary spanning. Coordination effectiveness has two components: implicit and explicit. The theoretical model of coordination in agile software development projects proposes that an agile coordination strategy increases coordination effectiveness. This model has application for practitioners who want to select appropriate practices from agile methods to ensure they achieve coordination coverage in their project. For the field of information systems development, this theory contributes to knowledge of coordination and coordination effectiveness in the context of agile software development. 
85|6||Obstacles to decision making in Agile software development teams|The obstacles facing decision making in Agile development are critical yet poorly understood. This research examines decisions made across four stages of the iteration cycle: Iteration Planning, Iteration Execution, Iteration Review and Iteration Retrospective. A mixed method approach was employed, whereby a focus group was initially conducted with 43 Agile developers and managers to determine decisions made at different points of the iteration cycle. Subsequently, six illustrative mini cases were purposefully conducted as examples of the six obstacles identified in these focus groups. This included interviews with 18 individuals in Agile projects from five different organizations: a global consulting organization, a multinational communications company, two multinational software development companies, and a large museum organization. This research contributes to Agile software development literature by analyzing decisions made during the iteration cycle and identifying six key obstacles to these decisions. Results indicate the six decision obstacles are unwillingness to commit to decisions; conflicting priorities; unstable resource availability; and lack of: implementation; ownership; empowerment. These six decision obstacles are mapped to descriptive decision making principles to demonstrate where the obstacles affect the decision process. The effects of these obstacles include a lack of longer-term, strategic focus for decisions, an ever-growing backlog of delayed work from previous iterations, and a lack of team engagement. 
85|6||Understanding post-adoptive agile usage: An exploratory cross-case analysis|While past research has contributed to the understanding of how organizations adopt agile methodologies (AM), little is known about their post-adoptive usage in organizations. By integrating theories from systems development methodologies, diffusion of innovations, and agile methodology literature, this paper proposes a new model that identifies a set of critical factors pertinent to post-adoptive usage of agile practices. This model is used to inform analysis of post-adoptive usage of agile practices in two major organizations. The results indicate relative advantage, team attitude and technical competence, championing, and top management support (TMS) are the key factors determining the extent to which agile practices can be assimilated into an organization. Specifically, both findings and this model confirm that the deeper the assimilation of agile practices into the organization, the better understanding of how assimilation leads to specific improvements in its systems development outcomes. 
85|6||Reconciling perspectives: A grounded theory of how people manage the process of software development|Social factors are significant cost drivers for the process of software development. In this field study we generate a grounded theory of how people manage the process of software development. The main concern of engineers involved in the process of software development is getting the job done. To get the job done, people engage in a four-stage process of Reconciling Perspectives. Reconciling Perspectives represents an attempt to converge individuals’ points of view or perspectives about a software project. The process emphasizes the importance of individuals’ abilities to both reach out and engage in negotiations and create shelter from environmental noise to bring a software project to fruition. 
85|6||âLeagileâ software development: An experience report analysis of the application of lean approaches in agile software development|In recent years there has been a noticeable shift in attention from those who use agile software development toward lean software development, often labelled as a shift “from agile to lean”. However, the reality may not be as simple or linear as this label implies. To provide a better understanding of lean software development approaches and how they are applied in agile software development, we have examined 30 experience reports published in past agile software conferences in which experiences of applying lean approaches in agile software development were reported. The analysis identified six types of lean application. The results of our study show that lean can be applied in agile processes in different manners for different purposes. Lean concepts, principles and practices are most often used for continuous agile process improvement, with the most recent introduction being the kanban approach, introducing a continuous, flow-based substitute to time-boxed agile processes. 
85|6||Automatic test case selection for regression testing of composite service based on extensible BPEL flow graph|Services are highly reusable, flexible and loosely coupled components whose changes make the evolution and maintenance of composite services more complex. The changes of composite service mainly cover three types, i.e., the processes, bindings, and interfaces. In this article, an approach is proposed to select test cases for regression testing of different versions of BPEL (business process execution language) composite service where these changes are involved. The approach identifies the changes by performing control flow analysis and comparing the paths in a new version of composite service with those in the old one using a kind of eXtensible BPEL flow graph (XBFG). Message sequence is appended to XBFG path so that XBFG can fully describe the behavior of composite service. The binding and predicate constraint information added in different XBFG elements can be used for path selection and even for test case generation. Both theoretic analysis and case study show that the proposed approach is effective. 
85|6||Efficient (n, t, n) secret sharing schemes|Recently, Harn and Lin introduced a notion of strong t-consistency of a (t, n) secret sharing scheme and proposed a strong (n, t, n) verifiable secret sharing (VSS). In this paper, we propose a strong (n, t, n) VSS which is more efficient than Harn and Lin's VSS. Using the same approach, we propose a (n, t, n) multi-secret sharing scheme (MSS) to allow shareholders to share n − t + 1 secrets. Also, the proposed (n, t, n) MSS can be modified to include the verifiable feature. All proposed schemes are unconditionally secure and are based on Shamir's (t, n) secret sharing scheme. 
85|6||An improved swarm optimized functional link artificial neural network (ISO-FLANN) for classification|
85|6||Mining frequent patterns from dynamic data streams with data load management|In this paper, we study the practical problem of frequent-itemset discovery in data-stream environments which may suffer from data overload. The main issues include frequent-pattern mining and data-overload handling. Therefore, a mining algorithm together with two dedicated overload-handling mechanisms is proposed. The algorithm extracts basic information from streaming data and keeps the information in its data structure. The mining task is accomplished when requested by calculating the approximate counts of itemsets and then returning the frequent ones. When there exists data overload, one of the two mechanisms is executed to settle the overload by either improving system throughput or shedding data load. From the experimental data, we find that our mining algorithm is efficient and possesses good accuracy. More importantly, it could effectively manage data overload with the overload-handling mechanisms. Our research results may lead to a feasible solution for frequent-pattern mining in dynamic data streams. 
85|6||Sharetouch: A system to enrich social network experiences for the elderly|The Sharetouch system is designed for raising users’ participation in community events. We put three subsystems into Sharetouch: (1) community pond, (2) Waterball interactive game, and (3) multimedia sharing. Sharetouch is based on an optical touch device designed by the Joyplux Company with an infrared LED and camera. This device can support multi-touch functions within a large display area. The software of Sharetouch was developed within XNA and .NET frameworks. We project the users as fish in our community pond. Sharetouch displays all the friends as fish when the users log into the system. Therefore, the number of fish equals the number of friends of the users. This design encourages users to make more friends to increase the number of fish. Waterball is a game that combines virtual images and real objects. The concept is based on the Nintendo Wii games, as players hold controllers (real objects) to play the games (virtual images). We also apply the concept of the cloud flash drive to multimedia sharing to avoid the trouble of carrying a real flash disk. This study employed the TAM measure to measure the validity of Sharetouch in this social platform. Our findings indicated that all proposed hypotheses had a positive and significant impact on the intention of older people to interact with Sharetouch. Unlike the computer-based system, Sharetouch is created as a user-friendly interface system. Sharetouch can enrich the users’ social network experiences through its hardware and software architectures. 
85|6||A family of case studies on business process mining using MARBLE|
85|6||Quasi-static fault-tolerant scheduling schemes for energy-efficient hard real-time systems|
85|6||Comparison of scheduling schemes for on-demand IaaS requests|
85|6||Strongly secure certificateless short signatures|Short certificateless signatures have come into limelight in recent years. On the one hand, the property of certificateless eliminates the certificate management problem in traditional PKI and the key-escrow problem in some ID-based signature schemes. On the other hand, due to the short signature length, short certificateless signatures can be applied to systems where signatures are typed in by human or systems with low-bandwidth channels and/or low-computation power, such as PDAs or cell phones. However, there has been a trade-off between short certificateless signature schemes and their security levels. All existing short certificateless signature schemes can only be proven secure against a normal type adversary rather than a stronger one, who can obtain valid certificateless signatures under public keys replaced by the adversary. In this paper, we solve this open problem by given an efficient strongly secure short certificateless signature scheme. The proposed scheme has the following features. Firstly, it is strongly unforgeable. Secondly, the security can be reduced to the Computational Diffie–Hellman (CDH) assumption – a classic complexity assumption. Lastly, the proposed scheme is provably secure against adversaries with access to a super signing oracle which generates valid certificateless signatures of messages and public keys chosen by the adversary (without providing the corresponding secret values). 
85|6||A symbolic analysis framework for static analysis of imperative programming languages|
85|6||Goal alignment in process improvement|Process improvement should improve an organisation's ability to achieve its business goals. While mapping an organisation's strategic goals through various layers of management is common, such mapping does not seem to continue through to their processes that create value to the organisation. Despite a number of process improvement methods being available, and almost two decades of experience with those methods, many process improvement projects do not end successfully.We explore the impact process assessment has on process improvement. In particular, we study the alignment of an organisation's process goals to its business goals; and the contribution of process assessment to this goal alignment. This paper illustrates the data gathered through industry survey reflecting the lack of focus on and alignment of organisation's business goals throughout process improvement. The results indicate that there is little knowledge and experience in industry in aligning the process goals and organisation's business goals. This, in turn, could explain the unsuccessful process improvement efforts or perhaps even the skepticism towards process improvement in general. 
85|7|http://www.sciencedirect.com/science/journal/01641212/85/7|Software ecosystems: Taking software development beyond the boundaries of the organization|
85|7||A longitudinal case study of an emerging software ecosystem: Implications for practice and theory|Software ecosystems is an emerging trend within the software industry, implying a shift from closed organizations and processes towards open structures, where actors external to the software development organization are becoming increasingly involved in development. This forms an ecosystem of organizations that are related through the shared interest in a software product, leading to new opportunities and new challenges to the industry and its organizational environment. To understand why and how this change occurs, we have followed the development of a software product line organization for a period of approximately five years. We have studied their change from a waterfall-like approach, via agile software product line engineering, towards an emerging software ecosystem. We discuss implications for practice, and propose a nascent theory on software ecosystems. We conclude that the observed change has led to an increase in collaboration across (previously closed) organizational borders, and to the development of a shared value consisting of two components: the technology (the product line, as an extensible platform), and the business domain it supports. Opening up both the technical interface of the product and the organizational interfaces are key enablers of such a change. 
85|7||From proprietary to open sourceâGrowing an open source ecosystem|In today's business and software arena, Free/Libre/Open Source Software has emerged as a promising platform for software ecosystems. Following this trend, more and more companies are releasing their proprietary software as open source, forming a software ecosystem of related development projects complemented with a social ecosystem of community members. Since the trend is relatively recent, there are few guidelines on how to create and maintain a sustainable open source ecosystem for a proprietary software. This paper studies the problem of building open source communities for industrial software that was originally developed as closed source. Supporting processes, guidelines and best practices are discussed and illustrated through an industrial case study. The research is paving the road for new directions in growing a thriving open source ecosystem. 
85|7||Understanding the role of licenses and evolution in open architecture software ecosystems|The role of software ecosystems in the development and evolution of open architecture systems whose components are subject to different licenses has received insufficient consideration. Such systems are composed of components potentially under two or more licenses, open source or proprietary or both, in an architecture in which evolution can occur by evolving existing components, replacing them, or refactoring. The software licenses of the components both facilitate and constrain the system's ecosystem and its evolution, and the licenses’ rights and obligations are crucial in producing an acceptable system. Consequently, software component licenses and the architectural composition of a system help to better define the software ecosystem niche in which a given system lies. Understanding and describing software ecosystem niches for open architecture systems is a key contribution of this work. An example open architecture software system that articulates different niches is employed to this end. We examine how the architecture and software component licenses of a composed system at design time, build time, and run time help determine the system's software ecosystem niche and provide insight and guidance for identifying and selecting potential evolutionary paths of system, architecture, and niches. 
85|7||Shades of gray: Opening up a software producing organization with the open software enterprise model|
85|7||Scaling up software architecture analysis|
85|7||Methodological construction of product-form stochastic Petri nets for performance evaluation|
85|7||Distributed goal-oriented computing|
85|7||Profiling all paths: A new profiling technique for both cyclic and acyclic paths|
85|7||Dynamic refinement of search engines results utilizing the user intervention|Nowadays, modern search engines quite satisfactorily answer users’ queries, but the top results returned are not always relevant to the data the user is actually looking for. Hence, considerable efforts are made by search engines in order to rank the most relevant to the query results at the top. This work addresses the above problem and improves the performance of a search engine, especially when it comes to queries which have for example twofold meanings. The matter which the user is interested in is identified based on the results that he/she chooses, and then the most relevant ones are ranked higher. In addition, the results are recognized not only as text but also as semantic entities, which contain various semantic features. The semantic relation between results and text coverage are used as the main tool to achieve an optimized ranking, as opposed to other research papers so far. As a result, a new meta search application is developed, which, given a set of terms, combines Google results and then reorganizes (re-ranks) them based on the disambiguation offered by user clicks. In particular, after a ranking is achieved, the user makes a choice (click), the ranking is updated and the process is repeated. In order to prove our claims, apart from the description of the algorithm for refining the ranking of results, a web application has been developed, which was used to test the effectiveness of the system proposed. 
85|7||Job allocation strategies for energy-aware and efficient Grid infrastructures|Complex distributed architectures, like Grid, supply effective platforms to solve computations on huge datasets, often at the cost of increased power consumption. This energy issue affects the sustainability of the infrastructures and increases their environmental impact. On the other hand, due to Grid heterogeneity and scalability, possible power savings could be achieved if effective energy-aware allocation policies were adopted. These policies are meant to implement a better coupling between application requirements and the Grid resources, also taking energy parameters into account. In this paper, we discuss different allocation strategies which address jobs submitted to Grid resources, subject to efficiency and energy constraints. Our aim is to analyze the potential benefits that can be obtained from the adoption of a metric able to capture both performance and energy-savings. Based on an experimental study, we simulated two alternative scenarios aimed at comparing the behavior of different strategies for allocating jobs to resources. Moreover we introduced the Performance/Energy Trade-off function as a useful means to evaluate the tendency of an allocation strategy toward efficiency or power consumption. Our conclusion seems to suggest that performance and energy-savings are not always enemies, and these objectives may be combined if suitable energy metrics are adopted. 
85|7||Balancing software engineering education and industrial needs|In the world of information and communications technologies the demand for professionals with software engineering skills grows at an exponential rate. On this ground, we have conducted a study to help both academia and the software industry form a picture of the relationship between the competences of recent graduates of undergraduate and graduate software engineering programmes and the tasks that these professionals are to perform as part of their jobs in industry. Thanks to this study, academia will be able to observe which skills demanded by industry the software engineering curricula do or do not cater for, and industry will be able to ascertain which tasks a recent software engineering programme graduate is well qualified to perform. The study focuses on the software engineering knowledge guidelines provided in SE2004 and GSwE2009, and the job profiles identified by Career Space. 
85|7||Blackboard architecture to integrate components and agents in heterogeneous distributed eLearning systems: An application for learning to program|To build complete and complex eLearning systems, eLearning engineers are used to applying standards that facilitate sharing information as well as distributed service-oriented architectures that provide reuse and interoperability by means of component integration. These concepts lead us to a Component-based Development Process that will allow us to implement tools that give full support to the teaching/learning process, taking advantage of the synergy effect created by the integration of the different components. Thus, throughout this article we analyse the proposals from the most relevant consortia concerned with eLearning standards, showing their service oriented approaches and the middleware technologies which can be used to implement them. This analysis will demonstrate that the use of middleware technologies that use the definition of services’ interface can limit the reuse and interoperability requisites desired by the main standards consortia. Then, we will show a proposal which tries to solve this shortfall, using a blackboard-based architecture for integrating and communicating heterogeneous distributed components, as well as a user environment that also allows us to perform component integration. As an example, we will demonstrate how we have built an application for learning to program by applying our approach and following a Component-based Development Process to implement different components (services, agents, clients, etc.) that integrate it. Hence, we will argue that using blackboard architecture and a Component-based Development Process helps us to solve the identified shortcomings. 
85|7||Octopus: An Upperware based system for building personal pervasive environments|
85|7||Malware characteristics and threats on the internet ecosystem|Malware encyclopedias now play a vital role in disseminating information about security threats. Coupled with categorization and generalization capabilities, such encyclopedias might help better defend against both isolated and clustered specimens.In this paper, we present Malware Evaluator, a classification framework that treats malware categorization as a supervised learning task, builds learning models with both support vector machines and decision trees and finally, visualizes classifications with self-organizing maps. Malware Evaluator refrains from using readily available taxonomic features to produce species classifications. Instead, we generate attributes of malware strains via a tokenization process and select the attributes used according to their projected information gain. We also deploy word stemming and stopword removal techniques to reduce dimensions of the feature space. In contrast to existing approaches, Malware Evaluator defines its taxonomic features based on the behavior of species throughout their life-cycle, allowing it to discover properties that previously might have gone unobserved. The learning and generalization capabilities of the framework also help detect and categorize zero-day attacks. Our prototype helps establish that malicious strains improve their penetration rate through multiple propagation channels as well as compact code footprints; moreover, they attempt to evade detection by resorting to code polymorphism and information encryption. Malware Evaluator also reveals that breeds in the categories of Trojan, Infector, Backdoor, and Worm significantly contribute to the malware population and impose critical risks on the Internet ecosystem. 
85|7||Loop fusion and reordering for register file optimization on stream processors|
85|7||Coding-error based defects in enterprise resource planning software: Prevention, discovery, elimination and mitigation|
85|8|http://www.sciencedirect.com/science/journal/01641212/85/8|Improving VRSS-based vulnerability prioritization using analytic hierarchy process|
85|8||Complex event processing with T-REX|
85|8||Adaptive co-scheduling for periodic application and update transactions in real-time database systems|
85|8||On âExploring alternatives for transition verificationâ|
85|8||Utilizing Layered Taxation to provide incentives in P2P streaming systems|
85|8||SEProf: A high-level software energy profiling tool for an embedded processor enabling power management functions|
85|8||Investigating intentional distortions in software cost estimation â An exploratory study|
85|8||Ordering features by category|
85|8||Context-oriented programming: A software engineering perspective|The implementation of context-aware systems can be supported through the adoption of techniques at the architectural level such as middlewares or component-oriented architectures. It can also be supported by suitable constructs at the programming language level. Context-oriented programming (COP) is emerging as a novel paradigm for the implementation of this kind of software, in particular in the field of mobile and ubiquitous computing. The COP paradigm tackles the issue of developing context-aware systems at the language-level, introducing ad hoc language abstractions to manage adaptations modularization and their dynamic activation. In this paper we review the state of the art in the field of COP in the perspective of the benefits that this technique can provide to software engineers in the design and implementation of context-aware applications. 
85|8||Generalized aggregate Quality of Service computation for composite services|
85|8||An encoding scheme based on fractional number for querying and updating XML data|
85|8||A user-friendly secret image sharing scheme with reversible steganography based on cellular automata|Secret image sharing is a mechanism to protect a secret image among a group of participants by encrypting the secret into shares and decrypting the secret with sufficient shares. Conventional schemes generate meaningless shares, which are hard to identify and lead to suspicion of secret image encryption. To overcome these problems, sharing schemes with steganography were presented. The meaningless shared data were embedded into the cover image to form stego images. However, distorted stego images cannot be reverted to original. In this work, a novel secret image sharing scheme with reversible steganography is proposed. Main contribution of this work is that two-dimensional reversible cellular automata with memory is utilized to encrypt a secret image into shared data, which are then embedded into cover image for forming stego images. By collecting sufficient stego images, not only the secret image is lossless reconstructed, but also distorted stego image is reverted to original. Simulation results shows that low computation cost and pleasing stego image quality are also achieved by the proposed scheme. 
85|8||Performance evaluation of moment-based watermarking methods: A review|
85|8||Propagating changes between aligned process models|
85|8||Mitigating starvation of Linux CPU-bound processes in the presence of network I/O|In prior research work, it has been demonstrated that Linux can starve CPU-bound processes in the presence of network I/O. The starvation of Linux CPU-bound processes occurs under the two Linux schedulers, namely the 2.6 O(1) scheduler and the more recent 2.6 Completely Fair Scheduler (CFS). In this paper, we analyze the underlying root causes of this starvation problem and we propose effective solutions that can mitigate such starvation. We present detailed implementations of our proposed solutions for both O(1) and CFS Linux schedulers. We empirically evaluate the effectiveness of our proposed solutions in terms of execution time and incoming traffic load. For our experimental study and analysis, we consider two types of mainboard architectures: Uni-Processing (UP) and Symmetric Multi-Processing (SMP). Our empirical results show that the proposed solutions are highly effective in mitigating the starvation problem for CPU-bound processes with no negative impact on the performance of network I/O-bound processes. 
85|8||Validated templates for specification of complex LTL formulas|
85|8||Analysis and application of an outsourcing risk framework|There is much reported research on risk, and risk management but research on strategic IT system development outsourcing risk, from the client perspective, remains unclear. A literature-based conceptual risk framework for strategic IT system development outsourcing from the client perspective has been developed. We then investigate, (1) critical client risks for strategic IT system development outsourcing projects, and (2) the most common critical client risk factors for such projects. In order to identify any serious omissions in our framework an initial validation of the risk framework is provided through a review of nine published cases of unsuccessful strategic IT system development outsourcing projects. The risks critical to a client are associated with complexity, contract, execution, financial, legal, the organizational environment, planning and control, scope and requirements, the team, and the user. Risks manifest in all nine published cases, include (1) complexity and (2) the team. Three risk factors not previously identified in the initial framework are included in a revised framework. The risk framework assisted us in identifying a number of critical risk factors affecting the outcome of strategic IT system development outsourcing projects. 
85|8||Using enterprise architecture and technology adoption models to predict application usage|
85|9|http://www.sciencedirect.com/science/journal/01641212/85/9|Special issue: Selected papers from the 9th Working IEEE/IFIP Conference on Software Architecture (WICSA 2011)|
85|9||Collaborative prioritization of architectural concerns|
85|9||RCDA: Architecting as a risk- and cost management discipline|We propose to view architecting as a risk- and cost management discipline. This point of view helps architects identify the key concerns to address in their decision making, by providing a simple, relatively objective way to assess architectural significance. It also helps business stakeholders to align the architect's activities and results with their own goals. We examine the consequences of this point of view on the architecture process. The point of view is the basis of RCDA, the Risk- and Cost Driven Architecture approach. So far, more than 150 architects have received RCDA training. For a majority of the trainees, RCDA has a significant positive impact on their architecting work. 
85|9||Reference architecture, metamodel, and modeling principles for architectural knowledge management in information technology services|Capturing and sharing design knowledge such as architectural decisions is becoming increasingly important in firms providing professional Information Technology (IT) services such as enterprise application development and strategic outsourcing. Methods, models, and tools supporting explicit knowledge management strategies have been proposed in recent years; however, several challenges remain unaddressed. In this paper, we extend our previous work to overcome these challenges and to satisfy the requirements of an additional user group, presales architects that are responsible for IT service solution proposals. In strategic outsourcing, such solution proposals require complex, contractually relevant design decisions concerning many different resources such as IT infrastructures, people, and real estate. To support both presales and project architects, we define a common reference architecture and a decision process-oriented metamodel. We also present a tool implementation of these concepts and discuss their application to outsourcing proposals and application development projects. Finally, we establish twelve decision modeling principles and practices that capture the practical experience gained and lessons learned during the application of our decision modeling concepts to both proposal development and architecture design work on projects. 
85|9||Industrial architectural assessment using TARA|
85|9||Dynamic service placement and replication framework to enhance service availability using team formation algorithm|The motivation of this work is to reduce the complexity in managing and administering services in the ever growing distributed environment via automated service placement and replication with team formation algorithm. The team formation algorithm is designed in a way that it would continuously search for resources with better performance and pool resources together to achieve better availability. The main intention of this work is not to replace the human administrators but to provide a better alternative in managing services in dynamic distributed environment. Instructions from the administrators are still required but at a different level. Administrators are freed from making low level decisions such as to decide the actual placement of the services and design the failover capabilities for each of the services. The evaluation results showed that the framework is capable of managing resources according to the requirements given by administrator, even during in the event of multiple consecutive resources failure. The proposed solution had the probability of 72.1% of its services that are still available after 83.3% of the available resources were shut down while conventional failover solution using three redundant units had only the probability of 40.8% of services that are still available. 
85|9||Learning extended FSA from software: An empirical assessment|
85|9||Cryptanalyzing a chaos-based image encryption algorithm using alternate structure|Recently, a chaos-based image encryption algorithm with an alternate structure (IEAS) was proposed. This paper applies the differential cryptanalysis on the IEAS and finds that some of its properties favor the differential attack which can recover an equivalent secret key with only a few number of chosen plain-images. Detailed procedures for cryptanalyzing IEAS with a lower round number are presented. Both theoretical analysis and experimental results are provided to show the vulnerability of IEAS against differential attack. In addition, some other security defects of IEAS, including insensitivity with respect to changes of plain-images and insufficient size of the key space, are also pointed out and verified. 
85|9||On using planning poker for estimating user stories|While most studies in psychology and forecasting stress the possible hazards of group processes when predicting effort and schedule, agile software development methods recommend the use of a group estimation technique called planning poker for estimating the size of user stories and developing release and iteration plans. It is assumed that the group discussion through planning poker helps in identifying activities that individual estimators could overlook, thus providing more accurate estimates and reducing the over-optimism that is typical for expert judgment-based methods. In spite of the widespread use of agile methods, there is little empirical evidence regarding the accuracy of planning poker estimates. In order to fill this gap a study was conducted requiring 13 student teams to develop a Web-based student records information system. All teams were given the same set of user stories which had to be implemented in three Sprints. Each team estimated the stories using planning poker and the estimates provided by each team member during the first round were averaged to obtain the statistical combination for further comparison. In the same way the stories were estimated by a group of experts. The study revealed that students’ estimates were over-optimistic and that planning poker additionally increased the over-optimism. On the other hand, the experts’ estimates obtained through planning poker were much closer to actual effort spent and tended to be more accurate than the statistical combination of their individual estimates. The results indicate that the optimism bias caused by group discussion diminishes or even disappears as the expertise of the people involved in the group estimation process increases. 
85|9||Differential fault analysis of ARIA in multi-byte fault models|
85|9||A computer assisted method for leukocyte nucleus segmentation and recognition in blood smear images|
85|9||Fast and accurate link prediction in social networking systems|Online social networks (OSNs) recommend new friends to registered users based on local-based features of the graph (i.e. based on the number of common friends that two users share). However, OSNs do not exploit all different length paths of the network. Instead, they consider only pathways of maximum length 2 between a user and his candidate friends. On the other hand, there are global-based approaches, which detect the overall path structure in a network, being computationally prohibitive for huge-sized social networks. In this paper we provide friend recommendations, also known as the link prediction problem, by traversing all paths of a limited length, based on the “algorithmic small world hypothesis”. As a result, we are able to provide more accurate and faster friend recommendations. We also derive variants of our method that apply to different types of networks (directed/undirected and signed/unsigned). We perform an extensive experimental comparison of the proposed method against existing link prediction algorithms, using synthetic and three real data sets (Epinions, Facebook and Hi5). We also show that a significant accuracy improvement can be gained by using information about both positive and negative edges. Finally, we discuss extensively various experimental considerations, such as a possible MapReduce implementation of FriendLink algorithm to achieve scalability. 
85|9||Security analysis of image cryptosystems only or partially based on a chaotic permutation|The paper proposes breaks for the permutation methods adopted in the chaos-based image cryptosystems. By a careful examination on the most chaotic image cryptosystems we can find that the permutation process constitute the main step or, in some cases, the only step to create the confusion. It can be applied on the pixels or on the pixel bits. A recently proposed image encryption scheme based on shuffling the pixel bits inspired from other works is treated as a case study. By applying a chosen plaintext attack, we demonstrate that a hacker can determine the permutation vectors (matrixes) used to permute the pixels bits or the pixels themselves and exploit them to reveal the plain image. 
85|9||A feedback-based decentralised coordination model for distributed open real-time systems|
85|9||Understanding socially oriented roles and goals through motivational modelling|
85|9||A systematic literature review of stakeholder identification methods in requirements elicitation|This paper presents a systematic review of relevant published studies related to topics in Requirements Engineering, specifically, concerning stakeholder identification methods in requirements elicitation, dated from 1984 to 2011. Addressing four specific research questions, this systematic literature review shows the following evidence gathered from these studies: current status of stakeholder identification in software requirement elicitation, the best practices recommended for its performance, consequences of incorrect identification in requirements quality, and, aspects which need to be improved. Our findings suggest that the analyzed approaches still have serious limitations in terms of covering all aspects of stakeholder identification as an important part of requirements elicitation. However, through correctly identifying and understanding the stakeholders, it is possible to develop high quality software. 
85|9||Benefits of supplementing use case narratives with activity diagramsâAn exploratory study|Use case narratives modeling the complex functionality of a given system often extend for several pages due to the need to include numerous alternative scenario specifications. In such situations, it is difficult to ensure the completeness and validity of the process logic embedded in such lengthy text narratives. This exploratory study investigates the benefits of supplementing each complex and lengthy use case narrative with an activity diagram for analysts and clients during requirements gathering and analysis. Our findings indicate that the process logic in corresponding activity diagrams is more complete and offers a greater degree of validity than that used in use case narratives. In addition, the quality of the process logic in these artifacts is not negatively affected by a use case narrative's length or complexity when they are used together to capture system requirements. Our research provides empirical evidence of beneficial improvements in the quality of these widely used artifacts that subsequently help eliminate or minimize inconsistencies among the requirements specified in different artifacts. 
85|9||Corrigendum to âGateway-oriented password-authenticated key exchange protocol in the standard modelâ [J. Syst. Softw. 85 (March (3)) (2012) 760â768]|
86|1|http://www.sciencedirect.com/science/journal/01641212/86/1|Signs of a thriving journal|
86|1||Failure prediction based on log files using Random Indexing and Support Vector Machines|Research problemThe impact of failures on software systems can be substantial since the recovery process can require unexpected amounts of time and resources. Accurate failure predictions can help in mitigating the impact of failures. Resources, applications, and services can be scheduled to limit the impact of failures. However, providing accurate predictions sufficiently ahead is challenging. Log files contain messages that represent a change of system state. A sequence or a pattern of messages may be used to predict failures.ContributionWe describe an approach to predict failures based on log files using Random Indexing (RI) and Support Vector Machines (SVMs).MethodRI is applied to represent sequences: each operation is characterized in terms of its context. SVMs associate sequences to a class of failures or non-failures. Weighted SVMs are applied to deal with imbalanced datasets and to improve the true positive rate. We apply our approach to log files collected during approximately three months of work in a large European manufacturing company.ResultsAccording to our results, weighted SVMs sacrifice some specificity to improve sensitivity. Specificity remains higher than 0.80 in four out of six analyzed applications.ConclusionsOverall, our approach is very reliable in predicting both failures and non-failures. 
86|1||iTravel: A recommender system in mobile peer-to-peer environment|Recommender systems in mobile tourism have attracted considerable interest during the past decade. However, most existing recommender systems in mobile tourism fail to exploit information, evaluations or ratings provided by other tourists of similar interests. In this research, we propose to facilitate attraction recommendation task by exploring other tourists’ ratings on their visited attractions. The proposed approach employs mobile peer-to-peer communications for exchanging ratings via their mobile devices. A cost-effective travel recommender system—iTravel—thus is developed to provide tourists with on-tour attraction recommendation. We propose three data exchange methods that allow users to effectively exchange their ratings toward visited attractions. Simulated experiments are performed to evaluate the proposed data exchange methods and a user study is conducted to validate the usability of the proposed iTravel system. 
86|1||A simulation model for strategic management process of software projects|In this study, a simulation model for the strategic management process of software development projects is presented. The proposed model simulates the implications of strategic decisions on factors such as cost, risk, budget and schedule of software projects. The main advantage of the proposed model is that it provides an integrated framework wherein risk management, cost estimation, and project management planning for the strategic management process of software development projects are linked. The results of the simulation of the project management planning determine the budget and schedule required for a project. Different strategic management decisions pose different sets of risks, each of which require different cost commitments. Hence, each strategic decision requires a project management plan with its own unique budget and schedule of software development. Thus, the simulation model estimates the risk and cost under different strategic decisions and maps them according to the project management plans. Therefore, the proposed integrated framework helps identify the best strategic option for the development and management of software projects. The proposed simulation model is nonspecific because it contains generic plug and play components that facilitate the use of any set of risk assessment, cost estimation models and project management tools. Therefore, it provides a flexible solution to software organisations and managers of software development projects. The simulation model is applied to a case study, which showed the effect of different strategic decisions on the risk and cost of the different phases of software development and ultimately on the budget and schedule required to complete the project. It therefore provides critical insights in identifying the best strategy for the development of software projects. 
86|1||Common carotid artery condition recognition technology using waveform features extracted from ultrasound spectrum images|
86|1||Collusion resilient spread spectrum watermarking in M-band wavelets using GA-fuzzy hybridization|
86|1||Improved multi-precision squaring for low-end RISC microcontrollers|We present an enhanced multi-precision squaring algorithm for low-end RISC microcontrollers. Generally, they have many general-purpose registers and limited bus size (8–32 bits). The proposed scheme employs a new technique, “lazy doubling” with optimizing computing sequences; so, it is significantly faster than the previous algorithms. Mathematical analysis shows that the number of clocks required by the proposed algorithm is about 67% of those required by the carry-catcher squaring algorithm. To the best of our knowledge this is known to be the fastest squaring algorithm. Experimental results on the ATmega128 microprocessor show that our algorithm is about 1.5 times faster than the carry-catcher squaring algorithm in terms of the number of clocks required. As squaring is a key operation in public key cryptography, the proposed algorithm can contribute to lowering power consumption in secure WSNs (wireless sensor networks) or secure embedded systems. 
86|1||Refactoring legacy AJAX applications to improve the efficiency of the data exchange component|
86|1||RDOTE â Publishing Relational Databases into the Semantic Web|A necessary step for the evolution of the traditional Web into a Semantic Web is the transformation of the vast quantities of data, currently residing in Relational Databases into semantically aware data. In addition, in cases where new ontology schemata are developed, considerable experimentation with real data for testing the consistency of classes, properties and entailment rules is required. During the last decade, there has been intense research and development in creating methodologies and tools able to map Relational Databases with the Resource Description Framework (RDF). Although some systems have gained wider acceptance in the Semantic Web community, they either require users to learn a declarative language for encoding mappings or, in case they support friendly user interfaces, they provide limited expressivity.Thereupon, we present RDOTE, a framework for easily transporting data residing in Relational Databases into the Semantic Web. RDOTE is available under GNU/GPL license and it provides friendly graphical user interfaces, as well as enough expressivity for creating automatic and custom RDF dumps of relational data. RDOTE is also compatible with D2RQ and R2RML mapping definitions. 
86|1||Improvement of trace-driven I-Cache timing attack on the RSA algorithm|The previous I-Cache timing attacks on the RSA algorithm which exploit the instruction path of a cipher are mostly proof-of-concept, and it is harder to put them into practice than D-Cache timing attacks. We propose a trace-driven timing attack model on the RSA algorithm via spying on the whole I-Cache, instead of the partial instruction cache to which the multiplication function mapped, by analyzing the complications in the previous I-Cache timing attack on the RSA algorithm. Then, an improved analysis algorithm of the exponent using the characteristic of the window size in SWE algorithm is provided, which could further reduce the search space of the key bits than the former. We further demonstrate how to recover the private key d from the scattered known bits of dp and dq, through demonstrating some conclusions and validating it by experimentation. In addition, an error detection mechanism to detect some erroneous decisions of the operation sequences is provided to reduce the number of the erroneous recovered bits, and improve the precision of decision. We implement an I-Cache timing attack on RSA of OpenSSL in a practical environment, the experimental results show that the feasibility and effectiveness of I-Cache timing attack can be improved. 
86|1||A content-aware bridging service for publish/subscribe environments|
86|1||From chaos to the systematic harmonization of multiple reference models: A harmonization framework applied in two case studies|At the present time, we can observe that in an effort to deal with the issue of quality, a variety of models, standards and methodologies have been developed to give support in different domains of the IT industry. This wide range of heterogeneous models makes it possible to resolve multiple needs. In recent years, as the integration of different models has increased, organizations have started to note that their business and technical processes can be aligned with more than one model. Currently, however, we are not aware of any other attempts to provide an explicit and systematic solution that would allow us to address the issue of harmonization of multiple reference models in such a way as to satisfy the needs of the companies. In the quest to help support the work of harmonization of multiple models, this paper presents (i) a framework that defines elements needed to support the harmonization of multiple reference models, (ii) a process, which is the backbone and way of integrating all the elements defined in the framework thus allowing the implementation of a harmonization project to be guided systematically, harmonizing multiple models through the configuration of a harmonization strategy, and (iii) a set of methods, which allows us to know “what to do”, as well as “how to put” two or more models in consonance with each other. The experience of the application of our proposal is illustrated in two case studies. The findings obtained show that the harmonization process has enabled us to harmonize and put the models involved in consonance with each other. 
86|1||Towards an early software estimation using log-linear regression and a multilayer perceptron model|Software estimation is a tedious and daunting task in project management and software development. Software estimators are notorious in predicting software effort and they have been struggling in the past decades to provide new models to enhance software estimation. The most critical and crucial part of software estimation is when estimation is required in the early stages of the software life cycle where the problem to be solved has not yet been completely revealed. This paper presents a novel log-linear regression model based on the use case point model (UCP) to calculate the software effort based on use case diagrams. A fuzzy logic approach is used to calibrate the productivity factor in the regression model. Moreover, a multilayer perceptron (MLP) neural network model was developed to predict software effort based on the software size and team productivity. Experiments show that the proposed approach outperforms the original UCP model. Furthermore, a comparison between the MLP and log-linear regression models was conducted based on the size of the projects. Results demonstrate that the MLP model can surpass the regression model when small projects are used, but the log-linear regression model gives better results when estimating larger projects. 
86|1||Empirical validation of a usability inspection method for model-driven Web development|Web applications should be usable in order to be accepted by users and to improve their success probability. Despite the fact that this requirement has promoted the emergence of several usability evaluation methods, there is a need for empirically validated methods that provide evidence about their effectiveness and that can be properly integrated into early stages of Web development processes. Model-driven Web development processes have grown in popularity over the last few years, and offer a suitable context in which to perform early usability evaluations due to their intrinsic traceability mechanisms. These issues have motivated us to propose a Web Usability Evaluation Process (WUEP) which can be integrated into model-driven Web development processes. This paper presents a family of experiments that we have carried out to empirically validate WUEP. The family of experiments was carried out by 64 participants, including PhD and Master's computer science students. The objective of the experiments was to evaluate the participants’ effectiveness, efficiency, perceived ease of use and perceived satisfaction when using WUEP in comparison to an industrial widely used inspection method: Heuristic Evaluation (HE). The statistical analysis and meta-analysis of the data obtained separately from each experiment indicated that WUEP is more effective and efficient than HE in the detection of usability problems. The evaluators were also more satisfied when applying WUEP, and found it easier to use than HE. Although further experiments must be carried out to strengthen these results, WUEP has proved to be a promising usability inspection method for Web applications which have been developed by using model-driven development processes. 
86|1||Semantic ranking of web pages based on formal concept analysis|
86|1||Improving Graph Cuts algorithm to transform sequence of stereo image to depth map|
86|1||Chaos-based detection of LDoS attacks|A low-rate denial of service (LDoS) attack behaves as a small signal in periodic pulses with low average rate, which hides in normal TCP traffic stealthily. LDoS attacks reduce link throughput and degrade QoS of a target. An approach of detecting LDoS attacks is proposed based on Duffing oscillator in chaos systems. The approach detects LDoS attacks by adopting the technology of digital signal processing (DSP), which takes an LDoS attack as a small signal and normal TCP traffic as background noise. Duffing oscillator is used to detect LDoS attacks in normal TCP traffic. Simulations show that the LDoS attacks can be detected through diagram of the chaotic state, and the period and pulse width of LDoS attacks can be estimated. 
86|1||A lossless copyright authentication scheme based on BesselâFourier moment and extreme learning machine in curvature-feature domain|
86|1||Analytical architecture-based performability evaluation of real-time software systems|
86|10|http://www.sciencedirect.com/science/journal/01641212/86/10|Quality optimisation of software architectures and design specifications|This special issue of the Journal of Systems and Software presents novel software architecture optimisation frameworks. The majority of the approaches consider the problem of optimising conflicting quality attributes simultaneously. Other approaches focus on effectively searching for better software architectures by either using smart problem-dependent heuristics or by combining the expression power of ADLs with architecture optimisation. 
86|10||Automatic optimisation of system architectures using EAST-ADL|There are many challenges which face designers of complex system architectures, particularly safety–critical or real-time systems. The introduction of Architecture Description Languages (ADLs) has helped to meet these challenges by consolidating information about a system and providing a platform for modelling and analysis capabilities. However, managing this wealth of information can still be problematic, and evaluation of potential design decisions is still often performed manually. Automatic architectural optimisation can be used to assist this decision process, enabling designers to rapidly explore many different options and evaluate them according to specific criteria. In this paper, we present a multi-objective optimisation approach based on EAST-ADL, an ADL in the automotive domain, with the goal of combining the advantages of ADLs and architectural optimisation. The approach is designed to be extensible and leverages the capabilities of EAST-ADL to provide support for evaluation according to different factors, including dependability, timing/performance, and cost. The technique is applied to an illustrative example system featuring both hardware and software perspectives, demonstrating the potential benefits of this concept to the design of embedded system architectures. 
86|10||Efficient optimization of large probabilistic models|
86|10||MOO: An architectural framework for runtime optimization of multiple system objectives in embedded control software|Today's complex embedded systems function in varying operational conditions. The control software adapts several control variables to keep the operational state optimal with respect to multiple objectives. There exist well-known techniques for solving such optimization problems. However, current practice shows that the applied techniques, control variables, constraints and related design decisions are not documented as a part of the architecture description. Their implementation is implicit, tailored for specific characteristics of the embedded system, tightly integrated into and coupled with the control software, which hinders its reusability, analyzability and maintainability. This paper presents an architectural framework to design, document and realize multi-objective optimization in embedded control software. The framework comprises an architectural style together with its visual editor and domain-specific analysis tools, and a code generator. The code generator generates an optimizer module specific for the given architecture and it employs aspect-oriented software development techniques to seamlessly integrate this module into the control software. The effectiveness of the framework is validated in the context of an industrial case study from the printing systems domain. 
86|10||S-IDE: A tool framework for optimizing deployment architecture of High Level Architecture based simulation systems|One of the important problems in High Level Architecture (HLA) based distributed simulation systems is the allocation of the different simulation modules to the available physical resources. Usually, the deployment of the simulation modules to the physical resources can be done in many different ways, and each deployment alternative will have a different impact on the performance. Although different algorithmic solutions have been provided to optimize the allocation with respect to the performance, the problem has not been explicitly tackled from an architecture design perspective. Moreover, for optimizing the deployment of the simulation system, tool support is largely missing. In this paper we propose a method for automatically deriving deployment alternatives for HLA based distributed simulation systems. The method extends the IEEE Recommended Practice for High Level Architecture Federation Development and Execution Process by providing an approach for optimizing the allocation at the design level. The method is realized by the tool framework, S-IDE (Simulation-IDE) that we have developed to provide an integrated development environment for deriving a feasible deployment alternative based on the simulation system and the available physical resources at the design phase. The method and the tool support have been validated using a case study for the development of a traffic simulation system. 
86|10||Hybrid multi-attribute QoS optimization in component based software systems|Design decisions for complex, component-based systems impact multiple quality of service (QoS) properties. Often, means to improve one quality property deteriorate another one. In this scenario, selecting a good solution with respect to a single quality attribute can lead to unacceptable results with respect to the other quality attributes. A promising way to deal with this problem is to exploit multi-objective optimization where the objectives represent different quality attributes. The aim of these techniques is to devise a set of solutions, each of which assures an optimal trade-off between the conflicting qualities. Our previous work proposed a combined use of analytical optimization techniques and evolutionary algorithms to efficiently identify an optimal set of design alternatives with respect to performance and costs. This paper extends this approach to more QoS properties by providing analytical algorithms for availability-cost optimization and three-dimensional availability-performance-cost optimization. We demonstrate the use of this approach on a case study, showing that the analytical step provides a better-than-random starting population for the evolutionary optimization, which lead to a speed-up of 28% in the availability-cost case. 
86|10||Quality-driven optimization of system architecture: Industrial case study on an automotive sub-system|
86|10||Supporting end-to-end quality of service properties in OMG data distribution service publish/subscribe middleware over wide area networks|Assuring end-to-end quality-of-service (QoS) in distributed real-time and embedded (DRE) systems is hard due to the heterogeneity and scale of communication networks, transient behavior, and the lack of mechanisms that holistically schedule different resources end-to-end. This paper makes two contributions to research focusing on overcoming these problems in the context of wide area network (WAN)-based DRE applications that use the OMG Data Distribution Service (DDS) QoS-enabled publish/subscribe middleware. First, it provides an analytical approach to bound the delays incurred along the critical path in a typical DDS-based publish/subscribe stream, which helps ensure predictable end-to-end delays. Second, it presents the design and evaluation of a policy-driven framework called Velox. Velox combines multi-layer, standards-based technologies—including the OMG DDS and IP DiffServ—to support end-to-end QoS in heterogeneous networks and shield applications from the details of network QoS mechanisms by specifying per-flow QoS requirements. The results of empirical tests conducted using Velox show how combining DDS with DiffServ enhances the schedulability and predictability of DRE applications, improves data delivery over heterogeneous IP networks, and provides network-level differentiated performance. 
86|10||On the reliability of mapping studies in software engineering|BackgroundSystematic literature reviews and systematic mapping studies are becoming increasingly common in software engineering, and hence it becomes even more important to better understand the reliability of such studies.ObjectiveThis paper presents a study of two systematic mapping studies to evaluate the reliability of mapping studies and point out some challenges related to this type of study in software engineering.MethodThe research is based on an in-depth case study of two published mapping studies on software product line testing.ResultsWe found that despite the fact that the two studies are addressing the same topic, there are quite a number of differences when it comes to papers included and in terms of classification of the papers included in the two mapping studies.ConclusionsFrom this we conclude that although mapping studies are important, their reliability cannot simply be taken for granted. Based on the findings we also provide four conjectures that further research has to address to make secondary studies (systematic mapping studies and systematic literature reviews) even more valuable to both researchers and practitioners. 
86|10||Power-aware scheduling algorithms for sporadic tasks in real-time systems|In this paper, we consider the canonical sporadic task model with the system-wide energy management problem. Our solution uses a generalized power model, in which the static power and the dynamic power are considered. We present a static solution to schedule the sporadic task set, assuming worst-case execution time for each sporadic tasks release, and propose a dynamic solution to reclaim the slacks left by the earlier completion of tasks than their worst-case estimations. The experimental results show that the proposed static algorithm can reduce the energy consumption by 20.63%–89.70% over the EDF* algorithm and the dynamic algorithm consumes 2.06%–24.89% less energy than that of the existing DVS algorithm. 
86|10||Reversible watermarking method based on asymmetric-histogram shifting of prediction errors|This paper tries to provide a new perspective for the research of reversible watermarking based on histogram shifting of prediction errors. Instead of obtaining one prediction error for the current pixel, we calculate multiple prediction errors by designing a multi-prediction scheme. An asymmetric error histogram is then constructed by selecting the suitable one from these errors. Compared with traditional symmetric histogram, the asymmetric error histogram reduces the amount of shifted pixels, thus improving the watermarked image quality. Moreover, a complementary embedding strategy is proposed by combining the maximum and minimum error histograms. As the two error histograms shift in the opposite directions during the embedding, some watermarked pixels will be restored to their original values, thus the image quality is further improved. Experimental findings also show that the proposed method re-creates watermarked images of higher quality that carry larger embedding capacity compared to conventional symmetric histogram methods, such as Tsai et al.’s and Luo et al.’s works. 
86|10||Affinity-aware modeling of CPU usage with communicating virtual machines|
86|10||Code smells as system-level indicators of maintainability: An empirical study|ContextCode smells are manifestations of design flaws that can degrade code maintainability. So far, no research has investigated if these indicators are useful for conducting system-level maintainability evaluations.AimThe research in this paper investigates the potential of code smells to reflect system-level indicators of maintainability.MethodWe evaluated four medium-sized Java systems using code smells and compared the results against previous evaluations on the same systems based on expert judgment and the Chidamber and Kemerer suite of metrics. The systems were maintained over a period of up to 4 weeks. During maintenance, effort (person-hours) and number of defects were measured to validate the different evaluation approaches.ResultsMost code smells are strongly influenced by size; consequently code smells are not good indicators for comparing the maintainability of systems differing greatly in size. Also, from the comparison of the different evaluation approaches, expert judgment was found as the most accurate and flexible since it considered effects due to the system's size and complexity and could adapt to different maintenance scenarios.ConclusionCode smell approaches show promise as indicators of the need for maintenance in a way that other purely metric-based approaches lack. 
86|10||Common weaving approach in mainstream languages for software security hardening|
86|10||SCRUMIAâAn educational game for teaching SCRUM in computing courses|Due to the increasing use of agile methods, teaching SCRUM as an agile project management methodology has become more and more important. In order to teach students to be able to apply SCRUM in concrete situations, often educational (simulation) games are used. However, most of these games have been developed more for professional trainings than taking into consideration typical restrictions of university courses (such as, class duration and low financial resources for instructional materials). Therefore, we present a manual paper and pencil game to reinforce and teach the application of SCRUM in undergraduate computing programmes complementing theoretical lectures. The game has been developed following a systematic instructional design process and based on our teaching experience. It has been applied several times in two undergraduate project management courses. We evaluated motivation, user experience and the game's contribution to learning through case studies on Kirkpatrick's level one based on the perception of the students. First results indicate the potential of the game to contribute to the learning of SCRUM in an engaging way, keeping students immersed in the learning task. In this regard, the game offers a low-budget alternative to complement traditional instructional strategies for teaching SCRUM in the classroom. 
86|10||Cell-related location area planning for 4G PCS networks with variable-order Markov model|
86|10||Reversible data hiding based on PDE predictor|
86|11|http://www.sciencedirect.com/science/journal/01641212/86/11|The Meta-Protocol framework|
86|11||Minimizing test-point allocation to improve diagnosability in business process models|
86|11||Genetic algorithm and difference expansion based reversible watermarking for relational databases|In this paper, we present a new robust and reversible watermarking approach for the protection of relational databases. Our approach is based on the idea of difference expansion and utilizes genetic algorithm (GA) to improve watermark capacity and reduce distortion. The proposed approach is reversible and therefore, distortion introduced after watermark insertion can be fully restored. Using GA, different attributes are explored to meet the optimal criteria rather than selecting less effective attributes for watermark insertion. Checking only the distortion tolerance of two attributes for a selected tuple may not be useful for watermark capacity and distortion therefore, distortion tolerance of different attributes are explored. Distortion caused by difference expansion can help an attacker to predict watermarked attribute. Thus, we have incorporated tuple and attribute-wise distortion in the fitness function of GA, making it tough for an attacker to predict watermarked attribute. From experimental analysis, it is concluded that the proposed technique provides improved capacity and reduced distortion compared to existing approaches. Problem of false positives and change in attribute order at detection side is also resolved. Additionally, the proposed technique is resilient against a wide range of attacks such as addition, deletion, sorting, bit flipping, tuple-wise-multifaceted, attribute-wise-multifaceted, and additive attacks. 
86|11||Model-based cache-aware dispatching of object-oriented software for multicore systems|
86|11||A high-performance reversible data-hiding scheme for LZW codes|
86|11||Virtualized Web server cluster self-configuration to optimize resource and power use|
86|11||The lean gap: A review of lean approaches to large-scale software systems development|Lean approaches to product development (LPD) have had a strong influence on many industries and in recent years there have been many proponents for lean in software development as it can support the increasing industry need of scaling agile software development. With it's roots in industrial manufacturing and, later, industrial product development, it would seem natural that LPD would adapt well to large-scale development projects of increasingly software-intensive products, such as in the automotive industry. However, it is not clear what kind of experience and results have been reported on the actual use of lean principles and practices in software development for such large-scale industrial contexts. This was the motivation for this study as the context was an ongoing industry process improvement project at Volvo Car Corporation and Volvo Truck Corporation.The objectives of this study are to identify and classify state of the art in large-scale software development influenced by LPD approaches and use this established knowledge to support industrial partners in decisions on a software process improvement (SPI) project, and to reveal research gaps and proposed extensions to LPD in relation to its well-known principles and practices.For locating relevant state of the art we conducted a systematic mapping study, and the industrial applicability and relevance of results and said extensions to LPD were further analyzed in the context of an actual, industrial case.A total of 10,230 papers were found in database searches, of which 38 papers were found relevant. Of these, only 42 percent clearly addressed large-scale development. Furthermore, a majority of papers (76 percent) were non-empirical and many lacked information about study design, context and/or limitations. Most of the identified results focused on eliminating waste and creating flow in the software development process, but there was a lack of results for other LPD principles and practices.Overall, it can be concluded that research in the much hyped field of lean software development is in its nascent state when it comes to large scale development. There is very little support available for practitioners who want to apply lean approaches for improving large-scale software development, especially when it comes to inter-departmental interactions during development. This paper explicitly maps the area, qualifies available research, and identifies gaps, as well as suggests extensions to lean principles relevant for large scale development of software intensive systems. 
86|11||A novel approach to evaluate software vulnerability prioritization|
86|11||Adaptive reversible watermarking with improved embedding capacity|
86|11||Adapting system execution traces to support analysis of software system performance properties|
86|11||Low bit-rate information hiding method based on search-order-coding technique|
86|11||A secure palm vein recognition system|With the increasing needs in security systems, vein recognition is one of the important and reliable solutions of identity security for biometrics-based identification systems. The obvious and stable line-feature-based approach can be used to clearly describe a palm vein patterns for personal identification. In this paper, a directional filter bank involving different orientations is designed to extract the vein pattern and the minimum directional code (MDC) is employed to encode the line-based vein features in binary code. In addition, there are many non vein pixels in the vein image and those pixels are unmeaning for vein recognition. To improve the accuracy, the non-vein pixels are detected by evaluating the directional filtering magnitude (DFM) and considered the non-orientation code. A total of 5120 palm vein images from 256 persons are used to verify the validity of the proposed palm vein recognition approach. High accuracies (>99%) and low equal error rate (0.54%) obtained by the proposed method show that our proposed approach is feasible and effective for palm vein recognition. 
86|11||A systematic review on the functional testing of semantic web services|
86|11||A domain-specific language for context modeling in context-aware systems|
86|11||SAAD, a content based Web Spam Analyzer and Detector|
86|11||Supporting concept location through identifier parsing and ontology extraction|Identifier names play a key role in program understanding and in particular in concept location. Programmers can easily “parse” identifiers and understand the intended meaning. This, however, is not trivial for tools that try to exploit the information in the identifiers to support program understanding. To address this problem, we resort to natural language analyzers, which parse tokenized identifier names and provide the syntactic relationships (dependencies) among the terms composing the identifiers. Such relationships are then mapped to semantic relationships.In this study, we have evaluated the use of off-the-shelf and trained natural language analyzers to parse identifier names, extract an ontology and use it to support concept location. In the evaluation, we assessed whether the concepts taken from the ontology can be used to improve the efficiency of queries used in concept location. We have also investigated if the use of different natural language analyzers has an impact on the ontology extracted and the support it provides to concept location. Results show that using the concepts from the ontology significantly improves the efficiency of concept location queries (e.g., in some cases, an improvement of 127% is observed). The results also indicate that the efficiency of concept location queries is not affected by the differences in the ontologies produced by different analyzers. 
86|11||Real-time risk monitoring in business processes: A sensor-based approach|This article proposes an approach for real-time monitoring of risks in executable business process models. The approach considers risks in all phases of the business process management lifecycle, from process design, where risks are defined on top of process models, through to process diagnosis, where risks are detected during process execution. The approach has been realized via a distributed, sensor-based architecture. At design-time, sensors are defined to specify risk conditions which when fulfilled, are a likely indicator of negative process states (faults) to eventuate. Both historical and current process execution data can be used to compose such conditions. At run-time, each sensor independently notifies a sensor manager when a risk is detected. In turn, the sensor manager interacts with the monitoring component of a business process management system to prompt the results to process administrators who may take remedial actions. The proposed architecture has been implemented on top of the YAWL system, and evaluated through performance measurements and usability tests with students. The results show that risk conditions can be computed efficiently and that the approach is perceived as useful by the participants in the tests. 
86|12|http://www.sciencedirect.com/science/journal/01641212/86/12|Introduction to the JSS special issue of Web 2.0 engineering: New practices and emerging Challenges|
86|12||Performance improvement of web caching in Web 2.0 via knowledge discovery|Web 2.0 systems are more unpredictable and customizable than traditional web applications. This causes that performance techniques, such as web caching, limit their improvements. Our study was based on the hypotheses that the use of web caching in Web 2.0 applications, particularly in content aggregation systems, can be improved by adapting the content fragment designs. We proposed to base this adaptation on the analysis of the characterization parameters of the content elements and on the creation of a classification algorithm. This algorithm was deployed with decision trees, created in an off-line knowledge discovery process. We also defined a framework to create and adapt fragments of the web documents to reduce the user-perceived latency in web caches. The experiment results showed that our solution had a remarkable reduction in the user-perceived latency even losses in the cache hit ratios and in the overhead generated on the system, in comparison with other web cache schemes. 
86|12||Re-engineering legacy Web applications into RIAs by aligning modernization requirements, patterns and RIA features|
86|12||Modeling users on the World Wide Web based on cognitive factors, navigation behavior and clustering techniques|
86|12||A model-driven approach to develop high performance web applications|The evolution of web technologies in the last few years has contributed to the improvement of web applications, and with the appearance of AJAX and Web 2.0 technology, a new breed of applications for the Internet has emerged: widgets, gadgets and mashups are some examples of this trend. However, as web applications become more and more complex, development costs are increasing in an exponential rate, and we believe that considering a software engineering methodology in the development process of this type of applications, contributes to the solution of this problem. In order to solve this question, this paper proposes a model-driven architecture to support web application development from the design to the implementation model. With this aim, the following tasks have been performed: first a new profile extends UML with new concepts extracted from the web domain, then a new framework supports web application development by composing heterogeneous web elements, and finally, a transformation model generates web applications from the UML extension proposed. The main contribution of this work is a cost and complexity reduction due to the incorporation of a model-driven architecture into the web application development process, but other advantages that can be mentioned are a high performance degree achieved by a prefetching cache mechanism, and a high reusability, since web elements can be reused in different web applications. 
86|12||Detecting Web requirements conflicts and inconsistencies under a model-based perspective|
86|12||Evaluating the perceived and estimated quality in use of Web 2.0 applications|
86|12||Reliability guaranteed energy-aware frame-based task set execution strategy for hard real-time systems|
86|12||What are the roles of software product managers? An empirical investigation|
86|12||A decision support framework for metrics selection in goal-based measurement programs: GQM-DSFMS|Software organizations face challenges in managing and sustaining their measurement programs over time. The complexity of measurement programs increase with exploding number of goals and metrics to collect. At the same time, organizations usually have limited budget and resources for metrics collection. It has been recognized for quite a while that there is the need for prioritizing goals, which then ought to drive the selection of metrics. On the other hand, the dynamic nature of the organizations requires measurement programs to adapt to the changes in the stakeholders, their goals, information needs and priorities. Therefore, it is crucial for organizations to use structured approaches that provide transparency, traceability and guidance in choosing an optimum set of metrics that would address the highest priority information needs considering limited resources. This paper proposes a decision support framework for metrics selection (DSFMS) which is built upon the widely used Goal Question Metric (GQM) approach. The core of the framework includes an iterative goal-based metrics selection process incorporating decision making mechanisms in metrics selection, a pre-defined Attributes/Metrics Repository, and a Traceability Model among GQM elements. We also discuss alternative prioritization and optimization techniques for organizations to tailor the framework according to their needs. The evaluation of the GQM-DSFMS framework was done through a case study in a CMMI Level 3 software company. 
86|12||A language-independent approach to black-box testing using Erlang as test specification language|Integration of reused, well-designed components and subsystems is a common practice in software development. Hence, testing integration interfaces is a key activity, and a whole range of technical challenges arise from the complexity and versatility of such components.In this paper, we present a methodology to fully test different implementations of a software component integration API. More precisely, we propose a black-box testing approach, based on the use of QuickCheck and inspired by the TTCN-3 test architecture, to specify and test the expected behavior of a component. We have used a real-world multimedia content management system as case study. This system offers the same integration API for different technologies: Java, Erlang and HTTP/XML. Using our method, we have tested all integration API implementations using the same test specification, increasing the confidence in its interoperability and reusability. 
86|12||Domain-Specific Modeling Languages to improve framework instantiation|Frameworks are reusable software composed of concrete and abstract classes that implement the functionality of a domain. Applications reuse frameworks to enhance quality and development efficiency. However, frameworks are hard to learn and reuse. Application developers must understand the complex class hierarchy of the framework to instantiate it properly. In this paper, we present an approach to build a Domain-Specific Modeling Language (DSML) of a framework and use it to facilitate framework reuse during application development. The DSML of a framework is built by identifying the features of this framework and the information required to instantiate them. Application generators transform models created with the DSML into application code, hiding framework complexities. In this paper, we illustrate the use of our approach in a framework for the domain of business resource transactions and a experiment that evaluated the efficiency obtained with our approach. 
86|12||The presence and development of competency in IT programs|Information technology (IT) programs are coordinated IT projects with a common business objective or underlying similar theme. Driving success in an IT program requires that the projects all work to achieve more global organizational goals than those of each individual project. These goals are better achieved in the presence of critical program team competences that include personnel development, dissemination of methodologies, and a key customer focus. These competences need to be developed to promote higher program performance where programs are dedicated to achieving business objectives of an organization. We propose a model based on the human resource model that considers the development of the critical competences when essential self and social competences are present in team members. Participation mechanisms of interpersonal cooperation and mutual support assist in the development of the critical competences. The model is supported by data collected from both quantitative survey and qualitative interviews with matched pairs of IT program managers and IT project managers. The results confirm the need to insure the presence of certain competences in team members and the construction of an environment that builds mutual support and cooperation. The human resource model is thus extended to include the inter-team environment of IT programs and further variables important to vendor competence. 
86|12||Effective scheduling strategies for boosting performance on rule-based spam filtering frameworks|
86|12||A study of cyclic dependencies on defect profile of software components|BackgroundEmpirical evidence shows that dependency cycles among software components are pervasive in real-life software systems, although such cycles are known to be detrimental to software quality attributes such as understandability, testability, reusability, build-ability and maintainability.Research goalsCan the use of extended object-oriented metrics make us better understand the relationships among cyclic related components and their defect-proneness?ApproachFirst, we extend such metrics to mine and classify software components into two groups – the cyclic and the non-cyclic ones. Next, we have performed an empirical study of six software applications. Using standard statistical tests on four different hypotheses, we have determined the significance of the defect profiles of both groups.ResultsOur results show that most defects and defective components are concentrated in cyclic-dependent components, either directly or indirectly.Discussion and conclusionThese results have important implications for software maintenance and system testing. By identifying the most defect-prone set in a software system, it is possible to effectively allocate testing resources in a cost efficient manner. Based on these results, we demonstrate how additional structural properties could be collected to understand component's defect proneness and aid decision process in refactoring defect-prone cyclic related components. 
86|12||Chaos-based selective encryption for H.264/AVC|
86|12||Software generated device exception for more intensive device-related software testing: An industrial field study|It is more important to properly handle exceptions, than to prevent exceptions from occurring, because they arise from so many different causes. In embedded systems, a vast number of exceptions are caused by hardware devices. In such cases, numerous software components are involved in these hardware device-originated exceptions, ranging from the device itself to the device driver, the kernel, and applications. Therefore, it takes a lot of time to debug software that fails to handle exceptions. This paper proposes a lightweight device exception testing method, and a related automation tool, AMOS v3.0. The proposed method artificially triggers more realistic device exceptions in runtime, and monitors how software components handle exceptions in detail. AMOS v3.0 has been applied to the exception testing of car-infotainment systems in an automobile company. The results based on this industrial field study have revealed that 39.13% of the failures in exception handling were caused by applications, 36.23% of the failures were caused by device drivers, and 24.64% were derived from the kernel. We conclude that the proposed method is highly effective, in that it can allow developers to identify the root cause of failure for exception handling. 
86|2|http://www.sciencedirect.com/science/journal/01641212/86/2|Matthew effect, ABC analysis and project management of scale-free information systems|
86|2||A robust blind color image watermarking in quaternion Fourier transform domain|Most of the existing color image watermarking schemes were designed to mark the image luminance component only, which have some disadvantages: (i) they are sensitive to color attacks because of ignoring the correlation between different color channels, (ii) they are always not robust to geometric distortions for neglecting the watermark desynchronization. It is a challenging work to design a robust color image watermarking scheme. Based on quaternion Fourier transform and least squares support vector machine (LS-SVM), we propose a robust blind color image watermarking in quaternion Fourier transform domain, which has good visual quality. Firstly, the original color image is divided into color image blocks. Then, the fast quaternion Fourier transform is performed on the color image block. Finally, the digital watermark is embedded into original color image by adaptively modulating the real quaternion Fourier transform coefficients of color image block. For watermark decoding, the LS-SVM correction with pseudo-Zernike moments is utilized. Experimental results show that the proposed color image watermarking is not only robust against common image processing operations such as filtering, JPEG compression, histogram equalization, and image blurring, but also robust against the geometrical distortions. 
86|2||Efficient support of dynamic inheritance for class- and prototype-based languages|Dynamically typed languages are becoming increasingly popular for different software development scenarios where runtime adaptability is important. Therefore, existing class-based platforms such as Java and .Net have been gradually incorporating dynamic features to support the execution of these languages. The implementations of dynamic languages on these platforms commonly generate an extra layer of software over the virtual machine, which reproduces the reflective prototype-based object model provided by most dynamic languages. Simulating this model frequently involves a runtime performance penalty, and makes the interoperation between class- and prototype-based languages difficult.Instead of simulating the reflective model of dynamic languages, our approach has been to extend the object-model of an efficient class-based virtual machine with prototype-based semantics, so that it can directly support both kinds of languages. Consequently, we obtain the runtime performance improvement of using the virtual machine JIT compiler, while a direct interoperation between languages compiled to our platform is also possible. In this paper, we formalize dynamic inheritance for both class- and prototype-based languages, and implement it as an extension of an efficient virtual machine that performs JIT compilation. We also present an extensive evaluation of the runtime performance and memory consumption of the programming language implementations that provide dynamic inheritance, including ours. 
86|2||3E: Energy-efficient elastic scheduling for independent tasks in heterogeneous computing systems|Reducing energy consumption is a major design constraint for modern heterogeneous computing systems to minimize electricity cost, improve system reliability and protect environment. Conventional energy-efficient scheduling strategies developed on these systems do not sufficiently exploit the system elasticity and adaptability for maximum energy savings, and do not simultaneously take account of user expected finish time. In this paper, we develop a novel scheduling strategy named energy-efficient elastic (3E) scheduling for aperiodic, independent and non-real-time tasks with user expected finish times on DVFS-enabled heterogeneous computing systems. The 3E strategy adjusts processors’ supply voltages and frequencies according to the system workload, and makes trade-offs between energy consumption and user expected finish times. Compared with other energy-efficient strategies, 3E significantly improves the scheduling quality and effectively enhances the system elasticity. 
86|2||Histogram-shifting-imitated reversible data hiding|This paper proposes a novel reversible data hiding scheme based on the histogram-shifting-imitated approach. Instead of utilizing the peak point of an image histogram, the proposed scheme manipulates the peak points of segments based on image intensity. The secret data can be embedded into the cover image by changing the peak point pixel value into other pixel value in the same segment. The proposed method uses a location map to guarantee the correct extraction of the secret data. Since the modification of the pixel value is limited within each segment, the quality of the stego image is only related to the size of the segmentation, which means after embedding data into the cover image, it can be reused to do the multi-layer data embedding while maintaining the high quality of the final stego image. The experimental results of comparison with other existing schemes demonstrate the performance of the proposed scheme is superior to the others. 
86|2||A covert communication method via spreadsheets by secret sharing with a self-authentication capability|
86|2||A zero-watermark scheme with geometrical invariants using SVM and PSO against geometrical attacks for image protection|
86|2||Layer assessment of object-oriented software: A metric facilitating white-box reuse|Software reuse has the potential to shorten delivery times, improve quality and reduce development costs. However software reuse has been proven challenging for most organizations. The challenges involve both organizational and technical issues. In this work we concentrate on the technical issues and we propose a new metric facilitating the reuse of object-oriented software based on the popular Chidamber and Kemerer suite for object-oriented design. We derive this new metric using linear regression on a number of OSS java projects. We compare and contrast this new metric with three other metrics proposed in the literature. The purpose of the proposed metric is to assist a software developer during the development of a software system in achieving reusability of classes considered important for future reuse and also in providing assistance during re-architecting and componentization activities of existing systems. 
86|2||A high performance inter-domain communication approach for virtual machines|
86|2||Effective pattern-driven concurrency bug detection for operating systems|As multi-core hardware has become more popular, concurrent programming is being more widely adopted in software. In particular, operating systems such as Linux utilize multi-threaded techniques heavily to enhance performance. However, current analysis techniques and tools for validating concurrent programs often fail to detect concurrency bugs in operating systems (OSes) due to the complex characteristics of OSes. To detect concurrency bugs in OSes in a practical manner, we have developed the COncurrency Bug dETector (COBET) framework based on composite bug patterns augmented with semantic conditions. The effectiveness, efficiency, and applicability of COBET were demonstrated by detecting 10 new bugs in file systems, device drivers, and network modules of Linux 2.6.30.4 as confirmed by the Linux maintainers. 
86|2||A novel VQ-based reversible data hiding scheme by using hybrid encoding strategies|
86|2||On using adversary simulators to evaluate global fixed-priority and FPZL scheduling of multiprocessors|
86|2||Constraint-based specification of model transformations|Model transformations are a central element of model-driven development (MDD) approaches. The correctness, modularity and flexibility of model transformations is critical to their effective use in practical software development. In this paper we describe an approach for the automated derivation of correct-by-construction transformation implementations from high-level specifications. We illustrate this approach on a range of model transformation case studies of different kinds (re-expression, refinement, quality improvement and abstraction transformations) and describe ways in which transformations can be composed and evolved using this approach. 
86|2||SF-PMIPv6: A secure fast handover mechanism for Proxy Mobile IPv6 networks|An efficient mobility management mechanism is one of the major challenges for ubiquitous computing. Recently, the IETF NETLMM working group proposed Proxy Mobile IPv6 (PMIPv6), a network-based localized mobility management protocol to support mobility management without the participation of mobile nodes (MNs) in any mobility-related signaling. Unfortunately, PMIPv6 still suffers from high packet losses and long authentication latency during handover. To address these issues, we propose a secure authentication mechanism and fast handover scheme called SF-PMIPv6 for PMIPv6 networks. The scheme provides low handover latency, supports local authentication procedures, resolves the packet loss problem, and deals with out-of-sequence packets. Moreover, SF-PMIPv6 is a robust authentication scheme that resists various attacks. Our simulation results demonstrate that it provides a better solution than existing schemes. 
86|2||Design of component-based real-time applications|This paper presents the key aspects of a model-based methodology that is proposed for the design of component-based applications with hard real-time requirements. The methodology relies on RT-CCM (Real-time Container Component Model), a component technology aimed to make the timing behaviour of the applications predictable and inspired in the Lightweight CCM specification of the OMG. Some new mechanisms have been introduced in the underlying framework that make it possible to schedule the execution of code and the transmission of messages of an application while guaranteeing that the application will meet its timing requirements when executed. The added mechanisms also enable the application designer to configure this scheduling without interfering with the opacity typically required in component management. Moreover, the methodology includes a process for generating the real-time model of a component-based application as a composition of the reusable real-time models of the components that form it. From the analysis of this model the application designer obtains the configuration values that must be applied to the component instances and the elements of the framework in order to make the application fulfil its timing requirements. 
86|2||A reliability optimization method for RAID-structured storage systems based on active data migration|
86|2||Invertible secret image sharing for gray level and dithered cover images|Secret image sharing approaches have been extended to obtain covers from stego images after the revealing procedure. Lin et al.’s work in 2009 uses modulus operator to decrease the share image distortion while providing recovery of original covers. Their work use gray level or binary image as cover. Stego images have approximately 43 dB and 38 dB PSNR for gray level and binary covers respectively. Lin et al.’s work in 2010 provides enhanced embedding capacity but does not support binary covers. Gray level covers’ PSNR is reported approximately 40 dB. The proposed method enhances the visual quality of stego images regardless of intensity range of covers. Exploiting Modification Direction method is used to hide the shared values into covers. The method also utilizes modulus operator to recover original cover pixels. Stego image PSNR is approximately 47 dB for gray level covers. The method provides 4–7 dB increase respectively on the stego image quality compared to others. Stego images have also higher PSNR (43 dB) for dithered covers. The proposed method generates stego images with higher PSNR regardless of the intensity range of the cover image. 
86|2||Building ubiquitous computing applications using the VERSAG adaptive agent framework|In this article, we describe a novel approach to build ubiquitous computing applications using adaptive software agents. Towards this, we propose VERSAG, a novel agent framework and architecture which combines agent mobility with the ability to dynamically change the internal structure and capabilities of agents and leads to highly versatile and lightweight software agents. We describe the framework in depth and provide design and implementation details of our prototype implementation. A case study scenario is used to illustrate the functional benefits achievable through the use of this framework in ubiquitous computing environments. Further experimental evaluation confirms the efficiency and feasibility of the VERSAG framework which outperforms traditional mobile agents, and also demonstrates applicability of the proposed framework to agent based systems where varying capabilities are required by agents over their lifecycle. 
86|2||Promoting cooperation in service-oriented MAS through social plasticity and incentives|
86|2||COTS integration and estimation for ERP|This paper presents a comprehensive set of effort and schedule estimating models for predicting Enterprise Resource Planning (ERP) implementations, available in the open literature. The first set of models uses product size to predict ERP software engineering effort as well as total integration effort. Product size is measured in terms of the number of report, interface, conversion, and extension (RICE) objects configured and customized within the commercial ERP tool. Total integration effort captures software engineering plus systems engineering, program management, change management, development test & evaluation, and training development. The second set of models predicts the duration of ERP implementation stages in terms of RICE objects, staffing, and the number of test cases. The statistical models are based on data collected from 20 programs implemented within the federal government over the course of nine years beginning in 2000. The data was collected during the time period from 2006 to 2010. The models focus on the vendor's implementation team, and therefore should be applicable to commercial ERP implementations. Finally, ERP adopters/customers can use these models to validate Vendor's Implementation Team cost proposals or estimates. 
86|2||A posteriori operation detection in evolving software models|As every software artifact, also software models are subject to continuous evolution. The operations applied between two successive versions of a model are crucial for understanding its evolution. Generic approaches for detecting operations a posteriori identify atomic operations, but neglect composite operations, such as refactorings, which leads to cluttered difference reports.To tackle this limitation, we present an orthogonal extension of existing atomic operation detection approaches for detecting also composite operations. Our approach searches for occurrences of composite operations within a set of detected atomic operations in a post-processing manner. One major benefit is the reuse of specifications available for executing composite operations also for detecting applications of them. We evaluate the accuracy of the approach in a real-world case study and investigate the scalability of our implementation in an experiment. 
86|2||Efficient reversible data hiding algorithm based on gradient-based edge direction prediction|
86|2||Image sharing method for gray-level images|In 1994, Naor and Shamir firstly proposed the concept of visual secret sharing. By using a codebook to encode a binary image into sharing images, nobody can obtain the original information from any one of the shared images unless superimposing all shared images. Although the above method can protect the security of the binary image, pixel expansion and lossy recovery are two unsolved problem. To improve the disadvantages mentioned above, a new image sharing method is proposed in this paper. The proposed method firstly use linear equations of Hill cipher to divide an image into several sub-images. Then the concept of the random grid is applied to the sub-images and to construct the shared images. Experimental result shows that the proposed scheme can effectively improve the above drawbacks. 
86|3|http://www.sciencedirect.com/science/journal/01641212/86/3|A mapping study to investigate component-based software system metrics|A component-based software system (CBSS) is a software system that is developed by integrating components that have been deployed independently. In the last few years, many researchers have proposed metrics to evaluate CBSS attributes. However, the practical use of these metrics can be difficult. For example, some of the metrics have concepts that either overlap or are not well defined, which could hinder their implementation. The aim of this study is to understand, classify and analyze existing research in component-based metrics, focusing on approaches and elements that are used to evaluate the quality of CBSS and its components from a component consumer's point of view. This paper presents a systematic mapping study of several metrics that were proposed to measure the quality of CBSS and its components. We found 17 proposals that could be applied to evaluate CBSSs, while 14 proposals could be applied to evaluate individual components in isolation. Various elements of the software components that were measured are reviewed and discussed. Only a few of the proposed metrics are soundly defined. The quality assessment of the primary studies detected many limitations and suggested guidelines for possibilities for improving and increasing the acceptance of metrics. However, it remains a challenge to characterize and evaluate a CBSS and its components quantitatively. For this reason, much effort must be made to achieve a better evaluation approach in the future. 
86|3||An improved DCT-based perturbation scheme for high capacity data hiding in H.264/AVC intra frames|
86|3||A sliding window based algorithm for frequent closed itemset mining over data streams|
86|3||Optimization of adaptation plans for a service-oriented architecture with cost, reliability, availability and performance tradeoff|
86|3||Sirius: A heuristic-based framework for measuring web usability adapted to the type of website|The unquestionable relevance of the web in our society has led to an enormous growth of websites offering all kinds of services to users. In this context, while usability is crucial in the development of successful websites, many barely consider the recommendations of experts in order to build usable designs. Including the measurement of usability as part of the development process stands out among these recommendations. One of the most accepted methods for usability evaluation by experts is heuristic evaluation. There is abundant literature on this method. However, there is a lack of clear and specific guidelines to be used in the development and evaluation process. This is probably an important factor contributing to the aforementioned generalized deficiency in web usability.We miss an evaluation method based on heuristics whose measure is adapted to the type of evaluated website. In this paper we define Sirius, an evaluation framework based on heuristics to perform expert evaluations that takes into account different types of websites. We also provide a specific set of evaluation criteria, and a usability metric that quantifies the usability level achieved by a website depending on its type. 
86|3||Improving feature location using structural similarity and iterative graph mapping|
86|3||Optimal univariate microaggregation with data suppression|Microaggregation is a disclosure limitation method that provides security through k-anonymity by modifying data before release but does not allow suppression of data. We define the microaggregation problem with suppression (MPS) to accommodate data suppression, and present a polynomial-time algorithm, based on dynamic programming, for optimal univariate microaggregation with suppression. Experimental results demonstrate the practical benefits of suppressing a few carefully selected data points during microaggregation using our method. 
86|3||Beyond ATAM: Early architecture evaluation method for large-scale distributed systems|
86|3||SMSCrypto: A lightweight cryptographic framework for secure SMS transmission|
86|3||BotMosaic: Collaborative network watermark for the detection of IRC-based botnets|
86|3||A reversible data hiding method by histogram shifting in high quality medical images|Enormous demands for recognizing complicated anatomical structures in medical images have been demanded on high quality of medical image such as each pixel expressed by 16-bit depth. Now, most of data hiding algorithms are still applied in 8-bit depth medical images. We proposed a histogram shifting method for image reversible data hiding testing on high bit depth medical images. Among image local block pixels, we exploit the high correlation for smooth surface of anatomical structure in medical images. Thus, we apply a different value for each block of pixels to produce a difference histogram to embed secret bits. During data embedding stage, the image blocks are divided into two categories due to two corresponding embedding strategies. Via an inverse histogram shifting mechanism, the original image will be accurately recovered after the hidden data extraction. Due to requirements of medical images for data hiding, we proposed six criteria: (1) well-suited for high quality medical images, (2) without salt-and-pepper, (3) applicable to medical image with smooth surface, (4) well-suited sparse histogram of intensity levels, (5) free location map, (6) ability of adjusting data embedding capacity, PSNR and Inter-Slice PSNR. We proposed a data hiding methods satisfying above 6 criteria. 
86|3||Efficient Hamming weight-based side-channel cube attacks on PRESENT|
86|3||A RAMCloud Storage System based on HDFS: Architecture, implementation and evaluation|
86|3||SQLIA detection and prevention approach for RFID systems|While SQL injection attacks have been plaguing web application systems for years, the possibility of them affecting RFID systems was only identified very recently. However, very little work exists to mitigate this serious security threat to RFID-enabled enterprise systems. At the same time, the drop in RFID tag prices coupled with the increase in storage capacity of the tags have motivated users to store more and more data on the tags for ease of access. This in turn has increased the ability that attackers have of leveraging the tags to try and mount SQLIA based malware attacks on RFID systems thereby increasing the potential threat that RFID-enabled systems pose to the enterprise systems. In this paper, we propose a detection and prevention method from RFID tag-born SQLIA attacks. We have tested all possible types of dynamic queries that may be generated in RFID systems with all possible types of attacks that can be mounted on those systems. We present an analysis and evaluation of the proposed approach to demonstrate its effectiveness in mitigating SQLIA attack. 
86|3||Constrained frequent pattern mining on univariate uncertain data|
86|3||Knowledge discovery of weighted RFM sequential patterns from customer sequence databases|In today's business environment, there is tremendous interest in the mining of interesting patterns for superior decision making. Although many successful customer relationship management (CRM) applications have been developed based on sequential pattern mining techniques, they basically assume that the importance of each customer is the same. Previous studies in CRM show that not all customers make the same contribution to a business, and it is indispensible to evaluate customer value before developing effective marketing strategies. Therefore, this study includes the concepts of recency, frequency, and monetary (RFM) analysis in the sequential pattern mining process. For a given subsequence, each customer sequence contributes its own recency, frequency, and monetary scores to represent customer importance. An efficient algorithm is developed to discover sequential patterns with high recency, frequency, and monetary scores. Empirical results show that the proposed method is efficient and can effectively discover more valuable patterns than conventional frequent pattern mining. 
86|3||A delay-constrained and priority-aware channel assignment algorithm for efficient multicast in wireless mesh networks|Many popular applications of wireless mesh networks (WMNs) depend on delay-constraint multicast communication. To support such multicast communication, this paper proposes a distributed and polynomial-time heuristic channel assignment algorithm for WMNs. The proposed algorithm considers that the multicast session requests arrive dynamically and have different priorities. When a delay-constrained multicast session is issued, the multicast tree corresponding to the session is first established. The proposed algorithm divides the path delay constraint of the multicast tree into a number of the node-based delay constraints. This algorithm also devises multiple channel selection criteria to exploit all available channels of the WMN. Using these selection criteria, each node on the multicast tree can select the best channel to meet its node delay constraint and minimize the total interference for all existing multicast sessions. In the interference minimization, the priority factor is taken into account to prevent high-priority multicast sessions from incurring more interference than low-priority multicast sessions. Finally, this paper performs simulations to demonstrate the effectiveness of the proposed heuristic channel assignment algorithm through comparison with the optimal solution. 
86|3||A computer virus spreading model based on resource limitations and interaction costs|
86|3||Securing color information of an image by concealing the color palette|This paper deals with a method to protect the color information of images by providing free access to the corresponding gray level images. Only with a secret key and the gray level images, it is then possible to view the images in color. The approach is based on a color reordering algorithm after a quantization step. Based on a layer scanning algorithm, the color reordering generates gray level images and makes it possible to embed the color palette into the gray level images using a data hiding algorithm. This work was carried out in the framework of a project aimed at providing limited access to the private digital painting database of the Louvre Museum in Paris, France. 
86|3||Triple-image encryption scheme based on one-time key stream generated by chaos and plain images|
86|3||Communities of Web service registries: Construction and management|The last few years have seen a democratization in the use of Internet technologies, mainly Web services, for electronic B2B transactions. This has triggered an increase in the number of companies’ Web service registries. In this paper, we propose to use communities as a means to organize Web service registries in such a context. We provide an automatic and implicit approach to create communities of Web service registries using registries’ WSRD descriptions. We also define the needed management operations to ensure the communities consistency during a registry/community life-cycle. Experiments we have made show the feasibility and validity of our community creation approach as well as the specified managing operations. 
86|3||AFChecker: Effective model checking for context-aware adaptive applications|
86|3||Corrigendum to âA variable-length model for masquerade detectionâ [J. Syst. Softw. 85 (2012) 2470â2478]|
86|4|http://www.sciencedirect.com/science/journal/01641212/86/4|Software Engineering in Brazil: Retrospective and prospective views|
86|4||25 years of software engineering in Brazil: Beyond an insider's view|The software engineering area is facing a growing number of challenges due to the continuing increase in software size and complexity. The challenges are addressed by the very relevant and high quality publications of the Brazilian Symposium on Software Engineering (SBES), in the past 25 editions. This article summarizes the findings from two different mapping studies about these 25 SBES editions. It also reports the results of an expert opinion survey with the most important Brazilian researchers in the software engineering (SE) area. The survey reinforces the findings of the mapping studies. It also provides guidance for future research. In addition, the studies report several findings that confirmed the validity of the research methods applied. All of these findings are important input to the current Brazilian SE scenario. Our findings also suggest that greater attention should be given to the SE area, by improving researchers’ interaction with industry and increasing collaboration between researchers, especially internationally. 
86|4||Contributions to the emergence and consolidation of Agent-oriented Software Engineering|
86|4||The crosscutting impact of the AOSD Brazilian research community|BackgroundAspect-Oriented Software Development (AOSD) is a paradigm that promotes advanced separation of concerns and modularity throughout the software development lifecycle, with a distinctive emphasis on modular structures that cut across traditional abstraction boundaries. In the last 15 years, research on AOSD has boosted around the world. The AOSD-BR research community (AOSD-BR stands for AOSD in Brazil) emerged in the last decade, and has provided different contributions in a variety of topics. However, despite some evidence in terms of the number and quality of its outcomes, there is no organized characterization of the AOSD-BR community that positions it against the international AOSD Research community and the Software Engineering Research community in Brazil.AimsIn this paper, our main goal is to characterize the AOSD-BR community with respect to the research developed in the last decade, confronting it with the AOSD international community and the Brazilian Software Engineering community.MethodData collection, validation and analysis were performed in collaboration with several researchers of the AOSD-BR community. The characterization was presented from three different perspectives: (i) a historical timeline of events and main milestones achieved by the community; (ii) an overview of the research developed by the community, in terms of key challenges, open issues and related work; and (iii) an analysis on the impact of the AOSD-BR community outcomes in terms of well-known indicators, such as number of papers and number of citations.ResultsOur analysis showed that the AOSD-BR community has impacted both the international AOSD Research community and the Software Engineering Research community in Brazil. 
86|4||A scoping study on the 25 years of research into software testing in Brazil and an outlook on the future of the area|
86|4||Evaluation studies of software testing research in Brazil and in the world: A survey of two premier software engineering conferences|This paper reports on a historical perspective of the evaluation studies present in software testing research published in the Brazilian Symposium on Software Engineering (SBES) in comparison to the International Conference on Software Engineering (ICSE). The survey characterizes the software testing-related papers published in the 25-year history of SBES, investigates the types of evaluation presented in these publications, and how the rate of evaluations has evolved over the years. A similar analysis within the same period is made for ICSE, allowing for a comparison between the national and international scenario. Results show that the rate of papers that present evaluation studies in SBES has significantly increased over the years. However, among the papers that described some kind of evaluation, only around 20% performed more rigorous evaluations (i.e. case studies, quasi experiments, or controlled experiments). Such percentage is low when compared to ICSE, which presented 40% of papers with more rigorous evaluations within the same period. Nevertheless, we noticed that both venues still lack the publication of research reporting controlled experiments: only a single paper in each conference presented this type of evaluation. 
86|4||Search Based Software Engineering: Review and analysis of the field in Brazil|Search Based Software Engineering (SBSE) is the field of software engineering research and practice that applies search based techniques to solve different optimization problems from diverse software engineering areas. SBSE approaches allow software engineers to automatically obtain solutions for complex and labor-intensive tasks, contributing to reduce efforts and costs associated to the software development. The SBSE field is growing rapidly in Brazil. The number of published works and research groups has significantly increased in the last three years and a Brazilian SBSE community is emerging. This is mainly due to the Brazilian Workshop on Search Based Software Engineering (WOES), co-located with the Brazilian Symposium on Software Engineering (SBES). Considering these facts, this paper presents results of a mapping we have performed in order to provide an overview of the SBSE field in Brazil. The main goal is to map the Brazilian SBSE community on SBES by identifying the main researchers, focus of the published works, fora and frequency of publications. The paper also introduces SBSE concerns and discusses trends, challenges, and open research problems to this emergent area. We hope the work serves as a reference to this novel field, contributing to disseminate SBSE and to its consolidation in Brazil. 
86|4||Relevance and perspectives of AAL in Brazil|
86|4||A Brazilian survey on UML and model-driven practices for embedded software development|
86|4||Comparing approaches to analyze refactoring activity on software repositories|Some approaches have been used to investigate evidence on how developers refactor their code, whether refactorings activities may decrease the number of bugs, or improve developers’ productivity. However, there are some contradicting evidence in previous studies. For instance, some investigations found evidence that if the number of refactoring changes increases in the preceding time period the number of defects decreases, different from other studies. They have used different approaches to evaluate refactoring activities. Some of them identify committed behavior-preserving transformations in software repositories by using manual analysis, commit messages, or dynamic analysis. Others focus on identifying which refactorings are applied between two programs by using manual inspection or static analysis. In this work, we compare three different approaches based on manual analysis, commit message (Ratzinger's approach) and dynamic analysis (SafeRefactor's approach) to detect whether a pair of versions determines a refactoring, in terms of behavioral preservation. Additionally, we compare two approaches (manual analysis and Ref-Finder) to identify which refactorings are performed in each pair of versions. We perform both comparisons by evaluating their accuracy, precision, and recall in a randomly selected sample of 40 pairs of versions of JHotDraw, and 20 pairs of versions of Apache Common Collections. While the manual analysis presents the best results in both comparisons, it is not as scalable as the automated approaches. Ratzinger's approach is simple and fast, but presents a low recall; differently, SafeRefactor is able to detect most applied refactorings, although limitations in its test generation backend results for some kinds of subjects in low precision values. Ref-Finder presented a low precision and recall in our evaluation. 
86|4||On the impact of trace-based feature location in the performance of software maintainers|
86|4||Safe composition of configuration knowledge-based software product lines|
86|4||Offshore insourcing in software development: Structuring the decision-making process|
86|4||Secret image sharing scheme with authentication and remedy abilities based on cellular automata and discrete wavelet transform|A meaningful secret image sharing scheme with authentication and remedy abilities is proposed in this paper. One dimensional cellular automata, discrete wavelet transform and hash function are adopted in the proposed scheme. The stego images are allowed to be verified to determine whether they are tampered or not. Once the stego images are tampered, shared bits retrieved from these tampered areas cannot be used to reconstruct the secret image. Instead, those damaged areas in the secret image can be repaired by the hidden information. Experimental results exhibit that low computation cost, high tamper detection rate and advanced remedy ability against tampering and cropping attacks are achieved by the proposed scheme. 
86|4||Petri net based techniques for constructing reliable service composition|
86|4||Federated broker system for pervasive context provisioning|Software systems that provide context-awareness related functions in pervasive computing environments are gaining momentum due to emerging applications, architectures and business models. In most context-aware systems, a central broker performs the functions of context acquisition, processing, reasoning and provisioning to facilitate context-consuming applications, but demonstrations of such prototypical systems are limited to small, focussed domains. In order to develop modern context-aware systems that are capable of accommodating emerging pervasive/ubiquitous computing scenarios, are easily manageable, administratively and geographically scalable, it is desirable to have multiple brokers in the system divided into administrative, network, geographic, contextual or load based domains. Context providers and consumers may be configured to interact only with their nearest, relevant or most convenient broker. This setup demands inter-broker federation so that providers and consumers attached to different brokers can interact seamlessly, but such a federation has not been proposed for context-aware systems. This article analyses the limiting factors in existing context-aware systems, postulates the design and functional requirements that modern context-aware systems need to accommodate, and presents a federated broker based architecture for provisioning of contextual information over large geographical and network spans. 
86|4||Comparing risk identification techniques for safety and security requirements|When developing systems where safety and security are important aspects, these aspects have to be given special attention throughout the development, in particular in the requirements phase. There are many similar techniques within the safety and security fields, but few comparisons about what lessons that could be learnt and benefits to be gained. In this paper different techniques for identifying risk, hazard and threat of computer-supported systems are compared. This is done by assessing the techniques’ ability to identify different risks in computer-supported systems in the environment where they operate. The purpose of this paper is therefore to investigate whether and how the techniques can mutually strengthen each other. The result aids practitioners in the selection and combination of techniques and researchers in focusing on gaps between the two fields. Among other things, the findings suggest that many safety techniques enforce a creative and systematic process by applying guide-words and structuring the results in worksheets, while security techniques tend to integrate system models with security models. 
86|5|http://www.sciencedirect.com/science/journal/01641212/86/5|MDE software process lines in small companies|Software organizations specify their software processes so that process knowledge can be systematically reused across projects. However, different projects may require different processes. Defining a separate process for each potential project context is expensive and error-prone, since these processes must simultaneously evolve in a consistent manner. Moreover, an organization cannot envision all possible project contexts in advance because several variables may be involved, and these may also be combined in different ways. This problem is even worse in small companies since they usually cannot afford to define more than one process. Software process lines are a specific type of software product lines, in the software process domain. A benefit of software process lines is that they allow software process customization with respect to a context. In this article we propose a model-driven approach for software process lines specification and configuration. The article also presents two industrial case studies carried out at two small Chilean software development companies. Both companies have benefited from applying our approach to their processes: new projects are now developed using custom processes, process knowledge is systematically reused, and the total time required to customize a process is much shorter than before. 
86|5||Evidence of software inspection on feature specification for software product lines|In software product lines (SPL), scoping is a phase responsible for capturing, specifying and modeling features, and also their constraints, interactions and variations. The feature specification task, performed in this phase, is usually based on natural language, which may lead to lack of clarity, non-conformities and defects. Consequently, scoping analysts may introduce ambiguity, inconsistency, omissions and non-conformities. In this sense, this paper aims at gathering evidence about the effects of applying an inspection approach to feature specification for SPL. Data from a SPL reengineering project were analyzed in this work and the analysis indicated that the correction activity demanded more effort. Also, Pareto's principle showed that incompleteness and ambiguity reported higher non-conformity occurrences. Finally, the Poisson regression analysis showed that sub-domain risk information can be a good indicator for prioritization of sub-domains in the inspection activity. 
86|5||Automated test data generation for branch testing using genetic algorithm: An improved approach using branch ordering, memory and elitism|One of the problems faced in generating test data for branch coverage using a metaheuristic technique is that the population may not contain any individual that encodes test data for which the execution reaches the predicate node of the target branch. In order to deal with this problem, in this paper, we (a) introduce three approaches for ordering branches for selection as targets for coverage with a genetic algorithm (GA) and (b) experimentally evaluate branch ordering together with elitism and memory to improve test data generation performance. An extensive preliminary study was carried out to help frame the research questions and fine tune GA parameters which were then used in the final experimental study. 
86|5||Testing Real-Time Embedded Systems using Timed Automata based approaches|
86|5||An efficient tree-based algorithm for mining sequential patterns with multiple minimum supports|Sequential pattern mining (SPM) is an important technique for determining time-related behavior in sequence databases. In real-life applications, the frequencies for various items in a sequence database are not exactly equal. If all items are set with the same minimum support, the rare item problem may result, meaning that we are unable to effectively retrieve interesting patterns regardless of whether minsup is set too high or too low. Liu (2006) first included the concept of multiple minimum supports (MMSs) to SPM. It allows users to specify the minimum item support (MIS) for each item according to its natural frequency. A generalized sequential pattern-based algorithm, named Multiple Supports – Generalized Sequential Pattern (MS-GSP), was also developed to mine complete set of sequential patterns. However, the MS-GSP adopts candidate generate-and-test approach, which has been recognized as a costly and time-consuming method in pattern discovery. For the efficient mining of sequential patterns with MMSs, this study first proposes a compact data structure, called a Preorder Linked Multiple Supports tree (PLMS-tree), to store and compress the entire sequence database. Based on a PLMS-tree, we develop an efficient algorithm, Multiple Supports – Conditional Pattern growth (MSCP-growth), to discover the complete set of patterns. The experimental result shows that the proposed approach achieves more preferable findings than the MS-GSP and the conventional SPM. 
86|5||A reference architecture for organizing the internal structure of metadata-based frameworks|
86|5||A pattern fusion model for multi-step-ahead CPU load prediction|
86|5||Quality-adaptive visual secret sharing by random grids|Visual secret sharing (VSS), classified into visual-cryptography (VC)-based and random-grid (RG)-based, suffers from the contrast problem that the reconstructed secret with low visual quality is not easy to recognize. Even worse, the more share images stacked, the lower visual quality of reconstructed secrets revealed. Therefore, it is promising to remove this innate drawback. In this paper, with security still kept, the light transmission of share images generated by the proposed scheme is redesigned to be higher than before such that the better visual quality of reconstructed secrets is obtained. To demonstrate the feasibility, the experimental results show the reconstructed secrets are visually recognizable and the goal that the more share images stacked, the better quality of reconstructed secrets we have is achieved. 
86|5||Measuring the impact of changes to the complexity and coupling properties of automotive software systems|BackgroundIn the past few decades exponential increase in the amount of software used in cars has been recorded together with enhanced requirements for functional safety of their embedded software. As the evolution of software systems in cars often entails changes to software architecture, it is important to be able to monitor their impact.MethodWe conducted a case study on a distributed software system in cars at Volvo Car Corporation with the goal to develop, apply and evaluate measures of complexity and coupling which could support software architects in monitoring changes.ResultsThe results showed that two metrics – structural complexity and coupling measures – can guide architectural work and turn attention of architects to most complex subsystems. The results were confirmed by monitoring a complete electrical system of a vehicle under two releases.ConclusionBy applying the metrics after each significant change in the architecture, it is possible to verify that certain quality attributes have not deteriorated and to identify new testing areas. Using these metrics increases the product quality with respect to stability, reliability, and maintainability and also has potential to reduce long-term software development/maintenance costs. 
86|5||Software ecosystems â A systematic literature review|A software ecosystem is the interaction of a set of actors on top of a common technological platform that results in a number of software solutions or services. Arguably, software ecosystems are gaining importance with the advent of, e.g., the Google Android, Apache, and Salesforce.com ecosystems. However, there exists no systematic overview of the research done on software ecosystems from a software engineering perspective. We performed a systematic literature review of software ecosystem research, analyzing 90 papers on the subject taken from a gross collection of 420. Our main conclusions are that while research on software ecosystems is increasing (a) there is little consensus on what constitutes a software ecosystem, (b) few analytical models of software ecosystems exist, and (c) little research is done in the context of real-world ecosystems. This work provides an overview of the field, while identifying areas for future research. 
86|5||3D architecture viewpoints on service automation|
86|5||Continuous range k-nearest neighbor queries in vehicular ad hoc networks|A driver should constantly keep an eye on nearby vehicles in order to avoid collisions. Unfortunately, the driver often does not see nearby vehicles because of obstacles (e.g., other vehicles, trees, buildings, etc.). This paper introduces a novel type of query, called a continuous range k-nearest neighbor (CRNN) query, in vehicular ad hoc networks, and it presents a new approach to process such a query. Most existing solutions to continuous nearest neighbor (CNN) queries focus on static objects, such as gas stations and restaurants, while this work concentrates on CRNN queries over moving vehicles. This is a challenging problem due to the high mobility of the vehicles. The CRNN query has characteristics in common with continuous range (CR) and CNN queries. In terms of CNN queries, the proposed approach achieves the same goal as the existing solutions, which is to decide effectively on valid intervals during which the query result remains unchanged. The proposed scheme aims to minimize the use of wireless network bandwidth, the computational cost, and the local storage while preserving information on the continuous movement of vehicles within the broadcast range of a given vehicle. Extensive experimental results confirm the effectiveness and superiority of the proposed scheme in comparison with an existing method. 
86|5||Agile requirements prioritization in large-scale outsourced system projects: An empirical study|The application of agile practices for requirements prioritization in distributed and outsourced projects is a relatively recent trend. Hence, not all of its facets are well-understood. This exploratory study sets out to uncover the concepts that practitioners in a large software organization use in the prioritization process and the practices that they deem good. We seek to provide a rich analysis and a deep understanding of three cases in an exploratory study that was carried out in a large and mature company, widely recognized for its excellence and its engagement in outsourced software development. We used in-depth interviews for data collection and grounded theory techniques for data analysis. Our exploration efforts yielded the following findings: (i) understanding requirements dependencies is of paramount importance for the successful deployment of agile approaches in large outsourced projects. (ii) Next to business value, the most important prioritization criterion in the setting of outsourced large agile projects is risk. (iii) The software organization has developed a new artefact that seems to be a worthwhile contribution to agile software development in the large: ‘delivery stories’, which complement user stories with technical implications, effort estimation and associated risk. The delivery stories play a pivotal role in requirements prioritization. (iv) The vendor's domain knowledge is a key asset for setting up successful client-developer collaboration. (v) The use of agile prioritization practices depends on the type of project outsourcing arrangement. Our findings contribute to the empirical software engineering literature by bringing a rich analysis of cases in agile and distributed contexts, from a vendor's perspective. We also discuss the possible implications of the results for research and in practice. 
86|5||A survey of software testing practices in Canada|Software testing is an important activity in the software development life-cycle. In an earlier study in 2009, we reported the results of a regional survey of software testing practices among practitioners in the Canadian province of Alberta. To get a larger nationwide view on this topic (across Canada), we conducted a newer survey with a revised list of questions in 2010. Compared to our previous Alberta-wide survey (53 software practitioners), the nation-wide survey had larger number of participants (246 practitioners). We report the survey design, execution and results in this article. The survey results reveal important and interesting findings about software testing practices in Canada. Whenever possible, we also compare the results of this survey to other similar studies, such as the ones conducted in the US, Sweden and Australia, and also two previous Alberta-wide surveys, including our 2009 survey. The results of our survey will be of interest to testing professionals both in Canada and world-wide. It will also benefit researchers in observing the latest trends in software testing industry identifying the areas of strength and weakness, which would then hopefully encourage further industry-academia collaborations in this area. Among the findings are the followings: (1) the importance of testing-related training is increasing, (2) functional and unit testing are two common test types that receive the most attention and efforts spent on them, (3) usage of the mutation testing approach is getting attention among Canadian firms, (4) traditional Test-last Development (TLD) style is still dominating and a few companies are attempting the new development approaches such as Test-Driven Development (TDD), and Behavior-Driven Development (BDD), (5) in terms of the most popular test tools, NUnit and Web application testing tools overtook JUnit and IBM Rational tools, (6) most Canadian companies use a combination of two coverage metrics: decision (branch) and condition coverage, (7) number of passing user acceptance tests and number of defects found per day (week or month) are regarded as the most important quality assurance metrics and decision factors to release, (8) in most Canadian companies, testers are out-numbered by developers, with ratios ranging from 1:2 to 1:5, (9) the majority of Canadian firms spent less than 40% of their efforts (budget and time) on testing during development, and (10) more than 70% of respondents participated in online discussion forums related to testing on a regular basis. 
86|5||An improvement of diamond encoding using characteristic value positioning and modulus function|
86|5||An enhanced variable-length arithmetic coding and encryption scheme using chaotic maps|We enhance the simultaneous arithmetic coding and encryption scheme previously proposed by us. By encoding a block of variable number of symbols to a codeword within the length of the computation register, the operating efficiency has been substantially improved. Moreover, the compressed sequence is processed by an additional diffusion operation which strengthens the security of the original scheme by having higher key and plaintext sensitivities. Simulation results show that the enhanced scheme runs faster than the original scheme and the traditional compress-then-encrypt approach at a comparable compression performance. 
86|5||Towards innovation measurement in the software industry|
86|5||Lifetime and QoS-aware energy-saving buffering schemes|
86|5||A novel semantic information retrieval system based on a three-level domain model|This paper presents a methodology and a prototype for extracting and indexing knowledge from natural language documents. The underlying domain model relies on a conceptual level (described by means of a domain ontology), which represents the domain knowledge, and a lexical level (based on WordNet), which represents the domain vocabulary. A stochastic model (the ME-2L-HMM2, which mixes – in a novel way – HMM and maximum entropy models) stores the mapping between such levels, taking into account the linguistic context of words. Not only does such a context contain the surrounding words; it also contains morphologic and syntactic information extracted using natural language processing tools. The stochastic model is then used, during the document indexing phase, to disambiguate word meanings. The semantic information retrieval engine we developed supports simple keyword-based queries, as well as natural language-based queries. The engine is also able to extend the domain knowledge, discovering new and relevant concepts to add to the domain model. The validation tests indicate that the system is able to disambiguate and extract concepts with good accuracy. A comparison between our prototype and a classic search engine shows that the proposed approach is effective in providing better accuracy. 
86|5||Controlling ERP consultants: Client and provider practices|Hiring consultants to implement the multiple components of an ERP installation is a common practice for securing expertise not found in client organizations. Ensuring the consultants work to the benefit of their client is potentially problematic as the consultants must adopt the goals of the client, coordinate with stakeholders within the client organization, and coordinate with those installing other components of the ERP. Controls are mechanisms that keep consultants on track with the objectives of client organizations. Control forms vary depending on the nature of the activity and the levels of expertise across clients and consultants. Just what forms of control are typically employed over consultants to promote the likelihood of a successful ERP implementation is not identified in the prior literature. Control theory is employed by this study to formulate an expectation of control, which is then examined through a multiple case study. Interviews of consultants, project managers in the client organizations, and project managers in the consulting firms confirm that client organizations employ performance controls on specified outcomes while the provider firms employ both outcome controls and behavioral controls to keep the ERP implementation project in line with client goals. 
86|5||Corrigendum to âT. Chen, K. Tsao, Threshold visual secret sharing by random gridsâ [J. Syst. Softw. 84 (2011) 1197â1208]|
86|6|http://www.sciencedirect.com/science/journal/01641212/86/6|New steganography algorithm to conceal a large amount of secret message using hybrid adaptive neural networks with modified adaptive genetic algorithm|In this paper, we propose a new steganography algorithm using non-uniform adaptive image segmentation (NUAIS) with an intelligent computing technique to conceal efficiently a large amount of confidential messages (Smsg) into color images. Whereas, the number of secret bits to be replaced is non uniform from byte to another byte; it based on byte characteristics, which are extracted by using 16 byte levels (BL) with variance distribution of the Neighboring Eight Bytes (NEB) around the current byte. Four security layers are introduced to increase resistance against statistical and visual attacks. These layers are designed to make an excellent imperceptible concealing Smsg with lower distortion of a color plane and high protection of Smsg. The proposed intelligent technique using the hybrid adaptive neural networks and modified adaptive genetic algorithm employing uniform adaptive relaxation (ANN_AGAUAR) is working as the fourth security layer to improve the quality of the stego image (Is). The results are discussed and compared with the previous steganography algorithms; it demonstrates that the proposed algorithm's effectiveness can be concealed efficiently the number of secret bits reached to four bits per byte with better visual quality. 
86|6||Toward automated refactoring of crosscutting concerns into aspects|Aspect-oriented programing (AOP) improves the separation of concerns by encapsulating crosscutting concerns into aspects. Thus, aspect-oriented programing aims to better support the evolution of systems. Along this line, we have defined a process that assists the developer to refactor an object-oriented system into an aspect-oriented one. In this paper we propose the use of association rules and Markov models to improve the assistance in accomplishing some of the tasks of this process. Specifically, we use these techniques to help the developer in the task of encapsulating a fragment of aspectizable code into an aspect. This includes the choice of a fragment of aspectizable code to be encapsulated, the selection of a suitable aspect refactoring, and the analysis and application of additional restructurings when necessary. Our case study of the refactoring of a J2EE system shows that the use of the process reduces the intervention of the developer during the refactoring. 
86|6||An exploration of technical debt|ContextWhilst technical debt is considered to be detrimental to the long term success of software development, it appears to be poorly understood in academic literature. The absence of a clear definition and model for technical debt exacerbates the challenge of its identification and adequate management, thus preventing the realisation of technical debt's utility as a conceptual and technical communication device.ObjectiveTo make a critical examination of technical debt and consolidate understanding of the nature of technical debt and its implications for software development.MethodAn exploratory case study technique that involves multivocal literature review, supplemented by interviews with software practitioners and academics to establish the boundaries of the technical debt phenomenon.ResultA key outcome of this research is the creation of a theoretical framework that provides a holistic view of technical debt comprising a set of technical debts dimensions, attributes, precedents and outcomes, as well as the phenomenon itself and a taxonomy that describes and encompasses different forms of the technical debt phenomenon.ConclusionThe proposed framework provides a useful approach to understanding the overall phenomenon of technical debt for practical purposes. Future research should incorporate empirical studies to validate heuristics and techniques that will assist practitioners in their management of technical debt. 
86|6||MostoDE: A tool to exchange data amongst semantic-web ontologies|
86|6||Incremental service level agreements violation handling with time impact analysis|
86|6||Does decision documentation help junior designers rationalize their decisions? A comparative multiple-case study|
86|6||Map-matched trajectory compression|The wide usage of location aware devices, such as GPS-enabled cellphones or PDAs, generates vast volumes of spatiotemporal streams of location data raising management challenges, such as efficient storage and querying. Therefore, compression techniques are inevitable also in the field of moving object databases. Related work is relatively limited and mainly driven by line simplification and data sequence compression techniques. Moreover, due to the (unavoidable) erroneous measurements from GPS devices, the problem of matching the location recordings with the underlying traffic network has recently gained the attention of the research community. So far, the proposed compression techniques have not been designed for network constrained moving objects, while on the other hand, existing map matching algorithms do not take compression aspects into consideration. In this paper, we propose solutions tackling the combined, map matched trajectory compression problem, the efficiency of which is demonstrated through an extensive experimental evaluation on offline and online trajectory data using synthetic and real trajectory datasets. 
86|6||A framework for query refinement with user feedback|SQL queries in the existing relational data model implement the binary satisfaction of tuples. That is, a data tuple is filtered out from the result set if it does not satisfy the constraints expressed in the predicates of the user submitted query. Posing appropriate queries for ordinary users is very difficult in the first place if they lack knowledge of the underlying dataset. Therefore, imprecise queries are commonplace for many users. In connection with this, this paper presents a framework for capturing user intent through feedback for refining the initial imprecise queries that can fulfill the users’ information needs. The feedback in our framework consists of both unexpected tuples currently present in the query output and expected tuples that are missing from the query output. We show that our framework does not require users to provide the complete set of feedback tuples because only a subset of this feedback can suffice. We provide the point domination theory to complement the other members of feedback. We also provide algorithms to handle both soft and hard requirements for the refinement of initial imprecise queries. Experimental results suggest that our approach is promising compared to the decision tree based query refinement approach. 
86|6||Introducing automated procedures in 3G network planning and optimization|
86|6||Closed inter-sequence pattern mining|
86|6||Testing techniques selection based on ODC fault types and software metrics|Software testing techniques differ in the type of faults they are more prone to detect, and their performance varies depending on the features of the application being tested. Practitioners often use informally their knowledge about the software under test in order to combine testing techniques for maximizing the number of detected faults.This work presents an approach to enable practitioners to select testing techniques according to the features of the software to test. A method to build a testing-related base of knowledge for tailoring the techniques selection process to the specific application(s) is proposed. The method grounds upon two basic steps: (i) constructing, on an empirical basis, models to characterize the software to test in terms of fault types it is more prone to contain; (ii) characterizing testing techniques with respect to fault types they are more prone to detect in the given context.Using the created base of knowledge, engineers within an organization can define the mix of techniques so as to maximize the effectiveness of the testing process for their specific software. 
86|6||Robust and secure watermarking scheme for breath sound|Due to the development of the Internet, security and intellectual property protection have attracted significant interest in the copyright protection field recently. A novel watermarking scheme for breath sounds, combining lifting wavelet transform (LWT), discrete cosine transform (DCT), singular value decomposition (SVD) and dither modulation (DM) quantization is proposed in this paper as a way to insert encrypted source and identity information in breath sounds while maintaining significant biological signals. In the proposed scheme, LWT is first performed to decompose the signal, and then DCT is applied on the approximate coefficients. SVD is carried out on the LWT–DCT coefficients to derive singular values. DM is adopted to quantize the singular values of each of the LWT–DCT blocks; thus, the watermark extraction is blind by using the DM algorithm. The novelty of our proposed method also includes the introduction of the particle swarm optimization (PSO) technique to optimize the quantization steps for the DM approach. The experimental results demonstrate that the proposed watermarking scheme obtains good robustness against common manipulation attacks and preserves imperceptivity. The performance comparison results verify that our scheme outperforms existing approaches in terms of robustness and imperceptibility. 
86|6||PS-QUASAR: A publish/subscribe QoS aware middleware for Wireless Sensor and Actor Networks|It has been more than 30 years since the first research into Wireless Sensor and Actor Networks appeared. However, WSANs are still not a ubiquitous technology due to several factors which include a lack of Quality of Service (QoS) support or the absence of high level programming models. New applications with heterogeneous QoS requirements where WSANs can be successfully applied, such as Critical Infrastructure Protection (CIP), have been recognized.PS-QUASAR, a middleware for WSANs that deals with these two issues, offers a high level simple programming model based on the publish/subscribe paradigm. In this model all nodes in the network are potential publishers of each of the topics. PS-QUASAR also handles QoS (reliability, deadline, priority) and supports a many-to-many exchange of messages between nodes in a fully distributed way by means of multicasting techniques. Performance evaluation via simulation using the Contiki operating system shows that the protocol can handle multiple publishers and subscribers at the same time whilst dealing with QoS requirements. 
86|6||A survey study of critical success factors in agile software projects in former Yugoslavia IT companies|Determining the factors that have an influence on the success of the software development projects has been the focus of extensive research for more than 30 years. In recent years agile methodology of software development has become the dominant one for all kinds of software development projects. In this paper we present the results of empirical study for determining critical factors that influence the success of agile software projects which we conducted among senior developers and project managers from IT companies located in the former Yugoslavia countries within South Eastern Europe (SEE) region. This study is inspired by the similar study conducted 5 years ago (Chow and Cao, 2008). With this study we were not able to confirm the model developed in the previous study. Moreover it disconfirmed not only part of the factors, but very much questioned the whole scheme. However, we were able to shed additional light regarding agile software development in former Yugoslavia countries from SEE region as a reference region for investigating outsourced projects done in agile way. 
86|6||Graph-based reference table construction to facilitate entity matching|Entity matching plays a crucial role in information integration among heterogeneous data sources, and numerous solutions have been developed. Entity resolution based on reference table has the benefits of high efficiency and being easy to update. In such kind of methods, the reference table is important for effective entity matching. In this paper, we focus on the construction of effective reference table by relying on co-occurring relationship between tokens to identify suitable entity names. To achieve high efficiency and accuracy, we first model data set as graph, and then cluster the vertices in the graph in two stages. Based on the connectivity between vertices, we also mine synonyms and get the expansive reference table. We develop an iterative system and conduct an experimental study using real data. Experimental results show that the method in this paper achieves both high accuracy and efficiency. 
86|6||Securing web-clients with instrumented code and dynamic runtime monitoring|Security and privacy concerns remain a major factor that hinders the whole scale adoption of web-based technology in sensitive situations, such as financial transactions (0085 and 0210). These concerns impact both end users and content generators. To tackle this problem requires a complimentary technology to the already developed and deployed infrastructure for web security. Hence, we have developed a multi-layer framework for web client security based on mobile code instrumentation. This architecture seeks to isolate exploitable security vulnerabilities and enforce runtime policies against malicious code constructs. Our instrumentation process uniquely integrates both static and dynamic engines and is driven by flexible (XML based) rewrite rules for a scalable operation and transparent deployment.Based on secure equivalents for vulnerable JavaScript objects and methods, our mechanism offers superior runtime performance compared to other approaches. Extensive investigation using four case studies shows that the instrumentation technique provides a potential solution to curb the rising number of security exploits that exist on the web today. In addition, performance data gathered from evaluations on active websites demonstrate that the mechanism has very little impact in terms of user experience; thus making it plausible for adoption by end-users. 
86|6||Compositional real-time scheduling framework for periodic reward-based task model|As the size and complexity of embedded software systems increase, compositional real-time scheduling framework is widely accepted as means to build large and complex systems. A compositional real-time scheduling framework proposes to decompose a system into independent subsystems and provides ways to assemble them into a flexible hierarchical real-time scheduling system while guaranteeing the internal real-time requirements of each subsystem. In this paper, we consider the imprecise reward-based periodic task model in compositional scheduling framework. Thus, we introduce the imprecise periodic resource model to characterize the imprecise resource allocations provided by the system to a single component, and the interface model to abstract the imprecise real-time requirements of the component. The schedulability of mandatory parts is also analyzed to meet the minimum requirement of tasks. Finally, we provide a scheduling algorithm to guarantee a certain amount of reward, which makes it feasible to efficiently compose multiple imprecise components. 
86|7|http://www.sciencedirect.com/science/journal/01641212/86/7|Collaborative computing technologies and systems|
86|7||The effects of a shared free form rationale space in collaborative learning activities|
86|7||Setting the best view of a virtual teacher in a mixed reality physical-task learning support system|
86|7||Bringing knowledge into recommender systems|
86|7||A groupware system to support collaborative programming: Design and experiences|
86|7||Metamodel-driven definition of a visual modeling language for specifying interactive groupware applications: An empirical study|
86|7||A high performance peer to cloud and peer model augmented with hierarchical secure communications|
86|7||SETZ logistics models and system framework for manufacturing and exporting large engineering assets|
86|7||Scheduling of scientific workflow in non-dedicated heterogeneous multicluster platform|Many scientific workflows can be structured as Parallel Task Graphs (PTGs), that is, graphs of data-parallel tasks. Adding data parallelism to a workflow provides opportunities for higher performance and scalability. Workflow tasks are data-parallel and moldable, and clusters are not only heterogeneous but also non-dedicated for workflow execution. Therefore, scheduling such scientific workflow in a multicluster platform becomes a challenging task. To address this problem, we study the scheduling of scientific workflow in a non-dedicated heterogeneous multicluster platform aimed at minimizing the makespan for workflow execution. In this paper, three scheduling algorithms for effective workflow task mapping and resource allocation are proposed, among them MHEFT-RSV and MHEFT-RSV-BD are heuristic algorithms. An exact branch-and-cut scheduling algorithm is implemented, which exploits the intertask precedence and resource constraints thereby accelerating the process of obtaining a feasible schedule with minimized makespan. Detailed simulation experiments show that on average the exact branch-and-cut algorithm obtains shorter makespan for small and medium size workflows, while MHEFT-RSV and MHEFT-RSV-BD achieves better tradeoff between makespan and computation time for large scientific workflows. 
86|7||Group and link analysis of multi-relational scientific social networks|
86|7||A mixed-method approach for the empirical evaluation of the issue-based variability modeling|
86|7||Design and testbed evaluation of RDMA-based middleware for high-performance data transfer applications|
86|7||A fault induction technique based on voltage underfeeding with application to attacks against AES and RSA|
86|7||Software effort models should be assessed via leave-one-out validation|ContextMore than half the literature on software effort estimation (SEE) focuses on model comparisons. Each of those requires a sampling method (SM) to generate the train and test sets. Different authors use different SMs such as leave-one-out (LOO), 3Way and 10Way cross-validation. While LOO is a deterministic algorithm, the N-way methods use random selection to build their train and test sets. This introduces the problem of conclusion instability where different authors rank effort estimators in different ways.ObjectiveTo reduce conclusion instability by removing the effects of a sampling method's random test case generation.MethodCalculate bias and variance (B&V) values following the assumption that a learner trained on the whole dataset is taken as the true model; then demonstrate that the B&V and runtime values for LOO are similar to N-way by running 90 different algorithms on 20 different SEE datasets. For each algorithm, collect runtimes, B&V values under LOO, 3Way and 10Way.ResultsWe observed that: (1) the majority of the algorithms have statistically indistinguishable B&V values under different SMs and (2) different SMs have similar run times.ConclusionIn terms of their generated B&V values and runtimes, there is no reason to prefer N-way over LOO. In terms of reproducibility, LOO removes one cause of conclusion instability (the random selection of train and test sets). Therefore, we depreciate N-way and endorse LOO validation for assessing effort models. 
86|7||Supporting adaptation of decentralized software based on application scenarios|
86|7||An approach for constructing private storage services as a unified fault-tolerant system|
86|7||An approach to software reliability prediction based on time series modeling|Reliability is the key factor for software system quality. Several models have been introduced to estimate and predict reliability based on results of software testing activities. Software Reliability Growth Models (SRGMs) are considered the most commonly used to achieve this goal. Over the past decades, many researchers have discussed SRGMs’ assumptions, applicability, and predictability. They have concluded that SRGMs have many shortcomings related to their unrealistic assumptions, environment-dependent applicability, and questionable predictability. Several approaches based on non-parametric statistics, Bayesian networks, and machine learning methods have been proposed in the literature. Based on their theoretical nature, however, they cannot completely address the SRGMs’ limitations. Consequently, addressing these shortcomings is still a very crucial task in order to provide reliable software systems. This paper presents a well-established prediction approach based on time series ARIMA (Autoregressive Integrated Moving Average) modeling as an alternative solution to address the SRGMs’ limitations and provide more accurate reliability prediction. Using real-life data sets on software failures, the accuracy of the proposed approach is evaluated and compared to popular existing approaches. 
86|7||Applying hybrid learning approach to RoboCup's strategy|
86|7||Research state of the art on GoF design patterns: A mapping study|Design patterns are used in software development to provide reusable and documented solutions to common design problems. Although many studies have explored various aspects of design patterns, no research summarizing the state of research related to design patterns existed up to now. This paper presents the results of a mapping study of about 120 primary studies, to provide an overview of the research efforts on Gang of Four (GoF) design patterns. The research questions of this study deal with (a) if design pattern research can be further categorized in research subtopics, (b) which of the above subtopics are the most active ones and (c) what is the reported effect of GoF patterns on software quality attributes. The results suggest that design pattern research can be further categorized to research on GoF patterns formalization, detection and application and on the effect of GoF patterns on software quality attributes. Concerning the intensity of research activity of the abovementioned subtopics, research on pattern detection and on the effect of GoF patterns on software quality attributes appear to be the most active ones. Finally, the reported research to date on the effect of GoF patterns on software quality attributes are controversial; because some studies identify one pattern's effect as beneficial whereas others report the same pattern's effect as harmful. 
86|7||A high capacity lossless data hiding scheme for JPEG images|In this paper, we propose a new high-capacity reversible data hiding method for JPEG-compressed images. This method is based on modifying the quantization table and quantized discrete cosine transformation (DCT) coefficients. Some elements of the quantization table are divided by an integer while the corresponding quantized DCT coefficients are multiplied by the same integer and added by an adjustment value to make space for embedding the data. By analyzing the effect of each single quantized DCT coefficient on the image quality, an embedding sequence is chosen in order to help control the increase of file size after hiding the data meanwhile the PSNR value between the original uncompressed image and stego JPEG image is high. Experimental results show that the proposed method achieves both high capacity and high image quality. 
86|8|http://www.sciencedirect.com/science/journal/01641212/86/8|Special section on automation of software test|
86|8||An orchestrated survey of methodologies for automated software test case generation|Test case generation is among the most labour-intensive tasks in software testing. It also has a strong impact on the effectiveness and efficiency of software testing. For these reasons, it has been one of the most active research topics in software testing for several decades, resulting in many different approaches and tools. This paper presents an orchestrated survey of the most prominent techniques for automatic generation of software test cases, reviewed in self-standing sections. The techniques presented include: (a) structural testing using symbolic execution, (b) model-based testing, (c) combinatorial testing, (d) random testing and its variant of adaptive random testing, and (e) search-based testing. Each section is contributed by world-renowned active researchers on the technique, and briefly covers the basic ideas underlying the method, the current state of the art, a discussion of the open research problems, and a perspective of the future development of the approach. As a whole, the paper aims at giving an introductory, up-to-date and (relatively) short overview of research in automatic test case generation, while ensuring a comprehensive and authoritative treatment. 
86|8||Improving logic-based testing|Logic-based testers design tests from logical expressions that appear in software artifacts such as source code, design models, and requirements specifications. This paper presents three improvements to logic-based test design. First, in the context of mutation testing, we present fault hierarchies for the six relational operators. Applying the ROR mutation operator causes each relational operator to generate seven mutants per clause. The fault hierarchies show that only three of these seven mutants are needed. Second, we show how to bring the power of the ROR operator to logic-based test criteria such as the widely used Multiple Condition-Decision Coverage (MCDC) test criterion. Third, we present theoretical results supported by empirical data that show that the more recent coverage criterion of minimal-MUMCUT can find significantly more faults than MCDC. The paper has three specific recommendations: (1) Change the way the ROR mutation operator is defined in existing and future mutation systems. (2) Augment logic-based test criteria to incorporate relational operator replacement from mutation. (3) Replace the use of MCDC with minimal-MUMCUT, both in practice and in standards documents like FAA-DO178B. 
86|8||On the integration of model-driven design and dynamic assertion-based verification for embedded software|
86|8||User acceptance of software as a service: Evidence from customers of China's leading e-commerce company, Alibaba|This paper proposes a model with which to analyze the user acceptance of Software as a Service (SaaS). To develop this model, empirical surveys were conducted through four rounds of questionnaires obtained from customers of China's leading e-commerce company, Alibaba. Firstly, based on the data from the first three rounds (1399 respondents), a SaaSQual of operationalizing perceived e-service quality of SaaS was developed, and its four dimensions (ease of use, security, reliability and responsiveness) were identified. Secondly, based on the data from the fourth round (1532 respondents), it was found that the level of three user perceptions (e-service quality, usefulness, and social influence) were predictive of the users’ behavioral intention to use SaaS, and their direct and indirect influences were tested. This study recommends engineering improvements to SaaS based upon a better understanding of the level of user acceptance of this service. 
86|8||Cooperative clustering for software modularization|
86|8||FORTUNAâA framework for the design and development of hardware-based secure systems|
86|8||SPAPE: A semantic-preserving amorphous procedure extraction method for near-miss clones|
86|8||Threshold visual secret sharing by random grids with improved contrast|
86|8||Relevance, benefits, and problems of software modelling and model driven techniquesâA survey in the Italian industry|ContextClaimed benefits of software modelling and model driven techniques are improvements in productivity, portability, maintainability and interoperability. However, little effort has been devoted at collecting evidence to evaluate their actual relevance, benefits and usage complications.GoalThe main goals of this paper are: (1) assess the diffusion and relevance of software modelling and MD techniques in the Italian industry, (2) understand the expected and achieved benefits, and (3) identify which problems limit/prevent their diffusion.MethodWe conducted an exploratory personal opinion survey with a sample of 155 Italian software professionals by means of a Web-based questionnaire on-line from February to April 2011.ResultsSoftware modelling and MD techniques are very relevant in the Italian industry. The adoption of simple modelling brings common benefits (better design support, documentation improvement, better maintenance, and higher software quality), while MD techniques make it easier to achieve: improved standardization, higher productivity, and platform independence. We identified problems, some hindering adoption (too much effort required and limited usefulness) others preventing it (lack of competencies and supporting tools).ConclusionsThe relevance represents an important objective motivation for researchers in this area. The relationship between techniques and attainable benefits represents an instrument for practitioners planning the adoption of such techniques. In addition the findings may provide hints for companies and universities. 
86|8||Supporting real-time multiple data items query in multi-RSU vehicular ad hoc networks (VANETs)|There has been increasing interest in the issue of multi-item queries in wireless broadcasting systems recently. Query starvation and bandwidth utilization have been identified as key issues for improved performance. In this paper, we examine this problem in the context of VANETs with multiple cooperating Road Side Units (RSUs). We characterize a query with two deadlines: Query Total Deadline (QTD) which is the actual deadline of a query and Query Local Deadline (QLD) which is the duration a query is valid for serving in an RSU. By considering these two deadlines together with vehicle speed, RSU range and inter-RSU distance, we formulate a Cooperative Query Serving (CQS) scheme which allows multiple RSUs to share residual bandwidth and effectively address the query starvation as well as the bandwidth utilization problems, hence maximizing the chance of serving multiple items queries. Extensive simulation results confirm that CQS outperforms other existing scheduling algorithms. 
86|8||A novel approach to collaborative testing in a crowdsourcing environment|
86|8||Clustering navigation sequences to create contexts for guiding code navigation|To guide programmer code navigation, previous approaches such as TeamTracks recommend pieces of code to visit by mining the associations between pieces of code in programmer interaction histories. However, these result in low recommendation accuracy. To create more accurate recommendations, we propose NavClus an approach that clusters navigation sequences from programmer interaction histories. NavClus automatically forms collections of code that are relevant to the tasks performed by programmers, and then retrieves the collections best matched to a programmer's current navigation path. This makes it possible to recommend the collections of code that are relevant to the programmer's given task. We compare NavClus’ recommendation accuracy with TeamTracks’ by simulating recommendations using 4397 interaction histories. The comparative experiment shows that the recommendation accuracy of NavClus is twice as high as that of TeamTracks. 
86|8||An improved VLC-based lossless data hiding scheme for JPEG images|
86|8||A robust data hiding algorithm for H.264/AVC video streams|
86|8||Performing and analyzing non-formal inspections of entity relationship diagram (ERD)|
86|8||A data mining approach to discovering reliable sequential patterns|Sequential pattern mining is a data mining method for obtaining frequent sequential patterns in a sequential database. Conventional sequence data mining methods could be divided into two categories: Apriori-like methods and pattern growth methods. In a sequential pattern, probability of time between two adjacent events could provide valuable information for decision-makers. As far as we know, there has been no methodology developed to extract this probability in the sequential pattern mining process. We extend the PrefixSpan algorithm and propose a new sequential pattern mining approach: P-PrefixSpan. Besides minimum support-count constraint, this approach imposes minimum time-probability constraint, so that fewer but more reliable patterns will be obtained. P-PrefixSpan is compared with PrefixSpan in terms of number of patterns obtained and execution efficiency. Our experimental results show that P-PrefixSpan is an efficient and scalable method for sequential pattern mining. 
86|8||Adaptive reversible data hiding based on block median preservation and modification of prediction errors|In this paper, two enhanced reversible data hiding methods are proposed; both of them are based on two novel reversible data hiding techniques. A latest predictor is adopted to achieve better data hiding capability for the first predicative reversible data hiding scheme, whereas another scheme utilizes a new approach by considering the nature of different images to classify the smoothness for each piece of image blocking regions such that more secret data can be hidden into the smooth regions rather than the non-smooth ones resulting in a better embedding capability. The experiments verify that these schemes outperform the original reversible data hiding algorithms and some state-of-the-art reversible data hiding schemes. 
86|9|http://www.sciencedirect.com/science/journal/01641212/86/9|The future of software engineering IN and FOR the cloud|
86|9||Cloud engineering is Search Based Software Engineering too|Many of the problems posed by the migration of computation to cloud platforms can be formulated and solved using techniques associated with Search Based Software Engineering (SBSE). Much of cloud software engineering involves problems of optimisation: performance, allocation, assignment and the dynamic balancing of resources to achieve pragmatic trade-offs between many competing technical and business objectives. SBSE is concerned with the application of computational search and optimisation to solve precisely these kinds of software engineering challenges. Interest in both cloud computing and SBSE has grown rapidly in the past five years, yet there has been little work on SBSE as a means of addressing cloud computing challenges. Like many computationally demanding activities, SBSE has the potential to benefit from the cloud; ‘SBSE in the cloud’. However, this paper focuses, instead, of the ways in which SBSE can benefit cloud computing. It thus develops the theme of ‘SBSE for the cloud’, formulating cloud computing challenges in ways that can be addressed using SBSE. 
86|9||A goal-oriented simulation approach for obtaining good private cloud-based system architectures|
86|9||Cloud computing security: The scientific challenge, and a survey of solutions|We briefly survey issues in cloud computing security. The fact that data are shared with the cloud service provider is identified as the core scientific problem that separates cloud computing security from other topics in computing security. We survey three current research directions, and evaluate them in terms of a running software-as-a-service example. 
86|9||An authentication model towards cloud federation in the enterprise|
86|9||A framework to support selection of cloud providers based on security and privacy requirements|Cloud computing is an evolving paradigm that is radically changing the way humans store, share and access their digital files. Despite the many benefits, such as the introduction of a rapid elastic resource pool, and on-demand service, the paradigm also creates challenges for both users and providers. In particular, there are issues related to security and privacy, such as unauthorised access, loss of privacy, data replication and regulatory violation that require adequate attention. Nevertheless, and despite the recent research interest in developing software engineering techniques to support systems based on the cloud, the literature fails to provide a systematic and structured approach that enables software engineers to identify security and privacy requirements and select a suitable cloud service provider based on such requirements. This paper presents a novel framework that fills this gap. Our framework incorporates a modelling language and it provides a structured process that supports elicitation of security and privacy requirements and the selection of a cloud provider based on the satisfiability of the service provider to the relevant security and privacy requirements. To illustrate our work, we present results from a real case study. 
86|9||A service-oriented framework for developing cross cloud migratable software|Whilst cloud computing has burst into the current scene as a technology that allows companies to access high computing rates at limited costs, cloud vendors have rushed to provide tools that allow developers to build software for their cloud platforms. The software developed with these tools is often tightly coupled to their services and restrictions. Consequently vendor lock in becomes a common problem which multiple cloud users have to tackle in order to exploit the full potential of cloud computing. A scenario where component-based applications are developed for being deployed across several clouds, and each component can independently be deployed in one cloud or another, remains fictitious due to the complexity and the cost of their development. This paper presents a cloud development framework for developing cloud agnostic applications that may be deployed indifferently across multiple cloud platforms. Information about cloud deployment and cloud integration is separated from the source code and managed by the framework. Interoperability between interdependent components deployed in different clouds is achieved by automatically generating services and service clients. This allows software developers to segment their applications into different modules that can easily be deployed and redistributed across heterogeneous cloud platforms. 
86|9||A common API for delivering services over multi-vendor cloud resources|The increasing pace of evolution in business computing services leads enterprises to outsource secondary operations that are not part of their core business. The cloud computing market has been growing over the past few years and, consequently, many cloud companies are now offering a rich set of features to their consumers. Unfortunately, those cloud players have created new services with different APIs, which imply that cloud-oriented applications might be instantiated in one single cloud provider. This scenario is not desirable to the IT industry because their applications will become provider-dependent. In this paper we present a platform that allows applications to interoperate with distinct cloud providers’ services using a normalized interface. The proposed approach provides a common API that minimizes the present deficit of cloud API standardization and provides secure and redundant services allocation. Moreover, services from different cloud providers can be combined and decorated with additional functionalities like, for instance, redundancy and ciphering on-the-fly. 
86|9||Cloud computing and its impact on mobile software development: Two roads diverged|Today, both desktop and mobile software systems are built to leverage resources available on the World Wide Web. However, in recent years desktop and mobile software systems have evolved in different directions. On desktop computers, the most popular application for accessing content and applications on the Web is the web browser. In mobile devices, in contrast, the majority of web content is consumed via custom-built native web apps. This divergence will not continue indefinitely. In the 2010's we will witness a major battle between two types of technologies: native web apps and Open Web applications that run in a web browser. This “Battle of the Decade” will determine the future of the software industry for years to come. 
86|9||Solidifying the foundations of the cloud for the next generation Software Engineering|
86|9||Industry's role in data and software curation in the cloud|
86|9||Teaching cloud computing: A software engineering perspective|This article discusses the teaching of cloud computing in a software engineering course. It suggests that all courses should have some material introducing students to cloud computing, that practical teaching should focus on Platform as a Service and that there is scope for a graduate course in cloud software engineering covering map-reduce, schema-free databases, service-oriented computing, security and compliance and design for resilience. 
86|9||A design rule language for aspect-oriented programming|Aspect-oriented programming is known as a technique for modularizing crosscutting concerns. However, constructs aimed to support crosscutting modularity might actually break class modularity. As a consequence, class developers face changeability, parallel development and comprehensibility problems, because they must be aware of aspects whenever they develop or maintain a class. At the same time, aspects are vulnerable to changes in classes, since there is no contract specifying the points of interaction amongst these elements. These problems can be mitigated by using adequate design rules between classes and aspects. We present a design rule specification language and explore its benefits since the initial phases of the development process, specially with the aim of supporting modular development of classes and aspects. We discuss how our language improves crosscutting modularity without breaking class modularity. We evaluate it using a real case study and compare it with other approaches. 
86|9||Multi-sprint planning and smooth replanning: An optimization model|Most agile methods divide a project into sprints (iterations), and include a sprint planning phase that is critical to ensure the project success. Several factors impact on the optimality of a sprint plan, which makes the planning problem difficult. In this paper we formalize the planning problem and propose an optimization model that, given the estimates made by the project team and a set of development constraints, produces a multi-sprint optimal plan that maximizes the business value perceived by users. To cope with the inherent flexibility and uncertainty of agile projects, our approach ensures that a baseline plan can be revised and re-optimized during project execution without disrupting it, which we call smooth replanning. The planning problem is converted into a generalized assignment problem, given a linear programming formulation, and solved using the IBM ILOG CPLEX Optimizer. Our model is validated on both real and synthetic projects. In particular, a case study on two real projects confirms the effectiveness of our approach; as to efficiency, for medium-sized problems an exact solution is found in a few minutes, while for large problems a heuristic solution that is less than 1% far from the exact one is returned in a few seconds. Finally, some smooth replanning tests investigate the trade-off between plan quality and stability. 
86|9||On evaluating commercial Cloud services: A systematic review|BackgroundCloud Computing is increasingly booming in industry with many competing providers and services. Accordingly, evaluation of commercial Cloud services is necessary. However, the existing evaluation studies are relatively chaotic. There exists tremendous confusion and gap between practices and theory about Cloud services evaluation.AimTo facilitate relieving the aforementioned chaos, this work aims to synthesize the existing evaluation implementations to outline the state-of-the-practice and also identify research opportunities in Cloud services evaluation.MethodBased on a conceptual evaluation model comprising six steps, the systematic literature review (SLR) method was employed to collect relevant evidence to investigate the Cloud services evaluation step by step.ResultsThis SLR identified 82 relevant evaluation studies. The overall data collected from these studies essentially depicts the current practical landscape of implementing Cloud services evaluation, and in turn can be reused to facilitate future evaluation work.ConclusionsEvaluation of commercial Cloud services has become a world-wide research topic. Some of the findings of this SLR identify several research gaps in the area of Cloud services evaluation (e.g., Elasticity and Security evaluation of commercial Cloud services could be a long-term challenge), while some other findings suggest the trend of applying commercial Cloud services (e.g., compared with PaaS, IaaS seems more suitable for customers and is particularly important in industry). This SLR study itself also confirms some previous experiences and records new evidence-based software engineering (EBSE) lessons. 
86|9||User Interface Transition Diagrams for customerâdeveloper communication improvement in software development projects|We formalize the definition and construction of the User Interface Transition Diagram (UITD) which is a modelling notation for the transitions between UI presentations and the necessary conditions to trigger these transitions. We show how the UITD is able to improve the communication between stakeholders in a software development project: Human–Computer Interaction specialists, Software Engineers and customers who have little or no training in specialized modelling notations. We compare the UITD with other existing similar modelling notations highlighting the features that are better expressed in the UITD. We also include a case study in order to show how the UITD can be helpful in different phases of a software development project. The understandability of the UITD was confirmed by means of a test where different types of potential users were involved. 
86|9||Countermeasure graphs for software security risk assessment: An action research|Software security risk analysis is an important part of improving software quality. In previous research we proposed countermeasure graphs (CGs), an approach to conduct risk analysis, combining the ideas of different risk analysis approaches. The approach was designed for reuse and easy evolvability to support agile software development.CGs have not been evaluated in industry practice in agile software development. In this research we evaluate the ability of CGs to support practitioners in identifying the most critical threats and countermeasures.The research method used is participatory action research where CGs were evaluated in a series of risk analyses on four different telecom products.With Peltier (used prior to the use of CGs at the company) the practitioners identified attacks with low to medium risk level. CGs allowed practitioners to identify more serious risks (in the first iteration 1 serious threat, 5 high risk threats, and 11 medium threats). The need for tool support was identified very early, tool support allowed the practitioners to play through scenarios of which countermeasures to implement, and supported reuse.The results indicate that CGs support practitioners in identifying high risk security threats, work well in an agile software development context, and are cost-effective. 
86|9||Image encryption based on the Jacobian elliptic maps|
86|9||Certified Information Access|
86|9||An object-oriented approach to language compositions for software language engineering|
87|-|http://www.sciencedirect.com/science/journal/01641212/87|On the relationships between QoS and software adaptability at the architectural level|
87|-||Assessing the reliability, validity and acceptance of a classification scheme of usability problems (CUP)|
87|-||A three-phase energy-saving strategy for cloud storage systems|
87|-||Modeling continuous integration practice differences in industry software development|Continuous integration is a software practice where developers integrate frequently, at least daily. While this is an ostensibly simple concept, it does leave ample room for interpretation: what is it the developers integrate with, what happens when they do, and what happens before they do? These are all open questions with regards to the details of how one implements the practice of continuous integration, and it is conceivable that not all such implementations in the industry are alike. In this paper we show through a literature review that there are differences in how the practice of continuous integration is interpreted and implemented from case to case. Based on these findings we propose a descriptive model for documenting and thereby better understanding implementations of the continuous integration practice and their differences. The application of the model to an industry software development project is then described in an illustrative case study. 
87|-||A distributed framework for demand-driven software vulnerability detection|
87|-||Surviving sensor node failures by MMU-less incremental checkpointing|
87|-||Performance optimization of deployed software-as-a-service applications|
87|-||Using SMCD to reduce inconsistencies in misuse case models: A subject-based empirical evaluation|
87|-||Evolving feature model configurations in software product lines|The increasing complexity and cost of software-intensive systems has led developers to seek ways of reusing software components across development projects. One approach to increasing software reusability is to develop a software product-line (SPL), which is a software architecture that can be reconfigured and reused across projects. Rather than developing software from scratch for a new project, a new configuration of the SPL is produced. It is hard, however, to find a configuration of an SPL that meets an arbitrary requirement set and does not violate any configuration constraints in the SPL.Existing research has focused on techniques that produce a configuration of an SPL in a single step. Budgetary constraints or other restrictions, however, may require multi-step configuration processes. For example, an aircraft manufacturer may want to produce a series of configurations of a plane over a span of years without exceeding a yearly budget to add features.This paper provides three contributions to the study of multi-step configuration for SPLs. First, we present a formal model of multi-step SPL configuration and map this model to constraint satisfaction problems (CSPs). Second, we show how solutions to these SPL configuration problems can be automatically derived with a constraint solver by mapping them to CSPs. Moreover, we show how feature model changes can be mapped to our approach in a multi-step scenario by using feature model drift. Third, we present empirical results demonstrating that our CSP-based reasoning technique can scale to SPL models with hundreds of features and multiple configuration steps. 
87|-||Corrigendum to âA novel approach to collaborative testing in a crowdsourcing environmentâ in the Journal of Systems and Software 86 (2013) 2143â2153|
88|-|http://www.sciencedirect.com/science/journal/01641212/88|Coherent clusters in source code|This paper presents the results of a large scale empirical study of coherent dependence clusters. All statements in a coherent dependence cluster depend upon the same set of statements and affect the same set of statements; a coherent cluster's statements have ‘coherent’ shared backward and forward dependence. We introduce an approximation to efficiently locate coherent clusters and show that it has a minimum precision of 97.76%. Our empirical study also finds that, despite their tight coherence constraints, coherent dependence clusters are in abundance: 23 of the 30 programs studied have coherent clusters that contain at least 10% of the whole program. Studying patterns of clustering in these programs reveals that most programs contain multiple substantial coherent clusters. A series of subsequent case studies uncover that all clusters of significant size map to a logical functionality and correspond to a program structure. For example, we show that for the program acct, the top five coherent clusters all map to specific, yet otherwise non-obvious, functionality. Cluster visualization also brings out subtle deficiencies in program structure and identifies potential refactoring candidates. A study of inter-cluster dependence is used to highlight how coherent clusters are connected to each other, revealing higher-level structures, which can be used in reverse engineering. Finally, studies are presented to illustrate how clusters are not correlated with program faults as they remain stable during most system evolution. 
88|-||Conceptual modeling of natural language functional requirements|Requirements analysts consider a conceptual model to be an important artifact created during the requirements analysis phase of a software development life cycle (SDLC). A conceptual, or domain model is a visual model of the requirements domain in focus. Owing to its visual nature, the model serves as a platform for the deliberation of requirements by stakeholders and enables requirements analysts to further refine the functional requirements. Conceptual models may evolve into class diagrams during the design and execution phases of the software project. Even a partially automated conceptual model can save significant time during the requirements phase, by quickening the process of graphical communication and visualization.This paper presents a system to create a conceptual model from functional specifications, written in natural language in an automated manner. Classes and relationships are automatically identified from the functional specifications. This identification is based on the analysis of the grammatical constructs of sentences, and on Object Oriented principles of design. Extended entity-relationship (EER) notations are incorporated into the class relationships. Optimizations are applied to the identified entities during a post-processing stage, and the final conceptual model is rendered.The use of typed dependencies, combined with rules to derive class relationships offers a granular approach to the extraction of the design elements in the model. The paper illustrates the model creation process using a standard case study, and concludes with an evaluation of the usefulness of this approach for the requirements analysis. The analysis is conducted against both standard published models and conceptual models created by humans, for various evaluation parameters. 
88|-||Resource failures risk assessment modelling in distributed environments|
88|-||Identifying organizational barriersâA case study of usability work when developing software in the automation industry|This study investigates connections between usability efforts and organizational factors. This is an important field of research which so far appears to be insufficiently studied and discussed. It illustrates problems when working with software engineering tasks and usability requirements. It deals with a large company that manufactures industrial robots with an advanced user interface, which wanted to introduce usability KPIs, to improve product quality. The situation in the company makes this difficult, due to a combination of organizational and behavioural factors that led to a “wicked problem” that caused conflicts, breakdowns and barriers. Addressing these problems requires a holistic view that places context in the foreground and technological solutions in the background. Developing the right product requires communication and collaboration between multiple stakeholders. The inclusion of end users, who fully understand their own work context, is vital. Achieving this is dependent on organizational change, and management commitment. One step to beginning this change process may be through studying ways to introduce user-centred design processes. 
88|-||Robust reversible watermarking scheme using Slantlet transform matrix|
88|-||Software architecture review by association|During the process of software design, software architects have their reasons to choose certain software components to address particular software requirements and constraints. However, existing software architecture review techniques often rely on the design reviewers’ knowledge and experience, and perhaps using some checklists, to identify design gaps and issues, without questioning the reasoning behind the decisions made by the architects. In this paper, we approach design reviews from a design reasoning perspective. We propose to use an association-based review procedure to identify design issues by first associating all the relevant design concerns, problems and solutions systematically; and then verifying if the causal relationships between these design elements are valid. Using this procedure, we discovered new design issues in all three industrial cases, despite their internal architecture reviews and one of the three systems being operational. With the newly found design issues, we derive eight general design reasoning failure scenarios. 
88|-||Adding semantic modules to improve goal-oriented analysis of data warehouses using I-star|
88|-||Generation and validation of traces between requirements and architecture based on formal trace semantics|The size and complexity of software systems make integration of the new/modified requirements to the software system costly and time consuming. The impact of requirements changes on other requirements, design elements and source code should be traced to determine parts of the software to be changed. Considerable research has been devoted to relating requirements and design artifacts with source code. Less attention has been paid to relating requirements (R) with architecture (A) by using well-defined semantics of traces. Traces between R&A might be manually assigned. This is time-consuming, and error prone. Traces might be incomplete and invalid. In this paper, we present an approach for automatic trace generation and validation of traces between requirements (R) and architecture (A). Requirements relations and architecture verification techniques are used. A trace metamodel is defined with commonly used trace types between R&A. We use the semantics of traces and requirements relations for generating and validating traces with a tool support. The tool provides the following: (1) generation and validation of traces by using requirements relations and/or verification of architecture, (2) generation and validation of requirements relations by using traces. The tool is based on model transformation in ATL and term-rewriting logic in Maude. 
88|-||A context awareness framework for cross-platform distributed applications|
88|-||Recovering test-to-code traceability using slicing and textual analysis|
88|-||Information centric services in Smart Cities|A “Smart City” is intended as an urban environment which, supported by pervasive ICT systems, is able to offer advanced and innovative services to citizens in order to improve the overall quality of their life. In this context, the present contribution formulates a pioneering proposal, by drawing an advanced information centric platform for supporting the typical ICT services of a Smart City. It can easily embrace all available and upcoming wireless technologies, while enforcing, at the same time, ubiquitous and secure applications in many domains, such as, e-government and public administration, intelligent transportation systems, public safety, social, health-care, educational, building and urban planning, environmental, and energy and water management applications. All the details of the proposed approach have been carefully described by means of pragmatical use-cases, such as the management of administrative procedures, the starting of a new business in a given country, the navigation assistance, the signaling of an urban accident aimed at improving the public safety, the reservation of a medical examination, the remote assistance of patients, and the management of waste in a city. This description makes evident the real effectiveness of the present proposal in future urban environments. 
88|-||Software product line scoping and requirements engineering in a small and medium-sized enterprise: An industrial case study|Software product line (SPL) engineering has been applied in several domains, especially in large-scale software development. Given the benefits experienced and reported, SPL engineering has increasingly garnered interest from small to medium-sized companies. It is possible to find a wide range of studies reporting on the challenges of running a SPL project in large companies. However, very little reports exist that consider the situation for small to medium-sized enterprises and these studies try develop universal truths for SPL without lessons learned from empirical evidence need to be contextualized. This study is a step towards bridging this gap in contextual evidence by characterizing the weaknesses discovered in the scoping (SC) and requirements (RE) disciplines of SPL. Moreover, in this study we conducted a case study in a small to medium sized enterprises (SMEs) to justify the use of agile methods when introducing the SPL SC and RE disciplines through the characterization of their bottlenecks. The results of the characterization indicated that ineffective communication and collaboration, long iteration cycles, and the absence of adaptability and flexibility can increase the effort and reduce motivation during project development. These issues can be mitigated by agile methods. 
88|-||An approach to testing commercial embedded systems|
88|-||Visualizing protected variations in evolving software designs|
88|-||Corrigendum to: âSPAPE: A semantic-preserving amorphous procedure extraction method for near-miss clonesâ: [J. Syst. Softw. 86 (2013) 2077â2093]|
89|-|http://www.sciencedirect.com/science/journal/01641212/89|Special issue on âTrustworthy Software Systems for the Digital Societyâ|
89|-||The social smart grid: Dealing with constrained energy resources through social coordination|
89|-||Workload-aware anomaly detection for Web applications|The failure of Web applications often affects a large population of customers, and leads to severe economic loss. Anomaly detection is essential for improving the reliability of Web applications. Current approaches model correlations among metrics, and detect anomalies when the correlations are broken. However, dynamic workloads cause the metric correlations to change over time. Moreover, modeling various metric correlations are difficult in complex Web applications. This paper addresses these problems and proposes an online anomaly detection approach for Web applications. We present an incremental clustering algorithm for training workload patterns online, and employ the local outlier factor (LOF) in the recognized workload pattern to detect anomalies. In addition, we locate the anomalous metrics with the Student's t-test method. We evaluated our approach on a testbed running the TPC-W industry-standard benchmark. The experimental results show that our approach is able to (1) capture workload fluctuations accurately, (2) detect typical faults effectively and (3) has advantages over two contemporary ones in accuracy. 
89|-||Software trustworthiness 2.0âA semantic web enabled global source code analysis approach|There has been an ongoing trend toward collaborative software development using open and shared source code published in large software repositories on the Internet. While traditional source code analysis techniques perform well in single project contexts, new types of source code analysis techniques are ermerging, which focus on global source code analysis challenges. In this article, we discuss how the Semantic Web, can become an enabling technology to provide a standardized, formal, and semantic rich representations for modeling and analyzing large global source code corpora. Furthermore, inference services and other services provided by Semantic Web technologies can be used to support a variety of core source code analysis techniques, such as semantic code search, call graph construction, and clone detection. In this paper, we introduce SeCold, the first publicly available online linked data source code dataset for software engineering researchers and practitioners. Along with its dataset, SeCold also provides some Semantic Web enabled core services to support the analysis of Internet-scale source code repositories. We illustrated through several examples how this linked data combined with Semantic Web technologies can be harvested for different source code analysis tasks to support software trustworthiness. For the case studies, we combine both our linked-data set and Semantic Web enabled source code analysis services with knowledge extracted from StackOverflow, a crowdsourcing website. These case studies, we demonstrate that our approach is not only capable of crawling, processing, and scaling to traditional types of structured data (e.g., source code), but also supports emerging non-structured data sources, such as crowdsourced information (e.g., StackOverflow.com) to support a global source code analysis context. 
89|-||Slice-based statistical fault localization|Recent techniques for fault localization statistically analyze coverage information of a set of test runs to measure the correlations between program entities and program failures. However, coverage information cannot identify those program entities whose execution affects the output and therefore weakens the aforementioned correlations. This paper proposes a slice-based statistical fault localization approach to address this problem. Our approach utilizes program slices of a set of test runs to capture the influence of a program entity's execution on the output, and uses statistical analysis to measure the suspiciousness of each program entity being faulty. In addition, this paper presents a new slicing approach called approximate dynamic backward slice to balance the size and accuracy of a slice, and applies this slice to our statistical approach. We use two standard benchmarks and three real-life UNIX utility programs as our subjects, and compare our approach with a sufficient number of fault localization techniques. The experimental results show that our approach can significantly improve the effectiveness of fault localization. 
89|-||An evaluation model for dependability of Internet-scale software on basis of Bayesian Networks and trustworthiness|Internet-scale software becomes more and more important as a mode to construct software systems when Internet is developing rapidly. Internet-scale software comprises a set of widely distributed software entities which are running in open, dynamic and uncontrollable Internet environment. There are several aspects impacting dependability of Internet-scale software, such as technical, organizational, decisional and human aspects. It is very important to evaluate dependability of Internet-scale software by integrating all the aspects and analyzing system architecture from the most foundational elements. However, it is lack of such an evaluation model. An evaluation model of dependability for Internet-scale software on the basis of Bayesian Networks is proposed in this paper. The structure of Internet-scale software is analyzed. An evaluating system of dependability for Internet-scale software is established. It includes static metrics, dynamic metrics, prior metrics and correction metrics. A process of trust attenuation based on assessment is proposed to integrate subjective trust factors and objective dependability factors which impact on system quality. In this paper, a Bayesian Network is build according to the structure analysis. A bottom-up method that use Bayesian reasoning to analyses and calculate entity dependability and integration dependability layer by layer is described. A unified dependability of the whole system is worked out and is corrected by objective data. The analysis of experiment in a real system proves that the model in this paper is capable of evaluating the dependability of Internet-scale software clearly and objectively. Moreover, it offers effective help to the design, development, deployment and assessment of Internet-scale software. 
89|-||GUI testing assisted by human knowledge: Random vs. functional|
89|-||A formal methodology for integral security design and verification of network protocols|In this work we propose a methodology for incorporating the verification of the security properties of network protocols as a fundamental component of their design. This methodology can be separated in two main parts: context and requirements analysis along with its informal verification; and formal representation of protocols and the corresponding procedural verification. Although the procedural verification phase does not require any specific tool or approach, automated tools for model checking and/or theorem proving offer a good trade-off between effort and results. In general, any security protocol design methodology should be an iterative process addressing in each step critical contexts of increasing complexity as result of the considered protocol goals and the underlying threats. The effort required for detecting flaws is proportional to the complexity of the critical context under evaluation, and thus our methodology avoids wasting valuable system resources by analyzing simple flaws in the first stages of the design process. In this work we provide a methodology in coherence with the step-by-step goals definition and threat analysis using informal and formal procedures, being our main concern to highlight the adequacy of such a methodology for promoting trust in the accordingly implemented communication protocols. Our proposal is illustrated by its application to three communication protocols: MANA III, WEP's Shared Key Authentication and CHAT-SRP. 
89|-||Demand-based schedulability analysis for real-time multi-core scheduling|In real-time systems, schedulability analysis has been widely studied to provide offline guarantees on temporal correctness, producing many analysis methods. The demand-based schedulability analysis method has a great potential for high schedulability performance and broad applicability. However, such a potential is not yet fully realized for real-time multi-core scheduling mainly due to (i) the difficulty of calculating the resource demand under dynamic priority scheduling algorithms that are favorable to multi-cores, and (ii) the lack of understanding how to combine the analysis framework with deadline-miss conditions specialized for those scheduling algorithms. Addressing those two issues, to the best of our knowledge, this paper presents the first demand-based schedulability analysis for dynamic job-priority scheduling algorithms: EDZL (Earliest Deadline first until Zero-Laxity) and LLF (Least Laxity First), which are known to be effective for real-time multi-core scheduling. To this end, we first derive demand bound functions that compute the maximum possible amount of resource demand of jobs of each task while the priority of each job can change dynamically under EDZL and LLF. Then, we develop demand-based schedulability analyses for EDZL and LLF, by incorporating those new demand bound functions into the existing demand-based analysis framework. Finally, we combine the framework with additional deadline-miss conditions specialized for those two laxity-based dynamic job-priority scheduling algorithms, yielding tighter schedulability analyses. Via simulations, we demonstrate that the proposed schedulability analyses outperform the existing schedulability analyses for EDZL and LLF. 
89|-||A reliability model for Service Component Architectures|
89|-||Sustainability of Open Source software communities beyond a fork: How and why has the LibreOffice project evolved?|Many organisations are dependent upon long-term sustainable software systems and associated communities. In this paper we consider long-term sustainability of Open Source software communities in Open Source software projects involving a fork. There is currently a lack of studies in the literature that address how specific Open Source software communities are affected by a fork. We report from a study aiming to investigate the developer community around the LibreOffice project, which is a fork from the OpenOffice.org project. In so doing, our analysis also covers the OpenOffice.org project and the related Apache OpenOffice project. The results strongly suggest a long-term sustainable LibreOffice community and that there are no signs of stagnation in the LibreOffice project 33 months after the fork. Our analysis provides details on developer communities for the LibreOffice and Apache OpenOffice projects and specifically concerning how they have evolved from the OpenOffice.org community with respect to project activity, developer commitment, and retention of committers over time. Further, we present results from an analysis of first hand experiences from contributors in the LibreOffice community. Findings from our analysis show that Open Source software communities can outlive Open Source software projects and that LibreOffice is perceived by its community as supportive, diversified, and independent. The study contributes new insights concerning challenges related to long-term sustainability of Open Source software communities. 
89|-||Reviewing the quality of awareness support in collaborative applications|
89|-||Process fragmentation, distribution and execution using an event-based interaction scheme|The combination of service oriented architectures and business processes creates an enactment environment in which processes can be deployed and executed automatically. From a managerial and technical point of view, the interpretation, control and execution of a process flow happen very often at one point in the organizational and IT structure. This creates an inflexible environment in which control over and visibility of cross-departmental processes cannot be distributed across these organizational entities. Although the process model may need to be designed as a whole (to have an end-to-end definition), the actual execution of the process may need to be distributed across all participating partners. There are several ways to achieve this distribution. In this paper, we look at an event-based process deployment and execution infrastructure in which a process model can be automatically partitioned and distributed over different enactment entities, provided some given distribution definition. We compare the performance and flexibility of the proposed technique with other approaches and discuss the potential advantages and drawbacks of the event-based distribution. 
89|-||A model view controller based Self-Adjusting Clustering Framework|
90|-|http://www.sciencedirect.com/science/journal/01641212/90|Special issue on Emerging Topics on Software Debugging|
90|-||HSFal: Effective fault localization using hybrid spectrum of full slices and execution slices|
90|-||A dynamic code coverage approach to maximize fault localization efficiency|Spectrum-based fault localization is amongst the most effective techniques for automatic fault localization. However, abstractions of program execution traces, one of the required inputs for this technique, require instrumentation of the software under test at a statement level of granularity in order to compute a list of potential faulty statements. This introduces a considerable overhead in the fault localization process, which can even become prohibitive in, e.g., resource constrained environments. To counter this problem, we propose a new approach, coined dynamic code coverage (DCC), aimed at reducing this instrumentation overhead. This technique, by means of using coarser instrumentation, starts by analyzing coverage traces for large components of the system under test. It then progressively increases the instrumentation detail for faulty components, until the statement level of detail is reached. To assess the validity of our proposed approach, an empirical evaluation was performed, injecting faults in six real-world software projects. The empirical evaluation demonstrates that the dynamic code coverage approach reduces the execution overhead that exists in spectrum-based fault localization, and even presents a more concise potential fault ranking to the user. We have observed execution time reductions of 27% on average and diagnostic report size reductions of 77% on average. 
90|-||An empirical study on the use of mutant traces for diagnosis of faults in deployed systems|Debugging deployed systems is an arduous and time consuming task. It is often difficult to generate traces from deployed systems due to the disturbance and overhead that trace collection may cause on a system in operation. Many organizations also do not keep historical traces of failures. On the other hand earlier techniques focusing on fault diagnosis in deployed systems require a collection of passing–failing traces, in-house reproduction of faults or a historical collection of failed traces. In this paper, we investigate an alternative solution. We investigate how artificial faults, generated using software mutation in test environment, can be used to diagnose actual faults in deployed software systems. The use of traces of artificial faults can provide relief when it is not feasible to collect different kinds of traces from deployed systems. Using artificial and actual faults we also investigate the similarity of function call traces of different faults in functions. To achieve our goal, we use decision trees to build a model of traces generated from mutants and test it on faulty traces generated from actual programs. The application of our approach to various real world programs shows that mutants can indeed be used to diagnose faulty functions in the original code with approximately 60–100% accuracy on reviewing 10% or less of the code; whereas, contemporary techniques using pass–fail traces show poor results in the context of software maintenance. Our results also show that different faults in closely related functions occur with similar function call traces. The use of mutation in fault diagnosis shows promising results but the experiments also show the challenges related to using mutants. 
90|-||Combining mutation and fault localization for automated program debugging|This paper proposes a strategy for automatically fixing faults in a program by combining the ideas of mutation and fault localization. Statements ranked in order of their likelihood of containing faults are mutated in the same order to produce potential fixes for the faulty program. The proposed strategy is evaluated using 8 mutant operators against 19 programs each with multiple faulty versions. Our results indicate that 20.70% of the faults are fixed using selected mutant operators, suggesting that the strategy holds merit for automatically fixing faults. The impact of fault localization on efficiency of the overall fault-fixing process is investigated by experimenting with two different techniques, Tarantula and Ochiai, the latter of which has been reported to be better at fault localization than Tarantula, and also proves to be better in the context of fault-fixing using our proposed strategy. Further experiments are also presented to evaluate stopping criteria with respect to the mutant examination process and reveal that a significant fraction of the (fixable) faults can be fixed by examining a small percentage of the program code. We also report on the relative fault-fixing capabilities of mutant operators used and present discussions on future work. 
90|-||Using SPIN for automated debugging of infinite executions of Java programs|This paper presents an approach for the automated debugging of reactive and concurrent Java programs, combining model checking and runtime monitoring. Runtime monitoring is used to transform the Java execution traces into the input for the model checker, the purpose of which is twofold. First, it checks these execution traces against properties written in linear temporal logic (LTL), which represent desirable or undesirable behaviors. Second, it produces several execution traces for a single Java program by generating test inputs and exploring different schedulings in multithreaded programs. As state explosion is the main drawback to model checking, we propose two abstraction approaches to reduce the memory requirements when storing Java states. We also present the formal framework to clarify which kinds of LTL safety and liveness formulas can be correctly analysed with each abstraction for both finite and infinite program executions. A major advantage of our approach comes from the model checker, which stores the trace of each failed execution, allowing the programmer to replay these executions to locate the bugs. Our current implementation, the tool TJT, uses Spin as the model checker and the Java Debug Interface (JDI) for runtime monitoring. TJT is presented as an Eclipse plug-in and it has been successfully applied to debug complex public Java programs. 
90|-||Distributed debugging for mobile networks|
90|-||Exploiting the potential of DTN for energy-efficient internetworking|
90|-||Evolutionary instance selection for text classification|Text classification is usually based on constructing a model through learning from training examples to automatically classify text documents. However, as the size of text document repositories grows rapidly, the storage requirement and computational cost of model learning become higher. Instance selection is one solution to solve these limitations whose aim is to reduce the data size by filtering out noisy data from a given training dataset. In this paper, we introduce a novel algorithm for these tasks, namely a biological-based genetic algorithm (BGA). BGA fits a “biological evolution” into the evolutionary process, where the most streamlined process also complies with the reasonable rules. In other words, after long-term evolution, organisms find the most efficient way to allocate resources and evolve. Consequently, we can closely simulate the natural evolution of an algorithm, such that the algorithm will be both efficient and effective. The experimental results based on the TechTC-100 and Reuters-21578 datasets show the outperformance of BGA over five state-of-the-art algorithms. In particular, using BGA to select text documents not only results in the largest dataset reduction rate, but also requires the least computational time. Moreover, BGA can make the k-NN and SVM classifiers provide similar or slightly better classification accuracy than GA. 
90|-||Uncertainty handling in goal-driven self-optimization â Limiting the negative effect on adaptation|Goal-driven self-optimization through feedback loops has shown effectiveness in reducing oscillating utilities due to a large number of uncertain factors in the runtime environments. However, such self-optimization is less satisfactory when there contains uncertainty in the predefined requirements goal models, such as imprecise contributions and unknown quality preferences, or during the switches of goal solutions, such as lack of understanding about the time for the adaptation actions to take effect. In this paper, we propose to handle such uncertainty in goal-driven self-optimization without interrupting the services. Taking the monitored quality values as the feedback, and the estimated earned value as the global indicator of self-optimization, our approach dynamically updates the quantitative contributions from alternative functionalities to quality requirements, tunes the preferences of relevant quality requirements, and determines a proper timing delay for the last adaptation action to take effect. After applying these runtime measures to limit the negative effect of the uncertainty in goal models and their suggested switches, an experimental study on a real-life online shopping system shows the improvements over goal-driven self-optimization approaches without uncertainty handling. 
90|-||Power-aware fixed priority scheduling for sporadic tasks in hard real-time systems|In this paper, we consider the generalized power model in which the focus is the dynamic power and the static power, and we study the problem of the canonical sporadic task scheduling based on the rate-monotonic (RM) scheme. Moreover, we combine with the dynamic voltage scaling (DVS) and dynamic power management (DPM). We present a static low power sporadic tasks scheduling algorithm (SSTLPSA), assuming that each task presents its worst-case work-load to the processor at every instance. In addition, a more energy efficient approach called a dynamic low power sporadic tasks scheduling algorithm (DSTLPSA) is proposed, based on reclaiming the dynamic slack and adjusting the speed of other tasks on-the-fly in order to reduce energy consumption while still meeting the deadlines. The experimental results show that the SSTLPSA algorithm consumes 26.55–38.67% less energy than that of the RM algorithm and the DSTLPSA algorithm reduces the energy consumption up to 18.38–30.51% over the existing DVS algorithm. 
90|-||SecureSMS: A secure SMS protocol for VAS and other applications|Nowadays, the SMS is a very popular communication channel for numerous value added services (VAS), business and commercial applications. Hence, the security of SMS is the most important aspect in such applications. Recently, the researchers have proposed approaches to provide end-to-end security for SMS during its transmission over the network. Thus, in this direction, many SMS-based frameworks and protocols like Marko's SMS framework, Songyang's SMS framework, Alfredo's SMS framework, SSMS protocol, and, Marko and Konstantin's protocol have been proposed but these frameworks/protocols do not justify themselves in terms of security analysis, communication and computation overheads, prevention from various threats and attacks, and the bandwidth utilization of these protocols. The two protocols SMSSec and PK-SIM have also been proposed to provide end-to-end security and seem to be little better in terms of security analysis as compared to the protocols/framework mentioned above. In this paper, we propose a new secure and optimal protocol called SecureSMS, which generates less communication and computation overheads. We also discuss the possible threats and attacks in the paper and provide the justified prevention against them. The proposed protocol is also better than the above two protocols in terms of the bandwidth utilization. On an average the SecureSMS protocol reduces 71% and 59% of the total bandwidth used in the authentication process as compared to the SMSSec and PK-SIM protocols respectively. Apart from this, the paper also proposes a scheme to store and implement the cryptographic algorithms onto the SIM card. The proposed scheme provides end-to-end SMS security with authentication (by the SecureSMS protocol), confidentiality (by encryption AES/Blowfish; preferred AES-CTR), integrity (SHA1/MD5; preferred SHA1) and non-repudiation (ECDSA/DSA; preferred ECDSA). 
90|-||A recommendation framework for remote sensing images by spatial relation analysis|
90|-||Radigost: Interoperable web-based multi-agent platform|Recent improvements of web development technologies, commonly referred to as HTML5, have resulted in an excellent framework for developing a fully-featured, purely web-based multi-agent platform. This paper presents an architecture of such a platform, named Radigost. Radigost agents and parts of the system itself are implemented in JavaScript and executed inside the client's web browser, while an additional set of Java-based components is deployed on an enterprise application server. Radigost is platform-independent, capable of running, without any prior installation or configuration steps, on a wide variety of software and hardware configurations, including personal computers, smartphones, tablets, and modern television sets. The system is standards-compliant and fully interoperable, in the sense that its agents can transparently interact with agents in existing, third-party multi-agent solutions. Finally, performance evaluation results show that the execution speed of Radigost is comparable to that of a non web-based implementation. 
90|-||Real-time data dissemination in mobile peer-to-peer networks|Mobile peer-to-peer networks have found many uses such as streaming of audio and video data. There are circumstances, such as emergency situations and disaster recovery, when real-time delivery is a fundamental requirement. The problem is challenging due to the limited network capacity, the variable transmission rates and the unpredictability with respect to the network conditions in the mobile peer-to-peer network.In this paper we address the problem of real-time data dissemination of multimedia streams in mobile peer-to-peer networks. Four routing algorithms are proposed based on a packet's deadline, priority or a combination of these metrics. They are simulated under different setups in a mobile peer-to-peer network with Bluetooth connectivity and nodes broadcasting audio and video streams using different priorities. We compare the performance of the algorithms using a number of metrics. Detailed experimental results are presented. Based on these results, propositions on the usage of the algorithms and the design of network requirements are presented. 
90|-||Accurate sub-swarms particle swarm optimization algorithm for service composition|Service composition (SC) generates various composite applications quickly by using a novel service interaction model. Before composing services together, the most important thing is to find optimal candidate service instances compliant with non-functional requirements. Particle swarm optimization (PSO) is known as an effective and efficient algorithm, which is widely used in this process. However, the premature convergence and diversity loss of PSO always results in suboptimal solutions. In this paper, we propose an accurate sub-swarms particle swarm optimization (ASPSO) algorithm by adopting parallel and serial niching techniques. The ASPSO algorithm locates optimal solutions by using sub-swarms searching grid cells in which the density of feasible solutions is high. Simulation results demonstrate that the proposed algorithm improves the accuracy of the standard PSO algorithm in searching the optimal solution of service selection problem. 
volume|issue|url|title|abstract
100|-|http://www.sciencedirect.com/science/journal/01641212/100|A controlled experiment to evaluate the understandability of KAOS and i* for modeling Teleo-Reactive systems|ContextTeleo-Reactive (TR) specifications allow engineers to define the behavior of reactive systems while taking into account goals and changes in the state of the environment.ObjectiveThis article evaluates two different Goal Oriented Requirements Engineering notations, i* and KAOS, to determine their understandability level for specifying TR systems.MethodA controlled experiment was performed by two groups of Bachelor students. Each group first analyzed a requirements model of a mobile robotic system, specified using one of the evaluated languages, and then they filled in a questionnaire to evaluate its understandability. Afterwards, each group proceeded similarly with the model of another system specified with the second language.ResultsThe statistical analysis of the data obtained by means of the experiment showed that the understandability of i* is higher than that of KAOS when modeling TR systems.ConclusionBoth languages are suitable for specifying TR systems although their notations should be specialized to maximize the understandability attribute. i* surpasses KAOS due to two main reasons: i* models represent dependencies between agents and goals or tasks; and notational differences between tasks and goals in i* are more evident than those between goals and requirements in KAOS. 
100|-||Iterated local search for microaggregation|Microaggregation is a disclosure control method used to protect microdata. We introduce a local search method and employ it in an iterated local search algorithm for the NP-hard minimum information loss microaggregation problem. Experimental results with benchmark data sets demonstrate that our algorithm consistently identifies better quality solutions than extant microaggregation methods. 
100|-||Web API growing pains: Loosely coupled yet strongly tied|Web APIs provide a systematic and extensible approach for application-to-application interaction. Developers using web APIs are forced to accompany the API providers in their software evolution tasks. In order to understand the distress caused by this imposition on web API client developers we perform a semi-structured interview with six such developers. We also investigate how major web API providers organize their API evolution, and we explore how this affects source code changes of their clients. Our exploratory qualitative study of the Twitter, Google Maps, Facebook and Netflix web APIs analyzes the state of web API evolution practices and provides insight into the impact of service evolution on client software. In order to complement the picture and also understand how web API providers deal with evolution, we investigate the server-side and client-side evolution of two open-source web APIs, namely VirtualBox and XBMC. Our study is complemented with a set of observations regarding best practices for web API evolution. 
100|-||Enhanced healthcare personnel rostering solution using mobile technologies|This paper presents a novel personnel rostering system for healthcare units, which incorporates mobile technologies to minimize time overheads and boost personnel satisfaction. This way, doctors nurses and administrative staff may provide solutions and suggestions to the process of shifts’ scheduling and rostering in a group based – social and organized manner, at any given time, using their smartphone or tablet. This system is designed and implemented according to wide research on requirements’ specification, carried out in Greek public hospitals and based on a study of healthcare units’ organization, at a practical and legal level. The personnel rostering system anticipates to facilitate the staff administration task, through real-time communication between hospital's personnel. It enables the formation of a micro-community with enhanced social communication tools, to provide dynamic management, recording and updating of changes that occur in scheduled duties, without mediators and delays. The proposed solution includes an intelligent mobile device application, designed for smartphones and tablets. It is provided to the personnel and enables them to participate in the process of scheduling duties and shifts. The XML based, back-end, supporting information system offers services that allow a smoother operation of the unit, minimize time overheads in case of arbitrary changes and maximize satisfaction of personnel. The overall operation of the units, that reclaim the features offered by this system, can be improved. Minimizing the time and other bureaucratic delays in personnel scheduling is a vital part of the way a healthcare facility is organized. Thus, facilitating this process, with any available technology, may prove to be cost effective and crucial. Systems that incorporate mobile applications are already widely accepted, and become increasingly important to the healthcare sector, as well. The mobile based, personnel shifts’ scheduling solution shown is an approach that already receives encouraging support and indicates that it assists in achieving remarkable results. 
100|-||Integrating usability work into a large inter-organisational agile development project: Tactics developed by usability designers|
100|-||MostoDEx: A tool to exchange RDF data using exchange samples|
100|-||Comprehensible software fault and effort prediction: A data mining approach|Software fault and effort prediction are important tasks to minimize costs of a software project. In software effort prediction the aim is to forecast the effort needed to complete a software project, whereas software fault prediction tries to identify fault-prone modules. In this research both tasks are considered, thereby using different data mining techniques. The predictive models not only need to be accurate but also comprehensible, demanding that the user can understand the motivation behind the model's prediction. Unfortunately, to obtain predictive performance, comprehensibility is often sacrificed and vice versa. To overcome this problem, we extract trees from well performing Random Forests (RFs) and Support Vector Machines for regression (SVRs) making use of a rule extraction algorithm ALPA. This method builds trees (using C4.5 and REPTree) that mimic the black-box model (RF, SVR) as closely as possible. The proposed methodology is applied to publicly available datasets, complemented with new datasets that we have put together based on the Android repository. Surprisingly, the trees extracted from the black-box models by ALPA are not only comprehensible and explain how the black-box model makes (most of) its predictions, but are also more accurate than the trees obtained by working directly on the data. 
100|-||Profiling and classifying the behavior of malicious codes|Malware is a major security threat confronting computer systems and networks and has increased in scale and impact from the early days of ICT. Traditional protection mechanisms are largely incapable of dealing with the diversity and volume of malware variants which is evident today. This paper examines the evolution of malware including the nature of its activity and variants, and the implication of this for computer security industry practices.As a first step to address this challenge, I propose a framework to extract features statically and dynamically from malware that reflect the behavior of its code such as the Windows Application Programming Interface (API) calls. Similarity based mining and machine learning methods have been employed to profile and classify malware behaviors. This method is based on the sequences of API sequence calls and frequency of appearance.Experimental analysis results using large datasets show that the proposed method is effective in identifying known malware variants, and also classifies malware with high accuracy and low false alarm rates. This encouraging result indicates that classification is a viable approach for similarity detection to help detect malware. This work advances the detection of zero-day malware and offers researchers another method for understanding impact. 
100|-||A benchmarking process to assess software requirements documentation for space applications|Poorly written requirements are a common source of software defects and, in application areas like space systems, the cost of malfunctioning software can be very high. This work proposes a benchmarking procedure for assessing the quality of software requirements that adopt the Packet Utilization Standard (PUS) defined by the European Cooperation for Space Standardization (ECSS) standards. The benchmark uses three checklists that aim at guaranteeing that the specifications comply with the PUS standard, consider faulty behaviour, and do not include errors typically found in this type of documents. The benchmark is defined for two services of the PUS standard: the telecommand verification and on board operating scheduling. A benchmark validation approach is also proposed in the paper. It uses the concept of fault injection to insert known errors in software requirements specification documents. The benchmark validation is performed through its application to three projects from different countries. Results show that our proposal provides a simple and effective way for identifying weaknesses and compare the degree of maturity of requirements documents. 
100|-||From source code identifiers to natural language terms|Program comprehension techniques often explore program identifiers, to infer knowledge about programs. The relevance of source code identifiers as one relevant source of information about programs is already established in the literature, as well as their direct impact on future comprehension tasks.Most programming languages enforce some constrains on identifiers strings (e.g., white spaces or commas are not allowed). Also, programmers often use word combinations and abbreviations, to devise strings that represent single, or multiple, domain concepts in order to increase programming linguistic efficiency (convey more semantics writing less). These strings do not always use explicit marks to distinguish the terms used (e.g., CamelCase or underscores), so techniques often referred as hard splitting are not enough.This paper introduces Lingua::IdSplitter a dictionary based algorithm for splitting and expanding strings that compose multi-term identifiers. It explores the use of general programming and abbreviations dictionaries, but also a custom dictionary automatically generated from software natural language content, prone to include application domain terms and specific abbreviations. This approach was applied to two software packages, written in C, achieving a f-measure of around 90% for correctly splitting and expanding identifiers. A comparison with current state-of-the-art approaches is also presented. 
100|-||A computer system architecture providing a user-friendly man machine interface for accessing assistive technology in cloud computing|Assistive Technology (AT) includes hardware peripherals, software applications and systems that enable a user with a disability to use a PC. Thus, when a disabled user needs to work in a particular environment (e.g., at work, at school, in a government office, etc.) he/she has to properly configure the used PC. However, often, the configuration of AT software interfaces is not trivial at all. This paper presents the software design, implementation, and evaluation of a computer system architecture providing a software user-friendly man machine interface for accessing AT software in cloud computing. The main objective of such an architecture is to provide a new type of software human–computer interaction for accessing AT services over the cloud. Thus, end users can interact with their personalized computer environments using any physical networked PC. The advantage of this approach is that users do not have to install and/or setup any additional software on physical PCs and they can access their own AT virtual environments from everywhere. In particular, the usability of prototype based on the Remote Desktop Protocol (RDP) is evaluated in both private and public cloud scenarios. 
100|-||Defining multi-tenancy: A systematic mapping study on the academic and the industrial perspective|Software as a service is frequently offered in a multi-tenant style, where customers of the application and their end-users share resources such as software and hardware among all users, without necessarily sharing data. It is surprising that, with such a popular paradigm, little agreement exists with regard to the definition, domain, and challenges of multi-tenancy. This absence is detrimental to the research community and the industry, as it hampers progress in the domain of multi-tenancy and enables organizations and academics to wield their own definitions to further their commercial or research agendas.In this article, a systematic mapping study on multi-tenancy is described in which 761 academic papers and 371 industrial blogs are analysed. Both the industrial and academic perspective are assessed, in order to get a complete overview. The definition and topic maps provide a comprehensive overview of the domain, while the research agenda, listing four important research topics, provides a roadmap for future research efforts. 
100|-||Extracting REST resource models from procedure-oriented service interfaces|During the past decade a number of procedure-oriented protocols and standards have emerged for making service-offering systems available on the Web. The WS-* stack of protocols is the most prevalent example. However, this procedure and message-oriented approach has not aligned with the true potential of the Web's own architectural principles, such as the uniform identification and manipulation of resources, caching, hypermedia, and layering. In this respect, Resource Oriented Architectures based on the REST architectural style, have been proposed as a possible alternative to the operation-based view of service offerings. To date, compiling a REST API for back-end procedure-oriented services is considered as a manual process that requires as input specialized models, such as, service requirements and behavioral models. In this paper, we propose a resource extraction method in which service descriptions are analyzed, using natural language processing techniques and graph transformations, in order to yield a collection of hierarchically organized elements forming REST resources that semantically correspond to the functionality offered by the service. The proposed approach has been applied as a proof of concept with positive results, for the extraction of resource models from a sizable number of procedure-oriented Web Service interfaces that have been obtained from an open service directory. 
100|-||An imperfect software debugging model considering log-logistic distribution fault content function|Numerous software reliability growth models based on the non-homogeneous Poisson process assume perfect debugging. Such models, including the Goel–Okumoto, delayed S-shaped, and inflection S-shaped models, have been successfully validated in software testing. However, complex and uncertain test factors, such as test resource, tester skill, or test tool, can seriously affect the testing process. When detected faults are removed, new faults can be introduced in practical testing. The process is referred to as imperfect debugging. Imperfect software debugging models proposed in the literature generally assume a constantly or monotonically decreasing fault introduction rate per fault. These models cannot adequately describe the fault introduction process in a practical test. In this study, we propose an imperfect software debugging model that considers a log-logistic distribution fault content function, which can capture the increasing and decreasing characteristics of the fault introduction rate per fault. We also use several historical fault data sets to validate the performance of the proposed model. The model can suitably fit historical fault data and accurately predict failure behavior. Confidence interval and sensitivity analyses are also conducted. 
100|-||Using SAN formalism to evaluate Follow-The-Sun project scenarios|
100|-||Dynamic cloud service selection using an adaptive learning mechanism in multi-cloud computing|Cloud service selection in a multi-cloud computing environment is receiving more and more attentions. There is an abundance of emerging cloud service resources that makes it hard for users to select the better services for their applications in a changing multi-cloud environment, especially for online real time applications. To assist users to efficiently select their preferred cloud services, a cloud service selection model adopting the cloud service brokers is given, and based on this model, a dynamic cloud service selection strategy named DCS is put forward. In the process of selecting services, each cloud service broker manages some clustered cloud services, and performs the DCS strategy whose core is an adaptive learning mechanism that comprises the incentive, forgetting and degenerate functions. The mechanism is devised to dynamically optimize the cloud service selection and to return the best service result to the user. Correspondingly, a set of dynamic cloud service selection algorithms are presented in this paper to implement our mechanism. The results of the simulation experiments show that our strategy has better overall performance and efficiency in acquiring high quality service solutions at a lower computing cost than existing relevant approaches. 
100|-||D-P2P-Sim+: A novel distributed framework for P2P protocols performance testing|
91|-|http://www.sciencedirect.com/science/journal/01641212/91|Variability in software architecture â State of the art|
91|-||An overview of Dynamic Software Product Line architectures and techniques: Observations from research and industry|Over the last two decades, software product lines have been used successfully in industry for building families of systems of related products, maximizing reuse, and exploiting their variable and configurable options. In a changing world, modern software demands more and more adaptive features, many of them performed dynamically, and the requirements on the software architecture to support adaptation capabilities of systems are increasing in importance. Today, many embedded system families and application domains such as ecosystems, service-based applications, and self-adaptive systems demand runtime capabilities for flexible adaptation, reconfiguration, and post-deployment activities. However, as traditional software product line architectures fail to provide mechanisms for runtime adaptation and behavior of products, there is a shift toward designing more dynamic software architectures and building more adaptable software able to handle autonomous decision-making, according to varying conditions. Recent development approaches such as Dynamic Software Product Lines (DSPLs) attempt to face the challenges of the dynamic conditions of such systems but the state of these solution architectures is still immature. In order to provide a more comprehensive treatment of DSPL models and their solution architectures, in this research work we provide an overview of the state of the art and current techniques that, partially, attempt to face the many challenges of runtime variability mechanisms in the context of Dynamic Software Product Lines. We also provide an integrated view of the challenges and solutions that are necessary to support runtime variability mechanisms in DSPL models and software architectures. 
91|-||Dynamic adaptation of service compositions with variability models|
91|-||Efficient customization of multi-tenant Software-as-a-Service applications with service lines|Application-level multi-tenancy is an architectural approach for Software-as-a-Service (SaaS) applications which enables high operational cost efficiency by sharing one application instance among multiple customer organizations (the so-called tenants). However, the focus on increased resource sharing typically results in a one-size-fits-all approach. In principle, the shared application instance satisfies only the requirements common to all tenants, without supporting potentially different and varying requirements of these tenants. As a consequence, multi-tenant SaaS applications are inherently limited in terms of flexibility and variability.This paper presents an integrated service engineering method, called service line engineering, that supports co-existing tenant-specific configurations and that facilitates the development and management of customizable, multi-tenant SaaS applications, without compromising scalability. Specifically, the method spans the design, implementation, configuration, composition, operations and maintenance of a SaaS application that bundles all variations that are based on a common core.We validate this work by illustrating the benefits of our method in the development of a real-world SaaS offering for document processing. We explicitly show that the effort to configure and compose an application variant for each individual tenant is significantly reduced, though at the expense of a higher initial development effort. 
91|-||Delta-oriented model-based integration testing of large-scale systems|
91|-||End-user development by application-domain configuration|
91|-||Improved anti-forensics of JPEG compression|
91|-||A methodology to automatically optimize dynamic memory managers applying grammatical evolution|
91|-||Cooperation, collaboration and pair-programming: Field studies on backup behavior|Considering that pair programming has been extensively studied for more than a decade, it can seem quite surprising that there is such a lack of consensus on both its best use and its benefits. We argue that pair programming is not a replacement of usual developer interactions, but rather a formalization and enhancement of naturally occurring interactions. Consequently, we study and classify a broader range of developer interactions, evaluating them for type, purpose and patterns of occurrence, with the aim to identify situations in which pair programming is likely to be truly needed and thus most beneficial. We study the concrete pair programming practices in both academic and industrial settings. All interactions between teammates were recorded as backup behavior activities. In each of these two projects, developers were free to interact when needed. All team interactions were self-recorded by the teammates. The analysis of the interaction tokens shows two salient features: solo work is an important component of teamwork and team interactions have two main purposes, namely cooperation and collaboration. Cooperative backup behavior occurs when a developer provides help to a teammate. Collaborative backup behavior occurs when the teammates are sharing the same goal toward solving an issue. We found that collaborative backup behavior, which occurred much less often, is close to the formal definition of pair programming. This study suggests that mandatory pair programming may be less efficient in organizations where solo work could be done and when some interactions are for cooperative activities. Based on these results, we discussed the potential implications concerning the best use of pair programming in practice, a more effective evaluation of its use, its potential benefits and emerging directions of future research. 
91|-||Existence of dumb nodes in stationary wireless sensor networks|Wireless sensor networks (WSNs), which are typically autonomous and unattended, require energy-efficient and fault-tolerant protocols to maximize the network lifetime and operations. In this work, we consider a previously unexplored aspect of the sensing nodes – dumb behavior. A sensor node is termed as “dumb”, when it can sense its surroundings, but cannot communicate with its neighbors due to shrinkage in communication range attributed to adverse environmental effects and can behave normally in the presence of favorable environment. As a result of this temporary behavior, a node may get isolated from the network when adverse environmental effects are present, but re-connects with the network with the resumption of favorable environmental conditions. We consider the effects of dumb nodes on the, otherwise, energy-efficient stationary WSNs having complete network coverage achieved using sufficient number of activated sensor nodes. While the presence of redundancy in the deployment of nodes, or the number of active nodes can guarantee communication opportunities, such deployment is not necessarily energy-efficient and cost-effective. The dumb behavior of nodes results in wastage of power, thereby reducing the lifetime of a network. Such effects can be detrimental to the performance of WSN applications. The simulation results exhibit that the network performance degrades in the presence of dumb nodes in stationary WSNs. 
91|-||Predictable integration and reuse of executable real-time components|We present the concept of runnable virtual node (RVN) as a means to achieve predictable integration and reuse of executable real-time components in embedded systems. A runnable virtual node is a coarse-grained software component that provides functional and temporal isolation with respect to its environment. Its interaction with the environment is bounded both by a functional and a temporal interface, and the validity of its internal temporal behaviour is preserved when integrated with other components or when reused in a new environment. Our realization of RVN exploits the latest techniques for hierarchical scheduling to achieve temporal isolation, and the principles from component-based software-engineering to achieve functional isolation. It uses a two-level deployment process, i.e. deploying functional entities to RVNs and then deploying RVNs to physical nodes, and thus also gives development benefits with respect to composability, system integration, testing, and validation. In addition, we have implemented a server-based inter-RVN communication strategy to not only support the predictable integration and reuse properties of RVNs by keeping the communication code in a separate server, but also increasing the maintainability and flexibility to change the communication code without affecting the timing properties of RVNs. We have applied our approach to a case study, implemented in the ProCom component technology executing on top of a FreeRTOS-based hierarchical scheduling framework and present the results as a proof-of-concept. 
91|-||An efficient design and validation technique for secure handover between 3GPP LTE and WLANs systems|Future generations wireless systems, which integrate different wireless access networks together, will support a secured seamless mobility and a wide variety of applications and services with different quality of service (QoS) requirements. Most of the existing re-authentication protocols during vertical handover still have certain limitations such as man in the middle, eavesdropping and session hijacking attacks, and unacceptable delay for real time applications. In this article, we propose two re-authentication schemes to secure handover between 3GPP LTE and WLANs systems: Initial Handover Re-authentication Protocol, and Local Re-authentication Protocol. The second proposed protocol is executed locally in a WLAN network without contacting the authentication server of the home network for credentials verification. In fact, after a successful execution of the Initial Handover Re-authentication Protocol, the local key (LK) is shared between USIM and the authentication server of the WLAN. It is then used for securing handover and traffic in WLAN networks. Performance evaluation results obtained using simulation analysis show that the proposed re-authentication protocol enhances handover parameters such as handover latency, handover blocking rate and packet loss rate. Additionally, the proposed enhanced fast re-authentication protocol has been modeled and verified using the software AVISPA and is found to be safe. 
91|-||Web application testing: A systematic literature review|ContextThe web has had a significant impact on all aspects of our society. As our society relies more and more on the web, the dependability of web applications has become increasingly important. To make these applications more dependable, for the past decade researchers have proposed various techniques for testing web-based software applications. Our literature search for related studies retrieved 193 papers in the area of web application testing, which have appeared between 2000 and 2013.ObjectiveAs this research area matures and the number of related papers increases, it is important to systematically identify, analyze, and classify the publications and provide an overview of the trends and empirical evidence in this specialized field.MethodsWe systematically review the body of knowledge related to functional testing of web application through a systematic literature review (SLR) study. This SLR is a follow-up and complimentary study to a recent systematic mapping (SM) study that we conducted in this area. As part of this study, we pose three sets of research questions, define selection and exclusion criteria, and synthesize the empirical evidence in this area.ResultsOur pool of studies includes a set of 95 papers (from the 193 retrieved papers) published in the area of web application testing between 2000 and 2013. The data extracted during our SLR study is available through a publicly-accessible online repository. Among our results are the followings: (1) the list of test tools in this area and their capabilities, (2) the types of test models and fault models proposed in this domain, (3) the way the empirical studies in this area have been designed and reported, and (4) the state of empirical evidence and industrial relevance.ConclusionWe discuss the emerging trends in web application testing, and discuss the implications for researchers and practitioners in this area. The results of our SLR can help researchers to obtain an overview of existing web application testing approaches, fault models, tools, metrics and empirical evidence, and subsequently identify areas in the field that require more attention from the research community. 
92|-|http://www.sciencedirect.com/science/journal/01641212/92|Introduction to the special issue on middleware for mobile data management|
92|-||Programming mobile context-aware applications with TOTAM|In tuple space approaches to context-aware mobile systems, the notion of context is defined by the presence or absence of certain tuples in the tuple space. Existing approaches define such presence either by collocation of devices holding the tuples or by replication of tuples across all devices. We show that both approaches can lead to an erroneous perception of context. Collocation ties the perception of context to network connectivity which does not always yield the expected result. Tuple replication can cause that a certain context is perceived even if the device has left the context a long time ago. We propose a tuple space approach in which tuples themselves carry a predicate that determines whether they are in the right context or not. We present a practical API for our approach and show its use by means of the implementation of various mobile applications. Benchmarks show that our approach can lead to a significant increase in performance compared to other approaches. 
92|-||Mosco: a privacy-aware middleware for mobile social computing|The proliferation of mobile devices coupled with Internet access is generating a tremendous amount of highly personal and sensitive data. Applications such as location-based services and quantified self harness such data to bring meaningful context to users’ behavior. As social applications are becoming prevalent, there is a trend for users to share their mobile data. The nature of online social networking poses new challenges for controlling access to private data, as compared to traditional enterprise systems. First, the user may have a large number of friends, each associated with a unique access policy. Second, the access control policies must be dynamic and fine-grained, i.e. they are content-based, as opposed to all-or-nothing. In this paper, we investigate the challenges in sharing of mobile data in social applications. We design and evaluate a middleware running on Google App Engine, named Mosco, that manages and facilitates sharing of mobile data in a privacy-preserving manner. We use Mosco to develop a location sharing and a health monitoring application. Mosco helps shorten the development process. Finally, we perform benchmarking experiments with Mosco, the results of which indicate small overhead and high scalability. 
92|-||SelfMotion: A declarative approach for adaptive service-oriented mobile applications|Modern society increasingly relies on mobile devices. This explains the growing demand for high quality software for such devices. To improve the efficiency of the development life-cycle, shortening time-to-market while keeping quality under control, mobile applications are typically developed by composing together ad-hoc developed components, services available on-line, and other third-party mobile applications. Applications are thus built as heterogeneous compositions, whose characteristics strongly depend on the components and services they integrate. To cope with unpredictable changes and failures, but also with the various settings offered by the plethora of available devices, mobile applications need to be as adaptive as possible. However, mainstream adaptation strategies are usually defined imperatively and require complex control strategies strongly intertwined with the application logic, yielding to applications that are difficult to build, maintain, and evolve. We address this issue by proposing a declarative approach to compose adaptive heterogeneous mobile applications. The advantages of this approach are demonstrated through an example inspired by an existing worldwide distributed mobile application, while the implementation of the proposed solution has been validated through a set of simulations and experiments aimed at illustrating its performance. 
92|-||Top-k query processing for replicated data in mobile peer to peer networks|In mobile ad hoc peer to peer (M-P2P) networks, since nodes are highly resource constrained, it is effective to retrieve data items using a top-k query, in which data items are ordered by the score of a particular attribute and the query-issuing node acquires data items with the k highest scores. However, when network partitioning occurs, the query-issuing node cannot connect to some nodes having data items included in the top-k query result, and thus, the accuracy of the query result decreases. To solve this problem, data replication is a promising approach. However, if each node sends back its own data items (replicas) responding to a query without considering replicas held by others, same data items are sent back to the query-issuing node more than once through long paths, which results in increase of traffic. In this paper, we propose a top-k query processing method considering data replication in M-P2P networks. This method suppresses duplicate transmissions of same data items through long paths. Moreover, an intermediate node stops transmitting a query message on-demand. 
92|-||Programmable context awareness framework|Context-awareness enables applications to provide end-users with a richer experience by enhancing their interactions with contextual information. Several frameworks have already been proposed to simplify the development of context-aware applications. These frameworks are focused on provisioning context data and on providing common semantics, definitions and representations of these context data. They assume that applications share the same semantic, which limits the range of use cases where a framework can be used, as that assumption induces a strong coupling between context management and application logic. This article proposes a framework that decouples context management from application business logic. The aim is to reduce the overhead on applications that run on resource-limited devices while still providing mechanisms to support context-awareness and behavior adaptation. The article presents an innovative approach that involves third-parties in context processing definition by structuring it using atomic functions. These functions can be designed by third-party developers using an XML-based programming language. Its implementation and evaluation demonstrates the benefits, in terms of flexibility, of using proven design patterns from software engineering for developing context-aware application. 
92|-||Towards an ideal service QoS in fuzzy logic-based adaptation planning middleware|Mobile applications require an adaptation phase to adapt to the user's and application context. Utility functions or rules are most often used to make the adaptation planning or decision, i.e. select the most adapted variant for each required service. Fuzzy controllers are used when it is difficult or even impossible to construct precise mathematical models. In the case of mobile applications, the large number of Quality of Service (QoS) and context parameters causes an exponential increase in the number of rules (aka. rule explosion problem), that increases the processing time of the adaptation planning. To reduce the processing time and simplify the fuzzy control system, we propose the concept of ideal QoS. Fuzzy values of ideal QoS parameters are calculated using several fuzzy control systems to fit the context state and user preferences. A fuzzy logic similarity metric based on fuzzy sets and fuzzy operators is proposed to select the service variant having the nearest QoS values to the ideal. Experiments show that our approach can significantly improve both the number of rules and the processing time when selecting the variant that well adapts to environment changes. 
92|-||Mobile Cloud Middleware|Mobile Cloud Computing (MCC) is arising as a prominent research area that is seeking to bring the massive advantages of the cloud to the constrained smartphones. Mobile devices are looking towards cloud-aware techniques, driven by their growing interest to provide ubiquitous PC-like functionality to mobile users. These functionalities mainly target at increasing storage and computational capabilities. Smartphones may integrate those functionalities from different cloud levels, in a service oriented manner within the mobile applications, so that a mobile task can be delegated by direct invocation of a service. However, developing these kind of mobile cloud applications requires to integrate and consider multiple aspects of the clouds, such as resource-intensive processing, programmatically provisioning of resources (Web APIs) and cloud intercommunication. To overcome these issues, we have developed a Mobile Cloud Middleware (MCM) framework, which addresses the issues of interoperability across multiple clouds, asynchronous delegation of mobile tasks and dynamic allocation of cloud infrastructure. MCM also fosters the integration and orchestration of mobile tasks delegated with minimal data transfer. A prototype of MCM is developed and several applications are demonstrated in different domains. To verify the scalability of MCM, load tests are also performed on the hybrid cloud resources. The detailed performance analysis of the middleware framework shows that MCM improves the quality of service for mobiles and helps in maintaining soft-real time responses for mobile cloud applications. 
92|-||A cross-layer middleware for context-aware cooperative application on mobile ad hoc peer-to-peer network|Mobile ad hoc peer-to-peer (P2P) applications become popular for providing the file sharing, voice communicating, and video streaming services due to entertainments and disaster recovery. However, both the topology of wireless network and the overlay of P2P network are dynamic, so the middleware is proposed to integrate such architectures of service-oriented applications. Therefore, we propose context-aware cooperative application (CACA) to overcome the frequent churn and high mobility problems. CACA proposes a cross-layer middleware to integrate DHT-based lookup, anycast query, and P2P delivery via the IPv6 routing header. Through anycast query, the response delay can be shortened and the query duplication can be minimized. Via IPv6 routing header, the delivery efficiency can be improved. Through the cross-layer design, the finger table in overlay layer is combined with the routing table in network layer to heighten proximity. The simulation results demonstrate that CACA has the outstanding performances of short download delay, high playback continuity, and low signaling overhead in mobile ad hoc network. 
92|-||A secure Boolean-based multi-secret image sharing scheme|An (n, n) multi-secret image sharing scheme shares n secret images among n shared images. In this type of schemes, n shared images can be used to recover all n secret images, but the loss of any shared image prevents the recovery of any secret image. Among existing image sharing techniques, Boolean-based secret schemes have good performance because they only require XOR calculation. This study presents a secure Boolean-based secret image sharing scheme that uses a random image generating function to generate a random image from secret images or shared images. The proposed function efficiently increases the sharing capacity on free of sharing the random image. The use of a bit shift subfunction in the random image generating function produces a random image to meet the random requirement. Experimental results show that the proposed scheme requires minimal CPU computation time to share or recover secret images. The time required to share n secret images is nearly the time as that required to recover n secret images. The bit shift subfunction takes more computation load than the XOR subfunction needs. 
92|-||Factors that motivate software engineering teams: A four country empirical study|Motivation, although difficult to quantify, is considered to be the single largest factor in developer productivity; there are also suggestions that low motivation is an important factor in software development project failure. We investigate factors that motivate software engineering teams using survey data collected from software engineering practitioners based in Australia, Chile, USA and Vietnam. We also investigate the relationship between team motivation and project outcome, identifying whether the country in which software engineering practitioners are based affects this relationship. Analysis of 333 questionnaires indicates that failed projects are associated with low team motivation. We found a set of six common team motivational factors that appear to be culturally independent (project manager has good communication with project staff, project risks reassessed, controlled and managed during the project, customer has confidence in the project manager and the development team, the working environment is good, the team works well together, and the software engineer had a pleasant experience). We also found unique groupings of team motivational factors for each of the countries investigated. This indicates that there are cultural differences that project managers need to consider when working in a global environment. 
92|-||Architecture for embedded open software ecosystems|Software is prevalent in embedded products and may be critical for the success of the products, but manufacturers may view software as a necessary evil rather than as a key strategic opportunity and business differentiator. One of the reasons for this can be extensive supplier and subcontractor relationships and the cost, effort or unpredictability of the deliverables from the subcontractors are experienced as a major problem.The paper proposes open software ecosystem as an alternative approach to develop software for embedded systems, and elaborates on the necessary quality attributes of an embedded platform underlying such an ecosystem. The paper then defines a reference architecture consisting of 17 key decisions together with four architectural patterns, and provides the rationale why they are essential for an open software ecosystem platform for embedded systems in general and automotive systems in particular.The reference architecture is validated through a prototypical platform implementation in an industrial setting, providing a deeper understanding of how the architecture could be realised in the automotive domain.Four potential existing platforms, all targeted at the embedded domain (Android, OKL4, AUTOSAR and Robocop), are evaluated against the identified quality attributes to see how they could serve as a basis for an open software ecosystem platform with the conclusion that while none of them is a perfect fit they all have fundamental mechanisms necessary for an open software ecosystem approach. 
92|-||A MIH-based approach for best network selection in heterogeneous wireless networks|In the next generation wireless networks, different technologies belonging to one or more operators should be integrated to form a heterogeneous environment based on an IP core network infrastructure. This ensures user mobility and service continuity by maintaining connections when switching between various technologies and it introduces new resources and possibilities for applications. In this context, an automatic interface selection based on instantaneous and practical constraints and user preferences (Quality of Service (QoS) parameters, available resources, security, power consumption, etc.) is therefore required. The different network selection and handover schemes proposed in the literature can be classified into three approaches according to who is responsible for making the handover decision: the terminal, the network or by a cooperation between both of them. However, these approaches keep presenting some drawbacks; namely the problem of resources management and network load balancing whenever the selection is controlled by the mobile terminal (MT) and the problem of scalability and unknown operator's management policy whenever the selection is rather controlled by the network.In this article, first we propose a MIH based approach for handover initiation and preparation for heterogeneous wireless network. The proposed framework is based on the principals of IEEE 802.21 for context information gathering and optimized handover decision making. Second, we propose a new architecture and new network selection scheme that explicitly take into account the current resource usage and the user preferences. Furthermore, our solution ensures the selection of the most suitable network for each flow while taking into consideration its expectations in terms of QoS. A feasibility study of implementing a new architecture on a single MT is evaluated by using typical scenarios and using various algorithms. Thanks to the introduced function entities and modules in the proposed architecture, network utilization balancing and user and application expectations, which are successfully assured without operator intervention. Performance analysis shows that the proposed algorithm best meets the common quality requirements. 
92|-||Failure factors of small software projects at a global outsourcing marketplace|The presented study aims at a better understanding of when and why small-scale software projects at a global outsourcing marketplace fail. The analysis is based on a data set of 785,325 projects/tasks completed at vWorker.com. A binary logistic regression model relying solely on information known at the time of a project's start-up correctly predicted 74% of the project failures and 67% of the non-failures. The model-predicted failure probability corresponded well with the actual frequencies of failures for most levels of failure risk. The model suggests that the factors connected to the strongest reduction in the risk of failure are related to previous collaboration between the client and the provider and a low failure rate of previous projects completed by the provider. We found the characteristics of the client to be almost as important as those of the provider in explaining project failures and that the risk of project failure increased with an increased client emphasis on low price and with an increased project size. The identified relationships seem to be reasonable stable across the studied project size categories. 
92|-||Privacy-preserving computation of participatory noise maps in the cloud|This paper presents a privacy-preserving system for participatory sensing, which relies on cryptographic techniques and distributed computations in the cloud. Each individual user is represented by a personal software agent, deployed in the cloud, where it collaborates on distributed computations without loss of privacy, including with respect to the cloud service providers. We present a generic system architecture involving a cryptographic protocol based on a homomorphic encryption scheme for aggregating sensing data into maps, and demonstrate security in the Honest-But-Curious model both for the users and the cloud service providers. We validate our system in the context of NoiseTube, a participatory sensing framework for noise pollution, presenting experiments with real and artificially generated data sets, and a demo on a heterogeneous set of commercial cloud providers. To the best of our knowledge our system is the first operational privacy-preserving system for participatory sensing. While our validation pertains to the noise domain, the approach used is applicable in any crowd-sourcing application relying on location-based contributions of citizens where maps are produced by aggregating data – also beyond the domain of environmental monitoring. 
93|-|http://www.sciencedirect.com/science/journal/01641212/93|On the verification of UML/OCL class diagrams using constraint programming|Assessment of the correctness of software models is a key issue to ensure the quality of the final application. To this end, this paper presents an automatic method for the verification of UML class diagrams extended with OCL constraints. Our method checks compliance of the diagram with respect to several correctness properties including weak and strong satisfiability or absence of constraint redundancies among others. The method works by translating the UML/OCL model into a Constraint Satisfaction Problem (CSP) that is evaluated using state-of-the-art constraint solvers to determine the correctness of the initial model. Our approach is particularly relevant to current MDA and MDD methods where software models are the primary artifacts of the development process and the basis for the (semi-)automatic code-generation of the final application. 
93|-||Predicting software defects with causality tests|
93|-||From AADL to Timed Abstract State Machines: A verified model transformation|Architecture Analysis and Design Language (AADL) is an architecture description language standard for embedded real-time systems widely used in the avionics and aerospace industry to model safety-critical applications. To verify and analyze the AADL models, model transformation technologies are often used to automatically extract a formal specification suitable for analysis and verification. In this process, it remains a challenge to prove that the model transformation preserves the semantics of the initial AADL model or, at least, some of the specific properties or requirements it needs to satisfy. This paper presents a machine checked semantics-preserving transformation of a subset of AADL (including periodic threads, data port communications, mode changes, and the AADL behavior annex) into Timed Abstract State Machines (TASM). The AADL standard itself lacks at present a formal semantics to make this translation validation possible. Our contribution is to bridge this gap by providing two formal semantics for the subset of AADL. The execution semantics provided by the AADL standard is formalized as Timed Transition Systems (TTS). This formalization gives a reference expression of AADL semantics which can be compared with the TASM-based translation (for verification purpose). Finally, the verified transformation is mechanized in the theorem prover Coq. 
93|-||Efficient distributed skyline computation using dependency-based data partitioning|
93|-||Assessment of institutions, scholars, and contributions on agile software development (2001â2012)|The number of scholarly publications on agile software development has grown significantly in recent years. Several researchers reviewed and attempted to synthesize studies on agile software development. However, no work has ranked the contributions of scholars and institutions to publications using a thorough process. This study presents findings on top publications, institutions, and scholars in the agile software development field from 2001 to 2012 based on the publication of such works in Science Citation Index journals. This paper highlights the key outlets for agile research and summarizes the most influential researchers and institutions as well as the most studied research areas. This study concludes by providing directions for future research. 
93|-||Scalable network file systems with load balancing and fault tolerance for web services|Because of the rapid growth of the World Wide Web and the popularization of smart phones, tablets and personal computers, the number of web service users is increasing rapidly. As a result, large web services require additional disk space, and the required disk space increases with the number of web service users. Therefore, it is important to design and implement a powerful network file system for large web service providers. In this paper, we present three design issues for scalable network file systems. We use a variable number of objects within a bucket to decrease internal fragmentation in small files. We also propose a free space and access load-balancing mechanism to balance overall loading on the bucket servers. Finally, we propose a mechanism for caching frequently accessed data to lower the total disk I/O. These proposed mechanisms can effectively improve scalable network file system performance for large web services. 
93|-||A clustering-based model for class responsibility assignment problem in object-oriented analysis|Assigning responsibilities to classes is a vital task in object-oriented analysis and design, and it directly affects the maintainability and reusability of software systems. There are many methodologies to help recognize the responsibilities of a system and assign them to classes, but all of them depend greatly on human judgment and decision-making. In this paper, we propose a clustering-based model to solve the class responsibility assignment (CRA) problem. The proposed model employs a novel interactive graph-based method to find inheritance hierarchies, and two novel criteria to determine the appropriate number of classes. It reduces the dependency of CRA on human judgment and provides a decision-making support for CRA in class diagrams. To evaluate the proposed model, we apply three different hierarchical agglomerative clustering algorithms and two different types of similarity measures. By comparing the obtained results of clustering techniques with the models designed by multi-objective genetic algorithm (MOGA), it is revealed that clustering techniques yield promising results. 
93|-||Performance models and dynamic characteristics analysis for HDFS write and read operations: A systematic view|Hadoop has emerged as a successful framework for large-scale data-intensive computing applications. However, there is no research on performance models for the Hadoop Distributed File System (HDFS). Due to the complexity of HDFS and the difficulty of modeling the multiple impact factors for HDFS performance, to establish HDFS performance models based directly on these impact factors is very complicated. In this paper, the relationship between file size and HDFS Write/Read (denoted as W/R for short) throughput, i.e., the average flow rate of a HDFS W/R operation, is studied to build HDFS performance models from a systematic view. Based on the measured data of specially designed experiments (in which HDFS W/R operations can be viewed as single-input single-output systems), a system identification-based approach is applied to construct performance models for HDFS W/R operations under different conditions. Furthermore, dynamic characteristics metrics for HDFS performance are defined, and based on the identified performance models and these metrics, the dynamic characteristics of HDFS W/R operations, such as steady state and overshoot, are studied, and the relationships between impact factors and dynamic characteristics are analyzed. These analysis results can provide effective guidance and implications for the design and configuration of HDFS and Hadoop-based applications. 
93|-||A high capacity data hiding scheme for binary images based on block patterns|This paper proposes a high capacity data hiding scheme for binary images based on block patterns, which can facilitate the authentication and annotation of scanned images. The scheme proposes block patterns for a 2 × 2 block to enforce specific block-based relationship in order to embed a significant amount of data without causing noticeable artifacts. In addition, two kinds of matching pair (MP) methods, internal adjustment MP and external adjustment MP, are designed to decrease the embedding changes. Shuffling is applied before embedding to reduce the distortion and improve the security. Experimental results show that the proposed scheme gives a significantly improved embedding capacity than previous approaches in the same level of embedding distortion. We also analyze the perceptual impact and discuss the robustness and security issues. 
93|-||Flexible resource monitoring of Java programs|Monitoring resource consumptions is fundamental in software engineering, e.g., in validation of quality requirements, performance engineering, or adaptive software systems. However, resource monitoring does not come for free as it typically leads to overhead in the observed program. Minimizing this overhead and increasing the reliability of the monitored data is a major goal in realizing resource monitoring tools. Typically, this is achieved by limiting capabilities, e.g., supported resources, granularity of the monitoring focus, or runtime access to results. Thus, in practice often several approaches must be combined to obtain relevant information.We describe SPASS-meter, a novel resource monitoring approach for Java and Android Apps, which combines these conflicting capabilities with low overhead. SPASS-meter supports a large set of resources, flexible configuration of the monitoring scope even for user-defined semantic units (components), runtime analysis and online access to monitoring results in a platform-independent way. We discuss the concepts of SPASS-meter, its architecture, realization and validation, the latter in terms of case studies and an overhead analysis based on performance experiments with SPASS-meter, OpenCore and Kieker. SPASS-meter provides a detailed view of the runtime resource consumption at reasonable overhead of less than 3% processing power and 0.5% memory consumption in our experiments. 
93|-||CodeCloud: A platform to enable execution of programming models on the Clouds|This paper presents a platform that supports the execution of scientific applications covering different programming models (such as Master/Slave, Parallel/MPI, MapReduce and Workflows) on Cloud infrastructures. The platform includes (i) a high-level declarative language to express the requirements of the applications featuring software customization at runtime, (ii) an approach based on virtual containers to encapsulate the logic of the different programming models, (iii) an infrastructure manager to interact with different IaaS backends, (iv) a configuration software to dynamically configure the provisioned resources and (v) a catalog and repository of virtual machine images. By using this platform, an application developer can adapt, deploy and execute parallel applications agnostic to the Cloud backend. 
93|-||A review on E-business Interoperability Frameworks|Interoperability frameworks present a set of assumptions, concepts, values, and practices that constitute a method of dealing with interoperability issues in the electronic business (e-business) context. Achieving interoperability in the e-business generates numerous benefits. Thus, interoperability frameworks are the main component of e-business activities. This paper describes the existing interoperability frameworks for e-business, and performs a comparative analysis among their findings to determine the similarities and differences in their philosophy and implementation. This analysis yields a set of recommendations for any party that is open to the idea of creating or improving an E-business Interoperability Framework. 
93|-||A trustworthy QoS-based collaborative filtering approach for web service discovery|Many network services which process a large quantity of data and knowledge are available in the distributed network environment, and provide applications to users based on Service-Oriented Architecture (SOA) and Web services technology. Therefore, a useful web service discovery approach for data and knowledge discovery process in the complex network environment is a very significant issue. Using the traditional keyword-based search method, users find it difficult to choose the best web services from those with similar functionalities. In addition, in an untrustworthy real world environment, the QoS-based service discovery approach cannot verify the correctness of the web services’ Quality of Service (QoS) values, since such values guaranteed by a service provider are different from the real ones. This work proposes a trustworthy two-phase web service discovery mechanism based on QoS and collaborative filtering, which discovers and recommends the needed web services effectively for users in the distributed environment, and also solves the problem of services with incorrect QoS information. In the experiment, the theoretical analysis and simulation experiment results show that the proposed method can accurately recommend the needed services to users, and improve the recommendation quality. 
94|-|http://www.sciencedirect.com/science/journal/01641212/94|Social cyber systemsâChallenges, opportunities, and beyond|
94|-||Peer impressions in open source organizations: A survey|
94|-||Twitter data analysis by means of Strong Flipping Generalized Itemsets|
94|-||Efficient unveiling of multi-members in a social network|With the rapid growth of the Web 2.0, the discovery of key actors in social networks, called influencers, mediators, ambassadors or experts, has recently received a renewed of attention. In this article, we consider a particular type of actor that we call a multi-member since he belongs to several communities. We introduce a methodological framework to identify these actors in a hypergraph, in which the vertices are the actors and the hyperedges are the communities. We also show that detecting such multi-members is similar to the problem of the determination of a subset of minimal transversals of a hypergraph. An efficient algorithm that relies on the connection between the definition of a multi-member and that of an essential itemset is also introduced. Experiments done on several datasets showed that the introduced algorithm outperforms the pioneering ones of the literature. 
94|-||DYSCS: A platform to build geographically and semantically enhanced social content sites|
94|-||Extended U+F Social Network Protocol: Interoperability, reusability, data protection and indirect relationships in Web Based Social Networks|An interconnected world is what current technologies look for, being Web Based Social Networks (WBSNs) a promising development in this regard. Four desirable WBSN features are identified, namely, interoperability, reusability, protection against WBSNs providers and indirect relationships. A protocol, called U+F, addressed interoperability and reusability of identity data, resources and access control policies between different WBSNs. In order to address the remaining couple of features, that is, achieving the protection of data against WBSNs providers and indirect relationships management across different WBSNs, this paper presents eU+F, an extension of U+F. A prototype is developed to verify the feasibility of implementing the proposed protocol in a real environment, as well as to compare its workload regarding three well-known WBSNs, Facebook, MySpace and LinkedIn. 
94|-||Automatic multi-partite graph generation from arbitrary data|In this paper we present a generic model for automatic generation of basic multi-partite graphs obtained from collections of arbitrary input data following user indications. The paper also presents GraphGen, a tool that implements this model. The input data is a collection of complex objects composed by a set or list of heterogeneous elements. Our tool provides a simple interface for the user to specify the types of nodes that are relevant for the application domain in each case. The nodes and the relationships between them are derived from the input data through the application of a set of derivation rules specified by the user. The resulting graph can be exported in the standard GraphML format so that it can be further processed with other graph management and mining systems. We end by giving some examples in real scenarios that show the usefulness of this model. 
94|-||A critical examination of recent industrial surveys on agile method usage|ContextPractitioners and researchers often claim that agile methods have moved into the mainstream for the last few years. To support this claim they refer to recent industrial surveys which tend to report high rates of agile method usage. However many of these industrial surveys are conducted by agile consultants, tool vendors, professional societies and independent technology and market research organizations. This raises some important concerns about the possible conflict of interest and the overall trustworthiness of these studies.ObjectiveIn response to the above concerns, a secondary study was carried out. Its objective was to examine industrial surveys published in 2011 and 2012, determine the extent to which we could trust their reported high rates of agile method usage and provide recommendations on how quality of research could be improved in the future.MethodFollowing a rigorous search procedure, nine industrial surveys on agile method usage published in 2011 and 2012 were extracted from both academia and industry. Their thoroughness in reporting and trustworthiness were evaluated using a newly proposed assessment framework based on Guba's four attributes of trustworthiness (truth value, applicability, consistency and neutrality) and a number of methods for assessing survey research in related fields as information, communication and management studies.ResultsThe careful examination of the reviewed surveys shows that most of the studies have insufficient thoroughness in reporting and (subsequently) low trustworthiness. Only one (out of nine) study is considered as a scientific contribution in determining the current 2011/2012 rate of agile method usage.ConclusionsThe obtained results support our initial considerations about the trustworthiness of recent industrial surveys on agile method usage and suggest a number of recommendations for increasing the quality and value of future survey research in this regard. 
94|-||A weight-aware channel assignment algorithm for mobile multicast in wireless mesh networks|
94|-||A component- and connector-based approach for end-user composite web applications development|
94|-||Avoiding, finding and fixing spreadsheet errors â A survey of automated approaches for spreadsheet QA|Spreadsheet programs can be found everywhere in organizations and they are used for a variety of purposes, including financial calculations, planning, data aggregation and decision making tasks. A number of research surveys have however shown that such programs are particularly prone to errors. Some reasons for the error-proneness of spreadsheets are that spreadsheets are developed by end users and that standard software quality assurance processes are mostly not applied. Correspondingly, during the last two decades, researchers have proposed a number of techniques and automated tools aimed at supporting the end user in the development of error-free spreadsheets. In this paper, we provide a review of the research literature and develop a classification of automated spreadsheet quality assurance (QA) approaches, which range from spreadsheet visualization, static analysis and quality reports, over testing and support to model-based spreadsheet development. Based on this review, we outline possible opportunities for future work in the area of automated spreadsheet QA. 
94|-||Handling slowly changing dimensions in data warehouses|
94|-||A systematic review of software architecture visualization techniques|ContextGiven the increased interest in using visualization techniques (VTs) to help communicate and understand software architecture (SA) of large scale complex systems, several VTs and tools have been reported to represent architectural elements (such as architecture design, architectural patterns, and architectural design decisions). However, there is no attempt to systematically review and classify the VTs and associated tools reported for SA, and how they have been assessed and applied.ObjectiveThis work aimed at systematically reviewing the literature on software architecture visualization to develop a classification of VTs in SA, analyze the level of reported evidence and the use of different VTs for representing SA in different application domains, and identify the gaps for future research in the area.MethodWe used systematic literature review (SLR) method of the evidence-based software engineering (EBSE) for reviewing the literature on VTs for SA. We used both manual and automatic search strategies for searching the relevant papers published between 1 February 1999 and 1 July 2011.ResultsWe selected 53 papers from the initially retrieved 23,056 articles for data extraction, analysis, and synthesis based on pre-defined inclusion and exclusion criteria. The results from the data analysis enabled us to classify the identified VTs into four types based on the usage popularity: graph-based, notation-based, matrix-based, and metaphor-based VTs. The VTs in SA are mostly used for architecture recovery and architectural evolution activities. We have also identified ten purposes of using VTs in SA. Our results also revealed that VTs in SA have been applied to a wide range of application domains, among which “graphics software” and “distributed system” have received the most attention.ConclusionSA visualization has gained significant importance in understanding and evolving software-intensive systems. However, only a few VTs have been employed in industrial practice. This review has enabled us to identify the following areas for further research and improvement: (i) it is necessary to perform more research on applying visualization techniques in architectural analysis, architectural synthesis, architectural implementation, and architecture reuse activities; (ii) it is essential to pay more attention to use more objective evaluation methods (e.g., controlled experiment) for providing more convincing evidence to support the promised benefits of using VTs in SA; (iii) it is important to conduct industrial surveys for investigating how software architecture practitioners actually employ VTs in architecting process and what are the issues that hinder and prevent them from adopting VTs in SA. 
94|-||Architectural reliability analysis of framework-intensive applications: A web service case study|
94|-||Corrigendum to âPower-aware scheduling algorithms for sporadic tasks in real-time systemsâ [J. Syst. Softw. 86 (2013) 2611â2619]|
95|-|http://www.sciencedirect.com/science/journal/01641212/95|Empirical research methodologies and studies in Requirements Engineering: How far did we come?|Since the inception of the RE conference series (1992), both researchers and practitioners in the RE community have acknowledged the significance of empirical evaluation as an instrument to gain knowledge about various aspects of RE phenomena and the validity of our research results. A significant number of empirical studies have been conducted in the search for knowledge about RE problems as well as evidence of successful and less successful application of proposed solutions. This editorial presents the progress empirical RE research has made since 1992. Based on a search in the Scopus digital library, we report from an analysis of peer-reviewed systematic literature reviews and mapping studies to showcase major areas of RE research that use methods from the Empirical Software Engineering paradigm. We summarize prior empirical research in RE and introduce the contributors to this special issue on empirical research methodologies and studies in RE. 
95|-||Software product management â An industry evaluation|Product management is a key success factor for software products as it spans the entire life-cycle and thus ensures both a technical and business perspective. With its many interfaces to various business processes and stakeholders across the life-cycle, it is a primary driver for requirements engineering in its focus on value-orientation and consistency across releases. This article provides an overview on product management in software and IT. It summarizes experiences with introducing, improving and deploying the role of a product manager. In order to get a profound industry overview we performed a field study with interviews and concrete insight across fifteen different organizations world-wide on the role of the product manager and its success factors. As a technical solution we present four success factors identified from the research and show how they address the challenges we identified in practice. The novel part of this research and technical study is the industry survey and evaluation of resulting solution proposals. We found that with increasing institutionalization of a consistent and empowered product management role, the success rate of projects in terms of schedule predictability, quality and project duration improves. 
95|-||Empirical research methods for technology validation: Scaling up to practice|Before technology is transferred to the market, it must be validated empirically by simulating future practical use of the technology. Technology prototypes are first investigated in simplified contexts, and these simulations are scaled up to conditions of practice step by step as more becomes known about the technology. This paper discusses empirical research methods for scaling up new requirements engineering (RE) technology.When scaling up to practice, researchers want to generalize from validation studies to future practice. An analysis of scaling up technology in drug research reveals two ways to generalize, namely inductive generalization using statistical inference from samples, and analogic generalization using similarity between cases. Both are supported by abductive inference using mechanistic explanations of phenomena observed in the simulations. Illustrations of these inferences both in drug research and empirical RE research are given. Next, four kinds of methods for empirical RE technology validation are given, namely expert opinion, single-case mechanism experiments, technical action research and statistical difference-making experiments. A series of examples from empirical RE will illustrate the use of these methods, and the role of inductive generalization, analogic generalization, and abductive inference in them. Finally, the four kinds of empirical validation methods are compared with lists of validation methods known from empirical software engineering. The lists are combined to give an overview of some of the methods, instruments and data analysis techniques that may be used in empirical RE. 
95|-||Using a grounded theory approach for exploring software product management challenges|
95|-||Stakeholder logistics of an interactive system|Although it seems that software metrics have moved beyond mere performance measurement, it is not too clear how machine effectiveness, efficiency, and effort pertain to human requirements on such matters. In industry as well as academia, the ISO 9241-11 norm provides the dominant view on usability, stating that usability is a function of effectiveness, efficiency, and satisfaction. Although intuitively, usability requirements should be part of a software's design in an early stage, conceptually and empirically, it seems more likely that performance requirements (i.e., the absence of errors) should be the center of concern. This paper offers an elaborated view on usability, satisfaction, and performance. Certain theoretical conceptions are tested with data gathered from professional users of banking and hospital systems by means of a 4-year single-item survey and a structured questionnaire, respectively. Results suggested that performance factors (i.e., efficiency) are more important than usability in understanding why stakeholders are satisfied with a system or not. Moreover, it neither is dissatisfaction with a system nor that a system is less usable that predicate requirements change. Instead, avoiding machine inaccuracy best predicted the variability in agreement to “must have” requirements, while achieving human accuracy predicted the variability in agreement to the “won’t have” requirements. The present contribution provides a consistent research framework that can bring more focus to design (i.e., prioritization), clarify discussions about design trade-offs, makes concepts measurable, and eventually may lead to better-informed designs. 
95|-||Assessing a requirements evolution approach: Empirical studies in the air traffic management domain|
95|-||Measure-independent characterization of contrast optimal visual cryptography schemes|Visual cryptography has been studied in two models and visual cryptography schemes have been evaluated using different contrast measures. Naor and Shamir introduced the deterministic model while Kafri and Keren introduced the random grid model. In the deterministic model, three different measures of contrast have been proposed, Î³ns, Î³vv and Î³es, although only Î³ns, has been thoroughly studied. Tight bounds on Î³ns are known for several classes of schemes. In the random grid model the contrast is Î³rg.In this paper we focus the attention on the deterministic model and follow a measure-independent approach, which, by using the structural properties of the schemes, enables us to provide a characterization of optimal schemes that is independent of the specific measure used to assess the contrast. In particular we characterize and provide constructions of optimal schemes for the cases of (2, n)-threshold and (n, n)-threshold schemes. Then, we apply the measure-independent results to the three measures Î³ns, Î³vv and Î³es, that have been used in the literature obtaining both new characterizations and constructions of optimal schemes as well as alternative proofs of known results.Finally we provide a connection between the deterministic and the random grid models showing that Î³es is the equivalent of Î³rg. This opens up a door between the two models which have been so far treated separately. 
95|-||Supporting SIP-based end-to-end Data Distribution Service QoS in WANs|
95|-||Waste identification as the means for improving communication in globally distributed agile software development|Agile approaches highly values communication between team members to improve software development processes, even though, communication in globally distributed agile teams can be difficult. Literature proposes solutions for mitigating the challenges encountered in these environments. These solutions range from general-level recommendations and practices to the use of communication tools. However, an approach covering the whole development process for identifying challenges, and improving communication in globally distributed agile development projects, is missing. In order to address this, we conducted a case study within a globally distributed agile software development project focused on using the concept of waste as a lens for identifying non-value producing communication elements. In order to achieve this, we constructed a waste identification approach through which we identified five communication wastes, and solutions to mitigate them. These wastes can help companies identify communication issues that are present in their development efforts, while the presented waste identification technique gives them a mechanism for waste identification and mitigation. This work contributes to the scientific community by increasing the knowledge about communication in globally distributed agile development efforts. 
95|-||Guilt-based handling of software performance antipatterns in palladio architectural models|
95|-||Surfing the optimization space of a multiple-GPU parallel implementation of a X-ray tomography reconstruction algorithm|
95|-||On the use of software design models in software development practice: An empirical investigation|Research into software design models in general, and into the UML in particular, focuses on answering the question how design models are used, completely ignoring the question if they are used. There is an assumption in the literature that the UML is the de facto standard, and that use of design models has had a profound and substantial effect on how software is designed by virtue of models giving the ability to do model-checking, code generation, or automated test generation. However for this assumption to be true, there has to be significant use of design models in practice by developers.This paper presents the results of a survey summarizing the answers of 3785 developers answering the simple question on the extent to which design models are used before coding. We relate their use of models with (i) total years of programming experience, (ii) open or closed development, (iii) educational level, (iv) programming language used, and (v) development type.The answer to our question was that design models are not used very extensively in industry, and where they are used, the use is informal and without tool support, and the notation is often not UML. The use of models decreased with an increase in experience and increased with higher level of qualification. Overall we found that models are used primarily as a communication and collaboration mechanism where there is a need to solve problems and/or get a joint understanding of the overall design in a group. We also conclude that models are seldom updated after initially created and are usually drawn on a whiteboard or on paper. 
95|-||Imperceptible visible watermarking based on postcamera histogram operation|
95|-||Face recognition based on curvelets and local binary pattern features via using local property preservation|In this paper, we propose a new feature extraction approach for face recognition based on Curvelet transform and local binary pattern operator. The motivation of this approach is based on two observations. One is that Curvelet transform is a new anisotropic multi-resolution analysis tool, which can effectively represent image edge discontinuities; the other is that local binary pattern operator is one of the best current texture descriptors for face images. As the curvelet features in different frequency bands represent different information of the original image, we extract such features using different methods for different frequency bands. Technically, the lowest frequency band component is processed using the local binary pattern method, and only the medium frequency band components are normalized. And then, we combine them to create a feature set, and use the local preservation projection to reduce its dimension. Finally, we classify the test samples using the nearest neighbor classifier in the reduced space. Extensive experiments on the Yale database, the extended Yale B database, the PIE pose 09 database, and the FRGC database illustrate the effectiveness of the proposed method. 
95|-||Xen2MX: High-performance communication in virtualized environments|Cloud computing infrastructures provide vast processing power and host a diverse set of computing workloads, ranging from service-oriented deployments to high-performance computing (HPC) applications. As HPC applications scale to a large number of VMs, providing near-native network I/O performance to each peer VM is an important challenge. In this paper we present Xen2MX, a paravirtual interconnection framework over generic Ethernet, binary compatible with Myrinet/MX and wire compatible with MXoE. Xen2MX combines the zero-copy characteristics of Open-MX with Xen's memory sharing techniques. Experimental evaluation of our prototype implementation shows that Xen2MX is able to achieve nearly the same raw performance as Open-MX running in a non-virtualized environment. On the latency front, Xen2MX performs as close as 96% to the case where virtualization layers are not present. Regarding throughput, Xen2MX saturates a 10 Gbps link, achieving 1159 MB/s, compared to 1192 MB/s of the non-virtualized case. Scales efficiently with the number of VMs, saturating the link for even smaller messages when 40 single-core VMs put pressure on the network adapters. 
95|-||Distributed collaborative filtering with singular ratings for large scale recommendation|
95|-||Scalable news recommendation using multi-dimensional similarity and JaccardâKmeans clustering|
96|-|http://www.sciencedirect.com/science/journal/01641212/96|GPU accelerated pivoting rules for the simplex algorithm|Simplex type algorithms perform successive pivoting operations (or iterations) in order to reach the optimal solution. The choice of the pivot element at each iteration is one of the most critical step in simplex type algorithms. The flexibility of the entering and leaving variable selection allows to develop various pivoting rules. In this paper, we have proposed some of the most well-known pivoting rules for the revised simplex algorithm on a CPU–GPU computing environment. All pivoting rules have been implemented in MATLAB and CUDA. Computational results on randomly generated optimal dense linear programs and on a set of benchmark problems (Netlib-optimal, Kennington, Netlib-infeasible, Mészáros) are also presented. These results showed that the proposed GPU implementations of the pivoting rules outperform the corresponding CPU implementations. 
96|-||Recommending software upgrades with Mojave|
96|-||An evolutionary approach to identify logical components|
96|-||Change impact analysis and changeability assessment for a change proposal: An empirical study ââ|Software change is a fundamental ingredient of software maintenance and evolution. Effectively supporting software modification is essential to provide a reliable high-quality evolution of software systems, as even a slight change may cause some unpredictable and undesirable effects on other parts of the software. To address this issue, this work used change impact analysis (CIA) to guide software modification. CIA can be used to help make correct decision on the change proposal, that is changeability assessment, and to implement effective changes for a change proposal. In this article, we conducted an empirical study on three Java open-source systems to show how CIA can be used during software modification. The results indicate that: (1) assessing changeability of a change proposal based on the impact results of the CIA is not accurate from the precision perspective; (2) the proposed impactness metric is an effective indicator of changeability assessment for the change proposal; and (3) CIA can make the change implementation process more efficient and easier. 
96|-||Sources of value in application ecosystems|Mobile application stores have revolutionised the dynamics of mobile ecosystems. Research on mobile application ecosystems has been significantly driven by data that is focused on the visualisation of an ecosystem's dynamics. This is a valuable step towards understanding the nature of the ecosystems, but it is limited in its explanatory power. Thus, a theory-driven approach is needed to understand the overall dynamics of such systems. This study applies a theoretical framework of value creation in e-business in the context of mobile application ecosystems, with a focus on application developers. A qualitative research strategy is employed in testing operationalisation in a sample of developers. The sample comprises 27 application developers from the three leading mobile application ecosystems. The results show that efficiency is the main source of value, products seldom create value through complementarities, and approaches towards lock-in and novelty seem to vary among application developers. The managerial and theoretical implications of such biased value creation in mobile ecosystems are considered. 
96|-||Development and validation of customized process models|
96|-||FPGA implementation of reversible watermarking in digital images using reversible contrast mapping|Reversible contrast mapping (RCM) and its various modified versions are used extensively in reversible watermarking (RW) to embed secret information into the digital contents. RCM based RW accomplishes a simple integer transform applied on pair of pixels and their least significant bits (LSB) are used for data embedding. It is perfectly invertible even if the LSBs of the transformed pixels are lost during data embedding. RCM offers high embedding rate at relatively low visual distortion (embedding distortion). Moreover, low computation cost and ease of hardware realization make it attractive for real-time implementation. To this aim, this paper proposes a field programmable gate array (FPGA) based very large scale integration (VLSI) architecture of RCM-RW algorithm for digital images that can serve the purpose of media authentication in real-time environment. Two architectures, one for block size (8 × 8) and the other one for (32 × 32) block are developed. The proposed architecture allows a 6-stage pipelining technique to speed up the circuit operation. For a cover image of block size (32 × 32), the proposed architecture requires 9881 slices, 9347 slice flip-flops, 11291 number 4-input LUTs, 3 BRAMs and a data rate of 1.0395 Mbps at an operating frequency as high as 98.76 MHz. 
96|-||A component-based process with separation of concerns for the development of embedded real-time software systems|Numerous component models have been proposed in the literature, a testimony of a subject domain rich with technical and scientific challenges, and considerable potential. Unfortunately however, the reported level of adoption has been comparatively low. Where successes were had, they were largely facilitated by the manifest endorsement, where not the mandate, by relevant stakeholders, either internal to the industrial adopter or with authority over the application domain. The work presented in this paper stems from a comprehensive initiative taken by the European Space Agency (ESA) and its industrial suppliers. This initiative also enjoyed significant synergy with interests shown for similar goals by the telecommunications and railways domain, thanks to the interaction between two parallel project frameworks. The ESA effort aimed at favouring the adoption of a software reference architecture across its software supply chain. The center of that strategy revolves around a component model and the software development process that builds on it. This paper presents the rationale, the design and implementation choices made in their conception, as well as the feedback obtained from a number of industrial case studies that assessed them. 
96|-||Empirical evaluation of a privacy-focused threat modeling methodology|Privacy is a key issue in today's society. Software systems handle more and more sensitive information concerning citizens. It is important that such systems are privacy-friendly by design. In previous work, we proposed a privacy threat analysis methodology, named LINDDUN. The methodology supports requirements engineers and software architects in identifying privacy weaknesses in the system they contribute to developing. As this is a fairly new technique, its results when applied in realistic scenarios are yet unknown. This paper presents a series of three empirical studies that thoroughly evaluate LINDDUN from a multi-faceted perspective. Our assessment characterizes the correctness and completeness of the analysis results produced by LINDDUN, as well as the productivity associated with executing the methodology. We also look into aspects such as the ease of use and reliability of LINDDUN. The results are encouraging, overall. However, some areas for further improvement have been identified as a result of this empirical inquiry. 
96|-||Memory leak detection in Java: Taxonomy and classification of approaches|Memory leaks are usually not associated with runtime environments with automatic garbage collection; however, memory leaks do happen in such environments and present a challenge to detect and find a root cause. Currently in the industry manual heap dump analysis is the most popular way of finding memory leaks, regardless of the number of automated methods proposed by scientists over the years. However, heap dump analysis alone cannot answer all questions needed to fix the leak effectively. The current paper reviews memory leak detection approaches proposed over the years and classifies them from the point of view of assessed metrics, performance overhead and intrusiveness. In addition, we classify the methods into online, offline and hybrid groups based on their features. 
96|-||Transforming an enterprise model into a use case model in business process systems|
96|-||Synthesizing interpreted domain-specific models to manage smart microgrids|
96|-||Broker-based SLA-aware composite service provisioning|QoS-aware service composition aims to satisfy users’ quality of services (QoS) needs during service composition. Traditional methods simply attempt to maximize user satisfaction by provisioning the composite service instance with the best QoS. These “best-effort” methods fail to take into account that there also exist other consumers competing for the service resources and their decisions of service selection/composition can impact on QoS. Since user's QoS needs can be met once the demanded level is reached, in this paper, we propose an “on-demand” strategy for QoS-aware service composition to replace the traditional “best-effort” strategy. The service broker is introduced to facilitate implementation of this strategy: it first purchases a number of service instances for each component from providers and then provisions the composite services with different QoS classes to consumers. This paper focuses on how the broker follows the service level agreement (SLA) to provision composite services in the “on-demand” manner. This problem is formally expressed as the minimization of the QoS distance function between SLA and QoS of composite service instances, under a series of constraints. Heuristic approaches are proposed for the problem and experiments are conducted at last to verify their effectiveness and efficiency. 
97|-|http://www.sciencedirect.com/science/journal/01641212/97|Search-based metamodel matching with structural and syntactic measures|The use of different domain-specific modeling languages and diverse versions of the same modeling language often entails the need to translate models between the different languages and language versions. The first step in establishing a transformation between two languages is to find their corresponding concepts, i.e., finding correspondences between their metamodel elements. Although, metamodels use heterogeneous terminologies and structures, they often still describe similar language concepts. In this paper, we propose to combine structural metrics (e.g., number of properties per concept) and syntactic metrics to generate correspondences between metamodels. Because metamodel matching requires to cope with a huge search space of possible element combinations, we adapted a local and a global metaheuristic search algorithm to find the best set of correspondences between metamodels. The efficiency and effectiveness of our proposal is evaluated on different matching scenarios based on existing benchmarks. In addition, we compared our technique to state-of-the-art ontology matching and model matching approaches. 
97|-||Comparing model-based and dynamic event-extraction based GUI testing techniques: An empirical study|
97|-||Hybrid address spaces: A methodology for implementing scalable high-level programming models on non-coherent many-core architectures|This paper introduces hybrid address spaces as a fundamental design methodology for implementing scalable runtime systems on many-core architectures without hardware support for cache coherence. We use hybrid address spaces for an implementation of MapReduce, a programming model for large-scale data processing, and the implementation of a remote memory access (RMA) model. Both implementations are available on the Intel SCC and are portable to similar architectures. We present the design and implementation of HyMR, a MapReduce runtime system whereby different stages and the synchronization operations between them alternate between a distributed memory address space and a shared memory address space, to improve performance and scalability. We compare HyMR to a reference implementation and we find that HyMR improves performance by a factor of 1.71× over a set of representative MapReduce benchmarks. We also compare HyMR with Phoenix++, a state-of-art implementation for systems with hardware-managed cache coherence in terms of scalability and sustained to peak data processing bandwidth, where HyMR demonstrates improvements of a factor of 3.1× and 3.2× respectively. We further evaluate our hybrid remote memory access (HyRMA) programming model and assess its performance to be superior of that of message passing. 
97|-||A systematic literature review on the industrial use of software process simulation|ContextSoftware process simulation modelling (SPSM) captures the dynamic behaviour and uncertainty in the software process. Existing literature has conflicting claims about its practical usefulness: SPSM is useful and has an industrial impact; SPSM is useful and has no industrial impact yet; SPSM is not useful and has little potential for industry.ObjectiveTo assess the conflicting standpoints on the usefulness of SPSM.MethodA systematic literature review was performed to identify, assess and aggregate empirical evidence on the usefulness of SPSM.ResultsIn the primary studies, to date, the persistent trend is that of proof-of-concept applications of software process simulation for various purposes (e.g. estimation, training, process improvement, etc.). They score poorly on the stated quality criteria. Also only a few studies report some initial evaluation of the simulation models for the intended purposes.ConclusionThere is a lack of conclusive evidence to substantiate the claimed usefulness of SPSM for any of the intended purposes. A few studies that report the cost of applying simulation do not support the claim that it is an inexpensive method. Furthermore, there is a paramount need for improvement in conducting and reporting simulation studies with an emphasis on evaluation against the intended purpose. 
97|-||O1FS: Flash file system with O(1) crash recovery time|The crash recovery time of NAND flash file systems increases with flash memory capacity. Crash recovery usually takes several minutes for a gigabyte of flash memory and becomes a serious problem for mobile devices. To address this problem, we propose a new flash file system, O1FS. A key concept of our system is that a small number of blocks are modified exclusively until we change the blocks explicitly. To recover from crashes, O1FS only accesses the most recently modified blocks rather than the entire flash memory. Therefore, the crash recovery time is bounded by the size of the blocks. We develop mathematical models of crash recovery techniques and prove that the time complexity of O1FS is O(1), whereas that of other methods is proportional to the number of blocks in the flash memory. Our evaluation shows that the crash recovery of O1FS is about 18.5 times faster than that of a state-of-the-art method. 
97|-||FlexIQ: A flexible interactive Querying Framework by Exploiting the Skyline Operator|
97|-||Efficient implementation of chaotic image encryption in transform domains|The primary goal of this paper is security management in data image transmission and storage. Because of the increased use of images in industrial operations, it is necessary to protect the secret data of the image against unauthorized access. In this paper, we introduce a novel approach for image encryption based on employing a cyclic shift and the 2-D chaotic Baker map in different transform domains. The Integer Wavelet Transform (IWT), the Discrete Wavelet Transform (DWT), and the Discrete Cosine Transform (DCT) are exploited in the proposed encryption approach. The characteristics of the transform domains are studied and used to carry out the chaotic encryption. A comparison study between the transform-domain encryption approaches in the presence of attacks shows the superiority of encryption in the DWT domain. 
97|-||RGB color image encryption based on Choquet fuzzy integral|In recent years, one can see an increasing interest in the security of digital images. This research presents a new RGB color image encryption using keystream generator based on Choquet fuzzy integral (CFI). The properties of the dynamical keystream generator with mathematical analysis are presented in this work. In the proposed method, the CFI is first used to generate pseudo-random keystreams. Then, each of the color pixels is decomposed into three gray-level components. The output of the CFI is used to randomly shift the bits of three gray-level components. Finally, three components of RGB color pixels and the generated keystream are coupled to encrypt the permuted components. Performance aspects of the proposed algorithm such as the entropy analysis, differential analysis, statistical analysis, cipher random analysis, and cipher sensitivity analysis are introduced to evaluate the security of the new scheme. The experimental results reveal the fact that the proposed algorithm is suitable for practical use in protecting the security of digital image information distributed via the Internet. 
97|-||Processes versus people: How should agile software development maturity be defined?|Maturity in software development is currently defined by models such as CMMI-DEV and ISO/IEC 15504, which emphasize the need to manage, establish, measure and optimize processes. Teams that develop software using these models are guided by defined, detailed processes. However, an increasing number of teams have been implementing agile software development methods that focus on people rather than processes. What, then, is maturity for these agile teams that focus less on detailed, defined processes? This is the question we sought to answer in this study. To this end, we asked agile practitioners about their perception of the maturity level of a number of practices and how they defined maturity in agile software development. We used cluster analysis to analyze quantitative data and triangulated the results with content analysis of the qualitative data. We then proposed a new definition for agile software development maturity. The findings show that practitioners do not see maturity in agile software development as process definition or quantitative management capabilities. Rather, agile maturity means fostering more subjective capabilities, such as collaboration, communication, commitment, care, sharing and self-organization. 
97|-||A learning-based module extraction method for object-oriented systems|Developers apply object-oriented (OO) design principles to produce modular, reusable software. Therefore, service-specific groups of related software classes called modules arise in OO systems. Extracting the modules is critical for better software comprehension, efficient architecture recovery, determination of service candidates to migrate legacy software to a service-oriented architecture, and transportation of such services to cloud-based distributed systems. In this study, we propose a novel approach to automatic module extraction to identify services in OO software systems. In our approach, first we create a weighted and directed graph of the software system in which vertices and edges represent the classes and their relations, respectively. Then, we apply a clustering algorithm over the graph to extract the modules. We calculate the weight of an edge by considering its probability of being within a module or between modules. To estimate these positional probabilities, we propose a machine-learning-based classification system that we train with data gathered from a real-world OO reference system. We have implemented an automatic module extraction tool and evaluated the proposed approach on several open-source and industrial projects. The experimental results show that the proposed approach generates highly accurate decompositions that are close to authoritative module structures and outperforms existing methods. 
97|-||Improving the communication performance of distributed animation rendering using BitTorrent file system|
97|-||Estimating confidence interval of software reliability with adaptive testing strategy|Software reliability assessment is a critical problem in safety-critical and mission-critical systems. In the reliability assessment of such a system, both an accurate reliability estimate and a tight confidence interval are required. Adaptive testing (AT) is an on-line testing framework, which dynamically selects test cases from different subdomains to achieve some optimization object. Although AT has been proved effective in minimizing reliability estimator variance, its performance on providing the corresponding confidence interval has not been investigated. In order to address this issue, an AT strategy combined with Bayesian inference (AT-BI) is proposed in this study. The novel AT-BI strategy is expected to be effective in providing both a low-variance estimator and a tight confidence interval. Experiments are set up to validate the effectiveness of the AT-BI strategy. 
98|-|http://www.sciencedirect.com/science/journal/01641212/98|âWith a little help from new friendsâ: Boosting information cascades in social networks based on link injection|
98|-||Toward a new aspect-mining approach for multi-agent systems|Many aspect mining techniques have been proposed for object-oriented systems. Unfortunately, aspect mining for multi-agent systems is an unexplored research area. The inherent specificities of multi-agent systems (such as autonomy, pro-activity, reactivity, and adaptability) make it difficult to understand, reuse and maintain their code. We propose, in this paper, a (semi-automatic) hybrid aspect mining approach for agent-oriented code. The technique is based on both static and dynamic analyzes. The main motivations of this work are (1) identifying cross-cutting concerns in existing agent-oriented code, and (2) making them explicitly available to software engineers involved in the evolution of agent-oriented code in order to facilitate its refactoring and, consequently, to improve its understandability, reusability and maintainability. The proposed approach is supported by a software tool, called MAMIT (MAS Aspect-MIning Tool), that we developed. The approach and the associated tool are illustrated using a concrete case study. 
98|-||DRE system performance optimization with the SMACK cache efficiency metric|
98|-||WAS: A weighted attribute-based strategy for cluster test selection|
98|-||Selecting software reliability growth models and improving their predictive accuracy using historical projects data|During software development two important decisions organizations have to make are: how to allocate testing resources optimally and when the software is ready for release. SRGMs (software reliability growth models) provide empirical basis for evaluating and predicting reliability of software systems. When using SRGMs for the purpose of optimizing testing resource allocation, the model's ability to accurately predict the expected defect inflow profile is useful. For assessing release readiness, the asymptote accuracy is the most important attribute. Although more than hundred models for software reliability have been proposed and evaluated over time, there exists no clear guide on which models should be used for a given software development process or for a given industrial domain.Using defect inflow profiles from large software projects from Ericsson, Volvo Car Corporation and Saab, we evaluate commonly used SRGMs for their ability to provide empirical basis for making these decisions. We also demonstrate that using defect intensity growth rate from earlier projects increases the accuracy of the predictions. Our results show that Logistic and Gompertz models are the most accurate models; we further observe that classifying a given project based on its expected shape of defect inflow help to select the most appropriate model. 
98|-||A method to optimize the scope of a software product platform based on end-user features|ContextDue to increased competition and the advent of mass customization, many software firms are utilizing product families – groups of related products derived from a product platform – to provide product variety in a cost-effective manner. The key to designing a successful software product family is the product platform, so it is important to determine the most appropriate product platform scope related to business objectives, for product line development.AimThis paper proposes a novel method to find the optimized scope of a software product platform based on end-user features.MethodThe proposed method, PPSMS (Product Platform Scoping Method for Software Product Lines), mathematically formulates the product platform scope selection as an optimization problem. The problem formulation targets identification of an optimized product platform scope that will maximize life cycle cost savings and the amount of commonality, while meeting the goals and needs of the envisioned customers’ segments. A simulated annealing based algorithm that can solve problems heuristically is then used to help the decision maker in selecting a scope for the product platform, by performing tradeoff analysis of the commonality and cost savings objectives.ResultsIn a case study, PPSMS helped in identifying 5 non-dominated solutions considered to be of highest preference for decision making, taking into account both cost savings and commonality objectives. A quantitative and qualitative analysis indicated that human experts perceived value in adopting the method in practice, and that it was effective in identifying appropriate product platform scope. 
98|-||Modeling and analysis of customer premise equipments registration process in IEEE 802.22 WRAN cell|The development of the IEEE 802.22 standard is aimed at providing broadband access in rural areas by effectively utilizing the unused TV band, provided no harmful interference is caused to the incumbent operation. The motivation behind TV band selection is of having lower frequencies compared to other licensed bands, which, therefore, results in lower propagation path loss. Due to this quality, the spectral power density of the radio signal reduces slowly, which results in a high coverage area. Further, it has been observed that many TV channels largely remain unoccupied, as most households and businesses rely on cable and satellite TV services. This is the first international standard for a wireless regional area network (WRAN) based on cognitive radio technologies. This standard provides both PHY and MAC layer functionalities in an infrastructure based network for communication between customer premise equipments (CPEs) through a base station (BS). The Spectrum Manager is the central part of the BS, which plays a significant role in maintaining spectrum availability information, channel selection, channel management, scheduling quiet periods for spectrum sensing, accessing to the database and implementing IEEE 802.22 policies. A WRAN can particularly accommodate up to 512 CPEs in a cell. Contention may occur during initial ranging, periodic ranging, bandwidth request and urgent coexistence situation notification. The medium access control (MAC) incorporates several schemes to control contention between CPEs within a cell and overlapping cells sharing the same channel. A CPE has to make decision to resolve collisions in the upstream direction. In the case of initial ranging and periodic ranging, code division multiple access (CDMA) is employed to resolve collisions. For bandwidth and UCS notification, either a CDMA or exponential time backoff approach can be applied for collision resolution. This paper presents the analytical framework to evaluate the number of active CPEs in a cognitive radio network. It is important to note that when the arrival rate becomes equal to the service rate, the active CPEs curve attains a constant value. Further, the active CPEs length is highly dependent on service rate. The different special cases have been addressed and the effectiveness of the proposed framework has been validated through various evaluation results. 
98|-||Blending design patterns with aspects: A quantitative study|
98|-||A new chaotic map based image encryption schemes for several image formats|This paper proposes several image encryption schemes for popular image formats as Joint Photographic Experts Group (JPEG), Graphics Interchange Format (GIF), Portable Network Graphics (PNG), and Tagged Image File Format (TIFF). A cross chaotic map proposed based on Devaney's theory and dynamic block dividing of the 3D baker using the cross chaotic map are used for diffusion and permutation in encryption. Moreover, in order to verify user's identity, authentication is carried out using information hiding based on the cross chaotic function. In our methods, image files syntax and structure are not destructed, and the original image can be recovered lossless. For GIF, it keeps the property of animation successfully. The security test results indicate the proposed methods have high security, and the speed of our algorithm is faster than classical solutions. JPEG, GIF, TIFF and PNG image formats are popular contemporarily. Therefore this paper shows that the prospect of chaotic image encryption is promising. 
98|-||On building a consistent framework for executable systems architecture|
98|-||Investigating the applicability of Agility assessment surveys: A case study|ContextAgile software development has become popular in the past decade without being sufficiently defined. The Agile principles can be instantiated differently which creates different perceptions of Agility. This has resulted in several frameworks being presented in the research literature to evaluate the level of Agility. However, the evidence of their actual use in practice is limited.ObjectiveThe objective is to identify online surveys that assess/profile Agility in practice, and to evaluate the surveys in an industrial setting.MethodThe Agility assessment surveys were identified through searching the web. Then, they were explored and two surveys were identified as most promising for our objective. The selected surveys were evaluated in a case study with three Agile teams in a software consultancy company.ResultsEach team and its customer separately judged the team's Agility. This outcome was compared with the two survey results in focus-group meetings, and finally one of the surveys was agreed to provide a more holistic assessment of Agility.ConclusionsDifferent surveys may judge Agility differently, which supports the viewpoint that Agile is not well defined. Therefore, practitioners must decide what Agile means to them and select the assessment survey that matches their definition. 
98|-||Generating combinatorial test suite using combinatorial optimization|Combinatorial testing (CT) is an effective technique to test software with multiple configurable parameters. It is used to detect interaction faults caused by the combination effect of parameters. CT test generation aims at generating covering arrays that cover all t-way parameter combinations, where t is a given covering strength. In practical CT usage scenarios, there are usually constraints between parameters, and the performance of existing constraint-handling methods degrades fast when the number of constraints increases.The contributions of this paper are (1) we propose a new one-test-at-a-time algorithm for CT test generation, which uses pseudo-Boolean optimization to generate each new test case; (2) we have found that pursuing the maximum coverage for each test case is uneconomic, and a possible balance point is to keep the approximation ratio in [0.8,0.9]; (3) we propose a new self-adaptive mechanism to stop the optimization process at a proper time when generating each test case; (4) extensive experimental results show that our algorithm works fine on existing benchmarks, and the constraint-handling ability is better than existing approaches when the number of constraints is large; and (5) we propose a method to translate shielding parameters (a common type of constraints) into normal constraints. 
99|-|http://www.sciencedirect.com/science/journal/01641212/99|PROW: A Pairwise algorithm with constRaints, Order and Weight|Testing systems with many variables and/or values is often quite expensive due to the huge number of possible combinations to be tested. There are several criteria available to combine test data and produce scalable test suites. One of them is pairwise. With the pairwise criterion, each pair of values of any two parameters is included in at least one test case. Although this is a widely-used coverage criterion, two main characteristics improve considerably pairwise: constraints handling and prioritisation.This paper presents an algorithm and a tool. The algorithm (called PROW: Pairwise with constRaints, Order and Weight) handles constraints and prioritisation for pairwise coverage. The tool called CTWeb adds functionalities to execute PROW in different contexts, one of them is product sampling in Software Product Lines via importing feature models. Software Product Line (SPL) development is a recent paradigm, where a family of software systems is constructed by means of the reuse of a set of common functionalities and some variable functionalities. An essential artefact of a SPL is the feature model, which shows the features offered by the product line, jointly with the relationships (includes and excludes) among them. Pairwise testing could be used to obtain the product sampling to test in a SPL, using features as pairwise parameters. In this context, the constraint handling becomes essential. As a difference with respect to other tools, CTWeb does not require SAT solvers.This paper describes the PROW algorithm, also analysing its complexity and efficiency. The CTWeb tool is presented, including two examples of the PROW application to two real environments: the first corresponds to the migration of the subsystem of transactions processing of a credit card management system from AS400 to Oracle with .NET; the second applies both the algorithm and the tool to a SPL that monitors and controls some parameters of the load in trucks. 
99|-||Towards energy-efficient scheduling for real-time tasks under uncertain cloud computing environment|Green cloud computing has become a major concern in both industry and academia, and efficient scheduling approaches show promising ways to reduce the energy consumption of cloud computing platforms while guaranteeing QoS requirements of tasks. Existing scheduling approaches are inadequate for real-time tasks running in uncertain cloud environments, because those approaches assume that cloud computing environments are deterministic and pre-computed schedule decisions will be statically followed during schedule execution. In this paper, we address this issue. We introduce an interval number theory to describe the uncertainty of the computing environment and a scheduling architecture to mitigate the impact of uncertainty on the task scheduling quality for a cloud data center. Based on this architecture, we present a novel scheduling algorithm (PRS1) that dynamically exploits proactive and reactive scheduling methods, for scheduling real-time, aperiodic, independent tasks. To improve energy efficiency, we propose three strategies to scale up and down the system's computing resources according to workload to improve resource utilization and to reduce energy consumption for the cloud data center. We conduct extensive experiments to compare PRS with four typical baseline scheduling algorithms. The experimental results show that PRS performs better than those algorithms, and can effectively improve the performance of a cloud data center. 
99|-||Aggregate-strength interaction test suite prioritization|Combinatorial interaction testing is a widely used approach. In testing, it is often assumed that all combinatorial test cases have equal fault detection capability, however it has been shown that the execution order of an interaction test suite's test cases may be critical, especially when the testing resources are limited. To improve testing cost-effectiveness, test cases in the interaction test suite can be prioritized, and one of the best-known categories of prioritization approaches is based on “fixed-strength prioritization”, which prioritizes an interaction test suite by choosing new test cases which have the highest uncovered interaction coverage at a fixed strength (level of interaction among parameters). A drawback of these approaches, however, is that, when selecting each test case, they only consider a fixed strength, not multiple strengths. To overcome this, we propose a new “aggregate-strength prioritization”, to combine interaction coverage at different strengths. Experimental results show that in most cases our method performs better than the test-case-generation, reverse test-case-generation, and random prioritization techniques. The method also usually outperforms “fixed-strength prioritization”, while maintaining a similar time cost. 
99|-||A practical approach to the assessment of quality in use of corporate web sites|The paper presents a practical approach to web site quality, based on a novel perspective that considers the relationships between the web site and its stakeholders. This perspective leads to identify four fundamental concepts of quality: final quality, quality in use, basic quality and internal quality. This paper focuses on quality in use, and proposes a new quality model including a well structured and balanced set of characteristics and sub-characteristics, which aim at capturing the main dimensions that impact on the quality of a web site. The distinction between actual and expected quality is then introduced and a practical assessment methodology for expected quality (EQ-EVAL) is proposed, which employs expert evaluators instead of actual users in order to make the evaluation less expensive, without sacrificing, however, accuracy and reliability. The results of the application of the methodology in the evaluation of a sample set of corporate web sites are finally discussed, showing how the model and the methodology can indeed meet the stated requirements. 
99|-||Integrating mixed transmission and practical limitations with the worst-case response-time analysis for Controller Area Network|
99|-||Enhanced fixed-priority real-time scheduling on multi-core platforms by exploiting task period relationship|
99|-||Modelling large-scale information systems using ADLs â An industrial experience report|An organisation that had developed a large information system wanted to embark on a programme that would involve large-scale evolution of it. As a precursor to this, it was decided to create a comprehensive architectural description to capture and understand the system's design. This undertaking faced a number of challenges, including a low general awareness of software modelling and software architecture practices. The approach taken by the software architects tasked with this project included the definition of a simple, very specific, architecture description language. This paper reports our experience of the project and a simple ADL that we created as part of it. 
99|-||Recommender systems based on social networks|The traditional recommender systems, especially the collaborative filtering recommender systems, have been studied by many researchers in the past decade. However, they ignore the social relationships among users. In fact, these relationships can improve the accuracy of recommendation. In recent years, the study of social-based recommender systems has become an active research topic. In this paper, we propose a social regularization approach that incorporates social network information to benefit recommender systems. Both users’ friendships and rating records (tags) are employed to predict the missing values (tags) in the user-item matrix. Especially, we use a biclustering algorithm to identify the most suitable group of friends for generating different final recommendations. Empirical analyses on real datasets show that the proposed approach achieves superior performance to existing approaches. 
99|-||Integrating non-parametric models with linear components for producing software cost estimations|A long-lasting endeavor in the area of software project management is minimizing the risks caused by under- or over-estimations of the overall effort required to build new software systems. Deciding which method to use for achieving accurate cost estimations among the many methods proposed in the relevant literature is a significant issue for project managers. This paper investigates whether it is possible to improve the accuracy of estimations produced by popular non-parametric techniques by coupling them with a linear component, thus producing a new set of techniques called semi-parametric models (SPMs). The non-parametric models examined in this work include estimation by analogy (EbA), artificial neural networks (ANN), support vector machines (SVM) and locally weighted regression (LOESS). Our experimentation shows that the estimation ability of SPMs is superior to their non-parametric counterparts, especially in cases where both a linear and non-linear relationship exists between software effort and the related cost drivers. The proposed approach is empirically validated through a statistical framework which uses multiple comparisons to rank and cluster the models examined in non-overlapping groups performing significantly different. 
99|-||Bringing Test-Driven Development to web service choreographies|Choreographies are a distributed approach for composing web services. Compared to orchestrations, which use a centralized scheme for distributed service management, the interaction among the choreographed services is collaborative with decentralized coordination. Despite the advantages, choreography development, including the testing activities, has not yet evolved sufficiently to support the complexity of the large distributed systems. This substantially impacts the robustness of the products and overall adoption of choreographies. The goal of the research described in this paper is to support the Test-Driven Development (TDD) of choreographies to facilitate the construction of reliable, decentralized distributed systems. To achieve that, we present Rehearsal, a framework supporting the automated testing of choreographies at development-time. In addition, we present a choreography development methodology that guides the developer on applying TDD using Rehearsal. To assess the framework and the methodology, we conducted an exploratory study with developers, whose result was that Rehearsal was considered very helpful for the application of TDD and that the methodology helped the development of robust choreographies. 
99|-||Adaptive thermal-aware task scheduling for multi-core systems|
99|-||Cost, benefits and quality of software development documentation: A systematic mapping|ContextSoftware documentation is an integral part of any software development process. Researchers and practitioners have expressed concerns about costs, benefits and quality of software documentation in practice. On the one hand, there is a lack of a comprehensive model to evaluate the quality of documentation. On the other hand, researchers and practitioners need to assess whether documentation cost outweighs its benefit.ObjectivesIn this study, we aim to summarize the existing literature and provide an overview of the field of software documentation cost, benefit and quality.MethodWe use the systematic-mapping methodology to map the existing body of knowledge related to software documentation cost, benefit and quality. To achieve our objectives, 11 Research Questions (RQ) are raised. The primary papers are carefully selected. After applying the inclusion and exclusion criteria, our study pool included a set of 69 papers from 1971 to 2011. A systematic map is developed and refined iteratively.ResultsWe present the results of a systematic mapping covering different research aspects related to software documentation cost, benefit and quality (RQ 1–11). Key findings include: (1) validation research papers are dominating (27 papers), followed by solution proposals (21 papers). (2) Most papers (61 out of 69) do not mention the development life-cycle model explicitly. Agile development is only mentioned in 6 papers. (3) Most papers include only one “System under Study” (SUS) which is mostly academic prototype. The average number of participants in survey-based papers is 106, the highest one having approximately 1000 participants. (4) In terms of focus of papers, 50 papers focused on documentation quality, followed by 37 papers on benefit, and 12 papers on documentation cost. (5) The quality attributes of documentation that appear in most papers are, in order: completeness, consistency and accessibility. Additionally, improved meta-models for documentation cost, benefit and quality are also presented. Furthermore, we have created an online paper repository of the primary papers analyzed and mapped during this study.ConclusionOur study results show that this research area is emerging but far from mature. Firstly, documentation cost aspect seems to have been neglected in the existing literature and there are no systematic methods or models to measure cost. Also, despite a substantial number of solutions proposed during the last 40 years, more and stronger empirical evidences are still needed to enhance our understanding of this area. In particular, what we expect includes (1) more validation or evaluation studies; (2) studies involving large-scale development projects, or from large number of study participants of various organizations; (3) more industry-academia collaborations; (4) more estimation models or methods to assess documentation quality, benefit and, especially, cost. 
99|-||VM scaling based on Hurst exponent and Markov transition with empirical cloud data|One of the major benefits of cloud computing is virtualization scaling. Compared to existing studies on virtual machine scaling, this paper introduces Hurst exponent which gives additional characteristics for data trends to supplement the often used Markov transition approach. This approach captures both the long and short-term behaviors of the virtual machines (VMs). The dataset for testing of this approach was gathered from the computer usage of key servers supporting a large university. Performance evaluation shows our approach can assist prediction of VM CPU usage toward effective resource allocation. In turn, this allows the cloud resource provider to monitor and allocate the resource usage of all VMs in order to meet the service level agreements for each VM client. 
volume|issue|url|title|abstract
100|-|http://www.sciencedirect.com/science/journal/01641212/100|A controlled experiment to evaluate the understandability of KAOS and i* for modeling Teleo-Reactive systems|ContextTeleo-Reactive (TR) specifications allow engineers to define the behavior of reactive systems while taking into account goals and changes in the state of the environment.ObjectiveThis article evaluates two different Goal Oriented Requirements Engineering notations, i* and KAOS, to determine their understandability level for specifying TR systems.MethodA controlled experiment was performed by two groups of Bachelor students. Each group first analyzed a requirements model of a mobile robotic system, specified using one of the evaluated languages, and then they filled in a questionnaire to evaluate its understandability. Afterwards, each group proceeded similarly with the model of another system specified with the second language.ResultsThe statistical analysis of the data obtained by means of the experiment showed that the understandability of i* is higher than that of KAOS when modeling TR systems.ConclusionBoth languages are suitable for specifying TR systems although their notations should be specialized to maximize the understandability attribute. i* surpasses KAOS due to two main reasons: i* models represent dependencies between agents and goals or tasks; and notational differences between tasks and goals in i* are more evident than those between goals and requirements in KAOS. 
100|-||Iterated local search for microaggregation|Microaggregation is a disclosure control method used to protect microdata. We introduce a local search method and employ it in an iterated local search algorithm for the NP-hard minimum information loss microaggregation problem. Experimental results with benchmark data sets demonstrate that our algorithm consistently identifies better quality solutions than extant microaggregation methods. 
100|-||Web API growing pains: Loosely coupled yet strongly tied|Web APIs provide a systematic and extensible approach for application-to-application interaction. Developers using web APIs are forced to accompany the API providers in their software evolution tasks. In order to understand the distress caused by this imposition on web API client developers we perform a semi-structured interview with six such developers. We also investigate how major web API providers organize their API evolution, and we explore how this affects source code changes of their clients. Our exploratory qualitative study of the Twitter, Google Maps, Facebook and Netflix web APIs analyzes the state of web API evolution practices and provides insight into the impact of service evolution on client software. In order to complement the picture and also understand how web API providers deal with evolution, we investigate the server-side and client-side evolution of two open-source web APIs, namely VirtualBox and XBMC. Our study is complemented with a set of observations regarding best practices for web API evolution. 
100|-||Enhanced healthcare personnel rostering solution using mobile technologies|This paper presents a novel personnel rostering system for healthcare units, which incorporates mobile technologies to minimize time overheads and boost personnel satisfaction. This way, doctors nurses and administrative staff may provide solutions and suggestions to the process of shifts’ scheduling and rostering in a group based – social and organized manner, at any given time, using their smartphone or tablet. This system is designed and implemented according to wide research on requirements’ specification, carried out in Greek public hospitals and based on a study of healthcare units’ organization, at a practical and legal level. The personnel rostering system anticipates to facilitate the staff administration task, through real-time communication between hospital's personnel. It enables the formation of a micro-community with enhanced social communication tools, to provide dynamic management, recording and updating of changes that occur in scheduled duties, without mediators and delays. The proposed solution includes an intelligent mobile device application, designed for smartphones and tablets. It is provided to the personnel and enables them to participate in the process of scheduling duties and shifts. The XML based, back-end, supporting information system offers services that allow a smoother operation of the unit, minimize time overheads in case of arbitrary changes and maximize satisfaction of personnel. The overall operation of the units, that reclaim the features offered by this system, can be improved. Minimizing the time and other bureaucratic delays in personnel scheduling is a vital part of the way a healthcare facility is organized. Thus, facilitating this process, with any available technology, may prove to be cost effective and crucial. Systems that incorporate mobile applications are already widely accepted, and become increasingly important to the healthcare sector, as well. The mobile based, personnel shifts’ scheduling solution shown is an approach that already receives encouraging support and indicates that it assists in achieving remarkable results. 
100|-||Integrating usability work into a large inter-organisational agile development project: Tactics developed by usability designers|In this paper we examine the integration of usability activities into a large inter-organisational agile development project. Inter-organisational agile projects possess unique attributes. They involve multiple stakeholders from different organisational contexts and are thus characterised by competing priorities. Team members also lack a mutual awareness of what constitutes work. These issues make the collaboration between project teams challenging. Meanwhile collaboration between usability designers and agile project teams is an integral part of the integration of usability activities into agile development projects. We carried out an interpretive case study on a large inter-organisational agile development project to examine how usability designers and agile project teams collaborate in this project type. Results showed integration goals were achieved through five tactics deployed by the usability designers. These tactics were negotiating inclusion; upward influencing, placating expert users, establishing credibility and diffusing designs. The implications of these findings are summarised in the form of three propositions that pertain to how usability designer–agile project team collaborations might be organised in agile development projects. Further, the role of the usability designer in ensuring the integration of usability activities is also emphasised. 
100|-||MostoDEx: A tool to exchange RDF data using exchange samples|The Web is evolving into a Web of Data in which RDF data are becoming pervasive, and it is organised into datasets that share a common purpose but have been developed in isolation. This motivates the need to devise complex integration tasks, which are usually performed using schema mappings; generating them automatically is appealing to relieve users from the burden of handcrafting them. Many tools are based on the data models to be integrated: classes, properties, and constraints. Unfortunately, many data models in the Web of Data comprise very few or no constraints at all, so relying on constraints to generate schema mappings is not appealing. Other tools rely on handcrafting the schema mappings, which is not appealing at all. A few other tools rely on exchange samples but require user intervention, or are hybrid and require constraints to be available. In this article, we present MostoDEx, a tool to generate schema mappings between two RDF datasets. It uses a single exchange sample and a set of correspondences, but does not require any constraints to be available or any user intervention. We validated and evaluated MostoDEx using many experiments that prove its effectiveness and efficiency in practice. 
100|-||Comprehensible software fault and effort prediction: A data mining approach|Software fault and effort prediction are important tasks to minimize costs of a software project. In software effort prediction the aim is to forecast the effort needed to complete a software project, whereas software fault prediction tries to identify fault-prone modules. In this research both tasks are considered, thereby using different data mining techniques. The predictive models not only need to be accurate but also comprehensible, demanding that the user can understand the motivation behind the model's prediction. Unfortunately, to obtain predictive performance, comprehensibility is often sacrificed and vice versa. To overcome this problem, we extract trees from well performing Random Forests (RFs) and Support Vector Machines for regression (SVRs) making use of a rule extraction algorithm ALPA. This method builds trees (using C4.5 and REPTree) that mimic the black-box model (RF, SVR) as closely as possible. The proposed methodology is applied to publicly available datasets, complemented with new datasets that we have put together based on the Android repository. Surprisingly, the trees extracted from the black-box models by ALPA are not only comprehensible and explain how the black-box model makes (most of) its predictions, but are also more accurate than the trees obtained by working directly on the data. 
100|-||Profiling and classifying the behavior of malicious codes|Malware is a major security threat confronting computer systems and networks and has increased in scale and impact from the early days of ICT. Traditional protection mechanisms are largely incapable of dealing with the diversity and volume of malware variants which is evident today. This paper examines the evolution of malware including the nature of its activity and variants, and the implication of this for computer security industry practices.As a first step to address this challenge, I propose a framework to extract features statically and dynamically from malware that reflect the behavior of its code such as the Windows Application Programming Interface (API) calls. Similarity based mining and machine learning methods have been employed to profile and classify malware behaviors. This method is based on the sequences of API sequence calls and frequency of appearance.Experimental analysis results using large datasets show that the proposed method is effective in identifying known malware variants, and also classifies malware with high accuracy and low false alarm rates. This encouraging result indicates that classification is a viable approach for similarity detection to help detect malware. This work advances the detection of zero-day malware and offers researchers another method for understanding impact. 
100|-||A benchmarking process to assess software requirements documentation for space applications|Poorly written requirements are a common source of software defects and, in application areas like space systems, the cost of malfunctioning software can be very high. This work proposes a benchmarking procedure for assessing the quality of software requirements that adopt the Packet Utilization Standard (PUS) defined by the European Cooperation for Space Standardization (ECSS) standards. The benchmark uses three checklists that aim at guaranteeing that the specifications comply with the PUS standard, consider faulty behaviour, and do not include errors typically found in this type of documents. The benchmark is defined for two services of the PUS standard: the telecommand verification and on board operating scheduling. A benchmark validation approach is also proposed in the paper. It uses the concept of fault injection to insert known errors in software requirements specification documents. The benchmark validation is performed through its application to three projects from different countries. Results show that our proposal provides a simple and effective way for identifying weaknesses and compare the degree of maturity of requirements documents. 
100|-||From source code identifiers to natural language terms|Program comprehension techniques often explore program identifiers, to infer knowledge about programs. The relevance of source code identifiers as one relevant source of information about programs is already established in the literature, as well as their direct impact on future comprehension tasks.Most programming languages enforce some constrains on identifiers strings (e.g., white spaces or commas are not allowed). Also, programmers often use word combinations and abbreviations, to devise strings that represent single, or multiple, domain concepts in order to increase programming linguistic efficiency (convey more semantics writing less). These strings do not always use explicit marks to distinguish the terms used (e.g., CamelCase or underscores), so techniques often referred as hard splitting are not enough.This paper introduces Lingua::IdSplitter a dictionary based algorithm for splitting and expanding strings that compose multi-term identifiers. It explores the use of general programming and abbreviations dictionaries, but also a custom dictionary automatically generated from software natural language content, prone to include application domain terms and specific abbreviations. This approach was applied to two software packages, written in C, achieving a f-measure of around 90% for correctly splitting and expanding identifiers. A comparison with current state-of-the-art approaches is also presented. 
100|-||A computer system architecture providing a user-friendly man machine interface for accessing assistive technology in cloud computing|Assistive Technology (AT) includes hardware peripherals, software applications and systems that enable a user with a disability to use a PC. Thus, when a disabled user needs to work in a particular environment (e.g., at work, at school, in a government office, etc.) he/she has to properly configure the used PC. However, often, the configuration of AT software interfaces is not trivial at all. This paper presents the software design, implementation, and evaluation of a computer system architecture providing a software user-friendly man machine interface for accessing AT software in cloud computing. The main objective of such an architecture is to provide a new type of software human–computer interaction for accessing AT services over the cloud. Thus, end users can interact with their personalized computer environments using any physical networked PC. The advantage of this approach is that users do not have to install and/or setup any additional software on physical PCs and they can access their own AT virtual environments from everywhere. In particular, the usability of prototype based on the Remote Desktop Protocol (RDP) is evaluated in both private and public cloud scenarios. 
100|-||Defining multi-tenancy: A systematic mapping study on the academic and the industrial perspective|Software as a service is frequently offered in a multi-tenant style, where customers of the application and their end-users share resources such as software and hardware among all users, without necessarily sharing data. It is surprising that, with such a popular paradigm, little agreement exists with regard to the definition, domain, and challenges of multi-tenancy. This absence is detrimental to the research community and the industry, as it hampers progress in the domain of multi-tenancy and enables organizations and academics to wield their own definitions to further their commercial or research agendas.In this article, a systematic mapping study on multi-tenancy is described in which 761 academic papers and 371 industrial blogs are analysed. Both the industrial and academic perspective are assessed, in order to get a complete overview. The definition and topic maps provide a comprehensive overview of the domain, while the research agenda, listing four important research topics, provides a roadmap for future research efforts. 
100|-||Extracting REST resource models from procedure-oriented service interfaces|During the past decade a number of procedure-oriented protocols and standards have emerged for making service-offering systems available on the Web. The WS-* stack of protocols is the most prevalent example. However, this procedure and message-oriented approach has not aligned with the true potential of the Web's own architectural principles, such as the uniform identification and manipulation of resources, caching, hypermedia, and layering. In this respect, Resource Oriented Architectures based on the REST architectural style, have been proposed as a possible alternative to the operation-based view of service offerings. To date, compiling a REST API for back-end procedure-oriented services is considered as a manual process that requires as input specialized models, such as, service requirements and behavioral models. In this paper, we propose a resource extraction method in which service descriptions are analyzed, using natural language processing techniques and graph transformations, in order to yield a collection of hierarchically organized elements forming REST resources that semantically correspond to the functionality offered by the service. The proposed approach has been applied as a proof of concept with positive results, for the extraction of resource models from a sizable number of procedure-oriented Web Service interfaces that have been obtained from an open service directory. 
100|-||An imperfect software debugging model considering log-logistic distribution fault content function|Numerous software reliability growth models based on the non-homogeneous Poisson process assume perfect debugging. Such models, including the Goel–Okumoto, delayed S-shaped, and inflection S-shaped models, have been successfully validated in software testing. However, complex and uncertain test factors, such as test resource, tester skill, or test tool, can seriously affect the testing process. When detected faults are removed, new faults can be introduced in practical testing. The process is referred to as imperfect debugging. Imperfect software debugging models proposed in the literature generally assume a constantly or monotonically decreasing fault introduction rate per fault. These models cannot adequately describe the fault introduction process in a practical test. In this study, we propose an imperfect software debugging model that considers a log-logistic distribution fault content function, which can capture the increasing and decreasing characteristics of the fault introduction rate per fault. We also use several historical fault data sets to validate the performance of the proposed model. The model can suitably fit historical fault data and accurately predict failure behavior. Confidence interval and sensitivity analyses are also conducted. 
100|-||Using SAN formalism to evaluate Follow-The-Sun project scenarios|Performance evaluation of projects can be used by companies and institutions as a tool to help the decision making process of Follow-The-Sun (FTS) projects. This paper main goal is to discuss a stochastic model definition to evaluate the performance of different aspects of FTS projects. Examples that can be addressed using the FTS model are provided with results comparing different model instances to evaluate aspects such as project execution time and project costs composition. 
100|-||Dynamic cloud service selection using an adaptive learning mechanism in multi-cloud computing|Cloud service selection in a multi-cloud computing environment is receiving more and more attentions. There is an abundance of emerging cloud service resources that makes it hard for users to select the better services for their applications in a changing multi-cloud environment, especially for online real time applications. To assist users to efficiently select their preferred cloud services, a cloud service selection model adopting the cloud service brokers is given, and based on this model, a dynamic cloud service selection strategy named DCS is put forward. In the process of selecting services, each cloud service broker manages some clustered cloud services, and performs the DCS strategy whose core is an adaptive learning mechanism that comprises the incentive, forgetting and degenerate functions. The mechanism is devised to dynamically optimize the cloud service selection and to return the best service result to the user. Correspondingly, a set of dynamic cloud service selection algorithms are presented in this paper to implement our mechanism. The results of the simulation experiments show that our strategy has better overall performance and efficiency in acquiring high quality service solutions at a lower computing cost than existing relevant approaches. 
100|-||D-P2P-Sim+: A novel distributed framework for P2P protocols performance testing|In recent technologies like IoT (Internet of Things) and Web 2.0, a critical problem arises with respect to storing and processing the large amount of collected data. In this paper we develop and evaluate distributed infrastructures for storing and processing large amount of such data. We present a distributed framework that supports customized deployment of a variety of indexing engines over million-node overlays. The proposed framework provides the appropriate integrated set of tools that allows applications processing large amount of data, to evaluate and test the performance of various application protocols for very large scale deployments (multi million nodes–billions of keys). The key aim is to provide the appropriate environment that contributes in taking decisions regarding the choice of the protocol in storage P2P systems for a variety of big data applications. Using lightweight and efficient collection mechanisms, our system enables real-time registration of multiple measures, integrating support for real-life parameters such as node failure models and recovery strategies. Experiments have been performed at the PlanetLab network and at a typical research laboratory in order to verify scalability and show maximum re-usability of our setup. D-P2P-Sim+ framework is publicly available at http://code.google.com/p/d-p2p-sim/downloads/list. 
volume|issue|url|title|abstract
101|-|http://www.sciencedirect.com/science/journal/01641212/101|Multi-criteria scheduling of Bag-of-Tasks applications on heterogeneous interlinked clouds with simulated annealing|Cloud computing has spurred the creation of a multitude of services that use the cloud to deliver their products on-demand. Behind it, stand multiple “Cloud Providers” that in the past few years have created data-centers, spread around the world, creating a mesh of distributed resources that can meet high availability and quality of service requirements. The growing number of cloud clients demand reliability, performance and better cost-to-performance ratios. Recently, scientific research has focused on the optimization of interlinked cloud systems, an aim which requires strategies for allocation of resources and distribution of computing tasks between them, while also considering their cost along with any factors that may differentiate them. In this study, we have evaluated the use of simulated annealing and thermodynamic simulated annealing in the scheduling of a dynamic multi-cloud system with virtual machines of heterogeneous performance serving Bag-of-Tasks applications. The scheduling heuristics applied, consider multiple criteria when scheduling said applications and try to optimize both for performance and cost, while also taking into account the heterogeneity of the virtual machines. Simulation results indicate that the use of these heuristics can have a significant impact in performance while maintaining a good cost-performance trade-off. 
101|-||Capturing urgency and parallelism using quasi-deadlines for real-time multiprocessor scheduling|Recent trends toward multi-core architectures in real-time embedded systems pose challenges in designing efficient real-time multiprocessor scheduling algorithms. We believe that it is important to take into consideration both timing constraints of tasks (urgency) and parallelism restrictions of multiprocessor platforms (parallelism) together when designing scheduling algorithms. Motivated by this, we define the quasi-deadline of a job as a weighted sum of its absolute deadline (capturing urgency) and its worst case execution time (capturing parallelism) with a system-level control knob to balance urgency and parallelism effectively. Using the quasi-deadline to prioritize jobs, we propose two new scheduling algorithms, called EQDF (earliest quasi-deadline first) and EQDZL (earliest quasi-deadline until zero laxity), that are categorized into job-level fixed-priority (JFP) scheduling and job-level dynamic-priority (JDP) scheduling, respectively. This paper provides a new schedulability analysis for EQDF/EQDZL scheduling and addresses the problem of priority assignment under EQDF/EQDZL by determining a right value of the system-level control knob. It presents optimal and heuristic solutions to the problem subject to our proposed EQDF and EQDZL analysis. Our simulation results show that EQDF and EQDZL can improve schedulability significantly compared to EDF and EDZL, respectively. 
101|-||Enabling improved IR-based feature location|
101|-||A scalable generic transaction model scenario for distributed NoSQL databases|
101|-||Manufacturing execution systems: A vision for managing software development|Software development suffers from a lack of predictability with respect to cost, time, and quality. Predictability is one of the major concerns addressed by modern manufacturing execution systems (MESs). A MES does not actually execute the manufacturing (e.g., controlling equipment and producing goods), but rather collects, analyzes, integrates, and presents the data generated in industrial production so that employees have better insights into processes and can react quickly, leading to predictable manufacturing processes. In this paper, we introduce the principles and functional areas of a MES. We then analyze the gaps between MES-vision-driven software development and current practices. These gaps include: (1) lack of a unified data collection infrastructure, (2) lack of integrated people data, (3) lack of common conceptual frameworks driving improvement loops from development data, and (4) lack of support for projection and simulation. Finally, we illustrate the feasibility of leveraging MES principles to manage software development, using a Modularity Debt Management Decision Support System prototype we developed. In this prototype we demonstrate that information integration in MES-vision-driven systems enables new types of analyses, not previously available, for software development decision support. We conclude with suggestions for moving current software development practices closer to the MES vision. 
101|-||A separation-based UI architecture with a DSL for role specialization|This paper proposes an architecture and associated methodology to separate front end UI concerns from back end coding concerns to improve the platform flexibility, shorten the development time, and increase the productivity of developers. Typical UI development is heavily dependent upon the underlying platform, framework, or tool used to create it, which results in a number of problems. We took a separation-based UI architecture and modified it with a domain specific language to support the independence of UI creation thereby resolving some of the aforementioned problems. A methodology incorporating this architecture into the development process is proposed. A climate science application was created to verify the validity of the methodology using modern practices of UX, DSLs, code generation, and model-driven engineering. Analyzing related work provides an overview of other methods similar to our method. Subsequently we evaluate the climate science application, conclude, and detail future work. 
101|-||LAYER: A cost-efficient mechanism to support multi-tenant database as a service in cloud|This paper presents a novel mechanism to cost-efficiently support multi-tenant database as a service (MTDBaaS) in cloud for small businesses. We aim at the scenarios where a large number of small tenants are served but only some of them are active simultaneously. By small tenants, we mean that a tenant may have many small-sized tables while only a small number of those tables are accessed concurrently for each query. As most MTDBaaS providers, we consolidate multiple tenants’ data into the same database management system (DBMS) to reduce the cost of operation. However, our solution distinguishes itself from the existing solutions by a novel mechanism: Load As You quERy (LAYER in short). Concretely, tenants can define and create their own tables with LAYER, and set up possible reference constraints between any two tables. A shared table is used to store all data for all tenants, but only a moderate number of working tables are maintained for answering queries from active tenants. When a new query is submitted, tables involved in the query but not yet in the DBMS will be restored: tables are created, and data are loaded to these newly-created tables. If an active tenant becomes inactive (logs out or no query is issued in a specified time period), tables belonging to the tenant could be dropped when necessary, and updates to these tables would be mirrored to the shared table for backup. We provide two implementations of the LAYER mechanism, one is LAYER-MySQL, which is based on the traditional disk-based relational DBMS MySQL, and can yield high consolidation and acceptable performance; the other is LAYER-VoltDB, which is based on the in-memory relational DBMS VoltDB, and can provide much higher performance. Experimental results validate the feasibility of the proposed mechanism. 
101|-||Energy efficiency heterogeneous wireless access selection for multiple types of applications|Mobile terminal (MT) users run various types of applications, such as e-mail, APPs, web browsers, and multimedia, through various types of wireless networks. Extending the battery life of MT, which requires a large amount of electricity for wireless transmission, has become critical. This study focused on the energy efficiency of wireless networks, such as 3G, 4G and Wi-Fi, based on application characteristics and transmission loads. The various applications are classified into idle-bound applications (e.g., e-mail service) and transmission-bound applications (e.g., multimedia) that require diverse types of wireless networks. The operation state of a wireless network includes transmitting, receiving, listening, and sleeping modes. According to the game theory of the energy consumption analysis between the characteristics of applications and wireless networks, three wireless network selection schemes IBLB (idle-bound with load-balancing), TBLB (transmission-bound with load balancing), and WLAT (weighted load and application type) were proposed to reduce the amount of power consumption. Previous studies were compared with the proposed schemes through (1) variation of the number of running applications, (2) various numbers of 3G/4G base stations and Wi-Fi access points, and (3) the combinations of various types of applications to evaluate the energy efficiency of Wi-Fi and 3G/4G access networks selections. 
101|-||Algorithms for automated live migration of virtual machines|
101|-||Neural networks for predicting the duration of new software projects|The duration of software development projects has become a competitive issue: only 39% of them are finished on time relative to the duration planned originally. The techniques for predicting project duration are most often based on expert judgment and mathematical models, such as statistical regression or machine learning. The contribution of this study is to investigate whether or not the duration prediction accuracy obtained with a multilayer feedforward neural network model, also called a multilayer perceptron (MLP), and with a radial basis function neural network (RBFNN) model is statistically better than that obtained by a multiple linear regression (MLR) model when functional size and the maximum size of the team of developers are used as the independent variables. The three models mentioned above are trained and tested by predicting the duration of new software development projects with a set of projects from the International Software Benchmarking Standards Group (ISBSG) release 11. Results based on absolute residuals, Pred(l) and a Friedman statistical test show that prediction accuracy with the MLP and the RBFNN is statistically better than with the MLR model. 
101|-||Soft competency requirements in requirements engineering, software design, implementation, and testing|
101|-||Hindering data theft with encrypted data trees|
101|-||Quality of service approaches in cloud computing: A systematic mapping study|Context: Cloud computing is a new computing technology that provides services to consumers and businesses. Due to the increasing use of these services, the quality of service (QoS) of cloud computing has become an important and essential issue since there are many open challenges which need to be addressed related to trust in cloud services. Many research issues have been proposed in QoS approaches in the cloud computing area.Objective: The aim of this study is to survey current research on QoS approaches in cloud computing in order to identify where more emphasis should be placed in both current and future research directions.Method: A systematic mapping study was performed to find the related literature, and 67 articles were selected as primary studies that are classified in relation to the focus, research type and contribution type.Result: The majority of the articles are of the validation research type (64%). Infrastructure as a service (48%) was the largest research focus area, followed by software as a service (36%). The majority of contributions concerned methods (48%), followed by models (32%).Conclusion: The results of this study confirm that QoS approaches in cloud computing have become an important topic in the cloud computing area in recent years and there remain open challenges and gaps which require future research exploration. In particular, tools, metrics and evaluation research are needed in order to provide useful and trustworthy cloud computing services that deliver appropriate QoS. 
101|-||An investigation into the best practices for the successful design and implementation of lightweight software process assessment methods: A systematic literature review|Software process assessment (SPA) is an effective tool to understand an organization's process quality and to explore improvement opportunities. However, the knowledge that underlies the best practices required to develop assessment methods, either lightweight or heavyweight methods, is unfortunately scattered throughout the literature. This paper presents the results of a systematic literature review to organize those recognized as the best practices in a way that helps SPA researchers and practitioners in designing and implementing their assessment methods. Such practices are presented in the literature as assessment requirements, success factors, observations, and lessons learned. Consequently, a set of 38 best practices has been collected and classified into five main categories, namely practices related to SPA methods, support tools, procedures, documentation, and users. While this collected set of best practices is important for designing lightweight as well as heavyweight assessment methods, it is of utmost importance in designing lightweight assessment methods, as the design of which depends on individual experience. 
101|-||A systematic mapping study on technical debt and its management|ContextTechnical debt (TD) is a metaphor reflecting technical compromises that can yield short-term benefit but may hurt the long-term health of a software system.ObjectiveThis work aims at collecting studies on TD and TD management (TDM), and making a classification and thematic analysis on these studies, to obtain a comprehensive understanding on the TD concept and an overview on the current state of research on TDM.MethodA systematic mapping study was performed to identify and analyze research on TD and its management, covering publications between 1992 and 2013.ResultsNinety-four studies were finally selected. TD was classified into 10 types, 8 TDM activities were identified, and 29 tools for TDM were collected.ConclusionsThe term “debt” has been used in different ways by different people, which leads to ambiguous interpretation of the term. Code-related TD and its management have gained the most attention. There is a need for more empirical studies with high-quality evidence on the whole TDM process and on the application of specific TDM approaches in industrial settings. Moreover, dedicated TDM tools are needed for managing various types of TD in the whole TDM process. 
101|-||Enhancing a model-based engineering approach for distributed manufacturing automation systems with characteristics and design patterns|Recent trends in modern manufacturing, such as the growing need for flexibility and the increasing degree of automation in industrial facilities, require distributed control solutions. Implementations of such control schemas and underlying architectures come along with an exponential increase of the automation system's complexity. Therefore, methods for supporting automation engineers during the development processes are highly required. This paper presents an approach to supporting model-based engineering (MBE) of distributed manufacturing automation systems. The approach is based on the combination of notation, characteristics, and design patterns across multiple levels of an adapted development process. Accordingly, a prototypical support tool has been implemented. The modeling approach has been evaluated by case studies and additional usability experiments to determine the benefit of its application within the design of manufacturing automation systems. 
101|-||Improving software reliability prediction through multi-criteria based dynamic model selection and combination|In spite of much research efforts to develop software reliability models, there is no single model which is appropriate in all circumstances. Accordingly, some recent studies on software reliability have attempted to use existing models more effectively in practice (e.g., model selection and combination). However, it is not easy to identify which model is likely to make the most trustworthy predictions and to assign appropriate weights to models for the combination. The improper model selection or weight assignment often causes unsuccessful software reliability prediction in practice, which leads to cost/schedule overrun. In this paper, we propose a systematic reliability prediction framework which dynamically selects and combines multiple software reliability models based on the decision trees learning of multi-criteria. For the model selection, the proposed approach uses the empirical patterns of multi-criteria derived from models. Reduced error pruning decision tree identifies the models with the best predictive patterns and automatically assign a weight to each model. Then, the identified models fall into two groups according to the likelihood of over- or under-prediction, and the competitive models from each group are combined based on their given weights. From the evaluation results, our approach outperformed existing methods on average prediction accuracy. 
101|-||Quantifying usability of domain-specific languages: An empirical study on software maintenance|A domain-specific language (DSL) aims to support software development by offering abstractions to a particular domain. It is expected that DSLs improve the maintainability of artifacts otherwise produced with general-purpose languages. However, the maintainability of the DSL artifacts and, hence, their adoption in mainstream development, is largely dependent on the usability of the language itself. Unfortunately, it is often hard to identify their usability strengths and weaknesses early, as there is no guidance on how to objectively reveal them. Usability is a multi-faceted quality characteristic, which is challenging to quantify beforehand by DSL stakeholders. There is even less support on how to quantitatively evaluate the usability of DSLs used in maintenance tasks. In this context, this paper reports a study to compare the usability of textual DSLs under the perspective of software maintenance. A usability measurement framework was developed based on the cognitive dimensions of notations. The framework was evaluated both qualitatively and quantitatively using two DSLs in the context of two evolving object-oriented systems. The results suggested that the proposed metrics were useful: (1) to early identify DSL usability limitations, (2) to reveal specific DSL features favoring maintenance tasks, and (3) to successfully analyze eight critical DSL usability dimensions. 
101|-||A solution of dynamic VMs placement problem for energy consumption optimization based on evolutionary game theory|Power saving of data centers has become an urgent problem in recent years. For a virtualized data center, optimizing the placement of virtual machines (VMs) dynamically is one of the most effective methods for power savings. Based on a deep study on VMs placement, a solution is proposed and described in this paper to solve the problem of dynamic placement of VMs toward optimization of their energy consumptions. A computational model of energy consumption is proposed and built. A novel algorithm based on evolutionary game theory is also presented, which successfully addresses the challenges faced by dynamic placement of VMs. It is proved that the proposed algorithm can reach the optimal solutions theoretically. Experimental results also demonstrate that, by adjusting VMs placement dynamically, the energy consumption can be reduced correspondingly. In comparison with the existing state of the arts, our proposed method outperforms other five algorithms tested and achieves savings of 30–40% on energy consumption. 
102|-|http://www.sciencedirect.com/science/journal/01641212/102|Special issue on software architectures and systems for Big data|
102|-||Progressive online aggregation in a distributed stream system|Interactive query processing aims at generating approximate results with minimum response time. However, it is quite difficult for a batch-oriented processing system to progressively provide cumulatively accurate results in the context of a distributed environment. MapReduce Online extends the MapReduce framework to support online aggregation, but it is hindered by its processing speed in keeping up with ongoing real-time data events. We deploy the online aggregation algorithm over S4, a scalable stream processing system that is inspired by the combined functionalities of MapReduce and Actor model. Our system applies an asynchronous message communication mechanism from actor model to support online aggregation. It can process large scale data stream with high concurrency in a short response time. In this system, we adopt a distributed weighted random sampling algorithm to solve biased distribution between different streams. Furthermore, a multi-level query processing topology is developed to reduce overlapped processing for multiple queries. Our system can provide continuous window aggregation with a confidence interval and error bound. We have implemented our system and conducted plentiful experiments over the TPC-H benchmark. A large number of experiments are carried out to demonstrate that by using our system, high-quality query results can be generated within a short response time and that the approach outperforms MapReduce Online on data streams. 
102|-||Countering the concept-drift problems in big data by an incrementally optimized stream mining model|Mining the potential value hidden behind big data has been a popular research topic around the world. For an infinite big data scenario, the underlying data distribution of newly arrived data may be appeared differently from the old one in the real world. This phenomenon is so-called the concept-drift problem that exists commonly in the scenario of big data mining. In the past decade, decision tree inductions use multi-tree learning to detect the drift using alternative trees as a solution. However, multi-tree algorithms consume more computing resources than the singletree. This paper proposes a singletree with an optimized node-splitting mechanism to detect the drift in a test-then-training tree-building process. In the experiment, we compare the performance of the new method to some state-of-art singletree and multi-tree algorithms. Result shows that the new algorithm performs with good accuracy while a more compact model size and less use of memory than the others. 
102|-||HaoLap: A Hadoop based OLAP system for big data|In recent years, facing information explosion, industry and academia have adopted distributed file system and MapReduce programming model to address new challenges the big data has brought. Based on these technologies, this paper presents HaoLap (Hadoop based oLap), an OLAP (OnLine Analytical Processing) system for big data. Drawing on the experience of Multidimensional OLAP (MOLAP), HaoLap adopts the specified multidimensional model to map the dimensions and the measures; the dimension coding and traverse algorithm to achieve the roll up operation on dimension hierarchy; the partition and linearization algorithm to store dimensions and measures; the chunk selection algorithm to optimize OLAP performance; and MapReduce to execute OLAP. The paper illustrates the key techniques of HaoLap including system architecture, dimension definition, dimension coding and traversing, partition, data storage, OLAP and data loading algorithm. We evaluated HaoLap on a real application and compared it with Hive, HadoopDB, HBaseLattice, and Olap4Cloud. The experiment results show that HaoLap boost the efficiency of data loading, and has a great advantage in the OLAP performance of the data set size and query complexity, and meanwhile HaoLap also completely support dimension operations. 
102|-||Load-prediction scheduling algorithm for computer simulation of electrocardiogram in hybrid environments|This paper proposes an algorithm that allows fully utilize the Central Processing Unit–Graphics Processing Unit (CPU–GPU) hybrid architecture to conduct parallel computation and reasonable scheduling for computer simulation of electrocardiogram (ECG). This algorithm is realized by accelerating calculation speed and increasing platform adaptability of the parallel algorithm.Today, many algorithms have been proposed to dynamically schedule a set of tasks in CPU–GPU hybrid environments. Among these scheduling algorithms, only Pure Self-Scheduling (PSS) algorithm can achieve load balancing in such an extremely heterogeneous environment. However, Pure Self-Scheduling can neither fully exploit the advantages of GPU performance, nor efficiently minimize the dynamic scheduling overhead. In this paper, Load-Prediction Scheduling (LPS) has been introduced to solve the aforementioned problems. Furthermore, to meet the demand for the best performance in a hybrid environment, which is formed by many heterogeneous computers, we propose an approach to adjust scheduling parameters dynamically. In order to validate our parallel algorithm and scheduling approach, we performed ECG simulation to confirm the efficiency and accuracy of ECG simulation algorithms based on the proposed method. At first, LPS predicts the workloads of each step in the simulation. The prediction results help to schedule heavy workloads to components with strong computational ability and light workloads to components with weak computational ability. LPS also synthesizes dynamic-scheduling and static-scheduling methods to minimize the disadvantages of these two scheduling methods. In the meantime, a Sliding Window Mechanism (SWM) adjusts the boundary between dynamic-scheduling and static-scheduling to make LPS perform better in hybrid environments. Experimental results of LPS on the computer simulation of ECG show that the LPS algorithm is more efficient than PSS. The ECG simulation is improved by about 20 times by using our proposed method. The ECG simulation of LPS with SWM is about 21% faster than that without SWM. 
102|-||A cloud-based framework for Home-diagnosis service over big medical data|Self-caring services are becoming more and more important for our daily life, especially under the urgent situation of global aging. Big data such as massive historical medical records makes it possible for users to have self-caring services, such as to get diagnosis by themselves with similar patients’ records. Developing such a self-caring service gives rises to challenges including highly concurrent and scalable medical record retrieval, data analysis, as well as privacy protection. In this paper, we propose a cloud-based framework for implementing a self-caring service named Home-diagnosis to address the above challenges. Concretely, a Lucene-based distributed search cluster is designed to support highly concurrent and scalable medical record retrieval, data analysis and privacy protection. Moreover, to speed up medical record retrieval, a Hadoop cluster is adopted for offline data storage and index building. The implementation of the Home-diagnosis service is discussed, where similar historical medical records as well as a disease-symptom lattice are obtained, to help users figure out which kind of disease they are probably infected with. Finally, a prototype system is designed and a running example is presented to demonstrate the scalability and efficiency of our proposal. 
102|-||An effective and economical architecture for semantic-based heterogeneous multimedia big data retrieval|
102|-||Semantic based representing and organizing surveillance big data using video structural description technology|
102|-||Measuring the veracity of web event via uncertainty|
102|-||The effects of different alphabets on free text keystroke authentication: A case study on the KoreanâEnglish users|Keystroke dynamics is one of the representative behavioral biometrics, and it has been consistently recognized as an alternative to physiological biometrics for user authentication to strengthen the level of security. This paper investigates the effects that languages with different alphabets and different familiarity levels have on the free text keystroke authentication performance using Korean–English data collected from 83 Korean participants. In order to exploit the familiarity level, two typing characteristics are measured and tested. Student’s t-test reveals that the participants have higher typing proficiency but lower typing consistency in the language with the more familiar alphabet, i.e., their primary language (Korean). Typing proficiency is found to be a critical factor when only keystroke latencies are utilized during authentication, whereas typing consistency is found to be a critical factor when key sequence information is utilized in addition to keystroke latencies. The experimental results can be applied to build a customized keystroke dynamics-based authentication system, which adaptively determines the authentication method as well as the keystroke size based on a user’s typing characteristics. 
102|-||QoS prediction for dynamic reconfiguration of component based software systems|
102|-||Semi-automatic architectural pattern identification and documentation using architectural primitives|In this article, we propose an interactive approach for the semi-automatic identification and documentation of architectural patterns based on a domain-specific language. To address the rich concepts and variations of patterns, we firstly propose to support pattern description through architectural primitives. These are primitive abstractions at the architectural level that can be found in realizations of multiple patterns, and they can be leveraged by software architects for pattern annotation during software architecture documentation or reconstruction. Secondly, using these annotations, our approach automatically suggests possible pattern instances based on a reusable catalog of patterns and their variants. Once a pattern instance has been documented, the annotated component models and the source code get automatically checked for consistency and traceability links are automatically generated. To study the practical applicability and performance of our approach, we have conducted three case studies for existing, non-trivial open source systems. 
102|-||Power-aware scheduling of compositional real-time frameworks|The energy consumption problem has become a great challenge in all computing areas from modern handheld devices to large data centers. Dynamic voltage scaling (DVS) is widely used as mean to reduce the energy consumption of computer systems by lowering whenever possible the voltage and operating frequency of processors. Unfortunately, existing compositional real-time scheduling frameworks have been focusing only on efficient scheduling of tasks inside their components given a resource model, providing no interest on power/energy consumption. In this paper, we define the real-time DVS problem for a compositional scheduling framework. Considering the periodic resource model, we propose optimal static DVS schemes at system, component, and task levels. We also introduce component and task level dynamic DVS schemes that take advantage of runtime unused slack times and resource availability to provide even better energy savings. Finally, we provide power-aware schedulability conditions to guarantee the feasibility of each component under DVS for the Earliest Deadline First and the Rate Monotonic scheduling algorithms. Through simulations, we showed that our schemes can reduce the energy consumption of a component by up to 96%. 
102|-||An insight into license tools for open source software systems|Free/Libre/Open Source Software (FLOSS) has gained a lot of attention lately allowing organizations to incorporate third party source code into their implementations. When open source software libraries are used, software resources may be linked directly or indirectly with multiple open source licenses giving rise to potential license incompatibilities. Adequate support in license use is vital in order to avoid such violations and address how diverse licenses should be handled. In the current work we investigate software licensing giving a critical and comparative overview of existing assistive approaches and tools. These approaches are centered on three main categories: license information identification from source code and binaries, software metadata stored in code repositories, and license modeling and associated reasoning actions. We also give a formalization of the license compatibility problem and demonstrate the role of existing approaches in license use decisions. 
102|-||Progressive Outcomes: A framework for maturing in agile software development|Maturity models are used to guide improvements in the software engineering field and a number of maturity models for agile methods have been proposed in the last years. These models differ in their underlying structure prescribing different possible paths to maturity in agile software development, neglecting the fact that agile teams struggle to follow prescribed processes and practices. Our objective, therefore, was to empirically investigate how agile teams evolve to maturity, as a means to conceive a theory for agile software development evolvement that considers agile teams nature. The complex adaptive systems theory was used as a lens for analysis and four case studies were conducted to collect qualitative and quantitative data. As a result, we propose the Progressive Outcomes framework to describe the agile software development maturing process. It is a framework in which people have the central role, ambidexterity is a key ability to maturity, and improvement is guided by outcomes agile teams pursue, instead of prescribed practices. 
102|-||A time-based approach to automatic bug report assignment|Bug assignment is one of the important activities in bug triaging that aims to assign bugs to the appropriate developers for fixing. Many recommended automatic bug assignment approaches are based on text analysis methods such as machine learning and information retrieval methods. Most of these approaches use term-weighting techniques, such as term frequency-inverse document frequency (tf-idf), to determine the value of terms. However, the existing term-weighting techniques only deal with frequency of terms without considering the metadata associated with the terms that exist in software repositories. This paper aims to improve automatic bug assignment by using time-metadata in tf-idf (Time-tf-idf). In the Time-tf-idf technique, the recency of using the term by the developer is considered in determining the values of the developer expertise. An evaluation of the recommended automatic bug assignment approach that uses Time-tf-idf, called ABA-Time-tf-idf, was conducted on three open-source projects. The evaluation shows accuracy and mean reciprocal rank (MRR) improvements of up to 11.8% and 8.94%, respectively, in comparison to the use of tf-idf. Moreover, the ABA-Time-tf-idf approach outperforms the accuracy and MRR of commonly used approaches in automatic bug assignment by up to 45.52% and 55.54%, respectively. Consequently, consideration of time-metadata in term weighting reasonably leads to improvements in automatic bug assignment. 
102|-||Stochastic thermal-aware real-time task scheduling with considerations of soft errors|With the continued scaling of the CMOS devices, the exponential increase in power density has strikingly elevated the temperature of on-chip systems. Dynamic voltage/frequency scaling is a widely utilized system level power management technique to reduce the energy consumption and lower the on-chip temperature. However, scaling the voltage or frequency for thermal management leads to an increase in soft error rates, thus has adverse impact on system reliability. In this paper, the authors propose a stochastic thermal-aware task scheduling algorithm that considers soft errors in real-time embedded systems. For the given customer-defined soft error related target reliability and the maximum peak temperature, the proposed scheduling algorithm generates an energy-efficient task schedule by selecting the energy efficient operating frequency for each task and alternating the execution of hot tasks and cool tasks at the scaled operating frequency. The proposed stochastic scheduling algorithm features the consideration of uncertainty in transient fault occurrences. To handle the uncertainty, a fault adaptation variable Î± is introduced to adapt task execution to the stochastic property of fault occurrences. An energy efficiency factor Î´ is also introduced to facilitate the enhancement of energy efficiency by maximizing the energy saved per unit slack. Extensive simulations of synthetic real-time tasks and real-life benchmarking tasks were performed to validate the effectiveness of the proposed algorithm. Experimental results show that the proposed algorithm consumes up to 17.8% less energy as compared to the benchmarking schemes, and the peak temperature of the proposed algorithm is always below the maximum temperature limit and can be up to 9.6 °C lower than that of the benchmarking schemes. 
102|-||An effective approach to estimating the parameters of software reliability growth models using a real-valued genetic algorithm|
103|-|http://www.sciencedirect.com/science/journal/01641212/103|Search Based Software Engineering (SBSE)|
103|-||Software requirements selection and prioritization using SBSE approaches: A systematic review and mapping of the literature|The selection and prioritization of software requirements represents an area of interest in Search-Based Software Engineering (SBSE) and its main focus is finding and selecting a set of requirements that may be part of a software release. This paper presents a systematic review and mapping that investigated, analyzed, categorized and classified the SBSE approaches that have been proposed to address software requirement selection and prioritization problems, reporting quantitative and qualitative assessment. Initially 39 papers returned from our search strategy in this area and they were analyzed by 18 previously established quality criteria. The results of this systematic review show which aspects of the requirements selection and prioritization problems were addressed by researchers, which approaches and search techniques are currently adopted to address these problems, as well as the strengths and weaknesses in this research area highlighted from the quality criteria. 
103|-||A robust optimization approach to the next release problem in the presence of uncertainties|The next release problem is a significant task in the iterative and incremental software development model, involving the selection of a set of requirements to be included in the next software release. Given the dynamic environment in which modern software development occurs, the uncertainties related to the input variables of this problem should be taken into account. In this context, this paper presents a formulation to the next release problem considering the robust optimization framework, which enables the production of robust solutions. In order to measure the “price of robustness”, which is the loss in solution quality due to robustness, a large empirical evaluation was executed over synthetical and real-world instances. Several next release planning situations were considered, including different number of requirements, estimating skills and interdependencies between requirements. All empirical results are consistent to show that the penalization with regard to solution quality is relatively small. In addition, the proposed model's behavior is statistically the same for all considered instances, which qualifies it to be applied even in large-scale real-world software projects. 
103|-||The optimisation of stochastic grammars to enable cost-effective probabilistic structural testing|
103|-||A Memetic Algorithm for whole test suite generation|The generation of unit-level test cases for structural code coverage is a task well-suited to Genetic Algorithms. Method call sequences must be created that construct objects, put them into the right state and then execute uncovered code. However, the generation of primitive values, such as integers and doubles, characters that appear in strings, and arrays of primitive values, are not so straightforward. Often, small local changes are required to drive the value toward the one needed to execute some target structure. However, global searches like Genetic Algorithms tend to make larger changes that are not concentrated on any particular aspect of a test case. In this paper, we extend the Genetic Algorithm behind the EvoSuite test generation tool into a Memetic Algorithm, by equipping it with several local search operators. These operators are designed to efficiently optimize primitive values and other aspects of a test suite that allow the search for test cases to function more effectively. We evaluate our operators using a rigorous experimental methodology on over 12,000 Java classes, comprising open source classes of various different kinds, including numerical applications and text processors. Our study shows that increases in branch coverage of up to 53% are possible for an individual class in practice. 
103|-||Subdomain-based test data generation|Considerable effort is required to test software thoroughly. Even with automated test data generation tools, it is still necessary to evaluate the output of each test case and identify unexpected results. Manual effort can be reduced by restricting the range of inputs testers need to consider to regions that are more likely to reveal faults, thus reducing the number of test cases overall, and therefore reducing the effort needed to create oracles. This article describes and evaluates search-based techniques, using evolution strategies and subset selection, for identifying regions of the input domain (known as subdomains) such that test cases sampled at random from within these regions can be used efficiently to find faults. The fault finding capability of each subdomain is evaluated using mutation analysis, a technique that is based on faults programmers are likely to make. The resulting subdomains kill more mutants than random testing (up to six times as many in one case) with the same number or fewer test cases. Optimised subdomains can be used as a starting point for program analysis and regression testing. They can easily be comprehended by a human test engineer, so may be used to provide information about the software under test and design further highly efficient test suites. 
103|-||Test data generation with a Kalman filter-based adaptive genetic algorithm|Software testing is a crucial part of software development. It enables quality assurance, such as correctness, completeness and high reliability of the software systems. Current state-of-the-art software testing techniques employ search-based optimisation methods, such as genetic algorithms to handle the difficult and laborious task of test data generation. Despite their general applicability, genetic algorithms have to be parameterised in order to produce results of high quality. Different parameter values may be optimal for different problems and even different problem instances. In this work, we introduce a new approach for generating test data, based on adaptive optimisation. The adaptive optimisation framework uses feedback from the optimisation process to adjust parameter values of a genetic algorithm during the search. Our approach is compared to a state of the art test data optimisation algorithm that does not adapt parameter values online, and a representative adaptive optimisation algorithm, outperforming both methods in a wide range of problems. 
103|-||An assessment of search-based techniques for reverse engineering feature models|Successful software evolves from a single system by adding and changing functionality to keep up with users’ demands and to cater to their similar and different requirements. Nowadays it is a common practice to offer a system in many variants such as community, professional, or academic editions. Each variant provides different functionality described in terms of features. Software Product Line Engineering (SPLE) is an effective software development paradigm for this scenario. At the core of SPLE is variability modelling whose goal is to represent the combinations of features that distinguish the system variants using feature models, the de facto standard for such task. As SPLE practices are becoming more pervasive, reverse engineering feature models from the feature descriptions of each individual variant has become an active research subject. In this paper we evaluated, for this reverse engineering task, three standard search based techniques (evolutionary algorithms, hill climbing, and random search) with two objective functions on 74 SPLs. We compared their performance using precision and recall, and found a clear trade-off between these two metrics which we further reified into a third objective function based on FÎ², an information retrieval measure, that showed a clear performance improvement. We believe that this work sheds light on the great potential of search-based techniques for SPLE tasks. 
103|-||Cost-effective test suite minimization in product lines using search techniques|Cost-effective testing of a product in a product line requires obtaining a set of relevant test cases from the entire test suite via test selection and minimization techniques. In this paper, we particularly focus on test minimization for product lines, which identifies and eliminates redundant test cases from test suites in order to reduce the total number of test cases to execute, thereby improving the efficiency of testing. However, such minimization may result in the minimized test suite with low test coverage, low fault revealing capability, low priority test cases, and require more time than the allowed testing budget (e.g., time) as compared to the original test suite. To deal with the above issues, we formulated the minimization problem as a search problem and defined a fitness function considering various optimization objectives based on the above issues. To assess the performance of our fitness function, we conducted an extensive empirical evaluation by investigating the fitness function with three weight-based Genetic Algorithms (GAs) and seven multi-objective search algorithms using an industrial case study and 500 artificial problems inspired from the industrial case study. The results show that Random-Weighted Genetic Algorithm (RWGA) significantly outperforms the other algorithms since RWGA can balance all the objectives together by dynamically updating weights during each generation. Based on the results of our empirical evaluation, we also implemented a tool called TEst Minimization using Search Algorithms (TEMSA) to support test minimization using various search algorithms in the context of product lines. 
103|-||Applying multiobjective evolutionary algorithms to dynamic software product lines for reconfiguring mobile applications|Mobile applications require dynamic reconfiguration services (DRS) to self-adapt their behavior to the context changes (e.g., scarcity of resources). Dynamic Software Product Lines (DSPL) are a well-accepted approach to manage runtime variability, by means of late binding the variation points at runtime. During the system’s execution, the DRS deploys different configurations to satisfy the changing requirements according to a multiobjective criterion (e.g., insufficient battery level, requested quality of service). Search-based software engineering and, in particular, multiobjective evolutionary algorithms (MOEAs), can generate valid configurations of a DSPL at runtime. Several approaches use MOEAs to generate optimum configurations of a Software Product Line, but none of them consider DSPLs for mobile devices. In this paper, we explore the use of MOEAs to generate at runtime optimum configurations of the DSPL according to different criteria. The optimization problem is formalized in terms of a Feature Model (FM), a variability model. We evaluate six existing MOEAs by applying them to 12 different FMs, optimizing three different objectives (usability, battery consumption and memory footprint). The results are discussed according to the particular requirements of a DRS for mobile applications, showing that PAES and NSGA-II are the most suitable algorithms for mobile environments. 
103|-||Investigating the effect of âdefect co-fixâ on quality assurance resource allocation: A search-based approach|
103|-||MOMM: Multi-objective model merging|Nowadays, software systems are complex and large. To cope with this situation, teams of developers have to cooperate and work in parallel on software models. Thus, techniques to support the collaborative development of models are a must. To this end, several approaches exist to identify the change operations applied in parallel, to detect conflicts among them, as well as to construct a merged model by incorporating all non-conflicting operations. Conflicts often denote situations where the application of one operation disables the applicability of another one. Consequently, one operation has to be omitted to construct a valid merged model in such scenarios. When having to decide which operation to omit, the importance of its application has to be taken into account depending on the operation type and the application context. However, existing works treat the operations to merge with equal importance. We introduce in this paper, for the first time, a multi-objective formulation of the problem of model merging, based on NSGA-II, that aims to find the best trade-off between minimizing the number of omitted operations and maximizing the number of successfully applied important operations. We evaluated our approach using seven open source systems and compared it with different existing model merging approaches. The merging solutions obtained with our approach were found in all of the scenarios of our experiments to be comparable in terms of minimizing the number of conflicts to those suggested by existing approaches and to carry a high importance score of merged operations. Our results also revealed an interesting feature concerning the trade-off between the two conflicting objectives that demonstrates the practical value of taking the importance of operations into account in model merging tasks. In fact, the shape of the Pareto front represents an interesting guidance for developers to select best solutions based on their preferences. 
103|-||The influence of search components and problem characteristics in early life cycle class modelling|
103|-||Collaboration optimization in software process composition|Purpose: The purpose of this paper is to describe an optimization approach to maximize collaboration in software process composition. The research question is: how to compose a process for a specific software development project context aiming to maximize collaboration among team members? The optimization approach uses heuristic search algorithms to navigate the solution space and look for acceptable solutions.Design/methodology/approach: The process composition approach was evaluated through an experimental study conducted in the context of a large oil company in Brazil. The objective was to evaluate the feasibility of composing processes for three software development projects. We have also compared genetic algorithm (GA) and hill climbing (HC) algorithms driving the optimization with a simple random search (RS) in order to determine which would be more effective in addressing the problem. In addition, human specialist point-of-view was explored to verify if the composed processes were in accordance with his/her expectations regarding size, complexity, diversity, and reasonable sequence of components.Findings: The main findings indicate that GA is more effective (best results regarding the fitness function) than HC and RS in the search of solutions for collaboration optimization in software process composition for large instances. However, all algorithms are competitive for small instances and even brute force can be a feasible alternative in such a context. These SBSE results were complemented by the feedback given by specialist, indicating his satisfaction with the correctness, diversity, adherence to the project context, and support to the project manager during the decision making in process composition.Research limitations: This work was evaluated in the context of a single company and used only three project instances. Due to confidentiality restrictions, the data describing these instances could not be disclosed to be used in other research works. The reduced size of the sample prevents generalization for other types of projects or different contexts.Implications: This research is important for practitioners who are facing challenges to handle diversity in software process definition, since it proposes an approach based on context, reuse and process composition. It also contributes to research on collaboration by presenting a collaboration management solution (COMPOOTIM) that includes both an approach to introduce collaboration in organizations through software processes and a collaboration measurement strategy. From the standpoint of software engineering looking for collaborative solutions in distributed software development, free/open source software, agile, and ecosystems initiatives, the results also indicate how to increase collaboration in software development.Originality/value: This work proposes a systematic strategy to manage collaboration in software development process composition. Moreover, it brings together a mix of computer-oriented and human-oriented studies on the search-based software engineering (SBSE) research area. Finally, this work expands the body of knowledge in SBSE to the field of software process which has not been properly explored by former research. 
103|-||A comprehensive approach to the recovery of design pattern instances based on sub-patterns and method signatures|Design patterns are formalized best practices that address concerns related to high-level structures for applications being developed. The efficient recovery of design pattern instances significantly facilitates program comprehension and software reengineering. However, the recovery of design pattern instances is not a straightforward task. In this paper, we present a novel comprehensive approach to the recovery of instances of 23 GoF design patterns from source codes. The key point of the approach lies in that we consider different design pattern instances consist of some commonly recurring sub-patterns that are easier to be detected. In addition, we focus not only on the class relationship, but also on the characteristics of underlying method signatures in classes. We first transform the source codes and predefined GoF patterns into graphs, with the classes as nodes and the relationships as edges. We then identify the instances of sub-patterns that would be the possible constituents of pattern instances by means of subgraph discovery. The sub-pattern instances are further merged by the joint classes to see if the collective matches one of the predefined patterns. Finally, we compare the behavioral characteristics of method invocation with the predefined method signature templates of GoF patterns to obtain the final pattern instances directly. Compared with existing approaches, we integrate and improve some of the previous ideas and put forward a comprehensive and elaborative approach also based on our own ideas. We detect sub-patterns via graph isomorphism based on prime number composition and the joint classes to reduce the search space. Meanwhile, we employ the method signatures to investigate the behavioral features to avoid choosing the test cases with full code coverage. The results of the extensive experiments on recovering pattern instances from nine open source software systems demonstrate that our approach obtains the balanced high precision and recall. 
103|-||Diagrams or structural lists in software project retrospectives â An experimental comparison|Root cause analysis (RCA) is a recommended practice in retrospectives and cause–effect diagram (CED) is a commonly recommended technique for RCA. Our objective is to evaluate whether CED improves the outcome and perceived utility of RCA. We conducted a controlled experiment with 11 student software project teams by using a single factor paired design resulting in a total of 22 experimental units. Two visualization techniques of underlying causes were compared: CED and a structural list of causes. We used the output of RCA, questionnaires, and group interviews to compare the two techniques. In our results, CED increased the total number of detected causes. CED also increased the links between causes, thus, suggesting more structured analysis of problems. Furthermore, the participants perceived that CED improved organizing and outlining the detected causes. The implication of our results is that using CED in the RCA of retrospectives is recommended, yet, not mandatory as the groups also performed well with the structural list. In addition to increased number of detected causes, CED is visually more attractive and preferred by retrospective participants, even though it is somewhat harder to read and requires specific software tools. 
103|-||An empirical evaluation of ensemble adjustment methods for analogy-based effort estimation|ContextEffort adjustment is an essential part of analogy-based effort estimation, used to tune and adapt nearest analogies in order to produce more accurate estimations. Currently, there are plenty of adjustment methods proposed in literature, but there is no consensus on which method produces more accurate estimates and under which settings.ObjectiveThis paper investigates the potential of ensemble learning for variants of adjustment methods used in analogy-based effort estimation. The number k of analogies to be used is also investigated.MethodWe perform a large scale comparison study where many ensembles constructed from n out of 40 possible valid variants of adjustment methods are applied to eight datasets. The performance of each method was evaluated based on standardized accuracy and effect size.ResultsThe results have been subjected to statistical significance testing, and show reasonable significant improvements on the predictive performance where ensemble methods are applied.ConclusionOur conclusions suggest that ensembles of adjustment methods can work well and achieve good performance, even though they are not always superior to single methods. We also recommend constructing ensembles from only linear adjustment methods, as they have shown better performance and were frequently ranked higher. 
103|-||A composite-metric based path selection technique for the Tor anonymity network|The Tor anonymous network has become quite popular with regular users on the Internet. In the Tor network, an anonymous path is created by selecting three relays through which the connection is redirected. Nevertheless, as the number of Tor users has increased substantially in recent years, the algorithm with which the relays are selected affects the performance provided by the Tor network. More importantly as the performance suffers, users will leave the network, resulting in a lower anonymity set and in turn lower security provided by Tor network. In this paper, we proposed an algorithm for improving performance and security of the Tor network, by employing a combination of different metrics in the process of the path selection between the source and destination node. These metrics are bandwidth and uptime of relays as node conditions and delays between the relays as a path condition. Through a number of experiments we show that we could double the performance observed by end users when using the proposed technique as opposed to the current Tor path selection algorithm. More importantly, the proposed technique only requires a software upgrade on the client side, and other Tor nodes do not need to be modified. 
103|-||Enabling high-level application development for the Internet of Things|Application development in the Internet of Things (IoT) is challenging because it involves dealing with a wide range of related issues such as lack of separation of concerns, and lack of high-level of abstractions to address both the large scale and heterogeneity. Moreover, stakeholders involved in the application development have to address issues that can be attributed to different life-cycles phases. when developing applications. First, the application logic has to be analyzed and then separated into a set of distributed tasks for an underlying network. Then, the tasks have to be implemented for the specific hardware. Apart from handling these issues, they have to deal with other aspects of life-cycle such as changes in application requirements and deployed devices.Several approaches have been proposed in the closely related fields of wireless sensor network, ubiquitous and pervasive computing, and software engineering in general to address the above challenges. However, existing approaches only cover limited subsets of the above mentioned challenges when applied to the IoT. This paper proposes an integrated approach for addressing the above mentioned challenges. The main contributions of this paper are: (1) a development methodology that separates IoT application development into different concerns and provides a conceptual framework to develop an application, (2) a development framework that implements the development methodology to support actions of stakeholders. The development framework provides a set of modeling languages to specify each development concern and abstracts the scale and heterogeneity related complexity. It integrates code generation, task-mapping, and linking techniques to provide automation. Code generation supports the application development phase by producing a programming framework that allows stakeholders to focus on the application logic, while our mapping and linking techniques together support the deployment phase by producing device-specific code to result in a distributed system collaboratively hosted by individual devices. Our evaluation based on two realistic scenarios shows that the use of our approach improves the productivity of stakeholders involved in the application development. 
103|-||Classifying metrics for assessing Object-Oriented Software Maintainability: A family of metricsâ catalogs|Object-Oriented Programming is one of the most used paradigms. Complementarily, the software maintainability is considered a software attribute playing an important role in quality level. In this context, Object-Oriented Software Maintainability (OOSM) has been studied through years, and many researchers have proposed a large number of metrics to measure it. Consequently, the decision-making process about which metrics can be adopted in experiments on OOSM is a hard task. Therefore, a metrics’ categorization has been proposed to facilitate this process. As result, 7 categories and 17 subcategories were identified. These categories represent the scenarios of OOSM metrics adoption, and a family of OOSM metrics catalog was generated based on the selection of a metrics’ categorization. Additionally, a quasi-experiment was conducted to check the coverage index of the catalogs generated using our approach over the catalogs suggested by experts. 90% of coverage was obtained with 99% of confidential level using the Wilcoxon Test. Complementarily, a survey was conducted to check the experts’ opinion about the catalog generated by the portal when they were compared by the catalogs suggested by them. Therefore, this evaluation can be the first evidences of the usefulness of the family of the catalogs based on the metrics’ categorization. 
103|-||On applying machine learning techniques for design pattern detection|The detection of design patterns is a useful activity giving support to the comprehension and maintenance of software systems. Many approaches and tools have been proposed in the literature providing different results. In this paper, we extend a previous work regarding the application of machine learning techniques for design pattern detection, by adding a more extensive experimentation and enhancements in the analysis method. Here we exploit a combination of graph matching and machine learning techniques, implemented in a tool we developed, called MARPLE-DPD. Our approach allows the application of machine learning techniques, leveraging a modeling of design patterns that is able to represent pattern instances composed of a variable number of classes. We describe the experimentations for the detection of five design patterns on 10 open source software systems, compare the performances obtained by different learning models with respect to a baseline, and discuss the encountered issues. 
103|-||On-demand data broadcast with deadlines for avoiding conflicts in wireless networks|On-demand data broadcast (ODDB) has attracted increasing interest due to its efficiency of disseminating information in many real-world applications such as mobile social services, mobile payment and mobile e-commerce. In an ODDB system, the server places client requested data items received from the uplink to a set of downlink channels for downloading by the clients. Most existing work focused on how to allocate client requested data items to multiple channels for efficient downloading, but did not consider the time constraint of downloading which is critical for many real-world applications. For a set of requests with deadlines for downloading, this paper proposes an effective algorithm to broadcast data items of each request within its specified deadline using multiple channels under the well-known 2-conflict constraint: two data items conflict if they are broadcast in the same time slot or two adjacent time slots in different channels. Our algorithm adopts an approach of allocating most urgent and popular data item first (UPF) for minimizing the overall deadline miss ratio. The performance of the UPF method has been validated by extensive experiments on real-world data sets against three popular on-demand data broadcast schemes. 
103|-||Automatic enforcement of constraints in real-time collaborative architectural decision making|
103|-||Approaches to promote product quality within software process improvement initiatives: A mapping study|Enhancing product quality might be a main goal of a software process improvement initiative (SPI). Quality is, however, a complex concept, and experts recommend identifying relevant product quality characteristics to satisfy users/customers’ needs. There is thus a need to understand how SPI initiatives contribute to the improvement of software product quality characteristics. This paper aims to provide an overview of an up-to-date state-of-the-art regarding initiatives that focus on promoting product quality improvement by applying SPI approaches. This goal was achieved by conducting a systematic mapping study, as a result of which we identified 74 primary papers including both theoretical (75.7%) and empirical (24.3%) papers. The main product quality characteristics addressed are security, usability and reliability. Security-related process models, on the other hand, are those most cited (53%). The empirical papers suggest that traditional process reference models, such as CMM, CMMI or ISO 9001, moderately increase product quality characteristics, these principally being maintainability and reliability. However, there is a need for more empirical research to evaluate the impact of SPI initiatives on software product quality by considering contextual factors. SPI initiatives should be more driven by performance goals related to product quality characteristics. 
103|-||A survey study on major technical barriers affecting the decision to adopt cloud services|In the context of cloud computing, risks associated with underlying technologies, risks involving service models and outsourcing, and enterprise readiness have been recognized as potential barriers for the adoption. To accelerate cloud adoption, the concrete barriers negatively influencing the adoption decision need to be identified. Our study aims at understanding the impact of technical and security-related barriers on the organizational decision to adopt the cloud. We analyzed data collected through a web survey of 352 individuals working for enterprises consisting of decision makers as well as employees from other levels within an organization. The comparison of adopter and non-adopter sample reveals three potential adoption inhibitor, security, data privacy, and portability. The result from our logistic regression analysis confirms the criticality of the security concern, which results in an up to 26-fold increase in the non-adoption likelihood. Our study underlines the importance of the technical and security perspectives for research investigating the adoption of technology. 
103|-||Practical and representative faultloads for large-scale software systems|The faultload is one of the most critical elements of experimental dependability evaluation. It should embody a repeatable, portable, representative and generally accepted fault set. Concerning software faults, the definition of that kind of faultloads is particularly difficult, as it requires a much more complex emulation method than the traditional stuck-at or bit-flip used for hardware faults. Although faultloads based on software faults have already been proposed, the choice of adequate fault injection targets (i.e., actual software components where the faults are injected) is still an open and crucial issue. Furthermore, knowing that the number of possible software faults that can be injected in a given system is potentially very large, the problem of defining a faultload made of a small number of representative faults is of utmost importance. This paper presents a comprehensive fault injection study and proposes a strategy to guide the fault injection target selection to reduce the number of faults required for the faultload and exemplifies the proposed approach with a real web-server dependability benchmark and a large-scale integer vector sort application. 
103|-||Automatic deployment of distributed software systems: Definitions and state of the art|Deployment of software systems is a complex post-production process that consists in making software available for use and then keeping it operational. It must deal with constraints concerning both the system and the target machine(s), in particular their distribution, heterogeneity and dynamics, and satisfy requirements from different stakeholders. In the context of mobility and openness, deployment must react to the instability of the network of machines (failures, connections, disconnections, variations in the quality of the resources, etc.). Thus, deployment should be an uninterrupted process which also works when software is running and requires adaptiveness in order to continually satisfy the constraints and the requirements. Originally managed “by hand”, software deployment demands an increasing level of automation and autonomy.This article first provides up-to-date terminology and definitions related to software deployment. Then, it proposes an analytical framework and reviews recent research works on automatic deployment with reference to this framework, and synthesizes the results. The review shows that existing solutions are incomplete, and possibly inefficient or unusable, when distribution, heterogeneity, scalability, dynamics and openness are primary concerns. In particular, they barely support dynamic reactions to unforeseeable events. Additionally, abstraction level and expressiveness at design time are rather limited regarding deployment complexity. 
103|-||Early effort estimation in web application development|Project planning in software industry represents one of the most complex tasks, especially when there is a need to estimate the time, cost and effort needed for development of software projects. In the field of development effort estimation for classical software projects a number of methods have been developed, tested and successfully implemented. Web projects are, by their nature, different than classical software projects, and there is a lack of methods and models that provides a high degree of confidence in development effort estimation. This paper analyzes the possibility of using a combination of functional size and conceptual models for the purpose of web application development effort estimation. Measurement of functional size can be effectively applied to the conceptual models of the data-driven web applications because of the existence of extensive count of data movements. For the purpose of this study 19 web applications with their conceptual models were employed. An effort model was built using simple linear regression analysis. Upon construction, evaluation and validation of the effort model prediction accuracy elements, R2, MMRE, and Pred(l), showed promising results for web projects used in the model construction and validation process. 
103|-||Designing an open source maintenance-free Environmental Monitoring Application for Wireless Sensor Networks|We discuss the entire process for the analysis and design of an Environmental Monitoring Application for Wireless Sensor Networks, using existing open source components to create the application. We provide a thorough study of the different alternatives, from the selection of the embedded operating system to the different algorithms and strategies. The application has been designed to gather temperature and relative humidity data following the rules of quality assurance for environmental measurements, suitable for use in both research and industry. The main features of the application are: (a) runs in a multihop low-cost network based on IEEE 802.15.4, (b) improved network reliability and lifetimes, (c) easy management and maintenance-free, (d) ported to different platforms and (e) allows different configurations and network topologies. The application has been tested and validated in several long-term outdoor deployments with very good results and the conclusions are aligned with the experimental evidence. 
103|-||A comprehensive study of the predictive accuracy of dynamic change-impact analysis|
104|-|http://www.sciencedirect.com/science/journal/01641212/104|A process to identify relevant substitutes for healing failed WS-* orchestrations|Orchestrating web services aims to compose multiple services into workflows that answer complex user requirements. Web services are software components which are exposed to errors and failures that can occur during web service orchestration execution. Thus, many error-handling and healing approaches have been proposed to guarantee reliable orchestrations. Some of these approaches rely on the identification of relevant service substitutes to heal (by substitution) the defected services. In this paper, we propose an identification process of web service substitutes for healing failed web service orchestrations based on the measurement of similarity between service interfaces. The process reveals both simple and complex (compositions of) substitutes. We validated the approach via a set of experiments conducted on a collection of real web services. 
104|-||Agent-based Cloud bag-of-tasks execution|
104|-||When did your project start? â The software supplier's perspective|
104|-||Software rejuvenation via a multi-agent approach|
104|-||QualityScan scheme for load balancing efficiency in vehicular ad hoc networks (VANETs)|The main terminal devices in vehicular ad-hoc networks (VANETs) are highly mobile moving cars that handoff much more frequently than handheld devices. Nevertheless, frequent handoff or high handoff latency can influence the quality of service (QoS) of real-time network services. Since conventional handoff mechanisms cannot fulfill the requirements of VANET, many fast handoff schemes have been proposed. However, the schemes based on the signal strength of APs (access points) ignore the loading states of different APs and thus cannot utilize the bandwidth effectively. Whenever some APs are very busy, the QoS will be degraded. In order to solve this problem, we can pre-establish the APs and regulate their number according to different traffic types. In this paper, we present a fast handoff scheme for VANET, QualityScan, which decreases the handoff latency and considers loading states of the regional APs simultaneously. By the pre-established AP controller (APC), our scheme gathers the loading states of the APs regularly and predicts network traffic of the next moment. Based on the parameters obtained by passive scanning, the mobile nodes (MNs) can choose the optimal AP for the optimal QoS. According to our simulation analysis, QualityScan not only achieves load balance of the APs, but also improves QoS and handoff efficiency in VANET. 
104|-||Automated fault localization via hierarchical multiple predicate switching|Single predicate switching forcibly changes the state of a predicate instance at runtime and then identifies the root cause by examining the switched predicate, called critical predicate. However, switching one predicate instance has its limitations: in our experiments, we found that single predicate switching can only find critical predicates for 88 out of 300 common bugs in five real-life utility programs. For other 212 bugs, overcoming them may require switching multiple predicate instances. Nonetheless, taking all possible combinations of predicate instances into consideration will result in exponential explosion. Therefore, we propose a hierarchical multiple predicate switching technique, called HMPS, to locate faults effectively. Specifically, HMPS restricts the search for critical predicates to the scope of highly suspect functions identified by employing spectrum-based fault localization techniques. Besides, instrumentation methods and strategies for switch combination are presented to facilitate the search for critical predicates. The empirical studies show that HMPS is able to find critical predicates for 111 out of 212 bugs mentioned above through switching multiple predicate instances. In addition, HMPS captures 62% of these 300 bugs when examining up to 1% of the executed code, while the Barinel and Ochiai approaches locate 18% and 16% respectively. 
104|-||Sentiment Analysis in monitoring software development processes: An exploratory case study on GitHub's project issues|Software process models, which allow us to develop software products, can be improved by using the corresponding quality model. However, current tendencies in the application of Global Software Engineering and Global Software Development, which forces geographically dispersed teams to collaborate, make the usual monitoring techniques obsolete. This situation has led to looking for new methods that can help in the decision making process, such as the case of the Social Network Analysis field.In this article we propose the introduction of Sentiment Analysis techniques in order to identify and monitor the underlying sentiments in the text written by developers in issues and tickets. Therefore, in order to check its viability we conducted an exploratory case study analysing polarity and emotional clues in development issues from nine well-known projects that are freely available. Results show that although both polarity and emotional analysis are applicable, the emotional analysis looks to be more suitable to this kind of corpus. The developers leave underlying sentiments in the text, and that information could be monitored as any other feature in the development process. 
104|-||Investigating security threats in architectural context: Experimental evaluations of misuse case maps|
104|-||Service portfolio management: A repository-based framework|
104|-||Modeling the QoS parameters of DDS for event-driven real-time applications|The Data Distribution Service (DDS) standard defines a data-centric distribution middleware that supports the development of distributed real-time systems. To this end, the standard includes a wide set of configurable parameters to provide different degrees of Quality of Service (QoS). This paper presents an analysis of these QoS parameters when DDS is used to build reactive applications normally designed under an event-driven paradigm, and shows how to represent them using the real-time end-to-end flow model defined by the MARTE standard. We also present an application-case study to illustrate the use and modeling of DDS in next-generation distributed real-time systems. 
104|-||Scheduling parallel jobs with tentative runs and consolidation in the cloud|Since the success of cloud computing, more and more high performance computing parallel applications run in the cloud. Carefully scheduling parallel jobs is essential for cloud providers to maintain their quality of service. Existing parallel job scheduling mechanisms do not take the parallel workload consolidation into account to improve the scheduling performance. In this paper, after introducing a prioritized two-tier virtual machines architecture for parallel workload consolidation, we propose a consolidation-based parallel job scheduling algorithm. The algorithm employs tentative run and workload consolidation under such a two-tier virtual machines architecture to enhance the popular FCFS algorithm. Extensive experiments on well-known traces show that our algorithm significantly outperforms FCFS, and it can even produce comparable performance to the runtime-estimation-based EASY algorithm, though it does not require users to provide runtime estimation of the job. Moreover, our algorithm allows inaccurate CPU usage estimation and only requires trivial modification on FCFS. It is effective and robust for scheduling parallel workload in the cloud. 
104|-||Network coding-based energy-efficient multicast routing algorithm for multi-hop wireless networks|Multi-hop multicast routing can provide better communication performance in multi-hop wireless networks. However, existing multi-hop multicast routing hardly take into account energy efficiency of networks. This paper studies the energy-efficient multicast communication aiming at multi-hop wireless networks. Firstly, we analyze energy metric and energy efficiency metric of multi-hop networks. Then the corresponding models are given. Secondly, network coding is used to improve network throughput. Different from previous methods, we here consider that network nodes are satisfied with a certain random distribution. In such a case, it is a challenge to construct the network structure that network coding requires. For the above random network topology, we propose three basic structures of network coding to overcome this problem. Thirdly, we present a flexible energy-efficient multicast routing algorithm for multi-hop wireless networks to extensively exploit the network structure proposed above to maximize network throughput and decrease network energy consumption. Finally, we perform numerical experiments by network simulation. Simulation results indicate that our approach is significantly promising. 
104|-||A semantic approach for designing Assistive Software Recommender systems|
105|-|http://www.sciencedirect.com/science/journal/01641212/105|Service-oriented approach to fault tolerance in CPSs|Cyber-physical systems (CPSs) are open and interconnected embedded systems that control or interact with physical processes. Failures in CPSs can lead to loss of production time, damage to the equipment and environment, or loss of life, meaning that dependability and resilience are key properties for their design. However, existing fault tolerance and safety approaches are inadequate for complex, networked and dynamic CPSs. Service-orientation, on the other hand, is generally considered to be a robust architectural style, but there is a limited amount of research on fault tolerance of service-oriented architecture (SOA), especially on distributed real-time systems. We propose an approach that utilizes the loosely coupled nature of services to implement fault tolerance using a middleware-based real-time SOA (RTSOA) for CPSs. The approach, based on the concepts of fault isolation and recovery at the service level, is empirically evaluated using a demanding bilateral teleoperation (remote handling) application. The empirical evaluation demonstrates that RTSOA supports real-time fault detection and recovery, use of services as a unit of fault isolation, and it provides capability to implement fault tolerance patterns flexibly and without significant overhead. 
105|-||Improving multi-objective code-smells correction using development history|One of the widely used techniques to improve the quality of software systems is refactoring. Software refactoring improves the internal structure of the system while preserving its external behavior. These two concerns drive the existing approaches to refactoring automation. However, recent studies demonstrated that these concerns are not enough to produce correct and consistent refactoring solutions. In addition to quality improvement and behavior preservation, studies consider, among others, construct semantics preservation and minimization of changes. From another perspective, development history was proven as a powerful source of knowledge in many maintenance tasks. Still, development history is not widely explored in the context of automated software refactoring. In this paper, we use the development history collected from existing software projects to propose new refactoring solutions taking into account context similarity with situations seen in the past. We propose a multi-objective optimization-based approach to find good refactoring sequences that (1) minimize the number of code-smells, and (2) maximize the use of development history while (3) preserving the construct semantics. To this end, we use the non-dominated sorting genetic algorithm (NSGA-II) to find the best trade-offs between these three objectives. We evaluate our approach using a benchmark composed of five medium and large-size open-source systems and four types of code-smells (Blob, spaghetti code, functional decomposition, and data class). Our experimental results show the effectiveness of our approach, compared to three different state-of-the-art approaches, with more than 85% of code-smells fixed and 86% of suggested refactorings semantically coherent when the change history is used. 
105|-||New approaches to usability evaluation in software development: Barefoot and crowdsourcing|Usability evaluations provide software development teams with insights on the degree to which software applications enable users to achieve their goals, how fast these goals can be achieved, how easy an application is to learn and how satisfactory it is in use. Although such evaluations are crucial in the process of developing software systems with a high level of usability, their use is still limited in small and medium-sized software development companies. Many of these companies are e.g. unable to allocate the resources that are needed to conduct a full-fledged usability evaluation in accordance with a conventional approach.This paper presents and assesses two new approaches to overcome usability evaluation obstacles: a barefoot approach where software development practitioners are trained to drive usability evaluations; and a crowdsourcing approach where end users are given minimalist training to enable them to drive usability evaluations. We have evaluated how these approaches can reduce obstacles related to limited understanding, resistance and resource constraints. We found that these methods are complementary and highly relevant for software companies experiencing these obstacles. The barefoot approach is particularly suitable for reducing obstacles related to limited understanding and resistance while the crowdsourcing approach is cost-effective. 
105|-||Emotion-led modelling for people-oriented requirements engineering: The case study of emergency systems|In the field of design, it is accepted that users’ perceptions of systems are influenced by emotion as much as cognition, and functionally-complete products will not be adopted if they do not appeal to emotions. While software engineering methodologies have matured to handle non-functional requirements such as usability, what has not been investigated fully is the emotional needs of people. That is, what do users want to feel, and how do they feel about a system? In this paper, we argue that these emotional desires should be treated as first-class citizens in software engineering methodology, and present preliminary work on including emotions in requirements models using emotional goals. We evaluate these models both with a controlled user study, and on a case study of emergency systems for older people. The results of the controlled user study indicate that people are comfortable interpreting and modifying our models, and view the inclusion of emotions as first-class entities as a positive step in software engineering. The results of our case study indicate that current emergency systems fail to address the emotional needs their users, leading to low adoption and low usage. We conceptualised, designed, and prototyped an improved emergency system, and placed it into the homes of nine older people over a period of approximately two weeks each, showing improved user satisfaction over existing systems. 
105|-||Software cost estimating for CMMI Level 5 developers|This article provides analysis results of Capability Maturity Model Integrated Level 5 projects for developers earning the highest level possible, using actual software data from their initial project estimates. Since there were no measures to verify software performance, this level was used a proxy for high quality software. Ordinary least squares regression was used to predict final effort hours with initially estimated variables obviates the need to estimate growth or shrinkage for typical changes occurring in software projects, regardless of software developer (contracted or in-house). The OLS equations, or cost estimating relationship equations, were evaluated by a series of standards: statistical significance, visual inspection, goodness of fit measures, and academically set thresholds for accuracy measures used in software cost estimating: mean magnitude of relative error and prediction (for determining the percentage of records with 25%, or less, based on their magnitude of relative error score). As several initial estimated variables were strongly correlated to the reported final effort hours and each other, each variable was examined separately. Thirty records from software projects completed in 2003–2008 for the highest process maturity level were used to compute statistically significant equations with implicit growth or shrinkage in their make-up. 
105|-||SMaRT: A novel framework for addressing range queries over nonlinear trajectories|A spatiotemporal database is a database that manages both space and time information. Common examples include tracking of moving objects, intelligent transportation systems, cellular communications and meteorology monitoring. A spatiotemporal query determines the objects included in a region at a specified period of time between two date-time instants referred as time window. In the context of this work, we present SMaRT: A novel Spatiotemporal Mysql ReTrieval framework, based on MySQL and PostgreSQL database management system. Moreover, we propose a demo user interface that implements all of its capabilities, in order to help user determine the most efficient spatiotemporal query method on user-defined 2D trajectories. To our knowledge, we are the first to study and compare methods of addressing range queries on nonlinear moving object trajectories, that are represented both in dual and native dimensional space. In particular, it is the first time a theoretically efficient dual approach was implemented for nonlinear trajectories and incorporated into a well-known open-source RDBMS. An experimental evaluation is included that shows the performance and efficiency of our approach. 
105|-||Input-based adaptive randomized test case prioritization: A local beam search approach|Test case prioritization assigns the execution priorities of the test cases in a given test suite. Many existing test case prioritization techniques assume the full-fledged availability of code coverage data, fault history, or test specification, which are seldom well-maintained in real-world software development projects. This paper proposes a novel family of input-based local-beam-search adaptive-randomized techniques. They make adaptive tree-based randomized explorations with a randomized candidate test set strategy to even out the search space explorations among the branches of the exploration trees constructed by the test inputs in the test suite. We report a validation experiment on a suite of four medium-size benchmarks. The results show that our techniques achieve either higher APFD values than or the same mean APFD values as the existing code-coverage-based greedy or search-based prioritization techniques, including Genetic, Greedy and ART, in both our controlled experiment and case study. Our techniques are also significantly more efficient than the Genetic and Greedy, but are less efficient than ART. 
105|-||Design and programming patterns for implementing usability functionalities in web applications|Usability is a software system quality attribute. There are usability issues that have an impact not only on the user interface but also on the core functionality of applications. In this paper, three web applications were developed to discover patterns for implementing two usability functionalities with an impact on core functionality: Abort Operation and Progress Feedback. We applied an inductive process in order to identify reusable elements to implement the selected functionalities. For communication purposes, these elements are specified as design and programming patterns (PHP, VB.NET and Java). Another two web applications were developed in order to evaluate the patterns. The evaluation explores several issues such as ease of pattern understanding and ease of pattern use, as well as the final result of the applications.We found that it is feasible to reuse the identified solutions specified as patterns. The results also show that usability functionalities have features, like the level of coupling with the application or the complexity of each component of the solution, that simplify or complicate their implementation. In this case, the Abort Operation functionality turned out to be more feasible to implement than the Progress Feedback functionality. 
106|-|http://www.sciencedirect.com/science/journal/01641212/106|Learning to detect representative data for large scale instance selection|Instance selection is an important data pre-processing step in the knowledge discovery process. However, the dataset sizes of various domain problems are usually very large, and some are even non-stationary, composed of both old data and a large amount of new data samples. Current algorithms for solving this type of scalability problem have certain limitations, meaning they require a very high computational cost over very large scale datasets during instance selection. To this end, we introduce the ReDD (Representative Data Detection) approach, which is based on outlier pattern analysis and prediction. First, a machine learning model, or detector, is used to learn the patterns of (un)representative data selected by a specific instance selection method from a small amount of training data. Then, the detector can be used to detect the rest of the large amount of training data, or newly added data. We empirically evaluate ReDD over 50 domain datasets to examine the effectiveness of the learned detector, using four very large scale datasets for validation. The experimental results show that ReDD not only reduces the computational cost nearly two or three times by three baselines, but also maintains the final classification accuracy. 
106|-||Engineering Future Internet applications: The Prime approach|The Future Internet is envisioned as a worldwide environment connecting a large open-ended collection of heterogeneous and autonomous resources, namely Things, Services and Contents, which interact with each other anywhere and anytime. Applications will possibly emerge dynamically as opportunistic aggregation of resources available at a given time, and will be able to self-adapt according to the environment dynamics. In this context, engineers should be provided with proper modeling and programming abstractions to develop applications able to benefit from Future Internet, by being at the same time fluid, as well as dependable. Indeed, such abstractions should (i) facilitate the development of autonomous and independent interacting resources (loose coupling), (ii) deal with the run-time variability of the application in terms of involved resources (flexibility), (iii) provide mechanisms for run-time resources discovery and access (dynamism), and (iv) enable the running application to accommodate unforeseen resources (serendipity).To this end, Prime (P-Rest at design/run tIME) defines the P-REST architectural style, and a set of P-REST oriented modeling and programming abstractions to provide engineers with both design-time and run-time support for specifying, implementing and operating P-RESTful applications. 
106|-||Information infrastructure risk prediction through platform vulnerability analysis|The protection of information infrastructures is important for the function of other infrastructure sectors. As vital parts for the information infrastructure operation, software-based platforms, face a series of vulnerabilities and threats. This paper aims to provide a complementary approach to existing vulnerability prediction solutions and launch the measurement of zero-day risk by introducing a risk prediction methodology for an information infrastructure. The proposed methodology consists of four steps and utilizes the outcomes of a proper analysis of security measurements provided by specifications from the Security Content Automation Protocol. First, we identify software platform assets that support an information infrastructure and second we measure the historical rate of vulnerability occurrences. Third, we use a distribution fitting procedure to estimate the statistical correlation between empirical and reference probability distributions and verify the statistical significance of the distribution fitting results with the Kolmogorov–-Smirnov test. Fourth, we develop conditional probability tables that constitute a Bayesian Belief Network topology as means to enable risk prediction and estimation on security properties. The practicality of the risk prediction methodology is demonstrated with an implementation example from the electronic banking sector. The contribution of the proposed methodology is to provide auditors with a proactive approach about zero-day risks. 
106|-||Safe evolution templates for software product lines|Software product lines enable generating related software products from reusable assets. Adopting a product line strategy can bring significant quality and productivity improvements. However, evolving a product line can be risky, since it might impact many products. When introducing new features or improving its design, it is important to make sure that the behavior of existing products is not affected. To ensure that, one usually has to analyze different types of artifacts, an activity that can lead to errors. To address this issue, in this work we discover and analyze concrete evolution scenarios from five different product lines. We discover a total of 13 safe evolution templates, which are generic transformations that developers can apply when evolving compositional and annotative product lines, with the goal of preserving the behavior of existing products. We also evaluate the templates by analyzing the evolution history of these product lines. In this evaluation, we observe that the templates can address the modifications that developers performed in the analyzed scenarios, which corroborates the expressiveness of our template set. We also observe that the templates could also have helped to avoid the errors that we identified during our analysis. 
106|-||A large-scale study on the usage of Javaâs concurrent programming constructs|In both academia and industry, there is a strong belief that multicore technology will radically change the way software is built. However, little is known about the current state of use of concurrent programming constructs. In this work we present an empirical work aimed at studying the usage of concurrent programming constructs of 2227 real world, stable and mature Java projects from SourceForge. We have studied the usage of concurrent techniques in the most recent versions of these applications and also how usage has evolved along time. The main findings of our study are: (I) More than 75% of the latest versions of the projects either explicitly create threads or employ some concurrency control mechanism. (II) More than half of these projects exhibit at least 47 synchronized methods and 3 implementations of the Runnable interface per 100,000 LoC, which means that not only concurrent programming constructs are used often but they are also employed intensively. (III) The adoption of the java.util.concurrent library is only moderate (approximately 23% of the concurrent projects employ it). (IV) Efficient and thread-safe data structures, such as ConcurrentHashMap, are not yet widely used, despite the fact that they present numerous advantages. 
106|-||An exploratory study on exception handling bugs in Java programs|Most mainstream programming languages provide constructs to throw and to handle exceptions. However, several studies argue that exception handling code is usually of poor quality and that it is commonly neglected by developers. Moreover, it is said to be the least understood, documented, and tested part of the implementation of a system. Nevertheless, there are very few studies that analyze the actual exception handling bugs that occur in real software systems or that attempt to understand developers’ perceptions of these bugs. In this work we present an exploratory study on exception handling bugs that employs two complementary approaches: a survey of 154 developers and an analysis of 220 exception handling bugs from the repositories of Eclipse and Tomcat.Only 27% of the respondents claimed that policies and standards for the implementation of error handling are part of the culture of their organizations. Moreover, in 70% of the organizations there are no specific tests for the exception handling code. Also, 61% of the respondents stated that no to little importance is given to the documentation of exception handling in the design phase of the projects with which they are involved. In addition, about 40% of the respondents consider the quality of exception handling code to be either good or very good and only 14% of the respondents consider it to be bad or very bad. Furthermore, the repository analysis has shown (with statistical significance) that exception handling bugs are ignored by developers less often than other bugs. We have also observed that while overly general catch blocks are a well-known bad smell related to exceptions, bugs stemming from these catch blocks are rare, even though many overly general catch blocks occur in the code. Furthermore, while developers often mention empty catch blocks as causes of bugs they have fixed in the past, we found very few bug reports caused by them. On top of that, empty catch blocks are frequently used as part of bug fixes, including fixes for exception handling bugs.Based on our findings, we propose a classification of exception handling bugs and their causes. The proposed classification can be used to assist in the design and implementation of test suites, to guide code inspections, or as a basis for static analysis tools. 
106|-||Automated analysis of security requirements through risk-based argumentation|Computer-based systems are increasingly being exposed to evolving security threats, which often reveal new vulnerabilities. A formal analysis of the evolving threats is difficult due to a number of practical considerations such as incomplete knowledge about the design, limited information about attacks, and constraints on organisational resources. In our earlier work on RISA (RIsk assessment in Security Argumentation), we showed that informal risk assessment can complement the formal analysis of security requirements. In this paper, we integrate the formal and informal assessment of security by proposing a unified meta-model and an automated tool for supporting security argumentation called OpenRISA. Using a uniform representation of risks and arguments, our automated checking of formal arguments can identify relevant risks as rebuttals to those arguments, and identify mitigations from publicly available security catalogues when possible. As a result, security engineers are able to make informed and traceable decisions about the security of their computer-based systems. The application of OpenRISA is illustrated with examples from a PIN Entry Device case study. 
106|-||The discourse on tool integration beyond technology, a literature survey|The tool integration research area emerged in the 1980s. This survey focuses on those strands of tool integration research that discuss issues beyond technology.We reveal a discourse centered around six frequently mentioned non-functional properties. These properties have been discussed in relation to technology and high level issues. However, while technical details have been covered, high level issues and, by extension, the contexts in which tool integration can be found, are treated indifferently. We conclude that this indifference needs to be challenged, and research on a larger set of stakeholders and contexts initiated.An inventory of the use of classification schemes underlines the difficulty of evolving the classical classification scheme published by Wasserman. Two frequently mentioned redefinitions are highlighted to facilitate their wider use.A closer look at the limited number of research methods and the poor attention to research design indicates a need for a changed set of research methods. We propose more critical case studies and method diversification through theory triangulation.Additionally, among disparate discourses we highlight several focusing on standardization which are likely to contain relevant findings. This suggests that open communities employed in the context of (pre-)standardization could be especially important in furthering the targeted discourse. 
106|-||Feature extraction approaches from natural language requirements for reuse in software product lines: A systematic literature review|Requirements for implemented system can be extracted and reused for a production of a new similar system. Extraction of common and variable features from requirements leverages the benefits of the software product lines engineering (SPLE). Although various approaches have been proposed in feature extractions from natural language (NL) requirements, no related literature review has been published to date for this topic. This paper provides a systematic literature review (SLR) of the state-of-the-art approaches in feature extractions from NL requirements for reuse in SPLE. We have included 13 studies in our synthesis of evidence and the results showed that hybrid natural language processing approaches were found to be in common for overall feature extraction process. A mixture of automated and semi-automated feature clustering approaches from data mining and information retrieval were also used to group common features, with only some approaches coming with support tools. However, most of the support tools proposed in the selected studies were not made available publicly and thus making it hard for practitioners’ adoption. As for the evaluation, this SLR reveals that not all studies employed software metrics as ways to validate experiments and case studies. Finally, the quality assessment conducted confirms that practitioners’ guidelines were absent in the selected studies. 
106|-||Toward the tools selection in model based system engineering for embedded systemsâA systematic literature review|Model based system engineering (MBSE) is a systematic approach of modeling which is frequently used to support requirement specification, design, verification and validation activities of system development. However, it is difficult to customize MBSE approach for the development of embedded systems due to their diverse behavioral aspects. Furthermore, appropriate tools selection to perform particular MBSE activities is always challenging. This paper focuses on the identification and classification of recent research practices pertaining to embedded systems development through MBSE approach. Consequently, a comprehensive analysis of various MBSE tools has been presented. Systematic literature review (SLR) has been used to identify 61 research practices published during 2008–2014. The identified researches have been classified into six different categories to analyze various aspects of MBSE approach for embedded systems. Consequently, 39 preliminary tools are identified that have been used in recent researches. Furthermore, classification and evaluation of tools have been presented. This research highlights important trends and approaches of MBSE to support development of embedded systems. A comprehensive investigation of tools in this article facilitates researchers, practitioners and developers to select appropriate tools according to their requirements. 
107|-|http://www.sciencedirect.com/science/journal/01641212/107|An experimental investigation on the innate relationship between quality and refactoring|Previous studies have investigated the reasons behind refactoring operations performed by developers, and proposed methods and tools to recommend refactorings based on quality metric profiles, or on the presence of poor design and implementation choices, i.e., code smells. Nevertheless, the existing literature lacks observations about the relations between metrics/code smells and refactoring activities performed by developers. In other words, the characteristics of code components increasing/decreasing their chances of being object of refactoring operations are still unknown. This paper aims at bridging this gap. Specifically, we mined the evolution history of three Java open source projects to investigate whether refactoring activities occur on code components for which certain indicators—such as quality metrics or the presence of smells as detected by tools—suggest there might be need for refactoring operations. Results indicate that, more often than not, quality metrics do not show a clear relationship with refactoring. In other words, refactoring operations are generally focused on code components for which quality metrics do not suggest there might be need for refactoring operations. Finally, 42% of refactoring operations are performed on code entities affected by code smells. However, only 7% of the performed operations actually remove the code smells from the affected class. 
107|-||Behavioral software engineering: A definition and systematic literature review|Throughout the history of software engineering, the human aspects have repeatedly been recognized as important. Even though research that investigates them has been growing in the past decade, these aspects should be more generally considered.The main objective of this study is to clarify the research area concerned with human aspects of software engineering and to create a common platform for future research. In order to meet the objective, we propose a definition of the research area behavioral software engineering (BSE) and present results from a systematic literature review based on the definition.The result indicates that there are knowledge gaps in the research area of behavioral software engineering and that earlier research has been focused on a few concepts, which have been applied to a limited number of software engineering areas. The individual studies have typically had a narrow perspective focusing on few concepts from a single unit of analysis. Further, the research has rarely been conducted in collaboration by researchers from both software engineering and social science.Altogether, this review can help put a broader set of human aspects higher on the agenda for future software engineering research and practice. 
107|-||The prospects of a quantitative measurement of agility: A validation study on an agile maturity model|Agile development has now become a well-known approach to collaboration in professional work life. Both researchers and practitioners want validated tools to measure agility. This study sets out to validate an agile maturity measurement model with statistical tests and empirical data. First, a pretest was conducted as a case study including a survey and focus group. Second, the main study was conducted with 45 employees from two SAP customers in the US. We used internal consistency (by a Cronbach’s alpha) as the main measure for reliability and analyzed construct validity by exploratory principal factor analysis (PFA). The results suggest a new categorization of a subset of items existing in the tool and provides empirical support for these new groups of factors. However, we argue that more work is needed to reach the point where a maturity models with quantitative data can be said to validly measure agility, and even then, such a measurement still needs to include some deeper analysis with cultural and contextual items. 
107|-||Modeling and verification of Functional and Non-Functional Requirements of ambient Self-Adaptive Systems|Self-Adaptive Systems modify their behavior at run-time in response to changing environmental conditions. For these systems, Non-Functional Requirements play an important role, and one has to identify as early as possible the requirements that are adaptable. We propose an integrated approach for modeling and verifying the requirements of Self-Adaptive Systems using Model Driven Engineering techniques. For this, we use Relax, which is a Requirements Engineering language which introduces flexibility in Non-Functional Requirements. We then use the concepts of Goal-Oriented Requirements Engineering for eliciting and modeling the requirements of Self-Adaptive Systems. For properties verification, we use OMEGA2/IFx profile and toolset. We illustrate our proposed approach by applying it on an academic case study. 
107|-||A UML model-based approach to detect infeasible paths|UML model-based analysis is gaining wide acceptance for its cost effectiveness and lower overhead for processing compared to code-based analysis. A possible way to enhance the precision of the results of UML based analysis is by detecting infeasible paths in UML models. Our investigation reveals that two interaction patterns called Null Reference Check (NLC) and Mutually Exclusive (MUX) can cause a large number of infeasible paths in UML sequence diagrams. To detect such infeasible paths, we construct a graph model (called SIG), generate MM paths from the graph model, where an MM path refers to an execution sequence of model elements from the start to end of a method scope. Subsequently, we determine infeasibility of the MM paths with respect to MUX and NLC patterns. Our proposed model-based approach is useful to help exclude generation of test cases and test data for prior-detected infeasible paths, refine test effort estimation, and facilitate better test planning in the early stages of software development life cycle. 
107|-||Entity resolution based EM for integrating heterogeneous distributed probabilistic data|Distributed computing is linked and equated to the industrial revolution. Its transformational nature is, however, associated with significant instances in the form of internet of thing operations. Entity resolution (ER) is a problem of matching and resolving records that represent the same real world entity. This is a long-standing challenge in distributed databases and information retrieval as a statistic. In a centralized approach, the problem of ER has not been scaled well as large amount of data need to be sent to a central node. In this paper, we present an algorithm which deals with heterogeneous distributed probabilistic data (HDPD) and also reduces processing time in a distributed environment. We propose two different approaches. First, we explore this instance with a matching (identification) problem to integrate different data models with expectation–maximization (EM) algorithm. Second, we apply ER methodology for HDPD to achieve major performance in terms of response time to produce the outcome. We validate HDPD through experiments over a 100-node cluster that records significant performance improvements over naive approaches. This paper is expected to provide insights in to database organizations and new technological development for the growth of distributed environment. 
107|-||A comprehensive modeling framework for role-based access control policies|Prohibiting unauthorized access to critical resources and data has become a major requirement for enterprises; access control (AC) mechanisms manage requests from users to access system resources. One of the most used AC paradigms is role-based access control (RBAC), in which access rights are determined based on the user’s role.Many different types of RBAC policies have been proposed in the literature, each one accompanied by the corresponding extension of the original RBAC model. However, there is no unified framework that can be used to define all these types of policies in a coherent way, using a common model.In this paper we propose a model-driven engineering approach, based on UML and the Object Constraint Language (OCL), to enable the precise specification and verification of such policies. More specifically, we first present a taxonomy of the various types of RBAC policies proposed in the literature. We also propose the GemRBAC model, a generalized model for RBAC that includes all the entities required to define the classified policies. This model is a conceptual model that can also serve as data model to operationalize data collection and verification. Lastly, we formalize the classified policies as OCL constraints on the GemRBAC model. 
107|-||Service deployment strategies for efficient execution of composite SaaS applications on cloud platform|Cloud computing has caused a revolution in our way of developing and using software. Software development and deployment based on the new models of Software as a Service (SaaS) and Service-Oriented Architecture (SOA) are expected to bring a lot of benefits for users. However, software developers and service providers have to address new challenging issues before such benefits can be realized. This paper explores one of the critical issues, service deployment, for reducing execution time of composite SaaS applications, and proposes an integrated approach to the service deployment problem which takes not only inter-service communication costs but also the potential parallelism among services into consideration. In the approach, two types of graphs are developed to model the communication costs between services, Service Dependency Graph (SDG), and potential parallelism among services, Service Concurrence Graph (SCG), respectively. Then, these two graphs are integrated into a single Service Relationship Graph (SRG) and the service deployment problem is transformed into a minimum k-cut problem for solution. A series of experiments were conducted to evaluate the proposed approach. The experimental results indicate that our approach outperforms previous deployment methods significantly in terms of service response time. 
107|-||An automated approach for noise identification to assist software architecture recovery techniques|Software systems’ concrete architecture often drifts from the intended architecture throughout their evolution. Program comprehension activities, like software architecture recovery, become very demanding, especially for large and complex systems due to the existence of noise, which is created by omnipresent and utility classes that obscure the system structure. Omnipresent classes represent crosscutting concerns, utilities or elementary domain concepts. The identification and filtering of noise is a necessary preprocessing step before attempting program comprehension techniques, especially for undocumented systems. In this paper, we propose an automated methodology for noise identification. Our methodology is based on the notion that noisy classes are widely used in a system, directly or indirectly. We combine classes’ usage significance with their participation in the system’s subgraphs, in order to identify the classes that are persistently used. Usage significance is measured according to Component Rank, a well-established metric in the literature, which ranks software artifacts according to their usage significance. The experimental results show that the proposed methodology successfully captures classes that produce noise and improves the results of existing algorithms for software systems’ architectural decomposition. 
107|-||Architectural tactics for cyber-foraging: Results of a systematic literature review|Mobile devices have become for many the preferred way of interacting with the Internet, social media and the enterprise. However, mobile devices still do not have the computing power and battery life that will allow them to perform effectively over long periods of time, or for executing applications that require extensive communication, computation, or low latency. Cyber-foraging is a technique to enable mobile devices to extend their computing power and storage by offloading computation or data to more powerful servers located in the cloud or in single-hop proximity. This article presents the results of a systematic literature review (SLR) on architectures that support cyber-foraging. Elements of the identified architectures were codified in the form of Architectural Tactics for Cyber-Foraging. These tactics will help architects extend their design reasoning toward cyber-foraging as a way to support the mobile applications of the present and the future. 
107|-||A small world based overlay network for improving dynamic load-balancing|Load-balancing algorithms play a key role in improving the performance of distributed-computing-systems that consist of heterogeneous nodes with different capacities. The performance of load-balancing algorithms and its convergence-rate deteriorate as the number-of-nodes in the system, the network-diameter, and the communication-overhead increase. Moreover, the load-balancing technical-factors significantly affect the performance of rebalancing the load among nodes. Therefore, we propose an approach that improves the performance of load-balancing algorithms by considering the load-balancing technical-factors and the structure of the network that executes the algorithm. We present the design of an overlay network, namely, functional small world (FSW) that facilitates efficient load-balancing in heterogeneous systems. The FSW achieves the efficiency by reducing the number-of-nodes that exchange their information, decreasing the network diameter, minimizing the communication-overhead, and decreasing the time-delay results from the tasks re-migration process. We propose an improved load-balancing algorithm that will be effectively executed within the constructed FSW, where nodes consider the capacity and calculate the average effective-load. We compared our approach with two significant diffusion methods presented in the literature. The simulation results indicate that our approach considerably outperformed the original neighborhood approach and the nearest neighbor approach in terms of response time, throughput, communication overhead, and movements cost. 
107|-||An empirically-developed framework for Agile transition and adoption: A Grounded Theory approach|To date, few Agile transition and adoption frameworks have been proposed in the software industry. However, using them is not easy in practice and primarily requires a huge organizational overhead because of their complex and non-flexible structure. These drawbacks make such frameworks difficult to apply in small and medium-sized companies. We have conducted a large-scale empirical research study using Grounded Theory approach with the participation of 49 Agile experts from 13 different countries. This study inductively developed a substantive Agile transition and adoption framework which appears to be simple and flexible. The main aim of this paper is to present the developed framework. The primary characteristics of this framework, including iterative, gradual, continuous, and value-based are in line with the Agile approach and show promise of being useful in software companies and organizations, regardless of size. This paper also describes how various steps of this framework could help software companies to achieve Agile transformation. 
