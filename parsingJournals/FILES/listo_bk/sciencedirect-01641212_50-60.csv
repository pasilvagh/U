volume|issue|url|title|abstract
50|-|http://www.sciencedirect.com/science/journal/01641212/50|The âmaintenance-firstâ software era|
50|-||Domain analysis for software reuse|A theory of domain knowledge is proposed that consists of `grounded domains' that model a set of cooperating objects that achieve a purpose. Grounded domains have spatial presence in the real world and contain agents that act on objects within a context of structures. More complex meta-domains use grounded domains as their subject matter and describe education, management, etc. The third component of the theory, generic tasks, describes problem solving activity such as diagnosis, searching, planning and scheduling. Generic tasks describe the behavioural components in both grounded and meta-domains. The reusable library of generic models is applied to the design of interactive systems by reusing the models as templates, and to reuse design knowledge in the form of associated design rationale. A process for recognising generic models is described with recognition heuristics structured in a walkthrough type of analysis for identifying key abstractions in new applications. The design process is illustrated with an information retrieval case study developed as a decision support system for emergency management, reusing information searching services. The discussion reviews the prospects for reusable patterns in interactive systems design, and similar approaches in software and knowledge engineering. 
50|-||A problem-oriented and rule-based component repository|Repository-based component reuse is an important technique for raising the efficiency of software development. Traditional approaches focus only on repository organisation and retrieval techniques. This paper proposes a problem-oriented component reuse framework through incorporating a problem-solving mechanism with the traditional component repository. The framework improves the traditional approaches in two aspects. Firstly, the reuse is promoted from the component level to the problem-solving level. User is encouraged to concentrate on the problem description and solving problem through top-down refinement rather than plunge into the technique details at the beginning. Secondly, the environment evolution during running the candidate components for composing an application is simulated through case-based rule reasoning. The candidate components are dynamically checked before composing an application system. We have designed a model base system for the application in scientific computing field based on the framework. The application and analysis show the approach feasible. 
50|-||An architecture for software that adapts to changes in requirements|The goal of the research presented in this paper was to study a new software paradigm – adaptive software – in which the structure of an adaptive program is patterned upon the structure of an adaptive controller. Towards this aim, we implemented a domain-specific (object/target recognition) program (A Reconfigurable Architecture for Adapting to Changes in the Requirements (RAACR)) that can adapt to changes in software requirements through the incorporation of feedback. RAACR is a hierarchy of domains (blackboards). Each domain includes multiple knowledge sources (KSs) and a domain scheduler (DS). In response to feedback, KSs change their processing parameters, while DSs change the scheduling policy of the KSs. A generic communication mechanism is implemented on the CORBA compliant SPRING operating system. The adaptability of the program is evaluated quantitatively using a requirements volatility measure and the probability of correct recognition. 
50|-||The impact of software architecture reuse on development processes and standards|We have developed an architecture for distributed simulations with reuse in mind from the start. During the past six years, two teams on five projects at US Army Tank-Automotive and Armaments Command (TACOM) have used it. We found that in order to reuse the architecture, it was not only necessary to design it for reuse but also to develop and provide associated development processes and standards for reuse. Not only its functionality for the task to be performed, but also its resultant impact on the software development process determine acceptance of an architecture for reuse. Software reuse is similar to both software tool assessment/technology transfer and requirements analysis in that in all three cases one needs to assess not only product functionality but also its resultant impact on the way that the users of the product work. The DoD is now mandating the use of the High Level Architecture Standard (HLA) for distributed military simulations. Because of our experience in architecture reuse, we realize that HLA adoption will impact not only our product, but also the way we work. In addition to examining HLA functionality, we are assessing HLA's impact on our existing development processes and standards. 
50|-||State restoration in Ada 95: a portable approach to supporting software fault tolerance|Studies indicate that techniques for tolerating hardware faults are so effective that software designerrors are the leading cause of all faults encountered. To handle these unanticipated software faults, two main approaches have been proposed: N-versionprogramming and recoveryblocks. Both are based on the concept of designdiversity: the assumption that different designs will exhibit different faults (if any) for the same inputs and will, therefore, provide alternatives for each other. Both approaches have advantages, but this paper focuses upon recovery blocks; specifically, the requirement to save and restore application state. Judicious saving of state has been described as “checkpointing” for over a decade. Using the object-oriented features of the revised Ada language (Ada 95) – a language widely used in this domain – we present three portable implementations of a checkpointing facility and discuss the trade-offs offered by each. Results of the implementation of these mechanisms are used to highlight both the strengths and weaknesses of some of the object-oriented features of Ada. We then show a reusable implementation of recovery blocks illustrating the checkpointing schemes. A performance analysis is made and measurements are presented in support of the analysis. 
50|1|http://www.sciencedirect.com/science/journal/01641212/50/1|Y2K behind us: smooth sailing and blue skies?|
50|1||An encompassing life cycle centric survey of software inspection|This paper contributes an integrated survey of the work in the area of software inspection. It consists of two main sections. The first one introduces a detailed description of the core concepts and relationships that together define the field of software inspection. The second one elaborates a taxonomy that uses a generic development life-cycle to contextualize software inspection in detail.After Fagan's seminal work presented in 1976, the body of work in software inspection has greatly increased and reached measured maturity. Yet, there is still no encompassing and systematic view of this research body driven from a life-cycle perspective. This perspective is important since inspection methods and refinements are most often aligned to particular life-cycle artifacts. It also provides practitioners with a roadmap available in their terms.To provide a systematic and encompassing view of the research and practice body in software inspection, the contribution of this survey is, in a first step, to introduce in detail the core concepts and relationships that together embody the field of software inspection. This lays out the field key ideas and benefits and elicits a common vocabulary. There, we make a strong effort to unify the relevant vocabulary used in available literature sources. In a second step, we use this vocabulary to build a contextual map of the field in the form of a taxonomy indexed by the different development stages of a generic process. This contextual map can guide practitioners and focus their attention on the inspection work most relevant to the introduction or development of inspections at the level of their particular development stage; or to help motivate the use of software inspection earlier in their development cycle.Our work provides three distinct, practical benefits: First, the index taxonomy can help practitioners identify inspection experience directly related to a particular life-cycle stage. Second, our work allows structuring of the large amount of published inspection work. Third, such taxonomy can help researchers compare and assess existing inspection methods and refinements to identify fruitful areas of future work. 
50|1||Software management and cost estimating error|Software cost estimating is an important concern for software managers and other software professionals. The hypothesized model in this research suggests that an organization's use of an estimate influences its estimating practices which influence both the basis of the estimating process and the accuracy of the estimate. The model also suggests that the estimating basis directly influences the accuracy of the estimate. A study of business information systems managers and professionals at 112 different organizations refined the model.The refined model shows that no managerial practice in this study discourages the use of intuition, guessing, and personal memory in cost estimating. Although user commitment and accountability appear to foster algorithm-based estimating, such an algorithmic basis does not portend greater accuracy. Only one managerial practice – the use of the estimate in performance evaluations of software managers and professionals – presages greater accuracy. By implication, the research suggests somewhat ironically that the most effective approach to improve estimating accuracy may be to make estimators, developers, and managers more accountable for the estimate even though it may be impossible to direct them explicitly on how to produce a more accurate one. 
50|1||An analysis of factors affecting software reliability|This paper presents the findings of empirical research from 13 companies participating in software development to identify the factors that may impact software reliability. Thirty-two potential factors involved in every stage of the software development process are defined. The study uses a survey instrument to analyze these factors and identify factors that have significant impact on software reliability. The survey focuses on the perspective of the primary participants, managers, system engineers, programmers, testers and other people involved in software research or development teams. Two techniques such as the relative weight method and analysis of variance technique (ANOVA) have been used to analyze all factors and rank them in terms of their impact on software reliability. The research findings have important implications for further research and the practice of software development. For researchers, it points to improvement schemes of existing reliability modeling and factors that may be further verified and extended in subsequent research. For practitioners, it provides a general guide to the important aspects to consider in the whole software development process. 
50|1||Module interconnection features in object-oriented development tools|The black-box reuse of library classes in the construction of an object-oriented (OO) application is difficult: the principle of information hiding may be violated if classes must know their partners, and code must be typically rewritten. One of the possible ways to increase class decoupling and thus reuse is to employ interconnection languages to describe OO architectures. In this paper we present a decoupling paradigm for individual classes or class collections that facilitates reuse, introducing interconnection features at the design and programming levels. We give examples in a new, second generation OO development system, using asynchronous messages sent to generically identified receivers. 
50|1||Reading between the lines: an examination of systems analysis and design texts|Systems analysis and design is a key component of most information systems academic programs. One of the primary sources of information provided in such courses is the textbook. The purpose of the study is three-fold: first, to determine on an individual text level, the amount of coverage of various tasks, activities and tools; second, to identify which texts capture the largest percentage of the topics being studied; and lastly, to compare the overall rankings of the tasks, activities and tools to those of educators and practitioners. This paper discusses the results of this content analysis, and their implications for IS educators. 
50|1||On the concept of coupling, its modeling and measurement|Although measurement is of importance, the difficulty to develop sound measures should not be underestimated. If a quality we want to assess is ambiguous, it is unlikely to develop reliable measures. For the fundamental concept of coupling, we reveal that there is a big difference between the commonly perceived essence of coupling and its usual definition. We show also that the classification of five coupling levels ranging from data, stamp to content coupling is overly simplified and inconsistent.To avoid further confusion, the concept of module coupling is redefined relying on its essence. By definition, we identify three basic attributes that cover all the aspects of coupling, enabling us to generalize the traditional classification of coupling, and remove the inconsistency. For modeling module coupling, an assumption about modules is introduced. Based on the assumption, we derive rigorously a mathematical model for measuring module coupling. We then demonstrate that the inter-module part of information flow metric cannot be valid theoretically. We show further that the almost unanimously recognized coupling ranking are not reliable, whether it is reckoned as a ordinal or interval scale.Our aim is not restricted to develop one measure more for module coupling, but to clarify confusion on the concept and problems relating to its heuristics, illustrate how to develop rigorously measures based on a clear concept and modeling and finally demonstrate its usefulness. We hope the methodological appraisal presented in this paper is illuminating and could furnish a lesson for software measurement research. 
50|1||Specifying and verifying real-time systems with timing uncertainty|Modeling timing behaviors of systems and verifying timing constraints against the model are major tasks in developing real-time systems. However, it is recognized to be extremely difficult to predict the timing behavior of systems precisely in the requirements phase. Timing uncertainty, if not considered properly in the modeling and verifying steps, may incur subtle, yet critical errors in final products. We propose a method of dealing with such timing uncertainty using an extended duration calculus formalism. The extended duration calculus, named fuzzy duration calculus, allows us to specify uncertain timing behavior of the system using the fuzzy theory. Semantics and proof system of the duration calculus are redefined in order to accommodate fuzzy concepts. Based on the semantics and proof system, we can verify timing constraints against the system model. A portion of nuclear power plant (NPP) control system is employed to demonstrate the feasibility and the effectiveness of our approach. 
50|2|http://www.sciencedirect.com/science/journal/01641212/50/2|The CMMI: it's formidable|
50|2||ElGamal-like digital signature and multisignature schemes using self-certified public keys|In this paper, we first present a digital signature scheme using self-certified public key. Subsequently, we present a digital multisignature scheme extended from the proposed digital signature. Both the proposed schemes have the advantage that the authentication of the public key can be accomplished with the verification of the signature or multisignature. The security of both the proposed schemes is based on the one-way hash function, the factorization and the discrete logarithm assumptions. Moreover, the proposed multisignature scheme preserves the main merits inherent in most of the previously developed schemes. We also demonstrate that the proposed multisignature scheme can withstand the active attack that some malicious impostors try to universally forge an individual signature or a multisignature for a given message that is rejected to be signed by the other co-signers. 
50|2||Robust transparent image watermarking system with spatial mechanisms|A very robust against JPEG compression, difficult for collusion attack, and transparent secret key digital watermarking system, which takes a spatial-domain approach, is presented in this paper. The unique watermark that consists of both the owner's logo and the user's licence number provides the ownership protection and traceability of illicit dissemination. The technique of block-oriented and modular-arithmetic–based watermark embedding and extraction achieves the robustness against image processing such as lossy compression, filtering, and cropping. The random perturbation of the pixels within the marked block as well as the unique and fused watermark information result in difference images with irregular-shape and uneven-luminance blobs, hence greatly increase the difficulty of collusion attack. Transparency is further fulfilled by means of embedding the information bits around but not on the exact edges of the features to avoid edge sharpening or smoothing, which effectively magnifies the contrast among the distributed images. The experimental results demonstrate that the proposed watermarking system is superior to Voyatazis and Pitas's scheme with respect to image processing, and is much more secure than the method of Cox et al.'s method. 
50|2||Towards integration of use case modelling and usage-based testing|This paper focuses on usage modelling as a basis for both requirements engineering (RE) and testing, and investigates the possibility of integrating the two disciplines of use case modelling (UCM) and statistical usage testing (SUT). The paper investigates the conceptual framework for each discipline, and discusses how they can be integrated to form a seamless transition from requirements models to test models for reliability certification. Two approaches for such an integration are identified: integration by model transformation and integration by model extension. The integration approaches are illustrated through an example, and advantages as well as disadvantages of each approach are discussed. Based on the fact that the two disciplines have models with common information and similar structure, it is argued that an integration may result in coordination benefits and reduced costs. Several areas of further research are identified. 
50|2||DHARMA: A tool for evaluating dynamic scheduling algorithms for real-time multiprocessor systems|A majority of today's real-time systems assume a priori knowledge of task characteristics and hence are based on static designs which contribute to their high cost and inflexibility. The next generation of hard real-time systems must be designed to be dynamic and flexible. This provides the motivation to study various dynamic scheduling proposals. In this paper, we evaluate new algorithms for scheduling and resource reclaiming in a dynamic multiprocessor system with fault-tolerant requirements. Resource reclaiming refers to the problem of utilizing resources left unused by a task when its actual computation time is less than its worst case computation time, or when a task is deleted in a fault-tolerant schedule. We also describe the design and implementation of a tool, called Dynamic (Heuristic) scheduling Algorithms for Real-time Multiprocessor systems (DHARMA), to study various dynamic scheduling algorithms, with or without fault-tolerance requirements, and associated resource reclaiming algorithms in a multiprocessor real-time system. 
50|2||The effect of compression on performance in a demand paging operating system|As engineers increase microprocessor speed, many traditionally computer-bound tasks are being transformed to input/output (I/O) bound tasks. Where processor performance had once been the primary bottleneck, I/O performance is now the primary inhibitor to faster execution. As the performance gap widens between processor and I/O, it is becoming more important to improve the efficiency of I/O in order to improve overall system performance. In a demand paging operating system, secondary memory is accessed during program execution when a page fault occurs. To improve program execution, it is increasingly important to decrease the amount of time required to process a page fault. This paper describes a compression format which is suitable for both pages of code and pages of data. We find that when the OS/2 operating system is modified to use this compression format, the time saved due to reduced I/O offsets the additional time required to decompress the page. 
volume|issue|url|title|abstract
51|-|http://www.sciencedirect.com/science/journal/01641212/51|Academics, and the scarlet letter `A'|
51|-||An impact factor model of Intranet adoption: an exploratory and empirical research|
51|-||Approaches for broadcasting temporal data in mobile computing systems|
51|-||An index replication scheme for wireless data broadcasting|In mobile distributed environment, data broadcasting has many applications because it has two desirable characteristics: energy efficiency and bandwidth efficiency. There have been some researches on the indexing mechanisms for wireless data broadcasting in the past. We first describe the problematic issue in the conventional index replication scheme, and then propose various index replication methods based on three criteria: accessibility, energy efficiency and adjacency. We evaluate the proposed scheme in analytic and experimental ways. We show that the proposed index replication scheme reduces the data access cost of mobile clients in an energy efficient manner. We also discuss the relaxation of the given criteria. 
51|-||TVIS: an interactive multimedia communication engine and its applications|
51|-||A multi-server video-on-demand system with arbitrary-rate playback support|
51|-||IDRS: an interactive digital radio station over Internet|
51|-||Exploring the relationships between design measures and software quality in object-oriented systems|
51|-||Corrigendum to: An assessment of systems and software engineering scholars and institutions (1994â1998) [The Journal of Systems and Software 49 (1) (1999) 81â86]|
51|1|http://www.sciencedirect.com/science/journal/01641212/51/1|The âsoftware-firstâ revolution in computer hardware design|
51|1||Cache affinity and resequencing in a shared-memory multiprocessing system|
51|1||An empirical incremental approach to tool evaluation and improvement|
51|1||A new routing control technique using active temporal data management|
51|1||Priority and deadline assignment to triggered transactions in distributed real-time active databases|
51|1||History-driven dynamic load balancing for recurring applications on networks of workstations|
51|1||A soft computing approach for recognition of occluded shapes|
51|2|http://www.sciencedirect.com/science/journal/01641212/51/2|Software requirements success predictors â behavioral factors beat technical ones|
51|2||Estimation support by lexical analysis of requirements documents|
51|2||Coupling and control flow measures in practice|
51|2||An empirical study of complexity metrics in Cobol programs|
51|2||Validating the ISO/IEC 15504 measures of software development process capability|
51|2||Improving the quality of the analysis phase|
52|1|http://www.sciencedirect.com/science/journal/01641212/52/1|On design|
52|1||Software development risks to project effectiveness|Controlling the risks to a software development project is a critical issue examined by information system researchers. Many of the studies and methods to date examine the risks from the broad aspect of system success but attempt to promote activities related to a narrow domain of risks. Our intent is to examine the impact of the spectrum of risks on different aspects of system development. We report the results of a survey of 86 project managers that indicate common aspects of project effectiveness are generally under control, but are most effected by lack of expertise on the project team. Significant relationships also show that lack of clear role definition and conflicts on the team are also elevated risks. Other items are not as critical or limited to a much smaller aspect of effectiveness than overall success. This focusing in on the more important risk aspects will allow for more effective management of the project and a narrowing of techniques to mitigate the significant risks. 
52|1||Breadth-first search oriented symbolic picture representation for spatial match retrieval|To provide sufficient information for percepting symbolic objects among pictures from pictorial information systems, an efficient spatial data structure is needed to identify objects and to specify the spatial relationships among objects in a picture. In this paper, a new symbolic picture representation is proposed through the traversal of a quadtree corresponding to a given picture using breadth-first search and thus is called BFS string. An iconic indexing scheme for retrieving symbolic pictures based on the proposed BFS strings is presented and the algorithms introduced in the bin-tree oriented iconic indexing scheme by Chang and Lee (1998b) [Journal of Data and Knowledge Engineering 26 (2)]. is applied to the scheme for manipulating spatial relationships among objects. Compared with the bin-tree oriented scheme in storage complexity, experimental results show that the proposed BFS string requires less storage space when the picture has a great number of objects. 
52|1||An on-line scheduling policy for IRIS real-time composite tasks|
52|1||Efficient run-time assurance in distributed systems through selection of executable assertions|
52|1||Data dependence analysis for array references|
52|1||Applying stack simulation for branch target buffers|
52|1||To Kanji or not to Kanji: a cognitive perspective|
52|2-3|http://www.sciencedirect.com/science/journal/01641212/52/2-3|Evaluation and assessment in software engineering|
52|2-3||Investigating principles of stakeholder evaluation in a modern IS development approach|
52|2-3||Quantitative analysis of static models of processes|
52|2-3||Evaluation of code review methods through interviews and experimentation|This paper presents the results of a study where the effects of introducing code reviews in an organisational unit have been evaluated. The study was performed in an ongoing commercial project, mainly through interviews with developers and an experiment where the effects of introducing code reviews were measured. Two different checklist-based review methods have been evaluated. The objectives of the study are to analyse the effects of introducing code reviews in the organisational unit and to compare the two methods. The results indicate that many of the faults that normally are found in later test phases or operation are instead found in code reviews, but no difference could be found between the two methods. The results of the study are considered positive and the organisational unit has continued to work with code reviews. 
52|2-3||The role of comprehension in software inspection|In spite of code inspections having been demonstrated as an effective defect detection process, little work has been done to determine how this process best supports the object-oriented paradigm. In contrast, this paradigm (or at least its questionable manifestation in C++) is well supported by tools that purport to aid comprehension. These tools typically take the form of visualisation tools designed to assist in the maintenance process, and it is natural to consider that these tools (or adaptations thereof) might also support inspection. However, since these tools claim to aid comprehension, it is important to consider the role of comprehension in inspection. Or put simply, does comprehension matter, or are there simple techniques in existence which are similarly effective in detecting defects? This paper presents the issues associated with inspections (and the complications presented by the object-oriented paradigm) and comprehension, and presents the results of two experiments which considered the relationship between comprehension and inspection. The results indicate a relationship, but further work is needed to determine the precise nature of this relationship and how inspections might best be supported in the future. 
52|2-3||A formative evaluation of information retrieval techniques applied to software catalogues|
52|2-3||Robust estimations of fault content with captureârecapture and detection profile estimators|
52|2-3||An evaluation of the business object approach to software development|In this paper, we report the result of an evaluation of the use of business objects and business components for developing business application software. This evaluation was a replicated product case study in which a part of an existing product was re-implemented using an UML-based development process. In order to assess the impact of re-use, a second related product was implemented using the new technology. We found that producing software from scratch using UML was less productive during the development lifecycle, but productivity improved when substantial reuse (48%) was achieved. Time to market was not much affected by the new technology but was greatly improved when substantial reuse was achieved. Defect rates appeared substantially lower for the new technology irrespective of reuse levels. The technology also had other benefits including provision of documentation and less reliance on individual members of staff. 
52|2-3||Coupling measures and change ripples in C++ application software|This paper describes an investigation into the effects of class couplings on changes made to a commercial C++ application over a period of  yr. The Chidamber and Kemerer CBO metric is used to measure class couplings within the application and its limitations are identified. Through an in-depth study of the ripple effects of changes to the source code, practical insight into the nature and extent of software couplings is provided. 
52|2-3||Dependability certification of software components|Software components need a uniform approach for rating their quality. The need for this stems from a licensee's inaccessibility to the source code as well as other information concerning how thoroughly the component was validated. This paper proposes a `test quality rating' (TQR) metric that will act as a component's dependability `score'. We envision a process whereby a software publisher submits a component to an independent certification organization that would then calculate TQR for that component. It is preferable for component dependability validation to be performed by an independent organization. This is because even an honest dependability overestimation error on the part of the publisher could be grounds for severe legal penalties. This score would be displayed on any marketing materials or contracts which license that component. In our paper we provides results from applying the metric to a commercial financial application written in Java in order to demonstrate the effectiveness of the metric. 
52|2-3||Experimental assessment of the effect of inheritance on the maintainability of object-oriented systems|
53|1|http://www.sciencedirect.com/science/journal/01641212/53/1|Empirical studies of software development and evolution|
53|1||Towards a framework for empirical assessment of changeability decay|Evolutionary development allows early and frequent adaptations to new or changed requirements. However, such unanticipated changes may invalidate design documentation and cause structural degradations of the software, which in turn may accelerate changeabilitydecay. Our definition of changeability decay focuses on the increased effort required to implement changes. We have identified three approaches to the assessment of changeability decay: (1) structure measurement, (2) change complexity measurement, and (3) benchmarking. Our research aims to evaluate and compare these approaches in order to develop an empirical assessment framework. In this paper we propose a set of change complexity measures (2) and compare them with structural attribute measures (1) using detailed process and product data collected from a commercial object-oriented development project. The preliminary results indicate that the change complexity measures capture some dimensions of changeability decay not accounted for with structural attribute measures. However, the current findings also suggest that many aspects of changeability decay cannot be accounted for by the indirect measures utilized in approach (1) and (2). As an alternative approach, we therefore propose using benchmarks (3) where change effort can be measured more directly. A research methodology for the development of benchmarks and benchmarking procedures are described. 
53|1||Use of friends in C++ software: an empirical investigation|
53|1||An investigation of machine learning based prediction systems|Traditionally, researchers have used either off-the-shelf models such as COCOMO, or developed local models using statistical techniques such as stepwise regression, to obtain software effort estimates. More recently, attention has turned to a variety of machine learning methods such as artificial neural networks (ANNs), case-based reasoning (CBR) and rule induction (RI). This paper outlines some comparative research into the use of these three machine learning methods to build software effort prediction systems. We briefly describe each method and then apply the techniques to a dataset of 81 software projects derived from a Canadian software house in the late 1980s. We compare the prediction systems in terms of three factors: accuracy, explanatory value and configurability. We show that ANN methods have superior accuracy and that RI methods are least accurate. However, this view is somewhat counteracted by problems with explanatory value and configurability. For example, we found that considerable effort was required to configure the ANN and that this compared very unfavourably with the other techniques, particularly CBR and least squares regression (LSR). We suggest that further work be carried out, both to further explore interaction between the end-user and the prediction system, and also to facilitate configuration, particularly of ANNs. 
53|1||Empirical analysis in software process simulation modeling|
53|1||Using evolution to evaluate reverse engineering technologies: mapping the process of software change|
53|1||Priorities for the education and training of software engineers|We present the complete results of our 1998 survey of software practitioners. In this survey we asked over 200 software developers and managers from around the world what they thought about 75 educational topics. For each topic, we asked them how much they had learned about it in their formal education, how much they know about it now and how important the topic has been in their career. The objective of the survey was to provide data that can be used to improve the education and training of information technology workers. The results suggest that some widely taught topics perhaps should be taught less, while coverage of other topics should be increased. 
53|1||Optimal binary vote assignment for replicated data|Data replication can be used to improve the availability of data in a distributed database system. In such a system, a mechanism is required to maintain the consistency of the replicated data. Weighted voting is a popular solution for maintaining the consistency. The performance of a voting system is determined by the vote assignment. It was shown in previous studies that finding the optimal solution among nonnegative integer vote assignments requires O(2N2) time. So it is not suitable to find the optimal integer vote assignment (OIVA) for large systems. In this paper, we propose the optimal binary vote assignment (OBVA) considering only binary vote assignments. It is shown that OBVA can achieve nearly the same availability of OIVA. On the other hand, OBVA requires only O(N2) time, which is preferred for large systems. 
53|1||Branch grafting method for R-tree implementation|
53|2|http://www.sciencedirect.com/science/journal/01641212/53/2|The End of the âOutsourcing Eraâ|
53|2||Coping with ârequirements-uncertaintyâ: the theories-of-action of experienced IS/software project managers|
53|2||A method and tool for assessing object-oriented projects and metrics management|
53|2||Estimating the extent of standards use: the case of ISO/IEC 15504|There has been a proliferation of software engineering standards in the last two decades. While the utility of standards in general is acknowledged, thus far little attempt has been made to evaluate the success of any of these standards. One suggested criterion of success is the extent of usage of a standard. In this paper we present a general method for estimating the extent to which a standard is used. The method uses a capture–recapture (CR) model that was originally proposed for estimating birth and death rates in human populations. We apply the method to estimate the number of software process assessments that were conducted world-wide between September 1996 and June 1998 using the emerging ISO/IEC 15504 International Standard. Our results indicate that 1264 assessments were performed with a 90% confidence interval of 916 and 1895. The method used here can be applied to estimate the extent of usage of other software engineering standards, and also of other software engineering technologies. Such estimates can benefit standards (or technology) developers, funding agencies, and researchers by focusing their efforts on the most widely used standards (or technologies). 
53|2||An investigation of risk perception and risk propensity on the decision to continue a software development project|Many information system (IS) failures may result from the inadequate assessment of project risk. To help managers appraise project risk more accurately, IS researchers have developed a variety of risk assessment tools including checklists and surveys. Implicit in this line of research, however, is the assumption that the use of such devices will lead to more accurate risk perceptions that will, in turn, lead to more appropriate decisions regarding project initiation and continuation. Little is known, though, about the factors that influence risk perception or the interrelationships that exist among risk perception, risk propensity, and decisions about whether or not to continue a project. Without a better understanding of these relationships it is difficult to know whether the application of risk instruments will be an effective means for reducing the incidence of IS failure. This study presents the results of a laboratory experiment designed to: (1) examine the relative contribution of two factors that are believed to shape risk perception: probability that a loss will occur and the magnitude of the potential loss, and (2) explore the relative influence of risk perception and risk propensity on the decision of whether or not to continue a software development project. The results indicate that magnitude of potential loss is the more potent factor in shaping risk perception and that a significant relationship exists between risk perception and decision-making. The implications of these findings are discussed along with directions for future research. 
53|2||A new approach to fault-tolerant scheduling using task duplication in multiprocessor systems|
53|2||TOFF-2: A high-performance fault-tolerant file service|
53|2||An experimental comparison of reading techniques for defect detection in UML design documents|
53|3|http://www.sciencedirect.com/science/journal/01641212/53/3|The missing piece of software development|
53|3||Extending the product family approach to support safe reuse|
53|3||Types of collaborative work in software engineering|This paper is an account on work distribution analyzed from the collaboration point of view. It presents a new classification of the collaborative work in software engineering project. Four types of collaborative work are defined derived from empirical measurements of activities. Mandatory collaborative works are formal scheduled meetings. Called collaborative work is defined when team members call a meeting to solve a problem, which is most often technical. Ad hoc collaborative work is defined when team members work on the same task at the same time and individual work occurs when a team member works on its own on a task related to the project. Data are extracted from the logbook filled out by four team members working on an industrial project that lasts 19 weeks. The characteristics of each type of collaborative activity are described and a quantitative breakdown of how people spend their time in collaboration within a single project is presented. 
53|3||A hybrid approach to analyze empirical software engineering data and its application to predict module fault-proneness in maintenance|
53|3||Function point counting: one programâs experience|
53|3||System Software support for distributed real-time systems|
53|3||Risky business: what we have yet to learn about risk management|
53|3||Testing software to detect and reduce risk|
53|3||Risk-based testing:: Risk analysis fundamentals and metrics for software testing including a financial application case study|The idea of risk-based testing is to focus testing and spend more time on critical functions. By combining the focused process with metrics it is possible to manage the test process by intelligent assessment and to communicate the expected consequences of decisions taken. This paper discusses an approach to risk-based testing and how risk-based testing was carried out in a large project in a financial institution. The paper concludes with how practical risk-based testing experience should inform theory and provide advice on organizational requirements that are necessary to achieve success. 
54|1|http://www.sciencedirect.com/science/journal/01641212/54/1|A letter from the frustrated author of a journal paper|
54|1||An information retrieval system based on a user profile|In this era of information explosion, providing the right information to the right person within reasonable time duration is a very important goal for today’s information retrieval (IR) systems. Due to the characteristics of retrieving methods, the conventional IR models often suffer from inaccurate and incomplete queries as well as inconsistent document relevance. Moreover, each user has his/her own interpretation of the semantic meaning of query terms during the retrieving process. Thus, high accuracy of retrieved information is somewhat hard to achieve. However, accuracy can be improved by designing an IR model that can adapt to the diverse needs of an individual and can perform a personalized search. In this paper, an adaptive information retrieval system embedded in an intelligent feedback tuning mechanism is proposed to capture the personal notions of query terms. This mechanism together with a correlation table is used as a user profile to ‘simulate’ the user’s notions of terms. This correlation table includes the degrees of semantic relevance (SR) and co-occurrence (CO) among terms. By using the personal profile during retrieval, the system can obtain a better match between the information needs and the retrieved results. It is also shown that the recall/precision rates for our system are approximately 91% and 81%, respectively, on average. 
54|1||Similarity retrieval based on group bounding and angle sequence matching in shape database systems|
54|1||An empirical analysis of debugging performance â differences between iterative and recursive constructs|
54|1||Applying meta-analytical procedures to software engineering experiments|
54|1||Performance evaluation of transmission schemes for real-time traffic in a high-speed timed-token MAC network|
54|1||Perceptions of contribution in software teams|
54|1||Non-blocking distributed transaction processing system|
54|1||An assessment of systems and software engineering scholars and institutions (1995â1999)|
54|2|http://www.sciencedirect.com/science/journal/01641212/54/2|Software maintenance, Y2K and other software non-crises|
54|2||Quantifying the closeness between program components and features|
54|2||Decomposing legacy programs: a first step towards migrating to clientâserver platforms|
54|2||Software evolution: code delta and code churn|
54|2||The application of subjective estimates of effectiveness to controlling software inspections|One of the recently proposed tools for controlling software inspections is capture–recapture models. These are models that can be used to estimate the number of remaining defects in a software document after an inspection. Based on this information one can decide whether to reinspect a document to ensure that it is below a prespecified defect density threshold, and that the inspection process itself has attained a minimal level of effectiveness. This line of work has also recently been extended with other techniques, such as the detection profile method (DPM). In this paper, we investigate an alternative approach: the use of subjective estimates of effectiveness by the inspectors for making the reinspection decision. We performed a study with 30 professional software engineers and found that the median relative error of the engineers’ subjective estimates of defect content to be zero, and that the reinspection decision based on that estimate is consistently more correct than the default decision of never reinspecting. This means that subjective estimates provide a good basis for ensuring product quality and inspection process effectiveness during software inspections. Since a subjective estimation procedure can be easily integrated into existing inspection processes, it represents a good starting point for practitioners before introducing more objective decision making criteria by means of capture–recapture models or the defect detection profile method. 
54|2||An experimental investigation of the impact of individual, program, and organizational characteristics on software maintenance effort|
54|2||Locality metrics and program physical structures|
54|2||Discussion|
54|3|http://www.sciencedirect.com/science/journal/01641212/54/3|Y2K, and believing in software practice|
54|3||Selecting reusable stand-alone routines: a proposal and a case study|
54|3||Software retrieval by samples using concept analysis|
54|3||Modularized design for wrappers/monitors in data warehouse systems|
54|3||A multi-granularity locking-based concurrency control in object-oriented database systems|
54|3||An efficient data structure for dynamic memory management|
54|3||Direct execution simulation of load balancing algorithms with real workload distribution|
54|3||Using viewpoints to derive object-oriented frameworks: a case study in the web-based education domain|
54|3||A practical run-time technique for exploiting loop-level parallelism|
55|1|http://www.sciencedirect.com/science/journal/01641212/55/1|Talk About a Software Crisis â Not!|
55|1||Kendra: adaptive Internet system|
55|1||Dynamic adaptation of sharing granularity in dsm systems|
55|1||Hierarchical loop scheduling for clustered NUMA machines|
55|1||A frame of reference for the performance evaluation of asynchronous, distributed decision-making algorithms|
55|1||Conflict free transaction scheduling using serialization graph for real-time databases|
55|1||Changing class behaviors at run-time in MRP systems|
55|1||Distinguishing sharing types to minimize communication in software distributed shared memory systems|
55|1||Simulating multiple inheritance in Java|Multiple inheritance is a feature whose worth has been vigorously debated, and even now there is no agreement as to whether a language should include it. Java does not have multiple inheritance, but its designers claim that many of the benefits of multiple inheritance can be gained through the use of a new feature, the interface. In this paper we explore the claims of the Java designers, and show how the interface may be used to simulate multiple inheritance. We first discuss the benefits of inheritance and multiple inheritance, and then demonstrate how the need for them arises in program development, using an extended example based on a real application. We then present the technique for simulating multiple inheritance, showing how it allows the main benefits of multiple inheritance to be achieved in Java. Finally, we discuss the limitations of the approach. In particular, we show how the approach faces difficulties when used with class libraries, such as the Java Core API, and we suggest a convention for Java class library designers that will mitigate this problem. 
55|2|http://www.sciencedirect.com/science/journal/01641212/55/2|A good-bye of sorts|
55|2||Heuristic search revisited|
55|2||Data placement schemes in replicated mirrored disk systems|
55|2||A spatiotemporal database model and query language|
55|2||An open and safe nested transaction model: concurrency and recovery|In this paper, we present an open and safe nested transaction model. We discuss the concurrency control and recovery algorithms for our model. Our nested transaction model uses the notion of a recovery point subtransaction in the nested transaction tree. It incorporates a prewrite operation before each write operation to increase the potential concurrency. Our transaction model is termed “open and safe” as prewrites allow early reads (before writes are performed on disk) without cascading aborts. The systems restart and buffer management operations are also modeled as nested transactions to exploit possible concurrency during restart. The concurrency control algorithm proposed for database operations is also used to control concurrent recovery operations. We have given a snapshot of complete transaction processing, data structures involved and, building the restart state in case of crash recovery. 
55|2||Newsmonger: a technique to improve the performance of atomic broadcast protocols|
55|2||List ranking on processor arrays|
55|2||Automated translation of JSD into CSP â a case study in methods integration|
55|2||Enhancing online catalog searches with an electronic referencer|
55|3|http://www.sciencedirect.com/science/journal/01641212/55/3|A simple micro-payment scheme|
55|3||A timed workflow process model|An internet-based workflow management system (WfMS) enables business participants to work co-operatively at sites belonging to different time zones. Time-related factors have to be incorporated into the traditional workflow processes so as to adapt to the globally distributed applications. This paper proposes a timed workflow process model through incorporating the time constraints, the duration of activities, the duration of flow, and the activity distribution with respect to the multiple time axes into the conventional workflow processes. The model provides an approach for temporal consistency checking during both build-time and run-time. The proposed model and approach provide a vehicle for global business process modeling, planning and monitoring. 
55|3||Handling signature purposes in workflow systems|In paper-based workflow systems, signatures of individuals or groups of people have been used extensively for different purposes. Currently there are numerous studies on computerizing workflow systems. Also there are studies on implementing digital signatures in electronic media. But the diversified purposes of a signature in workflow makes a straightforward implementation of digital signature schemes inadequate. There are few studies on the implication of different signature purposes on electronic workflow systems. The purposes of signature are closely associated with the two modes of decision making, namely single and group. These two modes of decision making in turn lead to single and group signatures. This paper reports our comprehensive studies on signature purposes. These include the analysis of common signature purposes in workflow, classification of these purposes, classification of modes of decision making associated with these signature purposes, signing and validation requirements for handling these signature purposes, and finally an architecture to be incorporated in workflow engines for handling these signature purposes. In summary, this paper addresses a commonly neglected problem in information systems research: management control by signature in workflow. 
55|3||Digital watermarking models for resolving rightful ownership and authenticating legitimate customer|
55|3||Anomalous intrusion detection system for hostile Java applets|
55|3||A modified remote login authentication scheme based on geometric approach|In 1995, Wu proposed an efficient smart card-oriented remote login authentication scheme. The scheme allows a user to freely choose his password, and no verification tables are required. Hwang recently showed the insecurity in Wu's scheme; however, he did not propose his improvement. In this article, authors show a different approach to break the scheme, and propose their improvement. The modified scheme can withstand all possible attacks. 
55|3||Optimizing storage utilization in R-tree dynamic index structure for spatial databases|Spatial databases have been increasingly and widely used in recent years. The R-tree proposed by Guttman is probably the most popular dynamic index structure for efficiently retrieving objects from a spatial database according to their spatial locations. However, experiments show that only about 70% storage utilization can be achieved in Guttman's R-tree and its variants. In this paper, we propose a compact R-tree structure which can achieve almost 100% storage utilization. Our experiments also show that the search performance of compact R-trees is very competitive as compared to Guttman's R-trees. In addition, the overhead cost of building a compact R-tree is much lower than that of a Guttman's R-tree because the frequency of node splitting is reduced significantly. 
55|3||Comparing case-based reasoning classifiers for predicting high risk software components|
55|3||Delegated multisignature scheme with document decomposition|
volume|issue|url|title|abstract
56|-|http://www.sciencedirect.com/science/journal/01641212/56|Job scheduling in heterogeneous distributed systems|This paper investigates scheduling policies in a heterogeneous distributed system, where half of the total processors have double the speed of the others. Processor performance is examined and compared under a variety of workloads. Two job classes are considered. Programs of the first class are dedicated to fast processors, while second class programs are generic in the sense that they can be allocated to any processor. It was our intention to find a policy that increases overall system throughput by increasing the throughput of the generic jobs without seriously degrading performance of the dedicated jobs. However, simulation results indicate that each scheduling policy considered has its merits and the best performer tended to depend on the degree of multiprogramming. 
56|-||An optimal scheduling algorithm for minimizing the computing period of cyclic synchronous tasks on multiprocessors|We present an efficient optimal algorithm that schedules cyclic synchronous tasks into multiprocessors to minimize the computing period of iterative execution. Due to the rapid development of higher speed microprocessors and digital signal processors (DSPs), small-scale parallel embedded systems and on-chip parallel processors with a simple network structure became feasible for applications such as large-scale simulations and computation-intensive plant control systems that were previously executed by massively parallel computers. We consider cyclic synchronous tasks with communication overhead, which run on multiprocessors with a fully connected network. We suggest the computing period as the performance measure to maximize overall computation speed and the individual start policy that allows overlapping different iterations. The concepts and characteristics of the local period and the global period are also introduced. To solve the complex optimal scheduling problem in an efficient way, our algorithm uses a new spatial scheduling technique using the scheduling space which represents all possible start-time schedules in a multi-dimensional space. By using spatial searching and an enhanced branch-and-bound technique, the optimal schedule which minimizes the computing period can be found. The scheduling results for power plant simulation verify the practicality of our algorithm. 
56|-||Autonomous agents for coordinated distributed parameterized heuristic routing in large dynamic communication networks|Parameterized heuristics offers an elegant and powerful theoretical framework for design and analysis of autonomous adaptive traffic management agents in communication networks. Routing of messages in such networks presents a real-time instance of a multi-criterion optimization problem in a dynamic and uncertain environment. This paper describes the analysis of the properties of heuristic routing agents through a simulation study within a large network with grid topology. A formal analysis of the underlying principles is presented through the incremental design of a set of autonomous agents that realize heuristic decision functions that can be used to guide messages along a near-optimal (e.g., minimum delay) path in a large network. This paper carefully derives the properties of such heuristics under a set of simplifying assumptions about the network topology and load dynamics and identify the conditions under which they are guaranteed to route messages along an optimal path, so as to avoid hotspots in the load landscape of the network. The paper concludes with a discussion of the relevance of the theoretical results to the design of intelligent autonomous adaptive communication networks and an outline of some directions of future research. 
56|-||Proteus: an efficient runtime reconfigurable distributed shared memory system|This paper describes Proteus, a distributed shared memory (DSM) system which supports runtime node reconfiguration. Proteus allows users to change the node set during the execution of a DSM program. The capability of node addition allows users to further shorten the execution time of their DSM programs by dynamically adding newly available nodes to the system. Furthermore, competition for resources between system users and computer owners can be avoided by dynamically deleting nodes from the system. To make the system adapt to the node configuration efficiently, Proteus employs several techniques, including adaptive workload redistribution, affinity page movement, and forced update. Proteus supports both sequential consistency and release consistency. It provides an object-oriented parallel programming environment. This paper describes the design and implementation of node reconfiguration in Proteus, and presents the performance of the system. Experimental results indicate that Proteus can further improve the performance of the tested programs by taking advantage of node reconfiguration. Our results further demonstrate that the techniques employed in Proteus minimize communication and overhead. 
56|-||Supporting parallel computing on a distributed object architecture|The availability of high-speed networks and increasingly powerful commodity microprocessors is making the usage of clusters, or networks, of computers an appealing platform for cost effective parallel computing. However, the ease of developing efficient high-performance parallel software to exploit these platforms presents a major challenge. Advances in distributed object software technology have made the management of distributed computing resources easier than before. This also brings many benefits for parallel computing. Firstly, distributed object technology facilitates the encapsulating of parallel computing resources into a uniform model despite their differences in implementations that are based on different languages executing on different platforms. Secondly, mature object-oriented analysis, design method, as well as component idea embodied in distributed object technology can enhance the reusability of parallel software. To support parallel computing in a distributed object-based computing platform, a uniform high performance distributed object architecture layer is necessary. In this paper, we propose a distributed object-based framework called DoHPC to support parallel computing on distributed object architectures. We present the use of dependence analysis technique to exploit intra-object parallelism and an interoperability model for supporting distributed parallel objects. Experimental results on a Fujitsu AP3000 workstation cluster consisting of a cluster of 32 UltraSPARC workstations show that the implementation of inter-object parallelism on a workstation cluster environment is efficient. With intra-object parallel computation speedup efficiency is greater than 90% and with overhead of less than 10% for large problem, and the interoperability model improves speedup by 20%. 
56|-||ODCHP: a new effective mechanism to maximize parallelism of nested loops with non-uniform dependences|There are many methods for nested loop partitioning. However, most of them perform poorly when partitioning loops with non-uniform dependences. This paper proposes a generalized and optimized loop partitioning mechanism to exploit parallelism from nested loops with non-uniform dependences. Our approach, based on dependence convex theory, will divide the loop into variable size partitions. Furthermore, the proposed algorithm partitions a nested loop by using the copy-renaming and the optimized partitioning techniques to minimize the number of parallel regions of the iteration space. Consequently, it outperforms the previous partitioning mechanisms of nested loops with non-uniform dependences. Many optimization techniques are used to reduce the complexity of the algorithm. Compared with other popular techniques, our scheme shows a dramatic improvement in the preliminary performance results. 
56|1|http://www.sciencedirect.com/science/journal/01641212/56/1|A distributed EDI model|Electronic commerce (EC) has marked a new era for contemporary business. Although electronic data interchange (EDI) is a major component of EC, it is not widely adopted mainly due to high entrance cost and the lack of interoperability among information systems. To alleviate these problems, this paper proposes a distributed model so that EDI transactions can be directly generated from or transformed to either local or remote databases without going through expensive mapping and translation procedures. The most superior outcome of this model is that it combines the advantages of both application-to-application and database-to-database approaches and thus results in an integrated EDI model independent of databases structures, network locations, development languages, etc. 
56|1||SMART mobile agent facility|With ever growing use of Internet for electronic commerce and data mining type applications there seems to be a need for new network computing paradigms that can overcome the barriers posed by network congestion and unreliability. Mobile agent programming is a paradigm that enables the programs to move from one host to another, do the processing locally and return results asynchronously. In this paper, we present the design and development of a mobile agent system that will provide a platform for developing mobile applications that are Mobile Agent Facility (MAF) specification compliant. We start by exploring mobile agent technology and establish its merits with respect to the client–server technology. Next, we introduce a concept called dynamic aggregation to improve the performance of mobile agent applications. We, then focus on the design and implementation issues of our system, Scalable, Mobile and Reliable Technology (SMART), which is based on the MAF specification. 
56|1||A mechanism for view consistency in a data warehousing system|A data warehouse is a repository of integrated information from distributed, autonomous and heterogeneous information sources. Materialized views in the data warehouse must be maintained when updating the data in the information sources. The conventionally used incremental view maintenance algorithm causes the problem of anomalies. In addition, previous attempts to solve anomalies focus on compensating the changes of the view. In this work, a novel method which uses the information already available at the data warehouse instead of performing the compensation is presented. Rules are used to predict information deemed necessary to maintain a materialized view. The information is stored as auxiliary views. The proposed method does not require any data transmission between the information sources and warehouse when a change of a materialized view from an updated message occurs in information sources. Moreover, the method proposed herein saves a significant amount of time when materialized views are incrementally maintained. 
56|1||Decentralized user group assignment in Windows NT|The notion of groups in Windows NT is much like that in other operating systems. Rather than set user and file rights individually for each and every user, the administrator can give rights to various groups, then place users within those groups. Each user within a group inherits the rights associated with that group. In this paper, we describe an experiment to extend the Windows NT group mechanism in two significant ways that are useful in managing group-based access control in large-scale systems. The goal of our experiment is to demonstrate how group hierarchies (where groups include other groups) and decentralized user-group assignment (where administrators are selectively delegated authority to assign certain users to certain groups) can be implemented by means of Microsoft remote procedure call (RPC) programs. In both respects the experimental goal is to implement previously published models (RBAC96 for group hierarchies and URA97 for decentralized user-group assignment). Our results indicate that Windows NT has adequate flexibility to accommodate sophisticated access control models to some extent. 
56|1||Effectiveness of the FDDI-M protocol in supporting synchronous traffic|Timed token networks such as fiber distributed data interface (FDDI) networks have been widely deployed to support synchronous traffic. However the medium access control (MAC) protocol of FDDI allows transmission of synchronous messages up to at most one-half of the total bandwidth of the network. Shin and Zheng have proposed a modification to the FDDI MAC protocol, called FDDI-M, which can double a ring's ability in supporting synchronous traffic (K.G. Shin, G. Zheng, IEEE Transactions on Parallel and Distributed Systems 6 (1995) 1125–1131). It is widely known that the ability of timed token protocols such as FDDI to guarantee synchronous message deadlines is very dependent on the synchronous bandwidth allocation (SBA) schemes used, but the original paper does not address this issue. In this paper, we will compare the ability of FDDI-M to support synchronous traffic under different SBA schemes with that of FDDI. We use a new taxonomy of SBA schemes based on the strategy used to partition the synchronous bandwidth, and present an analytical study of the timing properties of the FDDI-M protocol using the worst case achievable utilization (WCAU) as the performance metric. The results show that while FDDI-M improves the WCAU values under one class of SBA schemes, its performance under the other category of SBA schemes is mixed. We also perform extensive simulation to study performance of FDDI-M for MPEG video traffic, and conclude FDDI-M does outperform FDDI significantly at heavy load. The effect of SBA schemes under overload conditions is also shown to be relatively minor, with the local SBA schemes actually performing better than the global schemes. 
56|1||The prediction of faulty classes using object-oriented design metrics|Contemporary evidence suggests that most field faults in software applications are found in a small percentage of the software's components. This means that if these faulty software components can be detected early in the development project's life cycle, mitigating actions can be taken, such as a redesign. For object-oriented applications, prediction models using design metrics can be used to identify faulty classes early on. In this paper we report on a study that used object-oriented design metrics to construct such prediction models. The study used data collected from one version of a commercial Java application for constructing a prediction model. The model was then validated on a subsequent release of the same application. Our results indicate that the prediction model has a high accuracy. Furthermore, we found that an export coupling (EC) metric had the strongest association with fault-proneness, indicating a structural feature that may be symptomatic of a class with a high probability of latent faults. 
56|1||Interfacing MATLAB with a parallel virtual processor for matrix algorithms|This paper describes the results of a project to interface MATLAB with a parallel virtual processor (PVP) that allows execution of matrix operations in MATLAB on a set of computers connected by a network. The software, a connection-oriented BSD socket-based client–server model, automatically partitions a MATLAB problem and delegates work to server processes running on separate remote machines. Experimental data on the matrix multiply operation shows that the speed improvement of the parallel implementation over the single-processor MATLAB algorithm depends on the size of the matrices, the number of processes, the speed of the processors, and the speed of the network connection. In particular, the advantage of doing matrix multiply in parallel increases as the size of the matrices increase. A speedup of 2.95 times was achieved in multiplying 2048 by 2048 square matrices using 15 workstations. The algorithm was also implemented on a network of four PC's, which was up to 2.5 times as fast as four workstations. The study also showed that there is an optimal number of processes for a particular problem, and so using more processes is not always faster. 
56|1||A dynamic simulator of software processes to test process assumptions|Validation testing of software processes may provide both qualitative and quantitative suggestions to understand the ways to change the software process to improve its quality, or the ways to achieve specific organisation's needs. In many cases, however, such understanding has to be done without affecting the actual environment. To this purpose, this paper introduces the use of process simulators for validation testing. To be effective, however, simulators must combine the ability to sustain the complexity of the modelling problem with the so-called dynamic estimation capability, that is the capability of representing the dynamics of the simulated process. To achieve such objectives, the introduced simulator is based on the association of three conventional modelling methods (analytical, continuous and discrete-event) into a unique hybrid multi-level new model, called dynamic capability model (DCM). The paper applies DCM in the context of a waterfall-based software process to study the effects of three different quality assurance management policies on given process quality attributes, as effort, delivery time, productivity, rework percentage, and product quality. The verification of the simulator representativeness is also performed by reproducing empirically known facts in the process behaviour. 
56|1||Notable design patterns for domain-specific languages|The realisation of domain-specific languages (dsls) differs in fundamental ways from that of traditional programming languages. We describe eight recurring patterns that we have identified as being used for dsl design and implementation. Existing languages can be extended, restricted, partially used, or become hosts for dsls. Simple dsls can be implemented by lexical processing. In addition, dsls can be used to create front-ends to existing systems or to express complicated data structures. Finally, dsls can be combined using process pipelines. The patterns described form a pattern language that can be used as a building block for a systematic view of the software development process involving dsls. 
56|1||Dot-coms' coma|
56|2|http://www.sciencedirect.com/science/journal/01641212/56/2|Contents|
56|2||Employing multiple views to separate large-scale software systems|
56|2||A methodology for building content-oriented hypermedia systems|
56|2||Synchronization and flow adaptation schemes for reliable multiple-stream transmission in multimedia presentations|
56|2||Spatial database with each picture self-contained multiscape and access control in a hierarchy|
56|2||A fast content-based indexing and retrieval technique by the shape information in large image database|
56|2||Database management systems: design considerations and attribute facilities|
56|2||Seeking consonance in information systems|Information system projects are notorious for their failure rate. We propose that much failure is due to a difference in expectations prior to the start of a new system development. Much of the difference in expectations may be in the use of metrics not fully understood by every stakeholder in a new system. Current theory and management practice suggests a better focus on building an understanding of the critical evaluators to develop a common understanding of expectations will improve success rates. Such activity requires broader viewpoints of success and the input of more stakeholders well before any project tasks are conducted. Four studies are summarized that highlight the importance of reaching a priori mutual agreement on the metrics and targets. 
57|1|http://www.sciencedirect.com/science/journal/01641212/57/1|A note on the evolution of software engineering practices|
57|1||Management of process improvement by prescription|
57|1||Experimental comparison of coarse-grained concepts in UML, OML, and TOS|
57|1||A simple process for migrating server applications to SMP:s|
57|1||Incentive compatibility and systematic software reuse|
57|1||A simulation study on coordination strategies: decision cycle-time perspective|
57|1||Exploratory analysis of environmental factors for enhancing the software reliability assessment|
57|1||Empirical comparison of regression test selection algorithms|In the maintenance phase, the regression test selection problem refers to selecting test cases from the initial suite of test cases used in the development phase. In this paper, we empirically compare five representative regression test selection algorithms, which include: Simulated Annealing, Reduction, Slicing, Dataflow, and Firewall algorithms. The comparison is based on eight quantitative and qualitative criteria. These criteria are: number of selected test cases, execution time, precision, inclusiveness, preprocessing requirements, type of maintenance, level of testing, and type of approach. The empirical results show that the five algorithms can be used for different requirements of regression testing. For example the Simulated Annealing algorithm can be used for emergency non-safety-critical maintenance situations with a large number of small modifications. 
57|2|http://www.sciencedirect.com/science/journal/01641212/57/2|Contents|
57|2||A hybrid approach to OO development: the SUMMITrak project at TCI|
57|2||An industrial study of reuse, quality, and productivity|
57|2||A study of the allocation behavior of C++ programs|
57|2||Grounding the OML metamodel in ontology|
57|2||Non-linear array data dependence test|
57|2||An approach to modeling and evaluation of functional and timing specifications of real-time systems|Real-time systems need to be correct with respect to both functional and timing behaviors. Specification and modeling methods for real-time systems must thus permit the evaluation of functional and timing properties. Conventional real-time schedulability analyzers and simulators are not based on a formal specification and a model can thus not be formally verified. Formal specifications, on the other hand, frequently ignore preemptive scheduling and resource access protocols and the results obtained are thus only of limited value for systems using state-of-the-art scheduling algorithms. This paper proposes a novel Petri net-based approach for constructing specifications that are both formally verifiable and operational. It presents a solution to the preemption problem and suggests a pragmatic generic real-time system model which can be easily transformed into models for formal verification of functionality and for scheduling simulation. The well-known mine drainage system case study is used to demonstrate the application of this approach. 
57|3|http://www.sciencedirect.com/science/journal/01641212/57/3|References architectures for enterprise integration|
57|3||A conceptual foundation for component-based software deployment|
57|3||Using UML-F to enhance framework development: a case study in the local search heuristics domain|
57|3||Experience with identifying and characterizing problem-prone modules in telecommunication software systems|
57|3||A technique for function block counting|
57|3||How good is the critical factor concept in estimating the average number of character comparisons per item in string sorting?|
57|3||Communication cost of cognitive co-operation for distributed team development|Current team development approaches only focus on work co-operation and resource sharing. Cognitive co-operation has not been paid enough attention, while it becomes more important than ever when the development team is geographically distributed. This paper identifies five types of Internet-based communication approaches for cognitive co-operation among distributed team members: the email-based approach; the blackboard-centred approach; the pure flow-based approach; the combination between the flow-based approach and the email-based approach; and the combination between the flow-based approach and the blackboard-centred approach. The communication costs of these approaches are defined and compared based on a set of assumptions. These comparisons provide the development teams or the designers of team wares the evidence and the method for choosing a suitable cognitive co-operation approach. 
57|3||Contents Volume 57|
58|1|http://www.sciencedirect.com/science/journal/01641212/58/1|Minimizing the mean delay of quorum-based mutual exclusion schemes|
58|1||Hiding communication overheads in dynamic load balancing for multicomputers|
58|1||Fault-tolerant gamma interconnection network without backtracking|
58|1||A learning database system to observe malfunctions and to support network planning|
58|1||On the neural network approach in software reliability modeling|
58|1||Editor's Corner|
58|1||Proportional sampling strategy: a compendium and some insights|
58|2|http://www.sciencedirect.com/science/journal/01641212/58/2|A new encryption algorithm for image cryptosystems|There are two major differences of the characteristics of the text data and image data. One difference is that the size of image data is usually much larger than that of text data. The other is that plain data rarely permit loss when a compression technique is used, but image data do. In this paper, we design an efficient cryptosystem for images. Our method is based on vector quantization, which is one of the popular image compression techniques. Our method can achieve the following two goals. One goal is to design a high security image cryptosystem. The other goal is to reduce computational complexity of the encryption and decryption algorithms. 
58|2||Specification and analysis of n-way key recovery system by Extended Cryptographic Timed Petri Net|
58|2||A proxy-based security architecture for Internet applications in an extranet environment|
58|2||New nonrepudiable threshold proxy signature scheme with known signers|Based on Kim et al.'s threshold proxy signature scheme, Sun proposed an nonrepudiable threshold proxy signature scheme with known signers. In Sun's scheme, actual proxy signers cannot deny the signatures they have signed. However, his scheme is vulnerable against the conspiracy attack. Any t malicious proxy signers can collusively derive the secret keys of other proxy signers and impersonate some other proxy signers to generate proxy signatures. In this paper, we proposed a new nonrepudiable threshold proxy signature scheme that overcomes the above weakness. Furthermore, the proposed scheme is more efficient than Sun's in terms of computational complexities and communication costs. 
58|2||A dominance relation enhanced branch-and-bound task allocation|
58|2||Design of a scalable multiprocessor architecture and its simulation|
58|2||Understanding complex, real-world systems through asynchronous, distributed decision-making algorithms|
58|2||Controversy Corner|
58|2||Making inconsistency respectable in software development|
58|3|http://www.sciencedirect.com/science/journal/01641212/58/3|Case study of the evolution of routing algorithms in a network planning tool|Traffic routing is a key component in a network planning system. This paper concentrates on the routing algorithms and follows their evolution over multiple releases of a planning tool during a period of six years. The algorithms have grown from the initial stage of finding shortest paths with Dijkstra's algorithm to cover more complex routing tasks such as finding protected and unprotected routes and capacity limited routing. We present the algorithms and in particular emphasize the practical aspects: how previous algorithms were reused and what were the practical experiences of using the algorithms. A conclusion of the study is that algorithms should be considered with an engineering attitude. It is not enough to focus on selecting the most sophisticated state-of-the-art algorithm for a given problem. Evolution capability, potential for reuse, and the development cost over the system lifetime are equally important aspects. 
58|3||The design and implementation of an active replication scheme for distributing services in a cluster of workstations|
58|3||Improving the performance of time-constrained workflow processing|
58|3||A priority-based resource allocation strategy in distributed computing networks|
58|3||A study of page replacement performance in garbage collection heap|Large number of page-faults can severely degrade the performance of any system. While much attention has been paid on the virtual memory space of an entire process, the paging behavior of one particular memory segment has not been reported. Unlike the static behavior in most of the memory segments, the heap segment has unique characteristics closely related to its memory management scheme. First, this paper presents a thorough study on the effect of heap page-faults on the performance of Kaffe JVM version 1.0.5 and the Kaffe Java virtual machine (JVM) with the modified buddy system. Second, a modified least recently used (mLRU) is proposed to improve the performance of the LRU page-replacement policy. Four different page-replacement policies, first-in-first-out (FIFO), Random, LRU and mLRU, are used to study the performance of each JVM. Third, the Java applications used in this study are SPECjvm98 benchmark suite. These programs represent real-world workload and are highly dynamic memory intensive. Fourth, an instrumented Kaffe JVM is used to generate memory traces. Then, a simulator is used to analyze and reconstruct the heap as the benchmark programs are executed. Finally, this study has shown that the modified buddy system has less page-fault occurrences in six out of eight applications. The improvement can be up to 74%. Moreover, the proposed mLRU can improve the performance up to 68.2%. 
58|3||Joint scheduling of garbage collector and hard real-time tasks for embedded applications|
58|3||The effects of design pattern application on metric scores|
58|3||An empirical study of XML/EDI|
58|3||Contents volume 58|
59|1|http://www.sciencedirect.com/science/journal/01641212/59/1|Transitioning from Academia to Industrial Research|
59|1||Integrating scenario-based and measurement-based software product assessment|
59|1||An empirical evaluation of the ISO/IEC 15504 assessment model|The emerging international standard ISO/IEC 15504 (Software Process Assessment) includes an exemplar assessment model (known as Part 5). Thus far, the majority of users of ISO/IEC 15504 employ the exemplar model as the basis for their assessments. This paper describes an empirical evaluation of the exemplar model. Questionnaire data was collected from the lead assessors of 57 assessments world-wide. Our findings are encouraging for the developers and users of ISO/IEC 15504 in that they indicate that the current model can be used successfully in assessments. However, they also point out some weaknesses in the rating scheme that need to be rectified in future revisions of ISO/IEC 15504. 
59|1||The relationship between ISO/IEC 15504 process capability levels, ISO 9001 certification and organization size: An empirical study|The gradual spread in the use of ISO 9001 and ISO/IEC 15504 (also known as software process improvement and capability determination (SPICE)) has raised questions such as “At what ISO/IEC 15504 capability level would one expect an ISO 9001 certified organization's processes to be?” and “Is there any significant difference between the ISO/IEC 15504 capability levels achieved by the processes of ISO 9001 certified organizations and those of non ISO 9001 certified organizations?”. This paper provides answers to those questions as well as to the following question “Is there any significant difference in the capability levels achieved by the ISO/IEC 15504 processes of organizations with a large information technology (IT) staff and those with a small IT staff?” In order to answer these questions, we analyzed a data set including 691 process instances (PIs) taken from 70 SPICE phase 2 trial assessments performed over the two years from September 1996 to June 1998. Results show that the ISO/IEC 15504 processes of the ISO 9001 certified organizations attained capability levels of around 1–2.3 in 15504 terms. Results also show differences between the capability levels achieved by ISO 9001 certified organizations and non ISO 9001 certified organizations, as well as between organizations with a large IT staff and those with a small IT staff. 
59|1||An empirical study of certain object-oriented software metrics|
59|1||Using self-organizing maps to analyze object-oriented software measures|
59|1||Viewpoint representation validation: a case study on two metrics from the Chidamber and Kemerer suite|A metric definition follows from a comprehensive sequence of steps that takes into account all the requirements of the representational theory of measurement. The last step emphasises that numerical relations must preserve and are preserved by empirical relations or viewpoints. It is the preservation of viewpoints in numerical relations that we have tried to empirically validate for the metrics depth of inheritance tree (DIT) and number of children (NOC), proposed by Chidamber and Kemerer [S.R. Chidamber, C.F. Kemerer, IEEE Trans. Software Eng. 20 (6) (1994) 476–493], and which we also refer to as viewpoint representation validation. An analysis of the results indicates the possibility of the two metrics not preserving the viewpoints adequately. 
59|1||The determinants of visibility of software engineering researchers|
59|1||An assessment of Systems and Software Engineering scholars and institutions (1996â2000)|
59|2|http://www.sciencedirect.com/science/journal/01641212/59/2|Some informal thoughts about reviewing as a social process|
59|2||An integrated approach to distributed version management and role-based access control in computer supported collaborative writing|
59|2||A framework for evaluation and prediction of software process improvement success|The literature shows that software process improvement (SPI) is a current popular approach to software quality and that many companies are undertaking formal or informal SPI programs. However, the anticipated improvements to software quality through SPI have not, as yet, been fully realised. Many companies are neither ready nor equipped to implement a successful SPI program: how do companies evaluate and validate the necessary organisational requirements for the establishment of a successful SPI program? This paper examines the outcomes of a UK study of a sample of SPI programs and compares these programs with an evaluation framework we have developed. The validated framework will help companies conduct a self-assessment of their readiness to undertake SPI. 
59|2||Reschedulable-Group-SCAN scheme for mixed real-time/non-real-time disk scheduling in a multimedia system|
59|2||Object-oriented real-world modeling revisited|When applied to real-world problems, object-oriented modeling maps an entity in the real world to a class as it is. This seemingly natural “genuine” real-world modeling can be rightly applied to cases when the purpose of modeling is merely to represent a problem in a class diagram and thus to facilitate its understanding. Business process reengineering is a good example of this. Genuine real-world modeling can also be applied to the development of a program that simulates the real world on a computer. Contrary to these cases, however, “pseudo” real-world modeling has instead to be applied when a business assistance application is to be developed. It maps an entity whose information is dealt with by the business to be automated to a class that represents the information about the entity. These two modeling methods have to be appropriately applied according to the type of their target problems. This point, however, has not been sufficiently recognized. Many authors of the literature on object-oriented methodologies and techniques teach us “naive” real-world modeling, whose real nature is a mixture of genuine and pseudo real-world modeling methods. Naive analyzers who believe the literature are lured into severe modeling errors when they develop business assistance applications. 
59|2||Illustrating the cognitive consequences of object-oriented systems development|
59|2||Object-oriented design patterns recovery|
59|2||A comparative study of exception handling mechanisms for building dependable object-oriented software|
59|3|http://www.sciencedirect.com/science/journal/01641212/59/3|Software Process Simulation Modelling|
59|3||Hybrid simulation modelling of the software process|The paper proposes the combination of three traditional modelling methods (analytical, continuous and discrete-event), into a unique hybrid two-level modelling approach, to address software process simulation modelling issues. At the higher abstraction level, the process is modelled by a discrete-event queueing network, which represents the component activities, their interactions, and the exchanged artefacts. At the lower abstraction level, the analytical and continuous methods are used to describe the behaviour of the introduced activities.The hybrid approach is applied to a waterfall-based software process to study the effects of requirements instability on various process quality attributes, such as effort, delivery time, productivity, rework percentage and product quality. Simulation results show that the use of the model can provide both qualitative and quantitative suggestions on how to improve the software process or on how to satisfy specific organisational needs.Although, the model has been primarily designed to represent the behaviour of hypothetical projects, and to allow researchers to view the implications of their assumptions, it can be easily customised and extended to become a tool for analysing and predicting the behaviour of actual projects. 
59|3||Application of a hybrid process simulation model to a software development project|Simulation models of the software development process can be used to evaluate potential process changes. Careful evaluation should consider the change within the context of the project environment. While system dynamics models have been used to model the project environment, discrete event and state-based models are more useful when modeling process activities. Hybrid models of the software development process can examine questions that cannot be answered by either system dynamics models or discrete event models alone. In this paper, we present a detailed hybrid model of a software development process currently in use at a major industrial developer. We describe the model and show how the model was used to evaluate simultaneous changes to both the process and the project environment. 
59|3||Stochastic simulation of risk factor potential effects for software development risk management|
59|3||Behavioral characterization: finding and using the influential factors in software process simulation models|
59|3||System dynamics modelling of software evolution processes for policy investigation: Approach and example|
59|3||A CBT module with integrated simulation component for software project management education and training|Due to increasing demand for software project managers in industry, efforts are needed to develop the management-related knowledge and skills of the current and future software workforce. In particular, university education needs to provide to their computer science and software engineering (SE) students not only technology-related skills but, in addition, a basic understanding of typical phenomena occurring in industrial (and academic) software projects. The objective of this paper is to present concepts of a computer-based training (CBT) module for student education in software project management. The single-learner CBT module can be run using standard web-browsers (e.g. Netscape). The simulation component of the CBT module is implemented using the system dynamics (SD) simulation modelling method. The paper presents the design of the simulation model and the training scenario offered by the existing CBT module prototype. Possibilities for empirical validation of the effectiveness of the CBT module in university education are described, results of a first controlled experiment are presented and discussed, and future extensions of the CBT module towards collaborative learning environments are suggested. 
59|3||A simplified model of software project dynamics|The simulation of a dynamic model for software development projects (hereinafter SDPs) helps to investigate the impact of a technological change, of different management policies, and of maturity level of organisations over the whole project. In the beginning of the 1990s, with the appearance of the dynamic model for SDPs by Abdel-Hamid and Madnick [Software Project Dynamics: An Integrated Approach, Prentice-Hall, Englewood Cliffs, NJ, 1991], a significant advance took place in the field of project management. From this work, several dynamic models have been developed in order to simulate the behaviour of these kinds of projects. From the comparison made between one of the best known empirical estimation models and dynamic estimation models, we have analysed the existing problems in dynamic models in order to make dynamic estimations at the early stages of software projects, when little information is available. We present the results obtained from a Reduced Dynamic Model developed to estimate and analyse the behaviour of SDPs in the early phases, in which there is not much information regarding the project. The modelling approach followed to obtain this simplified model has been determined by the simplification of Abdel-Hamid and Madnick's model using the works of Eberlein [Syst. Dyn. Rev. 1(5) (1989) 51] about understanding and simplification of models. 
59|3||System dynamics modelling and simulation of collaborative requirements engineering|
59|3||Exploring bottlenecks in market-driven requirements management processes with discrete event simulation|
59|3||Analysing a process landscape by simulation|
59|3||Modeling and simulating software acquisition process architectures|
59|3||A tool for evaluation of the software development process|
59|3||Contents Volume 59|
60|1|http://www.sciencedirect.com/science/journal/01641212/60/1|A Final Good-bye|
60|1||Stepping up to the Plate|
60|1||On the independence of software inspectors|
60|1||A matching-based algorithm for page access sequencing in join processing|
60|1||Streaming extensibility in the Modify-on-Access file system|
60|1||Design and implementation of spatiotemporal database query processing system|
60|1||On the application of formal description techniques to the design of interception systems for GSM mobile terminals|
60|1||Formal modeling in a commercial setting: A case study|
60|1||The importance of ignorance in requirements engineering: An earlier sighting and a revisitation|
60|2|http://www.sciencedirect.com/science/journal/01641212/60/2|Guest editorial|
60|2||From program languages to software languages|
60|2||Designing reactive systems: integration of abstraction techniques into a synthesis procedure|
60|2||Formal design and development of a Corba-based application for cooperative HTML group editing support|
60|2||A software environment task object-oriented design (ETOOD)|
60|2||Implementation of distributed iterative algorithm for optimal control problems on several parallel architectures|
60|2||Galois connection, formal concepts and Galois lattice in real relations: application in a real classifier|In this paper, we introduce the notion of a real set as an extension of a crisp and a fuzzy set by using sequences of intervals as membership degrees, instead of a single value in [0,1]. We also propose, to extend the notion of Galois connection in a real binary relation as well as the notions of rectangular relation, formal concept and Galois lattice. We present finally a real classifier based on this mathematical foundation. 
60|3|http://www.sciencedirect.com/science/journal/01641212/60/3|Automated discovery of concise predictive rules for intrusion detection|
60|3||A group signature scheme with strong separability|Group signatures, introduced by Chaum and van Heijst, allow members of a group to sign messages anonymously on behalf of the group. Only a designated group manager is able to identify the group member who issued a given signature. Many applications of group signatures, for example, electronic market, require that the group manager can be split into a membership manager and a revocation manager. The former is responsible for adding new members to the group. The latter is responsible for opening signatures. Previously proposed group signatures schemes can only achieve a weak form of separability. That is, the revocation manager and the membership manager must work in concert to reveal the identity of the signer. In this paper, we propose a group signature scheme with strong separability in which the revocation manager can work without the involvement of the membership manager. 
60|3||Special section on Industrial information systems: progresses and perspectives in Pacific Rim|
60|3||Architectural design and evaluation of an efficient Web-crawling system|This paper presents an architectural design and evaluation result of an efficient Web-crawling system. The design involves a fully distributed architecture, a URL allocating algorithm, and a method to assure system scalability and dynamic reconfigurability. Simulation experiment shows that load balance, scalability and efficiency can be achieved in the system. Currently this distributed Web-crawling subsystem has been successfully integrated with WebGather, a well-known Chinese and English Web search engine, aimed at collecting all the Web pages in China and keeping pace with the rapid growth of Chinese Web information. In addition, we believe that the design can also be useful in other context such as digital library, etc. 
60|3||A reference system for internet based inter-enterprise electronic commerce|
60|3||Integrity protection for Code-on-Demand mobile agents in e-commerce|
60|3||A computerized causal forecasting system using genetic algorithms in supply chain management|
60|3||Applying a mediator architecture employing XML to retailing inventory control|
60|3||Introduction to an integrated methodology for development and implementation of enterprise information systems|
60|3||Contents of Volume 60|
