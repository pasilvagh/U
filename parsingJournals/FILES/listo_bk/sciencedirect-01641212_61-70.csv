volume|issue|url|title|abstract
61|-|http://www.sciencedirect.com/science/journal/01641212/61|Best practices in software engineering|
61|-||Classification and evaluation of defects in a project retrospective|
61|-||COTS-based software development: Processes and open issues|
61|-||Challenges of component-based development|
61|-||Producing reliable software: an experiment|
61|-||An empirical study of industrial security-engineering practices|This paper presents lessons learned and observations noted about the state of security-engineering practices by three information security practitioners with different perspectives – two in industry and one in academia. All authors have more than 20-years experience in this field and two were former members of the US National Computer Security Center during the early days of the Trusted Computer System Evaluation Criteria and the strong promotion of trusted operating systems that accompanied the release of that document. In the last 20 years, it has been argued that security-engineering practices have not kept pace with the escalating threats to information systems. Much has occurred since that time – new security paradigms, failure of evaluated products to emerge into common use, new systemic threats, and an increased awareness of the risk faced by information systems. This paper presents an empirical view of lessons learned in security-engineering, experiences in applying the trade, and observations made about the successes and failures of security practices and technology. This work was sponsored in part by NSF Grant. 
61|-||Interprocess communications in the AN/BSY-2 distributed computer system: a case study|
61|-||Contents Volume 61|
61|1|http://www.sciencedirect.com/science/journal/01641212/61/1|Strategies for resolving inter-class data conflicts in mixed real-time database systems|
61|1||Maintaining security and timeliness in real-time database system|
61|1||The impact of component architectures on interoperability|Component interoperability has become an important concern as companies migrate legacy systems, integrate COTS products, and assemble modules from disparate sources into a single application. While middleware is available for this purpose, it often does not form a complete bridge between components and may be inflexible as the application evolves. What is needed is the explicit design information that will forecast a more accurate, evolvable, and less costly integration solution implementation.Emerging research has shown that interoperability problems can be traced to the software architecture of the components and integrated application. Furthermore, the solutions generated for these problems are guided by an implicit understanding of software architecture. Current technology does not fully identify what must be made explicit about software architecture to aid in comparison of the architectures and expectations of participating entities within the integrated application. Thus, there can be no relief in the expense or the duration of implementing long-term reliance on middleware. The overall goal of our research is to extract and make explicit the information needed to define and build this technology. This paper focuses on identifying, classifying, and organizing characteristics that help to define an architectural style by using abstraction and semantic nets. We illustrate the relationships between the characteristics and their relevance to interoperability via examples of integrated applications. 
61|1||Experiences with ALMA: Architecture-Level Modifiability Analysis|
61|1||An estimation of the decision models of senior IS managers when evaluating the external quality of organizational software|
61|2|http://www.sciencedirect.com/science/journal/01641212/61/2|An XML approach for legacy code reuse|
61|2||SigDAQ: an enhanced XML query optimization technique|
61|2||Design erosion: problems and causes|Design erosion is a common problem in software engineering. We have found that invariably, no matter how ambitious the intentions of the designers were, software designs tend to erode over time to the point that redesigning from scratch becomes a viable alternative compared to prolonging the life of the existing design. In this paper, we illustrate how design erosion works by presenting the evolution of the design of a small software system. In our analysis of this example, we show how design decisions accumulate and become invalid because of new requirements. Also it is argued that even an optimal strategy for designing the system (i.e. no compromises with respect to e.g. cost are made) does not lead to an optimal design because of unforeseen requirement changes that invalidate design decisions that were once optimal. 
61|2||Availability analysis and improvement of Active/Standby cluster systems using software rejuvenation|
61|2||Visual requirement representation|
61|2||Software requirements validation via task analysis|As a baseline for software development, a correct and complete requirements definition is one foundation of software quality. Previously, a novel approach to static testing of software requirements was proposed in which requirements definitions are tested on a set of task scenarios by examining software behaviour in each scenario described by an activity list. Such descriptions of software behaviour can be generated automatically from requirements models. This paper investigates various testing methods for selecting test scenarios. Data flow, state transition and entity testing methods are studied. A variety of test adequacy criteria and their combinations are formally defined and the subsume relations between the criteria are proved. Empirical studies of the testing methods and the construction of a prototype testing tool are reported. 
62|1|http://www.sciencedirect.com/science/journal/01641212/62/1|The cost of errors in software development: evidence from industry|The search for and correction of errors in software are often time consuming and expensive components of the total cost of software development. The current research investigates to what extent these costs of software error detection and correction contribute to the total cost of software. We initiated the research reported here with the collection of a sample of transactions recording progress on one phase of development of a set of software programs. Each of these projects represented the completion of an identical phase of development (i.e., country localisation) for a different country. This enabled each project to be compared with the other, and provided an unusually high degree of control over the data collection and analysis in real-world empirical study. The research findings relied on programmers' self-assessment of the severity of errors discovered. It found that serious errors have less influence on total cost than errors that were classified as less serious but which occurred with more frequency once these less serious errors are actually resolved and corrected. The research suggests one explanation – that programmers have greater discretion in how and when to resolve these less severe errors. The data supports the hypothesis that errors generate significant software development costs if their resolution requires system redesign. Within the context of the research, it was concluded that uncorrected errors become exponentially more costly with each phase in which they are unresolved, which is consistent with earlier findings in the literature. The research also found that the number of days that a project is open is a log-linear predictor of the number of software errors that will be discovered, implying a bias in error discovery over time. This implies that testing results need to be interpreted in light of the length of testing, and that in practice, tests should take place both before and after systems release. 
62|1||A testing approach for large system portfolios in industrial environments|The Year 2000 and the Euro conversion have shown to be a major challenge to IT departments of many organizations. In addition to the project management and software maintenance issues the right regression testing approach has turned out to be a critical factor to the success of these organizations.In this paper, we describe an approach to regression testing that we have developed and meanwhile successfully implemented in various system migration projects. We point out the major challenges and success factors in the testing of an entire system portfolio, which has been adapted to meet the requirements of the European monetary union (EMU). Furthermore, we discuss how this end-to-end testing approach of key business processes is also applicable to other regression testing as well as other major system integration projects. 
62|1||A survey of communication protocol testing|Academic research has made significant advances in the generation of test sequences from formal specifications, and in the development of computer-aided test tools, with the aim of improving the effectiveness of protocol testing. However, this state-of-the-art research is not necessarily state-of-the-practice; these methods and tools are seldom used in the computer and communications industries. There is a big gap between testing practice and research results published in journals and reported at conference. To help the industrialization of academic techniques, empirical studies and a new orientation towards real issues need to be given higher priority by researchers, and real-life test coverage and field experience need to be more frequently reported [66]. This paper presents a literature survey of communication protocol testing. We have focused our survey on the following five areas: test sequence generation methods, test coverage, fault model and prediction, test tools, and experience reports. 
62|1||Designing a resourceful fault-tolerance system|This paper examines the feasibility of creating a “resourceful” software fault-tolerance system. Current fault-tolerant methods typically replace a faulty module with a redundant backup version, making no attempt to assess and correct errors in the original module. Error-recovery options are therefore limited by the number of backup modules. In contrast, a resourceful system dynamically generates alternative error-correction strategies. Periodically, the system determines which of its pre-defined goals has not been met, then executes different strategies until its goals are achieved. We outline a resourceful fault-tolerance system that defines recovery goals and specifies separate detection and correction procedures for each goal. When errors are detected, various sequences of correction procedures are examined to identify ones that meet the recovery goals. Implementation issues such as specifying recovery goals, creating recovery options, and reducing runtime overhead are examined. We describe a strategy to increase the efficiency of our method by planning each recovery before implementing it, eliminating strategies expected to be unsuccessful, impractical, or cyclical. 
62|1||Fundamental principles of software engineering â a journey|A set of fundamental principles can act as an enabler in the establishment of a discipline; however, software engineering still lacks a set of universally recognized fundamental principles. This article presents a progress report on an attempt to identify and develop a consensus on a set of candidate fundamental principles. A fundamental principle is less specific and more enduring than methodologies and techniques. It should be phrased to withstand the test of time. It should not contradict a more general engineering principle and should have some correspondence with “best practice”. It should be precise enough to be capable of support and contradiction and should not conceal a tradeoff. It should also relate to one or more computer science or engineering concepts. The proposed candidate set consists of fundamental principles which were identified through two workshops, two Delphi studies and a web-based survey. 
62|2|http://www.sciencedirect.com/science/journal/01641212/62/2|Key success factors for implementing software process improvement: a maturity-based analysis|We report on a questionnaire survey of key success factors that impact software process improvement (SPI). We analysed responses to identify factors that have a major impact, or no impact, on implementing SPI. We found four factors (reviews, standards and procedures, training and mentoring, and experienced staff) that practitioners generally considered had a major impact on successfully implementing SPI, and a further four factors (internal leadership, inspections, executive support and internal process ownership) that the more mature companies considered had a major impact on successfully implementing SPI. We also identified two factors (estimating tools and reward schemes) that may not have an impact on SPI. We briefly discuss how these factors may be broadly understood in terms of process, people, skills and leadership. We discuss some of the implications of our findings for research and practice. 
62|2||Motivators of Software Process Improvement: an analysis of practitioners' views|We present empirical findings from our study of Software Process Improvement (SPI) motivators in 13 UK software companies. Our analysis aims to provide SPI managers with some insight into designing appropriate SPI implementation strategies to maximise practitioner support for SPI.We identify what motivates developers, project managers and senior managers to be actively involved in SPI. We characterise motivations according to classic motivation theory. We find that most motivators are specific to a particular practitioner group with only a few common to all practitioner groups. Most of the common motivators are `rewarding', according to classic motivation theory. Finally we present findings showing that managers' perceptions of developer motivators are often highly inaccurate. 
62|2||Optimizing controllability of an interactive videoconferencing system with Web-based control interfaces|With the rapid advances in computing and communications technologies, interactive multimedia applications, such as videoconferencing, over wide-area networks are becoming feasible. We developed an ATM-based multipoint videoconferencing (AMV) system, which can support continuous presence with high-quality audio/video and provide Web-based control interfaces for conferees to conduct interactive videoconferences in wide-area networks. On the basis of a well-developed multimedia distribution platform, we developed an application development platform, on which we can prototype videoconferencing applications quickly. Through running a prototype AMV system, we obtained many valuable experiment results. This paper reports the practical experience and studies the controllability of the AMV system on basis of a proposed videoconference model. Since there is unpredictable delay in transmitting conferees' interaction requests, in general, we cannot simultaneously achieve high controllability and high interactivity. To gain more insight into this issue, we develop an approach which can dynamically adjust, according to network conditions, the interval for collecting interaction requests to achieve either high controllability or high interactivity. We conduct experiments to validate and evaluate the approach and, moreover, determine the proper parameter values that achieve optimum tradeoff between controllability and interactivity. 
62|2||Experimental analysis of specification language diversity impact on NPP software diversity|In order to increase computer system reliability, software fault tolerance methods have been adopted to some safety critical systems including nuclear power plants (NPPs). Prevention of software common mode failures is a crucial problem in software fault tolerance, but an effective method to solve this problem has not yet been found. Our research, to find an effective method to prevent software common mode failure s experimentally examined the impact of specification language diversity on NPP software diversity. Three specification languages were used to compose three requirements specifications, and programmers made 12 product codes from the specifications. From the product codes analysis using fault diversity criteria, we concluded that a diverse specification language method would enhance program diversity through diversification of requirements specification imperfections. 
62|2||Design of the Ajanta system for mobile agent programming|We describe the architecture and programming environment of Ajanta, a Java-based system for programming applications using mobile agents over the Internet. Agents are mobile objects which are hosted by servers on the network. Ajanta provides primitives for creating and dispatching agents, securely controlling agents at remote sites, and transferring agents from one server to another. For secure access to server resources by visiting agents, a proxy-based access control mechanism is used. The Ajanta design includes mechanisms to protect an agent's state and prevent misuse of its credentials. We describe the use of migration patterns for programming an agent's travel path. A pattern encapsulates the abstract notion of agent mobility. Pattern composition allows one to build complex travel plans using some basic migration patterns. Finally, we present agent-based distributed applications implemented using the Ajanta system to demonstrate Ajanta's functional capabilities. These include a distributed calendar management system, a middleware for sharing files over the Internet, an agent-based middleware for distributed collaborations, and an agent-based network monitoring system. 
62|3|http://www.sciencedirect.com/science/journal/01641212/62/3|ViSta: a tool suite for the visualization of behavioral requirements|ViSta is a tool suite designed to support the requirements specification phase of reactive systems. It enables the user to prepare and analyze a diagrammatic description of requirements using the statechart notation. ViSta includes a template wizard, a graphical editor and a statechart visualization tool. The template wizard guides the user through the steps necessary for the extraction of relevant information from a textual description of requirements. This information is stored in a database that is used by the statechart visualization tool to automatically generate statechart layouts. The statechart visualization tool offers a framework that combines hierarchical drawing, labeling, and floorplanning techniques. Hence, the automatically produced drawings enjoy several important aesthetic properties: they emphasize the natural hierarchical decomposition of states into substates; they have a low number of edge crossings; they have a good aspect ratio, and require a small area. These aesthetic features are invaluable since they allow the user to shift focus from organizing the mental or physical structure of the requirements document to its analysis. 
62|3||Heuristic approach for early separated filter and refinement strategy in spatial query optimization|
62|3||A placement strategy of multimedia objects in multimedia information systems|The storage management of an interactive multimedia information system (MMIS) is different from that of the conventional file system. In an MMIS, there could be relationships between the homepages stored on a disk. During browsing homepages, how to reduce the disk seek time and then to provide a fast browsing become a critic performance issue. In the paper, we propose two data placement algorithms for continuous and non-continuous spaces to place objects on a disk in order to reduce their access time. In the simulation, we conduct several experiments to validate the superiority of our algorithms over other algorithms. Besides we also show that the results derived by our algorithms are near optimal by comparing with the optimal solution. 
62|3||A public verifiable copy protection technique for still images|
62|3||Convertible authenticated encryption scheme|The digital signature provides the functions of integration, authentication, and non-repudiation for the signing message. In some applications, however, the signature only needs to be verified by some specified recipients while keeping the message secret from the public. The authenticated encryption schemes can be used to achieve this purpose. To protect the recipient's benefit in the case of a later dispute, we should further enable the recipient to convert the signature into an ordinary one that can be verified by anyone. Recently, Araki et al. proposed a convertible limited verifier scheme to resolve the problem. Their scheme equips the recipient with the ability to convert the signature into an ordinary one. However, the conversion requires the cooperation of the signer. In the paper, we proposed a convertible authenticated encryption scheme that can easily produce the ordinary signature without the cooperation of the signer. Further, the proposed scheme is more efficient than Araki et al.'s in terms of the computation complexities and the communication costs. 
62|3||Security of Lin's image watermarking system|
62|3||Contents Volume 62|
63|1|http://www.sciencedirect.com/science/journal/01641212/63/1|Editorial board|
63|1||A partitioning method for efficient system-level diagnosis|We propose a partitioning method for an adaptive distributed system-level diagnosis in arbitrary network topologies. It utilizes a biconnected component as a partitioning unit. In an adaptive distributed system-level diagnosis, testing assignment algorithm is performed before each node performs actual diagnosis to reduce the number of tests in the system. Existing testing assignment algorithms adopt non-partitioning approach covering the whole system, so they incur unnecessary extra message traffic and time. In our method, the whole system is partitioned into small groups (biconnected components), and testing assignment is performed within each group. By exploiting the property of an articulation point of a biconnected component, initial testing assignment of our method performs better than non-partitioning approach by reducing the number of nodes involved in testing assignment. It also localizes the testing reassignment caused by system reconfiguration within the related biconnected components only. It is shown that our system-level diagnosis method is correct, and the number of messages required for testing assignment of our method is smaller than that of the previous non-partitioning methods. Comparisons with other partitioning approach are also presented. 
63|1||Perception differences of software success: provider and user views of system metrics|The success of information systems (ISs) is normally defined as a composite of such performance measures as cost, time, and savings. With few systems being completed on time and within budget, application developers and users have moved to other perspectives that reflect value to organizations. Sometimes these other approaches include feedback from peers, subordinates and various other stakeholders instead of the traditional superior-subordinate performance evaluation models. Noticing the need to improve existing IS success measures for providers and users, common instruments were used to measure difference in perception over the distinct stakeholder groups of IS users and IS staff. Based on confirmatory factor analysis (CFA), the data support significant differences in perceptions between IS users and IS staff on the indicators of IS service, user satisfaction and IS staff job performance. The findings suggest efforts should be made to establish partnerships with all stakeholders to provide a more complete control process for IS development. This may include spelling out the requirements, metrics to be used, an the expectations of the deliverables and the current state of technology. Software metrics of success depend on where an individual is in the organization. Agreeing on the system success cannot occur until there is a mutual understanding of objectives and the purpose of the software. 
63|1||Business process reengineering and workflow automation: a technology transfer experience|In the last few years many public and private organizations have been changing the way of thinking their business processes to improve the quality of delivered services while achieving better efficiency and efficacy. This paper presents results and lessons learned from an on-going technology-transfer research project aimed at introducing service and technology innovation within a peripheral public administration while transferring enabling workflow methodologies and technologies to local Small and Medium Enterprises (SMEs). We discuss a process reverse engineering approach and its application in the technology transfer project. We also discuss an approach for evaluation and assessment of workflow technology and present a prototype implementation for a selected process of the subject organization. 
63|1||An abbreviated concept-based query language and its exploratory evaluation|Research on the use of conceptual information in database queries has primarily focused on semantic query optimization. Studies on the important aspects of conceptual query formulation are currently not as extensive. Only a relatively small number of works exist in this area. The existing concept-based query languages are similar in the sense that they require the user to specify the entire query path in formulating a query. In this study, we present the Conceptual Query Language (CQL), which does not require entire query paths to be specified but only their terminal points. CQL is an abbreviated concept-based query language that allows for the conceptual abstraction of database queries and exploits the rich semantics of semantic data models to ease and facilitate query formulation. CQL was developed with the aim of providing typical end-users like secretaries and administrators an easy-to-use database query interface for querying and report generation. A CQL prototype has been implemented and currently runs as a front-end to an underlying relational DBMS. A statistical experiment conducted to probe end-users' reaction to using CQL vis-à-vis SQL as a database query language indicates that end-users perform better with CQL and have a better perception of it than of SQL. This paper discusses the design of CQL, the strategies for CQL query processing, and the comparative study between CQL and SQL. 
63|1||A capacity planning model of unreliable multimedia service systems|To analyze the performance of multimedia service systems, which have unreliable resources, and to estimate the capacity requirement of the systems, we have developed a capacity planning model using an open queueing network. By acquisition of utilization, queue length of the resources and packet delay, and reliability of the systems, we have derived the service capacity of the systems along with the arrival rates of clients and the failure rates of the resources. We have validated the proposed capacity planning model by comparison of analytic results with simulation output. 
63|1||Call for papers|
63|2|http://www.sciencedirect.com/science/journal/01641212/63/2|Editorial board|
63|2||COPS: cooperative problem solving using DCOM|We present a framework that utilizes Distributed Component Object Model (DCOM) for distributed problem solving. The Cooperative Problem Solving (COPS) system provides for the coding and execution of a parallel algorithm by a number of networked computers running Microsoft Windows. We show the effectiveness of the COPS system by implementing parallel breadth-first search used in conjunction with a branch-and-bound algorithm for the Traveling Salesman Problem. 
63|2||A simple and general approach to parallelize loops with arbitrary control flow and uniform data dependence distances|Loop distribution is applied to exploit the parallelism to loops. For loops with dependence cycle(s) involving all of the statements embedded in control flow, previous methods have significant restrictions. In this paper, an algorithm is proposed to exploit the parallelism for loops under arbitrary control flow and uniform dependence distances. Experiments with benchmark cited from Vector loops, Parallel loops and Livermore loops showed that between 170 subroutines and 24 kernels tested 11 subroutines and 2 kernels had their parallelism exploited by our proposed method. 
63|2||A precise dependence analysis for multi-dimensional arrays under specific dependence direction|In process of automatic parallelizing/vectorizing constant-bound loops with multi-dimensional arrays under specific dependence direction, the Lambda test is claimed to be an efficient and precise data dependence analysis method that can check whether there exist generally inexact `real-valued' solutions to the derived dependence equations. In this paper, we propose a precise data dependence analysis method – the multi-dimensional direction vector I test. The multi-dimensional direction vector I test can be applied towards testing whether there exist generally accurate `integer-valued' solutions to the dependence equations derived from multi-dimensional arrays under specific dependence direction in constant-bound loops. Experiments with benchmark showed that the accuracy rate and the improvement rate for the proposed method are approximately 33.3% and 21.6%, respectively. 
63|2||Posting file partitioning and parallel information retrieval|The rapid growth in Internet usages brings new challenges on designing a scalable information retrieval system. To reduce the response time of a query to a large database, we parallelize both CPU computation and disk access of Boolean query processing on a cluster of workstations. The key issue is to partition the inverted file such that, during parallel query processing, each workstation consults only its own locally resident data to complete its task. To achieve this goal, we treat the set of all postings referring to a document ID as an object to be allocated in the develop data placement problem. Following the partitioning by document ID principle, we develop posting file partitioning algorithms to transform a sequential information retrieval system to a parallel information retrieval system. The advantage is that a better speed-up can be achieved by deriving from the fast sequential approach – the compressed posting file. The partitioning schemes are designed to balance work-load of workstations in parallel query processing without increasing the average disk access time per posting. The experiment shows that almost linear speed-up can be achieved and the performance bottleneck in previous work, which parallelize only disk access, can be removed. This work shows that, by using parallel processing technique, it is feasible to build a scalable information retrieval system. 
63|2||Treating uncertainty in distributed scheduling|In distributed systems the scheduler of an overloaded node may choose to transfer the execution of one or more tasks to other less busy nodes, in order to minimize their expected service times, or to increase the number of tasks that meet their deadlines, among other criteria. One solution makes use of Bayesian theory to infer the load state of the system and, based on this information, the scheduler of a busy node chooses an “appropriate” node to transfer a task too. The meaning of “appropriate” will be a function of the objectives established in the adopted location policy. In the Bayesian decision method, objectives are represented by a utility function. However, the development of a utility function can be a tricky and somewhat subjective task. In this paper, we describe a new approach that easily maps transfer objectives into useful mathematical expressions, representing location policy objectives by fuzzy sets. The proposed approach was successfully employed to add objectives to a Bayesian decision-based algorithm improving the number of tasks that are executed over time in a distributed real-time system. 
63|2||Checkpointing MPI applications on symmetric multi-processor machines using SMPCkpt|Researchers from many different areas have requirements for computational power to solve their specific problems. Symmetric multi-processor (SMP) machines are also widely available and their processing capacity is in demand particularly for applications in areas such as virtual reality and multimedia. Checkpointing provides the backbone for rollback recovery (fault-tolerance), playback debugging, process migration and job swapping. Numerous checkpointing tools have been designed and implemented but few are based on SMP machines for MPI applications. This work designs, develops, and implements SMPCkpt, a checkpointing system for symmetric multi-processor environments. SMPCkpt supports a range of facilities, including transparent checkpointing, fault detection, and rollback recovery. Two coordinated checkpointing algorithms, barrier and non-barrier, are developed and implemented in SMPCkpt that can be used to reduce the execution down time in the presence of failures. 
63|2||On the attractiveness of the star network|In defiance of the tremendous research the star network had attracted during the last decade, no parallel machine based on this network has yet been built. In this paper, we explore the possible reasons that made the star network not so viable for actual implementation. First we show that the major deficiencies related to poor scalability, cumbersome routing, and unsuitability for real applications make the construction of star-based parallel machines not so appealing. Furthermore, we show that each of the star network variants solves some of the deficiencies, but fails to solve others and introduces new concerns. We also show that the star network and its variants will find it difficult to compete with simpler topologies such as binary cubes, meshes, and k-ary n-cubes in terms of simplicity and suitability for real applications. The results in this paper can be used in two ways. In one hand, they can be used to discourage further research that might end up useless, and on the other hand they can be used as guidelines for useful research in the star interconnection network and its derivatives. 
63|2||Call for papers|
63|3|http://www.sciencedirect.com/science/journal/01641212/63/3|Editorial board|
63|3||The reflective practitioner perspective in software engineering education|This paper focuses on the application of the reflective practitioner (RP) perspective to the profession of software engineering (SE). The RP perspective guides professional people to rethink their professional creations during and after the accomplishment of the creation process. Analysis of the field of SE supports the adoption of the RP perspective to SE in general and to SE education in particular. The RP perspective emphasizes the studio––the basic training method in architecture schools––as the educational environment for design studies. In such studios students develop projects with a close guidance of a tutor. Analysis of the kind of tasks that architecture students are working on and a comparison of these tasks to the problems that SE students are facing, suggest that the studio may be an appropriate teaching method in SE as well. The paper presents the main ideas of the RP perspective and examines its fitness to SE in general and to SE education in particular. The discussion is based on analysis of the RP perspective and of the SE profession, visits to architecture studios, and conversations with tutors in architecture studios and with computing science practitioners. 
63|3||Analyzing software science data with partial repeatability|Halstead’s software science postulates that there exist physics-like laws that obey each piece of software. In this paper we reexamine this postulate by using two datasets collected from real programs, and argue that software science data are featured with partial repeatability. Conventional sciences embody the nature of full repeatability in the sense that they can either be proved repeatably in mathematics or be validated to a high accuracy repeatably in physics (experimentally). By partial repeatability we mean that complex phenomena may demonstrate an invariant property that neither can be proved in mathematics nor validated to a high accuracy in physics, but still (partially) governs the behavior of the phenomena. We propose a new kind of mathematical model, namely, parepeatic model, to characterize partial repeatability quantitatively. A parepeatic model defines the relationship between a central function and a fluctuation zone and identifies the degree of correctness of the relationship without making any statistical assumption. We develop parepeatic models for the relationships among several program complexity measures including the number of distinct operators, the number of distinct operands and the program length, among others, and present some new findings about the relationships. Illustrative case study shows that the developed parepeatic models can really help software engineering practice. 
63|3||DMMX: Dynamic memory management extensions|Dynamic memory management allows programmers to be more productive and increases system reliability and functionality. However, software algorithms for memory management are slow and non-deterministic. It is well known that object-oriented applications tend to be dynamic memory intensive. This has led programmers to eschew dynamic memory allocation for many real-time and embedded systems. Instead, programmers using Java or C++ as a development language frequently decide to allocate memory statically instead of dynamically. In this paper, we present the design of a bitmap-based memory allocator implemented primarily in combinational logic to allocate memory in a small, predictable amount of time. It works in conjunction with an application-specific instruction-set extension called the dynamic memory management extension (DMMX). Allocation is done through a complete binary tree of combinational logic, which allows constant-time object creation. The garbage collection algorithm is mark sweep, where the sweeping phase can be accomplished in constant time. This hardware scheme can greatly improve the speed and predictability of dynamic memory management. The proposed DMMX is an add-on approach, which allows easy integration into any CPU, hardware-implemented Java virtual machine, or processor in memory. 
63|3||Versioning concurrency control for hard real-time systems|The problems of hard real-time systems scheduling involve not only guaranteeing schedulability but also ensuring that shared data will not be corrupted. Maintaining shared data consistency has long been studied in database systems. In this paper, we discuss the key differences between database concurrency control and concurrency control for hard real-time systems and describe an approach to adapting advanced concurrency control techniques to systems requiring analytic worst-case latency guarantees. We describe an example concurrency control technique which avoids interference between queries and updaters. The versioning technique can be implemented with simple and predictable low-overhead algorithms and data structures and its application to a task set is driven by the schedulability analysis. Performance evaluation results via case studies and simulation experiments are presented that show that the versioning technique can improve pure locking protocols in a variety of settings. In particular, when tasks are computation-intensive, the improvement made by the versioning technique in reducing worst-case blocking and increasing schedulability is most significant. 
63|3||Performance evaluation of linear hash structure model in a nested transaction environment|
63|3||FasTLInC: a constraint-based tracing approach|In an approach to software monitoring called Dynamic Monitoring with Integrity Constraints (DynaMICs), integrity constraints are used to monitor program behavior at runtime. The constraints capture domain knowledge, limitations imposed by the design, and assumptions made by programmers. This paper introduces Fast Tracing with Links using Integrity Constraints (FasTLInC), a component of DynaMICs, that manages integrity-constraint specifications, software artifacts, and program state information, permitting tracing of constraints and artifacts, specifically requirements and source code. Because DynaMICs verifies that a program behaves in accordance to constraints, the traceability provided by FasTLInC is significant since the monitor targets the detection of faults that result from ambiguity and changes in requirements, conflicts among requirements, and change in program use. The automated identification of bi-directional links between constraints and code eliminates the laborious task of managing links, which can be problematic because of the evolutionary nature of code. 
63|3||Contents|
64|1|http://www.sciencedirect.com/science/journal/01641212/64/1|Editorial board|
64|1||Editorâs Corner|
64|1||Diagnosis of the significance of inconsistencies in object-oriented designs: a framework and its experimental evaluation|This paper presents: (a) a framework for assessing the significance of inconsistencies which arise in object-oriented design models that describe software systems from multiple perspectives, and (b) the findings of a series of experiments conducted to evaluate it. The framework allows the definition of significance criteria and measures the significance of inconsistencies as beliefs for the satisfiability of these criteria. The experiments conducted to evaluate it indicate that criteria definable in the framework have the power to create elaborate rankings of inconsistencies in models. 
64|1||Identifying the difficulties of object-oriented development|Identifying the perceptions of developers that use object-oriented (OO) system development techniques is necessary to understand why they are described as difficult to learn and use. Professional developers with a broad range of experience were asked to share their perceptions of the issues that contribute to the difficulties of using OO techniques. The 67 developers primarily were from the tele-communications and systems consulting industries in a large metropolitan area. Seventeen small groups of developers with similar levels of experience completed a 2.5-h group cognitive mapping process using a group support system (GSS). Each GSS session consisted of activities to identify difficult issues, define categories that classify the issues by similarity, rate the importance of the categories for causing difficulties, and identify causal relationships among the categories to form a cognitive map that represents the group’s shared perceptions of the difficulties of using OO techniques. The 1279 issues identified were organized into 141 categories by the 17 groups. These 141 group-generated categories were merged to identify a set of 9 overall categories to allow comparisons of perceptions across groups and levels of experience. The results reveal a common “core” of difficult issues associated with using OO techniques that was shared by novices, intermediates, and experts. Overall category group cognitive maps reveal substantial differences in the causal relationships perceived by novices and experts. 
64|1||Reengineering legacy systems for distributed environments|In the last decade, we have seen an increasing use of both the object-oriented paradigm and distributed systems. As a result, there is increasing interest in migrating and reengineering legacy systems to these new hardware technologies and software development paradigms. We define a reengineering environment that assists with the migration of legacy systems to distributed object environments. The reengineering environment includes the methodology and an integrated set of tools that support the implementation of the methodology. The methodology consists of multiple phases. First, we use reverse engineering techniques for program comprehension and design recovery. We then decompose the system into a hierarchy of subsystems by defining relationships between the entities of the underlying paradigm of the legacy system. The decomposition is driven by data mining, software metrics, and clustering techniques. Next, if the underlying paradigm of the legacy system is not object-based, we perform object-based adaptations on the subsystems. We then create components by wrapping objects and defining an interface. Finally, we allocate components to different sites by specifying the requirements of the system and characteristics of the network as an integer-programming model that minimizes the remote communication. We use middleware technologies for the implementation of the distributed object system. 
64|1||An empirical study of maintenance and development estimation accuracy|We analyzed data from 145 maintenance and development projects managed by a single outsourcing company, including effort and duration estimates, effort and duration actuals, and function points counts. The estimates were made as part of the company’s standard project estimating process that involved producing two or more estimates for each project and selecting one estimate to be the basis of client-agreed budgets. We found that effort estimates chosen as a basis for project budgets were, in general, reasonably good, with 63% of the estimates being within 25% of the actual value, and an average absolute error of 0.26. These estimates were significantly better than regression estimates based on adjusted function points, although the function point models were based on a homogeneous subset of the full data set, and we allowed for the fact that the model parameters changed over time. Furthermore, there was little evidence that the accuracy of the selected estimates was due to their becoming the target values for the project managers. 
64|1||An assessment of systems and software engineering scholars and institutions (1997â2001)|This paper presents the findings of a five-year study of the top scholars and institutions in the systems and software engineering field, as measured by the quantity of papers published in the journals of the field. The top scholar is Richard Lai of LaTrobe University in Australia, and the top institution is Carnegie Mellon University and its Software Engineering Institute. The paper lists the top 15 scholars and institutions. 
64|2|http://www.sciencedirect.com/science/journal/01641212/64/2|Editorial board|
64|2||An ERP-client benefit-oriented maintenance taxonomy|The worldwide installed base of enterprise resource planning (ERP) systems has increased rapidly over the past 10 years now comprising tens of thousands of installations in large- and medium-sized organizations and millions of licensed users. Similar to traditional information systems (IS), ERP systems must be maintained and upgraded. It is therefore not surprising that ERP maintenance activities have become the largest budget provision in the IS departments of many ERP-using organizations. Yet, there has been limited study of ERP maintenance activities. Are they simply instances of traditional software maintenance activities to which traditional software maintenance research findings can be generalized? Or are they fundamentally different, such that new research, specific to ERP maintenance, is required to help alleviate the ERP maintenance burden? This paper reports a case study of a large organization that implemented ERP (an SAP system) more than three years ago. From the case study and data collected, we observe the following distinctions of ERP maintenance: (1) the ERP-using organization, in addition to addressing internally originated change-requests, also implements maintenance introduced by the vendor; (2) requests for user-support concerning the ERP system behavior, function and training constitute a main part of ERP maintenance activity; and (3) similar to the in-house software environment, enhancement is the major maintenance activity in the ERP environment, encompassing almost 64% of the total change-request effort. In light of these and other findings, we ultimately: (1) propose a clear and precise definition of ERP maintenance; (2) conclude that ERP maintenance cannot be sufficiently described by existing software maintenance taxonomies; and (3) propose a benefits-oriented taxonomy, that better represents ERP maintenance activities. Three salient dimensions (for characterizing requests) incorporated in the proposed ERP maintenance taxonomy are: (1) who is the maintenance source? (2) why is it important to service the request? and (3) what––whether there is any impact of implementing the request on the installed module(s)? 
64|2||EIS data: findings from an evolutionary study|In such a competitive environment as the present one, it is no longer enough for managers to make the right choices, but they must also make and implement them as quickly as possible. For this reason, they need several tools to provide them with information. These tools are called executive information systems (EIS). However, an EIS is only as useful as the data it utilizes. Findings show that quantitative data is the most common type of information provided by the EIS. Operative databases in functional areas constitute the main source of internal data provided by EIS, closely followed by data warehouses. Soft data gives a great value to EIS completing usual data. Despite this fact, none of the EIS analyzed include soft data. The capability to provide access to reliable data from several sources (external and internal) is a major issue in EIS. The use of external data is increasing. The most outstanding sources are the services of financial institutions. Findings show that analyzed EIS are experiencing a transition from the first-generation EIS to second-generation EIS. 
64|2||Introduction of accounting capabilities in future service architectures|This paper proposes enhancements to the accounting support capabilities of legacy, standardised service architectures. The TINA service architecture is used as a reference, even though our approach is applicable to other models as well. The key points in this paper are the following. First, the definition of minimal extensions to the standard service architecture components, so as to enable them to offer accounting information. Second, the definition of the interface among the extended service architecture components and the new components that will undertake the accounting functionality. Third, the definition of the functionality of the new components that will undertake the accounting functionality. New components are defined so as to minimally impact the specified service architecture. 
64|2||Heterogeneous image database selection on the Web|Image databases on the Web have heterogeneous characteristics since they use different similarity measures and queries are processed depending on their own schemes. In the content-based image retrieval from distributed sites, it is crucial that the metaserver has the capability to find objects, similar to a given query object in terms of the global similarity measure, from different image databases with different local similarity measures. In this paper, we investigate the problem of finding databases, which contain more objects relevant to a given query than other databases, from many image databases dispersed on the Web. This problem is referred to as a database selection problem.We propose a new selection method to determine candidate databases. The selection of databases is based on the hybrid estimator using a few sample objects and compressed histogram information of image databases. Extensive experiments on a large number of image data demonstrate that our proposed method improves the effectiveness of distributed content-based retrieval in a heterogeneous environment. 
64|2||Detection and restoration of tampered JPEG compressed images|To protect the integrity of digital images, our scheme proposes a technique based on standard JPEG still image compression. The proposed scheme can detect tampered images and can also recover them. Images are compressed using the JPEG compression technique, and the proposed scheme also incorporates an edge detection technique. First, the edge detection technique identifies the edges of the image before image compression takes place. The obtained edge characteristic is then embedded into the image immediately after compression. If the image is tampered during transmission, the embedded edge characteristic can be used to detect the tampered areas, and these tampered areas can be reconstructed using the interpolation method and the embedded edges. Therefore, the proposed scheme allows recipients to know that the image has been tampered with during transmission. 
64|2||Sharing multiple secrets in digital images|The share of multiple secrets among participants in secret transmission is a critical topic for a new digital image scheme. The new scheme is derived from the least significant bit substitution method and the visual cryptography method. Given some secrets and a set of cover images, the proposed scheme will convert the secrets into many bit planes and modify the cover images based on these bit planes. In our method, each participant has a unique modified cover image called a stego-image. A pair of participants can therefore reconstruct a unique secret without destroying of its secrecy. Experiments shown that the quality of all stego-images is visually acceptable. The proposed scheme also prevents anyone who possesses only one stego-image from gaining information about the secret and the other stego-image. 
64|2||Linguistic kleptomania in computer science|
64|3|http://www.sciencedirect.com/science/journal/01641212/64/3|Editorial board|
64|3||An efficient broadcast data clustering method for multipoint queries in wireless information systems|Mobile computing has become a reality with the convergence of two technologies: powerful portable computers and wireless networks. The restrictions of wireless networks, such as bandwidth and energy limitations make data broadcasting an attractive data communication method. This paper addresses the clustering of wireless broadcast data for multipoint queries. By effective clustering of broadcast data, the mobile client can access the data on the air in short latency. In the paper, we define two affinity measures: data affinity and segment affinity. The data affinity is the degree that two data objects are accessed by queries, and the segment affinity is the degree that two sets of data (i.e., segments) are accessed by queries. Our method clusters data objects based on data and segment affinity measures. We show that the performance of our method is scarcely influenced by the growth of the number of queries. 
64|3||A two phase optimization technique for XML queries with multiple regular path expressions|As XML (eXtensible Markup Language) has emerged as a standard for information exchange on the World Wide Web, it has gained attention in database communities to extract information from XML seen as a database model. XML queries are based on regular path queries, which find objects reachable by given regular expressions. To answer many kinds of user queries, it is necessary to evaluate queries that have multiple regular path expressions. However, previous work on subjects such as query rewriting and query optimization in the frame work of semistructured data has usually dealt with a single regular path expression. For queries that have multiple regular path expressions we suggest a two phase optimizing technique: query rewriting using views by finding the mappings from the view’s body to the query’s body and for rewritten queries, evaluating each query conjunct and combining them. We show that our rewriting algorithm is sound and our query evaluation technique is more efficient than that of previous work on optimizing semistructured queries. 
64|3||XML query processing using document type definitions|As eXtensible Markup Language (XML) has become an emerging standard for information exchange on the World Wide Web, it has gained attention in database communities to extract information from XML seen as a database model. Data in XML can be mapped to a semistructured data model based on edge-labeled graph, and queries can be processed against it. Here, we propose new query optimization techniques using document type definitions which have the schema information about XML data. Our techniques reduce the large search space significantly while at the same time requiring less memory compared to the traditional index techniques. Also, as they preserve source database’s structure, they can process many kinds of complex queries. We implemented our techniques and provided preliminary performance results. 
64|3||An automatic load/extract scheme for XML documents through object-relational repositories|Extensible markup language (XML), a simplified version of standard generalized markup language (SGML), is designed to enable electronic text interchange in the Internet. XML documents have a rigorously described structure that may be analyzed by computers and easily understood by humans. Most current approaches store XML documents in file systems or in relational database systems. However, the nature and the design of file system or relational database schema may cause limitations on fitting with XML document structure. In this paper, we present an automatic load/extract scheme to store and retrieve XML documents through object-relational databases. We propose an architecture, called XML meta-generator (XMG), which, after reading a specific document type definition (DTD), automatically generates the corresponding object-relational database schema (OR-Schema), a DI-Decomposer and a DI-Reconstructor, which are explained as follows:1.OR-Schema––an object-relational database schema in UniSQL/X format for a specific DTD.2.DI-Decomposer––a module decomposes XML document instances (DIs) according to the specific DTD format and stores the elements into the corresponding object-relational database.3.DI-Reconstructor––a module retrieves elements from the object-relational database and reconstructs it to recover the original DI.These modules make XML documents be automatically decomposed into and reconstructed from object-relational databases in a seamless manner. Moreover, documents stored in the object-relational databases can be managed and inquired more easily than it could be in file systems or relational databases. Useful applications on various documents can also be easily built on top of the target database, such as digital libraries, data warehouses, and data or text mining systems. 
64|3||An effective query pruning technique for multiple regular path expressions|Regular path expressions are essential for formulating queries over the semistructured data without specifying the exact structure. The query pruning is an important optimization technique to avoid useless traversals in evaluating regular path expressions. While the previous query pruning optimizes a single regular path expression well, it often fails to fully optimize multiple regular path expressions. Nevertheless, multiple regular path expressions are very frequently used in nontrivial queries, and so an effective optimization technique for them is required. In this paper, we present a new technique called the two-phase query pruning that consists of the preprocessing phase and the pruning phase. Our two-phase query pruning is effective in optimizing multiple regular path expressions, and is more scalable and efficient than the combination of the previous query pruning and post-processing in that it never deals with exponentially many combinations of sub-results produced from all the regular path expressions. 
64|3||Estimating internal memory fragmentation for Java programs|Dynamic memory management has been an important part of a large class of computer programs and with the recent popularity of object oriented programming languages, more specifically Java, high performance dynamic memory management algorithms continue to be of great importance. In this paper, an analysis of Java programs, provided by the SPECjvm98 benchmark suite, and their behavior, as this relates to fragmentation, is performed. Based on this analysis, a new model is proposed which allows the estimation of the total internal fragmentation that Java systems will incur prior to the programs execution. The proposed model can also accommodate any variation of segregated lists implementation. A comparison with a previously introduced fragmentation model is performed as well as a comparison with actual fragmentation values that were extracted from SPECjvm98. Finally the idea of a test-bed application that will use the proposed model to provide to programmers/developers the ability to know, prior to a programs execution, the fragmentation and memory utilization of their programs, is also introduced. With this application at hand developers as well as designers of applications could better assess the stability, efficiency as well reliability of their applications at compile time. 
64|3||Software project management auditsââupdate and experience report|This paper describes software project management audits as they have been defined in the literature. It outlines the motivation for conducting such audits routinely, which differs markedly from earlier recommendations in the literature and outlines how an audit can be conducted efficiently, from both the points of view of the software project manager and the auditing team, using procedures that have not been previously published. Finally, it provides some data on concerns that were found during several audits in which the author has participated and recommends, based on the data, what issues project managers and project management auditors should focus on when planning projects and conducting audits in the future. 
64|3||Contents|
65|1|http://www.sciencedirect.com/science/journal/01641212/65/1|Editorial board|
65|1||Practical assessment of the models for identification of defect-prone classes in object-oriented commercial systems using design metrics|The goal of this paper is to investigate and assess the ability of explanatory models based on design metrics to describe and predict defect counts in an object-oriented software system. Specifically, we empirically evaluate the influence of design decisions to defect behavior of the classes in two products from the commercial software domain. Information provided by these models can help in resource allocation and serve as a base for assessment and future improvements.We use innovative statistical methods to deal with the peculiarities of the software engineering data, such as non-normally distributed count data. To deal with overdispersed data and excess of zeroes in the dependent variable, we use negative binomial (NB) and zero-inflated NB regression in addition to Poisson regression.Furthermore, we form a framework for comparison of models’ descriptive and predictive ability. Predictive capability of the models to identify most critical classes in the system early in the software development process can help in allocation of resources and foster software quality improvement. In addition to the correlation coefficients, we use additional statistics to assess a models’ ability to explain high variability in the data and Pareto analysis to assess a models’ ability to identify the most critical classes in the system.Results indicate that design aspects related to communication between classes and inheritance can be used as indicators of the most defect-prone classes, which require the majority of resources in development and testing phases. The zero-inflated negative binomial regression model, designed to explicitly model the occurrence of zero counts in the dataset, provides the best results for this purpose. 
65|1||Evaluating defect estimation models with major defects|Based on defect data from an inspection the number of defects present in a software product can be estimated to decide on further development or quality assurance activities. Project managers are primarily interested to determine the number of major defects in the product, which may have a strong impact on product quality as well as on project schedule and cost, if they go undetected.In this work we focus on the estimation of major defects in a software requirements document. We compare the performance of objective and subjective defect content estimation techniques. For validation of the techniques we conducted a controlled experiment with 31 inspection teams which consisted of 4–6 persons.Main findings of the experiment are: (a) Objective estimation models estimated major and all defects with similar accuracy. (b) The best subjective estimations models performed for major defects comparable to the best objective estimation models. (c) The defect detection technique used influenced the set of defects found in the team, but did not make a significant difference for the accuracy of defect estimation. (d) The second inspection cycle significantly improved the estimation accuracy for all but one estimation models. 
65|1||From diagnosis to diagnosability: axiomatization, measurement and application|Classical views on testing and their associated testing models are not dealing with the question of fault repairing but only focus on fault detection. Diagnosis consists of determining the nature of a detected fault, of locating it and hopefully repairing it. Correlatively, the only standardized quality factors implied in the detection/repair aspects of software engineering are testability and maintainability: those quality factors are misleading since they do not pinpoint this question of the location/repairing effort, that can be identified under the concept of diagnosability. This paper is thus concerned with diagnosability, its definition and the axiomatization of its expected behavior. The paper aims at:•introducing and analysing diagnosability as a significant and complementary dimension of software testability,•producing a high-level definition and axiomatization of a diagnosability measurement generic enough to be adapted to various software paradigms: this property-based approach serves as a measurement “specification”, independent on the application context and thus reusable,•detailing a diagnosability measure dedicated to data-flow software and especially test strategies impact on diagnosis and testing effort (from measure implementation to case study),•illustrating the reuse of the high-level axiomatization to the specific question of measuring the impact of assertions (or contracts for a designed by contract OO system) on diagnosis effort and preciseness.Throughout the paper, the concepts are illustrated on a case study provided by an industrial partner. At last, the reusability of the axiomatization is illustrated by proposing a measure of the impact of assertions (or contracts in a design by contract approach) on global software diagnosability. Main lessons concern both the diagnosability significance as a quality factor and the interest of an axiomatization-based methodology for building trustable software measurement. 
65|1||LMR, DTA: adaptive communication algorithms for asynchronous real-time distributed systems using token-ring networks|We present two adaptive communication algorithms called LMR and DTA, for asynchronous real-time distributed systems that use IEEE 802.5 token-ring networks and FDDI networks, respectively. The objective of the algorithms is to minimize end-to-end missed-deadline ratio of application tasks. While LMR adapts application tasks to workload fluctuations by reprioritizing trans-node application messages, DTA performs adaptation by dynamically changing token holding times of host machines, respectively. We study the performance of the algorithms through benchmark-driven experimental studies. The performance of the algorithms is compared with an adaptive resource allocation algorithm that performs adaptation by dynamically replicating application processes for load sharing. The experimental results indicate that LMR and DTA outperform the process replication algorithm for load patterns that cause communication latencies to grow faster than execution latencies. Furthermore, we observe that LMR and DTA perform as “good” as the process replication algorithm for load patterns that cause execution latencies to grow faster than communication latencies. 
65|1||Software architecture supporting integrated real-time systems|To achieve reliability, reusability, and cost reduction, a significant trend in building large complex real-time systems is to integrate separate application modules of different criticalities in a common hardware platform. An essential requirement of integrated real-time systems is to guarantee spatial and temporal partitioning among applications in order to ensure an exclusive access of physical and temporal resources to the applications. In this paper we propose software architecture, implemented as SPIRIT-Î¼Kernel, for strongly partitioned integrated real-time systems. The SPIRIT-Î¼Kernel has been designed and implemented based on a two-level hierarchical scheduling methodology such that the real-time constraints of each application can be guaranteed. To demonstrate the feasibility of the SPIRIT-Î¼Kernel, we have ported two real-time operating systems (RTOS), WindRiver’s VxWorks and Cygnus’s eCos, on the top of the microkernel. Thus, different RTOS can be applied in various partitions to provide required features for each application. Based on the measured performance results, the SPIRIT-Î¼Kernel architecture is practical and appealing due to its low overheads of kernel services and the support for dependable integration of real-time applications via scheduling algorithm. 
65|2|http://www.sciencedirect.com/science/journal/01641212/65/2|Editorial board|
65|2||Assessing the maintenance processes of a software organization: an empirical analysis of a large industrial project|
65|2||A comparison of methods for locating features in legacy software|Software engineers frequently need to locate the code that implements a specific feature of a program in order to fix a problem or add an enhancement. Several methods have recently been proposed to aid in feature location, notably the software reconnaissance method, which uses dynamic analysis of traces of execution, and the dependency graph method which involves static tracing of calling and data flow relationships in the program’s dependency graph. Most studies performed so far on these methods have used relatively modern C code. However there is a large body of existing legacy software in Fortran and similar languages which is often much more poorly structured. This paper describes a case study to locate two features in a sample of poorly structured legacy Fortran code. Both methods were applied to locate the features, along with the well known “grep” text search method for comparison. Both the software reconnaissance and dependency graph methods located both features, although some difficulties were encountered and adaptations were needed due to the very tangled nature of the code. The “grep” search method worked well for one of the two features, but was ineffective for the second, more complex case. 
65|2||A controlled experiment on inheritance depth as a cost factor for code maintenance|In two controlled experiments we compare the performance on code maintenance tasks for three equivalent programs with 0, 3, and 5 levels of inheritance. For the given tasks, which focus on understanding effort more than change effort, programs with less inheritance were faster to maintain. Daly et al. previously reported similar experiments on the same question with quite different results. They found that the 5-level program tended to be harder to maintain than the 0-level program, while the 3-level program was significantly easier to maintain than the 0-level program. We describe the design and setup of our experiment, the differences to the previous ones, and the results obtained. Ours and the previous experiments are different in several ways: We used a longer and more complex program, made an inheritance diagram available to the subjects, and added a second kind of maintenance task. When taken together, the previous results plus ours suggest that there is no such thing as usefulness or harmfulness of a certain inheritance depth as such. Code maintenance effort is hardly correlated with inheritance depth, but rather depends on other factors (partly related to inheritance depth). Using statistical modeling, we identify the number of relevant methods to be one such factor. We use it to build an explanation model of average code maintenance effort that is much more powerful than a model relying on inheritance depth. 
65|2||An empirical investigation of an object-oriented design heuristic for maintainability|This empirical study has two goals. First, to investigate the impact of a design heuristic on the maintainability of object-oriented designs, namely the ‘god class’ problem. In other words, we wish to better understand to what extent a specific design heuristic contributes to the quality of designs developed. The second goal is to investigate the relationship between that OO design heuristic and metrics. Namely, are we able to capture a specific design heuristic by applying a suitable subset of design metrics? The results of this study show that: (a) the investigated design heuristic significantly affects the performance of the participants; (b) it also affects the evolution of design structures; and (c) there is a considerable relationship between that design heuristic and metrics so that it could be feasible to conduct an assessment by using appropriate metrics. 
65|2||How well can we predict changes at architecture design time?|Two years ago, we analyzed the architecture of Sagitta 2000/SD, a large business information system being developed on behalf of Dutch Customs. We were in particular interested in assessing the capabilities of the system to accommodate future complex changes. We asked stakeholders to bring forward possible changes to the system, and next investigated how these changes would affect the software architecture. Since then, the system has been implemented and used, and actual modifications have been proposed and realized. We studied all 117 change requests submitted since our initial analysis. The present paper addresses how well we have been able to predict complex changes during our initial analysis, and how and to what extent the process to elicit and assess the impact of such changes might be improved. This study suggests that architecture analysis can be improved if we explicitly challenge the initial requirements. The study also hints at some fundamental limitations of this type of analysis: (1) fundamental modifiability-related decisions need not be visible in the documentation available, (2) the actual evolution of a system remains, to a large extent, unpredictable and (3) some changes concern complex components, and this complexity might not be known at the architecture level, and/or be unavoidable. 
65|2||Operational anomalies as a cause of safety-critical requirements evolution|
65|2||The future of programming languages: evidence to support a midwest university information systems curriculum|The university curriculum for an Information Systems program has become a challenge for colleges and universities. This paper determined the status of programming languages used by business and industry for application development in a regional area located in the Midwest. The paper reflected the percentage of what languages were being used, the increase, decrease, or non-usage of languages, ranked the current and future use of languages, and identified business and industry recommendations for curriculum content in a university program. 
65|3|http://www.sciencedirect.com/science/journal/01641212/65/3|Editorial board|
65|3||Guest Editorial|
65|3||Deadlock-free software architectures for COM/DCOM Applications|Many software projects are based on the integration of independently designed software components that are acquired on the market rather than developed within the project itself. Sometimes interoperability and composition mechanisms provided by component based integration frameworks cannot solve the problem of binary component integration in an automatic way. In this paper we present a technique to allow connectors synthesis for deadlock-free component based architectures [IEEE Proceedings of the 16th ASE, 2001] in the context of COM/DCOM applications. This work also provides guidelines to implement an automatic tool that derives the implementation of routing deadlock-free policies within the connector from the dynamic behavior specification of the COM components. Deadlock is then prevented by inserting the synthesized connector within the system via COM composition mechanisms while letting the system COM servers unmodified. We present a successful application of this technique on the (COM version of the) problem known as “The dining philosophers”. 
65|3||Enabling predictable assembly|Demands for increased functionality, better quality, and faster time-to-market in software products continue to increase. Component-based development is the software industry’s response to these demands. The industry has developed technologies such as EJB and CORBA to assemble components that are created in isolation. Component technologies available today allow designers to plug components together, but do little to allow the developer to reason about how well they will play together. Predictable assembly focuses on issues related to assembling component-based systems that predictably meet their quality attribute requirements. This paper introduces prediction-enabled component technology (PECT) as a means of packaging predictable assembly as a deployable product. A PECT is the integration of a component technology with one or more analysis technologies. Analysis technologies support prediction of assembly properties and also identify required component properties and their certifiable descriptions. This report describes the major structures of a PECT. It then discusses the means of validating the predictive powers of a PECT, which provides measurably bounded trust in design-time predictions. Last, it demonstrates the above concepts in an illustrative model problem: predicting average end-to-end latency of a ‘soft’ real time application built from off-the-shelf software components. 
65|3||Runtime verification of .NET contracts|We propose a method for implementing behavioral interface specifications on the .NET platform. Our interface specifications are expressed as executable model programs. Model programs can be run either as stand-alone simulations or used as contracts to check the conformance of an implementation class to its specification. We focus on the latter, which we call runtime verification.In our framework, model programs are expressed in the new specification language AsmL. We describe how AsmL can be used to describe contracts independently from any implementation language, how AsmL allows properties of component interaction to be specified using mandatory calls, and how AsmL is used to check the behavior of a component written in any of the .NET languages, such as VB, Câ¯, or C++. 
65|3||Revealing component properties through architectural styles|An underlying assumption in even using the phrase “component certification and system prediction” is that an understanding of individual components’ properties will lead to an understanding of a system’s properties by some form of compositional reasoning. Unfortunately, standard analytical composition techniques suffer from two problems: (1) they require that the internal structure of components be revealed in order to reason about them and (2) they deal clumsily with properties that require analysis of patterns of interaction. Here, based on the observation that a formal software architecture description itself is a constructive composition mechanism, I illustrate how the use of software architecture styles can sometimes alleviate the first problem and solve the latter. 
65|3||Trustworthy componentsââcompositionality and prediction|This article defines key requirements for an architecture-based approach to trustworthy components. We then provide a brief overview of our architecture definition language RADL with a focus on compositionality and extra-functional properties.RADL aims at very high-level specification and validation of hierarchical assemblies of distributed real-time components. Several ideas in RADL are oriented towards modern middleware technologies such as .NET and EJB and to software-engineering methods such as UML. RADL dynamic models are centered around contracts, state machines and Petri nets. These are associated to contact points and connectors for defining connection constraints in architectural specification. They define configuration and behavioral contracts when they are associated to components and architectural assemblies of components.RADL contracts permit static compatibility checks and automatic gate adaptation for true black-box reuse. Dynamic monitoring of deployed components complements this with execution-based mechanism enabling prediction of extra-functional properties during architectural design. 
65|3||Towards a composition model problem based on IEC61850|In order to derive real-world model problems for the breath of research challenges in the area of predictable assembly of certifiable software components, this paper introduces the substation automation domain, as a representative for the wide spectrum of data acquisition and process control systems. Special emphasis in the paper is put on those aspects of the application area, which make its employment as a model problem provider attractive. First, the compositions are required to meet several well defined quality requirements besides functionality and must therefore allow for the prediction of these qualities before the final system is assembled and is undergoing a potentially costly factory and/or onsite acceptance test. Second, the upcoming IEC61850 standard defines agreed upon domain models and quality attribute requirements on system operations. In order to make this problem domain attractive to research in the area of compositional reasoning of component assemblies, this paper provides the needed step of relating the domain models and the domain’s quality attribute requirements to software components and assembly properties. 
65|3||Contents Volume 65|
volume|issue|url|title|abstract
66|-|http://www.sciencedirect.com/science/journal/01641212/66|Editorial board|
66|-||Software architecture â Engineering quality attributes|
66|-||Linking usability to software architecture patterns through general scenarios|Usability is an important quality attribute to be considered during software architecture design. Up to this point, usability has been served only by separating a system’s user interface from its functionality to support iterative design. However, this has the effect of pushing revisions to achieve usability toward the end of the software development life cycle. Many usability benefits link directly to a variety of architectural tactics in addition to separation of the user interface and these benefits can be discovered early in the life cycle. For each of 27 scenarios, we identified potential usability benefits a user could realize and an architectural pattern that supports achievement of those benefits. We organized the scenarios into an emergent hierarchy of potential benefits to the user and into an emergent hierarchy of architectural tactics used in the supporting patterns. The range of architectural tactics identified in this hierarchy demonstrates that separation is far from the only architectural tactic necessary to support usability. We present techniques that permit important usability issues to be addressed proactively at architecture design time instead of retroactively after user testing. 
66|-||From problem to solution with quality attributes and design aspects|It is commonly accepted that quality attributes shape the architecture of a system. There are several means via which the architecture can support certain quality attributes. For example, to deal with reliability the system can be decomposed into a number of fault containment units, thus avoiding fault propagation. In addition to structural issues of architecture, qualities also influence architectural rules and guidelines, such as coding standards. In this paper we will focus on design aspects as a means of supporting quality attributes. An example of a design aspect is error handling functionality, which supports reliability. Quality attributes play a role in the problem domain; design aspects are elements in the solution domain.We will use an industrial case to illustrate our ideas. The discussion ranges from how design aspects are defined in the architecture based on quality attributes, to how design aspects can be used to verify the realized system against the prescribed architecture. The industrial case is a product family of medical imaging systems. For this product family, the family members are constructed from a component-based platform. Here, it is especially useful to achieve aspect-completeness of components, allowing system composition without worrying about individual design aspects. 
66|-||Patterns and performance of distributed real-time and embedded publisher/subscriber architectures|This paper makes four contributions to the design and evaluation of publisher/subscriber architectures for distributed real-time and embedded (DRE) applications. First, it illustrates how a flexible publisher/subscriber architecture can be implemented using standard CORBA middleware. Second, it shows how to extend the standard CORBA publisher/subscriber architecture so it is suitable for DRE applications that require low latency and jitter, periodic rate-based event processing, and event filtering and correlation. Third, it explains how to address key performance-related design challenges faced when implementing a publisher/subscriber architecture suitable for DRE applications. Finally, the paper presents benchmarks that empirically demonstrate the predictability, latency, and utilization of a widely used real-time CORBA publisher/subscriber architecture. Our results demonstrate that it is possible to strike an effective balance between architectural flexibility and real-time quality of service for important classes of DRE applications. 
66|-||Quality-driven software re-engineering|Software re-engineering consists of a set of activities intended to restructure a legacy system to a new target system that conforms with hard and soft quality constraints (or non-functional requirements, NFR). This paper presents a framework that allows specific NFR such as performance and maintainability to guide the re-engineering process. Such requirements for the migrant system are modeled using soft-goal interdependency graphs and are associated with specific software transformations. Finally, an evaluation procedure at each transformation step determines whether specific qualities for the new migrant system can be achieved. 
66|-||Reliability prediction for component-based software architectures|One of the motivations for specifying software architectures explicitly is the use of high level structural design information for improved control and prediction of software system quality attributes. In this paper, we present an approach for determining the reliability of component-based software architectures.Our method is based on rich architecture definition language (RADL) oriented towards modem industrial middleware platforms, such as Microsoft’s. NET and Sun’s EJB. Our methods involve parameterised contractual specifications based on state machines and thus permits efficient static analysis.We show how RADL allows software architects to predict component reliability through compositional analysis of usage profiles and of environment component reliability. We illustrate our approach with an e-commerce example and report about empirical measurements which confirm our analytical reliability prediction through monitoring in our reliability test-bed. Our evaluation confirms that prediction accuracy for software components necessitates modelling the behaviour of binary components and the dependency of provided services on required components. Fortunately, our measurements also show that an abstract protocol view of that behaviour is sufficient to predict reliability with high accuracy. The reliability of a component most strongly depends on its environment. Therefore, we advocate a reliability model parameterized by required component reliability in a deployment context. 
66|-||Analysis of a software product line architecture: an experience report|This paper describes experiences with the architectural specification and tool-assisted architectural analysis of a mission-critical, high-performance software product line. The approach used defines a “good” product line architecture in terms of those quality attributes required by the particular product line under development. Architectures are analyzed against several criteria by both manual and tool-supported methods. The approach described in this paper provides a structured analysis of an existing product line architecture using (1) architecture recovery and specification, (2) architecture evaluation, and (3) model checking of behavior to determine the level of robustness and fault tolerance at the architectural level that are required for all systems in the product line. Results of an application to a software product line of spaceborne telescopes are used to explain the approach and describe lessons learned. 
66|-||Quality-driven software architecture composition|
66|-||Contents Volume 66|
66|1|http://www.sciencedirect.com/science/journal/01641212/66/1|Editorial board|
66|1||Is software work routinized?: Some empirical observations from Indian software industry|This paper tries to find out whether software work is routinized. Six major hypotheses about routinization of software work were derived from the available literature and verified empirically. Data was collected from two software firms in Bangalore (India) using a semi-structured interview schedule and the participatory observation method. Findings of the study are discussed under six broad categories: chameleonic division of work, team work, symmetric information, level playing field, barrierless career and distributed control. The study did not find support for any of the hypotheses and thus the study rejects the routinization thesis. Some possible reasons for why software work is hard to routinize are also given. 
66|1||A quantitative and qualitative analysis of factors affecting software processes|Despite the growing body of research on software process improvement (SPI), there is still a great deal of variability in the success of SPI programmes. In this paper, we explore 26 factors that potentially affect SPI. We also consider the research strategies used to study these factors. We have used a multi-strategy approach for this study: first, by combining qualitative and quantitative analysis within case studies; second, by comparing our case study results with the results of a previously conducted survey study. Seven factors relevant to SPI (i.e. executive support, experienced staff, internal process ownership, metrics, procedures, reviews, and training) were identified by the case studies and the survey study. Two factors (reward schemes and estimating tools) were found, by both the case studies and the survey study, not to be relevant to SPI. Three additional factors (people, problems and change) were identified by the case studies. The frequency with which people, problems and change are discussed by practitioners suggests that these three factors may be pervasive in SPI, in a way that the other factors are not. These factors, however, require further investigation. 
66|1||De-motivators for software process improvement: an analysis of practitionersâ views|We present a study of software practitioners’ de-motivators for software process improvement (SPI). The aim of this study is to understand the nature of the issues that de-motivate software practitioners for SPI, so that SPI managers can better manage these de-motivators. This study compares what the SPI literature reports as the factors that hinder SPI success with software practitioners’ perception of the factors that de-motivate them. Focus groups are used to elicit the perceptions of over 200 software practitioners. Our findings show that software practitioners confirm what the literature reports as the major issues that de-motivate them for SPI. These issues are related to resistance to change, lack of evidence, imposed SPI initiatives, resource constraints and commercial pressures. Our findings also show that there are differences in de-motivators for SPI across staff groups and that these differences are related to the role that software practitioners have in software development generally. We offer these findings as insight to aid SPI managers to design more targeted SPI strategies. 
66|1||Generating test cases from class vectors|Specifications are the primary source for obtaining test cases in software testing. Specification based testing is becoming more and more important when black box components and COTS are widely used in software development. An important issue in system testing is to identify all the legitimate input. One of these systematic approaches is deriving test cases from classification tree. This approach partitions the input domain into classifications which are further partitioned into classes. Test cases are combinations of classes. Relations between classification and classes are identified and are used to construct the classification tree. From the classification tree, combination table is constructed. Test cases are derived from the combination table. However, human decisions are required in determining whether test cases derived from the combination table are legitimate. This problem is incurred by the limitation of the expressive power of classification trees which cannot express the relations among classes precisely. We propose an enhancement by expressing the relations among classes and the relations among classifications directly in vectors. We call this new approach Class Vectors. This paper presents the class vector approach with formal definitions of basic concepts and entities. We find that the expressive power of class vectors is higher than classification trees, that is, this approach can express the information given in the specification in a better way and derive all legitimate test cases with minimal human decisions. Furthermore, a method of generating legitimate test cases using class vectors is described. This method is derived from the formal semantics of class vector, hence it is theoretical sound. Finally, we discussed that the proposed method requires the least amount of human decisions, can be highly automatic and has good usability. 
66|1||Optimal testing-resource allocation with genetic algorithm for modular software systems|In software testing, an important issue is to allocate the limited testing resources to achieve maximum reliability. There are numerous publications on this issue, but the models are usually developed under the assumption of simple series or parallel modules. For complex system configuration, the optimization problem becomes difficult to solve. In this paper, we present a genetic algorithm for testing-resource allocation problems that can be used when the software systems structure is complex, and also when there are multiple objectives. We consider both system reliability and testing cost in the testing-resource allocation problems. The approach is easily implemented. Some numerical examples are shown to illustrate the applicability of the approach. 
66|1||Detecting associative shift faults in predicate testing|Fault-based predicate testing is a promising strategy for generating effective software test data. The test target is faults that may appear in predicates found in the specification or implementation of a computer program. A hierarchy of fault classes has been recently established, identifying associative shift faults (ASFs) as one of the strongest fault classes in predicate testing. The upto date proposed approaches resolve adequately the issue of test generation for weaker fault classes but do not guarantee the detection of all ASFs. In this paper we define a suitable fault model to represent this type of faults and propose a heuristic test strategy, while trying to keep low the number of required tests by fault simulation. Empirical results of the algorithm application on the TCAS-II expression suite are encouraging and suggest that it is easy to detect a significant subset of ASFs by considering only few additional tests. 
66|1||Quality assurance under the open source development model|The open source development model has defied traditional software development practices by generating widely accepted products (e.g., Linux, Apache, Perl) while following unconventional principles such as the distribution of free source code and massive user participation. Those achievements have initiated and supported many declarations about the potential of the open source model to accelerate the development of reliable software. However, the pronouncements in favor or against this model have been usually argumentative, lacking of empirical evidence to support either position. Our work uses a survey to overcome those limitations. The study explores how software quality assurance is performed under the open source model, how it differs from more traditional software development models, and whether some of those differences could translate into practical advantages given the right circumstances. The findings indicate that open source has certainly introduced a new dimension in large-scale distributed software development. However, we also discovered that the potential of open source might not be exploitable under all scenarios. Furthermore, we found that many of the open source quality assurance activities are still evolving. 
66|1||Open source softwareââan evaluation|The success of Linux and Apache has strengthened the opinion that the open source paradigm is one of the most promising strategies to enhance the maturity, quality, and efficiency of software development activities. This observation, however, has not been discussed in much detail and critically addressed by the software engineering community. Most of the claims associated with open source appear to be weakly motivated and articulated.For this reason, this paper proposes some qualitative reflections and observations on the nature of open source software and on the most popular and important claims associated with the open source approach. The ultimate goal of the paper is to identify the concepts and intuitions that are really peculiar to open source, and to distinguish them from features and aspects that can be equally applied to or found in proprietary software. 
66|2|http://www.sciencedirect.com/science/journal/01641212/66/2|Editorial board|
66|2||Combining techniques to optimize effort predictions in software project management|This paper tackles two questions related to software effort prediction. First, is it valuable to combine prediction techniques? Second, if so, how? Many commentators have suggested the use of more than one technique in order to support effort prediction, but to date there has been little or no empirical investigation to support this recommendation. Our analysis of effort data from a medical records information system reveals that there is little, or even negative, covariance between the accuracy of our three chosen prediction techniques, namely, expert judgment, least squares regression and case-based reasoning. This indicates that when one technique predicts poorly, one or both of the others tends to perform significantly better. This is a particularly striking result given the relative homogeneity of our data set. Consequently, searching for the single “best” technique, at least in this case, leads to a sub-optimal prediction strategy. The challenge then becomes one of identifying a means of determining a priori which prediction technique to use. Unfortunately, despite using a range of techniques including rule induction, we were unable to identify any simple mechanism for doing so. Nevertheless, we believe this remains an important research goal. 
66|2||Queueing network analysis: concepts, terminology, and methods|Queueing network analysis can be a valuable tool to analyze network models. However, the vast number and diverse nature of the tools available to analyze a problem can often leave the uninitiated frustrated or bewildered––awash in concepts, terminology, and methods not encountered elsewhere. As a primer for queueing network analysis, this paper emphasizes essential concepts and terminology. Selection of analytical methods based on the type of queueing network is discussed. Analytical methods are demonstrated using numerous examples and references for further study into advanced analysis are included throughout. 
66|2||Dynamic class-based queue management for scalable media servers|Real-time media servers are becoming increasingly important due to the rapid transition of the Internet from text- and graphics-based applications to multimedia-driven environments. In order to meet these ever increasing demands, real-time media servers are responsible for supporting a large number of clients with a heterogeneous mix of quality of service (QoS) requirements. In this paper, we propose a dynamic class-based queue management scheme that effectively captures the trade-off between scalability and QoS granularity in a media server. We examine the adaptiveness of the scheme and its integration with the existing schedulers. Finally, we evaluate the effectiveness of the proposed scheme through extensive simulation studies. 
66|2||3-Disjoint gamma interconnection networks|In this paper, we propose a new multistage interconnection network, called 3-disjoint gamma interconnection network (3DGIN). The 3DGIN is a modified gamma interconnection network that provides 3-disjoint paths to tolerate two switch or link faults between any source and destination pairs. The 3DGIN has lower hardware cost than GIN; furthermore, the routing and rerouting tags to generate 3-disjoint paths can be obtained in O(logN) time. To show the advantage features of 3DGIN, we also make a comparison between the gamma-related networks, the GIN, enhanced IADM, and 3DGIN. 
66|2||A stochastic software reliability model with imperfect-debugging and change-point|In this paper, we consider the software reliability growth model that incorporates with both imperfect debugging and change-point problem. The proposed model utilizes the failure data collected from software development projects to analyze the software reliability and the remaining errors of a released software program. The maximum likelihood approach is derived to estimate the unknown parameters of the new model. We investigate the new model and demonstrate its applicability in the software reliability engineering field. Our analysis suggests that if a change-point exists in a testing process, it should be considered in creating a software reliability estimation model. 
66|2||API documentation with executable examples|The rise of component-based software development has created an urgent need for effective application program interface (API) documentation. Experience has shown that it is hard to create precise and readable documentation. Prose documentation can provide a good overview but lacks precision. Formal methods offer precision but the resulting documentation is expensive to develop. Worse, few developers have the skill or inclination to read formal documentation.We present a pragmatic solution to the problem of API documentation. We augment the prose documentation with executable test cases, including expected outputs, and use the prose plus the test cases as the documentation. With appropriate tool support, the test cases are easy to develop and read. Such test cases constitute a completely formal, albeit partial, specification of input/output behavior. Equally important, consistency between code and documentation is demonstrated by running the test cases. This approach provides an attractive bridge between formal and informal documentation. We also present a tool that supports compact and readable test cases, and generation of test drivers and documentation, and illustrate the approach with detailed case studies. 
66|2||Variable-size data item placement for load and storage balancing|The rapid growth of Internet brings the need for a low cost high performance file system. Two objectives are to be pursued in building such a large scale storage system on multiple disks: load balancing and storage minimization. We investigate the optimization problem of placing variable-size data items onto multiple disks with replication to achieve the two objectives. An approximate algorithm, called LSB_Placement, is proposed for the optimization problem. The algorithm performs bin packing along with MMPacking to obtain a load balanced placement with near-optimal storage balancing. The key issue in deriving the algorithm is to find the optimal bin capacity for the bin packing to reduce storage cost. We derive the optimal bin capacity and prove that LSB_Placement algorithm is asymptotically 1-optimal on storage balancing. That is, when the problem size exceeds certain threshold, the algorithm generates a load balanced placement in which the data sizes allocated on disks are almost balanced. We demonstrate that, for various Web applications, a load balanced placement can be generated with disk capacity not exceeding 10% more than the balanced storage space. This shows that the LSB_Placement algorithm is useful in constructing a low cost and high performance storage system. 
66|2||Design and implementation of an Internet-based medical image viewing system|Electronic medical image viewers combined with picture archival and communication systems have become a common part of how hospitals manage their images. Typically, these electronic viewers are platform-dependent and are designed to make use of images stored on a workstation or local area network. However, the rise of the Internet and the ubiquity of the Web browser open up a wide range of new possibilities for distributing and accessing these images. The Internet allows us to pull information distributed across many geographically separated data sources, while the Web browser provides a common environment from where programs can be launched. This document details the design of an Internet-based, platform-independent medical image viewing system that harnesses the potential of these technologies. 
67|1|http://www.sciencedirect.com/science/journal/01641212/67/1|Editorial board|
67|1||Towards a systematic approach to the capture of patterns within a business domain|Although the reuse of patterns within business domains has many potential benefits, a lack of a situational method for sorting through the immensity of domain knowledge towards the capture of patterns may constitute a handicap. Realizing the difficulty associated with pattern development due to its empirical and knowledge-intensive nature, we propose a method to aid in the process of capturing and reusing patterns in a business domain. In this paper, we describe the first stage of the method dedicated to the capture of patterns. Our approach to pattern development is based on domain analysis principles and is process-oriented, so as to ensure a progressive and increasing understanding of the business domain and the awareness of new opportunities for improving business. We illustrate the application of the pattern development approach within the clothing manufacturing domain in the context of a business process improvement project and report the experiences obtained. 
67|1||OS Portal: an economic approach for making an embedded kernel extensible|With the rapid development of embedded system techniques and Internet technologies, network-enabled embedded devices have grown in their popularity. One critical design trend of such devices is that they are shifting from static and fixed-function systems to more dynamic and extensible ones, which are capable of running various kinds of applications. To support the diversity of the applications, kernels on these devices must be extensible. However, making embedded kernels extensible is challenging due to the shortage of resources on these devices.In this paper, we propose the operating system portal framework, which makes embedded kernels become extensible while keeping the added overheads minimal. By storing kernel modules on a resource-rich server and loading them on demand, the need for equipping a local storage on the device is eliminated. In addition, we propose mechanisms for reducing the memory requirements and performing on-line module replacement on the embedded devices.According to the performance evaluation, our approach requires only 1% of the resource requirements, compared to the traditional approaches. This allows our framework to be applied on a wide range of embedded devices. 
67|1||A dynamic protocol conformance test method|Protocol conformance test is used to promote the interoperability of protocol implementations developed by venders. Non-interoperability between protocol implementations may be caused by ambiguity and/or misinterpretation of protocol specifications by vendors or by different implementations using different options in specifications. The conventional method used for protocol conformance test has been standardized by ISO/IEC JTC1 and ITU-T with the purpose of determining whether a protocol implementation conforms to its specification. However, the conventional method sometimes gives wrong test results because the test is based on static test sequences. This problem is caused by some failed transitions of a protocol’s finite state machine included in a test sequence, which have an effect on the test result of transitions to be tested. In this paper, an approach called dynamic conformance test method (DCTM) is proposed to solve this problem. DCTM dynamically selects different test sequences during testing depending on whether alternative paths without failed transitions exist. As a result, the fault coverage of DCTM is better than that of the conventional test method. DCTM has been implemented and applied to the TCP protocol in order to demonstrate its improvement in the fault coverage compared to that of the conventional method. 
67|1||Improving system performance in contiguous processor allocation for mesh-connected parallel systems|Fragmentation is the main performance bottleneck of large, multiuser parallel computer systems. Current contiguous processor allocation techniques for mesh-connected parallel systems are restricted to rectangular submesh allocation strategies causing significant fragmentation problems. This paper presents an L-shaped submesh allocation (LSSA) strategy, which lifts the restriction on the rectangular shape formed by allocated processors in order to address the problem of fragmentation. LSSA can manipulate the shape of the required submesh to fit into the fragmented mesh system. As other strategies, LSSA first tries to allocate the conventional rectangular submeshes. If it fails, LSSA further tries to allocate more flexible L-shaped submeshes instead of signaling the allocation failure. Thus, LSSA accommodates incoming jobs faster than other strategies and results in the reduction of job response time. Extensive simulations show that LSSA performs more efficiently than other strategies in terms of the external fragmentation, the job response time and the system utilization. 
67|2|http://www.sciencedirect.com/science/journal/01641212/67/2|Editorial board|
67|2||CASE tool evaluation: experiences from an empirical study|While research activity in software engineering often results in the development of software tools and solutions that are intended to demonstrate the feasibility of an idea or concept, any resulting conclusions about the degree of success attained are rarely substantiated through the use of supporting experimental evidence. As part of the development of a prototype computer assisted software engineering (CASE) tool intended to support opportunistic software design practices, we sought to evaluate the use of the tool by both experienced and inexperienced software engineers. This work involved performing a review of suitable techniques, and then designing and performing a set of experimental studies to obtain data which could be used to assess how well the CASE tool met its design goals. We provide an assessment of how effective the chosen evaluation process was, and conclude by identifying the need for an ‘evaluation framework’ to help with guiding such studies. 
67|2||A morphology-driven string matching approach to Arabic text searching|In this paper, a morphological matching approach based on the lexical structure of Arabic words is introduced. Given a root R, the principle underlying morphological matching is to search for R in a text T to find all its occurrences. An algorithm is presented and its empirical and running time performance is analyzed. The results of searching experiments show that morphological matching provides high precision and recall ratios. But, since morphological searching is probabilistic, the results of experiments involved a number of errors caused by weak verbs. The analysis of running time performance indicates that morphological searching can be as efficient as pattern matching, if the root is entered by the user. 
67|2||Threshold signature scheme using self-certified public keys|Elaborating on the merits inherent in the self-certified public key systems, the authors propose a (t,n) threshold signature scheme using self-certified public keys. In the proposed scheme, the authentication of the self-certified individual/group public keys can be confirmed simultaneously in the procedure of verifying the individual/group signatures. As compared with threshold signature schemes designed based on the certificate-based public key systems, the proposed scheme is more efficient for generating and verifying group signatures in terms of computational efforts and communication costs. 
67|2||Controlling access in large partially ordered hierarchies using cryptographic keys|The problem of access control in a hierarchy is present in many application areas. Since computing resources have grown tremendously, access control is more frequently required in areas such as computer networks, database management systems, and operating systems. Many schemes based on cryptography have been proposed to solve this problem. However, previous schemes need large values associated with each security class. In this paper, we propose a new scheme to solve this problem achieving the following two goals. One is that the number of keys is reduced without affecting the security of the system. The other goal is that when a security class is added to the system, we need only update a few keys of the related security classes with simple operations. 
67|2||Lightweight agents for intrusion detection|We have designed and implemented an intrusion detection system (IDS) prototype based on mobile agents. Our agents travel between monitored systems in a network of distributed systems, obtain information from data cleaning agents, classify and correlate information, and report the information to a user interface and database via mediators.Agent systems with lightweight agent support allow runtime addition of new capabilities to agents. We describe the design of our Multi-agent IDS and show how lightweight agent capabilities allowed us to add communication and collaboration capabilities to the mobile agents in our IDS. 
67|2||A hybrid authentication protocol for large mobile network|
67|2||Detecting and restoring the tampered images based on iteration-free fractal compression|
67|3|http://www.sciencedirect.com/science/journal/01641212/67/3|Editorial board|
67|3||ARDIN extension for virtual enterprise integration|Virtual enterprise integration is the task of improving the performance of a temporary alliance of globally distributed independent enterprises that participate in the different phases of the life cycle of a product or service by efficiently managing the interactions among the participants. This is a very complex task that involves different approaches regarding technology, management and cultural elements. There are different proposals for enterprise integration (usually called Reference Architectures) that have been very useful in applications for a single-enterprise. However, they need to be adapted to support the new requirements that appear in virtual enterprise integration. This paper shows the modifications applied to ARDIN (Spanish acronym of Reference Architecture for INtegrated Development) to help in the design and management of an efficient and flexible virtual enterprise. The modifications are synthesized in a methodology, a set of reference models of best business practices, and in the design of a technological infrastructure. 
67|3||An empirical comparison and characterization of high defect and high complexity modules|We analyzed a large set of complexity metrics and defect data collected from six large-scale software products, two from IBM and four from Nortel Networks, to compare and characterize the similarities and differences between the high defect (HD) and high complexity modules. We observed that the most complex modules often have an acceptable quality and HD modules are not typically the most complex ones. This observation was statistically validated through hypothesis testing. Our analyses also indicated that the clusters of modules with the highest defects are usually those whose complexity rankings are slightly below the most complex ones. These results should help us better understand the complexity behavior of HD modules and guide future software development and research efforts. 
67|3||GM-WTA: An efficient workflow task allocation method in a distributed execution environment|A workflow is a collection of units of work called workflow tasks which cooperatively realize a business objective by utilizing system resources such as databases. The roles of workflow tasks are performed in the order driven by a computer representation of the workflow logic. During completing each task’s role, the remote control transfers and the remote resource accesses may often occur in a distributed workflow system. Hence, the efficient distribution of workflow components, especially workflow tasks, is so effective as to improve the performance of workflow processing. If we can place adjacent workflow tasks as close as possible and locate workflow tasks near to the required resources, we can significantly reduce the overhead of workflow processing.In this paper, we propose an efficient workflow task allocation method in a distributed workflow system, which is based on the locality principle. The method that utilizes the concept of graph partitioning can improve the performance of workflow processing by minimizing the remote processing costs incurred during workflow execution. In addition, we perform several experiments to evaluate our proposed method. 
67|3||Mining association rules on significant rare data using relative support|Recently, data mining, a technique to analyze the stored data in large databases to discover potential information and knowledge, has been a popular topic in database research. In this paper, we study the techniques discovering the association rules which are one of these data mining techniques. And we propose a technique discovering the association rules for significant rare data that appear infrequently in the database but are highly associated with specific data. Furthermore, considering these significant rare data, we evaluate the performance of the proposed algorithm by comparing it with other existing algorithms for discovering the association rules. 
67|3||Web-application centric object prefetching|The popularity of the Internet has produced an increased demand on performance and functionality in the form of Web-applications. A Web-application is a collection of logically related hyperlink Web pages that provide a specific service to Web clients. The combination of a hyperlink predictor in conjunction with Web-applications has the potential to reduce the loading time of Web-application pages. We propose a Web-application centric object prefetcher that augments the customary demand-fetching mechanism founded in most Web browsers utilized for object caching. This technique is complimentary to the operations of Web browser and proxy server caching. It predicts Web-application users’ click actions in order to launch requests for future Web page objects, such that they are available at the client’s system when they are needed. We have conducted trace-driven simulations that indicate the possible performance gains associated with Web-application centric prefetching, when combining demand fetching and prefetching within a Web-application’s bounded hyperlink domain. 
67|3||An inexact model matching approach and its applications|Matching between two model specifications is the key step of the repository-based model reuse. From the approximate specification point of view, this paper presents a quantified inexact matching theory for flexible model retrieval from large-scale model repositories. The theory specifies a model repository as two levels: a model level based on a multi-valued model specialization relationship and a fundamental function level based on a function specialization relationship. The matching degree between two models depends on their matching functions. The matching degree between two functions on a function specialization graph depends on the function-distance between them. A set of model specialization rules enables a new matching to be derived from the existing matchings. Embedded in an SQL-like command, the theory has been applied to a large-scale mathematical software model repository system. Users can use the command to retrieve the required models with an inexact query condition. Applications show that the approach is useful and tractable. 
67|3||A method of formal requirement analysis for NPP I&C systems based on UML modeling with software cost reduction|
67|3||Contents Volume 67|
68|1|http://www.sciencedirect.com/science/journal/01641212/68/1|Editorial board|
68|1||Information systems project management: an agency theory interpretation|The failure rate of information systems development projects is high. Agency theory offers a potential explanation for it. Structured interviews with 12 IS project managers about their experiences managing IS development projects show how it can be used to understand IS development project outcomes. Managers can use the results of the interviews to improve their own IS project management. Researchers can use them to examine agency theory with a larger number of project managers. 
68|1||SmartTutor: An intelligent tutoring system in web-based adult education|This paper describes the design of SmartTutor, an intelligent tutoring system implemented for distance learning in Hong Kong. Many projects and researches on online distance learning have been carried out in these few years. Most of them emphasized on the application of multimedia elements, but did not pay much attention on two crucial elements: personalization and intelligent tutoring, which are important for life-long/adult education. School of Professional and Continuing Education, The University of Hong Kong (HKU SPACE) has developed SmartTutor to address these two issues. This project can be treated as a case study of combining Internet technology, education research, and artificial intelligence. The SmartTutor has been integrated into the SPACE Online Universal Learning platform (an online learning platform), which provides support to courses offered by HKU SPACE. The effectiveness of SmartTutor has been evaluated and the results are very positive. 
68|1||Performance analysis of five interprocess communication mechanisms across UNIX operating systems|The availability of a variety of UNIX implementations necessitates the evaluation of their performance with respect to specific application characteristics. This paper concentrates on the performance evaluation of five different Interprocess Communication (IPC) mechanisms––pipes, FIFOs, messages, shared memory (with semaphores), and UNIX domain sockets. Benchmark programs were created for each mechanism, simulating a simple Producer/Consumer message transfer problem. Results were obtained for six UNIX releases––Linux 2.2.5-15, Linux 2.2.17, Linux 2.4.0-test9, RTLinux v2.3, FreeBSD 4.1, and FreeBSD 4.2. The UNIX source code was rigorously examined, and additional tests were created to determine implementation differences that would explain performance variations. Identified causes for performance differences include memory allocation schemes, transfer buffer sizes, data transfer mechanisms, locking mechanism implementations, and underlying code complexity. For all IPC mechanisms with the exception of shared memory, Linux 2.2.5-15 exhibited the best performance, followed by the remaining Linux kernels, with the FreeBSD releases finishing last. When compared against each other, pipes outperformed the other mechanisms. The developed benchmarks and ancillary tests attempt to isolate each aspect of the tested IPC mechanisms to facilitate a full understanding of IPC at the source code level. 
68|1||Amorphous program slicing|Traditional, syntax-preserving program slicing simplifies a program by deleting components (e.g., statements and predicates) that do not affect a computation of interest. Amorphous slicing removes the limitation to component deletion as the only means of simplification, while retaining the semantic property that a slice preserves the selected behaviour of interest from the original program. This leads to slices which are often considerably smaller than their syntax-preserving counterparts.A formal framework is introduced to define and compare amorphous and traditional program slicing. After this definition, an algorithm for computing amorphous slices, based on the system dependence graph, is presented. An implementation of this algorithm is used to demonstrate the utility of amorphous slicing with respect to code-level analysis of array access safety. The resulting empirical study indicates that programmers’ comprehension of array safety is improved by amorphous slicing. 
68|1||Designing electronic reference documentation for software component libraries|Contemporary software development is based on global sharing of software component libraries. As a result, programmers spend much time reading reference documentation rather than writing code, making library reference documentation a central programming tool. Traditionally, reference documentation is designed for textbooks even though it may be distributed online. However, the computer provides new dimensions of change, evolution, and adaptation that can be utilized to support efficiency and quality in software development. What is difficult to determine is how the electronic text dimensions best can be utilized in library reference documentation.This article presents a study of the design of electronic reference documentation for software component libraries. Results are drawn from a study in an industrial environment based on the use of an experimental electronic reference documentation (called Dynamic Javadoc or DJavadoc) used in a real-work situation for 4 months. The results from interviews with programmers indicate that the electronic library reference documentation does not require adaptation or evolution on an individual level. More importantly, reference documentation should facilitate the transfer of code from documentation to source files and also support the integration of multiple documentation sources. 
68|1||An assessment of systems and software engineering scholars and institutions (1998â2002)|This paper presents the findings of a five-year study of the top scholars and institutions in the Systems and Software Engineering field, as measured by the quantity of papers published in the journals of the field. The top scholar is Khaled El Emam of the Canadian National Research Council, and the top institution is Carnegie Mellon University and its Software Engineering Institute.This paper is part of an ongoing study, conducted annually, that identifies the top 15 scholars and institutions in the most recent five-year period. 
68|2|http://www.sciencedirect.com/science/journal/01641212/68/2|Editorial board|
68|2||Techniques for efficiently allocating persistent storage|Efficient disk storage is a crucial component for many applications. The commonly used method of storing data on disk using file systems or databases incurs significant overhead which can be a problem for applications which need to frequently access and update a large number of objects. This paper presents efficient algorithms for managing persistent storage which usually only require a single seek for allocations and deallocations and allow the state of the system to be fully recoverable in the event of a failure. We have developed a portable implementation of our algorithms in Java. Results in this paper demonstrate the superiority of our approach over file systems and databases for Web-related workloads. Our system has been a crucial component for persistently storing data at a number of highly accessed Web sites. We describe our experiences from a large real deployment of our system. 
68|2||Usability-based caching of query results in OLAP systems|In this paper we propose a new cache management scheme for online analytical processing (OLAP) systems based on the usability of query results in rewriting and processing other queries. For effective admission and replacement of OLAP query results, we consider the benefit of query results not only for recently issued queries but for the expected future queries of a current query. We exploit semantic relationships between successive queries in an OLAP session, which are derived from the interactive and navigational nature of OLAP query workloads, in order to classify and predict subsequent future queries. We present a method for estimating the usability of query results for the representative future queries using a probability model for them. Experimental evaluation shows that our caching scheme using the past and future usability of query results can reduce the cost of processing OLAP query workloads effectively only with a small cache size and outperforms the previous caching strategies for OLAP systems. 
68|2||On avoiding remote blocking via real-time concurrency control protocols|Locking protocols for hard-real-time systems have not generalized well from uniprocessors to multiprocessors. Bounding and reducing so-called “remote blocking” is widely recognized as an important problem for hard-real-time computing. We describe a combination of locking and versioning protocols together with a “chopping” analysis to shorten critical sections. Selective application of chopping and versioning reduces remote blocking and relaxes constraints imposed by pure locking protocols for multiprocessors. Using the same design-time information required for schedulability analysis in pure locking protocols, the integrated locking and versioning protocol can be implemented using only simple data structures with tunable bounded overheads and worst-case access times. Performance evaluation results via case studies and simulation experiments are presented that show that the protocol can improve pure locking protocols in a variety of settings. Experiments show that in some cases the number of tasks that can be guaranteed schedulability by our integrated protocol is 2.5 times more than the number of tasks that can be guaranteed schedulability by pure locking protocols. 
68|2||Concurrency control in real-time broadcast environments|Owing to the unique characteristics of real-time broadcast environments, serializability is too strong as a correctness criterion and not suitable for mobile real-time transactions. Considering that relaxing serializability such as epsilon and similarity serializability may sacrifice database consistency to some extent, we propose using a correctness criterion called weak serializability. In this paper, we formally define weak serializability at first. After the necessary and sufficient conditions for weak serializability are shown, corresponding concurrency control protocol based on this criterion is outlined for real-time broadcast environments. Finally, in a series of simulation studies, experimental results show that the proposed protocol helps more mobile real-time transactions to meet their deadlines and improves response time while database consistency is maintained. 
68|2||High performance distributed real-time commit protocol|In a distributed real-time database system, the only way to ensure transaction atomicity is to investigate and develop a real-time atomic commit protocol. This paper presents the model of distributed real-time transaction and analyses all kinds of dependencies because of data conflicts access. Based on this model, we propose an optimistic real-time commit protocol, double space commit (2SC), which is specifically designed for the high-performance distributed real-time transaction. 2SC allows a non-healthy transaction to lend its held data to the transactions in its commit dependency set. When the prepared transaction aborts, only the transactions in its abort dependency set are aborted while the transactions in its commit dependency set will execute as normal. The two properties of 2SC can reduce the data inaccessibility and the priority inversion that is inherent in distributed real-time commit processing. Extensive simulation experiments have been performed to compare the performance of the 2SC with that of other protocols such as the base protocol, the permits reading of modified prepared-data for timeliness [IEEE Transactions on Parallel and Distributed Systems 11 (2) (2000) 160–181] and the deadline-driven conflict resolution [The Computer Journal 42 (8) (1999) 674–692]. The simulation results show that 2SC has the best performance. Furthermore, it is easy to incorporate in current concurrency control protocols. 
68|2||Modeling and verification of a class of real-time systems by the use of High Level Petri Nets|Homogeneous, shared memory multiprocessors that incorporate real-time operating systems constitute in many corporations the basic platforms for developing applications of plant monitoring and automation. In this work, a template model based on the High Level Petri Net (HLPN) formalism is proposed for this class of computers. Mapping functional and timing requirements of the application software to states of this model and searching for their existence in the reachability tree of the net can verify the satisfaction of these requirements. A state searching algorithm has been developed for the case of a shared memory multiprocessor in which there is a bus-based interconnection network supporting a single communication channel for the exchange of data among the CPUs, the common memory and the computer interfaces. This algorithm groups the infinite number of states of the HLPN to a number of finite regions, identifies the region, which the desired state belongs to, and checks for the existence of a path from the initial state that leads to this region. In order to demonstrate the use of the template in the modeling and verification of timing and functional specifications of a system of the considered class, the implementation of the automation functions of a chemical reactor by a VME-bus based multiprocessor with two CPUs and running under the control of the OS-9 operating system was studied. In this study the template model was used to create a specific for this application HLPN model. The response times of two automation functions were predicted by the use of this model and compared with those derived from the operating characteristics of the reactor. 
68|3|http://www.sciencedirect.com/science/journal/01641212/68/3|Editorial board|
68|3||Best papers on Software Engineering from the SEKEâ01 Conference|
68|3||Handling variant requirements in domain modeling|Domain models describe common and variant requirements for a family of similar systems. Although most of the notations, such as UML, are meant for modeling a single system, they can be extended to model variants. We have done that and applied such extended notations in our projects. We soon found that our models with variants were becoming overly complicated, undermining the major role of domain analysis which is understanding. One variant was often reflected in many models and any given model was affected by many variants. The number of possible variant combinations was growing rapidly and mutual dependencies among variants even further complicated the domain model. We realized that our purely descriptive domain model was only useful for small examples but it did not scale up. In this paper, we describe a modeling method and a Flexible Variant Configuration tool (FVC for short) that alleviate the above mentioned problems. In our approach, we start by modeling so-called domain defaults, i.e., requirements that characterize a typical system in a domain. Then, we describe variants as deltas in respect to domain defaults. The FVC interprets variants to produce customized domain model views for a system that meets specific requirements. We implemented the above concepts using commercial tools Netron Fusion™ and Rational Rose™. In the paper, we illustrate our domain modeling method and tool with examples from the Facility Reservation System domain. 
68|3||A conceptual model completely independent of the implementation paradigm|Several authors have pointed out that current conceptual models have two main shortcomings. First, they are clearly oriented to a specific development paradigm (structured, objects, etc.). Second, once the conceptual models have been obtained, it is really difficult to switch to another development paradigm, because the model orientation to a specific development approach. This fact induces problems during development, since practitioners are encouraged to think in terms of a solution before the problem at hand is well understood, thus anticipating perhaps bad design decisions.An appropriate analysis task requires models that are independent of any implementation issues. In concrete, models should support developers to understand the problem and its constraints before any solution is identified. This paper proposes such an alternative approach to conceptual modelling, called “problem-oriented analysis method”. 
68|3||Bridging models across the software lifecycle|Numerous notations, methodologies, and tools exist to support software system modeling. While individual models help to clarify certain system aspects, the large number and heterogeneity of models may ultimately hamper the ability of stakeholders to communicate about a system. A major reason for this is the discontinuity of information across different models. In this paper, we present an approach for dealing with that discontinuity. We introduce a set of “connectors” to bridge models, both within and across the “upstream” activities in the software development lifecycle (specifically, requirements, architecture, and design). While the details of these connectors are dependent upon the source and destination models, they share a number of underlying characteristics. These characteristics can be used as a starting point in providing a general understanding of software model connectors. We illustrate our approach by applying it to a system we have designed and implemented in collaboration with a third-party organization. 
68|3||Evaluating dynamic correctness properties of domain reference architectures|The goals of evaluating correctness properties of software architectures include: (1) to provide an early opportunity to correct defects in requirements embodied in the software architecture, and (2) to ensure that the software architecture is an accurate blueprint for system implementers. While evaluation of both static and dynamic correctness properties is essential to achieve these goals, this paper focuses on dynamic correctness properties, including safety, liveness, and completeness. A new software architecture evaluation tool called Arcade, developed to support the Systems Engineering Process Activities (SEPA), provides dynamic correctness property evaluations using the complementary techniques of simulation and model checking. SEPA suggests a comprehensive approach to capture and represent yet separate different types of requirements as a multi-level software architecture. One SEPA architecture level, the Domain Reference Architecture (DRA), is employed early in the analysis process to represent requirements inherent to the domain, thereby specifying a reusable blueprint in terms of what processes, data, and timing are required, rather than how a system should be implemented. Arcade provides the architect with early feedback from correctness evaluations by leveraging the formal DRA meta-model to enable model checking and generating a Execution Space visualization to aid completeness validation. 
68|3||Application of an evaluation framework for analyzing the architecture tradeoff analysis methodSM|Evaluation is a critical analytical process in all disciplines and fields and therefore also in software engineering. For developing and analyzing an evaluation method a framework of six basic components (target, evaluation criteria, yardstick, data-gathering techniques, synthesis techniques, and evaluation process) can be applied. This framework was developed based on the analysis of theoretical and methodological evaluation concepts applied in software and non-software disciplines. In particular, in this paper we present the application of the framework for analyzing the architecture tradeoff analysis methodSM (ATAMSM), developed by the Software Engineering Institute (SEI). The results of the matching of the framework with the ATAM definition facilitate the identification of each evaluation component and stress some key aspects, such as the relevant role of stakeholders and the significance of attribute-based architectural styles in an ATAM evaluation. 
68|3||Temporal logic properties of Java objects|Applying finite-state verification techniques to software systems looks attractive because they are capable of detecting very subtle defects in the logic design of these systems. Nevertheless, the integration of existing formal verification tools within programming environments is not yet easy, mainly because of the semantic gap between widely used programming languages and the languages used to describe system requirements. In this paper, we propose a formal requirement specification notation based on linear temporal logic, with regard to object oriented program elements, such as classes and interfaces. The specification is inherently object oriented and is meant for the verification of concurrent and distributed software systems. 
68|3||Software effort estimation by analogy and âregression toward the meanâ|Estimation by analogy is, simplified, the process of finding one or more projects that are similar to the one to be estimated and then derive the estimate from the values of these projects. If the selected projects have an unusual high or low productivity, then we should adjust the estimates toward productivity values of more average projects. The size of the adjustments depends on the expected accuracy of the estimation model. This paper evaluates one adjustment approach, based on the findings made by Sir Francis Galton in the late 1800s regarding the statistical phenomenon “regression toward the mean” (RTM). We evaluate this approach on several data sets and find indications that it improves the estimation accuracy. Surprisingly, current analogy based effort estimation models do not, as far as we know, include adjustments related to extreme analogues and inaccurate estimation models. An analysis of several industrial software development and maintenance projects indicates that the effort estimates provided by software professionals, i.e., expert estimates, to some extent are RTM-adjusted. A student experiment confirms this finding, but also indicates a rather large variance in how well the need for RTM-adjustments is understood among software developers. 
68|3||Contents Volume 68|
69|1-2|http://www.sciencedirect.com/science/journal/01641212/69/1-2|Editorial board|
69|1-2||Service provider oriented management systems over open cellular network infrastructures|An interesting concept for the open cellular communications world of the near future is to enable service providers (SPs) to dynamically find, and co-operate with, the best cellular network providers (NPs), i.e., those offering desired quality levels, in the most cost-efficient manner, at a given service area region and time zone. This concept calls for an evolution of legacy management paradigms. In this direction, this paper presents parts of a Service Management System (SMS) that adopts the perspective of an SP (SP-SMS). The paper provides elements of the system design (functionality layers, components in each layer, component distribution pattern and high level functionality) and a more detailed look at the functionality of some of the SP-SMS components. The SP-SMS will be decomposed in three layers, namely session configuration, local planning, and global planning. A component type will be introduced in each layer. The role of the different components in the SP-SMS layers will be described. Emphasis will be given to the description of the functionality of the components in the local planning layer. Specifically, the problem addressed at that layer enables an SP to allocate its service demand, at the best possible (desired) quality levels, and at the most cost-efficient cellular NPs, within a service area region and time-zone. Finally, results showing merits from the introduction of an SP-SMS are presented, and concluding remarks are made. 
69|1-2||An adaptive flow control with the re-transmission policy over the serverâproxyâclient networking environment|The current server–client 2-stage-based multimedia transmission configuration is to be changed to a server–proxy–client 3-stage-based one when the broadband Internet becomes true. The traffic flow control scheme becomes different for the coming 3-stage-based multimedia transmission. In this paper, we propose a 3-stage traffic flow control scheme for transmitting multimedia data over the server–proxy–client transmission architecture. Based on the layered video streaming technique, the proposed traffic flow control scheme adjusts the presentation quality according to the currently available bandwidth, i.e., dropping/adding some video layers when the networking situation gets congested/smooth. Additionally, the proposed traffic flow control scheme provides layered video stream buffering control to smoothen video playout and adopts a re-transmission technique to reduce the loss possibility of some important video frames. With the proposed traffic flow control scheme, the coming 3-stage multimedia presentation systems can have a better optimization of bandwidth utilization and presentation quality over a broadband Internet environment. 
69|1-2||Parameter driven synthetic web database generation|To support intelligent data analysis on the web information, a data warehousing system called WHOWEDA (WareHouse Of WEb DAta) has been proposed. Unlike other relational data warehouses, WHOWEDA incorporates a Web Data Model that describes the web objects and their relationships as they are maintained within a data warehouse. A set of web operations has also been developed to manipulate the warehoused web information. In order to measure the performance of WHOWEDA and other similar systems that store and manipulate web information, a synthetic web database generator called WEDAGEN (WEb DAtabase GENerator) has been developed. It has the capability of generating collections of web objects of different sizes and complexities determined by a set of user-specified parameters. This paper presents the issues in the design and implementation of WEDAGEN. It also gives a detailed description of its system components and the strategy to generate synthetic web databases. A formal analysis of the generated web database and an empirical assessment of WEDAGEN has been reported. 
69|1-2||Efficient trip generation with a rule modeling system for crew scheduling problems|Trip generation is the most time consuming phase in the solution process of crew scheduling problems faced by large transportation companies such as airlines and railways. A large number of trips must be constructed while satisfying a complex set of regulations. In this paper, we present an efficient trip generation method that utilizes originally a rule modeling system in order to reduce the corresponding search space. Special pruning rules are defined using a high-level rule language, which also supports the modeling of the business regulations required in the scheduling process. In addition, the legality checking mechanism involved has been tuned to perform efficiently in order to cope with the vast amount of the legality checks required by the trip generator. The algorithms are tested as a module for a crew scheduling application satisfying the tight response time requirements of a production system. We present experimental results based on problems provided by a major European airline that validate the usefulness and applicability of our work. 
69|1-2||Viewpoints of DSP software and service architectures|The software architecture of a future mobile telecommunication system consists of three main parts: system infrastructure services, middleware services and application services. Infrastructure services provide access technologies and networking services for the middleware services that again provide richer capabilities for wireless applications through mobile Internet. Architecture describes the organization of software systems, components, their internal relationships and connections to the environment. Reusing architectural structures benefits companies, because the architecture is a pivotal part of any system, and a costly one to construct. Architecture is documented and reused through architectural views that describe identified stakeholders and concerns, e.g. the purpose of a system, and the feasibility of constructing, deploying, evolving and maintaining it. Views conform to special viewpoints defined for the domain. This paper describes the viewpoints selected for developing the architecture of middleware services and digital signal processing software and provides a general framework for comparing viewpoints. Comparison and analysis of the defined viewpoints show that domain and system size are the dominant issues to be considered when architectural viewpoints are being selected. 
69|1-2||Content-aware cooperative caching for cluster-based web servers|Most traditional cooperative caching schemes were developed for network file systems but were not designed for cluster-based web servers with content-based request distribution. Hence, although the schemes are applied to the web servers, their performance is not good due to high disk accesses and large block access latency. This paper proposes and evaluates a new cooperative caching scheme suitable to file systems in web servers. It uses a cache replacement policy, called duplicate first copy replacement, to avoid caching unnecessary data produced during serving requests and to minimize disk accesses. In addition it reduces block access latency required to fetch a file-block in the cooperative cache. A simulation shows that our cooperating caching decreases the disk access ratio by 29%, and reduces block access latency by about 26% when compared to the existing cooperative algorithms. 
69|1-2||Designing a high-performance database engine for the âDb4XMLâ native XML database system|eXtensible Markup Language (XML) is fast becoming the common electronic data interchange language between applications. In this paper, we describe a database engine called ‘Db4XML’, which provides storage for XML documents in native format. Db4XML is a high performance, main memory resident database engine. Db4XML is being used as a testbed for comparing various query evaluation techniques. The use of wild card (*, ?, ‘//’, etc.) in the path expressions of a query allows users to query documents whose structural information is not available. This paper lists different techniques that can be used to evaluate generalized path expressions (GPE) and presents a performance comparison of the same. A preliminary performance study of the effect of using concurrency control techniques on the various query evaluation techniques is also performed. This paper briefly discusses a suitable recovery technique for the database engine. 
69|1-2||Minimum distance queries for time series data|In this paper, we propose an indexing scheme for time sequences which supports the minimum distance of arbitrary Lp norms as a similarity measurement. In many applications where the shape of the time sequence is a major consideration, the minimum distance is a more suitable similarity measurement than the simple Lp norm. To support minimum distance queries, most of the previous work has the preprocessing step for vertical shifting which normalizes each sequence by its mean. The vertical shifting, however, has the additional overhead to get the mean of a sequence and to subtract it from each element of the sequence. The proposed method can match time series of similar shape without vertical shifting and guarantees no false dismissals. In addition, the proposed method needs only one index structure to support minimum distance queries in any arbitrary Lp norm. The experiments are performed on real data (stock price movement) to verify the performance of the proposed method. 
69|1-2||New methods for redistributing slack time in real-time systems: applications and comparative evaluations|This paper addresses the problem of scheduling hard and non-hard real-time sets of tasks that share the processor. The notions of singularity and k-schedulability are introduced and methods based on them are proposed. The execution of hard tasks is postponed in such a way that hard deadlines are not missed but slack time is advanced to execute non-hard tasks. In a first application, two singularity methods are used to schedule mixed systems with hard deterministic sets and stochastic non-hard sets. They are compared to methods proposed by other authors (servers, slack stealing), background and M/M/1. The metric is the average response time in servicing non-hard tasks and the proposed methods show a good relative performance. In a second application, the previous methods, combined with two heuristics, are used for the on-line scheduling of real-time mandatory/reward-based optional systems with or without depreciation of the reward with time. The objective is to meet the mandatory time-constraints and maximize the reward accrued over the hyperperiod. To the best of the authors’ knowledge, these are the only on-line methods proposed to address the problem and outperform Best Incremental Return, often used as a yardstick. 
69|1-2||Architecture-level modifiability analysis (ALMA)|Several studies have shown that 50–70% of the total lifecycle cost for a software system is spent on evolving the system. Organizations aim to reduce the cost of these adaptations, by addressing modifiability during the system’s development. The software architecture plays an important role in achieving this, but few methods for architecture-level modifiability analysis exist. Independently, the authors have been working on scenario-based software architecture analysis methods that focus exclusively on modifiability. Combining these methods led to architecture-level modifiability analysis (ALMA), a unified architecture-level analysis method that focuses on modifiability, distinguishes multiple analysis goals, has explicit assumptions and provides repeatable techniques for performing the steps. ALMA consists of five main steps, i.e. goal selection, software architecture description, change scenario elicitation, change scenario evaluation and interpretation. The method has been validated through its application in several cases, including software architectures at Ericsson Software Technology, DFDS Fraktarna, Althin Medical, the Dutch Department of Defense and the Dutch Tax and Customs Administration. 
69|1-2||EMBOT: An enhanced motion-based object tracker|The problem of tracking moving parts of image is a significant for many practical applications. There are good reasons to track a wide variety of objects, including airplanes, missiles, vehicles, people, animals, and microorganisms. There are several approaches that can be used to find out an optimal solution for this problem. Here, an enhanced motion-based tracking system is presented. The enhancements basically are:1.Focusing attention on informative pixels in the early stages of processing rather than waiting until a motion indication is obtained.2.Concentrating only on the region of interest that contains the moving object instead of the whole image.The effect of various factors is studied to measure the system performance and limitations. A comparison of the conventional and the proposed approach is given, and a case study is carried out in order to examine EMBOT behavior in various operational conditions. 
69|1-2||Location management in cellular mobile computing systems with dynamic hierarchical location databases|An important issue in the design of a mobile computing system is how to manage the location information of mobile clients. In the existing commercial cellular mobile computing systems, a two-tier architecture is adopted. However, the two-tier architecture is not scalable. In the literatures, a hierarchical database structure is proposed in which the location information of mobile clients within a cell is managed by the location database responsible for the cell. The location databases of different cells are organized into a tree-like structure to facilitate the search of mobile clients. Although this architecture can distribute the updates and the searching workload amongst the location databases in the system, location update overheads can be very expensive when the mobility of clients is high. In this paper, we study the issues on how to generate location updates under the distance-based method for systems using hierarchical location databases. A cost-based method is proposed for calculating the optimal distance threshold with the objective to minimize the total location management cost. Furthermore, under the existing hierarchical location database scheme, the tree structure of the location databases is static. It cannot adapt to the changes in mobility patterns of mobile clients. This will affect the total location management cost in the system. In the second part of the paper, we present a reorganization strategy to restructure the hierarchical tree of location databases according to the mobility patterns of the clients with the objective to minimize the location management cost. Extensive simulation experiments have been performed to investigate the reorganization strategy when our location update generation method is applied. 
69|1-2||Real-time broadcast algorithm for mobile computing|
69|1-2||Efficient validation of mobile transactions in wireless environments|In broadcast environments, the limited bandwidth of the upstream communication channel from the mobile clients to the server bars the application of conventional concurrency control protocols. In this paper, we propose a new variant of the optimistic concurrency control (OCC) protocol that is suitable for broadcast environments. At the server, forward validation of a transaction is done against currently running transactions, including mobile transactions and server transactions. At the mobile clients, partial backward validation of a transaction is done against committed transactions at the beginning of every broadcast cycle. Upon completion of execution, read-only mobile transactions can be validated and committed locally and update mobile transactions are sent to the server for final validation. These update transactions have a better chance of commitment because they have gone through the partial backward validation. In addition to the nice properties of conventional OCC protocols, this protocol provides autonomy between the mobile clients and the server with minimum upstream communication, which is a desirable feature to the scalability of applications running in broadcast environments. This protocol is able to process both update transactions and read-only transactions at the mobile clients at low space and processing overheads. 
69|1-2||Brute force web search for wireless devices using mobile agents|Web based search engines have been with us for a long time now. They proved to be an irreplaceable tool for researchers and Internet users all over the world. The exponential growth of the Internet has disclosed great challenges to these engines, as it is hard to maintain an accurate database of numerous web pages over time. This problem becomes wearisome, as it is often necessary to browse through several results before locating a web page that matches the given query. As today mobile devices are able to connect to the Internet through high-cost low-bandwidth wireless networks, this tactic can become very expensive. Motivated by these issues, we designed and implemented SearchSweep, a mobile agent based client-server system that uses existing search engine systems on the web to locate and download web pages. A refinement system on the server makes this solution ideal for mobile users, or users with limited bandwidth. The structure of SearchSweep platform is presented and the use of mobile agents on wireless devices is proposed as a way of attacking their limitations. 
69|3|http://www.sciencedirect.com/science/journal/01641212/69/3|Editorial board|
69|3||Ubiquitous computing|
69|3||Roam, a seamless application framework|One of the biggest challenges in future application development is device heterogeneity. In the future, we expect to see a rich variety of computing devices that can run applications. These devices have different capabilities in processors, memory, networking, screen sizes, input methods, and software libraries. We also expect that future users are likely to own many types of devices. Depending on users’ changing situations and environments, they may choose to switch from one type of device to another that brings the best combination of application functionality and device mobility (size, weight, etc.). Based on this scenario, we have designed and implemented a seamless application framework called the Roam system that can both assist developers to build multi-platform applications that can run on heterogeneous devices and allow a user to move/migrate a running application among heterogeneous devices in an effortless manner. The Roam system is based on partitioning of an application into components and it automatically selects the most appropriate adaptation strategy at the component level for a target platform. To evaluate our system, we have created several multi-platform Roam applications including a Chess game, a Connect4 game, and a shopping aid application. We also provide measurements on application performance and describe our experience with application development in the Roam system. Our experience shows that it is relatively easy to port existing applications to the Roam system and runtime application migration latency is within a few seconds and acceptable to most non-real-time applications. 
69|3||Tooling and system support for authoring multi-device applications|This paper presents a development model, tooling environment, and system support for building and deploying applications targeted to run on multiple heterogeneous end-user devices. Our approach is based on a device-independent application model and consists of three elements: (1) an automated process of specialization, by which device-specific versions of the application are generated, (2) support for hand-customization of generated applications, a process we call tweaking, both within our workbench and using external editors, and (3) a designer-in-the-loop process of generalization, by which a generic model is inferred from concrete interface artifacts such as HTML pages. We argue that this approach is cost-effective and results in usable applications that run on a variety of devices. 
69|3||Extending tuplespaces for coordination in interactive workspaces|The current interest in programming models and software infrastructures to support ubiquitous and environmental computing is heightened by the falling cost of hardware and the ubiquity of local-area wireless networking technologies. Interactive workspaces are technologically augmented team-project rooms that represent a specific sub-domain of ubiquitous computing. We argue both from related work and from our own experience with a prototype that the tuplespace model of communication forms the best basis for a coordination infrastructure for such workspaces. This paper presents the usage and characteristics expected of interactive workspaces, from which we derive a set of key system properties for any coordination infrastructure in an interactive workspace. We show that the design aspects of tuplespaces, augmented with some new extensions, yield a system model, which we call the Event Heap, that satisfies all of the desired properties. We also briefly discuss why other coordination models fall short of the desired properties, and describe our experience using our implementation of the Event Heap model. The paper focuses on a justification of the use of tuplespaces in interactive workspaces, and does not provide a detailed discussion of the Event Heap implementation or our more general experience with interactive workspaces, each of which is treated in detail elsewhere. 
69|3||The BEACH application model and software framework for synchronous collaboration in ubiquitous computing environments|In this paper, a conceptual model for synchronous applications in ubiquitous computing environments is proposed. To test its applicability, it was used to structure the architecture of the BEACH software framework that is the basis for the software infrastructure of i-LAND (the ubiquitous computing environment at FhG-IPSI). The BEACH framework provides the functionality for synchronous cooperation and interaction with roomware components, i.e. room elements with integrated information technology. To show how the BEACH model and framework can be applied, the design of a sample application is explained. Also, the BEACH model is positioned against related work. In conclusion, we provide our experiences with the current implementation. 
69|3||Contents Volume 69|
70|1-2|http://www.sciencedirect.com/science/journal/01641212/70/1-2|Editorial board|
70|1-2||Editorâs corner|
70|1-2||Software project control centers: concepts and approaches|On-line interpretation and visualization of project data are gaining increasing importance on the long road towards predictable and controllable software project execution. In the context of software development, only few techniques exist for supporting these tasks. This is caused particularly by the often insufficient use of engineering principles in the software development domain. Beyond that, interpretation and visualization techniques from other domains (such as business or production processes) are not directly applicable to software processes because of the specific characteristics of software development. A software project control center (SPCC) is a means for collecting, interpreting, and visualizing measurement data in order to provide purpose- and role-oriented information to all involved parties (e.g., project manager, quality assurer) during the execution of a project. This article presents a reference model for concepts and definitions around SPCCs. Based on this reference model, a characterization and classification of essential approaches contributing to this field is given. Finally, an outline for future research is derived from identified deficiencies of existing approaches. 
70|1-2||Supporting risks in software project management|Complex software development is a risky job. The number of unsuccessful projects surpasses the number of successful developments, particularly when large projects are analyzed. This paper describes an approach to develop, retrieve, and reuse management knowledge and experience concerned with software development risks. Scenarios are used to model risk impact and resolution strategies efficacy within risk archetypes. A risk archetype is an information structure that holds knowledge about software development risks. A risk management process organizes the use of risk archetypes within an application development effort. The process resembles a reuse process framework, where two sub-processes are respectively responsible for identifying and reusing risk information. Simulating the impact of the expected risks can support some of the decisions throughout the software development process. The contribution of this paper is to show how risk archetypes and scenario models can represent reusable project management knowledge. An observational analysis of applying such an approach in an industrial environment and a feasibility study are also described. 
70|1-2||A review of studies on expert estimation of software development effort|This paper provides an extensive review of studies related to expert estimation of software development effort. The main goal and contribution of the review is to support the research on expert estimation, e.g., to ease other researcher’s search for relevant expert estimation studies. In addition, we provide software practitioners with useful estimation guidelines, based on the research-based knowledge of expert estimation processes. The review results suggest that expert estimation is the most frequently applied estimation strategy for software projects, that there is no substantial evidence in favour of use of estimation models, and that there are situations where we can expect expert estimates to be more accurate than formal estimation models. The following 12 expert estimation “best practice” guidelines are evaluated through the review: (1) evaluate estimation accuracy, but avoid high evaluation pressure; (2) avoid conflicting estimation goals; (3) ask the estimators to justify and criticize their estimates; (4) avoid irrelevant and unreliable estimation information; (5) use documented data from previous development tasks; (6) find estimation experts with relevant domain background and good estimation records; (7) Estimate top-down and bottom-up, independently of each other; (8) use estimation checklists; (9) combine estimates from different experts and estimation strategies; (10) assess the uncertainty of the estimate; (11) provide feedback on estimation accuracy and development task relations; and, (12) provide estimation training opportunities. We found supporting evidence for all 12 estimation principles, and provide suggestions on how to implement them in software organizations. 
70|1-2||Architecting for usability: a survey|Over the years the software engineering community has increasingly realized the important role software architecture plays in fulfilling the quality requirements of a system. The quality attributes of a software system are, to a large extent determined by the system’s software architecture. In recent years, the software engineering community has developed various tools and techniques that allow for design for quality attributes, such as performance or maintainability, at the software architecture level. We believe this design approach can be applied not only to “traditional” quality attributes such as performance or maintainability but also to usability. This survey explores the feasibility of such a design approach. Current practice is surveyed from the perspective of a software architect. Are there any design methods that allow for design for usability at the architectural level? Are there any evaluation tools that allow assessment of architectures for their support of usability? What is usability? A framework is presented which visualizes these three research questions. Usability should drive design at all stages, but current usability engineering practice fails to fully achieve this goal. Our survey shows that there are no design techniques or assessment tools that allow for design for usability at the architectural level. 
70|1-2||Better sure than safe? Over-confidence in judgement based software development effort prediction intervals|The uncertainty of a software development effort estimate can be indicated through a prediction interval (PI), i.e., the estimated minimum and maximum effort corresponding to a specific confidence level. For example, a project manager may be “90% confident” or believe that is it “very likely” that the effort required to complete a project will be between 8000 and 12,000 work-hours. This paper describes results from four studies (Studies A–D) on human judgement (expert) based PIs of software development effort. Study A examines the accuracy of the PIs in real software projects. The results suggest that the PIs were generally much too narrow to reflect the chosen level of confidence, i.e., that there was a strong over-confidence. Studies B–D try to understand the reasons for the observed over-confidence. Study B examines the possibility that the over-confidence is related to type of experience or estimation process. Study C examines the possibility that the concept of confidence level is difficult to interpret for software estimators. Finally, Study D examines the possibility that there are unfortunate feedback mechanisms that reward over-confidence. 
70|1-2||Heterogeneous formal specification based on Object-Z and statecharts: semantics and verification|This work presents a specification language, called OZS, based on two formalisms: Object-Z and the statecharts. Such a specification style facilitates the modeling of systems with both reactive and functional aspects. The accent is placed on OZS semantics so as to give formal foundations to verification and simulation of OZS models. Every OZS model has a transition system as its semantic interpretation. Untimed and timed versions of the OZS semantics are presented. Both transition system models of an OZS class can be used for verification purposes by model checking. In this work, a real-word example is treated and the resulting specification is model-checked by using the Stanford Temporal Prover environment from Stanford. 
70|1-2||Flexible retrieval of Web Services|An important issue arising from Web Service applications is how to conveniently, accurately and efficiently retrieve services from large-scale and expanding service repositories. This paper proposes a flexible Web Service retrieval approach, which solves this issue by means of an orthogonal service space and establishing the multi-valued specialization relationships between services. The similarity degree between services is measured based on the specialization relationship between operations defined in services. An SQL-like flexible query language is used to support the flexible retrieval of services. The related programming environment and graphical user operation interface of the language have been implemented in the Service Grid environment. Compared with the current UDDI-based service retrieval approach, the proposed approach has the advantages of convenience, accuracy and efficiency. 
70|1-2||Timeboxing: a process model for iterative software development|In today’s business where speed is of essence, an iterative development approach that allows the functionality to be delivered in parts has become a necessity and an effective way to manage risks. In this paper we propose the timeboxing model for iterative software development in which each iteration is done in a time box of fixed duration, and the functionality to be built is adjusted to fit the time box. By dividing the time box into stages, pipelining concepts are employed to have multiple time boxes executing concurrently, leading to a reduction in the delivery time for product releases. We illustrate the use of this process model through an example of a commercial project that was successfully executed using the proposed model. 
70|1-2||Formally based modeling and inheritance of behaviour in object-oriented systems|Despite the popularity of (graphical) notations for the specification of object behaviour, there is no common understanding of what exactly constitutes the life cycle of an object. Consequently, different frameworks for the object-oriented modeling of systems allow for the specification of different kinds of behaviour. Unfortunately, the semantics of languages used in this area is often not clearly stated. In addition to the problems arising from this lack of formality, inheritance of behaviour is usually not covered by commonly used object-oriented languages. Therefore, flawless systems are difficult to build, because unpleasant surprises most easily occur if an object of a subclass is used in the context of its superclass.In the light of these problems this article states requirements for languages in the area of modeling and inheritance of object behaviour, surveys existing proposals, and introduces a novel approach. 
70|1-2||Defect evolution in a product line environment|One mechanism used for monitoring the development of the Space Shuttle flight control software, in order to minimize any risks to the missions, is the independent verification and validation (IV&V) process. Using data provided by both the Shuttle software developer and the IV&V contractor, in this paper we describe the overall IV&V process as used on the Space Shuttle program and provide an analysis of the use of metrics to document and control this process over multiple releases of this software. Our findings reaffirm the value of IV&V, show the impact of IV&V on multiple releases of a large complex software system, and indicate that some of the traditional measures of defect detection and repair are not applicable in a multiple-release environment such as this one. 
70|1-2||Uncertainty profile and software project performance: A cross-national comparison|Many software projects are inevitably associated with various types and degrees of uncertainty. It is not uncommon to see software project spiral out of control with escalated resource requirements. Thus, risk management techniques are critical issues to information system researchers. Previous empirical studies of US software firms support the adoption of development standardization and user requirement analysis techniques in risk-based software project management. Using data collected from software projects developed in Korea during 1999–2000, we conduct a comparative study to determine how risk management strategies impact software product and process performance in countries with dissimilar IT capabilities. In addition, we offer an alternative conceptualization of residual performance risk. We show that the use of residual performance risk as an intervening variable is inappropriate in IT developing countries like Korea where the role of late stage risk control remedies are critical. A revised model is proposed that generates more reliable empirical implications for Korean software projects. 
70|1-2||Research in computer science: an empirical study|In this paper, we examine the state of computer science (CS) research from the point of view of the following research questions:1.What topics do CS researchers address?2.What research approaches do CS researchers use?3.What research methods do CS researchers use?4.On what reference disciplines does CS research depend?5.At what levels of analysis do CS researchers conduct research?To answer these questions, we examined 628 papers published between 1995 and 1999 in 13 leading research journals in the CS field. Our results suggest that while CS research examines a variety of technical topics it is relatively focused in terms of the level at which research is conducted as well as the research techniques used. Further, CS research seldom relies on work outside the discipline for its theoretical foundations. We present our findings as an evaluation of the state of current research and as groundwork for future CS research efforts. 
70|1-2||Agent framework to support the computational grid|This paper presents an agent-based computational grid (ACG), which applies the concept of computational grid to agents. The ACG system is to implement a uniform higher-level management of the computing resources and services on the Grid, and provide users with a consistent and transparent interface for accessing such services. All entities in the Grid environment including computing resources and services can be represented as agents. Each entity is registered with a grid service manager. Service requestor agent locates a specific grid service by submitting requests to the grid service manager with descriptions of required services. XML is used to describe both grid service descriptions and service requestor agent’s queries. An ACG grid service can be a service agent that provides the actual grid service to the other grid member. In this paper, firstly, the conceptual model about ACG grid is described, and then the design and implementation are given. Finally, some conclusions are given. 
70|1-2||Incremental specification with SCTL/MUS-T: a case study|The past decade witnessed a great advance in the field of timed formal methods for the specification and analysis of real-time and safety-critical systems. In this context, timed automata and real-time temporal logics provide a simple, and yet general, way to model and specify the behavior of these systems. At the same time, iterative and incremental development has been massively adopted in professional practice. In order to get closer to this current trend, timed formal methods should be adapted to such lifecycle structures, getting over their traditional role of verifying that a model meets a set of fixed requirements. In the pursuit of this ultimate aim, we propose SCTL/MUS-T, a timed methodology in which many-valuedness let deal with both the uncertainty and the disagreement which are pervasive and desirable in an iterative and incremental process. To illustrate the main ideas behind SCTL/MUS-T methodology this paper focuses on the specification, synthesis and verification of the well known steam-boiler case study. 
70|1-2||An experimental evaluation of weak-branch criterion for class testing|In this paper an experiment has been presented in which we have formulated and tested various hypotheses on the fault detecting ability of the weak branch criterion ]J. Syst. Software 23 (1993) 95]. Further, sequences that satisfied the weak branch criterion were prefixed to another sequence so as to satisfy the strong branch criterion (ibid), and also to a sequence that was generated using the category partitioning method [Commun. ACM 31 (1988) 676]. The outcome of this addition has also been evaluated and compared. In particular, we have tried to test the possible influence of the following on the fault detecting ability: length of test-sequences, nature of the fault and class features. The experiment was carried out on eight classes that represented some of the important data structures used in programming and which were implemented in C++. 
70|3|http://www.sciencedirect.com/science/journal/01641212/70/3|Editorial board|
70|3||Rapid system prototyping|
70|3||Object-based hardware/software component interconnection model for interface design in system-on-a-chip circuits|The design of system-on-a-chip (SoC) circuits requires the integration of complex hardware/software components that are customized to efficiently execute a specific application. Nowadays, these components include many different embedded processors executing concurrent software tasks. In this paper, we present an object-based component interconnection model to represent both hardware and software components within a system architecture in a very high level of abstraction. This model is used in a design flow for automatic generation of hardware/software interfaces for SoC circuits. Design tools for automatic generation of embedded operating systems, hardware interfaces and associated device drivers are presented and evaluated using the results obtained with a VDSL application. 
70|3||Rapid design exploration of safety-critical distributed automotive applications via virtual integration platforms|Modern automotive applications such as Drive-by-Wire are implemented over distributed architectures where electronic control units (ECU’s) communicate via broadcast buses. In this paper, we present the concept of virtual integration platform for automotive applications. The platform provides the basis for early analysis and validation of distributed applications, therefore enhancing the current model based development process techniques that are applied to one ECU at a time. The virtual prototype includes both functional and performance (time) models of the application software, scheduling policies, and the bus communication protocols. As a result, since design errors can be found earlier in the design process before the different sub-systems are integrated in the car, savings in both production and development costs can be achieved. The virtual integration platform concept is supported by an integrated IP-based tool environment for authoring, integration, and validation. First, a model of the distributed application is built by composing models of HW and SW components. The models can be either authored or imported from different tools. Functional simulation of the overall distributed control algorithm can be carried out first. Then, the mapping phase can take place: sub-functions of the control algorithm are mapped to architectural resources (CPUs), and zero-time communication links between the sub-functions are mapped to bus protocol delay models. Changing mappings, parameters such as task priorities, and bus schedule enables the exploration of alternative implementations. The validation is carried out by simulating the resulted virtual prototype of the distributed control algorithm running on the ECU network. The design environment shortens design turn-around time by supporting (semi)-automatic configuration of the architecture model (e.g. frame packaging, redundancy level, communication matrix, bus and RTOS scheduling, etc.). 
70|3||Rapid prototyping of real-time control laws for complex mechatronic systems: a case study|Rapid prototyping of complex systems embedded in even more complex environments raises the need for a layered design approach. Our example is a mechatronic design taken from the automotive industry and illustrates the rapid-prototyping procedure of real-time-critical control laws. The approach is based on an object-oriented structuring allowing not only central control units but also distributed control units as needed by today’s designs. The implementation of control laws is a hardware-in-the-loop simulation, refined in steps and reducing the simulation part at every one of these. On the lower level, common platforms, such as FPGAs, microcontrollers or specialized platforms, can be instantiated. 
70|3||Distributed prototyping from validated specifications|We present vpl2cxx, a translator that automatically generates efficient, fully distributed C++ code from high-level system models specified in the mathematically well-founded VPL design language. As the Concurrency Workbench of the New Century (CWB-NC) verification tool includes a front-end for VPL, designers may use the full range of automatic verification and simulation checks provided by this tool on their VPL system designs before invoking the translator, thereby generating distributed prototypes from validated specifications. Besides being fully distributed, the code generated by vpl2cxx is highly readable and portable to a host of execution environments and real-time operating systems (RTOSes). This is achieved by encapsulating all generated code dealing with low-level interprocess communication issues in a library for synchronous communication, which in turn is built upon the adaptive communication environment (ACE) client-server network programming interface. Finally, example applications show that the performance of the generated code is very good, especially for prototyping purposes. We discuss two such examples, including the RETHER real-time Ethernet protocol for voice and video applications. 
70|3||FPGA based hardware acceleration for elliptic curve public key cryptosystems|This paper addresses public key cryptosystems based on elliptic curves, which are aimed to high-performance digital signature schemes. Elliptic curve algorithms are characterized by the fact that one can work with considerably shorter keys compared to the RSA approach at the same level of security. A general and highly efficient method for mapping the most time-critical operations to a configurable co-processor is proposed. By means of real-time measurements the resulting performance values are compared to previously published state of the art hardware implementations.A generator based approach is advocated for that purpose which supports application specific co-processor configurations in a flexible and straight forward way. Such a configurable CryptoProcessor has been integrated into a Java-based digital signature environment resulting in a considerable increase of its performance. The outlined approach combines in an unique way the advantages of mapping functionality to either hardware or software and it results in high-speed cryptosystems which are both portable and easy to update according to future security requirements. 
70|3||Model based testing in incremental system development|The spiraling nature of evolutionary software development processes produces executable parts of the system at the end of each loop. It is argued that these parts should consist not only of programming language code, but of executable graphical system models. As a main benefit of the use of more abstract, yet formal, modeling languages, a method for model based test sequence generation for reactive systems on the grounds of Constraint Logic Programming as well as its implementation in the CASE tool AutoFocus is presented. 
70|3||Contents Volume 70|
