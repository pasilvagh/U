volume|issue|url|title|abstract
42|-|http://www.sciencedirect.com/science/journal/09505849/42|QoS-based evaluation of file systems and distributed system services for continuous media provisioning|This paper presents a QoS-based performance analysis of file systems and distributed object services for Continuous Media (CM) provisioning, as well as the details and implementation experiences of a continuous media file system and associated CM servers. For this we have implemented QoS-driven CM servers and the Presto continuous media file system (PFS) in the context of a distributed multimedia application development environment, and validate the performance of PFS against that of the conventional Unix file system through an experimental evaluation. Using our CM server prototype, we next examine the effect of continuous media data delivery on the three different kinds of network protocols such as CORBA, UDP/IP, and TCP/IP, with respect to QoS provisioning and throughput. 
42|-||Strategies and visualization tools for enhancing user auditing of spreadsheet models|Spreadsheets are very widely used at various levels of the organization. Studies have shown that errors do occur frequently during the development of spreadsheet models. Empirical studies of operational spreadsheet models show that a large percentage of them contain errors. However, the identification of errors is difficult and tedious, and errors have led to drastically wrong decisions. It is thus important to develop new strategies and auditing tools for detecting errors. A suite of new auditing visualization tools have been designed and implemented in Visual Basic for Application (VBA), as an add-in module for easy inclusion in any Excel 97 or Excel 2000 installation. Furthermore, four strategies are proposed for detecting errors. These range from an overview strategy to identify logical components of the spreadsheet model, to specific strategies targeted at specific types of error. Illustrations show how these strategies can be supported with the new visualization tools. 
42|-||The development of OCPL, object conceptual prototyping language|This paper describes the development of OCPL (object conceptual prototyping language), an object–knowledge representation language. The language is based on CPL, conceptual prototyping language, developed at the Free University of Amsterdam. CPL has been extended to allow for the explicit representation of object-oriented constructs. These constructs include facilities for application system definition, generation and usage. A restricted use of the constraint model of CPL allows for systematic representation of events from which appropriate user interfaces can be generated. The paper describes OCPL and its relationship to CPL and related work. It also illustrates how the constraint model can be used to represent dynamics and provide intelligent user support. 
42|-||Enhanced ER to relational mapping and interrelational normalization|This paper develops a method that maps an enhanced Entity-Relationship (ER+) schema into a relational schema and normalizes the latter into an inclusion normal form (IN-NF). Unlike classical normalization that concerns individual relations only, IN-NF takes interrelational redundancies into account and characterizes a relational database schema as a whole. The paper formalizes the sources of such interrelational redundancies in ER+ schemas and specifies the method to detect them. Also, we describe briefly a Prolog implementation of the method, developed in the context of a Computed-Aided Software Engineering shell and present a case study. 
42|-||Index|
42|-||Index|
42|1|http://www.sciencedirect.com/science/journal/09505849/42/1|SoftPM: a software process management system reconciling formalism with easiness|Various formal approaches to process modeling and analysis have been proposed. With the emerging importance of practicality in this field, easiness in adopting formal technology should be taken into account. In this paper, we propose a PSEE called SoftPM that is based on a high level Petri net formalism for process modeling. To overcome the difficulty in using this formalism, SoftPM exploits a multi-level modeling mechanism for the representation of software processes. SoftPM supports three different levels for process representation. They are cognitive, MAM net, and Pr/T net. Each level has different roles in SoftPM. The cognitive-level modeling provides the means of getting free from difficulties in manipulating the modeling formalism. MAM net takes the role of core modeling formalism in SoftPM. Finally, Pr/T nets support the low-level modeling formalism as an existing Petri-net class. Moreover, SoftPM offers various analysis techniques to aid managerial decision making, as well as conventional Petri-net analysis techniques. Using a Java-based thin client/server architecture, SoftPM smoothly supports execution at distributed heterogeneous platforms over the Internet. 
42|1||An optimal representative set selection method|The optimal representative set selection problem is defined thus: given a set of test requirements and a test suite that satisfies all test requirements, find a subset of the test suite containing a minimum number of test cases that still satisfies all test requirements. Existing methods for solving the representative set selection problem do not guarantee that obtained representative sets are optimal (i.e. minimal). The enhanced zero–one optimal path set selection method [C.G. Chung, J.G. Lee, An enhanced zero–one optimal path set selection method, Journal of Systems and Software, 39(2) (1997) 145–164] solves the so-called optimal path set selection problem, and can be adapted to solve the optimal representative set selection problem by considering paths as test cases and components to be covered (e.g. branches) as test requirements. 
42|1||Software process and product improvement: an empirical assessment|Despite all the attention that software process improvement (SPI) practices have received, there is no solid evidence of how extensively they are used across organizations, and their impact on quality, cost, and on-time delivery. The findings of previous studies are based on case studies, often assessing the effectiveness of a particular methodology in a large company. In our attempt to obtain a broader insight into the software process improvement practices, we conducted a survey targeted at software developers in New England. We collected 67 responses and used descriptive statistics to analyze the survey results. In addition, we examined the impact of SPI methodologies on quality factors and compared the impact to the importance of quality factors for software developers. The Spearman correlation coefficient was used to determine the degree of correlation between the two. 
42|1||Distance-based software measurement: necessary and sufficient properties for software measures|Axiomatic approaches to software measurement present sets of necessary, but not sufficient measure axioms. The insufficiency of the measure axioms implies that they are useful to invalidate existing software measures, but not to validate them. In this paper, a set of measure axioms is presented whose sufficiency is guaranteed by measurement theory. The axioms referred to are the metric axioms, used in mathematics to define measures of distance. We present a constructive procedure that defines software measures satisfying these axioms. As an illustration of distance-based software measurement, a measure is defined for the aggregation coupling of object classes. 
42|1||An empirical study of a software reuse reference model|In software engineering there is a need for technologies that will significantly decrease effort in developing software products, increase quality of software products and decrease time-to-markets. The software development industry can be improved by utilizing and managing software reuse with an “empirically validated reference model” that can be customized for different kinds of software development enterprises. Our research thesis is that software development based on a software reuse reference model improves the competitive edge and time-to-market of many software development enterprises. The definition and study of such a model has been carried out using four steps. First, the reference model developed here is based on the existing software reuse concepts. Second, this reference model is an empirical study which uses both legacy studies and lessons learned studies. Third, the impact of the reference model on software development effort, quality, and time-to-market is empirically derived. Fourth, an initial set of successful cases, which are based on the software reuse reference model utilization, are identified. The main contribution of this paper is a reference model for the practice of software reuse. A secondary contribution is an initial set of cases from software development enterprises which are successful in the practice of reuse in terms of decreased effort, increased quality and a high correlation in their application of our software reuse reference model activities. 
42|10|http://www.sciencedirect.com/science/journal/09505849/42/10|Principles for modeling language design|Modeling languages, like programming languages, need to be designed if they are to be practical, usable, accepted, and of lasting value. We present principles for the design of modeling languages. To arrive at these principles, we consider the intended use of modeling languages. We conject that the principles are applicable to the development of new modeling languages, and for improving the design of existing modeling languages that have evolved, perhaps through a process of unification. The principles are illustrated and explained by several examples, drawing on object-oriented and mathematical modeling languages. 
42|10||ROCS: an object-oriented class-level testing system based on the Relevant Observable ContextS technique|Given an algebraic specification of a class of objects, we define a fundamental pair as two equivalent terms generated by substituting all the variables on both sides of an axiom by normal forms. For any implementation error in the class, if it can be revealed by two equivalent terms in general, it can also be revealed by a fundamental pair. Hence, we need only select test cases from the set of fundamental pairs instead of equivalent pairs in general. We highlight an algorithm for selecting a finite set of fundamental pairs as test cases. Further, by using the relevant observable contexts technique, we highlight another algorithm to determine whether the objects resulting from executing a fundamental pair are observationally equivalent. If not, it reveals an implementation error.Using these algorithms, we have constructed a system to test object-oriented programs at class-level. We describe in detail the implementation of a prototype system, including the data structure of a Data member Relevant Graph (DRG) for the class, the procedures for the construction and path traversal of the DRG, the generation and execution of relevant observable contexts on the objects under test, and the reporting of implementation errors. The implementation illustrates an innovative idea of embedding testing algorithms into an interpreter to facilitate software testing. 
42|10||A study of development and maintenance in Norway: assessing the efficiency of information systems support using functional maintenance|The large amount of work on information systems being taken up by maintenance activities has been one of the arguments of those speaking about a ‘software crisis’. We have investigated the applicability of this statement, and propose instead to look at the percentage of work being done on functional maintenance to assess the efficiency of the information systems support in an organisation. This paper presents the main results from a survey investigation performed in Norwegian organisations within the area of software development and maintenance. The results are based on responses from 53 Norwegian organisations. The investigation is compared with other investigations, both those performed in Norway where a similar investigation was conducted in 1994 and investigations performed in other countries. Similar to the investigation from 1994, the situation is better when looking at the situation from a functional point of view rather than using the traditional maintenance measures. Somewhat surprisingly, the amount of both traditional and functional maintenance work are significantly higher than in the similar investigation done five years earlier. It is also significantly higher than what was found in earlier investigations carried out in the USA and in other countries. One reason for this seems to be the extra maintenance and replacement-oriented work necessary to deal with the Y2K-problem. Even when considering this, too much of the scarce IT-personnel spent their time on tasks that do not add value for the users of the systems. 
42|10||Neuro-genetic prediction of software development effort|Prediction of resource requirements of a software project is crucial for the timely delivery of quality-assured software within a reasonable timeframe. Many conventional (model-based) and AI-oriented (model-free) resource estimators have been proposed in the recent past. This paper presents a novel genetically trained neural network (NN) predictor trained on historical data. We demonstrate substantial improvement in prediction accuracy by the neuro-genetic approach as compared to both a regression-tree-based conventional approach, as well as backpropagation-trained NN approach reported recently. The superiority of this new predictor is established using n-fold cross validation and Student's t-test on various partitions of merged Cocomo and Kemerer data sets incorporating data from 78 real-life software projects. 
42|10||Assessing organisational obstacles to component-based development: a case study approach|This paper discusses some human, social and organisational issues affecting the introduction of Component-Based Development (CBD) in organisations. In particular, the paper presents the outcome of a case study aimed at assessing organisational obstacles influencing successful application of CBD in the industry. We present some organisational problems experienced by three organisations in adopting and implementing CBD, including cognitive skills, disincentives, organisational politics and organisational culture. In each case we suggest some solutions that developers and managers should consider in order to minimise these organisational problems. We suggest that applying social–technical approaches can minimise the impact of these organisational obstacles. Examples of social–technical approaches include maintaining a relationship with customers throughout the development process and eliciting support from key sponsors and stakeholders. 
42|11|http://www.sciencedirect.com/science/journal/09505849/42/11|Using genetic algorithms to work out index configurations for the class-hierarchy indexing in object databases|Efficient indexing on a class hierarchy is essential for the achievement of high performance in query evaluation for object databases. In this paper, we present a practical indexing scheme, Partition Index Configuration Scheme (PINS), which provides good index configurations for any real database environment. PINS considers the distribution of key values, as well as query patterns such as query frequency on each class. PINS can easily be applied to any database system, since it uses the B+-tree structure. We develop a cost model and, through experiments, demonstrate the performance of the proposed policy over various class hierarchies. 
42|11||Proposing an information flow analysis model to measure memory load for software development|System analysis is used to develop application software. However, the conventional approach has difficulty to ensure that the software has high user-satisfaction. This research uses the user's memory load to measure the degree of satisfaction of application software. An information flow analysis model is proposed for analyzing the degree of memory load. Ten types of operations, including six internal operations and four external operations, are adopted to measure the memory load. The conventional data flow diagram (DFD) and state transition diagram (STD) are modified by adding more functional components to estimate the memory load. The information flow analysis is demonstrated by building a qualitative analysis software which requests heavier mental works but has simple software architecture. 
42|11||The financial profile of the software industry between 1980 and 1994|The objective of this work is to trace the financial profile of firms in the software industry between the years 1980–1994 using data from COMPUSTAT. Our results are useful both to academics, who strive to link theory to empirical observation, and to practitioners trying to better understand the environment in which they operate. Our analysis suggests that (1) the software industry has been steadily expanding over the sample period, (2) the relative market power of the industry leaders has remained fairly stable, (3) the median firm operating in the industry has became smaller through time, (4) firms have been spending increasingly more on R&D and less in capital investment through time, (5) profitability was declining over the first half of the sample period, stabilizing in the second half, and (6) the risk of bankruptcy for the median firm has similarly declined over the sample period. 
42|11||Dynamic data flow analysis for Java programs|A large portion of high-level computer programs consists of data declaration. Thus, an increased focus on testing the data flow aspects of programs should be considered. In this paper, we consider testing the data flow in Java programs dynamically. Data flow analysis has been applied for testing procedural and some object-oriented programs. We have extended the dynamic data flow analysis technique to test Java programs and show how it can be applied to detect data flow anomalies. 
42|11||Query language approach based on the deductive object-oriented database paradigm|The integration of data-oriented (structural), behavioral and deductive aspects is necessary in next generation information systems. The deductive object-oriented database paradigm offers a very promising starting point for the implementation of these kinds of information systems. So far in the context of this paradigm a big problem has been the lack of a query language suitable to an ordinary end user. Typically in existing proposals for deductive object-oriented databases the user has to master well both logic-based rule formulation and object-oriented programming. In this paper we introduce a set of high-level querying primitives which increases the degree of declarativeness compared to the deductive object-oriented query languages proposed so far. In terms of these primitives it is possible to offer for end users such application-specific concepts and structures whose interpretation is obvious to users but whose specification is too demanding for them. By combining these primitives in queries the user can integrate data-oriented, behavioral and deductive aspects with each other in a concept-oriented way. Our query language approach is based on the incorporation of deductive aspects to object-orientation. Among others this means that deductive aspects of objects are inherited in the specialization/generalization hierarchy like any other properties of objects. 
42|12|http://www.sciencedirect.com/science/journal/09505849/42/12|Introduction to the special issue on: model-based statistical testing of software intensive systems|
42|12||Improving software quality using statistical testing techniques|Cleanroom usage-based statistical testing techniques have been incorporated into the software development process for a program in the Electronic Systems business of Raytheon Company. Cost-effectively improving the quality of software delivered into systems integration was a driving criterion for the program. Usage-based statistical testing provided the capability to increase the number of test cases executed on the software and to focus the testing on expected usage scenarios. The techniques provide quantitative methods for measuring and reporting testing progress, and support managing the testing process. This article discusses the motivation and approach for adopting usage-based statistical testing techniques, and experiences using these testing techniques. 
42|12||Improving a product with usage-based testing|This paper presents the introduction and application of a usage-based testing approach for the acceptance of an aircraft traffic flow management system by the FAA. The FAA needed an approach to provide more effective testing than that typically achieved in their requirements-based acceptance testing. Testing in a manner more consistent with operational usage was desired. A testing technology that was new to the organization was introduced and applied successfully. The approach resulted in a greatly improved product being released to the field, a cost-effective approach for testing, and a commitment to apply the technology across the organization. 
42|12||Applying models in your testing process|Model-based testing allows large numbers of test cases to be generated from a description of the behavior of the system under test. Given the same description and test runner, many types of scenarios can be exercised and large areas of the application under test can be covered, thus leading to a more effective and more efficient testing process. 
42|12||A constraint-based approach to the representation of software usage models|Software usage models are the basis for statistical testing. They derive their structure from specifications and their probabilities from evolving knowledge about the intended use of the software product. The evolving knowledge comes from developers, customers and testers of the software system in the form of relationships that should hold among the parameters of a model. When software usage models are encoded as Markov chains, their structure can be represented by a system of linear constraints, and many of the evolving relationships among model parameters can be represented by convex constraints. Given a Markov chain usage model as a system of convex constraints, mathematical programming can be used to generate the Markov chain transition probabilities that represent a specific software usage model. 
42|12||TML: a description language for Markov chain usage models|Finite-state Markov chains have proven useful as a model to characterize a population of uses of a software system. This paper presents a language (TML) for describing these models in a manner that supports development, reuse, and automated testing. TML provides a simple representation of usage models, while formalizing modeling techniques already in use informally. 
42|12||Partition testing with usage models|The fundamental statistical strategy of improving sampling efficiency through partitioning the population is applied to software testing. Usage models make it possible to apply this strategy to improve the efficiency of testing. The testing budget is allocated to the blocks of the partition, and the software is executed on the sample of uses drawn from each block or sub-population of potential uses. Usage models support many strategies for automated partitioning and generating test cases from the partitioned population. Two strategies are shown here with the efficiency gains demonstrated. 
42|12||Stopping criteria for statistical testing|The decision to stop testing can be based on a number of criteria, such as (1) the confidence in a reliability estimate; (2) the degree to which testing experience has converged to the expected use of the software; and (3) model coverage criteria based on a degree of state, arc, or path coverage during crafted and random testing. In practice it is best to use multiple stopping criteria. For example, further evaluation of the testing performed is needed if the measure of correspondence between testing experience and expected use of the software indicates that the testing experience closely matches the expected use of the software, yet the variance of the reliability estimate is unacceptably large. One test of equality of testing experience and expected use is the Kullback discriminant from the usage chain to the testing chain. A new measure of “approximate equality” is introduced here for use in conjunction with the Kullback discriminant. 
42|12||Measuring complexity and coverage of software specifications|Coverage testing in the context of Markov chain usage models refers to coverage of a model of the specification and profile of intended use, rather than coverage of the code that implements the specification. A new measure of specification complexity based on the number of statistically typical paths through the model of the specification is derived. Formulae are presented to compute bounds on the expected number of test cases required to achieve state and arc coverage. Formulae are presented to compare different usage models with respect to the amount of testing required to achieve coverage of typical paths. Convexity properties are established for these formulae to facilitate their use in optimization calculations that are used to generate transition probabilities for the usage models. 
42|12||Representation and optimization of software usage models with non-Markovian state transitions|We extend Markov usage models by admitting non-Markovian transitions between states. The suggested description syntax preserves the visualization capacities of Markov models and, although it is kept simple, provides the computational power of a universal programming language. It is shown that a previously introduced optimization technique for computing optimal test transition probabilities can be generalized to the presented framework. A medium-size example from a real-world application illustrates the approach. 
42|12||A Markov chain model for predicting the reliability of multi-build software|In previous work we developed a method to model software testing data, including both failure events and correct behavior, as a finite-state, discrete-parameter, recurrent Markov chain. We then showed how direct computation on the Markov chain could yield various reliability related test measures. Use of the Markov chain allows us to avoid common assumptions about failure rate distributions and allows both the operational profile and test coverage of behavior to be explicitly and automatically incorporated into reliability computation.Current practice in Markov chain based testing and reliability analysis uses only the testing (and failure) activity on the most recent software build to estimate reliability. In this paper we extend the model to allow use of testing data on prior builds to cover the real-world scenario in which the release build is constructed only after a succession of repairs to buggy pre-release builds. Our goal is to enable reliability prediction for future builds using any or all testing data for prior builds.The technique we present uses multiple linear regression and exponential smoothing to merge multi-build test data (modeled as separate Markov chains) into a single Markov chain which acts as a predictor of the next build of testing activity. At the end of the testing cycle, the predicted Markov chain represents field use. It is from this chain that reliability predictions are made. 
42|13|http://www.sciencedirect.com/science/journal/09505849/42/13|Identifying relevant constraints for semantic query optimization|Semantic query optimization is the process of utilizing information implied by integrity constraints to reformulate the query into one that generates the same set of answers in a more efficient way. The difficulties of identifying relevant integrity constraints for a given query have been well recognized as have the various solutions. However, most of the previous works consider the query consisting of join(s) of base relations and the integrity constraints on base relations only. We generalize these restrictions and propose a method of identifying relevant integrity constraints for queries involving any combinations of joins and unions of base and defined relations. Our method utilizes a query graph that can be constructed dynamically during the query processing time, and, as a consequence, does not rely on heavy preprocessing or normalization. The method is extended to include the use of heuristics for generating a subset of answers. 
42|13||Performance comparison of CORBA and RMI|Distributed object architectures and Java are important for building modern, scalable, web-enabled applications. This paper is focused on qualitative and quantitative comparison of two distributed object models for use with Java: CORBA and RMI. We compare both models in terms of features, ease of development and performance. We present performance results based on real world scenarios that include single client and multi-client configurations, different data types and data sizes. We evaluate multithreading strategies and analyse code in order to identify the most time-consuming methods. We compare the results and give hints and conclusions. We have found that because of its complexity CORBA is slightly slower than RMI in simple scenarios. On the other hand, CORBA handles multiple simultaneous clients and larger data amounts better and suffers from far lower performance degradation under heavy client load. The article presents a solid basis for making a decision about the underlying distributed object model. 
42|13||Analyzing dependence locality for efficient construction of program dependence graph|A program dependence graph (PDG) is used for computing program slices. To construct a PDG, we need dependency analysis of statements, which is very expensive in terms of memory space and computation time. In order to increase the efficiency of this analysis, we propose an algorithm for constructing a PDG in which several statements share their dependency information. In order not to degrade the precision of the resulting slice, we investigate the dependency locality of statements to be shared. The effectiveness of this approach has been investigated. Our experimental results show that the space utility decreased by 6.72–49.79% and the analysis time decreased by 4.14–33.21%. Also, a criterion on the locality of statement merging has been studied. 
42|13||A survey of lead-time challenges in the development and evolution of distributed real-time systems|This paper presents a survey that identifies lead-time consumption in the development and evolution of distributed real-time systems DRTSs. Data has been collected through questionnaires, focused interviews and non-directive interviews with senior designers. Quantitative data has been analyzed using the Analytic Hierarchical Process (AHP). A trend in the 11 organizations is that there is a statistically significant shift of the main lead-time burden from programming to integration and testing, when distributing systems. From this, it is concluded that processes, tools and technologies that either reduce the need for or the time for testing have an impact on the development and evolution of lead-time of DRTSs. 
42|14|http://www.sciencedirect.com/science/journal/09505849/42/14|Software project control and metrics|
42|14||From process improvement to people improvement: enabling learning in software development|The importance of people factors for the success of software development is commonly accepted, because the success of a software project is above all determined by having the right people on the right place at the right time. As software development is a knowledge intensive industry; the ‘quality’ of developers is primarily determined by their knowledge and skills. This paper presents a conceptual model of nine ‘learning enablers’ to facilitate learning in software projects. These enablers help identifying whether individual and/or organisational learning is facilitated. The main question addressed in this paper is: ‘Which factors enable learning in software projects and to what extent?’ 
42|14||Design of a product-focused customer-oriented process|In an increasingly dynamic world where both needs and technologies are changing rapidly, especially in applications such as e-commerce systems, there is a requirement for whole-life and measurable customer-oriented processes (CoPs) for systems and software development. In the pursuit of delivering ‘fit for purpose’ systems, the design of a CoP requires a holistic approach to understanding, engineering, managing and evolving the system and software needs and requirements. This paper summarises the CoP concepts and requirements, describes some evaluation results and proposes the way ahead. 
42|14||Subjective evaluation as a tool for learning from software project success|This paper presents a method for using subjective factors to evaluate project success. The method is based on collection of subjective measures with respect to project characteristics and project success indicators. The paper introduces a new classification scheme for assessing software projects. Further, it is illustrated how the method may be used to predict software success using subjective measures of project characteristics. The classification scheme is illustrated in two case studies. The results are positive and encouraging for future development of the approach. 
42|14||Integration of functional, cognitive and quality requirements. A railways case study|The paper shows a SHEL oriented requirements engineering approach, which has been applied in a case study dealing with the definition of the requirements for a new railways traffic control system. The SHEL model provides an integrated view by considering any productive process or activity performed by a combination of Hardware, Software and Liveware resources within a specific Environment. A set of SHEL oriented requirements describes the different views in a complex system. The requirements are grouped into three main classes, namely, functional, cognitive and quality requirements. The paper points out the issue of integrating different types of requirements. 
42|14||Using simulation to analyse the impact of software requirement volatility on project performance|During the last decade, software process simulation has been used to address a variety of management issues and questions. These include: understanding; training and learning; planning; control and operational management; strategic management; process improvement and technology adoption.This paper presents a simulation model that was developed to demonstrate the impact of unstable software requirements on project performance, and to analyse how much money should be invested in stabilising software requirements in order to achieve optimal cost effectiveness. The paper reports on all steps of model building, describes the structure of the final simulation model, and presents the most interesting simulation results of an industrial application. 
42|14||A comparative study of two software development cost modeling techniques using multi-organizational and company-specific data|This research examined the use of the International Software Benchmarking Standards Group (ISBSG) repository for estimating effort for software projects in an organization not involved in ISBSG. The study investigates two questions: (1) What are the differences in accuracy between ordinary least-squares (OLS) regression and Analogy-based estimation? (2) Is there a difference in accuracy between estimates derived from the multi-company ISBSG data and estimates derived from company-specific data? Regarding the first question, we found that OLS regression performed as well as Analogy-based estimation when using company-specific data for model building. Using multi-company data the OLS regression model provided significantly more accurate results than Analogy-based predictions. Addressing the second question, we found in general that models based on the company-specific data resulted in significantly more accurate estimates. 
42|2|http://www.sciencedirect.com/science/journal/09505849/42/2|Special issue on constructing software engineering tools|
42|2||Issues in software engineering tool construction|A brief introduction to software engineering tools is presented, and issues involved in the construction of these tools are discussed. Some of the current issues concerning tool developers are highlighted, which include: metaCASE technology, cognitive support, evaluation and validation of tools and data interchange. Some recent developments in tool construction techniques are examined, and opportunities for further research and development in tool building are identified. 
42|2||Cognitive support, UML adherence, and XMI interchange in Argo/UML|Software design is a cognitively challenging task. Most software design tools provide support for editing, viewing, storing, and transforming designs, but lack support for the essential and difficult cognitive tasks facing designers. These cognitive tasks include decision-making, decision ordering, and task-specific design understanding.This paper describes Argo/UML, an object-oriented design tool using the unified modeling language (UML) design notation. Argo/UML supports several identified cognitive needs of software designers. This support is provided in the form of design tool features. We describe each feature in the context of Argo/UML and provide enough detail to enable other tool builders to provide similar support in their own tools. We also discuss our implementation of the UML and XMI standards, and our development approach. 
42|2||Connecting architecture reconstruction frameworks|A number of standalone tools are designed to help developers understand software systems. These tools operate at different levels of abstraction, from low level source code to software architectures. Although recent proposals have suggested how code-level frameworks can share information, little attention has been given to the problem of connecting software architecture level frameworks. In this paper, we describe the TA Exchange Format (TAXForm) exchange format for frameworks at the software architecture level. By defining mappings between TAXForm and formats that are used within existing frameworks, we show how TAXForm can be used as a “binding glue” to achieve interoperability between these frameworks without having to modify their internal structure. 
42|2||Constructing component-based software engineering environments: issues and experiences|Developing software engineering tools is a difficult task, and the environments in which these tools are deployed continually evolve as software developers’ processes, tools and tool sets evolve. To more effectively develop such evolvable environments, we have been using component-based approaches to build and integrate a range of software development tools, including CASE and workflow tools, file servers and versioning systems, and a variety of reusable software agents. We describe the rationale for a component-based approach to developing such tools, the architecture and support tools we have used some resultant tools and tool facilities we have developed, and summarise the possible future research directions in this area. 
42|2||MetaMOOSEâan object-oriented framework for the construction of CASE tools|This paper describes certain problems which can occur when attempting to build complex CASEtools with facilities not envisaged by the Metatool builders. A solution, based upon an object oriented approach combined with an interpreted OO language has been used to build the MetaMOOSE MetaCASE tool. MetaMOOSE uses an object model to describe the entities and behaviour of the SE development process. Use of the Itcl language gives platform independence and speeds the tool development cycle. A persistent object database ensures integration of the resulting CASE tools. In addition, the successful use of MetaMOOSE to construct a full lifecycle CASE toolset (MOOSE) and its subsequent use in real world engineering projects is described. 
42|2||Generating tools from graph-based specifications|This paper describes an approach for generating graphical, structure-oriented software engineering tools from graph-based specifications. The approach is based on the formal meta modeling of visual languages using graph rewriting systems. Besides the syntactical and semantical rules of the language, these meta models include knowledge from the application domains. This enables the resulting tools to provide the user with high level operations for editing, analysis and execution of models. Tools are constructed by generating source code from the meta model of the visual language, which is written in the very high level programming language PROGRES. The source code is integrated into a framework which is responsible for the invocation of commands and the visualization of graphs. As a case study, a visual language for modeling development processes together with its formal meta model is introduced. The paper shows how a process management tool based on this meta model is generated and reports on our experiences with this approach. 
42|2||A multi-layer multi-view architecture for software engineering environments|This paper presents our experience with constructing a multi-view environment for software process modeling. The environment (Spearmint) is designed to support the capture, analysis and maintenance of large, complex software process models. The environment uses multiple views to handle the inherent complexity of real software processes and to model the fact that different people within organizations have different, sometimes conflicting, views of the same process. Spearmint also supports multiple display representations for process information and addresses requirements for good maintainability, extensibility and performance. Our experience has been that a layered architecture that makes a clear separation of concerns in the application is invaluable for implementing such a multi-view tool. In this paper, we describe some of the experiences we have had with designing and implementing such an architecture. 
42|3|http://www.sciencedirect.com/science/journal/09505849/42/3|The formal specification of ORN semantics|Object Relationship Notation (ORN) is a declarative scheme that permits a variety of common types of relationships to be conveniently defined to a Database Management System (DBMS), thereby allowing the DBMS to automatically enforce their semantics. Though first proposed for object DBMSs, ORN is applicable to any data model that represents binary entity-relationships or to any DBMS that implements them. In this paper, we first describe ORN semantics informally as has been done in previous papers. We then provide a formal specification of these semantics using the Z-notation. Specifying ORN semantics via formal methods gives ORN a solid mathematical foundation. The semantics are defined in the context of an abstract database of sets and relations in a recursive manner that is precise, unambiguous, and noncircular. 
42|3||Designing a distributed database on a local area network: a methodology and decision support system|Local area networks (LANs) are important for an enterprise to hold a competitive edge. Many companies have therefore converted terminal-based computing systems to LAN-based distributed data processing systems. This paper proposes a design methodology for distributed databases connected by a LAN. Two primary objectives of the methodology are: (i) to allocate data files and workload among heterogeneous servers; and (ii) to determine the number of servers to satisfy the response time required for processing each transaction. The file and workload allocation decision is formulated as a nonlinear zero–one integer programming problem. This problem is proven to be NP-complete. A heuristic is developed to solve this problem effectively. A decision support system is implemented and an example is solved to illustrate the practical usefulness of the system. 
42|3||An effective recovery under fuzzy checkpointing in main memory databases|In main memory databases, fuzzy checkpointing gives less transaction overhead due to its asynchronous backup feature. However, till now, fuzzy checkpointing has considered only physical logging schemes. The size of the physical log records is very large, and hence it incurs space and recovery processing overhead. In this paper, we propose a recovery method based on a hybrid logging scheme, which permits logical logging under fuzzy checkpointing. The proposed method significantly reduces the size of log data, and hence offers faster recovery processing than other conventional fuzzy checkpointing with physical logging. 
42|3||Translating update operations from relational to object-oriented databases|In migrating a legacy relational database system to the object-oriented (OO) platform, when database migration completes, application modules are to be migrated, where embedded relational database operations are mapped into their OO correspondents. In this paper we study mapping relational update operations to their OO equivalents, which include UPDATE1, INSERT and DELETE operations. Relational update operation translation from relational to OO faces the touchy problem of transformation from a value-based relationship model to a reference-based model and maintaining the relational integrity constraints. Moreover, with a relational database where inheritance is expressed as attribute value subset relationship, changing of some attribute values may lead to the change of the position of an object in the class inheritance hierarchy, which we call object migration. Considering all these aspects, algorithms are given mapping relational UPDATE, INSERT and DELETE operations to their OO correspondents. Our work emphasize in examining the differences in the representation of the source schema's semantics resulting from the translation process, as well as differences in the inherent semantics of the two models. 
42|3||A business object-oriented environment for CCISs interoperability|A Command and Control Information System (CCIS) is a software application that provides information to be used by military users. The use of a single CCIS is rather easy. However, the operation becomes much complex when several CCISs, generally distributed across networks and heterogeneous at different levels, for instance vocabulary, are required to answer users' requests. As a solution, we intend to set up Business Objects (BOs) that support the interoperability of these CCISs. The development and management of these BOs are based on a number of concepts related to the following technologies: distributed-object computing, code mobility and workflow modeling. 
42|4|http://www.sciencedirect.com/science/journal/09505849/42/4|Path-based protocol verification approach|Protocol verification is one of the most challenging tasks in the design of protocols. Among the various proposed approaches, the one based on reachability analysis (or known as state enumeration approach) is of the most simple, automatic and effective. However, the state explosion problem is a principle obstacle toward successful and complete verifications of complex protocols. To overcome this problem, we proposed a new approach, the “path-based approach.” The idea is to divide a protocol into a collection of individual execution record, denoted as concurrent paths, a partial order representation recording the execution paths of individual entities. Then, the verification of the protocol is, thus, replaced by that of individual concurrent paths. Since concurrent paths can be automatically generated through Cartesian product of the execution paths of all modules, and verified independently, the memory requirement is limited to the complexity of individual concurrent path rather than the whole protocol. Thus, the state explosion problem is alleviated from such “divide and conquer” approach. Furthermore, we propose an algorithm, making the trade-off on memory requirement to generate the concurrent paths more efficiently; and utilize the technique of symmetric verification, parallel computing to improve the efficiency of verification. Eventually, our experience of verifying real protocols shows that our approach uses much less memory and time than reachability analysis. 
42|4||Symbolic path-based protocol verification|The principal problem in protocol verification is state explosion problem. In our work (W.C. Liu, C.G. Chung, Path-based Protocol Verification Approach, Technical Report, Department of Computer Science and Information Engineering, National Chiao-Tung University, Hsin-Chu, Taiwan, ROC, 1998), we have proposed a “divide and conquer” approach to alleviate this problem, the path-based approach. This approach separates the protocol into a set of concurrent paths, each of which can be generated and verified independently of the others. However, reachability analysis is used to identify the concurrent paths from the Cartesian product of unit paths, and it is time-consuming. Therefore, in this paper, we propose a simple and efficient checking algorithm to identify the concurrent paths from the Cartesian product, using only Boolean and simple arithmetic operations. 
42|4||Implementation and modeling of two-phase locking concurrency controlâa performance study|Two-phase locking (2PL) is the concurrency control mechanism that is used in most commercial database systems. In 2PL, for a transaction to access a data item, it has to hold the appropriate lock (read or write) on the data item by issuing a lock request. While the way transactions set their lock requests and the way the requests are granted would certainly affect a system's performance, such aspects have not received much attention in the literature. In this paper, a general transaction-processing model is proposed. In this model, a transaction is comprised of a number of stages, and in each stage the transaction can request to lock one or more data items. Methods for granting transaction requests and scheduling policies for granting blocked transactions are also proposed. A comprehensive simulation model is developed from which the performance of 2PL with our proposals is evaluated. Results indicate that performance models in which transactions request locks on an item-by-item basis and use first-come-first-served (FCFS) scheduling in granting blocked transactions underestimate the performance of 2PL. The performance of 2PL can be greatly improved if locks are requested in stages as dictated by the application. A scheduling policy that uses global information and/or schedules blocked transactions dynamically shows a better performance than the default FCFS. 
42|4||MOOV++: modular object-oriented VDM|This paper describes MOOV++, a methodology for formal object-oriented development. MOOV++ takes as its starting point an object-oriented specification, which is subsequently developed by means of an existing and well-established formal method, the Vienna Development Method (VDM). MOOV++ utilises the VDM module notation to represent a class; the paper demonstrates how all the important concepts, which together define Object-Orientation can be captured formally in VDM. It also provides a proof obligation by which the behaviour of an inherited class can be proved to be consistent with its base class. 
42|4||Dynamic partitioning of complex process models|The E3 modeling language offers a set of mechanisms to support the modeling of complex business and engineering processes. The language is based on well-known object-oriented concepts, extended with specific process-related notions. The E3 language is supported by a modeling tool (called E3 p-draw) that offers mechanisms to support the dynamic partitioning of the process model, and enables the creation and the exploitation of multiple model slices. These mechanisms make it possible to master complex and large process models, since the process engineer can focus on specific factors and aspects of the process, ignoring those information that are irrelevant in a specific stage of the modeling activity. The language and the supporting tool have been applied to the modeling of real industrial processes. 
42|5|http://www.sciencedirect.com/science/journal/09505849/42/5|Knowledge based information integration systems|Information integration systems provide facilities that support access to heterogeneous information sources in a way that isolates users from differences in the formats, locations and facilities of those sources. A number of systems have been proposed that exploit knowledge based techniques to assist with information integration, but it is not always obvious how proposals differ from each other in their scope, in the quality of integration afforded, or in the cost of exploitation. This paper presents a framework for the comparison of proposals for information integration systems, and applies the framework to a range of representative proposals. It is shown that proposals differ greatly in all of the criteria stated and that the selection of an approach is thus highly dependent on the requirements of specific applications. 
42|5||A decision-making pattern for guiding the enterprise knowledge development process|During enterprise knowledge development in any organisation, developers and stakeholders are faced with situations that require them to make decisions in order to reach their intentions. To help the decision-making process, guidance is required. Enterprise Knowledge Development (EKD) is a method offering a guided knowledge development process. The guidance provided by the EKD method is based on a decision-making pattern promoting a situation and intention oriented view of enterprise knowledge development processes. The pattern is iteratively repeated through the EKD process using different types of guiding knowledge. Consequently, the EKD process is systematically guided. The presentation of the decision-making pattern is the purpose of this paper. 
42|5||Knowledge based evaluation of software systems: a case study|Solving software evaluation problems is a particularly difficult software engineering process and many contradictory criteria must be considered to reach a decision. Nowadays, the way that decision support techniques are applied suffers from a number of severe problems, such as naive interpretation of sophisticated methods and generation of counter-intuitive, and therefore most probably erroneous, results. In this paper we identify some common flaws in decision support for software evaluations. Subsequently, we discuss an integrated solution through which significant improvement may be achieved, based on the Multiple Criteria Decision Aid methodology and the exploitation of packaged software evaluation expertise in the form of an intelligent system. Both common mistakes and the way they are overcome are explained through a real world example. 
42|5||LODâ: A C++ extension for OODBMSs with orthogonal persistence to class hierarchies|There exist some preprocessing based language extensions for database management where persistence is orthogonal to the class hierarchy. They allow a class hierarchy to be built from both database classes and non-database classes together. Such a property is important in that classes can be reused in implementing database classes, and vice versa. In this paper, we elaborate on the orthogonality of persistence to class-hierarchies, and find that the existing method to achieve this is not satisfactory because of the side-effects of the heterogeneosity of the links in a class hierarchy; some links represent subset(IsA) relationships between database classes, while the others denote inheritance for code-reuse. Finally, we propose ∗, a C++ extension to database access, which separates the different categories of links into independent hierarchies, and supports orthogonal persistence to the class hierarchy, overcoming the limitations in the previous methods. 
42|5||Towards modeling the query processing relevant shape complexity of 2D polygonal spatial objects|The shape complexity of two-dimensional (2D) polygonal spatial objects has implications for how the object can be best represented in a spatial database, and for the query-processing performance of that object. Nevertheless few useful definitions of query-processing relevant spatial complexity are available. A query-processing oriented shape complexity measure is likely to be different from a fractal measure of shape complexity that focused on compression/decompression or a shape complexity measure that would be used for pattern recognition, and should give better performance for the analysis of query processing. It could be used to classify spatial objects, cluster spatial objects in multiprocessor database systems. In a recent paper Brinkhoff et al. (T. Brinkhoff, H-P. Kriegel, R. Shneider, A. Braun, Measuring the complexity of spatial objects, Proceedings of the 3rd ACM International Workshop on Advances in Geographic Information Systems, Baltimore, MD, 1995, pp. 109–117) demonstrated the usefulness of a spatial complexity measure. They did not, however, offer much theoretical justification for their choice of parameters nor for the functional form that they used. In this paper we present a conceptual framework for discussing the query processing oriented shape complexity measures for spatial objects. It is hoped that this will lead to the development of improved measures of spatial complexity. 
42|6|http://www.sciencedirect.com/science/journal/09505849/42/6|An empirical study of object-oriented system evolution|Software metrics have been used to measure software artifacts statically—measurements are taken after the artifacts are created. In this study, three metrics—System Design Instability (SDI), Class Implementation Instability (CII), and System Implementation Instability (SII)—are used for the purpose of measuring object-oriented (OO) software evolution. The metrics are used to track the evolution of an OO system in an empirical study. We found that once an OO project starts, the metrics can give good indications of project progress, e.g. how mature the design and implementation is. This information can be used to adjust the project plan in real time.We also performed a study of design instability that examines how the implementation of a class can affect its design. This study determines that some aspects of OO design are independent of implementation, while other aspects are dependent on implementation. 
42|6||OO-CASE tools: an evaluation of Rose|Object-oriented software development utilizes new design methodologies. These methodologies can be supported by computer-aided software engineering tools, such as Rational Rose. A survey of software developers identifies the demand for various features, and reveals strengths and need for improvements in Rational Rose. Overall, respondents indicated that Rational Rose provides strong support for OO design, but could additional support for teamwork, prototyping, and improvements in ease of use. 
42|6||An extensible platform for the development of synchronous groupware|The development of groupware is a complex endeavor due to several inherent features not present in single-user applications. To address this complexity many authors have presented useful platforms that permit the reutilization of code to facilitate the implementation of groupware applications. However, design reusability could be of greater value than code reusability and facilitate the use and extension of groupware features. This paper describes COCHI (Collaborative Objects for Communication and Human Interaction), an extensible pattern system for groupware applications that aims to provide reusability and extensibility of design patterns represented as COCHI subsystems and implemented as a class framework.This pattern system has proved to be useful for the rapid development of groupware applications, while being flexible enough to incorporate important extensions to the original framework. Overall, it provides a good balance between ease of use and flexibility. 
42|6||A data abstraction approach for query relaxation|Since a query language is used as a handy tool to obtain information from a database, users want more user-friendly and fault-tolerant query interfaces. When a query search condition does not match with the underlying database, users would rather receive approximate answers than null information by relaxing the condition. They also prefer a less rigid querying structure, one which allows for vagueness in composing queries, and want the system to understand the intent behind a query. This paper presents a data abstraction approach to facilitate the development of such a fault-tolerant and intelligent query processing system. It specifically proposes a knowledge abstraction database that adopts a multilevel knowledge representation scheme called the knowledge abstraction hierarchy. Furthermore, the knowledge abstraction database extracts semantic data relationships from the underlying database and supports query relaxation using query generalization and specialization steps. Thus, it can broaden the search scope of original queries to retrieve neighborhood information and help users to pose conceptually abstract queries. Specifically, four types of vague queries are discussed, including approximate selection, approximate join, conceptual selection and conceptual join. 
42|6||Requirements engineering: making the connection between the software developer and customer|Requirements engineering are one of the most crucial steps in software development process. Without a well-written requirements specification, developer's do not know what to build, user's do not know what to expect, and there is no way to validate that the created system actually meets the original needs of the user. Much of the emphasis in the recent attention for a software engineering discipline has centered on the formalization of software specifications and their flowdown to system design and verification. Undoubtedly, the incorporation of such sound, complete, and unambiguous traceability is vital to the success of any project. However, it has been our experience through years of work (on both sides) within the government and private sector military industrial establishment that many projects fail even before they reach the formal specification stage. That is because too often the developer does not truly understand or address the real requirements of the user and his environment.The purpose of this research and report is to investigate the key players and their roles along with the existing methods and obstacles in Requirements Elicitation. The article will concentrate on emphasizing key activities and methods for gathering this information, as well as offering new approaches and ideas for improving the transfer and record of this information. Our hope is that this article will become an informal policy reminder/guideline for engineers and project managers alike. The success of our products and systems are largely determined by our attention to the human dimensions of the requirements process. We hope this article will bring attention to this oft-neglected element in software development and encourage discussion about how to effectively address the issue. 
42|6||Implementation of an efficient requirements-analysis supporting system using similarity measure techniques|As software becomes more complicated and larger, the software engineer's requirements-analysis becomes an important and uneasy activity. This paper proposes a requirements-analysis supporting system that supports informal requirements-analysis. The proposed system measures the similarity between requirement sentences to identify possible redundancies and inconsistencies, and extracts the possible ambiguous requirements. The similarity measurement method combines a sliding window model and a parser model. Using these methods, the proposed system supports to trace dependency between documents and improve quality of requirement sentences. Efficiency of the proposed system and a process for requirement specification analysis using the system are presented. 
42|7|http://www.sciencedirect.com/science/journal/09505849/42/7|Multi-level transaction model for semantic concurrency control in linear hash structures|In this paper, we present a version of the linear hash structure algorithm to increase concurrency using multi-level transaction model. We exploit the semantics of the linear hash operations at each level of transaction nesting to allow more concurrency. We implement each linear hash operation by a sequence of operations at lower level of abstraction. Each linear hash operation at leaf-level is a combination of search and read/write operations. We consider locks at both vertex (page) and key level (tuple) to further increase concurrency. As undo-based recovery is not possible with multi-level transactions, we use compensation-based undo to achieve atomicity. We have implemented our model using object-oriented technology and multithreading paradigm. In our implementation, linear hash operations such as find, insert, delete, split, and merge are implemented as methods and correspond to multi-level transactions. 
42|7||Statistical analysis of deviation of actual cost from estimated cost using actual project data|This paper analyzes an association of a deviation of the actual cost (measured by person-month) from the estimated cost with the quality and the productivity of software development projects. Although the obtained results themselves may not be new from the academic point of view, they could provide motivation for developers to join process improvement activities in a software company and thus become a driving force for promoting the process improvement.We show that if a project is performed faithfully under a well-organized project plan (i.e. the plan is first constructed according to the standards of good writing, and then a project is managed and controlled to meet the plan), the deviation of the actual cost from the estimated one becomes small. Next we show statistically that projects with small deviation of the cost estimate tend to achieve high quality of final products and high productivity of development teams. In this analysis, the actual project data on 37 projects at a certain company are extensively applied. 
42|7||Translating hierarchical predicate transition nets to CC++ programs|This paper presents an approach to translate hierarchical predicate transition nets into CC++ program skeletons. The approach consists of an overall translation architecture and a set of translation rules based on the syntax and semantics of hierarchical predicate transition nets. Our results have established a link between hierarchical predicate transition nets and concurrent object-oriented programming, and provided some building blocks for a hierarchical predicate transition net based on transformational software development methodology. 
42|7||Evaluation of Workflow-type software products: a case study|The main objective of this paper is to propose a set of indicators for the evaluation of Workflow software-type products within the context of Information Systems. This paper is mainly based on a comprehensive bibliographical review of all topics referring to the Workflow Technology and Information Systems. Next, sets of indicators are presented for the selection of a Workflow software based on the realities of the business world, including a method of examination so as to obtain an integral evaluation on the Workflow software. Finally, the evaluation method for two types of Workflow software is applied: Lotus Domino/Notes® and Microsoft Exchange®, for the billing subsystems of a company called MANAPRO Consultants, Inc.®. 
42|7||A framework and test-suite for assessing approaches to resolving heterogeneity in distributed databases|The problem of connecting together a number of different databases to produce an integrated information system has attracted a considerable amount of attention over the years and various approaches have been developed to handle this. However, the general problem of gathering related information from a number of existing heterogeneous databases is complex because of the differences in representation and meaning of data in different data sets. Many different approaches have been described to resolve this problem, and some prototype systems built. However, it is difficult to compare the effectiveness of different approaches and prototypes. This paper is aimed at addressing the specific issue of assessing the generality of different approaches. To this end it presents a framework for classifying the differences between data in different databases and a test-suite which can be used to evaluate and compare the extent to which different approaches handle different aspects of this heterogeneity. 
42|8|http://www.sciencedirect.com/science/journal/09505849/42/8|Semantic-based locking technique in object-oriented databases|This paper presents a locking-based concurrency control dealing with three important issues in object-oriented databases: semantics of methods, nested method invocation and referentially shared object. In the proposed scheme, locks are required for the execution of methods instead of atomic operations. This can reduce the locking overhead and deadlocks due to lock escalation. Also, a way of automating commutativity of methods is provided. In addition, concurrency is increased further by use of run-time information. A performance study is conducted by means of simulation using 007 benchmark. Through the simulation, the proposed technique is then compared with the two existing techniques. The performance studies show that the proposed scheme is superior to existing works. 
42|8||Bayesian probability distributions for assessing measurement of subjective software attributes|In order to validate objective indirect measures of software attributes, it is vital to demonstrate sufficiency of the representation condition. However, when a direct measurement on an ordinal scale is undertaken by human-raters, a significant degree of subjectivity may exist. Consequently, demonstrating that an objective indirect measure represents the attribute is difficult. It is difficult because subjectivity during direct measurement will lead to miss-classification. Further, during a demonstration the objective indirect measure itself cannot be assumed to give the ‘true’ measurement values. Modular cohesion is an attribute measured on an ordinal scale that exhibits subjectivity during direct measurement. By reference to cohesion classification data Bayesian inference probability distributions are constructed that represent error due to subjectivity during direct measurement. Using these probability distributions, an approach to demonstrate sufficiency of the representation condition for an objective indirect measure is proffered. In addition, Bayesian probability distributions can be used to provide informative estimates of the predictive capability of prediction systems for subjective attributes. 
42|8||An operational approach to the design of workflow systems|We construct models as an aid to our thought process. A particular class of models, operational models, can be used for simulation and prototyping. The OPJ modeling language is suitable for building operational models of complex software systems.The notion of operational parameterized building block is the key point of the approach, which focuses on two major phases: domain modeling and system modeling. Domain modeling consists in providing the classes of the building blocks grouped into different schemata. System modeling consists in building an actual model using the building blocks taken from the above-mentioned schemata; such building blocks are connected to each other according to the rules expressed in the schemata and are given actual parameters.As an example, a workflow management system supporting business process managing travel authorizations is presented. The workflow system is modeled and then used to synthesize a distributed prototype. 
42|8||The evaluation of access costs in object databases|Unfortunately, there is at present nothing to assist the system architect at design-time to determine whether a proposed architecture based on an object-oriented database system will perform as required. The problem is complex, the choice of suitable modelling approach difficult, and a construction of a model is often abstruse. In this paper we concentrate on a major model component: that describing the access of objects in a centralised database. We present the background for the research (modern corporate IS architectures), the choices we have made, the prototype design, and the mathematical model of the cost of object database access. We conclude the paper by describing a validation of the model and how it can be generalised. The paper has a number of objectives: first, to dispel the myth that performance modelling of object-oriented systems is an immensely difficult task; second, to show that techniques which have been in existence for some time for modelling are applicable, with some modification, to aspects of object-oriented database performance prediction; and, third, to detail a specific case study of access cost modelling which provides enough information to be replicated by other workers across a number of object-oriented database products. 
42|8||A methodology for transforming inheritance relationships in an object-oriented conceptual model to relational tables|With the increasing popularity of Object-Relational technology, it becomes necessary to have a methodology that allows database designers to exploit the great modeling power of object-oriented conceptual model, and still facilitate implementation on relational database systems. This paper presents a transformation methodology from inheritance relationships to relational tables. This includes transformation of different types of inheritance, such as union inheritance, mutual exclusion inheritance, partition inheritance and multiple inheritance. Performance comparison between the proposed transformation methodology and existing methods is also carried out. From the evaluation, we conclude that the proposed transformation methodology is more efficient than the others. 
42|9|http://www.sciencedirect.com/science/journal/09505849/42/9|An adaptable constrained locking protocol for high data contention environments: correctness and performance|Multiversions of data are used in database systems to increase concurrency and to provide efficient recovery. Data versions improve the concurrency by allowing the concurrent execution of “non-conflicting” read-write lock requests on different versions of data in an arbitrary fashion. A transaction that accesses a data item version, which later diagnosed to lead to an incorrect execution, is aborted. This act is reminiscent of the validation phase in the optimistic concurrency control schemes. Various performance studies suggest that these schemes perform poorly in high data contention environments where the excessive transaction aborts result due to the failed validation. We propose an adaptable constrained two-version two-phase locking (C2V2PL) scheme in which these “non-conflicting” requests are allowed only in a constrained manner. C2V2PL scheme assumes that a lock request failing to satisfy the specific constraints will lead to an incorrect execution and hence, must be either rejected or blocked. This eliminates the need for a separate validation phase. When the contention for data among the concurrent transactions is high, the C2V2PL scheduler in aggressive state rejects such lock requests. The deadlock free nature of C2V2PL scheduler adapts to the low data contention environments by accepting the lock request that have failed the specific constraints but contrary to the assumption, will not lead to an incorrect execution. Thus, C2V2PL scheme improves the potential concurrency due to reduced transaction aborts in this conservative state. We have compared performance of our scheme with other lock-based concurrency control schemes such as two phase locking, Wait-depth locking and Optimistic locking schemes. Our results show increase in throughput and reduced transaction-abort-ratio in case of C2V2PL scheme. 
42|9||Component mining: a process and its pattern language|An important issue in a component-based software development process is the supply source of mature, reliable, adaptable and maintainable components. We define as component mining the deliberate, organised and automated process of extracting reusable components from an existing rich software base and present a pattern language used for mining components from programs that are typically executed as non-interactive autonomous processes. We describe the patterns in terms of intent, motivation, applicability, structure, participants, consequences and implementation. Based on the pattern language, we describe the implementation of a set of COM components that encapsulate the Unix filters and an exemplar application that uses them. 
42|9||Deriving protocols for services supporting mobile users|A Prolog tool for automated derivation of protocol specifications from service specifications is described. The server for which the protocol is derived may consist of any finite number of protocol entities co-operating over reliable unbounded first-in-first-out channels. Its service is expected to consist of service primitives that read or write unstructured global virtual variables, implicitly receive or compute their current values or delete their local copies. In addition, service primitives may access distributed virtual queues, to which they append elements with desired priority or consume their head elements. Service users are allowed to dynamically select the service-access point through which they interact with the distributed server. The adopted specification language has been inspired by LOTOS. 
42|9||Natural language and message sequence chart representation of use cases|We study the relationship between natural language use cases and message sequence charts. The use cases are studied in the context of object-oriented methods. Natural language sentences and Message Sequence Charts (MSCs) are used for the representation of use cases. In this paper we establish a correspondence between three restricted classes of natural language sentences and MSC fragments. 
42|9||An empirical analysis of function point adjustment factors|In function point analysis, fourteen “general systems characteristics” (GSCs) are used to construct a “value adjustment factor” (VAF), with which a basic function point count is adjusted. Although the GSCs and VAF have been criticized on both theoretical and practical grounds, they are used by many practitioners. This paper reports on an empirical investigation into their use and practical value. We conclude that recording the GSCs may be useful for understanding project cost drivers and for comparing similar projects, but the VAF should not be used: doubts about its construction are not balanced by any practical benefit. A new formulation is needed for using the GSCs to explain effort; factors identified here could guide further research. 
43|1|http://www.sciencedirect.com/science/journal/09505849/43/1|PZ nets â a formal method integrating Petri nets with Z|In this paper, a formal method (called PZ nets) for specifying concurrent and distributed systems is presented. PZ nets integrate two well-known existing formal methods Petri nets and Z such that Petri nets are used to specify the overall structure, control flows, causal relation, and dynamic behavior of a system; and Z is used to define tokens, labels and constrains of the system. The essence, benefits, and problems of the integration are discussed. A set of heuristics and transformations to develop PZ nets and a technique to analyze PZ nets are proposed and demonstrated through a well-known example. 
43|1||The whole-part relationship in object modelling: a definition in cOlOr|In object models built according to popular object-oriented formalisms, the commonest relationship types (excluding inheritance) are the structural relationships of association and of whole-part (often called aggregation). This last type is well known to have no accurately prescribed semantics. Here, some of the aggregation semantics frequently presented in the literature and sometimes supported in current object-oriented modelling languages, especially UML, are analysed and criticised. Because of defects, the use of a modelling notation based on these aggregation semantics is dubious and limited. Moreover, many properties are candidates for characterising the whole-part relationship provided that no redundancy and no inconsistency exist between them. A framework known as cOlOr is then offered by means of a metamodel in which the Whole-Part metatype inherits from the Structural-Relationship metatype. Defining a specific aggregation semantics leads then, first, within cOlOr, to the creation of a subtype of the Whole-Part metatype. Next, the behaviour of this last type is extended and/or restricted in using a constraint-based approach. Such a process is developed particularly for considering Composition in UML and Aggregation in OML more formally, as well as for dealing with domain-dependent aggregation semantics. Since a non-negligible feature of cOlOr is the availability of a C++ library that implements the proposed metamodel, some implementation concerns are also briefly discussed. 
43|1||Data placement in a parallel DBMS with multiple disks|Various strategies have been developed to assist in determining an effective data placement for a parallel database system. However, little work has been done on assessing the relative performance obtained from different strategies. This paper studies the effects of different data placement strategies on a shared-nothing parallel relational database system when the number of disks attached to each node is varied. Three representative strategies have been used for the study and the performance of the resulting configuration has been assessed in the context of the TPC–C benchmark running on a specific parallel system. Results show an increase in sensitivity to data placement strategy with increasing number of disks per node. 
43|1||Combining boolean logic and linguistic structure|Descriptor languages with Boolean operators have often been applied for Information Retrieval (IR) and Filtering (IF). In addition, linguistically motivated descriptors, such as noun phrases and index expressions, have also proven of value for IR. However, the synthesis of logic and linguistics in one descriptor language is an open issue still. In this paper, Boolean index expressions (BIEs), combining Boolean logic and linguistic structure, are proposed as a good balance between expressiveness and practical issues. BIEs provide several favourable properties as descriptor language for IR and IF: BIEs are comprehensible, expressive, compact, and tractable. 
43|1||On the problem of the software cost function|The question of finding a function for software cost estimation is a long-standing issue in the software engineering field. The results of other works have shown different patterns for the unknown function, which relates software size to project cost (effort). In this work, the research about this problem has been made by using the technique of Genetic Programming (GP) for exploring the possible cost functions. Both standard regression analysis and GP have been applied and compared on several data sets. However, regardless of the method, the basic size–effort relationship does not show satisfactory results, from the predictive point of view, across all data sets. One of the results of this work is that we have not found significant deviations from the linear model in the software cost functions. This result comes from the marginal cost analysis of the equations with best predictive values. 
43|10|http://www.sciencedirect.com/science/journal/09505849/43/10|Effects of shaping characteristics on the performance of nested transactions|The nested transaction model was introduced to satisfy the requirements of advanced database applications. Moreover, it is currently the basic transaction model for new database applications like workflow systems and new database systems like mobile databases and object-relational databases. Though there are several performance evaluation studies of different concurrency control mechanisms in nested transactions, the effects of transaction parameters on the overall system performance have not received any attention. In this paper, we study the effects of transactions characteristics on system performance. We developed a detailed simulation model and conducted several experiments to measure the impact of transactions characteristics on the performance. First, the effect of the number of leaves on the performance of nested transactions is investigated under different shaping parameters. Also, effects of the depth of the transaction tree on the system performance are investigated. 
43|10||An approach to system design based on P/T net simulation|This work presents a technique of early simulation in the design phase of concurrent and distributed systems. A P/T net is used to model the system whose behavior is simulated by the net execution; the truly concurrent semantics of P/T nets establishes a partial order among the system events. The designer can interact with the simulator asking for measures about the system behavior that concern all executions respecting the same partial order. Some measures, such as the degree of parallelism exploited, are not easily obtainable from an interleaving semantics. Moreover, the designer can force the system behavior to reflect resource-constrained environments. 
43|10||Database sampling with functional dependencies|During the development of information systems, there is a need to prototype the database that the applications will use when in operation. A prototype database can be built by sampling data from an existing database. Including relevant semantic information when extracting a sample from a database is considered invaluable to support the development of data-intensive applications. Functional dependencies are an example of semantic information that could be considered when sampling a database. This paper investigates how a database relation can be sampled so that the resulting sample satisfies precisely a given set of functional dependencies (and its logical consequences), i.e. is an Armstrong relation. 
43|10||Measurement program success factors revisited|Success factors for measurement programs as identified in the literature typically focus on the ‘internals’ of the measurement program; incremental implementation, support from management, a well-planned metric framework, and so on. However, for a measurement program to be successful within its larger organizational context, it has to generate value for the organization. This implies that attention should also be given to the proper mapping of some identifiable organizational problem onto the measurement program, and the translation back of measurement results to organizational actions. We have extended the well-known ‘internal’ success factors for software measurement programs with four ‘external’ success factors. These success factors are aimed at the link between the measurement program and the usage of the measurement results. In this paper, we show how this combined set of internal and external success factors explains the success or failure of five industrial measurement programs. 
43|11|http://www.sciencedirect.com/science/journal/09505849/43/11|Non-functional requirements analysis: deficiencies in structured methods|This paper examines some deficiencies in structured systems development methods that have the effect of overlooking non-functional requirements. Evidence from four case studies shows that non-functional requirements are often overlooked, questioning users is insufficient, methodologies do not help in the elicitation of non-functional requirements and there is a lack of consensus about the meaning and utility of non-functional requirements. Some lessons are drawn from the domain of real time systems development. Finally a framework is advanced for taking a stakeholder approach to the conflict and organisational change issues associated with the elicitation stage of requirements analysis. 
43|11||Utilizing knowledge links in the implementation of system development methodologies|Developing technical ‘know-how’ is a slow process that can become a barrier in implementing complex administrative technologies such as a software development methodology. To overcome this barrier, organizations often seek knowledge links that can enhance learning and minimize inevitable problems that are encountered in an implementation process. This paper presents the findings of an empirical study that examines the prescribed versus actual use of external consultants, universities and vendors as knowledge links during the implementation of systems development methodologies (SDM). First, the study assesses the need and value of establishing and utilizing links to external sources of expertise for successful SDM implementation. We then identify and analyze a gap that exists between what the links to external knowledge are perceived to be capable of contributing and what the links to external knowledge are actually contributing during SDM implementation. In conclusion, possible reasons for the gap are discussed. 
43|11||A systematic approach for the design of post-transaction input error handling|Most of the errors that occur in the input of a program cannot be automatically detected through input validation. As a result, most of such errors made in the input of a database transaction are discovered only after the completion of the transaction. Therefore, the provision of error correction transactions for correcting such error is essential in any database application. Current system development methodologies do not provide much detail for this important design task. The task is left to the designer's experience to carry out in an ad hoc way. This paper formalizes the common design knowledge in designing these transactions. It proposes a novel approach for the systematic design of post-transaction input error handling in a database application. 
43|11||Software reuse in-the-small: automating group rewarding|In this paper we present a model-independent, multi-level, closed loop approach to software reuse that makes tuning of reuse systems mostly automatic, and includes a reward system for development of reusable components. The proposed techniques are shown to be suitable for development in-the-small by fixing some locally measured reuse coefficients for small teams. 
43|11||A survey of alternative designs for a search engine storage structure|Three information retrieval storage structures are considered to determine their suitability for a World Wide Web search engine: The Wolverhampton Web Library — The Next Generation. The structures are an inverted file, signature file and Pat tree. A number of implementations are considered for each structure. For the index of an inverted file a sorted array, B-tree, B+-tree, trie and hash table are considered. For the signature file vertical and horizontal partitioning schemes are considered and for the Pat tree a tree and array implementation are considered. A theoretical comparison of the structures is done on seven criteria that include: response time, support for results ranking, search techniques, file maintenance, efficient use of disk space (including the use of compression), scalability and extensibility. The comparison reveals that an inverted file is the most suitable structure, unlike the signature file and Pat tree, which encounter problems with very large corpora. 
43|11||Erratum to âAlternative approaches to effort prediction of ERP projectsâ [Inf. Software Technol. 43 (2001) 413â423]|
43|12|http://www.sciencedirect.com/science/journal/09505849/43/12|Demonstrating the usage of single-case designs in experimental software engineering|Experimental software engineering is the subdiscipline of empirical software engineering which uses experimentation to analyze, improve, and to validate software engineering methods (concepts, techniques, models). The efficacy and efficiency of some methods have already been demonstrated by experiments. To achieve this factorial designs were used so far. Critics of software engineering experiments argue that doing an experiment would be incredibly expensive. Furthermore, for doing software engineering experiments one would need hundreds of subjects. This paper shows that this is not necessarily right. An approach is presented that shows how to conduct software engineering experiments in a cost-effective way. The approach is useful to analyze software engineering problems specific to one subject, to conduct pilot experiments that precede in-depth experiments, and to accompany technology transfer. To demonstrate the usage of a single-subject experiment the domain of reuse was chosen. 
43|12||Specification of integrity-preserving operations in information systems by using a formal UML-based language|Information system specification requires careful consideration of data integrity. We present an approach to the definition of a variant of UML that allows the rigorous specification of operations and transactions that enforce data constraints. The approach to structural language definition, summarised in Appendix A, extends that used for conventional UML, comprising metamodels of the abstract syntax of structural aspects, with denotational semantics. We add operation semantics using a formal notation, B. Issues relating to the tool-assisted specification of integrity-preserving transactions are discussed. 
43|12||A qualitative comparison of two processes for object-oriented software development|Two of the leading object-oriented processes are the public domain Object-oriented Process, Environment and Notation (OPEN) and the proprietary Rational Unified Process (RUP). A qualitative evaluation is performed on RUP's public domain component and on OPEN using a set of stated criteria. In particular, we focus our comparison on aspects of the process architecture and underpinning metamodel, the concepts and terminology utilized and support for project management. 
43|12||Leaving inconsistency using fuzzy logic|Current software development methods do not provide adequate means to model inconsistencies and therefore force software engineers to resolve inconsistencies whenever they are detected. Certain kinds of inconsistencies, however, are desirable and should be maintained as long as possible. For instance, when multiple conflicting solutions exist for the same problem, each solution should be preserved to allow further refinements along the development process. An early resolution of inconsistencies may result in loss of information and excessive restriction of the design space. This paper aims to enhance the current methods by modeling and controlling the desired inconsistencies through the application of fuzzy logic-based techniques. It is shown that the proposed approach increases the adaptability and reusability of design models. 
43|12||Corrigendum to âA tunable class hierarchy index for object-oriented databases using a multidimensional index structureâ [Information and Software Technology 43 (2001) 309â323]|
43|13|http://www.sciencedirect.com/science/journal/09505849/43/13|Supporting tailorable program visualisation through literate programming and fisheye views|This paper describes the ‘Jaba’ program editor and browser that allows users to tailor the level of abstraction at which they visualise, browse, edit and document object-oriented programs. Its design draws on concepts from literate programming, holophrasting displays, fisheye visualisation and hypertext to allow programmers to rapidly move between abstract and detailed views of Java classes. The paper focuses on the motivation for, and user interface issues surrounding, the integration of these facilities in Jaba. Limitations in the current tools and theories for programming support are identified, and modifications are proposed and demonstrated. Examples include overcoming the static post-hoc documentation support provided by Javadoc, and normalising Furnas's ‘degree of interest’ fisheye visualisation formula to avoid excessive suppression of program segments. 
43|13||Managing uncertainty in project portfolio cost estimation|Although typically a software development organisation is involved in more than one project simultaneously, the available tools in the area of software cost estimation deal mostly with single software projects. In order to calculate the possible cost of the entire project portfolio, one must combine the single project estimates taking into account the uncertainty involved. In this paper, statistical simulation techniques are used to calculate confidence intervals for the effort needed for a project portfolio. The overall approach is illustrated through the adaptation of the analogy-based method for software cost estimation to cover multiple projects. 
43|13||Verifying scenarios with time Petri-nets|Recently, a substantial amount of research activities has been focused on a user-oriented perspective to the development of software systems. One of the key elements in this perspective is the notion of scenarios: a description of what people do and experience as they try to make usage of computer systems and applications. A variety of applications of scenarios has been proposed, for example, to elicit user requirements, or to validate requirements specifications. As scenarios are useful for the lifecycle of requirements engineering, it is important to enable verification of these scenarios, especially, to detect any wrong information and missing information that are hidden in scenarios. However, scenarios are usually stated in an informal way, which impedes the easiness for verification. The focus of this paper is on the use of time Petri-nets (TPNs) to serve as the verification mechanism for the acquired scenarios. Use cases are used to elicit the user needs and to derive the scenarios. Each of the use cases is described from a user's perspective and depicts a specific flow of events in the system. After specifying all possible scenarios, each of them can be transformed into its correspondent time Petri-nets model. Through the analysis of these TPNs models, wrong information and missing information in scenarios can be detected. The proposed approach is illustrated by means of a course registration problem domain. 
43|13||Generalized partition testing via Bayes linear methods|This paper explores the use of Bayes linear methods related to partition testing for software. If a partition of the input domain has been defined, the method works without the assumption of homogeneous (revealing) subdomains, and also includes the possibility to learn, from testing inputs in one subdomain, about inputs in other subdomains, through explicit definition of the correlations involved. To enable practical application, an exchangeability structure needs to be defined carefully, for which means the judgements of experts with relation to the software is needed. Next to presenting the basic idea of Bayes linear methods and how it can be used to generalize partition testing, some important aspects related to applications as well as for future research are discussed. 
43|13||A viewpoint on software engineering and information systems: what we can learn from the construction industry?|
43|13||An investigation of coupling, reuse and maintenance in a commercial C++ application|This paper describes an investigation into the use of coupling complexity metrics to obtain early indications of various properties of a system of C++ classes. The properties of interest are: (i) the potential reusability of a class and (ii) the likelihood that a class will be affected by maintenance changes made to the overall system. The study indicates that coupling metrics can provide useful indications of both reusable classes and of classes that may have a significant influence on the effort expended during system maintenance and testing. 
43|14|http://www.sciencedirect.com/science/journal/09505849/43/14|Editorial Note|
43|14||Editorial|
43|14||An overview of evolutionary algorithms: practical issues and common pitfalls|An overview of evolutionary algorithms is presented covering genetic algorithms, evolution strategies, genetic programming and evolutionary programming. The schema theorem is reviewed and critiqued. Gray codes, bit representations and real-valued representations are discussed for parameter optimization problems. Parallel Island models are also reviewed, and the evaluation of evolutionary algorithms is discussed. 
43|14||Search-based software engineering|This paper claims that a new field of software engineering research and practice is emerging: search-based software engineering. The paper argues that software engineering is ideal for the application of metaheuristic search techniques, such as genetic algorithms, simulated annealing and tabu search. Such search-based techniques could provide solutions to the difficult problems of balancing competing (and some times inconsistent) constraints and may suggest ways of finding acceptable solutions in situations where perfect solutions are either theoretically impossible or practically infeasible.In order to develop the field of search-based software engineering, a reformulation of classic software engineering problems as search problems is required. The paper briefly sets out key ingredients for successful reformulation and evaluation criteria for search-based software engineering. 
43|14||Evolutionary test environment for automatic structural testing|Testing is the most significant analytic quality assurance measure for software. Systematic design of test cases is crucial for the test quality. Structure-oriented test methods, which define test cases on the basis of the internal program structures, are widely used. A promising approach for the automation of structural test case design is evolutionary testing. Evolutionary testing searches test data that fulfil a given structural test criteria by means of evolutionary computation. In this work, an evolutionary test environment has been developed that performs fully automatic test data generation for most structural test methods. The introduction of an approximation level for fitness evaluation of generated test data and the definition of an efficient test strategy for processing test goals, increases the performance of evolutionary testing considerably. 
43|14||A prediction system for evolutionary testability applied to dynamic execution time analysis|Evolutionary testing (ET) is a test case generation technique based on the application of an evolutionary algorithm. It can be applied to timing analysis of real-time systems. In this instance, timing analysis is equivalent to testing. The test objective is to uncover temporal errors. This corresponds to the violation of the system's timing specification. Testability is the ability of the test technique to uncover faults. Evolutionary testability is the ability of an evolutionary algorithm to successfully generate test cases with the goal to uncover faults, in this instance violation of the timing specification. This process attempts to find the best- and worst-case execution time of a real-time system.Some attributes of real-time systems were found to greatly inhibit the successful generation of the best- and worst-case execution times through ET. These are small path domains, high data dependence, large input vectors and nesting. This paper defines software metrics, which aim to express the effects of these attributes on ET. ET is applied to generate the best- and worst-case execution paths of test programs. Their extreme timing paths are determined analytically and the average success of ET to cover these paths is assessed. This empirical data is mapped against the software metrics to derive a prediction system for evolutionary testability.The measurement and prediction system developed from the experiments is able to forecast evolutionary testability with almost 90% accuracy. The prediction system will be used to assess whether the application of ET to a real-time system will be sufficient to successful dynamic timing analysis, or whether additional testing strategies are needed. 
43|14||Can genetic programming improve software effort estimation? A comparative evaluation|Accurate software effort estimation is an important part of the software process. Originally, estimation was performed using only human expertise, but more recently, attention has turned to a variety of machine learning (ML) methods. This paper attempts to evaluate critically the potential of genetic programming (GP) in software effort estimation when compared with previously published approaches, in terms of accuracy and ease of use. The comparison is based on the well-known Desharnais data set of 81 software projects derived from a Canadian software house in the late 1980s. The input variables are restricted to those available from the specification stage and significant effort is put into the GP and all of the other solution strategies to offer a realistic and fair comparison. There is evidence that GP can offer significant improvements in accuracy but this depends on the measure and interpretation of accuracy used. GP has the potential to be a valid additional tool for software effort estimation but set up and running effort is high and interpretation difficult, as it is for any complex meta-heuristic technique. 
43|14||An evolutionary approach to estimating software development projects|The use of dynamic models and simulation environments in connection with software projects paved the way for tools that allow us to simulate the behaviour of the projects. The main advantage of a Software Project Simulator (SPS) is the possibility of experimenting with different decisions to be taken at no cost. In this paper, we present a new approach based on the combination of an SPS and Evolutionary Computation. The purpose is to provide accurate decision rules in order to help the project manager to take decisions at any time in the development. The SPS generates a database from the software project, which is provided as input to the evolutionary algorithm for producing the set of management rules. These rules will help the project manager to keep the project within the cost, quality and duration targets. The set of alternatives within the decision-making framework is therefore reduced to a quality set of decisions. 
43|14||The next release problem|Companies developing and maintaining complex software systems need to determine the features that should be added to their system as part of the next release. They will wish to select these features to ensure the demands of their client base are satisfied as much as possible while at the same time ensuring that they themselves have the resources to undertake the necessary development. This situation is modelled in this paper and the problem of selecting an optimal next release is shown to be NP-hard. The use of various modern heuristics to find a high quality but possibly suboptimal solution is described. Comparative studies of these heuristics are given for various test cases. 
43|14||Protocols are programs too: the meta-heuristic search for security protocols|Protocol security is important. So are efficiency and cost. This paper provides an early framework for handling such aspects in a uniform way based on combinatorial optimisation techniques. The belief logic of Burrows, Abadi and Needham (BAN logic) is viewed as both a specification and proof system and as a ‘protocol programming language’. The paper shows how simulated annealing and genetic algorithms can be used to generate correct and efficient BAN protocols. It also investigates the use of parsimonious and redundant representations. 
43|14||Software engineering using metaheuristic innovative algorithms: workshop report|This paper reports on the first International Workshop on Software Engineering using Metaheuristic Innovative Algorithms, which was held in Toronto on the 14th of May 2001 as a part of the IEEE International Conference on Software Engineering. 
43|14||Author Index|
43|14||Keyword Index|
43|14||Volume 43 Contents|
43|15|http://www.sciencedirect.com/science/journal/09505849/43/15|Special section: Controlled Experiments in Software Engineering|
43|15||Usage-based readingâan experiment to guide reviewers with use cases|Reading methods for software inspections are used for aiding reviewers to focus on special aspects in a software artefact. Many experiments were conducted for checklist-based reading and scenario-based reading concluding that the focus is important for software reviewers. This paper describes and evaluates a reading technique called usage-based reading (UBR). UBR utilises prioritised use cases to guide reviewers through an inspection. More importantly, UBR drives the reviewers to focus on the software parts that are most important for a user. An experiment was conducted on 27 third year Bachelor's software engineering students, where one group used use cases sorted in a prioritised order and the control group used randomly ordered use cases. The main result is that reviewers in the group with prioritised use cases are significantly more efficient and effective in detecting the most critical faults from a user's point of view. Consequently, UBR has the potential to become an important reading technique. Future extensions to the reading technique are suggested and experiences gained from the experiment to support replications are provided. 
43|15||Impact of effort estimates on software project work|This paper presents results from two case studies and two experiments on how effort estimates impact software project work. The studies indicate that a meaningful interpretation of effort estimation accuracy requires knowledge about the size and nature of the impact of the effort estimates on the software work. For example, we found that projects with high priority on costs and incomplete requirements specifications were prone to adjust the work to fit the estimate when the estimates were too optimistic, while too optimistic estimates led to effort overruns for projects with high priority on quality and well specified requirements.Two hypotheses were derived from the case studies and tested experimentally. The experiments indicate that: (1) effort estimates can be strongly impacted by anchor values, e.g. early indications on the required effort. This impact is present even when the estimators are told that the anchor values are irrelevant as estimation information; (2) too optimistic effort estimates lead to less use of effort and more errors compared with more realistic effort estimates on programming tasks. 
43|15||Empirical validation of referential integrity metrics|Databases are the core of Information Systems (IS). It is, therefore, necessary to ensure the quality of the databases in order to ensure the quality of the IS. Metrics are useful mechanisms for controlling database quality. This paper presents two metrics related to referential integrity, number of foreign keys (NFK) and depth of the referential tree (DRT) for controlling the quality of a relational database. However, to ascertain the practical utility of the metrics, experimental validation is necessary. This validation can be carried out through controlled experiments or through case studies. The controlled experiments must also be replicated in order to obtain firm conclusions. With this objective in mind, we have undertaken different empirical work with metrics for relational databases. As a part of this empirical work, we have conducted a case study with some metrics for relational databases and a controlled experiment with two metrics presented in this paper. The detailed experiment described in this paper is a replication of the later one. The experiment was replicated in order to confirm the results obtained from the first experiment.As a result of all the experimental works, we can conclude that the NFK metric is a good indicator of relational database complexity. However, we cannot draw such firm conclusions regarding the DRT metric. 
43|15||Senior executives' use of information technology|There is a paucity of literature focusing on the ingredients for effective top management, i.e. senior executives, use of Information Technology (IT). In practice, many senior executives argue that they do not see a connection between what IT does and their tasks as executives. Based on the Technology Acceptance Model (TAM), a research model was developed and tested to assess the factors that influence the use of IT by senior executives. A dedicated system supporting the task of a senior executive, an Executive Information System (EIS), was used as the IT tool under review. A large number of external variables were identified and hypothesized, influencing the core elements of TAM. To test the research model using structural equation modeling, cross-sectional data was gathered from eighty-seven senior executives drawn from twenty-one European-based multinationals. The results supported the core TAM and found only a small number of antecedent variables influencing actual use, either directly or indirectly. In addition to identifying the external factors, three of these key variables are under managerial control. They can be used to design organizational or managerial interventions that increase effective utilization of IT. 
43|15||Anomaly detection in concurrent Java programs using dynamic data flow analysis|Concurrency constructs are widely used when developing complex software such as real-time, networking and multithreaded client–server applications. Consequently, testing a program, which includes concurrency constructs is a very elaborate and complex process. In this work, we first identify the different classes of synchronization anomalies that may occur in concurrent Java programs. We then consider testing concurrent Java programs against synchronization anomalies using dynamic data flow analysis techniques. Moreover, we show how the data flow analysis technique can be extended to detect such anomalies. 
43|15||Call for Papers|
43|2|http://www.sciencedirect.com/science/journal/09505849/43/2|A dynamically mapped open hypermedia system framework for integrating information systems|The overall goal of this research is to design a distributed, extensible, cross-platform, collaorative, and integrated system that can supplement information systems with hypertext support. In this paper, we propose a dynamically mapped open hypermedia system (DMOHS) framework that can support information systems fully. Our framework has two axes: a logical component focus and an application requirement focus. In Axis 1, we propose a conceptual DMOHS architecture with eight logical components. In Axis 2, we define and discuss major aspects of a DMOHS that should be supported in a distributed and integrated environment. Together the two axes provide a grid for specifying the logical DHOMS functionality for supporting application requirements. Given this framework, we first evaluate five open hypermedia systems and the www, and then design our own system implemented on top of the www. This paper also contributes guidelines for building mapping routines that supplement on top of the www. Further, it contributes guidelines for building mapping routines that supplement information systems with hypertext support. 
43|2||Interfaces for computer and robot assisted surgical systems|Computer and robot assisted systems for surgery imply ergonomic requirements stronger than usual, due to the fact that the typical environment of an operating room is not presently suitable for the use of a computer system. The literature does not provide many specific indications for the design of user interfaces in this particular field or to match the usability needs of the quite conservative community of surgeons. This paper tries to fill this gap giving guidelines extrapolated from the design of the present robotic systems for surgery. They can be useful to software designers to reduce the time necessary to develop a reliable and safe interface for new computer assisted systems, and to improve the usability of the existing ones. 
43|2||On the maximin algorithms for test allocations in partition testing|The proportional sampling (PS) strategy is a partition testing strategy that has been proved to have a better chance than random testing to detect at least one failure. A near proportional sampling (NPS) strategy is one that approximates the PS strategy when the latter is not feasible. We have earlier proved that the (basic) maximin algorithm generates a maximin test allocation, that is, an allocation of test cases that will maximally improve the lower bound performance of the partition testing strategy, and shown that the algorithm may serve as a systematic means of approximating the PS strategy. In this paper, we derive the uniqueness and completeness conditions of generating maximin test allocations, propose the complete maximin algorithm that generates all possible maximin test allocations and demonstrate empirically that the new algorithm is consistently better than random testing as well as several other NPS strategies. 
43|2||Design of data warehouses using metadata|Data warehouses have become an instant phenomenon in many large organizations that deal with massive amounts of information. Drawing on the experiences from the systems development field, we surmise that an effective design tool will enhance the success of warehouse implementations. Thus, we present a CASE tool designed to generate the SQL queries necessary to build a warehouse from a set of operational relational databases. The warehouse designer simply specifies a list of attribute names that will appear in the warehouse, conditions if any are desired, and a description of the operational databases. The tool returns the queries needed to populate the warehouse table. 
43|2||Structuring requirement specifications with goals|One of the foci of the recent development in requirements engineering has been the study of conflicts and vagueness encountered in requirements. However, there is no systematic way in the existing approaches for handling the interactions among nonfunctional requirements and their impacts on the structuring of requirement specifications. In this paper, a new approach is proposed to formulate the requirement specifications based on the notion of goals along three aspects: (1) to extend use cases with goals to guide the derivation of use cases; (2) to analyze the interactions among nonfunctional requirements; and (3) to structure fuzzy object-oriented models based on the interactions found. The proposed approach is illustrated using the problem domain of a meeting scheduler system. 
43|2||Testing a system specified using Statecharts and Z|A hybrid specification language Î¼SZ, in which the dynamic behaviour of a system is described using Statecharts and the data and the data transformations are described using Z, has been developed for the specification of embedded systems. This paper describes an approach to testing from a deterministic sequential specification written in Î¼SZ. By considering the Z specifications of the operations, the extended finite state machine (EFSM) defined by the Statechart can be rewritten to produce an EFSM that has a number of properties that simplify test generation. Test generation algorithms are introduced and applied to an example. While this paper considers Î¼SZ specifications, the approaches described might be applied whenever the specification is an EFSM whose states and transitions are specified using a language similar to Z. 
43|2||Java garbage collection â a generic solution?|This paper first introduces the main approaches to garbage collection (GC) in modern high-level programming languages, and then focuses on how the ‘new’ language Java addresses GC. The object oriented nature of Java and the fact that it is designed to be used in networked/distributed systems poses additional GC related problems. Issues raised by these problems are discussed and suggestions made about ways in which the full advantages of GC in Java can be realized. The main conclusion is that while automatic GC in Java makes the writing of code easier, there is still the need for programmer awareness of Java memory management. 
43|3|http://www.sciencedirect.com/science/journal/09505849/43/3|Guest Editorial|
43|3||Behavioural analysis of component-based systems|Software Engineers continue to search for efficient ways to build high quality systems. Two contrasting techniques that promise to help with the effective construction of high quality systems are the use of formal models during design and the use of components during development. In this paper, we take the position that these techniques work well together.Hardware Engineers have shown that building systems from components has brought enormous benefits. Using components permits hardware engineers to consider systems at an abstract level, making it possible for them to build and reason about systems that would otherwise be too large and complex to understand. It also enables them to make effective reuse of existing designs. It seems reasonable to expect that using components in software development will also bring advantages.Formal methods provide a means to reason about a program (or system) enabling the creation of programs which can be proved to meet their specifications. However, the size of real systems makes these methods impractical for any but the simplest of structures — constructing a complete formal specification for a commercial system is a daunting task.Using formal methods for the whole of a large commercial system is not practical, but some of the advantages of using them can be obtained where a system is to be built from communicating components, by building and evaluating a formal model of the system. We describe how a model of a system to be implemented using COM might be constructed using a particular modelling tool, RolEnact. We discuss the extent to which validation of the model contributes to the validity of the eventual solution. 
43|3||Formal modeling of the Enterprise JavaBeansâ¢ component integration framework|An emerging trend in the engineering of complex systems is the use of component integration frameworks. Such a framework prescribes an architectural design that permits flexible composition of third-party components into applications. A good example is Sun Microsystems' Enterprise JavaBeans™ (EJB) framework, which supports object-oriented, distributed, enterprise-level applications, such as account management systems. One problem with frameworks like EJB is that they are documented informally, making it difficult to understand precisely what is provided by the framework, and what is required to use it. We believe formal specification can help, and in this paper show how a formal architectural description language can be used to describe and provide insight into such frameworks. 
43|3||Controllers: reusable wrappers to adapt software components|This paper discusses the modular development of software applications in Open Systems using reusable components and controllers. In this work a component model for open system is proposed, in which components encapsulate computation, systems deal with the creation and communication of components, and the rest of the context-specific requirements and concerns are implemented by first-class, reflective, reusable entities, called controllers. Controllers are thought to be the natural evolution of the traditional object wrappers or filters to the realm of component-oriented programming, that can be added to components in a modular way, modifying their behavior according to the requirements they implement. 
43|3||Understanding âvariationâ in component-based development: case findings from practice|This paper discusses the importance of understanding and capturing variation in component-based development using the context of a six-month action research project carried out within a major system solution provider. The issues raised in the empirical context show that the norms of given development disciplines need to adapt for component-based development projects and that this is neither trivial nor easy. In addressing such issues, the implications of adopting a domain-based approach to component-based development are explored alongside the means by which this may be undertaken. This exploration highlights the need to provide an explicit understanding of variation and the ramifications of providing a component with varying degrees of context independence are explored via a scenario developed in practice. The scenario shows that neither reuse nor variation should be allowed to be chaotic if component-based development is to handle flexibility in a more cost-effective and coherent manner than system maintenance currently does. The paper concludes by presenting an example of an elaborated use case to illustrate how existing techniques can be modified to provide a structured environment for understanding variation. Holistically, the discussion shows that component-based development should not be viewed as a singular discipline, but rather as an umbrella activity that requires the melding of a number of important research areas. 
43|4|http://www.sciencedirect.com/science/journal/09505849/43/4|The optimal retrieval start times of media objects for the multimedia presentation|We design the Multimedia Presentation Coordinator (MPC) to guarantee the precise presentation relation and presentation time on a storage-based multimedia presentation system. After determining the optimal retrieval start times of media objects to be presented, the MPC performs an instant scheduling and a buffer management algorithm during the actual presentation. The instant scheduling reduces synchronization breaks due to unexpected retrieval delays during the presentation. The buffer management algorithm schedules the buffer releasing times of media objects. The evaluation results show that the MPC provides much less presentation delay and less number of synchronization breaks during the presentation than other schemes. 
43|4||Experience with mural in formalising Dust-Expert|The mural system was an outcome of a significant effort to develop a support tool for the effective use of a full formal methods development cycle. Experience with it, however, has been limited to a small number of illustrative examples that have been carried out by those closely associated with its development and implementation. This paper aims to remedy this situation by describing the experience of using mural for specifying Dust-Expert, an expert system for the relief venting of dust explosions in chemical processes. The paper begins by summarising the main requirements for Dust-Expert, and then gives a flavour of the VDM specification that was formalised using mural. The experience of using mural is described with respect to users' expectations that a formal methods tool should: (i) spot any inconsistencies; (ii) help manage and organise the specifications and allow one to easily add, access, update and delete specifications; (iii) help manage and carry out the refinement process; (iv) help manage and organise theories; (v) help manage and carry out proofs. The paper concludes by highlighting the strengths and weaknesses of mural that could be of interest to those developing the next generation of formal methods development tools. 
43|4||Measuring post-transaction error handling in database applications|Error handling is crucial in any database application and is the most complex part of it. However, it has not received sufficient attention. No systematic techniques have been proposed for the verification of error handling in database applications. This paper proposes a metric suite for measuring the post-transaction error handling in database applications. The measurement helps in verifying the appropriate provision of post-transaction error handling during the development of database applications. 
43|4||A method for interactive articulation of information requirements for strategic decision support|Decision support for strategic management, an extension to the traditional decision support systems, has been seen as an important area of research where the theory, methods and technologies can bring in a great deal of benefit to the whole enterprise at the executive level. The work in the paper first of all examines two paradigms of information provision for strategic decision support. The information requirements for decision-making are studied to reveal that the level of detail, granularity, format of presentation, and broad range of information type are unique for the applications at the strategic level. The provision of relevant information involves articulation of requirements based on the decision problems described by the executive manager.Secondly, this paper presents the MITAIS approach that allows the user to describe a decision problem interactively and assists articulation of the problem into a presentation in a decision space, by prompting possible patterns of decision dimensions. It further assists a configuration of information requirements into an information space, which covers the information required for the identified problem. MITAIS comprises a set of techniques to support articulation of a decision problem into a decision space, configuration of information space and the mapping between these two spaces. A case study has been used throughout the paper to illustrate the method. A critical evaluation of the development of MITAIS has been carried out to assess the validity of the research. Finally, the paper is concluded with suggestions for future work. 
43|4||On analyzing errors in a selectivity estimation method based on dynamic maintenance of data distribution| RésuméIn this paper, we discuss the errors in selectivity estimation using the multilevel grid file (MLGF), a multidimensional file structure. We first demonstrate that estimation errors stem from the uniformity assumption that records are uniformly distributed in their belonging region represented by an entry in a level of an MLGF directory. Based on this demonstration, we then investigate five factors affecting the accuracy of estimation: (1) the data distribution in a region, (2) the number of records stored in an MLGF, (3) the page size, (4) the query region size, and (5) the level of an MLGF directory. Next, we present the tendency of estimation errors according to the change of values for each factor through extensive experiments. The results show that the errors decrease when: (1) the distribution of records in a region becomes closer to the uniform one; (2) the number of records in an MLGF increases; (3) the page size decreases; (4) the query region size increases; and (5) the level of an MLGF directory employed as data distribution information becomes lower. After defining the Granule Ratio, the core formula representing the basic relationship between the estimation errors and the above factors, we finally examine the change of estimation errors according to the change of the values for the Granule Ratio through experiments. The results indicate that errors tend to be similar depending on the values for the Granule Ratio regardless of the various changes of the values for the factors. 
43|4||Practitioners' views on the use of formal methods: an industrial survey by structured interview|The recognised deficiency in the level of empirical investigation of software engineering methods is particularly acute in the area of formal methods, where reports about their usefulness vary widely. We interviewed several formal methods users about the use of formal methods and their impact on various aspects of software engineering including the effects on the company, its products and its development processes as well as pragmatic issues such as scalability, understandability and tool support. The interviews are a first stage of empirical assessment. Future work will investigate some of the issues raised using formal experimentation and case studies. 
43|5|http://www.sciencedirect.com/science/journal/09505849/43/5|A two-layered-class approach for the reuse of synchronization code|In concurrent object-oriented languages, the inheritance anomaly is an important and difficult problem which makes synchronization code difficult to reuse. Based on the two-layered-class model, this paper proposes a new approach to provide flexible and sufficient support for the reuse of synchronization code. 
43|5||Automated reusability quality analysis of OO legacy software|Software reuse increases productivity, reduces costs, and improves quality. Object-oriented (OO) software has been shown to be inherently more reusable than functionally decomposed software; however, most OO software was not specifically designed for reuse [Software Reuse Guidelines and Methods, Plenum Press, New York, 1991]. This paper describes the analysis, in terms of quality factors related to reusability, contained in an approach that aids significantly in assessing existing OO software for reusability. An automated tool implementing the approach is validated by comparing the tool's quality determinations to that of human experts. This comparison provides insight into how OO software metrics should be interpreted in relation to the quality factors they purport to measure. 
43|5||A tunable class hierarchy index for object-oriented databases using a multidimensional index structure|This paper presents a tunable two-dimensional class hierarchy indexing technique (2D-CHI) for object-oriented databases. We use a two-dimensional file organization as the index structure. 2D-CHI deals with the problem of clustering objects in a two-dimensional domain space consisting of the key attribute domain and the class identifier domain. In conventional class indexing techniques using one-dimensional index structures such as the B+-tree, the clustering property is owned exclusively by one attribute. These indexing techniques do not handle the queries that address both the attribute keys and the class identifiers efficiently. 2D-CHI enhances query performance by adjusting the degree of clustering between the key value domain and the class identifier domain based on the precollected usage pattern. For performance evaluation, we first compare 2D-CHI with the conventional class indexing techniques using an analytic cost model based on the assumption of uniform object distribution, and then, verify the cost model through experiments using the multilevel grid file as the two-dimensional index. We further perform experiments with nonuniform object distributions. The experiments show that our proposed method builds optimal class index structures in terms of the total number of page accesses for given the precollected usage pattern regardless of query types and object distributions. 
43|5||Safety checking in an automatic train operation system|Keeping the trains and tracks in a safe state is important for railway systems, which include automated control. ATO-2000 is an automated railway system that plans, operates, monitors and controls a small railway network of driver-less trains within a mine. The formal specifications, design and implementation of Checker Function (CF), a software sub-system responsible for maintaining safety in ATO-2000 are described. CF is an important component in a safety-critical, real-time, distributed, mobile computing system. The formal specifications (in Z) of the core safety requirements in ATO-2000 are presented which include a new representation of the track topology. Some fault tolerance of the data received from the field is achieved through data validation constraints. Command safety constraints conservatively validate outgoing commands so that no possible future system state is unsafe. A simple approach used to integrate formal methods in the industrial software development process is discussed. The paper concludes with a review of the lessons learnt. 
43|5||A conformity model of software processes|In this paper we present a new idea to compare different software process models. We propose a five-level model measuring the conformity of two process models. The approach is of practical importance for a number of applications, e.g. when a company's process model has to be checked against a generic process model required in some project, or when two companies, each working with its own process model, intend to cooperate. We illustrate our approach with a conformance study of Catalysis and the V-Model. 
43|5||Calendar|
43|6|http://www.sciencedirect.com/science/journal/09505849/43/6|Encapsulating distribution by remote objects|Middleware for modern office environments and many other application areas needs to provide support for a myriad of different, highly mobile objects. At the same time, it should be able to scale to vast numbers of objects that may possibly be dispersed over a large wide-area network. The combination of flexibility and scalability requires support for object-specific solutions that is hardly addressed by current object-based systems such as CORBA. We have developed a middleware solution that seamlessly integrates traditional remote objects with physically distributed objects that can fully encapsulate their own distribution strategy. We describe how this integration takes place, and how it can be applied to existing systems such as CORBA. 
43|6||The application of use case definitions in system design specification|We review the application of use case analysis to the software development process and identify several problems associated with the representation of complex scenario interleaving, particularly with respect to concurrent activity. In order to represent such behaviour more succinctly, we have investigated the use of finite state techniques in the form of statecharts to represent use case structures and present two case studies to demonstrate their application. These use case definitions are concerned with modelling software and peripheral behaviour and are hence proposed as a first level of design specification. Of particular interest is the use of such structures to represent the scenario interleaving associated with concurrent activity. 
43|6||Ontological analysis of wholeâpart relationships in OO-models|Earlier semantic and formal analyses of whole–part (WP) relationships in object-oriented models have led to a framework, which distinguishes between primary, consequential, secondary and dependent characteristics of WP relationships. This paper interprets, validates and elaborates on that framework using an existing ontological theory and an associated formal model of objects. The revised framework confirms most of the original characteristics and suggests a number of additions and modifications. The analysis also grounds the characteristics in the framework and thereby suggests more precise definitions for some of them. 
43|6||Decomposing legacy systems into objects: an eclectic approach|The identification of objects in procedural programs has long been recognised as a key to renewing legacy systems. As a consequence, several authors have proposed methods and tools that achieve, in general, some level of success, but do not always precisely identify a coherent set of objects. We show that using an eclectic approach, where a domain expert software engineer is encouraged to tailor and combine existing approaches, may overcome the limitation of the single approaches and helps to better address the particular goals of the project at hand and the unique aspects of the subject system. The eclectic approach is illustrated by reporting experiences from a case study of identifying coarse-grained, persistent objects to be used in the migration of a COBOL system to a distributed component system. 
43|7|http://www.sciencedirect.com/science/journal/09505849/43/7|Alternative approaches to effort prediction of ERP projects|There exist many effort prediction systems but none specifically devised for enterprise resource planning (ERP) projects, and the empirical evidence is neither convincing nor adequate from a human user perspective. Consequently, this non-empirical evaluation contributes knowledge by investigating: (i) their applicability to ERP projects, (ii) their added value to a human user beyond making a prediction, and (iii) if they make sense. The analysis suggests that regression analysis seems to be the best choice as an ERP prediction system, and that ANGEL, ACE, CART and OSR primarily add value to a user in exploratory data analysis by their ability to identify similar projects. 
43|7||OgDesk: an orthogonal graphical interface for object-oriented database systems that supports schema management, browsing and querying|This paper reports on a graphical interface, OgDesk, for object-oriented database systems (OODBs). It orthogonally supports the activities of•Browsing types and instances•Creating and maintaining instances•Creating, using and programming with graphical representations of single type queries•Creation, maintenance and evolution of the schema. 
43|7||Measuring the value of information technology in technical efficiency with stochastic production frontiers|With the vast amounts of resources being invested in information technology (IT), the issue of how to measure and manage the impact of IT on organizational performance has received increased attention. Based on the production theory in microeconomics, this paper investigates the relationship between IT investments and technical efficiency in the firm's production process. The application of stochastic production frontiers to a comprehensive firm-level panel data set provides us with empirical evidence that IT has a significantly positive effect on technical efficiency and, hence, contributes to the productivity growth in organizations, claimed by some earlier studies with the same data set. The stochastic production frontiers considered include the popular Cobb–Douglas function and the more flexible translog function. Both specifications of production technology lead to the same conclusion. Managerial implications derived from the empirical results are also presented. 
43|7||A pattern system for the development of collaborative applications|Collaborative applications provide a group of users with the facility to communicate and share data in a coordinated way. Building collaborative applications is still a complex task. In this paper we propose a pattern system to design basic aspects of data sharing, communication, and coordination for collaborative applications. These patterns are useful for the design and development of collaborative applications as well as for the development of platforms for the construction of collaborative applications. 
43|8|http://www.sciencedirect.com/science/journal/09505849/43/8|Knowledge for network support|Computer network support is a growing area of IT activities within most organisations. Even small businesses are typically moving towards computer networks rather than separate stand-alone PCs. However, the skills and knowledge required for effective computer network support are still largely uncertain. In this paper the results of a research exercise investigating the skills and knowledge required for computer network support, based on case studies in 25 organisations are reported. 
43|8||Maintenance and testing effort modeled by linear and nonlinear dynamic systems|Maintenance and testing activities — conducted, respectively, on the release currently in use/to be delivered — absorb most of total lifetime cost of software development. Such economic relevance suggests investigating the maintenance and testing processes to find models allowing software engineers to better estimate, plan and manage costs and activities.Ecological systems in which predators and prey compete for surviving were investigated by applying suitable mathematical models. An analogy can be drawn between biological prey and software defects, and between predators and programmers. In fact, when programmers start trying to recognize and correct code defects, while the number of residual defects decreases, the effort spent to find any new defect has an initial increase, followed by a decline, when almost all defects are removed, similar to prey and predator populations.This paper proposes to describe the evolution of the maintenance and testing effort by means of the predator–prey dynamic model. The applicability of the model is supported by the experimental data about two real world projects. The fit of the model when parameters are estimated on all available data is high, and accurate predictions can be obtained when an initial segment of the available data is used for parameter estimation. 
43|8||An integrated interactive environment for knowledge discovery from heterogeneous data resources|Discovering knowledge such as causal relations among objects in large data collections is very important in many decision-making processes. In this paper, we present our development of an integrated environment acting as a software agent for discovering correlative attributes of data objects from multiple heterogeneous resources. The environment provides necessary supporting tools and processing engines for acquiring, collecting, and extracting relevant information from multiple data resources, and then forming meaningful knowledge patterns. The agent system is featured with an interactive user interface that provides useful communication channels for human supervisors to actively engage in necessary consultation and guidance in the entire knowledge discovery processes. A cross-reference technique is employed for searching and discovering coherent set of correlative patterns from the heterogeneous data resources. A Bayesian network approach is applied as a knowledge representation scheme for recording and manipulating the discovered causal relations. The system employs common data warehousing and OLAP techniques to form integrated data repository and generate database queries over large data collections from various distinct data resources. 
43|8||Wrapping legacy systems for use in heterogeneous computing environments|With the advent and widespread use of object-oriented and client–server technologies, companies expect their legacy systems, developed for the centralized environment, to take advantage of these new technologies and also cooperate with their heterogeneous environments. An alternative to migrating legacy systems from the mainframe to a user-centered, distributed object computing, and client–server platform is to wrap the legacy systems on the mainframe and expose the interfaces of the legacy systems to the remote clients. The enabling middleware technologies such as Common Object Request Broker Architecture (CORBA), Component Object Model/Distributed Component Object Model (COM/DCOM), and Java RMI make the migration of the legacy systems to a heterogeneous distributed computing environment possible. In this paper, we present an approach and practical experience for integrating the legacy systems to a heterogeneous distributed computing environment by adopting the CORBA technology. It has been reported quite often that an approach like this will improve system maintainability, portability, and interoperability. We also illustrate this approach with an industrial project. The project is viewed as a reengineering effort where a centralized reengineering system is wrapped to operate in a heterogeneous distributed computing environment by leveraging CORBA technology. The reengineering approach is a combination of redesign and simple facelift. The resulting legacy integration architecture through the application of the approach is evaluated using the equality attributes proposed by Bass, Clements, and Kazman. 
43|8||User requirements for OO CASE tools|This study assesses developers' demands for OO CASE tool features and indicates the comparative strengths of various OO development features. Important OO features include support for a variety of message types, design analysis support, support for security, and concurrency and multi-thread controls. Developers are also relying on inheritance, data abstraction, and event-oriented/messaging designs in their assessment of CASE tool features. Many system developers indicate that OO CASE tool features are important; however, existing products do not address current design requirements. 
43|8||Building deductive object-oriented database systems in the ODMG era|Deductive object-oriented database systems (DOODs) have been a subject of intensive research for the last 13 years, with results embodied in several research prototypes and one commercial system produced so far [1]. However, despite the considerable number of systems available, there has been surprisingly little work on organising and analysing the several system architectures and query processing strategies that have been employed in the construction of DOODs. Furthermore, with the consolidation of the ODMG specification as a standard for object databases, it is important to assess which architectural approach is better suited for building DOODs considering the ODMG framework. This paper categorises several representative DOOD systems based on their architecture and query processing approach, and provides an analysis of the issues involved in building DOOD systems compliant with the ODMG standard. 
43|9|http://www.sciencedirect.com/science/journal/09505849/43/9|An interdisciplinary model of complexity in embedded intelligent real-time systems|Embedded Intelligent Real-Time Systems (EIRTS) are proliferating in many safety-critical large-scale systems where safety and reliability are important. EIRTS are often introduced in safety-critical large-scale systems to improve the reliability and safety, although, in many cases, they also increase the system's complexity. Defining, understanding, and measuring the complexity in EIRTS can aid us in designing and building more reliable and effective EIRTS, particularly in safety-critical settings.In this paper, we focus on the challenges associated with modeling and measurement of complexity in EIRTS and the possible impact areas of this complexity in safety-critical large-scale settings. We propose an interdisciplinary model for EIRTS complexity that shows four different complexities, metrics for measuring them, and impact areas for each complexity type. An application of our model to a real-life EIRTS is also shown along with quantification and operationalization of architectural/structural complexity which is one of four proposed complexities in our model. 
43|9||Design units â a layered approach for design driven software development|The challenge faced by software developers is to establish a manageable relationship between design and implementation. This paper describes an integrated, traceable software development approach in the context of a use case design methodology that achieves several quality control properties. The foundation for this approach lies in partitioning the design schemata into a layered architecture of functional components called design units. Design units provide the basis for the automatic generation of modular source code, the traceability of requirements throughout the software development process and the framework for a systematic approach to change management. 
43|9||Testing a distributed system: generating minimal synchronised test sequences that detect output-shifting faults|A distributed system may have a number of separate interfaces called ports and in testing it may be necessary to have a separate tester at each port. This introduces a number of issues, including the necessity to use synchronised test sequences and the possibility that output-shifting faults go undetected. This paper considers the problem of generating a minimal synchronised test sequence that detects output-shifting faults when the system is specified using a finite state machine with multiple ports. The set of synchronised test sequences that detect output-shifting faults is represented by a directed graph G and test generation involves finding appropriate tours of G. This approach is illustrated using the test criterion that the test sequence contains a test segment for each transition. 
43|9||Reuse strategies in software development: an empirical study|We report on a study of reuse strategies, as employed in performing component-based software design, that we have conducted using the Unix environment. A number of subjects were asked to develop shell scripts that would perform a set of tasks, using a design support tool that was intended to provide support for reuse of Unix processes by providing a number of recognised searching strategies to assist with identifying suitable components. We review some of the methodological problems that were posed by the study, and we also conclude that for the inexperienced subjects the early identification of reusable elements was the most consistently successful strategy to adopt. 
43|9||Erratum to âOntological analysis of wholeâpart relationships in OO-modelsâ [Information and Software Technology 43 (2001) 387â389]|
44|1|http://www.sciencedirect.com/science/journal/09505849/44/1|Software development processes â an assessment|The development and maintenance of software systems has become a major challenge. Many best practices have emerged and processes have been developed which conform to such practices. This paper reviews these approaches and provides a basis for the assessment of three processes used for the development of systems. 
44|1||The question of scale economies in softwareâwhy cannot researchers agree?|This paper investigates the different research results obtained when different researchers have investigated the issue of economies and diseconomies of scale in software projects. Although researchers have used broadly similar sets of software project data sets, the results of their analyses and the conclusions they have drawn have differed. The paper highlights methodological differences that have lead to the conflicting results and shows how in many cases the differing results can be reconciled. It discusses the application of econometric concepts such as production frontiers and data envelopment analysis (DEA) to software data sets. It concludes that the assumptions underlying DEA may make it unsuitable for most software datasets but stochastic production frontiers may be relevant. It also raises some statistical issues that suggest testing hypothesis about economies and diseconomies of scale may be much more difficult than has been appreciated. The paper concludes with a plea for agreed standards for research synthesis activities. 
44|1||The architecture of an information tool for de-mining: mine identification core module (MICM)|In this work the architecture of a mine identification core module (MICM) system is presented. This system focuses on the objects to be identified rather than the sensor technology, where most R&D efforts are concentrated. Fusing sensorial with accumulated data, it could be the base of a sensor independent decision support system forming the common core of the future multi-sensorial landmine detection/identification systems.The system is based on a landmine CAD database, a database of the different sensor types and a constantly updated false alarm database that exchange information through layers constructed using Object Oriented programming techniques. The overall structure of the tool is based on the agent/object unit concept, which represents the participating entities and their interactions. The action sequences of the de-mining operations are formed with the same technique, giving to the system the characteristics of openness, versatility and reusability. 
44|1||Translating object-oriented database transactions into relational transactions|In this paper, we present methods of translating transactions from object-oriented database (OODB) to relational database (RDB). The process involves schema mapping in data definition language and transaction translation in data manipulation language. They include scheme definition, data query and transaction operation of insert, update, and deletion. We also discuss the object-oriented features in OODB operations that are not supported by RDB, such as class hierarchy, class composition hierarchy, and set attribute, and provide a general solution to realize those mechanisms by traditional relation operations. The result of the transaction translation can be applied into adding object-oriented interface into relational database management system and to the interoperability between OODB and RDB. 
44|1||Case study: factors for early prediction of software development success|Project managers can make more effective and efficient project adjustments if they detect project high-risk elements early. We analyzed 42 software development projects in order to investigate some early risk factors and their effect on software project success. Developers in our organization found the most important factors for project success to be: (1) the presence of a committed sponsor and (2) the level of confidence that the customers and users have in the project manager and development team. However, several other software project factors, which are generally recognized as important, were not considered important by our respondents. 
44|10|http://www.sciencedirect.com/science/journal/09505849/44/10|Automated compression of state machines using UML statechart diagram notation|In object-oriented design, behavioral modeling aims at describing the behavior of objects using state machines. State machines can also be used in dynamic reverse engineering to capture the overall run-time behavior of the objects of interest. The unified modeling language (UML) statechart diagram notation provides powerful means to structure state machines, thus avoiding the plague of combinatorial explosion.Tool support for constructing statechart diagrams automatically from example scenarios is currently available. These tools are applicable in both forward and reverse engineering. For complicated objects, the statechart diagrams tend to be large, which makes them difficult to visualize and understand. In this paper, we discuss information preserving means to transform flat statechart diagrams into a more compact form. These algorithms can be applied to both manually constructed and automatically generated diagrams. The proposed technique is integrated with a UML-based software design environment TED. 
44|10||Design breakdowns, scenarios and rapid application development|In this paper we consider the way in which two representational forms, scenarios and design breakdowns, which have emerged in the traditions of human-centred design are relevant within the recent commercial emphasis on rapid application development (RAD). RAD is a contingent approach to interactive software development that is characterised by large amounts of user involvement, incremental prototyping and product-based project management. Scenarios have become popular as an intermediate representation within the human–computer interaction and computer supported co-operative work communities. Design breakdowns have been suggested as a useful organising device and design technique within the co-operative prototyping literature. Both these representational forms are not currently utilised within the commercial RAD tradition. In order to detail the relevance of these concepts to commercial development, we describe the ‘natural history’ of one particular RAD project and show how scenarios, breakdowns and the resolution of such breakdowns contributed to the successful implementation of an information system within a small commercial organisation. We conclude with a discussion of lessons from our work and some intended future work in this area. 
44|10||Measuring software evolution at a nuclear fusion experiment site: a test case for the applicability of OO and reuse metrics in software characterization|A set of software metrics has been used to provide empirical evidence on how code organization changes when a software product evolves. A Java application for graphical data display used in experimental physics has been used as a test case. Exploiting common patterns in the way software applications evolve is desirable as it would give designers and managers a better understanding of the software process. The analysis in which framework reuse has also been considered, highlighted a limited use of the inheritance mechanism and, despite an increase in the overall complexity, a substantial invariance of the internal application organization. This fact is explained by the increasing framework usage and integration during the product's lifetime. 
44|10||Understanding the use of an electronic process guide|This paper presents a case study of the installation and use of an electronic process guide within a small-to-medium software development company. The purpose of the study is to better understand how software engineers use this technology so that it can be improved and better used to support software process improvement. In the study the EPG was used to guide new processes in a software improvement programme. The use of the EPG was studied over a period of 8 months with data collected through access logs, by questionnaires and by interviews. The results show that the improvement programme was successful in improving project documentation, project management and the company's relationship with its customers. The EPG contributed to the improvement programme by providing support for the creation of templates for key project documentation, assisting with project planning and estimation and providing a forum for discussion of process and work practices. The biggest improvements that could be made to the EPG would be to provide better navigation tools including a graphical overview of the process, provide tailoring facilities, include examples and experience and link to a project management tool. 
44|11|http://www.sciencedirect.com/science/journal/09505849/44/11|INSIDE FRONT COVER|
44|11||Molecule-oriented programming in Java|Molecule-oriented programming is introduced as a programming style carrying some perspective for Java. A sequence of examples is provided. Supporting the development of the molecule-oriented programming style several matters are introduced and developed: profile classes allowing the representation of class protocols as Java classes, the ‘empirical semantics’ of null, a jargon for the description of molecules, some terminology on software life-cycles related to molecule-oriented programming, and the notion of reconstruction semantics (a guiding principle behind the set of case studies). 
44|11||Empirical study of exchange patterns during software peer review meetings|An observational empirical study is performed on the peer review meeting (PRM). The purpose of the study is to define a model for participant interactions based on statistical analysis of the moves that occur during the PRM. Protocol analysis is applied to the data from the observation of seven representative meetings held during a professional software development project. Lag sequential analysis is used to find significant relationships between moves, and hierarchical clustering is used to define a model for the exchange relationships. The approach illustrates the building up of communication patterns through three successive analysis iterations. Four significant types of exchanges are identified as characteristic of PRMs: cognitive synchronization, review, elaboration and refinement. A model is built to represents the qualitative and quantitative importance of the various exchanges occurring during PRMs. The central role of cognitive synchronization is illustrated. 
44|11||Reconstructing a formal security model|Role-based access control (RBAC) is a flexible approach to access control, which has generated great interest in the security community. The principal motivation behind RBAC is to simplify the complexity of administrative tasks. Several formal models of RBAC have been introduced. However, there are a few works specifying RBAC in a way which system developers or software engineers can easily understand and adopt to develop role-based systems. And there still exists a demand to have a practical representation of well-known access control models for system developers who work on secure system development. In this paper we represent a well-known RBAC model with software engineering tools such as Unified Modeling Language (UML) and Object Constraints Language (OCL) to reduce a gap between security models and system developments. The UML is a general-purpose visual modeling language in which we can specify, visualize, and document the components of a software system. And OCL is part of the UML and has been used for object-oriented analysis and design as a de facto constraints specification language in software engineering arena. Our representation is based on a standard model for RBAC proposed by the National Institute of Standards and Technology. We specify this RBAC model with UML including three views: static view, functional view, and dynamic view. We also describe how OCL can specify RBAC constraints that is one of important aspects to constrain what components in RBAC are allowed to do. In addition, we briefly discuss future directions of this work. 
44|11||Conceptual Modeling in the eXtreme|Conceptual Modeling-based methods and their corresponding CASE tools have traditionally had one main weak point: the use of different notations for the problem space system view (centered on the what the system is) and the solution space view (centered on the how it is to be represented). The real value of Conceptual Modeling, from a pragmatic perspective, is lost due to these different notations and the complex and often ambiguous paths required to go from one view to another. To overcome this problem, a Conceptual Schema should be a precise representation of the user requirements and should also be executable. This means that the programming tasks are really done at a higher level of abstraction, using Conceptual Modeling constructs. In this paper, we present a Conceptual Modeling in the Extreme approach for automatic software production this approach focuses the developer's efforts in the Requirements and Conceptual Modeling phases in an extreme way. This approach together with a Conceptual Model Compiler strategy produces a fully executable application. 
44|12|http://www.sciencedirect.com/science/journal/09505849/44/12|INSIDE FRONT COVER|
44|12||Automatic generation of MPEG test streams from high-level grammars|This paper describes a technique to generate complex, moving picture experts group (MPEG) data streams containing packets which range through a selected set of variants, as allowed by the grammar of the packet stream. The Prolog logic programming language has been used, whose declarative power allows data generation almost directly from the grammar, i.e. without the need for explicitly programming a grammar traversal mechanism as would be the case with an imperative language. A reasonably declarative style of grammar and variation definition is achieved, and at the same time, a reasonably efficient generation process. The basic idea is to use a declarative fragment of Prolog for the grammar, but to use imperative features of Prolog for matters like packet enumeration and packet payload generation. Generation of test data from grammars is not new, nor is the use of Prolog programs for generation of test data, but as far as we know, the combination of both has not reported on in the literature, nor its application to MPEG demultiplexers/decoders. 
44|12||Confidence intervals for captureârecapture estimations in software inspections|Software inspections are an efficient method to detect faults in software artefacts. In order to estimate the fault content remaining after inspections, a method called capture–recapture has been introduced. Most research published in fault content estimations for software inspections has focused upon point estimations. However, confidence intervals provide more information of the estimation results and are thus preferable. This paper replicates a capture–recapture study and investigates confidence intervals for capture–recapture estimators using data sets from two recently conducted software inspection experiments. Furthermore, a discussion of practical application of capture–recapture with confidence intervals is provided. In capture–recapture, used for software inspection, most research papers have reported Mh-JK to be the best estimator, but only one study has investigated its subestimators. In addition, confidence intervals based on the log-normal distribution have not been evaluated before with software inspection data. These two investigations together with a discussion provide the main contribution of this paper. The result confirms the conclusions of the replicated study and shows when confidence intervals for capture–recapture estimators can be trusted. 
44|12||On the efficiency of domain-based COTS product selection method|Use of commercial-off-the-shelf (COTS) products is becoming a popular software development method. Current methods of selecting COTS products involve using the intuition of software developers or use a direct assessment (DA) of the products. The former approach is subjective, whereas the latter approach is expensive. This high cost is because the efficiency of the DA approach is inversely proportional to the product of the number of modules in the system to be developed and the total number of modules in the candidate COTS products. With the increase in the number of available COTS components, the time spent on choosing the appropriate COTS products could easily offset the advantages of using them. Neither of the selection approaches mentioned leads to quality results. Furthermore, inappropriately chosen COTS components may cause much greater damage to a development project than faults in software units that are developed in-house. A domain model is a generic model of the domain of an application system. It captures all of the features and characteristics of the domain. We have developed a new indirect selection approach, called the domain-based COTS product selection method, which makes use of domain models. We have successfully applied our selection method to the development of an on-line margin trading application. In this paper, we first analyze the efficiency of the domain-based COTS product selection method qualitatively. Then, we study the efficiency of the method by means of a formal approach and also through the case study of the on-line margin trading application. All of these results show that the domain-based COTS product selection method is more efficient than the DA methods. 
44|13|http://www.sciencedirect.com/science/journal/09505849/44/13|INSIDE FRONT COVER|
44|13||Source code analysis and manipulation|
44|13||Characterization and automatic identification of type infeasible call chains|Many software engineering applications utilize static program analyses to gain information about programs. Some applications perform static analysis over the whole program's call graph, while others are more interested in specific call chains within a program's call graph. A particular static call chain for an object-oriented program may in fact be impossible to execute, or infeasible, such that there is no input for which the chain will be taken. Identifying infeasible static call chains can save time and resources with respect to the targeted software development tool. This paper examines type infeasibility of call chains, which may be caused by inherently polymorphic call sites and are sometimes due to imprecision in call graphs. The problem of determining whether a call chain is type infeasible is defined and exemplified, and a key property characterizing type infeasible call chains is described. An empirical study was performed on a set of Java programs, and results from examining the call graphs of these programs are presented. Finally, an algorithm that automatically determines the type infeasibility of a call chain due to object parameters is presented. 
44|13||Concurrent Ada dead statements detection|In a concurrent environment, due to schedule, race conditions and synchronisation among concurrent units, some program statements may never be executed. Such statements are dead statements and have no influence on the programs except making them more difficult to analyse and understand. Since the execution of concurrent programs is non-deterministic, it is hard to detect dead statements. In this paper, we develop a data flow approach to detect dead statements for concurrent Ada programs. In this method, concurrent Ada programs are represented by concurrent control flow graphs in a simple and precise way, and detecting rules are extracted by analysing program behaviours. Based on these rules, a dead statement detecting algorithm is proposed. 
44|13||Flow insensitive points-to sets|Pointer analysis is an important part of source code analysis. Many programs that manipulate source code take points-to sets as part of their input. Points-to related data collected from 27 mid-sized C programs (ranging in size from 1168 to 87,579 lines of code) is presented. The data shows the relative sizes and the complexities of computing points-to sets. Such data is useful in improving algorithms for the computation of points-to sets as well as algorithms that make use of this information in other operations. 
44|13||Analyzing cloning evolution in the Linux kernel|Identifying code duplication in large multi-platform software systems is a challenging problem. This is due to a variety of reasons including the presence of high-level programming languages and structures interleaved with hardware-dependent low-level resources and assembler code, the use of GUI-based configuration scripts generating commands to compile the system, and the extremely high number of possible different configurations.This paper studies the extent and the evolution of code duplications in the Linux kernel. Linux is a large, multi-platform software system; it is based on the Open Source concept, and so there are no obstacles in discussing its implementation. In addition, it is decidedly too large to be examined manually: the current Linux kernel release (2.4.18) is about three million LOCs.Nineteen releases, from 2.4.0 to 2.4.18, were processed and analyzed, identifying code duplication among Linux subsystems by means of a metric-based approach. The obtained results support the hypothesis that the Linux system does not contain a relevant fraction of code duplication. Furthermore, code duplication tends to remain stable across releases, thus suggesting a fairly stable structure, evolving smoothly without any evidence of degradation. 
44|13||The documentary structure of source code|Many tools designed to help programmers view and manipulate source code exploit the formal structure of the programming language. Language-based tools use information derived via linguistic analysis to offer services that are impractical for purely text-based tools. In order to be effective, however, language-based tools must be designed to account properly for the documentary structure of source code: a structure that is largely orthogonal to the linguistic but no less important. Documentary structure includes, in addition to the language text, all extra-lingual information added by programmers for the sole purpose of aiding the human reader: comments, white space, and choice of names. Largely ignored in the research literature, documentary structure occupies a central role in the practice of programming. An examination of the documentary structure of programs leads to a better understanding of requirements for tool architectures. 
44|13||A cache-aware program transformation technique suitable for embedded systems|In embedded systems caches are very precious for keeping low the memory bandwidth and to allow employing slow and narrow off-chip devices. Conversely, the power and die size resources consumed by the cache force the embedded system designers to use small and simple cache memories. This kind of caches can experience poor performance because of their not flexible placement policy. In this scenario, a big fraction of the misses can originate from the mismatch between the cache behavior and the memory accesses' locality features (conflict misses).In this paper we analyze the conflict miss phenomenon and define a cache utilization measure. Then we propose an object level Cache Aware allocation Technique (CAT) to transform the application to fit the cache structure, minimize the number of conflict misses and maximize cache exploitation. The solution transforms the program layout using the standard functionalities of a linker.The CAT approach allowed the considered applications to deliver the same performance on two times and sometimes four times smaller caches. Moreover the CAT improved programs on direct-mapped caches outperformed the original versions on set-associative caches. In this way, the results highlight that our approach can help embedded system designers to meet the system requirements with smaller and simpler cache memories. 
44|13||Semantic and behavioral library transformations|While software methodology encourages the use of libraries and advocates architectures of layered libraries, in practice the composition of libraries is not always seamless and the combination of two well-designed libraries not necessarily well designed, since it could result in suboptimal call sequences, lost functionality, or avoidable overhead. In this paper we introduce Simplicissimus, a framework for rewrite-based source code transformations that allows for code replacement in a systematic and safe manner. We discuss the design and implementation of the framework and illustrate its functionality with applications in several areas. 
44|13||Web application transformations based on rewrite rules|During the evolution phase, the structure (pages and links) of a Web application tends unavoidably to degrade. A solution to reverse this degradation can be restructuring the Web application, but this work may take a lot of time and effort if conducted without appropriate tools.The theory of rewrite rules has been used with success in many real restructuring works on traditional software. Our idea is trying to apply rewrite rules to Web applications with the aim of restructuring them.The purpose of this paper is threefold: to describe some examples of HTML transforms improving the quality of Web applications, to present some details about the implementation of a tool, based on rewrite rules, that can help designers in Web application restructuring, and to describe the results obtained on some real world applications. 
44|13||Source transformation in software engineering using the TXL transformation system|Many tasks in software engineering can be characterized as source to source transformations. Design recovery, software restructuring, forward engineering, language translation, platform migration, and code reuse can all be understood as transformations from one source text to another. The tree transformation language, TXL, is a programming language and rapid prototyping system specifically designed to support rule-based source to source transformation. Originally conceived as a tool for exploring programming language dialects, TXL has evolved into a general purpose source transformation system that has proven well suited to a wide range of software maintenance and reengineering tasks, including the design recovery, analysis and automated reprogramming of billions of lines of commercial Cobol, PL/I, and RPG code for the Year 2000. In this paper, we introduce the basic features of modern TXL and its use in a range of software engineering applications, with an emphasis on how each task can be achieved by source transformation. 
44|14|http://www.sciencedirect.com/science/journal/09505849/44/14|INSIDE FRONT COVER|
44|14||Special Issue for the Second AsiaâPacific Conference on Quality Software|
44|14||Optimal software testing and adaptive software testing in the context of software cybernetics|Software cybernetics explores the interplay between software theory/engineering and control theory/engineering. Following the idea of software cybernetics, the controlled Markov chains (CMC) approach to software testing treats software testing as a control problem. The software under test serves as a controlled object, and the (optimal) testing strategy determined by the theory of controlled Markov chains serves as a controller. This paper analyzes the behavior of the corresponding optimal test profile determined by the CMC approach to software testing and introduces adaptive software testing. It is shown that in some cases the optimal test profile is Markovian, whereas in some other cases the optimal test profile demonstrates a different scenario. The adaptive software testing adjusts software testing strategy on-line by using testing data collected during software testing in response to changes in our understanding of the software under test. Simulation results show that the adaptive strategy of software testing is feasible and significantly reduces the number of test cases required to detect and remove a certain number of software defects in comparison with the random strategy of software testing. 
44|14||Detection of dynamic execution errors in IBM system automation's rule-based expert system|We formally verify aspects of the rule-based expert system of IBM's system automation software for IBM's zSeries mainframes. Starting with a formalization of the expert system in propositional dynamic logic (PDL), we encode termination and determinism properties in PDL and its extension ÎPDL. We then translate our decision problems to propositional logic and apply advanced SAT techniques for automated proofs. In order to locate real program bugs for each failed proof attempt, we apply extra formalization steps and represent propositional error formulae in concise normal form as binary decision diagrams. In our experiments, we revealed residual non-termination bugs in a tested program version close to shipment, and, after correcting them, we formally verified the absence of this class of bugs in the production code. 
44|14||A CSP and Z combined modeling of document exchange processes in e-commerce protocols|E-commerce protocols comprise a vital component of the trading infrastructure over the Internet. Effectiveness and reliability of trading transactions likely depend on the quality of such protocols. However, building a quality e-commerce protocol may be more difficult than traditional protocols, as they are deployed in an open environment and data context-dependent. Application of formal methods has been suggested by many researchers as a viable approach for improving the reliability of software systems. But, can formal methods really help with the current crop of e-commerce protocols?As such, this paper made a study to formally formulate e-commerce protocols and identify their key features. A formal approach for modeling e-commerce protocols in a language combined of CSP and Z is developed. Compared to other works in formal modeling and analysis of e-commerce protocols, our approach concentrates on the XML document exchange processes of the protocols, which are a central part of most e-commerce protocols. The approach is illustrated by applying it to the Internet Open Trading Protocol, a trading protocol recommended by the Internet Engineering Task Force. The illustration demonstrates how the quality of a trading protocol can be improved through formalization. 
44|14||Software processes for the development of electronic commerce systems|The development of electronic commerce (EC) systems is subject to different conditions than that of conventional software systems. This includes the introduction of new activities to the development process and the removal of others. An adapted process must cope with important idiosyncrasies of EC system development: EC systems typically have a high degree of interaction, which makes factors like ergonomics, didactics and psychology especially important in the development of user interfaces. Typically, they also have a high degree of integration with existing software systems such as legacy or groupware systems. Integration techniques have to be selected systematically in order not to endanger the whole software development process. This article describes the development of an EC system and it generalizes salient features of the software process used. The result is a process model which can be used for other highly integrative EC system development projects. The processes described are determined by short process lifecycles, by an orientation towards integration of legacy systems and by a strict role-based cooperation approach. 
44|15|http://www.sciencedirect.com/science/journal/09505849/44/15|INSIDE FRONT COVER|
44|15||An information-leak analysis system based on program slicing|For programs using secret information such as credit card numbers, preventing information-leaks is important. Denning, for example, has proposed a mechanism to certify that a given program does not violate a security policy. Kuninobu, on the other hand, has proposed a more practical framework for calculating the secrecy level of each output value from the secrecy level set to each input value, but no implementation has been yet explored. In this paper, we propose an implementation method for information-leak analysis, and show a system we have implemented based on program slicing. We have applied this system to a credit card program. Our results show that information-leak analysis before practical use of the program is important. 
44|15||Comparison of artificial neural network and regression models for estimating software development effort|Estimating the amount of effort required for developing an information system is an important project management concern. In recent years, a number of studies have used neural networks in various stages of software development. This study compares the prediction performance of multilayer perceptron and radial basis function neural networks to that of regression analysis. The results of the study indicate that when a combined third generation and fourth generation languages data set were used, the neural network produced improved performance over conventional regression analysis in terms of mean absolute percentage error. 
44|15||Generating three-tier applications from relational databases: a formal and practical approach|This article describes a method for building applications with a three-tier structure (presentation, business, persistence) from an existing relational database. The method works as a transformation function that takes the relational schema as its input, producing three sets of classes (which depend on the actual system being reengineered) to represent the final application, as well as some additional auxiliary classes (which are ‘constant’ and always generated, such as an ‘About’ dialog, for example). All the classes generated are adequately placed along the three-tiers.The method is based on (1) the formalization of all the sets involved in the process, and (2) the mathematical formulation of the required functions to get the final application. For this second step, we have taken into account several well-known, widely used design and transformation patterns that produce high quality designs and highly maintainable software.The method is implemented in a tool that we have successfully used in several projects of medium size. Obviously, it is quite difficult for the obtained software to fulfill all the requirements desired by the customer, but the uniformity and understandability of its design makes very easy its modification. 
44|15||author index|
44|15||keyword index|
44|15||volume contents|
44|2|http://www.sciencedirect.com/science/journal/09505849/44/2|Editorial|
44|2||Bases for the development of LAST: a formal method for business software requirements specification|This paper proposes a possible approach to IS requirements specification. It relies on the application of standard (i.e. conventional) discrete mathematics, more precisely, it uses a fairly limited number of concepts from the fields of linear algebra and set theory (hence its name, LAST). The use of LAST for data definition and query–answer are discussed in some detail, given the data-rich quality of Business IS and the fact that a solid data-model is therefore essential to their specification. The proposed approach implies integration with other semiformal specification methods, two of the possibilities being integration with UML–OCL and with the Entity Relationship Model, which are discussed in this paper. Finally, mapping of LAST specifications to the Relational Model is also addressed; this possibility having an interest both, for (partial) implementation and for model simulation. 
44|2||A methodology for designing toolkits for specification level verification of interval-constrained information systems requirements|In developing automated tools for verification for an Information systems (IS), there exists a need for a methodology for the development of information requirements maintenance toolkits which will maintain IS with unconstrained and constrained requirements. In this paper, we present a methodology for building generalized designs of IS maintenance toolkits that maintain the requirements specification of IS. Our methodology will decrease the effort in building and increase the structural quality of IS maintenance toolkits that are used for verification of an IS in areas such as command, control, communication, computer, intelligence, surveillance, and reconnaissance (C4ISR) organizations. The methodology represents a reuse-oriented Unified Modeling Process (UMP) and consists of a method for developing an enterprise model, a method for developing use cases of the toolkit services, a method for developing logic designs and a method for developing the component view of the software. 
44|2||Braille to print translations for Chinese|In this paper, we study Braille word segmentation and transformation of Mandarin Braille to Chinese characters. The former consists of rule, sign and knowledge bases for disambiguation and mistake correction by using adjacent constraints and bi-directional maximal matching in which segmentation precision is better than 99%. The latter can be divided into two stages: Braille to Chinese pinyin (a phonemic Romanization) and pinyin to characters. By incorporating a pinyin knowledge dictionary into the system, we have perfectly solved the problem of ambiguity in the translation from Braille to pinyin and developed a statistical language model based on the transformation of pinyin to characters. By using Viterbi search, we have built a multi-level graph and found the sequence of Chinese characters with maximal likelihood. By using an N-Best algorithm to get the N most likely character sequences and probing into the means of measurement, our correct candidates within the top-five have a further improvement of 3%. By testing on 40,000 Chinese characters for the evaluation of the system performance, our overall translation precision of Braille codes to Chinese characters for common documents arrives at 94.38%; if proper nouns are not considered, our improvement reaches 2%. 
44|2||Soft-link hypertext for information retrieval|This paper provides a formal specification in Z of a new intelligent hypertext model called the soft-link hypertext model (SLHM). This model has been implemented and extensively tested, and provides a new methodology for constructing the future generation of information retrieval systems. SLHM has the following three major advantages. First, it is automatically formulated. Second, powerful neural learning mechanisms are applied, thereby improving its efficiency and applicability. Third, machine intelligence installed can be utilised for on-line assistance during navigating and information browsing. This specification has been developed by application of an existing formal framework for specifying hypertext systems. 
44|2||Non-specification-based approaches to logic testing for software|Testing is a crucial part of the development of software systems. In this paper, we consider testing of an implementation that is intended to satisfy a Boolean formula. In the literature, specification-based testing has been suggested for this purpose. Typically, such methods first hypothesize a fault class and then generate tests. However, there is almost no research that justifies fault classes proposed previously. Moreover, specifications amenable to automatic test generation are not always available to testers in practice. Based on these observations, we examine the applicability of non-specification-based approaches, which need no specification in the form of a Boolean formula to create tests. We compare a specification-based approach to three non-specification-based approaches, namely, random testing, antirandom testing, and combinatorial testing. The results of an experiment show that combinatorial testing is often comparative to specification-based testing and is superior to both random testing and antirandom testing. 
44|2||An efficient dynamic program slicing technique|An important application of the dynamic program slicing technique is program debugging. In applications such as interactive debugging, the dynamic slicing algorithm needs to be efficient. In this context, we propose a new dynamic program slicing technique that is more efficient than the related algorithms reported in the literature. We use the program dependence graph as an intermediate program representation, and modify it by introducing the concepts of stable and unstable edges. Our algorithm is based on marking and unmarking the unstable edges as and when the dependences arise and cease during run-time. We call this algorithm edge-marking algorithm. After an execution of a node x, an unstable edge (x, y) is marked if the node x uses the value of the variable defined at node y. A marked unstable edge (x, y) is unmarked after an execution of a node z if the nodes y and z define the same variable var, and the value of var computed at the node y does not affect the present value of var defined at the node z. We show that our algorithm is more time and space efficient than the existing ones. The worst case space complexity of our algorithm is O(n2), where n is the number of statements in the program. We also briefly discuss an implementation of our algorithm. 
44|2||Referee List|
44|3|http://www.sciencedirect.com/science/journal/09505849/44/3|Temporal fault trees|Fault tree (FT) is a simple, visual, popular and standardized notation for representing relationships between a fault in a system and the associated events. FTs are widely used for supporting products and systems in diverse industries like process control, avionics, aerospace, nuclear power systems, etc. where they are used to capture specialized and experiential knowledge for diagnosis and maintenance. FTs are also used to represent safety requirements of a system, obtained during the hazard analysis phase of the system development cycle. However, a problem that prevents more analytical use of FT is their lack of rigorous semantics. Users' understanding of an FT depends on the clarity and correctness of the natural language annotations used to label and describe various parts. Moreover, it is not clear how to adapt the FT notation to represent temporal relationships between faults and events in dynamic systems. We propose to augment the FT notation by adding simple temporal gates to capture temporal dependence between events and faults. We propose techniques to perform qualitative analysis of such temporal fault trees (TFT) to detect the causes of the top event fault by matching the TFT with the trace (or log) of the system activities. We present two algorithms for depth-first traversal and cut-set computations for a given TFT that can be used for diagnosis based on TFTs. 
44|3||Methodologies for developing Web applications|The Internet has had a significant impact on the process of developing information systems. However, there has been little research that has examined specifically the role of the development methodologies in this new era. Although there are many new forces driving systems development, many other issues are extensions of problems that have been there for some years. This paper identifies the main requirements of methodologies for developing e-commerce applications. A number of e-commerce application development approaches are examined and a methodology is proposed which attempts to address a number of issues identified within the literature. The Internet commerce development methodology (ICDM) considers evolutionary development of systems, provides a business and strategic focus and includes a management structure in addition to covering the engineering aspects of e-commerce application development.Many traditional systems development methodologies are perceived as being inadequate for dealing with the development of e-commerce systems. The paper proposes that there is a need for an overarching development framework where other more sub-system specific approaches can be integrated. However, any such framework should consider the strategic business drivers of the system, the evolutionary nature of systems, effective management structures and the development of a conducive organisational culture. 
44|3||Extending the ODMG standard with views|Views are an important functionality provided by the relational database systems. However, commercial object-oriented database systems do not support a view mechanism because defining the semantics of views in the context of an object-oriented model is more difficult than in the relational model. Indeed, views are not included in the ODMG standard. In this paper, we present a proposal aimed at including views in the ODMG, by extending the object model and the object definition language (ODL). We consider object-oriented views as having the same functionality as relational views. Views are included in the object model in such a way that (i) views make a new kind of data type definition, just as are classes, interfaces and literals, (ii) an IS-VIEW relationship is introduced in order to specify the derivation of a view from its base class, and (iii) a view instance preserves the identity of its base instance. A view can import attributes, relationships and operations from its base class, and it can also add new operations, derived attributes and derived relationships. The extent of the view is defined by an object query language (OQL) predicate. We also describe a C++ binding showing the practicability of the proposed model. 
44|3||Development of CORBA-based engineering applications from legacy Fortran programs|A majority of scientific and engineering applications in aerodynamics and solid mechanics are written in Fortran. To reduce the high cost of software development, NASA researchers reuse most of the legacy Fortran codes instead of developing them from scratch in the numerical propulsion system simulation project. In this paper, we present an efficient methodology for integrating legacy applications written in Fortran into a distributed object framework. Issues and strategies regarding the conversion and wrapping of Fortran codes into common object request broker architecture objects are discussed. Fortran codes are modified as little as possible when they are decomposed into modules and wrapped as objects. We implement a wrapper generator which takes the Fortran application as input and generates the C++ wrapper files and interface definition language file. Tedious programming tasks for wrapping the codes can therefore be reduced. 
44|3||L'E-Lyee: coupling L'Ecritoire and LyeeALL|The paper deals with the requirements engineering environment provided by L'Ecritoire to the L'E-Lyee project. The project aims to reduce the software development cycle to two explicit steps, requirements engineering and program generation, by coupling L'Ecritoire to the program generation features of LyeeALL. The basis of L'Ecritoire is a set of enactable rules to guide the requirements elicitation process through interleaved goal modelling and scenario authoring. The paper gives an overview of the enactment rules and illustrates their use through an L'Ecritoire session. Thereafter, the matching of the technical features of L'Ecritoire with those of LyeeALL is outlined and the resulting benefits are highlighted. 
44|4|http://www.sciencedirect.com/science/journal/09505849/44/4|Objects, XML and databases|The 16th Annual ACM Conference on Object-Oriented Programming Systems, Languages, and Applications (OOPSLA) was held in Tampa Bay, Florida in October 2001. This special issue of Information and Software Technology includes selected papers from the Workshop on Objects, XML and databases that was held at the conference on Sunday and Monday, 14 and 15 October. 
44|4||Mutant query plans|We propose a flexible and robust framework for distributed query processing based on mutant query plans (MQP). A MQP is an XML representation of a query plan that can also include verbatim XML data, references to resource locations (URLs), or abstract resource names (URNs). Servers work using local, possibly incomplete knowledge, partially evaluate as much of the query plan as they can, incorporate the partial results into a new, mutated query plan and transfer it to some other server that can continue processing. We have implemented an initial version of this framework, and present preliminary performance results. 
44|4||Institutions: integrating objects, XML and databases|A general model theory based on institutions is proposed as a formal framework for investigating typed object-oriented, XML and other data models equipped with integrity constraints. A major challenge in developing such a unified model theory is in the requirement that it must be able to handle major structural differences between the targeted models as well as the significant differences in the logic bases of their associated constraint languages. A distinctive feature of this model theory is that it is transformation-oriented. It is based on structural transformations within a particular category of models or across different categories with a fundamental requirement that the associated constraints are managed in such a way that the database integrity is preserved. 
44|4||An approach to high-level language bindings to XML|Values of existing typed programming languages are increasingly generated and manipulated outside the language jurisdiction. Instead, they often occur as fragments of XML documents, where they are uniformly interpreted as labelled trees in spite of their domain-specific semantics. In particular, the values are divorced from the high-level type with which they are conveniently, safely, and efficiently manipulated within the language.We propose language-specific mechanisms which extract language values from arbitrary XML documents and inject them in the language. In particular, we provide a general framework for the formal interpretation of extraction mechanisms and then instantiate it to the definition of a mechanism for a sample language core L. We prove that such mechanism can be built by giving a sound and complete algorithm that implements it.The values, types, and type semantics of L are sufficiently general to show that extraction mechanisms can be defined for many existing typed languages, including object-oriented languages. In fact, extraction mechanisms for a large class of existing languages can be directly derived from L's. As a proof of this, we introduce the SNAQue prototype system, which transforms XML fragments into CORBA objects and exposes them across the ORB framework to any CORBA-compliant language. 
44|4||V-DOM and P-XMLâtowards a valid programming of XML-based applications|Many WWW applications generate hypertext markup language or extensible markup language (XML) documents dynamically. Current tools, however, like languages using document object model (DOM) or JAVA Server Pages do not allow to check the validity of the generated documents statically. Instead, validity has to be ‘checked’ dynamically by appropriate test runs. This paper addresses this problem by introducing a new distinct class for each element type of a document type definition (DTD). Each class extends the Element-class of the DOM. The resulting object model is called validating DOM (V-DOM). Parametric XML (P-XML) is an extension of XML allowing to insert values of the newly defined classes in places, where the corresponding element types are allowed according to the underlying DTD. Like V-DOM, P-XML guarantees the validity of all XML documents generated by using P-XML. V-DOM and P-XML are illustrated by several examples. 
44|4||Using meta-data to automatically wrap bioinformatics sources|Currently there are a huge number of bioinformatics sources available over the web. Accessing these sources manually is infeasible for individual biologists. Our goal is to provide a single point of access for scientists that will retrieve data from each applicable source. One fundamental problem is automating the retrieval of data from each site. We propose a meta-data description language to delineate both the steps required to retrieve data, as well as the mechanisms necessary to access the web site that contains the data. This description will enable the automatic generation of wrappers that can extract the appropriate data. Our meta-data language is based on DARPA Agent Markup Language-S (DAML-S), extending the description to include a grounding which details the mechanics of data access. 
44|4||A method for the unification of XML schemata|Extensible Markup Language (XML) is a common standard for data representation and exchange over the Web. Considering the increasing need for managing data on the Web, integration techniques are required to access heterogeneous XML sources. In this paper, we describe a unification method for heterogeneous XML schemata. The input to the unification method is a set of object-oriented-based canonical schemata that conceptually abstract local Document Type Definitions of the involved sources. The unification process applies specific algorithms and rules to the concepts of the canonical schemata to generate a preliminary ontology. Further adjustments on this preliminary ontology generate a reference ontology that acts as a front-end for user queries to the XML sources. 
44|4||XML schema mappings for heterogeneous database access|The unprecedented increase in the availability of information, due to the success of the World Wide Web, has generated an urgent need for new and robust methods that simplify the querying and integration of data. In this research, we investigate a practical framework for data access to heterogeneous data sources. The framework utilizes the extensible markup language (XML) Schema as the canonical data model for the querying and integration of data from heterogeneous data sources. We present algorithms for mapping relational and network schemas into XML schemas using the relational mapping algorithm. We also present library system of databases (libSyD), a prototype of a system for heterogeneous database access. 
44|4||ARK: an object(ive) view of extensible minimal languages for system configuration|This paper presents ARK, a minimal, XML-based configuration language and framework for our particular problem domain: UNIX System Administration. The language allows the capture of poorly structured configuration data and code fragments written in various programming languages, while the ARK engine is responsible for interpreting the XML and allowing applications to query data and invoke code fragments.The paper discusses the need for a minimal, simple approach, and provides examples which illustrate the use of the prototype-based model supported by the language. The ARK framework is currently being used to manage several academic and industrial UNIX installations. 
44|4||Management of XML documents without schema in relational database systems|Many applications deal with highly flexible XML documents from different sources, which makes it difficult to define their structure by a fixed schema or a DTD. Therefore, it is necessary to explore ways to cope with such XML documents. The paper analyzes different storage and retrieval methods for schemaless XML documents using the capabilities of relational systems. We compare our experiences with the implementation of an XML-to-relational mapping with an LOB implementation in a commercial RDBMS. The paper concludes with a vision of how different storage methods could converge towards a common high-level XML-API for databases. 
44|5|http://www.sciencedirect.com/science/journal/09505849/44/5|Preface|
44|5||An architecture in support of universal access to electronic commerce|In order to increase accessibility to electronic commerce applications, an overall reduction in the complexity of interactions with computerized systems is required. This paper describes an architecture that uses personalization information to customize interactions with end-users in a way that reduces interaction complexity. The storage and manipulation of personal information under the control of the end-user for the dynamic customization of interactions requires a paradigm shift from the client–server model of computing to the more general peer-to-peer model. In this paper, the existing three-tier architecture is extended with a new, human-centric layer that is configurable by domain experts who are not necessarily computer science or computer engineering professionals, allowing for a ‘technology pull’ approach to the configuration of systems. The human-centric layer will support a process-oriented peer-to-peer interaction paradigm and will use existing middleware services for access to network services and legacy systems. This paper focuses on the conceptual model and the functional framework of this new human-centric layer. 
44|5||Multi-item auctions for automatic negotiation|Available resources can often be limited with regard to the number of demands. In this paper we propose an approach for solving this problem, which consists of using the mechanisms of multi-item auctions for allocating the resources to a set of software agents. We consider the resource problem as a market in which there are vendor agents and buyer agents trading on items representing the resources. These agents use multi-item auctions, which are viewed here as a process of automatic negotiation, and implemented as a network of intelligent software agents. In this negotiation, agents exhibit different acquisition capabilities that let them act differently depending on the current context or situation of the market. For example, the ‘richer’ an agent is, the more items it can buy, i.e. the more resources it can acquire. We present a model for this approach based on the English auction, then we discuss experimental evidence of such a model. 
44|5||Documenting electronic commerce systems and software using the unified modeling language|Electronic commerce (EC) systems are complex systems consisting of cooperating heterogeneous software, hardware and database subsystems that are distributed among processing nodes [1]. They are reactive, real-time and concurrent distributed systems. They are financially critical systems since they perform distributed business functions, the success of which is very critical for the business operation. The use of well-defined specification and documentation techniques is very essential for the effective development and maintenance of these systems. In this paper, we propose the use of the unified modeling language (UML) [2] as a technique for documenting and specifying EC systems at various levels of abstractions and from different views. We believe that the use of UML ensures a better reliability and reusability of these systems. 
44|5||Performance testing of a negotiation platform|Accessible from all over the world, the EC became an indispensable element to our society. It allows the use of electronic systems to exchange products, services and information between the different existent users. During these exchanges, it is very important to assure a good quality of service. However, the enormous expansion of the Internet users push its resources to the maximum of its limits, which provoke, in many cases, an important degradation in its performance. Consequently, it is primordial to analyze the capacity of servers in order to handle heavy workloads that are growing considerably as a function of the number of users. It is, therefore, necessary to conduct performance tests before servers' deployment in order to detect any imperfection and predict their behavior under stress.In this context, this paper present a simplified performance evaluation of the “alpha” version of a negotiation platform called Generic Negotiation Platform (GNP) dated on September 2000. This platform is still under development. Many performance factors could be examined in this evaluation. However, we considered only the response time factor because of its important impact on auctions and negotiations applications. We mostly oriented this study to give us an idea about the variation of the average response time of the server as a function of the number of users and the type of different transactions. We also tried to evaluate the effect of the auctions' rules on the server average response time. We limited our study to the close and open auctions at the second price.This study followed the traditional way of doing performance tests. Therefore, we fixed our test objectives and criterion and then create our own scripts. Once the nature of the workload of the server was specified, we created an adequate benchmark to generate requests to the server. Afterwards, the average response time of each considered transaction was collected. In order to interpret these results properly, we calculated the standard deviation and the coefficient of variation of each set of values. 
44|6|http://www.sciencedirect.com/science/journal/09505849/44/6|Signature caching in parallel object database systems|In many application areas, the access pattern is navigational and a large fraction of the accesses are perfect match accesses/queries on one or more words in text strings in the objects. One example of such an application area is XML data stored in object database systems. Such systems will frequently store large amounts of data, and in order to provide the necessary computing power and data bandwidth, a parallel system based on a shared-nothing architecture can be necessary. In this paper, we describe how the signature cache approach can significantly reduce the average object access cost in parallel object database systems. 
44|6||A model for availability analysis of distributed software/hardware systems|System availability is a major performance concern in distributed systems design and analysis. A typical kind of application on distributed systems has a homogeneously distributed software/hardware structure. That is, identical copies of distributed application software run on the same type of computers. In this paper, the system availability for this type of system is studied. Such a study is useful when studying optimal testing time or testing resource allocation. We consider both the case of simple two-host system, and also the more general case of multi-host system. A Markov model is developed and equations are derived to obtain the steady-state availability. Both software and hardware failures are considered, assuming that software faults are constantly being identified and removed upon a failure. Although a specific model for software reliability is used for illustration, the approach is a general one. Comparisons show that system availability changes in a similar way to single-host based software/hardware systems. Sensitivity analysis is also presented. In addition, the assumptions used in this paper are discussed. 
44|6||An evaluation of the impact of component-based architectures on software reusability|Component-based software development offers a promising solution to the production of complex distributed large-scale software systems. Development for reuse—the production of reusable components—and development with reuse—the production of systems with reusable components—provide the characteristics necessary to break the complexity of large-scale distributed systems. Two criteria for reuse in component-based architectures (CBAs) include inter-operability (focus on development for reuse) and integration (focus on development with reuse). Interoperability concerns how well components interact and integration defines how well components plug and play. The objective of this work is to evaluate the impact of three popular CBAs, namely, Enterprise Java Beans, Distributed interNetwork Architecture, and Object Management Architecture on reusability. A framework is introduced for a systematic and comprehensive analysis and evaluation of CBAs. The proposed framework is used to determine which of the above architectures more effectively addresses integration and interoperability. The results allow businesses to determine which CBA, of the above three, is ideal for reuse for a particular application. Further research opportunities in this area are discussed at the end. 
44|6||Modeling software architectures with goals in virtual university environment|Recently, the coupling of goal-based and user-centered approaches has resulted in a tremendous impact on the research of software engineering. However, there is no systematic way in the extant approaches to handling the effects of requirements on the structuring of software architectures. As an attempt towards the investigation of the interactions among goals, scenarios, and software architectures, we proposed, in this paper, a goal-driven architecture trade-off analysis method to analyze and construct software architectures in an incremental manner. We also identified criteria for architecture evaluation and verification and explored the possible types of realization of software architectures for goals. The proposed approach is illustrated using the problem domain of virtual university environment. 
44|6||Methodologies and website development: a survey of practice|Website development work is a growing aspect of the IT activities within many organisations. However, the manner in which website development actually takes place within organisations is still largely uncertain. In this paper, we examine the results of a research exercise involving case studies in 25 UK organisations aimed at investigating the way in which website development activities are currently carried out within UK organisations. In particular, this paper discusses the activities that are typically involved in website development projects, and the techniques and standards actually used for website development found within 25 organisations studied. One of the main findings of the research project was that in 25 organisations studied there was only limited use of formalised website design techniques (mainly hierarchy charts and storyboards). However, roughly half of the organisations studied did use some form of website layout standards. Website documentation was only produced in roughly a third of the organisations and only roughly a quarter of the organisations had any formalised website testing procedures in place. 
44|6||Erratum to âBases for the development of LAST: a formal method for business software requirements specificationâ: [Information and Software Technology, 44 (2002) 65â75]|
44|7|http://www.sciencedirect.com/science/journal/09505849/44/7|A framework for evaluating the effectiveness of real-time object-oriented models|The design of a real-time system needs to incorporate methods specifically developed to represent the temporal properties of the system under consideration. Real-time systems contain time and event driven actions. Structured design methods provided a reasonable set of abstractions for design of time and event driven factors in real-time designs. As program complexity, size, and time to market pressure grows, the real-time community migrated towards object-oriented technology. Evidence suggests that object-oriented technology in non-real-time systems is most effective in abstraction, modeling, implementation, and reuse of software systems. Many design models and methods exist for object-oriented real-time designs. However, the selection process of a model for a particular application remains a tedious task. This paper introduces an analysis framework that can be applied to a design model to evaluate its effectiveness according to desired performance specifications. To illustrate our approach, we present a case study using the popular automotive cruise control example on two real-time object-oriented models. 
44|7||Critical path identification in the context of a workflow|The concept of the critical path has been widely discussed in many areas of computer engineering such as parallel and distributed programs, a computer circuit, and a directed acyclic graph. The critical path analyzes in a workflow can allow us to utilize it in many workflow issues, especially workflow resource management and workflow time management. In this paper, we first describe our workflow model using the workflow queuing network. Then, we propose a method to systematically determine the critical path under the workflow model, and give an overall example that shows how our method works. In addition, several experiments are made to evaluate the accuracy of the critical path found. 
44|7||The clustering property of corner transformation for spatial database applications|Spatial access methods (SAMs) are often used as clustering indexes in spatial database systems. Therefore, a SAM should have the clustering property both in the index and in the data file. In this paper, we argue that corner transformation preserves the clustering property such that objects having similar sizes and positions in the original space tend to be placed in the same region in the transform space. We then show that SAMs based on corner transformation are able to maintain clustering both in the index and in the data file for storage systems with fixed object positions and propose the MBR-MLGF as an example to implement such an index. In the storage systems with fixed object positions, the inserted objects never move during the operation of the system. Most storage systems currently available adopt this architecture. Extensive experiments comparing with the R∗-tree show that corner transformation indeed preserves the clustering property, and therefore, it can be used as a useful method for spatial query processing. This result reverses the common belief that transformation will adversely affect the clustering and shows that the transformation maintains as good clustering in the transform space as conventional techniques, such as the R∗-tree, do in the original space. 
44|7||Reduction-based methods and metrics for selective regression testing|In corrective maintenance, modified software is regression tested using selected test cases in order to ensure that the modifications have not caused adverse effects. This activity of selective regression testing involves regression test selection, which refers to selecting test cases from the previously run test suite, and test-coverage identification. In this paper, we propose three test-selection methods and two coverage identification metrics. The three methods aim to reduce the number of selected test cases for retesting the modified software. The first method, referred to as modification-based reduction version 1 (MBR1), selects a reduced number of test cases based on the modification made and its effects in the software. The second method, referred to as modification-based reduction version 2 (MBR2) improves MBR1 by further omitting tests that do not cover the modification. The third method, referred to as precise reduction (PR), reduces the number of test cases selected by omitting non-modification-revealing tests from the initial test suite. The two coverage metrics are McCabe-based regression test metrics, which are referred to as the Reachability regression Test selection McCabe-based metric (RTM), and data-flow Slices regression Test McCabe-based metric (STM). These metrics aim to assist the regression tester in monitoring test-coverage adequacy, reveal any shortage or redundancy in the test suite, and assist in identifying, where additional tests may be required for regression testing.We empirically compare MBR1, MBR2, and PR with three reduction and precision-oriented methods on 60 test-problems. The results show that PR selects the least number of test cases most of the time and omits non-modification-revealing tests. We also demonstrate the applicability of our proposed methods to object-oriented regression testing at the class level. Further, we illustrate typical application of the RTM and STM metrics using the 60 test-problems and two coverage-oriented selective regression-testing methods. 
44|8|http://www.sciencedirect.com/science/journal/09505849/44/8|A process matching approach for flexible workflow process reuse|Matching between two workflow processes is the key step of workflow process reuse. This paper presents an inexact matching approach for flexible workflow process reuse. A multi-valued process specialization relationship is defined based on the definition of activity specialization and the characteristics of workflow process. The matching degree between two workflow processes is determined by the matching degrees of their corresponding sub-processes or activities. The matching degree between two activities is determined by the activity-distance between them in the activity-ontology repository. A set of process specialization rules enables a new process matching to be derived from the existing matchings. Users are provided with an SQL-like command to retrieve the required processes in an inexact query condition from the workflow-process-ontology repository. 
44|8||A general rule structure|In this paper, we present a theoretical development of a new rule structure called general rule structure (GRS). GRS is a rule structure based on the ideas of variable precision logic (VPL) rules and ripple down rules. GRS can be application independent and comprehensible (understandable). The term ‘general’ used in GRS indicates that the developed rule structure takes care of generality in terms of application and reasoning process (forward and backward chaining). GRS can be used for real time applications, control applications (from the characteristics of VPL) and standard rule-based expert system applications. Based on hierarchical censored production rule, GRS can give more certain and specific answers, whenever time permits. GRS can be easily used in directing the system to decide, which rule should be checked next if the currently checked rule is failed. One of the main advantages of the GRS structure is the simplicity to train it; therefore, the system can determine the most commonly used rules. This would reduce the time consumed for finding the proper rule to be fired. 
44|8||Architectural styles for distributed processing systems and practical selection method|The software architecture of a system has influences against various software characteristics of the system such as efficiency, reliability, maintainability, etc. For supporting to design the software architecture, we have developed architectural styles for distributed processing systems. The styles classify the architecture for distributed processing systems into nine categories based on the location of data storage and the type of processing between a client and a server. This paper describes our architectural styles and proposes a simple but practical method to select an appropriate architectural style for developing an application system. The selection method introduces the characterization of architectural styles and the characteristic charts to visualize their characteristics of architectural styles. Next, we propose a method to select an appropriate architectural style using the conformity between characteristic charts of a system and architectural styles. We have verified the applicability of this selection method using our customers' real application systems. 
44|8||A methodology of testing high-level Petri nets|Petri nets have been extensively used in the modelling and analysis of concurrent and distributed systems. The verification and validation of Petri nets are of particular importance in the development of concurrent and distributed systems. As a complement to formal analysis techniques, testing has been proven to be effective in detecting system errors and is easy to apply. An open problem is how to test Petri nets systematically, effectively and efficiently. An approach to solve this problem is to develop test criteria so that test adequacy can be measured objectively and test cases can be generated efficiently, even automatically. In this paper, we present a methodology of testing high-level Petri nets based on our general theory of testing concurrent software systems. Four types of testing strategies are investigated, which include state-oriented testing, transition-oriented testing, flow-oriented testing and specification-oriented testing. For each strategy, a set of schemes to observe and record testing results and a set of coverage criteria to measure test adequacy are defined. The subsumption relationships and extraction relationships among the proposed testing methods are systematically investigated and formally proved. 
44|8||Research in software engineering: an analysis of the literature|In this paper, we examine the state of software engineering (SE) research from the point of view of the following research questions:1.What topics do SE researchers address?2.What research approaches do SE researchers use?3.What research methods do SE researchers use?4.On what reference disciplines does SE research depend?5.At what levels of analysis do SE researchers conduct research?To answer those questions, we examined 369 papers in six leading research journals in the SE field, answering those research questions for each paper.From that examination, we conclude that SE research is diverse regarding topic, narrow regarding research approach and method, inwardly-focused regarding reference discipline, and technically focused (as opposed to behaviorally focused) regarding level of analysis.We pass no judgment on the SE field as a result of these findings. Instead, we present them as groundwork for future SE research efforts. 
44|9|http://www.sciencedirect.com/science/journal/09505849/44/9|Application of traditional system design techniques to web site design|After several decades of computer program construction there emerged a set of principles that provided guidance to produce more manageable programs. With the emergence of the plethora of Internet web sites one wonders if similar guidelines are followed in their construction. Since this is a new technology no apparent universally accepted methods have emerged to guide the designer in Web site construction. This paper reviews the traditional principles of structured programming and the preferred characteristics of Web sites. Finally a mapping of how the traditional guidelines may be applied to Web site construction is presented. The application of the traditional principles of structured programming to the design of a Web site can provide a more usable site for the visitors to the site. The additional benefit of using these time-honored techniques is the creation of a Web site that will be easier to maintain by the development staff. 
44|9||Anatomy of the coupling query in a web warehouse|To populate a data warehouse specifically designed for Web data, i.e. web warehouse, it is imperative to harness relevant documents from the Web. In this paper, we describe a query mechanism called coupling query to glean relevant Web data in the context of our web warehousing system called Warehouse Of Web Data (WHOWEDA). Coupling query may be used for querying both HTML and XML documents. Some of the important features of our query mechanism are ability to query metadata, content, internal and external (hyperlink) structure of Web documents based on partial knowledge, ability to express constraints on tag attributes and tagless segment of data, ability to express conjunctive as well as disjunctive query conditions compactly, ability to control execution of a web query and preservation of the topological structure of hyperlinked documents in the query results. We also discuss how to formulate query graphically and in textual form using coupling graph and coupling text, respectively. 
44|9||Access privilege management in protection systems|We consider the problem of managing access privileges on protected objects. We associate one or more locks with each object, one lock for each access right defined by the object type. Possession of an access right on a given object is certified by possession of a key for this object, if this key matches one of the object locks. We introduce a number of variants to this basic key–lock technique. Polymorphic access rights make it possible to decrease the number of keys required to certify possession of complex access privileges that are defined in terms of several access rights. Multiple locks on the same access right allow us to exercise forms of selective revocation of access privileges. A lock conversion function can be used to reduce the number of locks associated with any given object to a single lock. The extent of the results obtained is evaluated in relation to alternative methodologies for access privilege management. 
44|9||QoS-adaptive bandwidth scheduling in continuous media streaming|In this paper, we present a QoS-adaptive admission control and resource scheduling framework for continuous media (CM) servers. The framework consists of two parts. One is a reserve-based admission control mechanism in which new streams, arriving during periods of congestion, are offered lower QoS, instead of being blocked. The other part is a scheduler for continuous media with dynamic resource allocation to achieve higher utilization than non-dynamic schedulers by effectively sharing available resources among contending streams and by reclamation which is a scheduler-initiated negotiation to reallocate resources among streams to improve overall QoS. This soft-QoS framework recognizes that CM applications can generally tolerate certain variations on QoS parameters; that is, it exploits the findings about human tolerance to degradation in quality of multimedia streams. Using our policy, we could increase the number of simultaneously running clients that could be supported and could ensure a good response ratio and better resource utilization under heavy traffic requirements. Our admission control and scheduling strategy provides three principle advantages over conventional mechanisms. First, it guarantees better total system utilization. Second, it provides better disk utilization and larger admission ratio for input CM streams, which is a major advantage. Third, it still presents acceptable play-out qualities compared to the conventional greedy admission control algorithm. 
45|1|http://www.sciencedirect.com/science/journal/09505849/45/1|Editorial Board|
45|1||Introduction of a web-based submission, tracking and review system|
45|1||Fault-based testing without the need of oracles|There are two fundamental limitations in software testing, known as the reliable test set problem and the oracle problem. Fault-based testing is an attempt by Morell to alleviate the reliable test set problem. In this paper, we propose to enhance fault-based testing to alleviate the oracle problem as well. We present an integrated method that combines metamorphic testing with fault-based testing using real and symbolic inputs. 
45|1||An efficient inverted index technique for XML documents using RDBMS|The inverted index is widely used in the existing information retrieval field. In order to support containment queries for structured documents such as XML, it needs to be extended. Previous work suggested an extension in storing the inverted index for XML documents and processing containment queries, and compared two implementation options: using an RDBMS and using an Information Retrieval (IR) engine. However, the previous work has two drawbacks in extending the inverted index. One is that the RDBMS implementation is generally much worse in the performance than the IR engine implementation. The other is that when a containment query is processed in an RDBMS, the number of join operations increases in proportion to the number of containment relationships in the query and a join operation always occurs between large relations. In order to solve these problems, we propose in this paper a novel approach to extend the inverted index for containment query processing, and show its effectiveness through experimental results. In particular, our performance study shows that (1) our RDBMS approach almost always outperforms the previous RDBMS and IR approaches, (2) our RDBMS approach is not far behind our IR approach with respect to performance, and (3) our approach is scalable to the number of containment relationships in queries. Therefore, our results suggest that, without having to make any modifications on the RDBMS engine, a native implementation using an RDBMS can support containment queries as efficiently as an IR implementation. 
45|1||Efficient storage and querying of sequential patterns in database systemsâ|The number of patterns discovered by data mining can become tremendous, in some cases exceeding the size of the original database. Therefore, there is a requirement for querying previously generated mining results or for querying the database against discovered patters. In this paper, we focus on developing methods for the storage and querying of large collections of sequential patterns. We describe a family of algorithms, which address the problem of considering the ordering among elements, that is crucial when dealing with sequential patterns. Moreover, we take into account the fact that the distribution of elements within sequential patterns is highly skewed, to propose a novel approach for the effective encoding of patterns. Experimental results, which examine a variety of factors, illustrate the efficiency of the proposed method. 
45|1||Probability graph based data hoarding for mobile environment|The vision of mobile environment is severely challenged by disconnected operation. Automated hoarding is an attractive approach solution to this issue. On the other hand, the large overhead of automatic hoarding algorithm is a serious problem for handheld devices. In this paper, we propose an application-independent automatic hoarding algorithm based on probability graph. Simulation results show that it can improve cache hit rate with low time and space overhead in disconnected operation effectively, especially for small cache size, which makes our proposed algorithm preferable for handheld devices. 
45|1||Case studies to evaluate a domain specific application framework based on complexity and functionality metrics|An object-oriented framework is the reusable design or a system or subsystem implemented through a collection of concrete and abstract classes and their collaboration. It provides a generic solution to a set of similar problems in an application domain. However, it is difficult to introduce it to the organization that has been using a traditional reuse method. For effective technology transfer, the usefulness of the framework should be evaluated and shown to the organization. This paper evaluates the usefulness of a domain specific business application framework from a viewpoint of saving cost and quality of the software in a company. Here, we conducted two case studies. In the case studies, four kinds of applications are developed. Each of them is developed in two ways: using framework-based reuse and conventional module-based reuse. Then, we evaluate the difference among them using the several functionality and complexity metrics. As the results, the framework-based reuse would be more efficient than the module-based reuse in the company. 
45|1||On the use of Bayesian belief networks for the prediction of software productivity|In spite of numerous methods proposed, software cost estimation remains an open issue and in most situations expert judgment is still being used. In this paper, we propose the use of Bayesian belief networks (BBNs), already applied in other software engineering areas, to support expert judgment in software cost estimation. We briefly present BBNs and their advantages for expert opinion support and we propose their use for productivity estimation. We illustrate our approach by giving two examples, one based on the COCOMO81 cost factors and a second one, dealing with productivity in ERP system localization. 
45|10|http://www.sciencedirect.com/science/journal/09505849/45/10|Editorial Board|
45|10||An experiment in software component retrieval|Our research centers around exploring methodologies for developing reusable software, and developing methods and tools for building inter-enterprise information systems with reusable components. In this paper, we focus on an experiment in which different component indexing and retrieval methods were tested. The results are surprising. Earlier work had often shown that controlled vocabulary indexing and retrieval performed better than full-text indexing and retrieval [IEEE Trans. Software Engng (1994) 1, IEEE Trans. Software Engng 17 (1991) 800], but the differences in performance were often so small that some questioned whether those differences were worth the much greater cost of controlled vocabulary indexing and retrieval [Commun. Assoc. Comput. Mach. 28 (1985) 289, Commun. Assoc. Comput. Mach. 29 (1986) 648]. In our experiment, we found that full-text indexing and retrieval of software components provided comparable precision but much better recall than controlled vocabulary indexing and retrieval of components. There are a number of explanations for this somewhat counter-intuitive result, including the nature of software artifacts, and the notion of relevance that was used in our experiment. We bring to the fore some fundamental questions related to reuse repositories. 
45|10||PageGen: an effective scheme for dynamic generation of web pages|We present a scheme for dynamic generation of web pages. The scheme separates presentation from content. Furthermore, by utilizing the theme metaphor, the scheme makes it easy to develop a web site with several design themes, each having its own template, graphics and style sheet characteristics. The proposed scheme relies on versatile substitution mechanisms, which nonetheless use simplified syntax. Most importantly, the scheme utilizes XML for defining custom tags that are transformed into HTML using the innovative concept of HTML patterns. The scheme was initially implemented as a COM component (PageGen) and later ported to Microsoft .NET. it has proven to be quite effective for Active Server Pages (and ASP.NET) sites used to host online books and course material. However, the scheme is general enough for use with any database-centric site or content as well as being adapted to other web application frameworks such as PHP and JSP. 
45|10||A new approach to verify rule-based systems using petri nets|In the past several years, various graphical techniques were proposed to analyze various types of structural errors, including inconsistency (conflict rules), incompleteness (missing rules), redundancy (redundant rules), and circularity (circular depending rules), in rule-based systems in which rules can be represented in propositional logic. In this paper, we present a special reachability graph technique based on Ï-nets (a special type of low-level petri nets) to detect all of the above types of structural errors. Our new technique is simple, efficient, and can be easily automated. We highlight the unique features of this new approach and demonstrate its application through two examples. 
45|10||Efficient management of inspections in software development projects|During the last two decades a universal agreement has been established on the fact that software inspections play a fundamental role in improving software quality. The number of software organizations that have incorporated formal reviews in their development process is constantly increasing and the belief that efficient inspections can not only detect defects but also reduce cycle time and lower costs is spreading. However, despite the importance of the inspections in a software development project, scheduling of inspections has not been given the necessary attention so far. As a result, inspections tend to accumulate towards internal project deadlines, possibly leading to excess overtime costs, quality degradation and difficulties in meeting milestones. In this paper, data from a major telecommunications software project is analyzed in an effort to illustrate the problems that can arise from inefficient planning of inspections and their related activities. 
45|10||Operator programs and operator processes|We define a notion of program which is not a computer program but an operator program: a detailed description of actions performed and decisions taken by a human operator (computer user) performing a task to achieve a goal in a simple setting consisting of that user, one or more computers and a work environment.Our definition and notations are based on the program algebra PGA: a small body of theory allowing one to reason fundamentally and practically about programs viewed as instruction sequences.This article is entirely self-contained and introduces all concepts and notations used. We offer some small examples, and we sketch one limitation of our approach. 
45|10||MAPBOT: a Web based map information retrieval system|Many types of information are geographically referenced and interactive maps provide a natural user interface to such data. However, map presentation in geographical information systems and on the Web is closed related to traditional cartography and provides a very limited interactive experience. In this paper, we present MAPBOT, an interactive Web based map information retrieval system in which Web users can easily and efficiently search geographical information with the assistance of a user interface agent (UIA). Each kind of map feature such as a building or a motorway works as an agent called a Maplet. Each Maplet has a user interface level to assist the user to find information of interest and a graphic display level that controls the presence and the appearance of the feature on the map. The semantic relationships of Maplets are defined in an Ontology Repository provided by the system which is used by the UIA to assist a user to semantically and efficiently search map information interested. An Ontology Editor with a graphic user interface has been implemented to update the Ontology Repository. Visualization on the client is based on Scalable Vector Graphics which provides a high quality Web map. 
45|11|http://www.sciencedirect.com/science/journal/09505849/45/11|Editorial Board|
45|11||Preface|
45|11||Distributed system requirement modeling with message sequence charts: the case of the RMTP2 protocol|This document describes a case study realized for the INTERVAL european project. The aim of this study was to test time features of MSC'2000 and the real time extensions proposed by the INTERVAL partners The starting point for this study, was an IETF document describing requirements for a multicast protocol called RMTP2. Some significant requirements could not be modeled with MSC 2000, which lead to extension proposals for MSC. The main extension proposed is the introduction of multicast communications in MSC. 
45|11||Implicit integration of scenarios into a reduced timed automaton|We aim at synthesizing an executable specification for a real-time reactive system by integrating real-time scenarios into a reduced timed automaton (TA). A scenario is a part of the specification of a system behavior. The integration of scenarios into a single TA is based on its formal semantics. The TA, which results from the integration of a set of scenarios, is independent of the order in which the scenarios are added to. We also present an algorithm to reduce such resulting TA in order to prevent combinatorial explosion. 
45|11||Consolidating and applying the SDL-pattern approach: a detailed case study|This paper is on design methodology for communication systems. The SDL-pattern approach proposed recently is consolidated and applied rigorously and in detail to the design of a typical communication system on two levels of abstraction. The design is decomposed into a number of steps, each of which is carried out systematically, building on well-proven, generic pieces of solutions that have proven useful in previous projects. These generic solutions—termed SDL patterns—support reuse-driven design of communication systems, raise the vocabulary of protocol engineer to a problem-oriented level, assist the discovery and exploitation of commonalities, and lead to well-justified designs. The selection and use of SDL patterns is supported by a fine-grained incremental design process, the pattern definition takes advantage of formal design languages, and a set of heuristics addresses the decomposition of communication requirements. All these elements are presented and applied in detail to the design of a simple, but functionally complete communication system. 
45|11||A rigorous approach for constructing self-evolving real-time reactive systems|Self-evolving systems adapt themselves automatically to changing external situations and internal conditions. This paper proposes an architecture with four components, that, respectively, provides the infrastructure for formal architectural modeling of the evolving system, reuse of design elements, predicting the reliability of evolution, and managing change. The tools in TROMLAB, a framework for developing real-time reactive systems support and sustain the evolution of system components. 
45|11||Formal verification and validation for e-commerce: theory and best practices|In this paper, we describe a formal, model-driven, CORBA-based approach to developing and testing e-commerce systems. We indicate advantages and limitations of formal verification techniques using the Specification and Description Language (SDL), and relate the CORBA-based distributed object architecture to standard test methods and TTCN, the international standard test language. Finally, we enumerate industrial challenges and best practices at one of the electronic commerce software test organizations in IBM, and suggest a strategy for adapting formal test methods to the evolving industrial e-commerce test process. 
45|11||Bisimulation-based non-deterministic admissible interference and its application to the analysis of cryptographic protocols|In this paper, we first define bisimulation-based non-deterministic admissible interference (BNAI), derive its process-theoretic characterisation and present a compositional verification method with respect to the main operators over communicating processes, generalising in this way the similar trace-based results obtained [J. Univ. Comput. Sci. 6 (2000) 1054] into the finer notion of observation-based bisimulation [Logic and Models of Concurrent Systems, 1985]. Like its trace-based version, BNAI admits information flow between secrecy levels only through a downgrader (e.g. a cryptosystem), but is phrased into a generalisation of observational equivalence [Communication and Concurrency, 1989]. We then describe an admissible interference-based method for the analysis of cryptographic protocols, extending, in a non-trivial way, the non-interference-based approach presented by Focardi et al. [Proceedings of DERA/RHUL Workshop on Secure Architectures and Information Flow, 2000]. Confidentiality and authentication for cryptoprotocols are defined in terms of BNAI and their respective bisimulation-based proof methods are derived. Finally, as a significant illustration of the method, we consider simple case studies: the paradigmatic examples of the Wide Mouthed Frog protocol [ACM Trans. Comput. Syst. 8 (1990) 18] and the Woo and Lam one-way authentication protocol [IEEE Comput. 25 (1992) 39]. The original idea of this methodology is to prove that the intruder may interfere with the protocol only through selected channels considered as admissible when leading to harmless interference. 
45|12|http://www.sciencedirect.com/science/journal/09505849/45/12|Editorial Board|
45|12||Preface|
45|12||UIO sequence based checking sequences for distributed test architectures|This study addresses the construction of a preset checking sequence that will not pose controllability (synchronization) and observability (undetectable output shift) problems when applied in distributed test architectures that utilize remote testers. The controllability problem manifests itself when a tester is required to send the current input and because it did not send the previous input nor did it receive the previous output it cannot determine when to send the input. The observability problem manifests itself when a tester is expecting an output in response to either the previous input or the current input and because it is not the one to send the current input, it cannot determine when to start and stop waiting for the output. Based on UIO sequences, a checking sequence construction method is proposed to yield a sequence that is free from controllability and observability problems. 
45|12||Synthesis of distributed testers from true-concurrency models of reactive systems|Automatic synthesis of test cases for conformance testing has been principally developed with the objective of generating sequential test cases. In the distributed system context, it is worth extending the synthesis techniques to the generation of multiple testers. We base our work on our experience in using model-checking techniques, as successfully implemented in the Test Generation using the Verification tool. Continuing the works of Ulrich and König, we propose to use a true-concurrency model based on graph unfolding. The article presents the principles of a complete chain of synthesis, starting from the definition of test purposes and ending with a projection onto a set of testers. 
45|12||Experience in developing and testing network protocol software using FDTs|This paper presents the research effort to formally specify, develop and test a complex real-life protocol for mobile network radios (MIL-STD 188-220). As a result, the team of researchers from the University of Delaware and the City College of the City University of New York, collaborating with scientists from CECOM (an R&D facility of the US Army) and the US Army Research Laboratory, have helped advance the state-of-the-art in the design, development, and testing of wireless communications protocols. Estelle is used both as the formal specification language for MIL-STD 188-220 and the source to automatically generate conformance test sequences. The formal test generation effort identified several theoretical problems for wireless communication protocols (possibly applicable to network protocols in general): (1) the timing constraint problem, (2) the controllability problem, (3) inconsistency detection and elimination problem and (4) the conflicting timers problem. Based on the collaborative research results, two software packages were written to generate conformance test sequences for MIL-STD 188-220. These packages helped generate tests for MIL-STD 188-220’s Data Link Types 1 and 4 services that were realizable without timer interruptions while providing a 200% increase in test coverage. The test cases have been delivered and are being used by a CECOM conformance testing facility. 
45|12||New approaches for passive testing using an Extended Finite State Machine specification|This paper presents two new approaches for passive testing using an Extended Finite State Machine (EFSM) specification. The state of the art of passive testing shows us that all the methods for detection of errors based on EFSMs try to match the trace to the specification. Indeed, one searches a state succession in the specification machine that is able to generate the trace observed on the implementation. Using this approach, processing is performed on the specification and the trace remains in the background since no operation is applied to it. This made us realise that focusing our efforts on the trace could be beneficial and has given as result two approaches presented in this paper that extract information from the specification and then work on the trace. Thus, they take a different direction than the previous methods. We first present an approach to test traces by using invariants resulting from the specification. We formally define these invariants and we see how to extract them. We also discuss their ability to detect errors appearing in the implementation. This approach is able to test the data flow, but not in a very satisfactory way. This is the reason for a second approach seeking to apply a set of constraints to the trace. We develop in detail its principles. Both approaches are applied to a Simple Connection Protocol (SCP) and the results of preliminary experiments are presented. 
45|12||Testing distributed real-time systems in the presence of inaccurate clock synchronizations|This article deals with testing distributed real-time systems. More precisely, we propose: (1) a model for describing the specification of the implementation under test, (2) a distributed architecture of the test system (TS), (3) an approach for coordinating the testers which constitute the TS, and (4) a procedure for deriving automatically the test sequence of each tester from a global test sequence. In comparison with a very recent work, the proposed test method has the following important advantage: the clocks used by the different testers are not assumed perfectly synchronized. Rather, we assume more realistically that each clock is synchronized with a reference clock with a given (nonnull) inaccuracy. This advantage is very relevant if, for example, the testers communicate through the Internet. 
45|12||Fault detection in Rule-based Software systems|Motivated by packet filtering of firewall systems in Internet applications, we study the fault detection problem in the general Rule-based Software systems. We discuss algorithms for the detection of conflicts in a given set of rules. We first study a constrained version of the fault detection problem and propose a two-phase algorithm. The first phase is to do the rule normalization. The second phase is to detect conflicting rules. For this constrained version of the fault detection problem, the algorithm takes polynomial time. For the general problem, it is NP-hard. We apply the algorithms to the Rule Table getting from one of the firewalls in Bell Labs and report the experiment result. 
45|12||Formal verification of dependable distributed protocols|Dependable distributed systems often employ a hierarchy of protocols to provide timely and reliable services. Such protocols have both dependability and real-time attributes, and the verification of such composite services is a problem of growing complexity even when using formal approaches. Our intention in this paper is to exploit the modular design aspects appearing in most dependable distributed protocols to provide formal level of assurance for their correctness. We highlight the capability of our approach through a case study in formal modular specification and tool-assisted verification of a timestamp-based checkpointing protocol. Furthermore, during the process of verification, insights gained in such a stack of protocols have assisted in validating some additional properties those dealing with failure recovery. 
45|13|http://www.sciencedirect.com/science/journal/09505849/45/13|Editorial Board|
45|13||Architecture layers and engineering approach for agent-based system|It is necessary to support user-centric service provision paradigm in distributed, dynamic and complex computing environment. Software agent technology is considered as one of the technologies suitable to adopt such computing environment. Many researchers have emphasized on agent-based system development, but, many agent-based systems are designed and constructed in ad hoc. In particular, they do not enough consider system organization and performance aspects. More systematic engineering approach of agent-based system is required. We propose the layered architecture and engineering approach for agent-based system design. We devise the layers necessary to design agent-based system, and methods to engineer each layer. Also we show that the devised approach can be used to design agent-based system and analyze system features. The layered architecture and engineering approach of agent-based system proposed in this paper support that engineer designs efficient agent-based system. 
45|13||XDependency: maintaining relationships between XML data resources|This paper proposes an XDependency description model and language that can be used as part of a distributed integrity control system for XML data resources. The dependency description language allows relationships to be specified between XML data available from different organizations. The paper also includes a prototype agent-based system capable of enforcing distributed dependency relationships. An XDependency maintenance system should make it possible to automate the distributed integrity control for XML resources. 
45|13||Collaboration and coordination in process-centered software development environments: a review of the literature|Process-centered software development environments are systems that provide automated support for software development activities.Such environments mediate the efforts of potentially large groups of developers working on a common project. This mediation is based on runtime support for actual work performance based on formal representations of work.In the present work, we survey and assess the contributions of the software process literature under the perspective of support for collaboration and coordination. A broad range of alternative approaches to various aspects of representation and runtime support are identified, based on the analysis of an expressive number of systems. The identified functionality can serve both as a guide for the evaluation and selection of systems of this kind as well as a roadmap for the development of new, improved systems. 
45|13||Conceptual framework and architecture for service mediating workflow management|This paper proposes a three-layer workflow concept framework to realize workflow enactment flexibility by dynamically binding activities to their implementations at run time. A service mediating layer is added to bridge business process definition and its implementation. Based on this framework, we propose an extended workflow management systems architecture, in which service contracting layer, service binding layer and service invocation layer are extensions to support the proposed service mediating concept. According to an enactment specification, different instances of the same activity can be bound to different services to achieve enactment flexibility. The conceptual framework and architecture together provide a comprehensive approach to flexible service enactment in B2B collaborative settings. 
45|14|http://www.sciencedirect.com/science/journal/09505849/45/14|Editorial Board|
45|14||Eighth International Workshop on Requirements EngineeringâFoundation for Software Quality (REFSQ'02)|
45|14||The fundamental nature of requirements engineering activities as a decision-making process|The requirements engineering (RE) process is a decision-rich complex problem solving activity. This paper examines the elements of organization-oriented macro decisions as well as process-oriented micro decisions in the RE process and illustrates how to integrate classical decision-making models with RE process models. This integration helps in formulating a common vocabulary and model to improve the manageability of the RE process, and contributes towards the learning process by validating and verifying the consistency of decision-making in RE activities. 
45|14||A controlled experiment to evaluate how styles affect the understandability of requirements specifications|This paper presents a controlled experiment in which two different requirements specification styles (white-box and black-box) were compared concerning the understandability of two requirements specifications from the viewpoint of a customer. The results of the experiment confirm the common belief that black-box requirements specifications (e.g., documented with SCR) are easier to understand from a customer point of view than white-box specifications (e.g., documented with UML). Questions about particular functions and behavior of the specified system were answered faster and more correctly by the participants. This result suggests that using a black-box specification style when communicating with customers is beneficial. 
45|14||Precluding incongruous behavior by aligning software requirements with security and privacy policies|Keeping sensitive information secure is increasingly important in e-commerce and web-based applications in which personally identifiable information is electronically transmitted and disseminated. This paper discusses techniques to aid in aligning security and privacy policies with system requirements. Early conflict identification between requirements and policies enables analysts to prevent incongruous behavior, misalignments and unfulfilled requirements, ensuring that security and privacy are built in rather than added on as an afterthought. Validated techniques to identify conflicts between system requirements and the governing security and privacy policies are presented. The techniques are generalizable to other domains, in which systems contain sensitive information. 
45|14||Modelling access policies using roles in requirements engineering|Pressures are increasing on organisations to take an early and more systematic approach to security. A key to enforcing security is to restrict access to valuable assets. We regard access policies as security requirements that specify such restrictions. Current requirements engineering methods are generally inadequate for eliciting and analysing these types of requirements, because they do not allow complex organisational structures and procedures that underlie policies to be represented adequately. This paper discusses roles and why they are important in the analysis of security. The paper relates roles to organisational theory and how they could be employed to define access policies. A framework is presented, based on these concepts, for analysing access policies. 
45|14||On the interplay between consistency, completeness, and correctness in requirements evolution|The initial expression of requirements for a computer-based system is often informal and possibly vague. Requirements engineers need to examine this often incomplete and inconsistent brief expression of needs. Based on the available knowledge and expertise, assumptions are made and conclusions are deduced to transform this ‘rough sketch’ into more complete, consistent, and hence correct requirements. This paper addresses the question of how to characterize these properties in an evolutionary framework, and what relationships link these properties to a customer's view of correctness. Moreover, we describe in rigorous terms the different kinds of validation checks that must be performed on different parts of a requirements specification in order to ensure that errors (i.e. cases of inconsistency and incompleteness) are detected and marked as such, leading to better quality requirements. 
45|15|http://www.sciencedirect.com/science/journal/09505849/45/15|Editorial Board|
45|15||Special issue on modelling organisational processes|
45|15||Business processesâattempts to find a definition|Definitions of business process given in much of the literature on Business Process Management are limited in depth and their related models of business processes are correspondingly constrained. After giving a brief history of the progress of business process modeling techniques from production systems to the office environment, this paper proposes that most definitions are based on machine metaphor type explorations of a process. While these techniques are often rich and illuminating it is suggested that they are too limited to express the true nature of business processes that need to develop and adapt to today's challenging environment. 
45|15||Active meta-process models: a conceptual exposition|The difficult and complex interactions between an organisation and its Information and Communications Technology (ICT) systems have presented process modelling research with a very fundamental challenge. From these concerns has emerged the concept of the active meta-process model. This sets out a conceptual basis for understanding the organisation/ICT relationship, and has the potential to influence ICT developments. The central concern of this paper is to develop the theoretical grounding of the idea of the active meta-process model. This concept has its lineage in process modelling research. Fundamentally, it is concerned with the relationship between organisations and ICT systems and, in particular, the notion that ICT systems should be viewed as providing an active canvas supporting the organisation. A conceptual exposition is developed by reference to cybernetic theory through the Viable System Model. The paper advocates an holistic and systemic approach to the comprehension and realisation of the ICT/organisational relationship which necessarily requires (a) an understanding of all of the behaviours of an organisation (including specifically the meta behaviours) and (b) a systemic and architectural view of how ICT systems should be formed in order to address the needs of organisations as systems. The outcome of this is a superior conceptual grounding for the notion of active meta-process models, and insights into how the concept might influence practice. 
45|15||Using Alloy in process modelling|Many notations and supporting tools are available to be used for describing business processes, but most lack the ability to do more than a syntactic check on the form of the model. Checking that they correctly depict the semantics of the situation is left to inspection. In this paper, we examine the uses that the analysable language Alloy can play in process modelling. We explore its application to descriptions of the organisation and its rules, to the description of processes and in the meta-modelling of process models. From the experiment with modelling a somewhat complex organisational situation, we conclude that Alloy has potential to increase the value of process modelling. 
45|15||Traceability in requirements through process modelling, applied to social care applications|This paper relates experiences of using a business-process approach to the determination of requirements for social care systems. A method has been developed and used successfully with a number of major research projects, most specifically PLANEC. A protocol and framework are presented that utilise the Unified Modelling Language and adopts best practice from IT and social science methods. It utilises a loose-coupled hierarchical grouping of processes as a strategic view, and more tightly coupled models such as workflows. The method, as it has evolved, has produced a clear linkage between stakeholder goals and expectations, and IT functionality expressed as UML use cases. 
45|15||Bridging the gap between business models and system models|This paper discusses links that may be made between process models and Unified Modelling Language (UML) software specification techniques, working from an argument that the whole complexity of organisational activity cannot be captured by UML alone. The approach taken is to develop a set of use cases, which would be capable of providing information support to a pre-defined organisational process. The nature of the thinking, which is necessary to derive the use cases, is outlined using the pre-defined process as a case study. The grouping of transactions and state changes into Use Cases is shown to require design choices, which may vary between particular organisational contexts. Conclusions are drawn about the direction of further investigation of links between process modelling and UML. 
45|15||Flexible B2B processes: the answer is in the nodes|The time and costs involved in connecting the IT systems of two companies impact the actual formation of business relationships. A flexible infrastructure for process management is instrumental for rapid and cost-effective B2B integration. One dimension of flexibility that system integrators identify as critical is node-level interaction. In this paper, we discuss the findings of the Nile project on B2B process integration. In particular, we present the methodology defined in Nile for the semi-automatic reconciliation of node-level incompatibilities. 
45|15||Business process managementâthe third wave: business process modelling language (bpml) and its pi-calculus foundations|This paper introduces the ideas behind BPML, the business process modelling language published by BPMI. BPML provides a process-centric (as opposed to a datacentric) metalanguage and execution model for business systems. It is underpinned by a strong mathematical foundation, the pi-calculus. The current paper is derived from supplementary appendices to a book which describes a ‘third wave’ approach to business process management [Business Process Management: The Third Wave, 2003]. The aim is to model business processes directly in an executable form, so that the mobility and mutability inherent in business behaviour is reflected and supported in the corresponding IT systems, erasing the present IT-business divide. 
45|15||Preconditions for putting processes back in the hands of their actors|Enterprises have now been crushed by ‘enterprise solutions’. The enterprise's information is back in the hands of the IT department. Will the same happen to the enterprise's processes? Or can computer support of those processes be put into the hands of the people who carry out the processes, who need to change them, combine them, separate them, and spread them? What properties must our business process management methods and technology have to make this possible? This paper identifies four. 
45|15||Author Index|
45|15||keyword Index|
45|15||Volume Contents|
45|2|http://www.sciencedirect.com/science/journal/09505849/45/2|Editorial Board|
45|2||A multi-method for defining the organizational change|The assumption of the work presented in this paper is the situatedness of the change process. The Enterprise Knowledge Development-Change Management Method (EKD-CMM) provides multiple and dynamically constructed ways of working to organize and to guide the change management. The method is built on the notion of labeled graph of intentions and strategies called a road map and the associated guidelines. The EKD-CMM road map is a navigational structure that supports the dynamic selection of the intention to be achieved next and the appropriate strategy to achieve it whereas guidelines help in the operationalization of the selected intention following the selected strategy. This paper presents the EKD-CMM road map and guidelines and exemplifies their use with a real case study. 
45|2||From use cases to classes: a way of building object model with UML|In a use case-driven process, classes in the class diagram need to be identified from use cases in the use case diagram. Current object modelling approaches identify classes either from use case descriptions, or using classic categories. Both ways are inefficient when use cases can be described with many scenarios in different words. This paper represents a new approach that identifies classes based on goals of use cases without descriptions. The approach produces use case-entity diagrams as a vehicle for deriving classes from use cases and to show the involvement of classes in use cases of a system. 
45|2||Building XML application in rich detailed genealogical information|Genealogical information is rich and complex [2]. Numerous studies showed that XML technologies could be employed to deal with complex information systems [15]. This paper reports an investigation of building XML application in rich and complex genealogical information. XML technologies, such as XML DTD, schema, XSL and XPath, were employed to tackle this task. Three commercial software packages are used to obtain reference results for further comparison. It is found that XML application could store more specific data than those in the evaluated software; further more, it appears to offer more diverse display options compared with the evaluated software packages. Finally, it concluded that XML application is the better approach for dealing with rich detailed genealogical information and displaying them upon Internet. 
45|2||Empirical studies of some hashing functions|The best hash function for a particular data set can often be found by empirical studies. The studies reported here are aimed at discovering the most appropriate function for hashing Nigerian names. Five common hash functions—the division, multiplication, midsquare, radix conversion and random methods—along with two collision—handling techniques—linear probing and chaining—were initially tried out on three data sets, each consisting of about 1000 words. The first data set consists of Nigerian names, the second of English names, and the third of words with computing associations. The major finding is that the performance of these functions with the Nigerian names is comparable to those for the other data sets. Also, the superiority of the random and division methods over others is confirmed, even though the division method will often be preferred for its ease of computation. It is also demonstrated that chaining, as a technique for collision-handling, is to be preferred. The hash methods and collision-handling methods were further tested by using much larger data sets and long multiple word strings. These further tests confirmed the previous findings. 
45|3|http://www.sciencedirect.com/science/journal/09505849/45/3|Editorial Board|
45|3||A computational argumentation methodology for capturing and analyzing design rationale arising from multiple perspectives|Contemporary development methodologies provide for the traceability of design decisions to functional requirements. However, they fail to adequately provide this type of traceability for non-functional requirements. This paper describes a methodology for the computational evaluation of the impact of design decisions on the non-functional requirements of the various stakeholders involved in the development of a system. A design dialog process based upon a design argumentation model is described. The dialog process provides for the capture of design rationale and for their management. Fuzzy inference is used to identify conflicts, to simply arguments, and to measure the favorability of an argument. 
45|3||An effort prediction interval approach based on the empirical distribution of previous estimation accuracy|When estimating software development effort, it may be useful to describe the uncertainty of the estimate through an effort prediction interval (PI). An effort PI consists of a minimum and a maximum effort value and a confidence level. We introduce and evaluate a software development effort PI approach that is based on the assumption that the estimation accuracy of earlier software projects predicts the effort PIs of new projects. First, we demonstrate the applicability and different variants of the approach on a data set of 145 software development tasks. Then, we experimentally compare the performance of one variant of the approach with human (software professionals') judgment and regression analysis-based effort PIs on a data set of 15 development tasks. Finally, based on the experiment and analytical considerations, we discuss when to base effort PIs on human judgment, regression analysis, or our approach. 
45|3||Design of a simulation environment based on software agents and the high level architecture|We present the SSAHLA project, standing for Simulation based on Software Agents and the High Level Architecture. This project aims at designing a simulation environment for training scenarios, using software agents and the High Level Architecture (HLA). Software agents are considered due to their relevant features to simulation, such as autonomy, adaptability, and collaboration. The HLA is considered as a communication middleware between the participants who take part to the training sessions. Each participant is associated with different types of agents that work for him and thus, perform operations on his behalf. 
45|3||The use of adapters to support interoperability of components for reusability|Interoperability of heterogeneous applications is defined as the ability for multiple software applications written in different programming languages running on different platforms with different operating systems to communicate and interact with one another over different computer networks. The emerging middleware technologies, including CORBA, COM/DCOM, and Enterprise JavaBeans offer an industrial defacto standard communication infrastructure to support the interoperability of heterogeneous applications in components. However, the implementation of a component suffers from high interaction complexities in the component that seriously degrades the application independence. Software components should be built to be independent of the context in which they are used, allowing them to be reused in many different computing environments. In this paper, we are presenting an adapter to isolate, encapsulate, and manage a component's interactions outside the component. The dynamic interface binding was designed to allow an adapter to examine the signature of the requested services at runtime such as operation names, parameters orders, parameters types, and parameters sizes. The interfaces of interconnecting components are bound at runtime. In addition, the interface language mapping allows an interface in a specific programming language to be automatically generated from an IDL interface. The use of adapters increases the reusability of components and also simplifies the integration of the components to an application. 
45|3||Formal specification of design pattern combination using BPSL|Pattern users are faced with difficulties in understanding when and how to use the increasing number of available design patterns due the inherent ambiguity in the existing means (textual and graphical) of describing them. Since patterns are seldom used in isolation but are usually combined to solve complex problems, the above-mentioned difficulties have even worsen.Hence, there is an appealing need to introduce formalism to accurately describe patterns and pattern combination to allow rigorous reasoning about them. The main problem of existing formal specification languages for design patterns is lack of completeness. This is mainly due either because they were not originally conceived to specify design patterns and have been adapted to do so, or they tend to focus on specifying either the structural or behavioral aspect of design patterns but not both of them. Moreover, only few of them venture in specifying design pattern combination.We propose a simple yet Balanced Pattern Specification Language that is aimed to achieve equilibrium by specifying the structural as well as behavioral aspects of design patterns. This is achieved by combining two subsets of logic one from First Order Logic and one from Temporal Logic of Actions. Moreover it can be used to formally specify pattern combination. 
45|4|http://www.sciencedirect.com/science/journal/09505849/45/4|Editorial Board|
45|4||A method for module architecture verification and its application on a large component-based system|A method for module architecture verification is described, which yields support for checking on an architectural level whether the implicit module architecture of the implementation of a system is consistent with its specified module architecture, and which facilitates achieving architecture conformance by relating architectural-level violations to the code-level entities that cause them, hence making it easier to resolve them. Module architecture conformance is needed to enable implementing and maintaining the system and reasoning about it. We describe our experience having applied the proposed method to check a representative part of the module architecture of a large industrial component-based software system. 
45|4||Performance and power evaluation of C++ object-oriented programming in embedded processors|The development of high-performance and lower power portable devices relies on both the underlying hardware architecture and technology as well as on the application software that executes on embedded processor cores. One way to confront the increasing complexity and decreasing time-to-market of embedded software is by means of modular and reusable code, forcing software designers to use objected oriented programming languages such as C++ [6]. However, the object-oriented approach is known to introduce a significant performance penalty compared to classical procedural programming. In this paper, the object oriented programming style is evaluated in terms of both performance and power for embedded applications. Profiling results indicate that C++ programs apart from being slower than their corresponding C versions, consume significantly more energy. Further analysis shows that this is mainly due to the increased instruction count, larger code size and increased number of accesses to the data memory for the object-oriented versions. 
45|4||A multi-step approach for partial similarity search in large image data using histogram intersection|We investigate the problem of retrieving partially similar images from a large image database. The region-based image retrieval technique is a method of retrieving partially similar images and has been proposed as a way to efficiently process queries in an image database In region-based image retrieval, region matching is indispensable for computing the partial similarity between two images because the query processing is based upon regions instead of the entire image. A naive method of region matching is a pairwise comparison between regions; this causes severe overhead and deteriorates the performance of query processing. In this paper, we focus on the development of a filtering function for the reduction of overall search time in region-based image retrieval, which is of special importance in the case of retrieving partially similar images from a large image database. To prune irrelevant images in a database, we introduce a correct and efficient similarity function by using the Histogram Intersection, which is needed for a crude selection based on a lower bounding property. Subsequently the result is refined by the pairwise region comparison between the query image and selected images. We have performed extensive experiments on synthetic and real image data to evaluate our proposed method. The experimental results reveal that our proposed technique achieves a significant pruning of up to 99% of irrelevant images and is up to 22 times faster than pairwise comparison, where the number of bins is set at 100. 
45|4||A comparison of âtraditionalâ and multimedia information systems development practices|As multimedia information systems begin to infiltrate organizations, there arises a need to capture and disseminate knowledge about how to develop them. Little is thus far known about the realities of multimedia systems development practice, or about how the development of multimedia systems compares to that of ‘traditional’ information systems. Herein are discussed the findings of a survey of multimedia developers in Ireland. Practitioners generally agree that systematic approaches are desirable in order to beneficially add structure to development processes, but they are predominantly using their own in-house methods rather than those prescribed in the literature. 
45|5|http://www.sciencedirect.com/science/journal/09505849/45/5|Editorial Board|
45|5||A methodological approach for hypermedia security modeling|In hypermedia systems security is becoming a key requirement to preserve both confidentiality and integrity. Although some authorization models for hypermedia have been proposed, what is needed is to integrate security modeling into the whole development process, so that designers count on methods to specify all the features of their hypermedia systems, including navigation capabilities, interactivity, multimedia features as well as security. In this paper, we describe how security modeling is approached in a methodology for hypermedia systems, called Ariadne, that offers a number of conceptual and platform-independent tools to specify the features of any hyperdocument in an integrated and progressive way. 
45|5||Efficient processing of regular path joins using PID|XML is data that has no fixed structure. So it is hard to design a schema for storing and querying an XML data. Instead of a fixed schema, graph-based data models are widely adopted for querying XML. Queries on XML are based on paths in a data graph.A meaningful query usually has several paths in it, but much of recent research is more concerned with optimizing a single path in a query. In this paper, we present an efficient technique for processing multiple path expressions in a query. We implemented our technique and present preliminary performance results. 
45|5||Multiuser collaborative work in virtual environment based CASE tool|VRCASE is a virtual environment based Computer Aided Software Engineering (CASE) tool. It provides a 3D multiuser collaborative software modeling environment with automatic object-class abstraction, class diagram generation, and C++ skeleton generation facilities for assisting Object-Oriented software development. It allows multiple concurrent users to model software system collaboratively. To achieve efficient collaborative software development in VRCASE, we have proposed and implemented a Fine-grained locking and notification mechanism together with visual indicators to maintain system consistency among multiple concurrent users. The system evaluation shows that our approach can effectively support multiuser collaborative software design in VRCASE. 
45|5||Communicating X-machines: a practical approach for formal and modular specification of large systems|An X-machine is a general computational machine that can model: (a) non-trivial data structures as a typed memory tuple and (b) the dynamic part of a system by employing transitions, which are not labelled with simple inputs but with functions that operate on inputs and memory values. The X-machine formal method is valuable to software engineers since it is rather intuitive, while at the same time formal descriptions of data types and functions can be written in any known mathematical notation. These differences allow the X-machines to be more expressive and flexible than a Finite State Machine. In addition, a set of X-machines can be viewed as components, which communicate with each other in order to specify larger systems. This paper describes a methodology as well as an appropriate notation, namely X-machine Description Language (XMDL), for building communicating X-machines from existing stand-alone X-machine models. The proposed methodology is accompanied by an example model of a traffic light junction, which demonstrates the use of communicating X-machines towards the incremental modelling of large-scale systems. It is suggested that through XMDL, the practical development of such complex systems can be split into two separate activities: (a) the modelling of stand-alone X-machine components and (b) the description of the communication between these components. The approach is disciplined, practical, modular and general in the sense that it subsumes the existing methods for communicating X-machines. 
45|5||A viewpoint on software engineering and information systems: integrating the disciplines|
45|5||Architectural images of computer-based information systems development: A response to âA viewpoint of software engineering and information systems: integrating the disciplinesâ|
45|6|http://www.sciencedirect.com/science/journal/09505849/45/6|Editorial Board|
45|6||An expert screen design and evaluation assistant that uses knowledge-based backtracking|Current user interface builders provide only low-level assistance, because they have knowledge of neither the application, nor the principles by which interface elements are combined effectively. We describe a design tool that unites the knowledge components essential for effective dialog box layout. The knowledge base model consists of (1) style knowledge, which provides design-time constraints on interface specifications; (2) style application system, which handles the actual layout of dialog boxes based on the style knowledge base; (3) design knowledge, which provides quantitative metrics to analyse the spatial properties of the dialog box layout; and (4) design evaluation system, which uses evaluation metrics to compare and evaluate alternative designs. As our empirical study shows, the redesigns using the design tool had significant effect on the preference data. 
45|6||A technique to analyze information-flow in object-oriented programs|Traditional information-flow analysis is mainly based on data-flow and control-flow equations. In object-oriented programs, because of the mechanisms such as encapsulation, inheritance, and polymorphism, information-flow analysis becomes quite complex and therefore it is not enough to analyze object-oriented information-flow using traditional techniques. Some new techniques are required in order to efficiently analyze the information-flow between basic components (such as statements, methods, classes or packages) in object-oriented programs. Based on object-oriented program slicing techniques, we discuss how to compute the amount of information-flow, the width of information-flow, the correlation coefficient between basic components as well as degree of coupling of a basic component can be computed. We also discuss some applications of the information-flow. 
45|6||Data model for warehousing historical Web information|In this paper, we present a temporal web data model designed for warehousing historical data from World Wide Web (WWW). As the Web is now populated with large volume of information, it has become necessary to capture selected portions of web information in a data warehouse that supports further information processing such as data extraction, data classification, and data mining. Nevertheless, due to the unstructured and dynamic nature of Web, the traditional relational model and its temporal variants could not be used to build such a data warehouse. In this paper, we therefore propose a temporal web data model that represents web documents and their connectivities in the form of temporal web tables. To represent web data that evolve with time, a visible time interval is associated with each web document. To manipulate temporal web tables, we have defined a set of web operators with capabilities ranging from extracting WWW information into web tables, to merging information from different web tables. We further illustrate the use of our temporal web data model using some realistic motivating examples. 
45|6||Converting relational database into XML documents with DOM|The revolution of XML is recognized as the trend of technology on the Internet to researchers as well as practitioners. Companies need to adopt XML technology. With investment in the current relational database systems, they want to develop new XML documents while running existing relational databases on production. They need to reengineer the relational databases into XML documents with constraints preservation. In the process, schema translation must be done before data conversion. Since the existing relational databases are usually normalized, they have to be reconstructed into XML document tree structures. This can be accomplished through denormalization by joining the normalized relations into tables according to their data dependencies constraints. The joined tables are mapped into DOMs, which are then integrated into XML document trees. The user specifies an XML document root with its relevant nodes to form a partitioned XML document tree to meet their requirements. The selected XML document tree is mapped into an XML schema in the form of DTD. We then load joined tables into DOMs, integrate them into a DOM, and transform it into an XML document. 
45|6||DocFlow: workflow based requirements elicitation|Use cases are the favoured technique for defining the functional requirements of a software system, but their use implies that the desired functionality of the new system is well known. The aim of this work is to present an alternative technique—and a supporting tool—to accurately define this functionality, expressed as use cases, starting from the workflows that describe the end user work. The use of hypergraphs in the proposed algorithm of transformation reinforces the generation process. In addition, the technique is independent of the development paradigm and a variation in the algorithm allows obtaining Data Flow Diagrams. 
45|7|http://www.sciencedirect.com/science/journal/09505849/45/7|Editorial Board|
45|7||Introduction to software engineering with computational intelligence|
45|7||Application of fuzzy expert systems in assessing operational risk of software|Risk is the potential for realization of undesirable consequences of an event. Operational risk of software is the likelihood of untoward events occurring during operations due to software failures. NASA IV&V Facility is an independent institution which conducts Independent Assessments for various NASA projects. Its responsibilities, among others, include the assessments of operational risks of software. In this study, we investigate Independent Assessments that are conducted very early in the software development life cycle.Existing risk assessment methods are largely based on checklists and analysis of a risk matrix, in which risk factors are scored according to their influence on the potential operational risk. These scores are then arithmetically aggregated into an overall risk score. However, only incomplete project information is available during the very early phases of the software life cycle, and thus, a quantitative method, such as a risk matrix, must make arbitrary assumptions to assess operational risk.We have developed a fuzzy expert system, called the Research Prototype Early Assessment System, to support Independent Assessments of projects during the very early phases of the software life cycle. Fuzzy logic provides a convenient way to represent linguistic variables, subjective probability, and ordinal categories. To represent risk, subjective probability is a better way than quantitative objective probability of failure. Furthermore, fuzzy severity categories are more credible than numeric scores. We illustrated how fuzzy expert systems can infer useful results by using the limited facts about a current project, and rules about software development. This approach can be extended to add planned IV&V level, history of past NASA projects, and rules from NASA experts. 
45|7||Software source code sizing using fuzzy logic modeling|Knowing the likely size of a software product before it has been constructed is potentially beneficial in project management: for instance, size can be an important factor in determining an appropriate development/integration schedule, and it can be a significant input in terms of the allocation of personnel and other resources. In this study we consider the applicability of fuzzy logic modeling methods to the task of software source code sizing, using a previously published data set. Our results suggest that, particularly with refinement using data and knowledge, fuzzy predictive models can outperform their traditional regression-based counterparts. 
45|7||Software quality analysis with the use of computational intelligence|Quality of individual objects composing a software system is one of the important factors that determine quality of this system. Quality of objects, on the other hand, can be related to a number of attributes, such as extensibility, reusability, clarity and efficiency. These attributes do not have representations suitable for automatic processing. There is a need to find a way to support quality related activities using data gathered during quality assurance processes, which involve humans.This paper proposes an approach, which can be used to support quality assessment of individual objects. The approach exploits techniques of Computational Intelligence that are treated as a consortium of granular computing, neural networks and evolutionary techniques. In particular, self-organizing maps and evolutionary-based developed decision trees are used to gain a better insight into the software data and to support a process of classification of software objects. Genetic classifiers—a novel algorithmic framework—serve as ‘filters’ for software objects. These classifiers are built on data representing subjective evaluation of software objects done by humans. Using these classifiers, a system manager can predict quality of software objects and identify low quality objects for review and possible revision. The approach is applied to analyze an object-oriented visualization-based software system for biomedical data analysis. 
45|7||A formal specification for a fuzzy expert system|A fuzzy logic toolkit has been developed for the formal specification language Z. It permits the incorporation of fuzzy concepts into the language while retaining the precision of any Z specification. The toolkit provides the necessary operators, measures and modifiers for the definition and manipulation of fuzzy sets and relations. This paper illustrates how the toolkit can be used to specify a simple fuzzy expert system. The focus is on the specification of the rule base and the operations necessary for fuzzy inferencing. In particular the example illustrates the use of the fuzzy cartesian product and fuzzy set truncation operators and offers a generic definition for a centroid defuzzification function. 
45|7||Complex object comparison in a fuzzy context|The comparison concept plays a determining role in many problems related to object management in an Object-Oriented Database Model. Object comparison is appropriately managed in a crisp object-oriented context by means of the concepts of identity and value equality. However, when dealing with imprecise or imperfect objects, questions like ‘To which extent may two objects be the same one?’ or ‘How similar are two objects?’ have not a clear answer, because the equality concept becomes fuzzy. In this paper we present a set of operators that are useful when comparing objects in a fuzzy environment. In particular, we introduce a generalized resemblance degree between two fuzzy sets of imprecise objects and a generalized resemblance degree to compare complex fuzzy objects within a given class. 
45|7||Modeling imprecise requirements with XML|Fuzzy theory is suitable to capture and analyze the informal requirements that are imprecise in nature, meanwhile, XML is emerging as one of the dominant data formats for data processing on the internet. In this paper, we have developed a fuzzy object-oriented modeling technique (FOOM) schema based on XML to model requirements specifications and incorporated the notion of stereotype to facilitate the modeling of imprecise requirements. FOOM schema is also transformed into a set of application programming interfaces (APIs) in an automatic manner. A schema graph is proposed to serve as an intermediate representation for the structure of FOOM schema to bridge the FOOM schema and APIs for both content validation and data access for the XML documents. 
45|8|http://www.sciencedirect.com/science/journal/09505849/45/8|Editorial Board|
45|8||Computer security and operating system updates|Application and operating system errors are a continuing source of problems in computer security. As businesses increase the number of servers through distributed computing and server farms, it becomes more difficult to keep the systems up to date. A survey of security professionals reveals that most find it difficult to keep up to date with security patches. Consequently, developing more automated management tools is an important step in improving enterprise security. 
45|8||A conceptual framework on the adoption of negotiation support systems|An exploratory study was conducted to identify factors affecting the intention to adopt negotiation support systems (NSS) by managers and executives. Drawing from past literature, the Theory of Planned Behavior and the Technology Acceptance Model provided basis for analyzing our results. Overall, subjective norm and perceived behavioral control emerged as strongest determinants of intention to adopt NSS. Further probing of subjective norm revealed organizational culture and industrial characteristics to play significant roles. A new conceptual framework is proposed which would be of both theoretical and practical importance. 
45|8||An affiliated search system for an electronic commerce and software component architecture|This paper describes an Electronic Commerce Goods Search System (ECGSS) that has functions that increase the precision of search results through training of the search system and uses affiliated business transaction processes. The software component architecture for ECGSS also allows the effective deployment of the system on every local business site, in view of the evolving trend in information technology toward easier configuration and re-usability. A general information gathering system with the infrastructure to accept every Internet communication protocol and access control is described. In this affiliated business transaction model, we classify Internet sites into two groups: a cooperative sites group, and a non-cooperative sites group. While designing the components, we optimized their specifications with respect to the whole architecture analysis, dependencies, and interface types using a component-based software development process. Experiments on the effectiveness of the user training function for the search system and the response time for simple queries for each communication protocol are presented. Comparisons of commercial search solutions and architectural standards of several organizations are also given. 
45|8||Computation of intraprocedural dynamic program slices|Dynamic slicing algorithms are used in interactive applications such as program debugging and testing. Therefore, these algorithms need to be very efficient. In this context, we propose three intraprocedural dynamic slicing algorithms which are more space and time efficient than the existing algorithms. Two of the proposed algorithms compute precise dynamic slices of structured programs using Program Dependence Graph as an intermediate representation. To compute precise dynamic slices of unstructured programs, we introduce the concepts of jump dependence and Unstructured Program Dependence Graph. The third algorithm uses Unstructured Program Dependence Graph as the intermediate program representation, and computes precise dynamic slices of unstructured programs. We show that each of our proposed algorithms is more space and time efficient than the existing algorithms. 
45|8||Beyond productivityâproductivity and the three types of efficiencies of information technology industries|The purpose of this paper is to analyze productivity and efficiency of information technology industries of fourteen OECD countries. IT productivity has been studied for years and most papers have confirmed the IT productivity. Some even proved that IT productivity is far better than other factors. The impact of IT to the economy depends not only on the productivity but also on the size of IT production. Thus, to analyze productivity and efficiency of IT industries is important. They are the drive for the economic growth in the new economy. Although productivity has been analyzed and discussed in the information systems field for years, little research has been done in efficiency of IT.This paper analyzes the productivity and productive efficiency of fourteen OECD countries and compared them to each other. The models we used in this paper follow Kumbhakar's 1989 models [Rev. Econ. Stat. (1989) 595] and some other econometric models in other productive efficiency studies. By using these models, we were allowed to estimate three different types of (in)efficiencies—technical, allocative and scale, and the percentage loss due to the inefficiency. 
45|8||Communication issues in requirements elicitation: a content analysis of stakeholder experiences|The gathering of stakeholder requirements comprises an early, but continuous and highly critical stage in system development. This phase in development is subject to a large degree of error, influenced by key factors rooted in communication problems. This pilot study builds upon an existing theory-based categorisation of these problems through presentation of a four-dimensional framework on communication. Its structure is validated through a content analysis of interview data, from which themes emerge, that can be assigned to the dimensional categories, highlighting any problematic areas. The paper concludes with a discussion on the utilisation of the framework for requirements elicitation exercises. 
45|8||Code and data spatial complexity: two important software understandability measures|In order to maintain the software, the programmers need to understand the source code. The understandability of the source code depends upon the psychological complexity of the software, and it requires cognitive abilities to understand the source code. The individual needs to correlate the orientation and location of various entities with their processing, which requires spatial abilities. This paper presents two measures of spatial complexity, which are based on two important aspects of the program—code as well as data. The measures have been applied to 15 different software projects and results have been used to draw many conclusions. The validation of the results has been done with help of perfective maintenance data. Lower values of code as well as data spatial complexity denote better understandability of the source code. 
45|9|http://www.sciencedirect.com/science/journal/09505849/45/9|Editorial Board|
45|9||Real-time disk scanning for timely retrieval of continuous media objects|In recent years, demands for multimedia applications have rapidly grown with the advances in computing power and network technology. To fulfill the demands, it is crucial to establish the multimedia information systems supporting continuous media (CM) streams without hiccups. Many useful methods have been proposed in the past so that CM steams with VCR-quality can be supported. Although these earlier approaches can be used for the service of VCR-quality streams, they cannot provide a good system performance for the CM streams with various playback rates and rather short durations of playback. In the paper, we propose a new CM playback method that consists of bulk-scanning and an earliest-deadline-first style scheduling algorithm. Our proposed method provides both good response times and high I/O utilization using the efficient disk scheduling. In addition, our mechanism for admission control makes CM streams never experience hiccups until the ends of playback. Through experimental analyses, we show the performance advantages of the proposed method over the earlier methods. 
45|9||Global intelligence benevolent builder (GIBB): a system automating integration of heterogeneous classical databases and web|In this paper we investigate highly sophisticated mechanisms that merge and automate interoperability of heterogeneity classical database systems together with the World Wide Web as one world. In particular, we introduce the global intelligence benevolent builder (GIBB) system that employs Intelligent Benevolent Architecture (IBA), which is comprised of assertions, integration rules, conceptual model constructs and agents. IBAs boost the components' versatility to reconcile the semantics involved in data sharing in order to withstand the dynamic computer technology in the present and future information age. Due to the IBAs power of intelligence, and in order to save costs and time, GIBB also has the capability to filter out and process only the relevant operational sources like preferences (i.e. customer's interest) from the sites. 
45|9||Further investigations of reading techniques for object-oriented design inspection|This paper describes an experimental evaluation of two reading techniques, namely Checklist-based reading (CBR) and Perspective-based reading (PBR) for Object-Oriented (OO) design document, written using the notation of Unified Modelling Language, inspection. Inspections are usually applied for defect detection in requirement documents or code modules, and there is a significant lack of information how inspections should be applied to OO design documents.The paper reports on a controlled experiment with 59 subject students to compare CBR and PBR techniques with respect to individual time spent on inspection, cost per defect, effectiveness, and 3-person simulated team effectiveness. 
45|9||Evolving a model of transaction management with embedded concurrency control for mobile database systems|Transactions within a mobile database management system face many restrictions. These cannot afford unlimited delays or participate in multiple retry attempts for execution. The proposed embedded concurrency control (ECC) techniques provide support on three counts, namely—to enhance concurrency, to overcome problems due to heterogeneity, and to allocate priority to transactions that originate from mobile hosts. These proposed ECC techniques can be used to enhance the server capabilities within a mobile database management system. Adoption of the techniques can be beneficial in general, and for other special cases of transaction management in distributed real-time database management systems. The proposed model can be applied to other similar problems related to synchronization, such as the generation of a backup copy of an operational database system. 
45|9||Software reuse through re-engineering the legacy systems|Software reuse is widely considered to be a way to increase the productivity and improve the quality and reliability of new software systems. Identifying, extracting and re-engineering software components that implement abstractions within existing systems is a promising cost-effective way to create reusable assets and re-engineer legacy systems. This paper summarizes our experiences with using computer-supported methods to develop a software architecture to support the re-engineering of the Janus Combat Simulation System. In this effort, we have developed an object-oriented architecture for the Janus Combat Simulation Subsystem, and validated the architecture with an executable prototype. In this paper, we propose methods to facilitate the reuse of the software components of the legacy systems by recovering the behavior of the systems using systematic methods, and illustrate their use in the context of the Janus System. 
45|9||Migration of active objects in proactive|ProActive, which supports active objects as the basic units of activity and distribution used for building applications using ProActive, is a Java library for parallel, distributed and concurrent computing. In this paper, we discuss an important distribution function—migration of objects—that is not readily available in many systems designed for implementing distributed applications. The migration is done in a scalable and secure way based-on ProActive. It has the ability to easily migrate computation, especially in heterogeneous environments. Moreover, it can be used for dynamic load balancing and dynamic deployment purposes to gain improved performance and flexibility of distributed applications. 
45|9||An architecture for security-oriented perfective maintenance of legacy software|This work presents an implementation strategy which exploits the separation of concerns and reuse in a multi-tier architecture to improve the security (availability, integrity, and confidentiality) level of an existing application. Functional properties are guaranteed via wrapping of the existing software modules. Security mechanisms are handled by the business logic of the middle-tier: availability and integrity are achieved via replication of the functional modules and the confidentiality is obtained via cryptography. The technique is presented with regard to a case study application. We believe that our experience can be used as a guideline for software practitioners to solve similar problems. We thus describe the conceptual model behind the architecture, discuss implementation issues, and present technical solutions. 
46|1|http://www.sciencedirect.com/science/journal/09505849/46/1|Editorial Board|
46|1||Publisher's Note|
46|1||Top-down and bottom-up expert estimation of software development effort|Expert estimation of software development effort may follow top-down or bottom-up strategies, i.e. the total effort estimate may be based on properties of the project as a whole and distributed over project activities (top-down) or calculated as the sum of the project activity estimates (bottom-up). The explorative study reported in this paper examines differences between these two strategies based on measurement and video recording of the discussions of seven estimation teams. Each estimation team applied a top-down estimation strategy on one project and a bottom-up estimation strategy on another. The main contribution of the study is the observation that the recall of very similar previously completed projects seemed to be a pre-condition for accurate top-down strategy based estimates, i.e. the abilities of the software estimators to transfer estimation experience from less similar projects was poor. This suggests that software companies should apply the bottom-up strategy unless the estimators have experience from, or access to, very similar projects. 
46|1||Implementing a community web site: a scenario-based methodology|The use of the Internet to facilitate commerce promises vast benefits. An important challenge in the age of Internet business is to fine-tune the Internet business system with customers. For this alignment, this paper proposes a scenario-based object-oriented methodology for developing a community web site. The methodology consists of five phases such as customer analysis, value analysis, web design, implementation design, and construction. Scenarios are used to analyze customers' needs in a natural fashion. A meta-data scheme is proposed for supporting our methodology. A real-life community web is illustrated to demonstrate the usefulness of the methodology. Scenario thinking can help companies use the Internet to buttress and extend their values. 
46|1||Evaluation of the comprehension of the dynamic modeling in UML|There is a certain degree of difficulty in developing and understanding the diagrams used for representing the dynamic behavior of a software application, specified in the Unified Modeling Language (UML). In this paper we evaluate the comprehension of the dynamic modeling in UML designs by using two split-plot factorial experiments. The metrics used for assessing the results are the time spent and the scores obtained in answering a questionnaire. In the first study three factors were controlled: the diagram type (sequence, collaboration and state), the application domain of the UML designs and the order of presentation of the documents. We observe that state diagrams provide higher semantic comprehension of dynamic modeling in UML when the domain is real-time and sequence diagrams are better in the case of a management information application. In the second study two factors were controlled: the paired combination of dynamic diagrams and the application domain. The main conclusion of the second study is that regardless of the domain, a higher semantic comprehension of the UML designs is achieved when the dynamic behavior is modeled by using the pair Sequence–State. Combining the results of both experiments we obtain the conditions which must concur to get an effective comprehension of the UML dynamic models: (a) if it is a management information application, the diagrams are sequence or the composition Sequence–State or Collaboration–State; (b) for a real-time non-reactive system the diagrams are collaboration or the pair Collaboration–State or Sequence–State; and (c) finally, when it is the design of the real-time reactive system, the best diagram is the State. 
46|1||Efficient data mining for web navigation patterns|The concept of preference is proposed on the analysis of the present algorithms for mining user navigation patterns. It is based on the following hypothesis: if there are many different selections to leave a page, those selections that occur more frequently and the next page is viewed longer reveal user interest and preference. Representing user navigation interest and intention accurately by comparing relatively access ratio and the average of relatively access ratio of viewing time and selective intention, preference can be used for mining user navigation pattern instead of confidence. The higher preference, the more prefer to choose the selection. According to the conception, we propose two efficient algorithms based on the concept, UAM and PNT, which are developed for mining user preferred navigation patterns. Considering the structure of Web site, UAM can get user access preferred path by the page–page transition probabilities statistics of all users behaviours. PNT looks far into the past to correctly discriminate the different behavioral modes of the different users. Experiments show accuracy and scalability of the algorithms. It is suitable for applications in E-business, such as to optimize Web site or to design personalized service. 
46|1||The productivity impact of information technology in the healthcare industry: an empirical study using a regression spline-based approach|This paper explores the productivity impact of information technology (IT) in the healthcare industry using a regression spline (RS)-based approach. Application of the RS-based approach offered additional valuable insights that contribute to our understanding of the complex relationship between investments in IT and organizational productivity. For example, the results of this study suggest that investments in the IT Stock has a positive impact on productivity only under certain conditions, and that this impact of IT is not uniform but is conditioned both by the amount invested in the IT Stock and the investments in Non-IT Capital. 
46|1||Erratum to âOn the interplay between consistency, completeness, and correctness in requirements evolutionâ [Information and Software Technology 45 (2003) 993â1009]|
46|1||Reviewers List|
46|10|http://www.sciencedirect.com/science/journal/09505849/46/10|Editorial Board|
46|10||A methodological framework for generic conceptualisation: problem-sensitivity in software engineering|The first step towards developing quality software is to conceptually model the problem raised in its own context. Software engineering, however, has traditionally focused on implementation concepts, and has paid little or no attention to the problem domain. This paper presents a generic methodological framework to guide conceptual modelling, focusing on the problem within its domain. This framework is defined considering aspects related to a generic conceptualisation, and its application to software engineering—illustrated using the IFIP Case—achieves the called-for problem-sensitivity. 
46|10||Cost effective management frameworks: the impact of IDS deployment technique on threat mitigation|In this paper we measure the financial benefit of an intrusion detection system (IDS) deployment. To this end, we use a standard risk analysis framework and extend it by introducing the Cascading Threat Multiplier (CTM). The idea behind the CTM is that a security compromise incurs two types of costs: (a) The direct cost of lost integrity/confidentiality/availability, and (b) the indirect cost, of the compromised component serving as a potential stepping stone for future attacks. The CTM tries to capture the second type of costs, which are typically ignored in the classic risk analysis framework. We propose new risk analysis formulas that tie the CTM concept into accurate calculation of Return on Investment (ROI), otherwise commonly known as Return on Security Investment. Finally, through a case study we demonstrate the effect of IDS deployment techniques on threat mitigation and the ROI. The result of the case can be used to support effective decision-making about which techniques are appropriate for the cost effective management of the IDS in a given environment. 
46|10||Defect detection oriented lifecycle modeling in complex product development|As the complexity of today's products increases, single projects, single departments or even single companies can no longer develop total products, resulting in concurrent and distributed development. To manage the resulting organizational complexity, projects need a lifecycle that explicitly reflects the concurrent and distributed nature of the project context. This paper addresses the essence of lifecycle modeling, with emphasis on defect detection. An adequately modeled lifecycle allows the localization and recognition of defect-sensitive areas in complex product development. A case study involving real-life development projects indicates that transitions between constituent sub-projects are in particular defect-sensitive. A second case study shows that by a defect detection-driven construction of a project-specific lifecycle, fewer residual defects can be expected. 
46|10||A comparison of cohesion metrics for object-oriented systems|Cohesion is the degree to which the elements of a class or object belong together. Many different object-oriented cohesion metrics have been developed; many of them are based on the notion of degree of similarity of methods. No consensus has yet arisen as to which of these metrics best measures cohesion; this is a problem for software developers since there are so many suggested metrics, it is difficult to make an informed choice. This research compares various cohesion metrics with ratings of two separate teams of experts over two software packages, to determine which of these metrics best match human-oriented views of cohesion. Additionally, the metrics are compared statistically, to determine which tend to measure the same kinds of cohesion. Differences in results for different object-oriented metrics tools are discussed. 
46|10||Measurement of object-oriented software spatial complexity|One of the important activities of the maintenance phase is to understand the source-code first, and then change it. Understandability of the software gets affected by psychological complexity of the source-code and cognitive abilities are needed to understand it. The correlation between the orientation and location of various entities with their processing needs to be established by the programmers, which requires spatial abilities. These spatial abilities play an important role in object-oriented software, in which the use of data as well as the methods of the class needs to be understood in a combined way. This paper presents two measures of spatial complexity of object-oriented software, which are based on definition and usage of classes and objects. The values of proposed measures get affected suitably because of inheritance and polymorphism as well, due to change in the distances. The significance of object-oriented spatial complexity has been demonstrated with the help of 15 object-oriented projects of varied length and results have been validated with the help of reverse engineering data and perfective maintenance data. 
46|11|http://www.sciencedirect.com/science/journal/09505849/46/11|Editorial Board|
46|11||Expressing and organising business rules|Business rules represent projections of organisations' constraints and ways of working on their supporting information systems. Therefore, their collection, expression, structuring and organisation should be central activities within information systems analysis. The work presented in this paper concerns the definition of a repository schema for managing business rules, taking into account the objectives (a) to facilitate the rule collection process, (b) to assist the transition from analysis to design and implementation of the information system, and (c) to support business change once the new system has been delivered. These objectives are achieved through the enhancement of the rule repository schema with information on the logistics of the collection process and references to underlying enterprise informational and behavioural knowledge models. The proposed schema and way of working are demonstrated through a number of examples, which are derived from an industrial project concerning electronic procurement in the construction sector. 
46|11||A controller synthesis algorithm for building self-adaptive software|A novel approach for building self-adaptive software based on a controller synthesis algorithm is presented. Self-adaptive software is a relatively new idea aiming at producing applications that can readily adapt in the face of changing user needs, desires and environment. Self-adaptive software has multiple ways of accomplishing its purpose, enough knowledge of its construction and is capable of changing behaviour when it does not accomplish its goal or when better functionality or performance is possible. The presented approach for building self-adaptive software uses ontological models of software components, which represent the environment, the composition, the required behaviour, and the possible configurations for the self-adaptive software. Self-adaptation is based on a supervisory control algorithm that reconfigures and controls software components in order to achieve their required behaviour. 
46|11||Dynamic adaptation to object state change in an information flow control model|Dynamic adaptation to object state change is necessary for an information flow control model because object state may change in an unpredictable manner during runtime. To accomplish the adaptation, an information flow control model should offer the following features: (a) access rights can be dynamically changed according to object state change, (b) purpose-oriented method invocation can be achieved, and (c) information flows among variables can be controlled. According to our survey, no existing model offers the adaptation well. This paper proposes a solution to the adaptation, which is offered by the information flow control model object-oriented role-based access control. 
46|11||Multi-way R-tree joins using indirect predicates|Since spatial join processing consumes much time, several algorithms have been proposed to improve spatial join performance. Spatial join has been processed in two steps, called filter step and refinement step. The M-way R-tree join (MRJ) is a filter step join algorithm, which synchronously traverses M R-trees. In this paper, we introduce indirect predicates which do not directly come from the multi-way join conditions but are indirectly derived from them. By applying indirect predicates as well as direct predicates to MRJ, we can quickly remove the minimum bounding rectangle (MBR) combinations which do not satisfy the direct predicates or the indirect predicates at the parent level. Hence we can reduce the intermediate MBR combinations for the input to the child level processing and improve the performance of MRJ. We call such a multi-way R-tree join algorithm using indirect predicates indirect predicate filtering (IPF). Through experiments using synthetic data and real data, we show that IPF significantly improves the performance of MRJ. 
46|11||The space efficiency of XML|XML is the future language for data exchange, and support for XML has been extensive. Although XML has numerous benefits including self-describing data, improved readability, and standardization, there are always tradeoffs in the introduction of new technologies that replace existing systems. The tradeoff of XML versus other data exchange languages is improved readability and descriptiveness versus space efficiency. There has been limited work on examining the space efficiency of XML. This paper compares XML to other data exchange formats. Experiments are performed to measure the overhead in XML files and determine the amount of space used for data, schema, and overhead in a typical XML document. 
46|11||Erratum to âOn the interplay between consistency, completeness, and correctness in requirements evolutionâ [Information and Software Technology 45 (2003) 993â1009]|
46|11||Erratum to âOn the interplay between consistency, completeness, and correctness in requirements evolutionâ|The initial expression of requirements for a computer-based system is often informal and possibly vague. Requirements engineers need to examine this often incomplete and inconsistent brief expression of needs. Based on the available knowledge and expertise, assumptions are made and conclusions are deduced to transform this ‘rough sketch’ into more complete, consistent, and hence correct requirements. This paper addresses the question of how to characterize these properties in an evolutionary framework, and what relationships link these properties to a customer's view of correctness. Moreover, we describe in rigorous terms the different kinds of validation checks that must be performed on different parts of a requirements specification in order to ensure that errors (i.e. cases of inconsistency and incompleteness) are detected and marked as such, leading to better quality requirements. 
46|12|http://www.sciencedirect.com/science/journal/09505849/46/12|Editorial Board|
46|12||Prediction of software failures through logistic regression|The quality of software has been a main concern since the inception of computer software. To be able to produce high quality software, software developers and software testers alike need continuous improvements in their developing and testing methodologies. These improvements should result in better coverage of the input domain, efficient test cases, and in spending fewer testing resources. In this paper we focus on an approach for generating efficient test cases based on the special properties of Design of Experiments and developing a logistic regression model of predicting test case outcomes. Design of Experiments will be utilized to efficiently minimize the number of test cases and the logistic regression model will be used to predict software failures. This approach, in turn, would provide the software tester with a model that reduces the number of test cases, predicts test case outcomes, reduces cost, and allows better forecast of release readiness. We demonstrate our approach using two case studies (TI Interactive software and Microsoft's Pocket PC operating system). 
46|12||A formal framework for analyzing reusability complexity in component-based systems|In this paper, we present a methodology to estimate the impact of modifying a given software system design. In addition, we will be able to evaluate its reusability as well as the coupling of its components. In order to do that, the designer defines the system in terms of its components, their dependencies, the properties they fulfill, and the properties each component requires to other components. Besides, some auxiliary functions are used to define the relations between properties and the cost associated with their modification. Putting together all this information the different ways to perform a modification can be systematically generated and studied. We have applied our methodology to a medium-size system. Specifically, we dealt with an on-line intelligent tutoring system allowing users to learn the programming language Haskell. 
46|12||An industrial study on building consensus around software architectures and quality attributes|When creating an architecture for a software system it is important to consider many aspects and different sides of these aspects at an early stage, lest they are misunderstood and cause problems at later stages during development. In this paper, we report from an industry study to understand and select between different architecture candidates. The company uses a method that focuses discussions of architecture candidates to where there are disagreements between the participating domain experts. The results indicate that the used method pinpoints for the company where further investigations are necessary and that the decision concerning which architecture to use is taken with more confidence as a result of the focused discussions. 
46|12||A role-driven component-oriented methodology for developing collaborative commerce systems|As the commerce environment becomes more competitive, companies are compelled to adopt a collaborative commerce (c-commerce) paradigm to sustain a competitive edge over the Internet. C-commerce demands a variety of collaborative interactions among multiple stakeholders. In order to develop c-commerce systems, this paper proposes a role-driven component-oriented methodology (RCOM), which consists of four phases: collaboration analysis, component analysis, component design, and implementation. The roles can help implement well-defined business functions, while the components enable the production of reusable artifacts in a systematic fashion. Accordingly, RCOM is likely to help improve the system development process. To demonstrate the practical usefulness of this methodology, a real-life case is illustrated. 
46|12||Normal forms for XML documents|This paper studies the normalization problem of XML documents with DTDs as their schemas. XML documents may contain redundant information due to a bad designed DTD which implies the similar anomaly dependencies among elements and attributes just as in relational database schema. The concepts of partial functional dependency and transitive functional dependency for XML DTD are proposed. And three XML normal forms: 1XNF, 2XNF and 3XNF, are defined based on the concepts of partial and transitive functional dependency. The concept of lossless join decomposition for DTD is defined by the relational representation of DTD. Finally, two lossless join decomposition algorithms are given for transforming a DTD into 2XNF and 3XNF, respectively. 
46|12||Independent examination of software: an experiment|This paper summarises our experience in using model-checking technology to understand the behaviour of a system. A simple model of the system under test is created from the informal documentation. This model is used to create a test environment for the system. The behaviour of system combined with the test environment is then verified using the tool Verisoft. This requires a few changes to the original source code. The aim to minimize the number of changes to the original source code so that its original behaviour is not affected. The use of Verisoft to create a model, which can subsequently be used for testing is also studied. The main conclusion is that it is possible to use bounded model checking on source code with a view towards verifying key behavioural properties. 
46|13|http://www.sciencedirect.com/science/journal/09505849/46/13|INSIDE FRONT COVER|
46|13||Automated rapid prototyping of TUG specifications using Prolog|A prototype is usually built quickly and cheaply to explore poorly understood user requirements in the front-end of the software development life cycle. There are several techniques to construct a prototype such as fourth generation languages, functional and logic programming languages, and simulation techniques. Despite their benefits, these techniques do not directly support formal user requirements and specifications. In this paper, a formal specification language, called TUG, is presented to support an automatic derivation of a prototype in Prolog from a specification in the language via a set of transformation rules. The work described in this paper is distinct from existing rapid prototyping techniques. There is a close correspondence between TUG and Prolog that makes the process of transformation relatively mechanical. The process also allows specifiers not to consider low-level details at implementation such as selection of data structures and algorithms due to the features of the TUG specification language and the Prolog programming language. In addition, rederivation of a prototype in Prolog from a TUG specification is also avoided whenever the specification is modified. A Change Request Script is used to update the prototype in response to the revised specification. The prototype construction and specification acquisition are integrated to handle the construction of user requirements. The formality of the TUG specification language improves the quality of the description of user requirements. Rapid prototyping from the specification via software transformations improves the understanding of user requirements in a cost effective way. 
46|13||Assessing defect detection performance of interacting teams in object-oriented design inspection|Software inspection is one of the methods to ensure the quality of software by finding and repairing defect early in software development process. In a software inspection process, inspectors first review software artifacts individually and then meet in a team in order to find as many defects as possible and to eliminate false positives. However, several empirical studies suggest that inspection meeting may not be necessary since an insignificant number of new defects are found as a result of the meeting.In this paper we report on a controlled experiment with 54 undergraduate students who inspection Object-Oriented design documents. Firstly, we compare the performance of inspection teams using two reading techniques: Checklist-Based Reading (CBR) and Perspective-Based Reading (PBR). Secondly, we compare the performance of interacting and nominal teams. The results of comparison between CBR and PBR do not reveal a significant difference between these techniques. Meanwhile, the results of comparison between interacting and nominal teams show that interacting teams do not detect a significant number of new defects during a meeting, and they are less effective than nominal teams; however interacting teams reported fewer false positives as compared to nominal teams. 
46|13||On the identification of categories and choices for specification-based test case generation|The category-partition method and the classification-tree method help construct test cases from specifications. In both methods, an early step is to identify a set of categories (or classifications) and choices (or classes). This is often performed in an ad hoc manner due to the absence of systematic techniques. In this paper, we report and discuss three empirical studies to investigate the common mistakes made by software testers in such an ad hoc approach. The empirical studies serve three purposes: (a) to make the knowledge of common mistakes known to other testers so that they can avoid repeating the same mistakes, (b) to facilitate researchers and practitioners develop systematic identification techniques, and (c) to provide a means of measuring the effectiveness of newly developed identification techniques. Based on the results of our studies, we also formulate a checklist to help testers detect such mistakes. 
46|13||An empirical study of the effect of knowledge integration on software development performance|Although the role of integrating application domain knowledge with technical knowledge is implicitly recognized in software engineering practice, no large scale study has attempted to validate this empirically in a field setting. In this paper, a large-scale empirical study of 232 software development projects in 232 software development organizations shows that higher integration of business application domain knowledge with technical knowledge during the software development process increases software development effectiveness, reduces defect density throughout the development trajectory, lowers warranty defects, and increases software development efficiency. The findings highlight the influence of knowledge integration on various dimensions of software development performance. 
46|13||A statecharts-based software development process for mobile agents|Although mobile agents and their supporting infrastructures have been extensively developed, it is still an emerging technology. A wider acceptance of mobile agents would be facilitated with the exploitation of suitable methodologies and tools which fully support their development lifecycle. This paper proposes a Statecharts-based development process for mobile agents, which allows for a seamless transition from the specification of mobile agent behaviour to its implementation and adaptation to target mobile agent systems. In particular, modelling of the mobile agent behaviour is visual and its coding is seamlessly supported by the Mobile Active Object Framework. The coded agent behaviour can be adapted to platform-specific mobile agents by means of the Mobile Agent Adaptation Framework thus enabling re-use of existing mobile agent systems. 
46|14|http://www.sciencedirect.com/science/journal/09505849/46/14|INSIDE FRONT COVER|
46|14||Scalable distributed compact trie hashing (CTH*)|This last decade, a new class of data structures named Scalable Distributed Data Structures (SDDSs), is appeared completely dedicated to a distributed environment. This type of data structures opened an important axis of research, considering that the data management in a transparent manner is fundamental in a computer network. All the existing methods are mainly based on Linear hashing (LH*) and Range-partitioning (RP*). In this paper, we propose a new method with the constraints of the SDDS. Our approach is an adaptation of the well-known method Trie hashing (TH) for a distributed environment, i.e. a network of interconnected computers. The latter uses a digital tree (trie) as access function. Our major objective is the distribution of file buckets and the tree representing the hashing function. We have considered TH with the tree represented in compact form (CTH) because this option is probably more interesting for the reduction of the message size circulating on the network. Contrary to the majority of the existing methods, the proposed one provides the order of distributed files, then facilitates both the range query operations and the ordered traversal of files. Moreover, the following properties make our method a promising opening towards a new class of SDDS: (a) preservation of the order of records, (b) works without multicast (c) three bytes are sufficient to address a server, (d) the transfer of some bytes is enough for the update of the client trees. The access performances should exceed the ones of traditional files and some competitive scalable and distributed data structures. 
46|14||Implementing requirements engineering processes throughout organizations: success factors and challenges|This paper aims at identifying critical factors affecting organization-wide implementation of requirements engineering (RE) processes. The paper is based on a broad literature review and three longitudinal case studies that were carried out using an action research method. The results indicate that RE process implementation is a demanding undertaking, and its success greatly depends on such human factors as motivation, commitment and enthusiasm. Therefore, it is essential that the RE process is useful for its individual users. Furthermore, the results indicate that organizations can gain benefits from RE by defining a simple RE process, by focusing on a small set of RE practices, and by supporting the systematic usage of these practices. 
46|14||Experimental comparison of the comprehensibility of a Z specification and its implementation in Java|Comprehensibility is often raised as a problem with formal notations, yet formal methods practitioners dispute this. In a survey, one interviewee said ‘formal specifications are no more difficult to understand than code’. Measurement of comprehension is necessarily comparative and a useful comparison for a specification is against its implementation. Practitioners have an intuitive feel for the comprehension of code. A quantified comparison will transfer this feeling to formal specifications. We performed an experiment to compare the comprehension of a Z specification with that of its implementation in Java. The results indicate there is little difference in comprehensibility between the two. 
46|14||Adaptive development and maintenance of user-centric software systems|A software system cannot be developed without considering the various facets of its environment. Stakeholders—including the users that play a central role—have their needs, expectations, and perceptions of a system. Organisational and technical aspects of the environment are constantly changing. The ability to adapt a software system and its requirements to its environment throughout its full lifecycle is of paramount importance in a constantly changing environment. The continuous involvement of users is as important as the constant evaluation of the system and the observation of evolving environments. We present a methodology for adaptive software systems development and maintenance. We draw upon a diverse range of accepted methods including participatory design, software architecture, and evolutionary design. Our focus is on user-centred software systems. 
46|15|http://www.sciencedirect.com/science/journal/09505849/46/15|INSIDE FRONT COVER|
46|15||Editorial|
46|15||Optimal and adaptive testing for software reliability assessment|Optimal software testing is concerned with how to test software such that the underlying testing goal is achieved in an optimal manner. Our previous work shows that the optimal testing problem for software reliability growth can be treated as closed-loop or feedback control problem, where the software under test serves as a controlled object and the software testing strategy serves as the corresponding controller. More specifically, the software under test is modeled as controlled Markov chains (CMCs) and the control theory of Markov chains is used to synthesize the required optimal testing strategy. In this paper, we show that software reliability assessment can be treated as a feedback control problem and the CMC approach is also applicable to dealing with the optimal testing problem for software reliability assessment. In this problem, the code of the software under test is frozen and the software testing process is optimized in the sense that the variance of the software reliability estimator is minimized. An adaptive software testing strategy is proposed that uses the testing data collected on-line to estimate the required parameters and selects next test cases. Simulation results show that the proposed adaptive software testing strategy can really work in the sense that the resulting variance of the software reliability estimate is much smaller than that resulting from the random testing strategies. The work presented in this paper is a contribution to the new area of software cybernetics that explores the interplay between software and control. 
46|15||Mirror adaptive random testing|Recently, adaptive random testing (ART) has been introduced to improve the fault-detection effectiveness of random testing for non-point types of failure patterns. However, ART requires additional computations to ensure an even spread of test cases, which may render ART less cost-effective than random testing. This paper presents a new technique, namely mirror ART, to reduce these computations. It is an integration of the technique of mirroring and ART. Our simulation results clearly show that mirror ART does improve the cost-effectiveness of ART. 
46|15||Verifying Haskell programs by combining testing, model checking and interactive theorem proving|We propose a program verification method that combines random testing, model checking and interactive theorem proving. Testing and model checking are used for debugging programs and specifications before a costly interactive proof attempt. During proof development, testing and model checking quickly eliminate false conjectures and generate counterexamples which help to correct them. With an interactive theorem prover we also ensure the correctness of the reduction of a top level problem to subproblems that can be tested or proved. We demonstrate the method using our random testing tool and binary decision diagrams-based (BDDs) tautology checker, which are added to the Agda/Alfa interactive proof assistant for dependent type theory. In particular we apply our techniques to the verification of Haskell programs. The first example verifies the BDD checker itself by testing its components. The second uses the tautology checker to verify bitonic sort together with a proof that the reduction of the problem to the checked form is correct. 
46|15||Instrumenting scenarios in a model-driven development environment|SpecExplorer is an integrated environment for model-driven development of .NET software. In this paper we discuss how scenarios can be described in SpecExplorer's modeling language, Spec#, and how the SpecExplorer tool can be used to validate those scenarios by various means. 
46|15||The software evaluation framework âSEFâ extended|The primary objective of this paper was to present a study on the metrics which can be applied to the Software Evaluation Framework. The paper presents the results of a preliminary study, which focuses on the measurements applied to the framework. It provides a description of the metrics used and an analysis of how they compare to each other in the measurement of the software characteristics. This objective was accomplished by empirically testing the model with quantitative techniques. An earlier qualitative study provided a list of metrics, which were then tested with the quantitative study described in this paper. The results of this study are important as it identifies the metrics, perceived by stakeholders as essential for applying the Software Evaluation Framework to software evaluation. 
46|15||Call for papers|
46|15||Author Index|
46|15||Keyword Index|
46|15||Volume Contents|
46|2|http://www.sciencedirect.com/science/journal/09505849/46/2|Editorial Board|
46|2||An aspect-oriented framework for developing component-based software with the collaboration-based architectural style|Component-based development (CBD) technique for software has emerged to fulfill the demand on the reuse of existing artifacts. In comparison to traditional object-oriented techniques, CBD can provide more advanced abstraction concepts such as subsystem-level reusability, gross structure abstraction, and global control flow abstraction. Unfortunately, existing software development techniques are not mature enough to make it come true that components developed in the third party can be used in a highly flexible way. It is notable that there are certain kinds of software requirements, such as non-functional requirements, that must be implemented cross-cutting multiple classes, largely losing the modularity in object-oriented design and implementation code. Therefore, it is not easy that components are reused without consideration of their low-level implementation details.In this article, we propose Aspect-Oriented Development Framework (AODF) in which functional behaviors are encapsulated in each component and connector, and particular non-functional requirements are flexibly tuned separately in the course of software composition. To support the modularity for non-functional requirements in component-based software systems, we devise Aspectual Composition Rules (ACR) and Aspectual Collaborative Composition Rule (ACCR). Note that AODF makes component-based software built to provide both supports of modularity and manageability of non-functional requirements such as synchronization, performance, physical distribution, fault tolerance, atomic transaction, and so on. With the Collaboration-Based architectural style, AODF explicitly enables to deal with non-functional requirements at the intra-component and inter-component levels. 
46|2||Factors related to the difficulty of learning to program in Javaâan empirical study of non-novice programmers|Due to its relative newness and popularity, Java is being taught to numerous non-novice programmers both in industry and in academia. Claims have been made that certain background characteristics of programmers relate to ease in learning Java. In this study, background information of 135 non-novice programmers was obtained, together with data relating to their difficulty of learning several different features of Java. Results of this study could be used by software project managers contemplating the use of Java, and by academicians involved in curricular planning. 
46|2||Automated elicitation of functional dependencies from source codes of database transactions|Enforcement of data dependency is a major component in any database application. Functional dependency (FD) is an important data dependency. Due to the nature of software development and maintenance and the limitation of database management system (DBMS) in handling FD enforcement, many functional dependencies are enforced in the transactions that update the database, but not through DBMS. It is very difficult and time consuming to elicit functional dependencies from transactions manually for the reengineering, verification and maintenance of database applications. With the use of program analysis, this paper proposes a novel approach for the automated elicitation of functional dependencies from source codes of transactions that update the database. The approach is based on detecting program path patterns for enforcing FD. 
46|2||On handling time-varying data in the relational data model|The article addresses the key issues in modeling and querying temporal data within the relational framework. These issues include representation of temporal data, temporal grouping (object) identifiers and primary keys of temporal relations, temporal integrity constraints, and fundamental operations of a temporal query language. The paper develops taxonomy for temporal relational databases and establishes criteria to evaluate various proposed extensions to the relational data model. We expect that it will benefit the researchers and guide the practitioners in choosing the right approach for managing temporal data in their applications. 
46|2||Evaluating the learning effectiveness of using simulations in software project management education: results from a twice replicated experiment|The increasing demand for software project managers in industry requires strategies for the development of management-related knowledge and skills of the current and future software workforce. Although several educational approaches help to develop the necessary skills in a university setting, few empirical studies are currently available to characterise and compare their effects.This paper presents the results of a twice replicated experiment that evaluates the learning effectiveness of using a process simulation model for educating computer science students in software project management. While the experimental group applied a System Dynamics simulation model, the control group used the well-known COCOMO model as a predictive tool for project planning.The results of each empirical study indicate that students using the simulation model gain a better understanding about typical behaviour patterns of software development projects. The combination of the results from the initial experiment and the two replications with meta-analysis techniques corroborates this finding. Additional analysis shows that the observed effect can mainly be attributed to the use of the simulation model in combination with a web-based role-play scenario. This finding is strongly supported by information gathered from the debriefing questionnaires of subjects in the experimental group. They consistently rated the simulation-based role-play scenario as a very useful approach for learning about issues in software project management. 
46|3|http://www.sciencedirect.com/science/journal/09505849/46/3|Editorial Board|
46|3||A distributed and interoperable object-oriented support for safe e-commerce transactions|Business via Internet is becoming popular. A number of organizations doing business in the traditional way are extending themselves to do business over the Web. Most business-to-business dealings are done through Value Added Networks but for general consumer-to-business dealings, the Internet provides a powerful base. However, customer confidence in Internet commerce needs to be further strengthened before large scale Internet purchasing and selling becomes a reality. While security standards have been established through efficient cryptographic techniques to ensure that network communications are not intercepted, the lack of trust between the endpoint parties involved in an e-commerce transaction remains an obstacle for a smooth completion of the transaction. This is because transacting parties need to trust each other before making any commitment. In this paper, we show how trust can be provided automatically through a network of Trust Service Providers (TSP). The solution we propose allows both customers and merchants to deal with each other confidently through trusted intermediaries which role is to guarantee the goods delivery to the customer and the payment to the merchant for issuing those goods. The transactions are conducted atomically and transparently. That is, a transaction process does not experience any off-line transition delay at the intermediaries and the transacting parties deal virtually directly with each other. This solution requires building a trust web based on a network of TSPs, which we implemented in the form of distributed CORBA objects. 
46|3||The impact of cognitive complexity on project leadership performance|In today's complex development environments, the ability to integrate project components has been found to be a key responsibility for project leaders. This paper reports results of a preliminary experiment examining the importance of cognitive differentiation and integration (i.e. cognitive complexity) to project leadership performance. Results from this preliminary study show the importance of cognitive complexity to success in project leadership. Surprisingly, results suggest that leaders with lower levels of cognitive differentiation abilities (less information fragmentation) perform better on project definition tasks. Results also suggest that higher levels of cognitive integrative ability are associated with higher performance in project definition tasks. To this end, we suggest ways to improve complex thinking in project leaders. This study is important to IS managers responsible for identifying and training project leaders, and researchers seeking to understand factors important to successful project leadership. 
46|3||Virtual visits to cultural heritage supported by web-agents|The integration between information technologies and cultural heritage can impact on everyday life, both from the point of view of institutions and of users. The cultural heritage community have recently been attracted by the chance offered by information technology, and, in particular, by the possibility of making cultural information available to a wide range of people. Museums, exhibitions and cultural institutions can now supply new services to access cultural information, and this calls for suitable infrastructures and tools. In such a context, this paper proposes a Web-based application that enables virtual visits to access cultural information tailored on the basis of user profiles and devices. The application is closely integrated within the Web; it also permits one group to build up virtual visits that can be attended by different people interested in the same subject. The application is based on an infrastructure exploiting innovative technologies such as active proxy servers and mobile agents; it grants a high degree of flexibility and is particularly suitable for an improved promulgation of cultural information. 
46|3||Self-assessment of performance in software inspection processes|Software inspection is accepted as an effective technique in the battle against defects. However, many organizations fail to maximize these benefits because, for example, the inspection team has difficulty setting exit or re-inspection criteria. This should ideally be based upon an estimate of the number of remaining defects in the source document. It is proposed that a subjective estimate of these remaining defects from individual inspectors can be an effective method for estimating the number of remaining defects, and hence it can be utilized as a mechanism to implement the exit or re-inspection decision. 
46|3||A metadata approach to multimedia database federations|Recent research in federated database systems has advanced in the direction of federations of multimedia databases. However, in each project studied, there has been little emphasis placed on the subject of metamodels. Conversely, standard database systems (relational, object-relational and object-oriented) define metadata models that lack multimedia and federation features. In this paper, a specification of a metadata architecture for a multimedia database federation is presented. The architecture is based on a new object-oriented metamodel that has support for multimedia types and federated metadata. By using this metamodel, administrators and users of a multimedia federation can determine precisely how data is structured and the types of operations that are possible. 
46|4|http://www.sciencedirect.com/science/journal/09505849/46/4|Editorial Board|
46|4||Object-oriented conceptual modeling for commitment-based collaboration management in virtual enterprises|For the rapid progress of internet technologies in recent years, Electronic Commerce (EC) has gained attention as a major theme for enterprises to keep their competitiveness. From the perspective of effective resources utilization, it becomes now an important goal for an enterprise to promote its performance and competitiveness through integrating itself and relevant suppliers and consumers as a virtual group to achieve the so-called Business-to-Business EC. In this paper, we propose an object-oriented modeling approach that addresses the management of collaboration on the Internet between enterprises. The approach divides those required mechanisms for collaboration management into three layers: commitment, role, and activity ones. With this architecture, two enterprises may collaborate via the establishment and maintenance of commitment, the collaboration and coordination between roles, and the interaction and coordination between activities. For specification, an object-oriented model is presented for each layer that describes the working details of that layer. To illustrate, these models are applied in a typical manufacturing supply chain application among various enterprises. 
46|4||M-UML: an extension to UML for the modeling of mobile agent-based software systems|The Unified Modeling Language (UML) is a language for the specification, visualization, and documentation of object-oriented software systems [The Unified Modeling Language User Guide, 1998]. However, UML cannot describe in an explicit manner the mobility requirements needed for modeling mobile agent-based software systems. In this paper, we present M-UML, our proposed extension to UML covering all aspects of mobility at the various views and diagrams of UML. The use of M-UML is illustrated using a simple mobile voting system example. 
46|4||An adaptive indexing technique using spatio-temporal query workloads|Many spatio-temporal access methods, such as the HR-tree, the 3DR-tree, and the MV3R-tree, have been proposed for timestamp and interval queries. However, these access methods have the following problems: the poor performance of the 3DR-tree for timestamp queries, the huge size and the poor performance of the HR-tree for interval queries, and the large size and the high update cost of the MV3R-tree. We address these problems by proposing an adaptive partitioning technique called the Adaptive Partitioned R-tree (APR-tree) using workloads with timestamp and interval queries. The APR-tree adaptively partitions the time domain using query workloads. Since the time domain of the APR-tree is automatically fitted to query workloads, the APR-tree outperforms the other access methods for various query workloads. The size of the APR-tree is on the average 1.3 times larger than that of the 3DR-tree which has the smallest size. The update cost of the APR-tree is on the average similar to that of the 3DR-tree which has the smallest update cost. 
46|4||Software release planning: an evolutionary and iterative approach|To achieve higher flexibility and to better satisfy actual customer requirements, there is an increasing tendency to develop and deliver software in an incremental fashion. In adopting this process, requirements are delivered in releases and so a decision has to be made on which requirements should be delivered in which release. Three main considerations that need to be taken account of are the technical precedences inherent in the requirements, the typically conflicting priorities as determined by the representative stakeholders, as well as the balance between required and available effort. The technical precedence constraints relate to situations where one requirement cannot be implemented until another is completed or where one requirement is implemented in the same increment as another one. Stakeholder preferences may be based on the perceived value or urgency of delivered requirements to the different stakeholders involved. The technical priorities and individual stakeholder priorities may be in conflict and difficult to reconcile. This paper provides (i) a method for optimally allocating requirements to increments; (ii) a means of assessing and optimizing the degree to which the ordering conflicts with stakeholder priorities within technical precedence constraints; (iii) a means of balancing required and available resources for all increments; and (iv) an overall method called EVOLVE aimed at the continuous planning of incremental software development. The optimization method used is iterative and essentially based on a genetic algorithm. A set of the most promising candidate solutions is generated to support the final decision. The paper evaluates the proposed approach using a sample project. 
46|4||FINDIT: a fast and intelligent subspace clustering algorithm using dimension voting|The aim of this paper is to present a novel subspace clustering method named FINDIT. Clustering is the process of finding interesting patterns residing in the dataset by grouping similar data objects from dissimilar ones based on their dimensional values. Subspace clustering is a new area of clustering which achieves the clustering goal in high dimension by allowing clusters to be formed with their own correlated dimensions.In subspace clustering, selecting correct dimensions is very important because the distance between points is easily changed according to the selected dimensions. However, to select dimensions correctly is difficult, because data grouping and dimension selecting should be performed simultaneously. FINDIT determines the correlated dimensions for each cluster based on two key ideas: dimension-oriented distance measure which fully utilizes dimensional difference information, and dimension voting policy which determines important dimensions in a probabilistic way based on V nearest neighbors' information. Through various experiments on synthetic data, FINDIT is shown to be very successful in the high dimensional clustering problem. FINDIT satisfies most requirements for good clustering methods such as accuracy of results, robustness to the noise and the cluster density, and scalability to the dataset size and the dimensionality. Moreover, it is gracefully scalable to full dimension without any modification to algorithm. 
46|4||The design of an anonymous file sharing system based on group anonymity|There are a variety of contexts in which network users have legitimate reasons to want to exchange information, e.g. documents and media files, while maintaining anonymity. One such context is an online user group with a special interest in a particular medical condition that is associated with unfair or illegal discrimination. The challenge for designing a secure and anonymous file sharing system is that it seems necessary for some external source to know the network destination of the requested information. This is problematic because the identity of the recipient can potentially be determined from a network address. Even if the source or conduit of the information is trusted, it may be compromised by an outside agent who cannot be trusted. In this paper we use a general mechanism called ‘group anonymity’ that provides a new level of network privacy and security. We also describe a particular use of this mechanism in a software system which is an anonymous file sharing application that preserves the anonymity of users using the system and the privacy of the data being exchanged. 
46|5|http://www.sciencedirect.com/science/journal/09505849/46/5|Editorial board|
46|5||Special Issue on Software Engineering: Applications, Practices and Tools from the ACM Symposium on Applied Computing 2003|
46|5||An integrated framework for formal development of open distributed systems|This paper contributes to the discussion on issues related to the formal development of open distributed systems (ODSs). Deficiencies of traditional formal notations in this setting are highlighted. We argue that there is no single formalism exhibiting all the features required to capture properties of ODSs. As a solution, we propose an integrated development framework that involves two notations: the Unified Modeling Language and the Prototype Verification System. We discuss the motivation for the choice of these notations, provide an overview of a CASE tool we have developed to support the proposed framework, and present a case study to demonstrate usability of our approach. 
46|5||On formalizing UML state machines using ASMs|We present a transparent yet rigorous conceptual framework for defining the semantics of dynamic UML diagrams. We illustrate the method for UML state machines, making the “semantic variation points” of UML explicit, as well as various ambiguities and omissions in the official UML documents. This includes the event deferring and completion mechanism, the meaning of atomic and durative actions, concurrent internal activities and conflict situations which may arise through the concurrent behavior of active objects. 
46|5||Adding pattern related information in structural and behavioral diagrams|Design patterns capture the distilled experience of expert designers. The compositions of design patterns may reuse design experience and solve a set of problems. Design patterns and their compositions are usually modeled using unified modeling language (UML). When a design pattern is applied or composed with other patterns, the pattern-related information may be lost because UML does not track this information. Thus, it is hard for a designer to identify a design pattern when it is applied or composed. The benefits of design patterns are compromised because the designers cannot communicate with each other in terms of the design patterns they use when the design patterns are applied or composed. In this paper, we present notations to explicitly represent the structural and behavioral aspects of each pattern in the applications and compositions of design patterns. The notations allow us to maintain pattern-related information in class and collaboration diagrams. Thus, a design pattern is identifiable and traceable from its application and composition with others in these diagrams. A case study is used to illustrate our approach. 
46|5||On the composition of Java frameworks control-flows|Object oriented programming languages provide, in principle, mechanisms to enhance code reuse. As an effort of designing object oriented software, design patterns and frameworks are recognised as good techniques for reuse. Frameworks are of particular interest as design and code reuse are achieved. Despite of that, most frameworks were designed to be adapted to applications and not to be composed with other frameworks. As a result, problems such as control-flows composition, legacy components composition, frameworks gap, entities overlap and composition of frameworks behaviour arise. The present work is a study on the composition of Java frameworks control-flows, where a third framework is created from two existing ones. With this study, we have checked the potential problems that may appear as two control-flows are composed via message-passing. 
46|5||A method for the automatic generation of test suites from object models|This paper shows how object-oriented specifications, written in the Unified Modeling Language (UML) can be translated into formal, behavioural descriptions and used as a basis for automatic test generation. The behavioural descriptions are written in a language of communicating state machines: the Intermediate Format (IF). The translation from UML to IF is based upon an earlier formal semantics, written in the Abstract State Machine (ASM) notation. Descriptions written in IF can be automatically explored; the results of these explorations are test trees, ready for input to a variety of testing packages. 
46|5||Towards model-based generation of self-priming and self-checking conformance tests for interactive systems|This paper describes a model-based approach to generate conformance tests for interactive applications. Our method addresses generation of: (1) small yet effective set of test frames for testing individual operations, (2) a Set up sequence that brings the system under test in an appropriate state for a test frame (self-priming), (3) a Verification sequence for expected output and state changes (self-checking), and, (4) negative test cases in the presence of exceptions. Our method exploits a novel mutation scheme applied to operations modeled as relationships among parameters and state variables; a set of novel abstraction techniques which result in a compact finite state automaton; and search techniques to automatically generate the set up and verification sequences. We illustrate our method with a simple ATM application. 
46|5||How to judge testing progress|It is usual to base the assessment of software testing progress on a coverage measure such as code coverage or specification coverage, or on the percentage of the input domain exercised. In this paper it is argued that these characteristics do not provide good indications of the degree to which the software has been tested. Instead we propose that the assessment of testing progress be based on the total percentage of the probability mass that corresponds to the test cases selected and run. To do this, it is necessary to collect data that profiles how the software will be used once it is operational in the field. By so doing, we are able to accurately determine how much testing has been done, and whether it has met the standards of completeness for the product under consideration. 
46|5||On the testing methods used by beginning software testers|This paper describes our experiences of the methods used by novice software testers to test their own programs, as well as their perception of the classification-tree method, which is a black box testing method first introduced by Grochtmann and Grimm. We conducted two case studies involving novice software testers. The subjects in the first study possessed one-year working experience while those in the second study had a wider range of working experiences. Both studies found that white box testing methods were initially far more popular than black box methods, but the majority of the subjects were convinced of the benefits of the classification-tree method after they had learned and used it. About two-third of them indicated their preference of the classification-tree method over the methods they originally used. 
46|5||A structured experiment of test-driven development|Test Driven Development (TDD) is a software development practice in which unit test cases are incrementally written prior to code implementation. We ran a set of structured experiments with 24 professional pair programmers. One group developed a small Java program using TDD while the other (control group), used a waterfall-like approach. Experimental results, subject to external validity concerns, tend to indicate that TDD programmers produce higher quality code because they passed 18% more functional black-box test cases. However, the TDD programmers took 16% more time. Statistical analysis of the results showed that a moderate statistical correlation existed between time spent and the resulting quality. Lastly, the programmers in the control group often did not write the required automated test cases after completing their code. Hence it could be perceived that waterfall-like approaches do not encourage adequate testing. This intuitive observation supports the perception that TDD has the potential for increasing the level of unit testing in the software industry. 
46|5||A secure methodology for interchangeable services|Computing today requires the use of many software packages, but only a few packages are used on a daily basis. This infrequent usage pattern often does not justify purchasing full licenses and therefore motivates a need for a more flexible way to use and pay for the usage of software. This paper describes a design philosophy in which similar services provide the same interface to clients. Services based on this design are interchangeable, allow payment per use, handle payment conveniently and securely, are platform independent, and frequently do not require local installation. Clients can therefore easily utilize resources based on application needs and services available at the time that the application is executing. An example implementation using this methodology is also discussed. 
46|5||Efficient mediators with closures for handling dynamic interfaces in an imperative language|Mediators are well-known software components in the construction of distributed systems and applications, with clear advantages when adding functionality to legacy code. However, mediators that must handle dynamic interfaces (i.e. those that may change at run-time such as callback functions) are not easy to build in most imperative languages such as C, due to the many variants implied by the dynamic interfaces. We call this kind of mediators intermediators. We propose a systematic implementation method based on the concept of closures to implement intermediators using the Tempo run-time specializer for C programs. To illustrate this method, we implemented a unified user authentication intermediator for Unix and Windows 2000 called GINA-IM. GINA-IM gets the password entries from a Unix NIS server and performs user authentication based on the Graphical Identification and Authentication (GINA) model of Windows 2000. The code of GINA-IM is a quarter of the code size of a conventional component written without our tools. GINA-IM is in production use at authors' university by two thousands of freshmen in class and several thousands of students daily. 
46|6|http://www.sciencedirect.com/science/journal/09505849/46/6|Editorial Board|
46|6||Software engineering aspects of constraint-based timetablingâa case study|This paper details the stages of building a substantial, carefully specified, fully tested and fully operational university and school timetabling system. This is reported as a case study in applying Constraint Satisfaction techniques. The emphasis is on the software engineering aspects of the problem. That is, Constraint Satisfaction problems are expressed in a language more familiar to the formal software engineering community. Moreover, this language is used to formulate domain constraints and heuristic information. In addition to that, the user's needs are looked at more closely. For instance, the system supplies indications useful for relaxing or reformulating the constraints of the problem when a solution satisfying these constraints is impossible to produce. This has a value in bringing Constraint Satisfaction one-step closer to formal specification, program verification and transformation. 
46|6||A document-driven agent-based approach for business processes management|Due to the development of Internet and the desire of almost all departments of business organizations to be interconnected and to make data accessible at any time and any place, more and more workflow management systems are applied to business process management. In this paper, a mobile, intelligent and document-driven agent framework is proposed to model business process management system. Each mobile agent encapsulates a single document, which includes a set of business logic. It can achieve (1) trace ability: a function that enables administrators to monitor document processes easily, (2) document life cycle: a feature using agent life cycle to manage document life cycle and concurrent processing, and (3) dynamic scheduling: a document agent can dynamically schedule its itinerary, and a document control agent can dynamically schedule its services. We also implemented an official document management system explaining our approach by Aglets. 
46|6||DPEM: a decentralized software process enactment model|This paper proposes a decentralized process enactment model called DPEM, which operates on the network. It consists of multiple developer sites, a name server site, a developer coordinator site, and an event manager site. DPEM also provides an intermediate process language called DPEL. To enact a process program written in a source process language (e.g. CSPL and APPL/A), the process program is first translated into DPEL segments, in which those for developer sites are composed of activities and synchronization information. Next, DPEL segments are transferred to DPEM sites. Then, activities in developer sites are enacted concurrently, during which activities are synchronized using synchronization information transferred to the developer sites. Major features offered by DPEM are: (1) every site in DPEM can enact DPEL segments and hence no site will become a bottleneck, (2) a software product is managed by the site that accesses it frequently, which reduces network flows, and (3) DPEM can coordinate heterogeneous process-centered software engineering environments using the intermediate process language DPEL. 
46|6||Network application programming interfaces (APIs) performance on commodity operating systems|Network Application Programming Interfaces (APIs) are important components of network-based applications. They play a central role in the end-to-end performance ultimately delivered by networked applications. In addition, most network architectures exploit the underlying networking APIs in their designs. We present an empirical performance evaluation on the PC platform of the most popular networking APIs, namely: Winsock/BSD, Java, and RMI.To explore the impact of the underlying operating system and the Java Virtual Machine architecture on the networking APIs, we conducted performance tests on four widely used operating systems namely, Windows NT 4.0, Windows 2000, Linux, and Solaris 8. We found that RMI latency is 1.7 times higher over Java. Latency over Java is around two to three times higher than over native Windows or BSD sockets. Moreover, native sockets yield around 1.8 and 3.5 times higher throughput over Java and RMI, respectively. We hope that our results will be useful to application designers and developers and help them better optimize the end-to-end performance of their applications with a knowledge of the performance of the underlying networking APIs. 
46|6||Reengineering windows software applications into reusable CORBA objects|CORBA is becoming one of the most important middleware for supporting object-oriented and client/server paradigms in distributed computing systems. However, application systems based on CORBA are still scarce to date. One reason is that only few CORBA object services have been developed. To have a new CORBA application, a programmer must make the effort to design a program with a CORBA interface. In our previous work [Proceedings of the Sixth IEEE Computer Society Workshop on Future Trends of Distributed Computing Systems (1997) 2], a re-engineering approach was proposed to convert RPC-based programs into CORBA objects. This has successfully increased the development of CORBA applications. However, the source code is required in this approach. In many cases, software designers cannot acquire the source code. This prevents adapting existing PC software applications, particularly for Windows applications. Our study addresses this problem. A graphic factory temperature monitor system, which integrates MS-Excel under MS-Windows, was implemented to demonstrate the feasibility of our approach. 
46|6||Structuring professional cooperation|We present an element suitable to structure professional cooperation. This structuring element is independent of any domain or area of expertise, although it is biased towards design. The element describes modes of operation of a single expert and the cooperation between multiple experts. Based on this, aggregates representing complex project situations can be rendered. We offer several examples. 
46|6||A peer to peer (P2P) architecture for dynamic workflow management|This paper presents the architecture of a novel Peer to Peer (P2P) workflow management system. The proposed P2P architecture is based on concepts such as a Web Workflow Peers Directory (WWPD) and Web Workflow Peer (WWP). The WWPD is an active directory system that maintains a list of all peers (WWPs) that are available to participate in Web workflow processes. Similar to P2P systems such as Napster and Gnutella, it allows peers to register with the system and offer their services and resources to other peers over the Internet. Furthermore, the architecture supports a novel notification mechanism to facilitate distributed workflow administration and management.Employing P2P principles can potentially simplify the workflow process and provide a more open, scalable process model that is shared by all workflow participants. This would enable for example a WWP to connect directly to another without going through an intermediary, currently represented by the workflow process management server. P2P workflow becomes more efficient as the number of peers performing the same role increases. Available peers can be discovered dynamically from the WWPD.The few currently existing P2P based workflow systems fail to utilise state of the art Web technologies such as Web Services. In contrast, using the approach described here it is possible to expose interoperable workflow processes over the Internet as services. A medical consultation case study is used to demonstrate the proposed system. 
46|7|http://www.sciencedirect.com/science/journal/09505849/46/7|Inside Front Cover|
46|7||An object-oriented analysis method for customer relationship management information systems|For the advances of Internet technologies in recent years, Electronic Commerce (EC) has gained many attentions as a major theme for enterprises to keep their competitiveness. Amongst all possibly desired endeavors for the EC, research has shown that effective management of customer relationships is a major source for keeping competitive differentiation. Therefore, it is commonly recognized as an important goal for an enterprise to promote its management of customer relationships through a prospect information system on the Internet to achieve the so-called Business-to-Customer EC. In this paper, we propose an object-oriented analysis method for the development of such a Customer Relationship Management Information System (CRMIS). The approach starts from the identification of prospect customers and their desired behaviors under preferable execution environments, and ends with the specification of system—internal objects/entities that collaborate to satisfy these behaviors and environments. The method is a use case driven approach with UML utilized and extended as its tool. To illustrate, the method is applied to an exemplified CRMTS for house agency. 
46|7||A distributed execution environment for shared java objects|This paper discusses the implementation of a distributed execution environment, DJO, which supports the use of shared Java objects for parallel and distributed applications and provides the Java programmer with the illusion of a network-wide shared object space on loosely coupled distributed systems. DJO supports shared objects through an implementation of multiple reader/single writer write-invalidate DSM protocol in software, providing the shared memory abstraction at object granularity. Object distribution and sharing are implemented through the replication mechanism, transparently to application. The system enforces mutual consistency among replicas of an object. The main benefits of DJO are enhanced availability and performance due to the replicated object model and easier application design, as the underlying software takes care of distribution and memory consistency issues. 
46|7||Migrating legacy codes to distributed computing environments: a CORBA approach|This paper describes techniques used to migrate legacy codes as CORBA components to a distributed problem solving environment (PSE). It presents a CORBA compliant wrapper generator (WG), which can be used to automatically wrap high performance legacy codes in C or Fortran as CORBA components for reuse in the PSE. Using WG, a finite element based computational fluid dynamics (CFD) legacy code has been wrapped as a CORBA component and a PSE for simulating incompressible Navier-Stokes flows has been implemented. A user can submit a task to the wrapped CFD component through a Web page without knowing the exact implementation of the component. In this way, a user's desktop computing environment can be extended to a computing environment using a cluster of workstations or a parallel computer. 
46|7||Safer language subsets: an overview and a case history, MISRA C|This paper gives an overview of safer language subsets in general and considers one widely-used one, MISRA C, in particular.The rationale, specification, implementation and enforcement of a safer language subset each introduce particular problems which has led to their inconsistent take-up over the years even in applications which may be safety-related and definitely need subset restrictions. Each of these areas will be discussed illustrating practical problems which may be encountered with standards in general before focussing on the widely used MISRA C standard [MISRA C guidelines (1998)]. The approach taken is necessarily empirical and where it is able quotes measurements.The real objective of this paper is to produce an empirically based taxonomy of programming language subset rules to bring all these issues together and promote the concept that a safer subset must be based on measurement principles however crudely they are practised currently in software development.The concept of signal to noise ratio of a programming standard is also introduced. 
46|7||Communication and co-ordination practices in software engineering projects|In this paper we report on the investigation, description and analysis of communication and co-ordination practices in software engineering projects. We argue that existing models of the software process do not adequately address the situated, day-to-day practices in which software engineers collectively engage, yet it is through these practices that effective co-ordination is achieved. Drawing on concepts from organizational theory, we describe an approach for studying co-ordination activity in software engineering and the application of this approach to two real-world software projects. We describe key co-ordination issues in these projects and discuss their implications for software engineering practice. 
46|7||Object versioning and information management|It is already widely accepted that the use of data abstraction in object oriented modelling enables real world objects to be well represented in information systems. However, the issue of how to deal with the continuity problem during gradual or sudden changes of objects continues to pose conceptual and technical challenges. This paper investigates the use of object versioning techniques to examine the continuity and pattern of changes of objects over time. In adopting this the authors assess not only attributes changes to homogenous objects, but also behaviour changes that lead to transforming or destroying existing objects and creating new ones. 
46|8|http://www.sciencedirect.com/science/journal/09505849/46/8|Inside Front Cover|
46|8||Toward reuse of object-oriented software design models|In software reuse, which is an important approach to improving the practice of software engineering, many factors may hinder reusing software artifacts. Among those factors are the availability of software artifacts at a different level of abstraction and a method to classify and retrieve them. This paper proposes an approach based on faceted classification scheme for the classification and retrieval of software design artifacts, namely Object-Oriented Design Models, thus facilitating their reuse. Six facets, Domain, Abstractions, Responsibilities, Collaborations, Design View, and Asset Type have been defined to constitute the classification and the retrieval attributes. Each of the facets describes one aspect of an Object-Oriented design model. It contains a number of predefined terms chosen through the analysis of various software systems specifications. The selected terms of each facet are arranged on a conceptual graph to aid the retrieval process. A design artifact is classified by associating with it a software descriptor through the selection of one or more terms from each facet. The role of a descriptor is to emphasize the important structural and behavioral properties of a design artifact and also to document the artifacts associated with the design model. The associated similarity-based retrieval mechanism helps users to search for candidate design artifacts that best match their target specification. The similarity analysis is based on the estimation of the conceptual distance between the terms in a query descriptor and the terms in the specified descriptors of various design models in a software repository. A case study is presented to illustrate the classification and the retrieval process. 
46|8||Prediction of software development faults in PL/SQL files using neural network models|Database application constitutes one of the largest and most important software domains in the world. Some classes or modules in such applications are responsible for database operations. Structured Query Language (SQL) is used to communicate with database middleware in these classes or modules. It can be issued interactively or embedded in a host language. This paper aims to predict the software development faults in PL/SQL files using SQL metrics. Based on actual project defect data, the SQL metrics are empirically validated by analyzing their relationship with the probability of fault detection across PL/SQL files. SQL metrics were extracted from Oracle PL/SQL code of a warehouse management database application system. The faults were collected from the journal files that contain the documentation of all changes in source files. The result demonstrates that these measures may be useful in predicting the fault concerning with database accesses. In our study, General Regression Neural Network and Ward Neural Network are used to evaluate the capability of this set of SQL metrics in predicting the number of faults in database applications. 
46|8||Comparison of fault classes in specification-based testing|Our results extending Kuhn's fault class hierarchy provide a justification for the focus of fault-based testing strategies on detecting particular faults and ignoring others. We develop a novel analytical technique which allows us to elegantly prove that the hierarchy applies to arbitrary expressions, not just those in disjunctive normal form. We also use the technique to extend the hierarchy to a wider range of fault classes. To demonstrate broad applicability, we compare faults in practical situations and analyze previous results. In particular, using our technique, we show that the basic meaningful impact strategy of Weyuker et al. tests for stuck-at faults, not just variable negation faults. 
46|8||Component documentationâa key issue in software product lines|Product lines embody a strategic reuse of both intellectual effort and existing artefacts, such as software architectures and components. Third-party components are increasingly being used in product line based software engineering, in which case the integration is controlled by the product line architecture. However, the software integrators have difficulties in finding out the capabilities of components, because components are not documented in a standard way. Documentation is often the only way of assessing the applicability, credibility and quality of a third-party component.Our contribution is a standard documentation pattern for software components. The pattern provides guidelines and structure for component documentation and ensures the quality of documentation. The pattern has been validated by applying and analysing it in practice. 
46|8||Data fusion application from evidential databases as a support for decision making|We present in this paper, a data fusion [Third International Conference in Information Fusion, Paris, 2000] algorithm from evidential databases [IEEE SMC’02, 2002]. This algorithm has the specificity of using a hybrid operator avoiding the overwhelming trace problem. Besides, it uses a specific scale for computing reliabilities of information's sources. We apply then data fusion to a company in order to improve the decision-making process. 
46|9|http://www.sciencedirect.com/science/journal/09505849/46/9|Editorial Board|
46|9||The design, implementation, and performance of the V2 temporal document database system|It is now feasible to store previous versions of documents, and not only the most recent version which has been the traditional approach. This is of interest in a number of application, both temporal document databases as well as web archiving systems and temporal XML warehouses. In this paper, we describe the architecture and the implementation of V2, a temporal document database system that supports storage, retrieval, and querying of temporal documents. We also give some performance results from a mini-benchmark run on the V2 prototype. 
46|9||An aspect-based approach to modeling access control concerns|Specifying, enforcing and evolving access control policies is essential to prevent security breaches and unavailability of resources. These access control design concerns impose requirements that allow only authorized users to access protected computer-based resources. Addressing these concerns in a design results in the spreading of access control functionality across several design modules. The pervasive nature of access control functionality makes it difficult to evolve, analyze, and enforce access control policies. To tackle this problem, we propose using an aspect-oriented modeling(AOM) approach for addressing access control concerns. In the AOM approach, functionality that addresses a pervasive access control concern is localized in an aspect. Other functional design concerns are addressed in a model of the application referred to as a primary model. Composing access control aspects with a primary model results in an application model that addresses access control concerns. We illustrate our approach using a form of Role-Based Access Control. 
46|9||Size and effort estimation for applications written in Java|The paper presents a methodology for estimation of software size and effort at early stages of software development. The research concentrates on the size estimation problem, which seems to be the weakest element of cost estimation. The methodology concerns the object-oriented technology and the Java language.Authors review current techniques of size and cost estimation to identify their strengths and weaknesses. The paper describes statistical characteristics of class and method sizes for programs written in Java. The analysis of nearly one million lines of code lead to the conclusion that the average class size and the average method size is independent from application size. This feature is useful in calculating the final application size if the number of classes is known or could be estimated during software development.The paper contains definitions of three simple models of size estimation that are based on class and method sizes. The presented approach may be easily applied as it uses data that is typically produced during early stages of software development. The experimental model was theoretically verified and analysed. Further, an independent set of applications written in Java was used to verify the correctness of the acquired equations and models. Statistical characteristics were acquired and analysed with the use of a dedicated tool that was implemented as a part of the research. 
46|9||How useâoriented development can take place|Usability is still a problem for software development. As the introduced software changes the use context, use qualities cannot be fully anticipated. Close co-operation between users and developers during development has been proposed as a remedy. Others fear such involvement of users as it might jeopardize planning and control. Based on the observation of an industrial project, we show how user participation and control can be achieved at the same time. The present article discusses the specific measures that allowed for co-operation between users and developers in an industrial context. It indicates measures to improve software development by focusing on use-orientation, i.e. allowing for user–developer co-operation. 
46|9||Method configuration: adapting to situational characteristics while creating reusable assets|The world of systems engineering methods is changing as rigorous ‘off-the-shelf’ methods gain popularity. The need for configuration of such methods is increasing accordingly. In this paper, method configuration is treated as a kind of method engineering, focusing on adaptation of a base method. A meta-method based on the concepts of Configuration Packages and Configuration Templates is proposed. Configuration Packages are pre-made reusable configurations of a base method suitable for a specific characteristic of a development situation. Configuration Templates with different characteristics can be related to different Configuration Packages and used as a base for reaching a situational method efficiently. The paper presents experiences from two empirical studies in which the Method for Method Configuration was developed and validated. These studies indicate that this meta-method eases the burden of the method engineer in configuring a method for particular project characteristics. Specifically it helped in deciding what in the base method to omit and to make sure that omissions made were congruent with the overall situational method. 
47|1|http://www.sciencedirect.com/science/journal/09505849/47/1|INSIDE FRONT COVER|
47|1||Systematic Reviews in Evidence-based Software Technology and Software Engineering|
47|1||Assessing effort estimation models for corrective maintenance through empirical studies|We present an empirical assessment and improvement of the effort estimation model for corrective maintenance adopted in a major international software enterprise. Our study was composed of two phases. In the first phase we used multiple linear regression analysis to construct effort estimation models validated against real data collected from five corrective maintenance projects. The model previously adopted by the subject company used as predictors the size of the system being maintained and the number of maintenance tasks. While this model was not linear, we show that a linear model including the same variables achieved better performances. Also we show that greater improvements in the model performances can be achieved if the types of the different maintenance tasks is taken into account. In the second phase we performed a replicated assessment of the effort prediction models built in the previous phase on a new corrective maintenance project conducted by the subject company on a software system of the same type as the systems of the previous maintenance projects. The data available for the new project were finer grained, according to the indications devised in the first study. This allowed to improve the confidence in our previous empirical analysis by confirming most of the hypotheses made. The new data also provided other useful indications to better understand the maintenance process of the company in a quantitative way. 
47|1||Software productivity and effort prediction with ordinal regression|In the area of software cost estimation, various methods have been proposed to predict the effort or the productivity of a software project. Although most of the proposed methods produce point estimates, in practice it is more realistic and useful for a method to provide interval predictions. In this paper, we explore the possibility of using such a method, known as ordinal regression to model the probability of correctly classifying a new project to a cost category. The proposed method is applied to three data sets and is validated with respect to its fitting and predictive accuracy. 
47|1||Adaptive fuzzy logic-based framework for software development effort prediction|Algorithmic effort prediction models are limited by their inability to cope with uncertainties and imprecision present in software projects early in the development life cycle. In this paper, we present an adaptive fuzzy logic framework for software effort prediction. The training and adaptation algorithms implemented in the framework tolerates imprecision, explains prediction rationale through rules, incorporates experts knowledge, offers transparency in the prediction system, and could adapt to new environments as new data becomes available. Our validation experiment was carried out on artificial datasets as well as the COCOMO public database. We also present an experimental validation of the training procedure employed in the framework. 
47|1||A comparison of four process metamodels and the creation of a new generic standard|Software development processes and methodologies to date have frequently been described purely textually. However, more recently, a number of metamodels have been constructed to both underpin and begin to formalize these methodologies. We have critically examined four of these: the Object Management Group's Software Process Engineering Metamodel (SPEM), the OPEN Process Framework (OPF), the OOSPICE metamodel for capability assessment and the LiveNet approach for computer-supported collaborative work (CSCW). Based on this analysis, a new, combined metamodel, named Standard Metamodel for Software Development Methodologies (SMSDM) has been constructed which supports not only process but also products and capability assessment in the contexts of both software development and CSCW. As a proof of concept we conclude with a partial example to show how the SMSDM metamodel (and by inference the other metamodels) are used in practice by creating a simple yet usable methodology. 
47|10|http://www.sciencedirect.com/science/journal/09505849/47/10|Combating architectural degeneration: a survey|As software systems evolve over time, they invariably undergo changes that can lead to a degeneration of the architecture. Left unchecked, degeneration may reach a level where a complete redesign is necessary, a task that requires significant effort. In this paper, we present a survey of technologies developed by researchers that can be used to combat degeneration, that is, technologies that can be employed in identifying, treating and researching degeneration. We also discuss the various causes of degeneration and how it can be prevented. 
47|10||Parallel testing of distributed software|The paper presents the experience of the use of parallel computing technologies to accelerate the testing of a complex distributed programming system such as Orbix 3, which is IONA's implementation of the CORBA 2.1 standard. The design and implementation of the parallel testing system are described in detail. Experimental results proving the high efficiency of the system are given. 
47|10||A theoretical foundation of variability in component-based developmentâ|Component-Based Development (CBD) is revolutionizing the process of building applications by assembling pre-built reusable components. Components should be designed more for inter-organizational reuse, rather than intra-organization reuse through domain analysis which captures the commonality of the target domain. Moreover, the minor variations within the commonality should also be modeled and reflected in the design of components so that family members can effectively customize the components for their own purpose. To carry out domain analysis effectively and design widely reusable components, precise definitions of variability-related terms and a classification of variability types must be made. In this paper, we identify the fundamental difference between conventional variability and component variability, and present five types of variability and three kinds of variability scopes. Each type of variability is precisely defined for its applicable situations and guidelines. Having a formal view on variability, not only the domain analysis but also component customization can be effectively carried out in a precise manner. 
47|10||Quality and comprehension of UML interaction diagrams-an experimental comparison|UML (Unified Modeling Language) is a collection of somewhat overlapping modeling techniques, thus creating a difficulty in establishing practical guidelines for selecting the most suitable techniques for modeling OO artifacts. This is true mainly with respect to two types of interaction diagrams: Sequence and collaboration. Attempts have been made to evaluate the comprehensibility of these diagram types for various types of applications, but they did not address the issue of quality of diagrams created by analysts. This article reports the findings from a controlled experiment where both the comprehensibility and quality of the interaction diagrams were investigated in two application domains: management information systems (MIS) and real-time (RT) systems.Our results indicate that collaboration diagrams are easier to comprehend than sequence diagrams in RT systems, but there is no difference in comprehension of the two diagram types in MIS. Irrespective of the diagram type, it is easier to comprehend interaction diagrams of MIS than of RT systems. With respect to diagram quality, in the case of MIS, analysts create collaboration diagrams of better quality than sequence diagrams, but there is no significant difference in quality of diagrams created in RT systems. Irrespective of the diagram type, more correct diagrams are created in MIS applications than in RT applications. 
47|10||A survey of component based system quality assurance and assessment|Component Based Software Development (CBSD) is focused on assembling existing components to build a software system, with a potential benefit of delivering quality systems by using quality components. It departs from the conventional software development process in that it is integration centric as opposed to development centric. The quality of a component based system using high quality components does not therefore necessarily guarantee a system of high quality, but depends on the quality of its components, and a framework and integration process used. Hence, techniques and methods for quality assurance and assessment of a component based system would be different from those of the traditional software engineering methodology. It is essential to quantify factors that contribute to the overall quality, for instances, the trade off between cost and quality of a component, analytical techniques and formal methods, and quality attribute definitions and measurements. This paper presents a literature survey of component based system quality assurance and assessment; the areas surveyed include formalism, cost estimation, and assessment and measurement techniques for the following quality attributes: performance, reliability, maintainability and testability. The aim of this survey is to help provide a better understanding of CBSD in these aspects in order to facilitate the realisation of its potential benefits of delivering quality systems. 
47|11|http://www.sciencedirect.com/science/journal/09505849/47/11|Communicating bugs: global bug knowledge distribution|Unfortunately, software-component libraries shared on a global scale contain bugs. Members of the library user community often report bugs, workarounds, and fixes. This bug knowledge, however, generally remain undiscovered on library web site or in open bug databases.In this article I describe design criteria for bug handing from a global user community perspective. I also describe a distribution architecture for bug knowledge. The architecture focuses on bug awareness and bug visibility in the standard work environment. 
47|11||On the specification and implementation of distributed systems using NMDS and LIPS|This paper describes a graphical notation called NMDS and an implementation language called LIPS for producing distributed systems. NMDS is a set of notations based on Role Activity Diagrams with features for expressing concurrency, dataflow and communication and it lends itself to elicitation and verification while expressing concurrency unambiguously in a concise manner. It also fits in with the syntax and semantics of LIPS. LIPS is a distributed message passing language that promotes the separation of communication from computation by using the concept of guarded processes. One of the major advantages of using NMDS with LIPS is that reverse engineering of LIPS programs can easily be achieved. 
47|11||A study on managing the performance requirements of a distributed service delivery software system|Like other non-functional requirements (NFR), performance requirements can have global impact on a system. This article presents a preliminary study on the management of performance requirements of a distributed service delivery (DSD) system using the ‘Performance Requirement Framework (PeRF)’ developed by Nixon [B.A. Nixon, Management of performance requirements for information systems, IEEE Transactions on Software Engineering 20 (12), (2000)]. Various steps in the framework are explained in the context of the DSD system, and the effects of some design decisions on the performance have been evaluated. PeRF was originally developed for information systems. The article shows how the framework can be adapted to a distributed system. However, additional studies on a number of distributed systems need to be carried out in order to develop a full framework for performance requirements for distributed systems, or perhaps for different categories of such systems. 
47|11||A framework for evaluating a software bidding model|This paper discusses the issues involved in evaluating a software bidding model. We found it difficult to assess the appropriateness of any model evaluation activities without a baseline or standard against which to assess them. This paper describes our attempt to construct such a baseline. We reviewed evaluation criteria used to assess cost models and an evaluation framework that was intended to assess the quality of requirements models. We developed an extended evaluation framework and an associated evaluation process that will be used to evaluate our bidding model. Furthermore, we suggest the evaluation framework might be suitable for evaluating other models derived from expert-opinion based influence diagrams. 
47|11||Experiences of using an evaluation framework|This paper reports two trials of an evaluation framework intended to evaluate novel software applications. The evaluation framework was originally developed to evaluate a risk-based software bidding model, and our first trial of using the framework was our evaluation of the bidding model. We found that the framework worked well as a validation framework but needed to be extended before it would be appropriate for evaluation. Subsequently, we compared our framework with a recently completed evaluation of a software tool undertaken as part of the Framework V CLARiFi project. In this case, we did not use the framework to guide the evaluation; we used the framework to see whether it would identify any weaknesses in the actual evaluation process. Activities recommended by the framework were not undertaken in the order suggested by the evaluation process and we found problems relating to that oversight surfaced during the tool evaluation activities. Our experiences suggest that the framework has some benefits but it also requires further practical testing. 
47|12|http://www.sciencedirect.com/science/journal/09505849/47/12|An approach to ontology for institutional facts in the semantic web|Refinement in software engineering allows a specification to be developed in stages, with design decisions taken at earlier stages constraining the design at later stages. Refinement in complex data models is difficult due to lack of a way of defining constraints, which can be progressively maintained over increasingly detailed refinements. Category theory provides a way of stating wide scale constraints. These constraints lead to a set of design guidelines, which maintain the wide scale constraints under increasing detail. Previous methods of refinement are essentially local, and the proposed method does not interfere very much with these local methods. The result is particularly applicable to semantic web applications, where ontologies provide systems of more or less abstract constraints on systems, which must be implemented and therefore refined by participating systems. With the approach of this paper, the concept of committing to an ontology carries much more force. 
47|12||Multi-way spatial join selectivity for the ring join graph|Efficient spatial query processing is very important since the applications of the spatial DBMS (e.g. GIS, CAD/CAM, LBS) handle massive amount of data and consume much time. Many spatial queries contain the multi-way spatial join due to the fact that they compute the relationships (e.g. intersect) among the spatial data. Thus, accurate estimation of the spatial join selectivity is essential to generate an efficient spatial query execution plan that takes advantages of spatial access methods efficiently. For the multi-way spatial joins, the selectivity estimation formulae only for the two kinds of query types, tree and clique, have been developed. However, the selectivity estimation for the general query graph which contains cycles has not been developed yet. To fill this gap, we devise a formula for the multi-way spatial ring join selectivity. This is an indispensable step to compute the selectivity of the general multi-way spatial join whose join graph contains cycles. Our experiment shows that the estimated sizes of query results using our formula are close to the sizes of actual query results. 
47|12||Enhancing class commutability in the deployment of design patterns|A design pattern provides a structure to facilitate program changes with respect to a design concern. For example, the State pattern manages object behaviour in different internal states of objects. It allows new internal states of an object to be supported with the reuse of the object context. The deployment of a design pattern in a software program comprises a set of classes following the structure of the pattern. Within the set, classes that implement the managed concern of the pattern are commuted to new ones when changes related to the concern occur. However, commutation efforts can be tedious if these classes are accessed arbitrarily throughout the software. To confine the commutation efforts, these classes should be properly encapsulated. This paper proposes design restrictions in pattern deployments to achieve proper encapsulation. The approach is illustrated by a pattern-based program that supports appointment scheduling of multiple users. Preliminary experiments show that our approach facilitates program changes subject to multiple design concerns. 
47|12||Computing dynamic slices of concurrent object-oriented programs|We propose a novel dynamic program slicing technique for concurrent object-oriented programs. Our technique uses a Concurrent System Dependence Graph (CSDG) as the intermediate program representation. We mark and unmark the edges in the CSDG appropriately as and when the dependencies arise and cease during run-time. We mark an edge when its associated dependence exists and unmark an edge when the dependence ceases to exist. Our approach eliminates the use of trace files. Another advantage of our approach is that when a request for a slice is made, it is already available. This appreciably reduces the response time of slicing commands. 
47|12||A formal framework for database sampling|Database sampling is commonly used in applications like data mining and approximate query evaluation in order to achieve a compromise between the accuracy of the results and the computational cost of the process. The authors have recently proposed the use of database sampling in the context of populating a prototype database, that is, a database used to support the development of data-intensive applications. Existing methods for constructing prototype databases commonly populate the resulting database with synthetic data values. A more realistic approach is to sample a database so that the resulting sample satisfies a predefined set of integrity constraints. The resulting database, with domain-relevant data values and semantics, is expected to better support the software development process. This paper presents a formal study of database sampling. A Denotational Semantics description of database sampling is first discussed. Then the paper characterises the types of integrity constraints that must be considered during sampling. Lastly, the sampling strategy presented here is applied to improve the data quality of a (legacy) database. In this context, database sampling is used to incrementally identify the set of tuples which are the cause of inconsistencies in the database, and therefore should be the ones to be addressed by the data cleaning process. 
47|13|http://www.sciencedirect.com/science/journal/09505849/47/13|Efficient discovery of multilevel spatial association rules using partitions|Spatial data mining has been identified as an important task for understanding and use of spatial data- and knowledge-bases. In this paper, we present a new approach to discover strong multilevel spatial association rules in spatial databases based on partitioning the set of rows with respect to the spatial relations denoted as relation table R. Meanwhile, the introduction of the equivalence partition tree makes the discovery of multilevel spatial association rules easy and efficient. Experiments show that the new algorithm is efficient. 
47|13||XML-based requirements engineering for an electronic clearinghouse|We present methods and tools to support XML-based requirements engineering for an electronic clearinghouse that connects trading partners in the telecommunications area. The original semi-structured requirements, locally known as business rules, were written as message specifications in a non-standardized and error-prone format using MS Word. To remedy the resulting software failures and faults, we first formalized the requirements by designing an W3C XML Schema for the precise definition of the requirements structure. The schema allows a highly structured representation of the essential information in eXtensible Markup Language (XML). Second, to offer the requirements engineers the ability to edit the XML documents in a friendly way while preserving their information structure, we developed a custom editor called XLEdit. Third, by developing a converter from MS Word to the target XML format, we helped the requirements engineers to migrate the existing business rules. Fourth, we developed translators from the structured requirements to schema languages, which enabled automated generation of message-validation code. The increase in customer satisfaction and clearinghouse-service efficiency are primary gains from the investment in the technology for structured requirements editing and validation. 
47|13||Measuring design testability of a UML class diagram|Design-for-testability is a very important issue in software engineering. It becomes crucial in the case of OO designs where control flows are generally not hierarchical, but are diffuse and distributed over the whole architecture. In this paper, we concentrate on detecting, pinpointing and suppressing potential testability weaknesses of a UML class diagram. The attribute significant from design testability is called ‘class interaction’ and is generalized in the notion of testability anti-pattern: it appears when potentially concurrent client/supplier relationships between classes exist in the system. These interactions point out parts of the design that need to be improved, driving structural modifications or constraints specifications, to reduce the final testing effort. In this paper, the testability measurement we propose counts the number and the complexity of interactions that must be covered during testing. The approach is illustrated on application examples. 
47|13||Automated software size estimation based on function points using UML models|A systematic approach to software size estimation is important for accurate project planning. In this paper, we will propose the unified mapping of UML models into function points. The mapping is formally described to enable the automation of the counting procedure. Three estimation levels are defined that correspond to the different abstraction levels of the software system. The level of abstraction influences an estimate's accuracy. Our research, based on a small data set, proved that accuracy increases with each subsequent abstraction level. Changes to the FPA complexity tables for transactional functions will also be proposed in order to better quantify the characteristics of object-oriented software. 
47|14|http://www.sciencedirect.com/science/journal/09505849/47/14|A roadmap of problem frames research|It has been a decade since Michael Jackson introduced problem frames to the software engineering community. Since then, he has published further work addressing problem frames as well as presenting several keynote addresses. Other authors have researched problem frames, have written about their experiences and have expressed their opinions. It was not until 2004 that an opportunity presented itself for researchers in the field to gather as a community. The first International Workshop on Advances and Applications of Problem Frames (IWAAPF'04) was held at the International Conference on Software Engineering in Edinburgh on 24th May 2004. This event attracted over 30 participants: Jackson delivered a keynote address, researchers presented their work and an expert panel discussed the challenges of problem frames.Featuring in this special issue are two extended papers from the workshop, an invited contribution from Jackson in which he positions problem frames in the context of the software engineering discipline, and this article, where we provide a review of the literature. 
47|14||Problem frames and software engineering|A general account is given of the problem frames approach to the development of software-intensive systems, assuming that the reader is already familiar with its basic ideas. The approach is considered in the light of the long-standing aspiration of software developers to merit a place among practitioners of the established branches of engineering. Some of its principles are examined, and some comments offered on the range of its applicability. A view of the approach is suggested by an important account of engineering in the aeronautical industry: in particular, the problem classes captured by elementary problem frames are likened to those solved in established engineering branches by normal, rather than radical, design. The relative lack of specialisation in software development is identified as an important factor holding back the evolution of normal design practice in some areas. 
47|14||An approach to formal automated analysis of problem-frame concerns|We present an approach to performing automated formal analysis of the concerns found in descriptions that follow the problem-frames technique. The descriptions of the problem domains, machine and requirements are written in a formal language. The approach is explained through a case study using the Alloy language. We show that an evaluation of results and counter-examples provided by a model checker can reveal useful information that can help remove inconsistencies as well as composition errors. 
47|14||A UML-based approach for problem frame oriented software development|We propose a software development approach that combines the use of the structuring concepts provided by problem frames, the use of the UML notation, together with our methodological approach for well-founded methods. Problem frames are used to provide a first idea of the main elements of the problem under study. Then we provide ad hoc UML based development methods for some of the most relevant problem frames together with precise guidelines for the users. The general idea of our method is that, for each frame, several artifacts have to be produced, each one corresponding to a part of the frame. The description level may range from informal and sketchy, to formal and precise, while this approach is drawn from experience in formal specifications. Thus we show how problem frames may be used upstream of a development method to yield an improved and more efficient method equipped with the problem frames structuring concepts. 
47|15|http://www.sciencedirect.com/science/journal/09505849/47/15|Most cited journal articles in software engineering|
47|15||An analysis of the most cited articles in software engineering journalsâ1999|Citations and related work are crucial in any research to position the work and to build on the work of others. A high citation count is an indication of the influence of specific articles. The importance of citations means that it is interesting to analyze which articles are cited the most. Such an analysis has been conducted using the ISI Web of Science to identify the most cited software engineering journal articles published in 1999. The objective of the analysis is to identify and list the articles that have influenced others the most as measured by citation count. An understanding of which research is viewed as most valuable to build upon may provide valuable insights into what research to focus on now and in the future. Based on the analysis, a list of the 20 most cited articles is presented here. The intention of the analysis is twofold. First, to actually show the most cited articles, and second, to invite the authors of the most cited articles in 1999 to contribute to a special issue of Information and Software Technology. Five invited authors have accepted the invitation and their articles are appearing in this special issue. Moreover, the research topics and methods of the most cited articles in 1999 are compared with those from the most cited articles in 1994 to provide a picture of similarities and differences between the years. 
47|15||Engineering a software tool for gene structure prediction in higher organisms|The research area now commonly called ‘bioinformatics’ has brought together biologists, computer scientists, statisticians, and scientists of many other fields of expertise to work on computational solutions to biological problems. A large number of algorithms and software packages are freely available for many specific tasks, such as sequence alignment, molecular phylogeny reconstruction, or protein structure determination. Rapidly changing needs and demands on data handling capacity challenge the application providers to consistently keep pace. In practice, this has led to many incremental advances and re-writing of code that present the user community with confusing options and a large overhead from non-standardized implementations that need to be integrated into existing work flows. This situation gives much scope for contributions by software engineers. In this article, we describe an example of engineering a software tool for a specific bioinformatics task known as spliced alignment. The problem was motivated by disabling limitations in an original, ad hoc, and yet widely popular implementation by one of the authors. The present collaboration has led to a robust, highly versatile, and extensible tool (named GenomeThreader) that not only overcomes the limitations of the earlier implementation but greatly improves space and time requirements. 
47|15||A measurement framework for object-oriented software testability|Testing is an expensive activity in the development process of any software system. Measuring and assessing the testability of software would help in planning testing activities and allocating required resources. More importantly, measuring software testability early in the development process, during analysis or design stages, can yield the highest payoff as design refactoring can be used to improve testability before the implementation starts.This paper presents a generic and extensible measurement framework for object-oriented software testability, which is based on a theory expressed as a set of operational hypotheses. We identify design attributes that have an impact on testability directly or indirectly, by having an impact on testing activities and sub-activities. We also describe the cause-effect relationships between these attributes and software testability based on thorough review of the literature and our own testing experience. Following the scientific method, we express them as operational hypotheses to be further tested. For each attribute, we provide a set of possible measures whose applicability largely depends on the level of details of the design documents and the testing techniques to be applied. The goal of this framework is twofold: (1) to provide structured guidance for practitioners trying to measure design testability, (2) to provide a theoretical framework for facilitating empirical research on testability. 
47|15||BP's multi-enterprise asset management system|BP is one of the largest energy companies in the world with 2003 revenues of $233 billion. In this paper, we analyse its use of an innovative ‘multi-enterprise asset management system’ that supports and enables the asset management strategy of BP's exploration and production division on the UK continental shelf (UKCS). The analysis focuses on how BP connects its business processes with over 1500 suppliers to co-ordinate the maintenance, operation and repair of specialised exploration and production equipment. The systems strategy is novel because it takes the enterprise computing concept and implements it across organisational boundaries—hence the term ‘multi-enterprise system’. This use of a shared system with all of its suppliers is distinctive from the most common way of connecting with economic partners which is to use shared data systems based on common data standards and communication technologies such as EDI and more recently XML-based systems within vertical industries such as RosettaNet. The design of the multi-enterprise system is based on a sophisticated business process management system called Maximo and this is used to illustrate the systems design aspect of the overall information system in the broader contexts of business strategy and information technology infrastructure. 
47|15||Software project management using PROMPT: A hybrid metrics, modeling and utility framework|In this paper, we present a ‘forward-looking’ decision support framework that integrates up-to-date metrics data with simulation models of the software development process in order to support the software project management control function. This forward-looking approach (called the PROMPT method) provides predictions of project performance and the impact of various management decisions. Tradeoffs among performance measures are accomplished using outcome based control limits (OBCLs) and are augmented using multi-criteria utility functions and financial measures of performance to evaluate various process alternatives. The decision support framework enables the program manager to plan, manage and track current software development activities in the short term and to take corrective action as necessary to bring the project back on track. The model provides insight on potential performance impacts of the proposed corrective actions. A real world example utilizing a software process simulation model is presented. 
47|15||Simulating families of studies to build confidence in defect hypotheses|While it is clear that there are many sources of variation from one development context to another, it is not clear a priori what specific variables will influence the effectiveness of a process in a given context. For this reason, we argue that knowledge about software process must be built from families of studies, in which related studies are run within similar contexts as well as very different ones. Previous papers have discussed how to design related studies so as to document as precisely as possible the values of likely context variables and be able to compare with those observed in new studies. While such a planned approach is important, we argue that an opportunistic approach is also practical. The approach would combine results from multiple individual studies after the fact, enabling recommendations to be made about process effectiveness in context.In this paper, we describe two processes with which we have been working to build empirical knowledge about software development processes: one is a manual and informal approach, which relies on identifying common beliefs or ‘folklore’ to identify useful hypotheses and a manual analysis of the information in papers to investigate whether there is support for those hypotheses; the other is a formal approach based around encoding the information in papers into a structured hypothesis base that can then be searched to organize hypotheses and their associated support. We test these processes by applying them to build knowledge in the area of defect folklore (i.e. commonly accepted heuristics about software defects and their behavior). We show that the formal methodology can produce useful and feasible results, especially when it is compared to the results output from the more manual, expert-based approach. The formalized approach, by relying on a reusable hypothesis base, is repeatable and also capable of producing a more thorough basis of support for hypotheses, including results from papers or articles that may have been overlooked or not considered by the experts. 
47|15||Referree List (see INFSOF 44/2 for template)|
47|15||Author Index|
47|15||Keyword Index|
47|15||Volume Contents|
47|2|http://www.sciencedirect.com/science/journal/09505849/47/2|INSIDE FRONT COVER|
47|2||Business-oriented process improvement: practices and experiences at Thales Naval The Netherlands (TNNL)|Over the last decade many organizations are increasingly concerned with the improvement of their hardware/software development processes. The Capability Maturity Model and ISO9001 are well-known approaches that are applied in these initiatives. However, one of the major bottlenecks to the success of process improvement is the lack of business goal orientation. Additionally, business-oriented improvement approaches often show a lack of process orientation. This paper reports on a process improvement initiative at Thales Naval Netherlands that attempts to combine the best of both worlds, i.e. process improvement and business goal orientation. Main factors in this approach are goal decomposition and the implementation of goal-oriented measurement on three organizational levels, i.e. the business, the process and the team level. 
47|2||Designing an adaptable heterogeneous abstract machine by means of reflection|The concepts of abstract and virtual machines have been used for many different purposes to obtain diverse benefits such as code portability, compiler simplification, interoperability, distribution and direct support of specific paradigms. Despite of these benefits, the main drawback of virtual machines has always been execution performance. Consequently, there has been considerable research aimed at improving the performance of virtual machine's application execution compared to its native counterparts. Techniques like adaptive Just In Time compilation or efficient and complex garbage collection algorithms have reached such a point that Microsoft and Sun Microsystems identify this kind of platforms as appropriate to implement commercial applications.What we have noticed in our research work is that these platforms have heterogeneity, extensibility, platform porting and adaptability limitations caused by their monolithic designs. Most designs of common abstract machines are focused on supporting a fixed programming language and the computation model they offer is set to the one employed by the specific language. We have identified reflection as a basis for designing an abstract machine, capable of overcoming the previously mentioned limitations. Reflection is a mechanism that gives our platform the capability to adapt the abstract machine to different computation models and heterogeneous computing environments, not needing to modify its implementation. In this paper we present the reflective design of our abstract machine, example code extending the platform, a reference implementation, and a comparison between our implementation and other well-known platforms. 
47|2||A documentation infrastructure for the management of data mining projects|Effective project management is a key factor for successful Knowledge Discovery in Databases (KDD) projects. The systematic documentation of previous knowledge, experiments, data and results is a helpful means of keeping track of the project current status. Despite its value, documentation is most often perceived as an overhead. We propose a documentation infrastructure composed of a documentation model and a supporting environment that allows the capture, storage and retrieval of KDD process-related information and artifacts. The paper describes this infrastructure, and reports preliminary experiences on its use. Preliminary results reveal generalized satisfaction with regard to infrastructure expressiveness and functionality, and highlight the contributions of the documentation produced for improving project management, project execution and team communication. The role of documentation in learning and reuse was also identified. 
47|2||An exploratory study into the use of qualitative research methods in descriptive process modelling|The paper describes an exploratory study that investigated two descriptive software process models derived from the same process data using two different techniques. To set the context, the paper describes qualitative methods, particularly grounded theory and its techniques, and then explores the nature of the differences in the two models produced. It suggests ways in which constant comparison may contribute to the process-modelling task. As far as we are aware, it also serves as the first exploratory research on the application of this method in the software engineering process research domain. Based on data analysis using the technique of constant comparison often used in grounded theory research, a naive process modeller derived one of the models. An experienced process engineer relying heavily on experience and skill using an ad hoc approach derived the second model. The aim of the study was to explore differences in the models derived and to use this comparison as a basis for reflection on the method conventionally used in descriptive process modelling in contrast with the use of more formal qualitative analysis. The results show that (1) data analysis using the technique of constant comparison could be successfully applied to analyse process data, (2) the person with little experience in process modelling could produce a process model based on the data analysis using constant comparison and (3) the process model produced by the naive modeller was not equivalent to that produced by an experienced process engineer. 
47|2||Fast mining of frequent tree structures by hashing and indexing|Hierarchical semistructured data arise frequently in the Web, or in biological information processing applications. Semistructured objects describing the same type of information have similar but not identical structure. Usually they share some common ‘schema’. Finding the common schema of a collection of semistructured objects is a very important task and due to the huge amount of such data encountered, data mining techniques have been employed.In this paper, we study the problem of discovering frequently occurring structures in semistructured objects using the notion of association rules. We identify that discovering the frequent structures in the early phases of the mining procedure is the dominant cost and we provide a fast algorithm addressing this issue. We present experimental results, which demonstrate the superiority of the proposed algorithm and also its efficiency in reducing dramatically the processing cost. 
47|3|http://www.sciencedirect.com/science/journal/09505849/47/3|INSIDE FRONT COVER|
47|3||Evaluating the ordering of the SPICE capability levels: an empirical study|The Standard ISO/IEC PDTR 15504 (Software Process Assessment) defines process attributes (PAs) and associated practices must be implemented according to a process capability level. This definition implies that PA practices at lower capability levels must be implemented before moving to higher capability levels. The purpose of this study is to evaluate empirically whether the ordering of set of PAs, as measures of capability, is consistent with the Standard. For this purpose, the study estimates the Coefficient of Reproducibility (CR) statistic that measures the extent to which the observed ratings are identical to the pattern inferred by the Standard. Our analyses based on ratings of 689 process instances show that generally PA order of capability levels is consistent with that inferred by the Standard. However, our results also show that the definition of PA3.2 (Process resource) could be improved. This evaluation is capable of providing a substantiated basis for using the notion of capability, as well as providing information for necessary improvements to the Standard. 
47|3||Î¼cROSE: automated measurement of COSMIC-FFP for Rational Rose RealTime|During the last 10 years, many organizations have invested resources and energy in order to be rated at the highest level as possible according to some maturity models for software development. Since measures play an important role in these models, it is essential that CASE tools offer facilities to automatically measure the sizes of various documents produced using them. This paper introduces a tool, called Î¼cROSE, that automatically measures the functional software size, as defined by the COSMIC-FFP method, for Rational Rose RealTime models. Î¼cROSE streamlines the measurement process, ensuring repeatability and consistency in measurement while reducing measurement cost. It is the first tool to address automatic measurement of COSMIC-FFP and it can be integrated into the Rational Rose RealTime toolset. 
47|3||DMC: a more precise cohesion measure for classes|In object-oriented systems, a single class consists of attributes and methods and its cohesion denotes the degree of relatedness among these elements. To quantify the cohesiveness of a class, a large number of measures that only depict method–attribute reference relationships have been proposed in last decade. However, the flow-dependence relationships among attributes, the direction of method–attribute references, and the potential dependence relationships among the elements in the class are ignored. To address this problem, this paper first depicts four types of explicit dependence relationships and uses a class element dependence graph to represent all dependencies among the elements in a class. Then, a dependence matrix that reflects the degree of direct dependence and indirect dependence among the elements in a class is computed. Finally, a more precise cohesion measure for classes is proposed. 
47|3||ECâa measurement based safer subset of ISO C suitable for embedded system development|With the explosive growth of embedded systems, there is a major need for a standardised code of practice in the use of C. Although this area has been explored before, progress has been ad hoc and most importantly, carried out in the general absence of any measurement support. This paper attempts to define a relatively small number of rules which avoid known fault modes in the language which have published occurrence rates. It will studiously avoid any rules for which no measurement support is available. It is anticipated that the base subset may be extended as time goes by as further data becomes available. Much of the subset is equally relevant to ISO C++, although very little data exists to guide similar initiatives for the considerable part of C++, which lies outside C. 
47|3||NCDS: data mining for discovering interesting network characteristics|This paper presents an approach to observe network characteristics based on data mining framework. Consequently, such observations may be expressed in structured patterns to support the process of network planning. The underlying system monitors the network protocol tables that describe each network connection or host session in order to discover interesting patterns. To achieve this purpose a data abstraction procedure is applied to learn rules that may express the behavior of network characteristics. Thus, the system is capable to discover various operational patterns, provide sensible advices, and support the network planning activity.A database system has been designed and implemented for monitoring the network traffic. Also the results from the experiments have been used to classify real traffic data. The system presented in this paper called network characteristics discovery system. 
47|3||An address mapping approach for test data generation of dynamic linked structures|Software testing is an important technique to assure the correctness of the software. One of the essential prerequisite tasks of software testing is test data generation. This paper proposes an approach to generate test data specifically for dynamic pointer structures. In our context, a pointer is considered and handled as a location in memory, represented by a dynamic linear array that expands and shrinks during execution. As such, pointer test data can be directly generated from this linear array. The proposed technique can also support any dynamic structures, as well as homogeneous and heterogeneous recursive structures. 
47|4|http://www.sciencedirect.com/science/journal/09505849/47/4|An information extraction approach to reorganizing and summarizing specifications|Materials and Process Specifications are complex semi-structured documents containing numeric data, text, and images. This article describes a coarse-grain extraction technique to automatically reorganize and summarize spec content. Specifically, a strategy for semantic-markup, to capture content within a semantic ontology, relevant to semi-automatic extraction, has been developed and experimented with. The working prototypes were built in the context of Cohesia's existing software infrastructure, and use techniques from Information Extraction, XML technology, etc. 
47|4||Replicating software engineering experiments: a poisoned chalice or the Holy Grail|Recently, software engineering has witnessed a great increase in the amount of work with an empirical component; however, this work has often little or no established empirical framework within the topic to draw upon. Frequently, researchers use frameworks from other disciplines in an attempt to alleviate this deficiency. A common underpinning in these frameworks is that experimental replication is available as the cornerstone of knowledge discovery within the discipline. This paper investigates the issues involved in accepting this premise as a fundamental building block with empirical software engineering and recommends extending the traditional view of replication to improve the effectiveness of this essential process within our domain. 
47|4||A unified classification system for research in the computing disciplines|The field of computing is made up of several disciplines of which Computer Science, Software Engineering, and Information Systems are arguably three of the primary ones. Despite the fact that each discipline has a specific focus, there is also considerable overlap. Knowledge sharing, however, is becoming increasingly difficult as the body of knowledge in each discipline increases and specialization results. For effective knowledge sharing, it is therefore important to have a unified classification system by means of which the bodies of knowledge that constitute the field may be compared and contrasted. This paper presents a multi-faceted system based on five research-focused characteristics: topic, approach, method, unit of analysis, and reference discipline. The classification system was designed based on the requirements for effective classification systems, and was then used to investigate these five characteristics of research in the computing field. 
47|4||Software development productivity on a new platform: an industrial case study|The high non-functional requirements on mobile telecommunication applications call for new solutions. An example of such a solution can be a software platform that provides high performance and availability. The introduction of such a platform may, however, affect the development productivity. In this study, we present experiences from research carried out at Ericsson. The purpose of the research was productivity improvement and assessment when using the new platform. In this study, we quantify and evaluate the current productivity level by comparing it with UNIX development. The comparison is based on two large, commercially, available systems. We reveal a factor of four differences in productivity. Later, we decompose the problem into two issues: code writing speed and average amount of code necessary to deliver a certain functionality. We assess the impact of both these issues. We describe the nature of the problem by identifying factors that affect productivity and estimating their importance. To the issues identified we suggest a number of remedies. The main methods used in the study are interviews and historical data research. 
47|4||Comprehension and quality of analysis specificationsâa comparison of FOOM and OPM methodologies|FOOM—Functional and Object Oriented Methodology—combines two essential software-engineering paradigms: the functional (or process-oriented) approach and the object-oriented (OO) approach. The two main products of the analysis phase of FOOM are an initial class diagram and OO-DFDs (dataflow diagrams including data classes rather than traditional data-stores). We evaluated these analysis products by comparing them with the analysis products of OPM—Object-Process Methodology—which also combines the functional and object-oriented approaches, using a unified diagrammatic notation. FOOM and OPM were compared in two controlled experiments from two main points of view: users and analysts. From the point of view of users we compared mainly comprehension of analysis specifications in each methodology. From the point of view of analysts we compared mainly quality, namely correctness of specifications created by analysts who utilized the two methodologies. The main results of the experiments are that FOOM specifications are more comprehensible and preferred by users, and that analysts create more correct specifications when using FOOM methodology. 
47|5|http://www.sciencedirect.com/science/journal/09505849/47/5|Change of Editors|
47|5||Postmortem reviews: purpose and approaches in software engineering|Conducting postmortems is a simple and practical method for organisational learning. Yet, not many companies have implemented such practices, and in a survey, few expressed satisfaction with how postmortems were conducted. In this article, we discuss the importance of postmortem reviews as a method for knowledge sharing in software projects, and give an overview of known such processes in the field of software engineering. In particular, we present three lightweight methods for conducting postmortems found in the literature, and discuss what criteria companies should use in defining their way of conducting postmortems. 
47|5||Virtual workgroups in offshore systems development|The market for offshore systems development, motivated by lower costs in developing countries, is expected to increase and reach about $15 billion in the year 2007. Virtual workgroups supported by computer and communication technologies enable offshore systems development. This article discusses the limitations of using virtual work in offshore systems development, and describes development processes and management procedures amenable to virtual work in offshore development projects. It also describes a framework to use virtual work selectively, while offshore developing various types of information systems. 
47|5||Deriving requirements from process models via the problem frames approach|Jackson's problem frames is an approach to describing a recurring software problem. It is presumed that some knowledge of the application domain and context has been gathered so that an appropriate problem frame can be determined. However, the identification of aspects of the problem, and its appropriate ‘framing’ is recognised as a difficult task. One way to describe a software problem context is through process modelling. Once contextual information has been elicited, and explicitly described, an understanding of what problems need to be solved should emerge. However, this use of process models to inform requirements is often rather ad hoc; the traceability from business process to software requirement is not always as straightforward as it ought to be. Hence, this paper proposes an approach for deriving and contextualising software requirements through use of the problem frames approach from business process models. We apply the approach on a live industrial e-business project in which we assess the relevance and usefulness of problem frames as a means of describing the requirements context. We found that the software problem did not always match easily with Jackson's five existing frames. Where no frame was identified, however, we found that Jackson's problem diagrams did couch the requirements in their right context, and thus application of the problem frames approach was useful. This implies a need for further work in adapting a problem frames approach to the context of e-business systems. 
47|5||Supporting cartoon animation techniques in direct manipulation graphical user interfaces|If judiciously applied, the techniques of cartoon animation can enhance the illusion of direct manipulation that many human computer interfaces strive to present. In particular, animation can convey a feeling of substance in the objects that a user manipulates, strengthening the sense that real work is being done. This paper describes algorithms and implementation issues to support cartoon style graphical object distortion effects for direct manipulation user interfaces. Our approach is based on suggesting a range of animation effects by distorting the view of the manipulated object. To explore the idea, we added a warping transformation capability to the InterViews user interface toolkit. 
47|5||Implementation of fuzzy classification in relational databases using conventional SQL querying|In this paper, a framework for implementing fuzzy classifications in information systems using conventional SQL querying is presented. The fuzzy classification and use of conventional SQL queries provide easy-to-use functionality for data extraction similar to the conventional non-fuzzy classification and SQL querying. The developed framework can be used as data mining tool in large information systems and easily integrated with conventional relational databases. The benefits of using the presented approach include more flexible data analysis and improvement of information presentation at the report generation phase. To confirm the theory, a prototype was developed based on the stored procedures and database extensions of Microsoft SQL Server 2000. 
47|6|http://www.sciencedirect.com/science/journal/09505849/47/6|A novel approach for component-based fault-tolerant software development|With the recent advancements in component-based software engineering, there is an increasing trend in developing applications for highly reliable and critical systems using pre-validated and reusable software components. As these applications are inherently complex and component-interactions are not straightforward, there is an immediate need for a methodology that could aid in composition of these reusable components ensuring the correctness of the composed software system. In this paper, we illustrate how the concepts of category theory can be utilized to develop component-based fault-tolerant software systems that encompass software components capable of tolerating particular types of faults. Our proposed framework for the development of a composite fault-tolerant program and verification of its overall correctness has been realized through a mechanized formal tool. 
47|6||Does UML make the grade? Insights from the software development community|The Unified Modeling Language (UML) has become the de facto standard for systems development and has been promoted as a technology that will help solve some of the longstanding problems in the software industry. However, there is still little empirical evidence supporting the claim that UML is an effective approach to modeling software systems. Indeed, there is much anecdotal evidence suggesting the contrary, i.e. that UML is overly complex, inconsistent, incomplete and difficult to learn. This paper describes an investigation into the adoption and use of UML in the software development community. A web-based survey was conducted eliciting responses from users of UML worldwide. Results indicate a wide diversity of opinion regarding UML, reflecting the relative immaturity of the technology as well as the controversy over its effectiveness. This paper discusses the results of the survey and charts of the course for future research in UML usage. 
47|6||Exploring defect causes in products developed by virtual teams|This paper explores the effects of virtual development on product quality, from the viewpoint of ‘conformance to specifications’. Virtual Development refers to the development of products by teams distributed across space, time, and organization boundaries (hence virtual teams). Specifically, causes of defect injection and non- or late-detection are explored. Because of the practical difficulties of obtaining hard project-specific defect data, an approach was taken that relied upon accumulated expert knowledge. The accumulated expert knowledge based approach was found to be a practical alternative to an in-depth defect causal analysis on a per-project basis. Defect injection causes are concentrated in the Requirements Specification phases. Thus defect dispersion is likely to increase, as requirements specifications are input for derived requirements specifications in multiple, related sub-projects. Similarly, a concentration of causes for the non- or late-detection of defects was found in the Integration Test phases. Virtual development increases the likelihood of defects in the end product because of the increased likelihood of defect dispersion, because of new virtual development related defect causes, and because causes already existing in co-located development are more likely to occur. The findings are important for virtual development environments and (1) allow further research focusing on a framework for lowering residual defects, and (2) give insights that can be used immediately by practitioners to devise strategies for lowering residual defects. 
47|6||Resource conscious development of middleware for control environments: a case of CORBA-based middleware for the CAN bus systems|While it is imperative to exploit middleware technologies in developing software for distributed embedded control systems, it is also necessary to tailor them to meet the stringent resource constraints and performance requirements of embedded control systems. In this paper, we propose a CORBA-based middleware for Controller Area Network (CAN) bus systems. Our design goals are to reduce the memory footprint and remote method invocation overhead of the middleware and make it support group communication that is often needed in embedded control systems. To achieve these, we develop a transport protocol on the CAN and a group communication scheme based on the publisher/subscriber model by realizing subject-based addressing that utilizes the message filtering mechanism of the CAN. We also customize the method invocation and message passing protocol of CORBA so that CORBA method invocations are efficiently serviced on a low-bandwidth network such as the CAN. This customization includes packed data encoding and variable-length integer encoding for compact representation of IDL data types.We have implemented our CORBA-based middleware using GNU ORBit. We report on the memory footprint and method invocation latency of our implementation. 
47|6||Stepwise deployment methodology of a service oriented architecture for business communities|This paper describes the deployment of a Service Oriented Architecture in the specific context of the ‘Business Communities’, i.e. Communities of heterogeneous actors that cooperate in the same business area. The architecture is based on XML and Web Services technologies.More specifically the paper analyzes the structure and the requirements of Business Communities in general, derives the requirements of the architecture and describes its implementation. Finally, a case study is presented to show how the architecture has been implemented for the Business Community of the Port of Genoa characterized by users operating in the cargo and transport business area. 
47|7|http://www.sciencedirect.com/science/journal/09505849/47/7|Translating relational schema into XML schema definition with data semantic preservation and XSD graph|Many legacy systems have been created by using relational database operating not for the Internet expression. Since the relational database is not an efficient way for data explosion, electronic transfer of data, and electronic business on the Web, we introduce a methodology in which a relational schema will be translated to an Extensible Markup Language (XML) schema definition for creating an XML database that is a simple and efficient format on the Web. We apply the Indirect Schema Translation Method that is a semantic-based methodology in this project. The mechanism is that the Relational Schema will be translated into the conceptual model, an Extended Entity Relationship (EER) Model using Reverse Engineering. Afterward, the EER model will be mapped to an XML Schema Definition Language (XSD) Graph as an XML conceptual schema using Semantic Transformation. Finally, the XSD Graph will be mapped into the XSD as an XML logical schema in the process of Forward Engineering, and the data semantics of participation, cardinality, generalization, aggregation, categorization, N-ary and U-ary relationship are preserved in the translated XML schema definition. 
47|7||Designing secure databases|Security is an important issue that must be considered as a fundamental requirement in information systems development, and particularly in database design. Therefore security, as a further quality property of software, must be tackled at all stages of the development. The most extended secure database model is the multilevel model, which permits the classification of information according to its confidentiality, and considers mandatory access control. Nevertheless, the problem is that no database design methodologies that consider security (and therefore secure database models) across the entire life cycle, particularly at the earliest stages currently exist. Therefore it is not possible to design secure databases appropriately. Our aim is to solve this problem by proposing a methodology for the design of secure databases. In addition to this methodology, we have defined some models that allow us to include security information in the database model, and a constraint language to define security constraints. As a result, we can specify a fine-grained classification of the information, defining with a high degree of accuracy which properties each user has to own in order to be able to access each piece of information. The methodology consists of four stages: requirements gathering; database analysis; multilevel relational logical design; and specific logical design. The first three stages define activities to analyze and design a secure database, thus producing a general secure database model. The last stage is made up of activities that adapt the general secure data model to one of the most popular secure database management systems: Oracle9i Label Security. This methodology has been used in a genuine case by the Data Processing Center of Provincial Government. In order to support the methodology, we have implemented an extension of Rational Rose, including and managing security information and constraints in the first stages of the methodology. 
47|7||Requirements engineering for organizational transformation|Traditional approaches to requirements elicitation stress systematic and rational analysis and representation of organizational context and system requirements. This paper argues that the introduction of any computer-based system to an organization transforms the organization and changes the work patterns of the system's users in the organization. These changes interact with the users' values and beliefs and trigger emotional responses which are sometimes directed against the computer-based system and its proponents. The paper debunks myths about how smoothly such organizational transformations take place, describes case studies showing how organizational transformation really takes place, and introduces and confirms by case studies some guidelines for eliciting requirements and the relevant emotional issues for a computer-based system that is being introduced into an organization to change its work patterns. 
47|7||Automatic implementation of constraints in component based applications|Component-based software architectures have become one of the predominant solutions in the software technologies scenario. As well, constraints have been assuming an ever more relevant role in modeling distributed systems as long as business rules implementation, design-by-contract practice, and fault-tolerance requirements are concerned. Nevertheless, component developers are not sufficiently supported by existing tools to implement these features.We address such a deficiency and we propose some implementation patterns to translate constraint models into source code and two automatic tools: the Component Constraint Modeler (CoCoMod) and the Component Constraint Generator (CoCoGen). CoCoMod enables designers to specify both component interfaces and constraints as visual UML models and automatically generates textual models for interfaces and constraints. CoCoGen executes the implementation patters to translate constraint models into source code. A simple case study is presented in order to show an application of the proposed approach. 
47|8|http://www.sciencedirect.com/science/journal/09505849/47/8|From requirements negotiation to software architecture decisions|Architecture design and requirements negotiations are conceptually tightly related but often performed separately in real-world software development projects. As our prior case studies have revealed, this separation causes uncertainty in requirements negotiation that hinders progress, limits the success of architecture design, and often leads to wasted effort and substantial re-work later in the development life-cycle. Explicit requirements elicitation and negotiation is needed to be able to appropriately consider and evaluate architecture alternatives and the architecture alternatives need be understood during requirements negotiation. This paper propose the WinCBAM framework, extending an architecture design method, called cost benefit analysis method (CBAM) framework to include an explicit requirements negotiation component based on the WinWin methodology. We then provide a retrospective case study that demonstrates the use of the WinCBAM. We show that the integrated method is substantially more powerful than the WinWin and CBAM methods performed separately. The integrated method can assist stakeholders to elicit, explore, evaluate, negotiate, and agree upon software architecture alternatives based on each of their requirement Win conditions. By understanding the architectural implication of requirements they can be negotiated more successfully: potential requirements conflicts can be discovered or alleviated relatively early in the development life-cycle. 
47|8||A lightweight approach for migrating web frameworks|Web application development frameworks, like the Java Server Pages framework (JSP), provide web applications with essential functions such as maintaining state information across the application and access control. In the fast paced world of web applications, new frameworks are introduced and old ones are updated frequently. A framework is chosen during the initial phases of the project. Hence, changing it to match the new requirements and demands is a cumbersome task.We propose an approach (based on Water Transformations) to migrate web applications between various web development frameworks. This migration process preserves the structure of the code and the location of comments to facilitate future manual maintenance of the migrated code. Consequently, developers can move their applications to the framework that meets their current needs instead of being locked into their initial development framework. We give an example of using our approach to migrate a web application written using the Active Server Pages (ASP) framework to the Netscape Server Pages (NSP) framework. 
47|8||Deriving objects from use cases in real-time embedded systems|In recent years, a number of use case-driven processes have emerged for the development of real-time embedded systems. In these processes, once requirements have been defined by use cases, the next step is usually to identify from that use cases, the central objects in the system and describing how they interact with one another. However, identifying objects/classes from the requirements is both a critical and hard task. This is mainly due to the lack of pragmatic technique that steers such a task. In this article, we present a systematic approach to identify objects from the use case model for the real-time embedded systems. After hierarchically decomposing the system into its parts, we first transform the use case structured-text style into an activity diagram, which may be reused in the next development activities. Second, we use the derived activity diagram for identifying objects. With the behavioural model, an object model can be viewed as a first cut at a design model, and is thus an essential input when the system is shaped in design and design implementation. 
47|8||The impacts of quality and productivity perceptions on the use of software process improvement innovations|Numerous software process improvement (SPI) innovations have been proposed to improve software development productivity and system quality; however, their diffusion in practice has been disappointing. This research investigates the adoption of the Personal Software Process on industrial software projects. Quantitative and qualitative analyses reveal that perceived increases in software quality and development productivity, project management benefits, and innovation fit to development tasks, enhance the usefulness of the innovation to developers. Results underscore the need to enrich current technology acceptance models with these constructs, and serve to encourage project managers to adopt formal SPI methods if developers perceive the methods will have positive impacts on their productivity and system quality. 
47|8||Embedded System Paranoia: a tool for testing embedded system arithmetic|The quality of arithmetic implementation is of concern to all who work with or depend on the results of numerical computations. Embedded systems have become enormously complicated and widespread in most if not all consumer devices in recent years so there is a clear need to measure the quality of the arithmetic in the same way that conventional systems have been measured for some time using programs such as the well-known paranoia. A new version of paranoia has been introduced specifically to extend the domain of testable systems to embedded control systems. This paper describes the development of ESP (Embedded System Paranoia) and gives example outputs and free download sites. The example outputs indicate that even today, the quality of arithmetic implementations cannot be taken for granted with numerous implementation problems being reported in those embedded environments tried so far. 
47|8||iJob: an Internet-based job execution environment using asynchronous messaging|Various toolkits exist today for the distributed execution of computational algorithms on clusters of machines. These toolkits are often referred to by the terms ‘Grid Toolkits’, ‘Job Execution Environments’, and ‘Problem Solving Environments (PSEs)’. Here, we introduce iJob—an Internet-based job execution environment that sets out to meet many of the goals of PSEs, such as providing facilities and services to solve a class of problems. In addition, the iJob software allows execution of computational algorithms utilizing standard Internet technologies such as Java, XML, and asynchronous communication protocols. The goals of this project include: (1) deploying the toolkit easily to multiple platforms using the Java technologies; (2) running multiple types of algorithms and supporting multiple users simultaneously; (3) providing a web-based GUI for monitoring and controlling the status of jobs; and (4) providing security at both the user-level and at the network-level. The toolkit has been tested using several simulation codes on pools of Windows 2000 and Solaris systems. 
47|9|http://www.sciencedirect.com/science/journal/09505849/47/9|From timetabling to train regulationâa new train operation model|A new train operation model proposed here not only considers the flexibility of train regulation, or train rescheduling problem, but also the objectives of timetabling process. A genetic algorithm is applied to solve this problem efficiently. Thus no matter the problem is planning the timetable of trains, the unusual passenger flow occurrence or the incident caused delay, our model will make the train regulation as the same as the timetable construction. This will simplify the work of administration. Our model also shows that once the delay occurred, the waiting time of the passengers will be the cost to delay every train. If the delay is not large enough, the system can have some rooms for removing the influence of the delay through our model. 
47|9||How to steer an embedded software project: tactics for selecting the software process model|Modern large new product developments (NPD) are typically characterized by many uncertainties and frequent changes. Often the embedded software development projects working on such products face many problems compared to traditional, placid project environments. One of the major project management decisions is then the selection of the project's software process model. An appropriate process model helps coping with the challenges, and prevents many potential project problems. On the other hand, an unsuitable process choice causes additional problems. This paper investigates the software process model selection in the context of large market-driven embedded software product development for new telecommunications equipment. Based on a quasi-formal comparison of publicly known software process models including modern agile methodologies, we propose a process model selection frame, which the project manager can use as a systematic guide for (re)choosing the project's process model. A novel feature of this comparative selection model is that we make the comparison against typical software project problem issues. Some real-life project case examples are examined against this model. The selection matrix expresses how different process models answer to different questions, and indeed there is not a single process model that would answer all the questions. On the contrary, some of the seeds to the project problems are in the process models themselves. However, being conscious of these problems and pitfalls when steering a project enables the project manager to master the situation. 
47|9||Web cache management based on the expected cost of web objects|With the recent explosion in usage of the World Wide Web, Web caching has become increasingly important. However, due to the non-uniform cost/size property of data objects in this environment, design of an efficient caching algorithm becomes an even more difficult problem compared to the traditional caching problems. In this paper, we propose the Least Expected Cost (LEC) replacement algorithm for Web caches that provides a simple and robust framework for the estimation of reference probability and fair evaluation of non-uniform Web objects. LEC evaluates a Web object based on its cost per unit size multiplied by the estimated reference probability of the object. This results in a normalized assessment of the contribution to the cost-savings ratio, leading to a fair replacement algorithm. We show that this normalization method finds optimal solution under some assumptions. Trace-driven simulations with actual Web cache logs show that LEC offers the performance of caches more than twice its size compared with other algorithms we considered. Nevertheless, it is simple, having no parameters to tune. We also show how the algorithm can be effectively implemented as a Web cache replacement module. 
47|9||The application of use cases in systems analysis and design specification|This paper begins by reviewing the application of use cases in the analysis and design phases of software development. At present, a use case derived in analysis is generally mapped into design through the synthesis of object behaviour for all scenarios associated with the use case. Hence the use case level of abstraction is not directly used in this process and a semantic gap exists between analysis and design. With informal textually based use case definitions this is to be expected, however, if the use cases themselves are given a more concrete structure, for example in the form of a statechart, then their direct use becomes more feasible.In this paper we therefore investigate the application of use case structures in the initial design phases of software development. A novel approach is proposed that applies a state based use case model directly to each object in the design architecture. This requires the derivation of a set of repeatable refinement procedures, which remove redundancy and allow the assignment of behaviour to objects with respect to their responsibilities. It is then shown how such procedures may be used in design, filling the semantic gap between analysis and design. By applying the procedures to a case study we identify and evaluate the characteristics of the mapping from use case model to object behaviour and review our approach with respect to other methods. It is concluded that state based use case structures not only represent a succinct analysis format, but may also be used to map analysis models directly into the design process. 
48|1|http://www.sciencedirect.com/science/journal/09505849/48/1|Modelling software development across time zones|Economic factors and the World Wide Web are turning software usage and its development into global activities. Many benefits accrue from global development not least from the opportunity to reduce time-to-market through ‘around the clock’ working.This paper identified some of the factors and constraints that influence time-to-market when software is developed across time zones. It describes a model of the relationships between development time and the factors and overheads associated with such a pattern of work. The paper also reports on a small-scale empirical study of software development across time zones and presents some lessons learned and conclusions drawn from the theoretical and empirical work carried out. 
48|1||XQuery speedup by deploying structural redundancy in mapping XML into relations|In designing a relational schema, we often consider that an attribute of a table is replicated into other table to reduce the join cost. Maybe such a possible redundancy will be grasped through E/R model (i.e. semantic analysis). Similarly, in mapping XML into relations, we can consider some redundancies to enhance query performance and they can be grasped through the structural traits of DTD (or XML schema). Several practical structural redundancies are formulated in this paper. If given XML data and queries are very large and complex, finding essential replications may also be difficult, and two efficient search methods are introduced for helping the search. Since the search problem is NP-hard, the methods are heuristically designed. Finally, read and update query costs arising by employing the structural redundancy are analyzed experimentally and the efficiency of two search methods is analyzed. They showed that the replication strategy can be very useful. 
48|1||Testing web applications|Traditional testing techniques are not adequate for web-based applications, since they miss their additional features such as their multi-tier nature, hyperlink-based structure, and event-driven feature. Limited work has been done on testing web applications. In this paper, we propose new techniques for white box testing of web applications developed in the .NET environment with emphasis on their event-driven feature. We extend recent work on modeling of web applications by enhancing previous dependence graphs and proposing an event-based dependence graph model. We apply data flow testing techniques to these dependence graphs and propose an event flow testing technique. Also, we present a few coverage testing approaches for web applications. Further, we propose mutation testing operators for evaluating the adequacy of web application tests. 
48|1||Supporting use case based requirements engineering|Use cases that describe possible interactions involving a system and its environment are increasingly being accepted as effective means for functional requirements elicitation and analysis. In the current practice, informal definitions of use cases are used and the analysis process is manual. In this paper, we present an approach supported by a tool for use cases based requirements engineering. Our approach includes use cases formalization, a restricted form of natural language for use cases description, and the derivation of an executable specification as well as a simulation environment from use cases. 
48|1||An application of Bayesian network for predicting object-oriented software maintainability|As the number of object-oriented software systems increases, it becomes more important for organizations to maintain those systems effectively. However, currently only a small number of maintainability prediction models are available for object-oriented systems. This paper presents a Bayesian network maintainability prediction model for an object-oriented software system. The model is constructed using object-oriented metric data in Li and Henry's datasets, which were collected from two different object-oriented systems. Prediction accuracy of the model is evaluated and compared with commonly used regression-based models. The results suggest that the Bayesian network model can predict maintainability more accurately than the regression-based models for one system, and almost as accurately as the best regression-based model for the other system. 
48|10|http://www.sciencedirect.com/science/journal/09505849/48/10|Guest co-editorâs comments|
48|10||A quest for appropriate software fault models: Case studies on fault detection effectiveness of model-based test generation techniques|Model-based test generation (MBTG) is becoming an area of active research. These techniques differ in terms of (1) modeling notations used, and (2) the adequacy criteria used for test generation. This paper (1) reviews different classes of MBTG techniques at a conceptual level, and (2) reports results of three case studies comparing various techniques in terms of their fault detection effectiveness. Our results indicate that MBTG technique which employs mutation and explicitly generates state verification sequences has better fault detection effectiveness than those based on boundary values, and predicate coverage criteria for transitions. Instead of a default adequacy criteria, certain techniques allow the user to specify test objectives in addition to the model. Our experience indicates that the task of defining appropriate test objectives is not intuitive. Furthermore, notations provided to describe such test objectives may have inadequate expressive power. We posit the need for a suitable fault modeling notation which also treats domain invariants as first class entities. 
48|10||Prioritized interaction testing for pair-wise coverage with seeding and constraints|Interaction testing is widely used in screening for faults. In software testing, it provides a natural mechanism for testing systems to be deployed on a variety of hardware and software configurations. In many applications where interaction testing is needed, the entire test suite is not run as a result of time or budget constraints. In these situations, it is essential to prioritize the tests. Here, we adapt a “one-test-at-a-time” greedy method to take importance of pairs into account. The method can be used to generate a set of tests in order, so that when run to completion all pair-wise interactions are tested, but when terminated after any intermediate number of tests, those deemed most important are tested. In addition, practical concerns of seeding and avoids are addressed. Computational results are reported. 
48|10||Environment behavior models for automation of testing and assessment of system safety|This paper presents an approach to automatic scenario generation from environment behavior models for testing of real-time reactive systems. The model of behavior is defined as a set of events (event trace) with two basic relations: precedence and inclusion. The attributed event grammar (AEG) specifies possible event traces and provides a uniform approach for automatically generating and executing test cases. The environment model includes a description of hazardous states in which the system may arrive and makes it possible to gather statistics for system safety assessment. The approach is supported by a generator that creates test cases from the AEG models. We demonstrate the approach with a case study of a software prototype of the computer-assisted resuscitation algorithm for a safety-critical casualty intravenous fluid infusion pump. 
48|10||Extended state identification and verification using a model checker|This article presents a method for the application of model checking, i.e., verifying a finite state system against a given temporal specification, on the problem of generating test inputs. The generated test inputs allow state characterization, i.e., the identification of internal states of the software under test by observation of the input/output behavior only. A test model is derived semi-automatically from a given state-based specification and the testing goal is specified in terms of temporal logic. On the basis of these inputs, a model checking tool performs the testing input generation automatically. In consequence, the complexity of our approach is strongly depending on the input model, the testing goal, and the model checking algorithm, which is implemented in the used tool. The presented approach can be adapted with small changes to other model checking tools. It is a capable test generation method, whenever a finite state model of the software under test exists. Furthermore, it provides a descriptive view on state-based testing, which may be beneficial in other contexts, e.g., education and program comprehension. 
48|11|http://www.sciencedirect.com/science/journal/09505849/48/11|A longitudinal study of development and maintenance in Norway: Report from the 2003 investigation|The amount of work on application systems being taken up by maintenance activities (work done on an IT-system after being put in production) has been one of the arguments of those speaking about a ‘software crisis’. We have earlier investigated the applicability of this notion, and propose to rather look at the percentage of work being done on application portfolio upkeep (work made to keep up the functional coverage of the application system portfolio of the organization. This also includes the development of replacement systems), to assess the efficiency of the application systems support in an organisation. This paper presents the main results of a survey investigation performed in 2003 in 54 Norwegian organisations within this area. The amount of application portfolio upkeep is significantly higher than in a similar investigation conducted in 1993. The level of maintenance is smaller (although not significantly) than in another similar investigation conducted in 1998. There was a significant increase in both maintenance and application portfolio upkeep from 1993 to 1998, which could partly be attributed to be the extra maintenance and replacement-oriented work necessary to deal with the ‘year 2000 problem.’ As for the 2003 investigation, the slow IT-market in general seems to have influenced the results negatively seen from the point of view of application systems support efficiency in organization. 
48|11||Using an experimental study to develop group awareness support for real-time distributed collaborative writing|Supporting group awareness is vital for the success of real-time, distributed, collaborative writing systems. Many awareness mechanisms have been introduced, but highly effective solutions are few. The research presented in this paper focuses on the development of awareness mechanisms using an experimental study of synchronous distributed collaborative writing. Our study has made two major contributions to research on group awareness.First, the study compares the importance of different awareness elements in supporting group awareness for collaborative writing. The results of our Wilcoxon test on awareness elements identify the five most important elements, including “Being able to comment on what other users have done,” “Knowing what actions other users are currently taking,” “Providing a communication tool when audio is not available,” “Knowing other user's working areas in the document,” and “Knowing other user's tasks.”Second, the research proposes mechanisms corresponding to the above-mentioned five awareness elements. The mechanisms include Dynamic Task List (DTL), Modification Director (MD), Advanced Chat (AC) and Split Window View (SWV). These mechanisms provide support for various aspects of group awareness, and add many enhanced features to existing awareness mechanisms. For example, DTL presents high-level information about authors' responsibilities and the correlation between their work allocations. MD notifies users instantaneously whenever their work is modified by other authors. AC enhances communication between users by allowing them to attach document objects such as text and diagrams to a conversation message. And, SWV provides the views of other authors' working areas and viewing areas simultaneously. 
48|11||Identifying knowledge brokers that yield software engineering knowledge in OSS projects|Much research on open source software development concentrates on developer lists and other software repositories to investigate what motivates professional software developers to participate in open source software projects. Little attention has been paid to individuals who spend valuable time in lists helping participants on some mundane yet vital project activities. Using three Debian lists as a case study we investigate the impact of knowledge brokers and their associated activities in open source projects. Social network analysis was used to visualize how participants are affiliated with the lists. The network topology reveals substantial community participation. The consequence of collaborating in mundane activities for the success of open source software projects is discussed. The direct beneficiaries of this research are in the identification of knowledge experts in open source software projects. 
48|11||Optimization of analogy weights by genetic algorithm for software effort estimation|A reliable and accurate estimate of software development effort has always been a challenge for both the software industry and academia. Analogy is a widely adopted problem solving technique that has been evaluated and confirmed in software effort or cost estimation domains. Similarity measures between pairs of effort drivers play a central role in analogy-based estimation models. However, hardly any research has addressed the issue of how to decide on suitable weighted similarity measures for software effort drivers. The present paper investigates the effect on estimation accuracy of the adoption of genetic algorithm (GA) to determine the appropriate weighted similarity measures of effort drivers in analogy-based software effort estimation models. Three weighted analogy methods, namely, the unequally weighted, the linearly weighted and the nonlinearly weighted methods are investigated in the present paper. We illustrate our approaches with data obtained from the International Software Benchmarking Standards Group (ISBSG) repository and the IBM DP services database. The experimental results show that applying GA to determine suitable weighted similarity measures of software effort drivers in analogy-based software effort estimation models is a feasible approach to improving the accuracy of software effort estimates. It also demonstrates that the nonlinearly weighted analogy method presents better estimate accuracy over the results obtained using the other methods. 
48|11||Empirical assessment of the impact of structural properties on the changeability of object-oriented software|The changeability of software can be viewed as the quality of being capable of change, which among others implies that the task of changing the software requires little effort. It is hypothesized that structural properties of the software affect changeability, in which case measures of such properties can be used as changeability indicators.Ways in which structural properties of the software can be measured are described and empirically validated based on data collected from an industrial Java development project. The measures are validated by using them as candidate variables in a multivariate regression model of the actual effort required to make modifications to the evolving software system.The results suggest that some measures that combine existing structural attribute measures with a weighting factor based on the relative proportion of change in each class can explain a large amount of the variation in change effort. This constitutes initial, empirical evidence that the proposed measures are valid changeability indicators. Consequently, they may help designers to identify and correct design problems during the development and maintenance of object-oriented software. 
48|11||Bayesian statistical effort prediction models for data-centred 4GL software development|Constructing an accurate effort prediction model is a challenge in Software Engineering. This paper presents three Bayesian statistical software effort prediction models for database-oriented software systems, which are developed using a specific 4GL toolsuite. The models consist of specification-based software size metrics and development team's productivity metric. The models are constructed based on the subjective knowledge of human expert and calibrated using empirical data collected from 17 software systems developed in the target environment. The models' predictive accuracy is evaluated using subsets of the same data, which were not used for the models' calibration. The results show that the models have achieved very good predictive accuracy in terms of MMRE and pred measures. Hence, it is confirmed that the Bayesian statistical models can predict effort successfully in the target environment. In comparison with commonly used multiple linear regression models, the Bayesian statistical models'predictive accuracy is equivalent in general. However, when the number of software systems used for the models' calibration becomes smaller than five, the predictive accuracy of the best Bayesian statistical models are significantly better than the multiple linear regression model. This result suggests that the Bayesian statistical models would be a better choice when software organizations/practitioners do not posses sufficient empirical data for the models' calibration. The authors expect these findings to encourage more researchers to investigate the use of Bayesian statistical models for predicting software effort. 
48|11||An empirical study of relationships among extreme programming engineering activities|Extreme programming (XP) is an agile software process that promotes early and quick production of working code. In this paper, we investigated the relationship among three XP engineering activities: new design, refactoring, and error fix. We found that the more the new design performed to the system the less refactoring and error fix were performed. However, the refactoring and error fix efforts did not seem to be related. We also found that the error fix effort is related to number of days spent on each story, while new design is not. The relationship between the refactoring effort and number of days spent on each story was not conclusive. 
48|11||On using cache conscious clustering for improving OODBMS performance|The two main techniques of improving I/O performance of Object Oriented Database Management Systems (OODBMS) are clustering and buffer replacement. Clustering is the placement of objects accessed near to each other in time into the same page. Buffer replacement involves the selection of a page to be evicted, when the buffer is full. The page evicted ideally should be the page needed least in the future. These two techniques both influence the likelihood of a requested object being memory resident. We believe an effective way of reducing disk I/O is to take advantage of the synergy that exists between clustering and buffer replacement. Hence, we design a framework, whereby clustering algorithms incorporating buffer replacement cache behaviour can be conveniently employed for enhancing the I/O performance of OODBMS. We call this new type of clustering algorithm, Cache Conscious Clustering (C3). In this paper, we present the C3 framework, and a C3 algorithm that we have developed, namely C3-GP. We have tested C3-GP against three well known clustering algorithms. The results show that C3-GP out performs them by up to 40% when using popular buffer replacement algorithms such as LRU and CLOCK. C3-GP offers the same performance as the best existing clustering algorithm when the buffer size compared to the database size is very small. 
48|11||Test diversity|This paper describes a novel method for measuring the degree to which a set of test cases executes a given program in diverse ways with respect to the two fundamental programming concepts: control and data. Test diversity is a method for measuring the variety of software control flow and data flow, comprising of four new measures: conditional diversity, data diversity, standard deviation of diversity, and test orthogonality. These closely related measures could be used to evaluate the test effectiveness and the test-effort distribution of a test suite.The Diversity Analyzer is a novel industrial-strength testing tool that can currently perform diversity analysis on software written in C/C++/C#/VB/Java in Windows and .NET environments. The Diversity Analyzer is used to evaluate the fault-detection effectiveness of Test Diversity on various types of industrial projects. 
48|11||Modelling Spatial WholeâPart relationships using an ISO-TC211 conformant approach|The important role of Spatial Whole–Part relationships in spatial database design is widely recognized and several researches have proposed specific spatial models to classify these relationships and their related topological constraints.The recent ISO-TC211 standards have defined a spatial data model and its use in spatial database design. However, the modelling of topological constraints of Spatial Whole–Part relationships requires additional complex and counterintuitive expressions specified by using a formal constraint language.This paper shows the complexity of modelling Spatial Whole–Part relationships in ISO and proposes an approach for the definition of ISO conformant modelling abstractions which hide this kind of complexity to the database designer. This approach is applied to the definition of the modelling abstractions for Spatial Whole–Part relationships which cover all the Spatial Whole–Part relationships proposed in the literature. 
48|11||VBP: An approach to modelling process variety and best practice|The concept of best practice is both attractive and highly problematic. Whilst organisations can learn from the practices of others there is also a danger that local variety may be squeezed out and that “one size fits all” solutions may stifle local context-specific innovations. This paper outlines an approach to modelling process specialisation hierarchies and best practice patterns with the Unified Modelling Language (UML). The process Variety and Best Practice approach, VBP, is applied to a recent e-government project that explored variety and best practice in citizens’ access portals for four UK local authorities. 
48|11||An analysis of web services support for dynamic business process outsourcing|Outsourcing of business processes is crucial for organizations to be effective, efficient and flexible. In fast changing markets, dynamic outsourcing is required, in which business relationships are established and enacted on-the-fly in an adaptive, fine-grained way. This requires automated means for the establishment of outsourcing relationships and for the enactment of services performed in these relationships. Due to wide industry support and their model of loose coupling, Web Services have become the mechanism of choice to interconnect organizations. This paper analyzes Web Services support for the dynamic process outsourcing paradigm. We discuss contract-based outsourcing to define requirements, introduce the Web Services framework and investigate the match between the two. We observe that the framework requires further support for cross-organizational business processes and mechanisms for contracting, QoS management and transaction management. We suggest an approach to fill these gaps based on a business process support application layer implemented on Web Service technology. 
48|12|http://www.sciencedirect.com/science/journal/09505849/48/12|Assuring quality of web-based applications|
48|12||Data flow analysis and testing of JSP-based Web applications|Web applications often rely on server-side scripts to handle HTTP requests, to generate dynamic contents, and to interact with other components. The server-side scripts are usually not checked by any compiler and, hence, can be error-prone. In this paper, we adapt traditional data flow testing techniques into the context of Java Server Pages (JSP), a very popular server-side script for developing Web applications with Java Technology. We point out that the JSP implicit objects and action tags can introduce several unique data flow test artifacts which need to be addressed. A test model is presented to capture the data flow information of JSP pages with considerations of various implicit objects and action tags. Based on the test model, we describe an approach to compute the intraprocedural, interprocedural, and sessional data flow test paths for uncovering the data anomalies of JSP pages. 
48|12||Binary analysis and automated hot patching for Web-based applications|Patching technologies are commonly applied to improve the dependability of software after release. This paper reports the design of an automated hot patching (AHP) framework that fully automates reasoning for the causes of failures and patching the binary code of Web-based applications. AHP admits the hardness for rooting out all faults before product release, and autonomously patches problems of application programs. By directly operating on binary code, AHP is universal to virtually all applications. A promising application of AHP is to shortcut a function of the remote maintenance center (RMC) and hence to reduce the turn around time for patches. 
48|12||An agent-based data-flow testing approach for Web applications|In recent years, Web applications (WAs) have grown so quickly that they have already become crucial to the success of businesses. However, since they are built on Internet and open standard technologies, WAs possess their own unique features, such as dynamic behaviors, heterogeneous representations, and novel data handling mechanisms. These features provide concrete support to the success of WAs, but they bring new challenges to researchers and developers, especially in regard to testing WAs and ensuring their quality. Testing approaches for non-WAs have to be extended to handle these features before they are used in WA testing. This paper presents an agent-based approach to perform data-flow testing of WAs. More precisely, the data-flow testing will be performed by autonomous test agents at the method level, object level, and object cluster level, from low abstraction level to high abstraction level. In the process of the recommended data-flow testing, an agent-based WA testing system (WAT) will automatically generate and coordinate test agents to decompose the task of testing an entire WA into a set of subtasks that can be accomplished by test agents. The test agents, rooted in the Belief–Desire–Intention (BDI) model, cooperate with each other to complete the testing of a WA. An example is used to show the feasibility of the proposed approach. 
48|12||Testing Web-based applications: The state of the art and future trends|Software testing is a difficult task and testing Web-based applications may be even more difficult, due to the peculiarities of such applications. In the last years, several problems in the field of Web-based applications testing have been addressed by research work, and several methods and techniques have been defined and used to test Web-based applications effectively. This paper will present the main differences between Web-based applications and traditional ones, how these differences impact the testing of the former ones, and some relevant contributions in the field of Web application testing developed in recent years. The focus is mainly on testing the functionality of a Web-based application, even if some discussion about the testing of non-functional requirements is provided too. Some indications about future trends in Web application testing are also outlined in the paper. 
48|12||Code-coverage guided prioritized test generation|Most automatic test generation research focuses on generation of test data from pre-selected program paths or input domains or program specifications. This paper presents a methodology for a full solution to code-coverage-based test case generation, which includes code coverage-based path selection, test data generation and actual test case representation in program’s original languages. We implemented this method in an automatic testing framework, eXVantage. Experimental results and industrial trials show that the framework is able to generate tests to achieve program line coverage from 20% to 98% with reduced overall testing effort. Our major contributions include an innovative coverage-based program prioritization algorithm, a novel path selection algorithm that takes into consideration program priority and functional calling relationship, and a constraint solver for test data generation that derives constraints from bytecode and solves complex constraints involving strings and dynamic objects. 
48|12||Referree List|
48|12||Author Index|
48|12||Keyword Index|
48|12||Volume Contents|
48|2|http://www.sciencedirect.com/science/journal/09505849/48/2|Bridging patterns: An approach to bridge gaps between SE and HCI|Adding usability improving solutions during late stage development is to some extent restricted by the software architecture. However, few software engineers and human–computer interaction engineers are aware of this important constraint and as a result avoidable rework is frequently necessary. In this paper we present a new type of pattern called a bridging pattern. Bridging patterns extend interaction design patterns by adding information on how to generally implement this pattern. Bridging patterns can be used for architectural analysis: when the generic implementation is known, software architects can assess what it means in their context and can decide whether they need to modify the software architecture to support these patterns. This may prevent part of the high costs incurred by adaptive maintenance activities once the system has been implemented and leads to architectures with better support for usability. 
48|2||Compacting XML documents|Nowadays, one of the most common formats for storing information is XML. The biggest drawback of XML documents is that their size is rather large compared to the information they store. XML documents may contain redundant attributes, which can be calculated from others. These redundant attributes can be deleted from the original XML document if the calculation rules can be stored somehow. In an Attribute Grammar environment there is an analog description for these rules: semantic rules. In order to use this technique in an XML environment we defined a new metalanguage called SRML. We have developed a method, which enables us to use this SRML metalanguage for compacting XML documents. After compaction it is possible to use XML compressors to make the compacted document much smaller. By using this combined approach we could achieve a significant size reduction compared to the compressed size of the XML specific compressors. This article extends the method published earlier to provide the possibility of automatically generating rules using machine learning techniques, with which it can find relationships between attributes which might not have been noticed by the user beforehand. 
48|2||MAPIS, a multi-agent system for information personalization|In the domain of multi-user and agent-oriented information systems, personalized information systems aim to give specific and customized responses to individual user requests. In addition to the ability to analyze user needs and to retrieve, understand and act on distributed data that is offered by any agent-oriented system, multi-agent systems also offer interesting possibilities for interaction, particularly with regard to information sharing and task coordination. Our approach exploits these interactive possibilities in order to make the system capable of personalizing information. In addition, reusable models at both the social and individual levels were chosen for this approach in order to facilitate subsequent information system design. With these two ideas in mind, several models of agent interaction (social) and the internal activity cycles (individual) have been proposed with the aim of creating a multi-agent system for information personalization. 
48|2||Partial rollback in object-oriented/object-relational database management systems with dual buffer|Partial rollback mechanism has been widely supported by many database management systems (DBMSs). It allows a transaction to be rolled back partially, that is, only back to a certain savepoint set by the user. A partial rollback, however, makes the DBMS buffer management complicated because it requires the DBMS to restore the state of not only the database but also the buffers. There are several literatures addressing such a partial rollback in a relational DBMS (RDBMS), which has page buffer only. However, to our knowledge, there exists no literature addressing it in an object-oriented/relational DBMS (OO/ORDBMS). The RDBMS partial rollback scheme cannot be applied to OO/ORDBMSs directly. The reason is that, unlike RDBMSs, many OO/ORDBMSs use dual buffer which consists of object buffer and page buffer. In this paper, we thoroughly study the partial rollback schemes for OO/ORDBMSs with dual buffer. For this, we propose four different partial rollback schemes which are based on (single) page buffer, (single) object buffer, dual buffer using a soft log, and dual buffer using shadows, respectively. The schemes proposed are practical enough to be implemented in a real OO/ORDBMS. The results of performance evaluations show that the dual buffer-based scheme using shadows achieves the best performance. 
48|3|http://www.sciencedirect.com/science/journal/09505849/48/3|Genetic algorithm based software integration with minimum software risk|This paper investigates an approach of integrating software with a minimum risk using Genetic Algorithms (GA). The problem was initially proposed by the need of sharing common software components among various departments within a same organization. Two significant contributions have been made in this study: (1) an assimilation exchange based software integration approach is proposed; (2) the software integration problem is formulated as a search problem and solved by using a GA. A case study is based on an on-going software integration project carried out in the Derbyshire Fire Rescue Service, and used to illustrate the application of the approach. 
48|3||The role of cultural diversity and leadership in computer-supported collaborative learning: a content analysis|Computer-mediated communication (CMC) technologies are increasingly being used to support collaborative learning in groups. Its potential to shift the traditional pedagogical paradigm triggers considerable amount of research. However, very few of the research studies focus on the social interactions and their influences on the learning process, which are crucial to understanding computer-supported collaborative learning (CSCL). This paper reports on a laboratory experiment with a 2×2 factorial design, conducted to investigate the influences of cultural diversity and leadership availability on the CSCL process using a content analysis approach. With the mediation of CMC systems, cultural diversity is found to engender more informational influences but reduce normative influences. Leadership has a positive effect on both normative and informational influences. Taking into account the learning outcomes, it is evident that the influences of the interaction process are closely related to CSCL effectiveness. 
48|3||Dynamic graphical UML views from formal B specifications|This paper addresses the graphical representation of the behaviour of B specifications, using state transition diagrams. These diagrams can help understand the specification for stakeholders who are not familiar with the B method, such as customers or certification authorities. The paper first discusses the principles of the graphical representation on a deterministic example, featuring a small set of states. It then discusses the representation of specifications which feature a large or infinite set of states, or which are non-deterministic. Abstraction techniques are used to overcome these difficulties. They result in a variety of possible representations. Finally, three techniques, based on animation and proof, are presented to help construct the diagrams. 
48|3||Dreamer: A resource management architecture for Jini federation|In the paper, we developed a resource management architecture for Jini federation. Jini introduces in a leasing concept that handles partial failure of distributed applications, and also supports service resource allocation. However, current leasing model does not lay a stress on fairness and priority issues on the resource allocation. On the other hand, current lookup service lacks the capability to handle sophisticated service assignment, such as load balance issue, non-functional requirement matching, urgent service request and so on. In this paper, we aim to solve the deficiency described above by providing an architecture, which uses the information contained in resource leases to achieve better resource management. 
48|3||Model for measuring quality of software in DVRS using the gap concept and fuzzy schemes with GA|A model of software quality is proposed to measure the quality of the software in a digital video recorder system (DVRS) during its development stage. The characteristics and metrics of this model are adopted from ISO/IEC 9126 and 14598. The model incorporates a Î»-fuzzy measure, a genetic algorithm and a hierarchical Choquet integral. It is based on the gap concept between perceive performance by the developers and satisfaction by the end-users, acquirers and evaluators of third parties in software development stage. A checklist about of the software quality is used to reduce the gap between the quality of the DVRS software quality as assessed by the developers and that as assessed by the end-users, acquirers and the evaluators of third parties. 
48|3||Testing spreadsheet accuracy theory|Electronic spreadsheets are used extensively to support financial analysis and problem solving processes; however, research has revealed that experienced professionals and students make many errors when developing spreadsheets. Practitioners recognize the importance of accuracy and have published many techniques for improving the accuracy of their spreadsheets. Their prescriptions and results of research are consistent and together these works form the basis for spreadsheet accuracy theory. Three propositions describe how the three constructs influence spreadsheet accuracy in a laboratory experiment. The results of this study indicate that the Spreadsheet Accuracy Theory developed three aids that significantly improve development of accurate spreadsheets. 
48|3||The relative influence of management control and userâIS personnel interaction on project performance|Research has failed to establish a conclusive link between levels of user involvement and information system project success. Communication and control theories indicate that the quality of interactions between users and inofrmation personnel may serve to better the coordinaton in a project and lead to greater success. A model is developed that directly relates management control to the quality of interaction and project success, with interaction quality as a potential intermediary. These variables provide a more distinct relationship to success as interactions are more structurally defined and controlled. A survey of 196 IS professionals provides evidence that management control techniques improve the quality of user–IS personnel interactions and eventual project success. These formal structures provide guidelines for managers in controlling the critical relations between users and IS personnel. 
48|4|http://www.sciencedirect.com/science/journal/09505849/48/4|DEVSpecL: DEVS specification language for modeling, simulation and analysis of discrete event systems|Discrete EVent Systems Specification (DEVS) formalism supports specification of discrete event models in a hierarchical modular manner. This paper proposes a DEVS modeling language called DEVS Specification Language (DEVSpecL) based on which discrete event systems are modeled, simulated and analyzed within a DEVS-based framework for seamless systems design. Models specified in DEVSpecL can be translated in different forms of codes by code generators, which are executed with various tools for models verification, logical analysis, performance evaluation, and others. 
48|4||Group cohesion in organizational innovation: An empirical examination of ERP implementation|Enterprise Resource Planning systems present unique difficulties in implementation in that they typically involve changes to the entire organization and are a novel application for the organization. These characteristics add to the importance of making groups more cohesive in their goals, commitment, and ability to work toward completion of the new system project. Such cohesiveness is built partly through the willingness of the team members to participate and commitment to learning the new system. To determine if these relationships hold, a survey of users and managers in Taiwan was conducted to test a model derived from social capital theory. The data support the positive relationships between group cohesion and both willingness to participate and commitment to learning. Group cohesion is likewise positively related to meeting management goals. Resources within an organization should support the climate of learning and the building of team participation. 
48|4||Mapping DTDs to relational schemas with semantic constraints|XML is becoming a prevalent format and standard for data exchange in many applications. With the increase of XML data, there is an urgent need to research some efficient methods to store and manage XML data. As relational databases are the primary choices for this purpose considering their data management power, it is necessary to research the problem of mapping XML schemas to relational schemas. The semantics of XML schemas are crucial to design, query, and store XML documents and functional dependencies are very important representations of semantic information of XML schemas. As DTDs are one of the most frequently used schemas for XML documents in these days, we will use DTDs as schemas of XML documents here. This paper proposes the concept and the formal definition of XML functional dependencies over DTDs. A method to map XML DTDs to relational schemas with constraints such as functional dependencies, domain constraints, choice constraints, reference constraints, and cardinality constraints over DTDs is given, which can preserve the structures of DTDs as well as the semantics implied by the above constraints over DTDs. The concepts and method of mapping DTDs to relational schemas presented in the paper can be extended to the field of XML Schema just with some modifications in related formal definitions. 
48|4||From a B formal specification to an executable code: application to the relational database domain|This paper presents a formal approach for the development of trustworthy database applications. This approach consists of three complementary steps. Designers start by modeling applications using UML diagrams dedicated to database applications domain. These diagrams are then automatically translated into B specifications suitable not only for reasoning about data integrity checking but also for the derivation of trustworthy implementations. In this paper, we present a process based on the B refinement technique for the derivation of a SQL relational implementation, embedded in the JAVA language (JAVA/SQL), from a B specification obtained by the first translation phase. 
48|4||An interactive service customization model|Mass customization has become one of the key strategies for a service provider to differentiate itself from its competitors in a highly segmented global service market. This paper proposes an interactive service customization model to support individual service offering for customers. In this model, not only that the content of an activity is customizable, but the process model can also be constructed dynamically according to the customer's requirements. Based on goal ontology, the on-demand customer requirements are transformed into a high-level service process model. Process components, which are building blocks for reusable standardized service processes, are designed to support on-demand process composition. The customer can incrementally define the customized service process through a series of operations, including activation of goal decomposition, reusable component selection, and process composition. In this paper, we first discuss the key requirements of the service customization problem. We then present in detail a knowledge-based customizable service process model and the accompanying customization method. Finally we demonstrate the feasibility of the our approach through a case study of the well-known travel planning problem and present a prototype system that enables users to interactively organize a satisfying travel plan. 
48|4||How large are software cost overruns? A review of the 1994 CHAOS report|The Standish Group reported in their 1994 CHAOS report that the average cost overrun of software projects was as high as 189%. This figure for cost overrun is referred to frequently by scientific researchers, software process improvement consultants, and government advisors. In this paper, we review the validity of the Standish Group's 1994 cost overrun results. Our review is based on a comparison of the 189% cost overrun figure with the cost overrun figures reported in other cost estimation surveys, and an examination of the Standish Group's survey design and analysis methods. We find that the figure reported by the Standish Group is much higher than those reported in similar estimation surveys and that there may be severe problems with the survey design and methods of analysis, e.g. the population sampling method may be strongly biased towards ‘failure projects’. We conclude that the figure of 189% for cost overruns is probably much too high to represent typical software projects in the 1990s and that a continued use of that figure as a reference point for estimation accuracy may lead to poor decision making and hinder progress in estimation practices. 
48|4||Software effort estimation terminology: The tower of Babel|It is well documented that the software industry suffers from frequent cost overruns. A contributing factor is, we believe, the imprecise estimation terminology in use. A lack of clarity and precision in the use of estimation terms reduces the interpretability of estimation accuracy results, makes the communication of estimates difficult, and lowers the learning possibilities. This paper reports on a structured review of typical software effort estimation terminology in software engineering textbooks and software estimation research papers. The review provides evidence that the term ‘effort estimate’ is frequently used without sufficient clarification of its meaning, and that estimation accuracy is often evaluated without ensuring that the estimated and the actual effort are comparable. Guidelines are suggested on how to reduce this lack of clarity and precision in terminology. 
48|5|http://www.sciencedirect.com/science/journal/09505849/48/5|Evaluation and Assessment in Software Engineering (EASE 05)|
48|5||Assessing multiview framework (MF) comprehensibility and efficiency: A replicated experiment|
48|5||Process improvement for small firms: An evaluation of the RAPID assessment-based method|With increasing interest by the software development community in software process improvement (SPI), it is vital that SPI programs are evaluated and the reports of lessons learned disseminated. This paper presents an evaluation of a program in which low-rigour, one-day SPI assessments were offered at no cost to 22 small Australian software development firms. The assessment model was based on ISO/IEC 15504 (SPICE). About 12 months after the assessment, the firms were contacted to arrange a follow-up meeting to determine the extent to which they had implemented the recommendations from the assessment.Comparison of the process capability levels at the time of assessment and the follow-up meetings revealed that the process improvement program was effective in improving the process capability of 15 of these small software development firms. Analysis of the assessment and follow-up reports explored important issues relating to SPI: elapsed time from assessment to follow-up meeting, the need for mentoring, the readiness of firms for SPI, the role of the owner/manager, the advice provided by the assessors, and the need to record costs and benefits. Based on an analysis of the program and its outcomes, firms are warned not to undertake SPI if their operation is likely to be disrupted by events internal to the firm or in the external environment. Firms are urged to draw on the expertise of assessors and consultants as mentors, and to ensure the action plan from the assessment is feasible in terms of the timeframe for evaluation. The RAPID method can be improved by fostering a closer relationship between the assessor and the firm sponsor; by making more extensive use of feedback questionnaires after the assessment and follow-up meeting; by facilitating the collection and reporting of cost benefit metrics; and by providing more detailed guidance for the follow-up meeting.As well as providing an evaluation of the assessment model and method, the outcomes from this research have the potential to better equip practitioners and consultants to undertake software process improvement, hence increasing the success of small software development firms in domestic and global markets. 
48|5||A preliminary study on the impact of a pair design phase on pair programming and solo programming|The drawback of pair programming is the nearly doubled personnel cost. The extra cost of pair programming originates from the strict rule of extreme programming where every line of code should be developed by a pair of developers. Is this rule not a waste of resources? Is it not possible to gain a large portion of the benefits of pair programming by only a small fraction of the meeting time of a pair programming session? We conducted a preliminary study to answer this question by splitting the pair programming process into a pair design and a pair implementation phase. The pair implementation phase is compared to a solo implementation phase, which in turn was preceeded by a pair design phase, as well. The study is preliminary as its major goal was to identify an appropriate sample size for subsequent experiments. The data from this study suggest that there is no difference in terms of development cost between a pair and a solo implementation phase if the cost for developing programs of similar level of correctness is concerned. 
48|5||Trust in software outsourcing relationships: An empirical investigation of Indian software companies|This paper investigates trust in software outsourcing relationships. The study is based on an empirical investigation of eighteen high maturity software vendor companies based in India. Our analysis of the literature suggests that trust has received a lot of attention in all kinds of business relationships. This includes inter-company relationships, whether cooperative ventures or subcontracting relationships, and relationship among different parts of a single company. However, trust has been relatively under-explored in software outsourcing relationships. In this paper, we present a detailed empirical investigation of trust in commercial software outsourcing relationships. The investigation presents what vendor companies perceive about getting trust from client companies in outsourcing relationships. We present the results in two parts—(1) achieving trust initially in outsourcing relationships and (2) maintaining trust in ongoing outsourcing relationships. Our findings confirm that the critical factors to achieving trust initially in an outsourcing relationship include previous clients' reference and experience of vendor in outsourcing engagements. Critical factors identified for maintaining trust in an established outsourcing relationship include transparency, demonstrability, honesty, process followed and commitment. Our findings also suggest that trust is considered to be very fragile in outsourcing relationships. 
48|6|http://www.sciencedirect.com/science/journal/09505849/48/6|Special Issue Editorial: WAMIS 2005 Workshop|
48|6||Efficient mining and prediction of user behavior patterns in mobile web systems|The development of wireless and web technologies has allowed the mobile users to request various kinds of services by mobile devices at anytime and anywhere. Helping the users obtain needed information effectively is an important issue in the mobile web systems. Discovery of user behavior can highly benefit the enhancements on system performance and quality of services. Obviously, the mobile user's behavior patterns, in which the location and the service are inherently coexistent, become more complex than those of the traditional web systems. In this paper, we propose a novel data mining method, namely SMAP-Mine that can efficiently discover mobile users' sequential movement patterns associated with requested services. Moreover, the corresponding prediction strategies are also proposed. Through empirical evaluation under various simulation conditions, SMAP-Mine is shown to deliver excellent performance in terms of accuracy, execution efficiency and scalability. Meanwhile, the proposed prediction strategies are also verified to be effective in measurements of precision, hit ratio and applicability. 
48|6||Object-relational complex structures for XML storage|XML data can be stored in various database repositories, including Object-Relational Database (ORDB). Using an ORDB, we get the benefit of the relational maturity and the richness of Object-Oriented modeling, including various complex data types. These data types resemble the true nature of XML data and therefore, the conceptual semantic of XML data can be preserved. However, very often when the data is stored in an ORDB repository, they are treated as purely flat tables. Not only do we not fully utilize the facilities in current ORDB, but also we do not preserve the conceptual semantic of the XML data.In this paper, we propose novel methodologies to store XML data into new ORDB data structures, such as user-defined type, row type and collection type. Our methodology has preserved the conceptual relationship structure in the XML data, including aggregation, composition and association. For XML data retrieval, we also propose query classification based on the current SQL.Compared to the existing techniques, this work has several contributions. Firstly, it utilizes the newest features of ORDB for storing XML data. Secondly, it covers a full database design process, from the conceptual to the implementation phase. Finally, the proposed transformation methodologies maintain the conceptual semantics of the XML data by keeping the structure of the data in different ORDB complex structures. 
48|6||FCVW agent framework|The rising cost and growing complexity of software development is a triggering force for the development of frameworks. Frameworks provide reusability of components and they have been developed for many domains. However, very few attempts have been made to develop agent frameworks for Collaborative Virtual Environments (CVEs). This paper presents processes used in developing an agent framework for Federated Collaborative Virtual Workspace (FCVW) based on Agent Oriented Software Engineering (AOSE) techniques. FCVW is an extension of MITRE's Collaborative Virtual Workspace (CVW). The main objective of this framework is to allow FCVW users to create software agents more easily. 
48|6||Performance evaluation of e-commerce requests in wireless cellular networks|Recent technological advances in mobile devices and wireless networks enable mobile users to order goods in an anywhere and anytime fashion. Quality of Service (QoS) provision is one of the most challenging issues in the heterogeneous wireless network-based e-commerce systems. Such e-commerce systems enable users to roam between different wireless networks operators and geographical areas while providing interactive broadband services and seamless connectivity. Due to movement of users during e-commerce requests, one of the most important QoS factors for successful completion of users' requests is related to handover of request from one cell to another. A handover could fail due to unavailability of sufficient bandwidth in the destination cell. Such failure of ongoing e-commerce requests is highly undesirable and can cause serious problems to the e-commerce users and the service providers. This paper proposes an enhanced priority queuing based handover scheme in order to ensure a seamless connectivity of e-commerce requests. It focuses on the performance anaylsis of the proposed scheme. Experimental study demonstrates that the proposed scheme provides QoS with low connection failure and mean response time for handover of e-commerce requests. 
48|6||Design and evaluation of a panoramic visualization environment on Semantic Web|Information visualization has been popularly applied to the Semantic Web to facilitate the presentation of data semantics. Since, the past visualization interfaces, based either on RDF or on Topic Maps, only present a partial view of the Semantic Web space, an integrated view would be needed to demonstrate the panorama and help users access the Semantic Web resources. We have presented an Integrated Semantic Web interactive visualization environment (ISWIVE) to incorporate topic features from Topic Maps into RDF, and showed that both the detailed resource descriptions and the overall topic relationships can be clearly visualized. In this paper, we present an improved ISWIVE interface that provides a mixed-model visualization in a clearer layout and supports a concurrent display of both RDF and Topic Maps views. Quantitative analyses were conducted to show the enhancement of the improved ISWIVE interface and the differences between the textual interface and the original ISWIVE prototype. 
48|6||Reaching consensus: A moderated fuzzy web services discovery method|Web services are used for developing and integrating highly distributed and heterogeneous systems in different domains such as e-business, grid services, and e-government systems. Web services discovery is a key to dynamically locating desired web services across the Internet. Prevailing research trend is to dynamically discover and compose web services in order to develop composite services that provide enhanced functionality. Existing discovery techniques do not take into account the diverse preferences and expectations of service consumers and providers which are generally used for searching or advertising web services. This paper presents a moderated fuzzy web service discovery approach to model subjective and fuzzy opinions, and to assist service consumers and providers in reaching a consensus. The method achieves a common consensus on the distinct opinions and expectations of service consumers and providers. This process is iterative such that further fuzzy opinions and preferences can be added to improve the precision of web service discovery. The proposed method is implemented as a prototype system and is tested through various experiments. Experimental results demonstrate the effectiveness of the proposed method. 
48|6||The implementation of a secure and pervasive multimodal Web system architecture|While most users currently access Web applications from Web browser interfaces, pervasive computing is emerging and offering new ways of accessing Internet applications from any device at any location, by utilizing various modes of interfaces to interact with their end users. The PC and its back-end servers remain important in a pervasive system, and the technology could involve new ways of interfacing with a PC and/or various types of gateways to back-end servers. In this research, cellular phone was used as the pervasive device for accessing an Internet application prototype, a multimodal Web system (MWS), through voice user interface technology.This paper describes how MWS was developed to provide a secure interactive voice channel using an Apache Web server, a voice server, and Java technology. Securing multimodal applications proves more challenging than securing traditional Internet applications. Various standards have been developed within a context of Java 2 Micro Edition (J2ME) platform to secure multimodal and wireless applications. In addition to covering these standards and their applicability to the MWS system implementation, this paper also shows that multimodal user-interface page can be generated by using XSLT stylesheet which transforms XML documents into various formats including XHTML, WML, and VoiceXML. 
48|7|http://www.sciencedirect.com/science/journal/09505849/48/7|On the relationship between two control-flow coverage criteria: all JJ-paths and MCDC|Coverage criteria may be used to assess the adequacy of software test data. Improved test data, that takes account of any inadequacies identified by lack of coverage, may then be developed. It is natural to seek ways of comparing different criteria and the ‘subsumes’ relationship is one such way: one criterion subsumes another, if satisfying the first always implies satisfaction of the second. This paper considers two criteria: ‘all jump-to-jump paths’ (all JJ-paths) and ‘modified condition/decision coverage’ (MCDC). It might be anticipated that there would be a relationship between these criteria since both are based on advanced control-flow concepts. MCDC has particular importance since it is involved in the DO-178B standard for avionics software. However, it is shown that ‘all JJ-paths’ and MCDC are, in general, incomparable, but for programs written under certain specific constraints ‘all JJ-paths’ subsumes MCDC. 
48|7||Towards an ontology-based approach for specifying and securing Web services|With the increasing popularity of Web services and increasing complexity of satisfying needs of users, there has been a renewed interest in Web services composition. Composition addresses the case of a user request that cannot be satisfied by any available Web service, whereas a composite service obtained by integrating Web services might be used. Because Web services originate from different providers, their composition faces the obstacle of the context heterogeneity of Web services. An unawareness or poor consideration of this heterogeneity during Web services composition and execution result in a lack of the quality and relevancy of information that permits tracking the composition, monitoring the execution, and handling exceptions. This paper presents an ontology-based approach for context reconciliation. The approach also focuses on the security breaches that threaten the integrity of the context of Web services, and proposes appropriate means to achieve this integrity. 
48|7||A framework for classifying and developing extensible architectural views|Despite its widespread use in the software architecture community, architectural views and relationships among them are poorly defined. A solid taxonomy of views is a critical factor in tackling this problem since it must adopt an unambiguous definition of views and provide rigorous criteria for classification. Nevertheless, the existing taxonomies of views fail to eliminate vagueness surrounding the definitions of views and their inter-relationships mainly due to their informal nature. One of the most significant consequences of these failures is inability to systematically define new views in support of domain-specialization. This paper is an attempt to resolve these outstanding problems by proposing a sound framework for creating new, customized taxonomies of views in a repeatable manner, based on the formal concept of refinement. 
48|7||An event-driven framework for inter-user communication applications|This paper presents an event-driven framework for inter-user communication applications, such as Internet gaming or chatting, that require frequent communication among users. This paper addresses two major blocking problems for event-driven programming for inter-user communication applications, namely output blocking and request blocking. For the former, an output buffering mechanism is presented to solve this problem. For the latter, a service requesting mechanism with helper processes is presented to solve this problem. The above two mechanisms are incorporated into the framework presented in this paper to facilitate application development. In practice, this framework has been applied to online game development. 
48|7||Comparison of software architecture reverse engineering methods|Problems related to interactions between components is a sign of problems with the software architecture of the system and are often costly to fix. Thus it is very desirable to identify potential architectural problems and track them across releases to see whether some relationships between components are repeatedly change-prone.This paper shows a study of combining two technologies for software architecture: architecture recovery and change dependency analysis based on version control information. More specifically, it describes a reverse engineering method to derive a change architecture from Revision Control System (RCS) change history. It compares this method to other reverse engineering methods used to derive software architectures using other types of data. These techniques are illustrated in a case study on a large commercial system consisting of over 800 KLOC of C, C++, and microcode. The results show identifiable problems with a subset of the components and relationships between them, indicating systemic problems with the underlying architecture. 
48|7||Applying Model-Driven Architecture to achieve distribution transparencies|This paper proposes a principled methodology for the realization of distribution transparencies. The proposed methodology is placed within the general context of Model-Driven Architecture (MDA) development. Specifically, it consists of a UML-based representation for the specification of platform independent models of a system. Moreover, it comprises an automated aspect-oriented method for the refinement of platform independent models into platform specific ones (i.e. models describing the realization of the system's distribution transparency requirements, based on a standard middleware platform like CORBA, J2EE, COM+, etc.). Finally, the proposed methodology includes an aspect-oriented method for the generation of platform specific code from platform specific models. 
48|7||Definition of a problem-sensitive conceptual modelling language: foundations and application to software engineering|A conceptual modelling language should provide constructors that can be used to represent the conceptualisation of a problem considering the problem domain. However, software engineering has traditionally focused on implementation concepts.This paper considers the appropriate generic conceptualisation theoretical aspects to identify the conceptual elements for which constructors have to be provided in a problem-sensitive conceptual modelling language. These elements match the formal definition of any conceptualisation and are derived from natural language. By looking at these elements, we have defined a conceptual modelling language that has been successfully applied in knowledge engineering and software engineering. 
48|7||A technique for expressing IT security objectives|At the specification phase, the developer of an IT security product identifies and documents applicable security objectives. Specifications are often intuitive and hard to assess and while being syntactically correct may still fail to appropriately capture the security problem addressed. A technique is proposed for expressing Common Criteria compliant security environments and security objectives for high assurance IT security products. The technique is validated by an analysis of the security specification for a device computing digital signatures within the European Union PKI framework. Modifications to the specification are proposed and the possibility of extending the CC treatment of security objectives is discussed. 
48|7||On coordinating personalized composite web services|This paper presents a research project on the coordination of personalized composite web services. By coordination, it is meant the mechanisms that specify the orchestration of the component web services of a composite web service. The orchestration is about the execution chronology of the component web services, the data that the component web services exchange, the states that the component web services take, and the actions that the component web services perform. By personalization, it is meant the integration of user preferences into the specification that orchestrates the component web services. These preferences concern when the component web services are to be executed. In this research project, the operations of coordination and personalization are entrusted to software agents, which, for instance, monitor the context surrounding users and trigger as a result the appropriate component web services. In addition, software agents engage in conversations with their peers when it comes to tracking the personalized component web services. 
48|7||Theory and algorithms for slicing unstructured programs|Program slicing identifies parts of a program that potentially affect a chosen computation. It has many applications in software engineering, including maintenance, evolution and re-engineering of legacy systems. However, these systems typically contain programs with unstructured control-flow, produced using goto statements; thus, effective slicing of unstructured programs remains an important topic of study.This paper shows that slicing unstructured programs inherently requires making trade-offs between three slice attributes: termination behaviour, size, and syntactic structure. It is shown how different applications of slicing require different tradeoffs. The three attributes are used as the basis of a three-dimensional theoretical framework, which classifies slicing algorithms for unstructured programs. The paper proves that for two combinations of these dimensions, no algorithm exists and presents algorithms for the remaining six combinations. 
48|7||The use and effects of an electronic process guide and experience repository: a longitudinal study|This paper presents a consolidated view of two evaluations on the use of an electronic process guide and experience repository within a small software development company. The use and effects of the tool were studied over a period of one and a half years, first for 6 months and then 1 year after its installation, for another 5 months. The tool was used regularly and in a consistent manner in both studies but declining usage was observed in the second study. The repository remained used to retrieve mostly examples and templates but the number of retrievals of anecdotal experiences, such as lessons learned had noticeably increased. Similar benefits such as time saving and improved documentation quality were observed in both studies, with additional benefits in the second study like improved project planning and cost estimation, and easier negotiation and traceability of altered or new system requirements with clients. The initial load that users experienced in learning to use the tool was not observed in the second study. The results show that tangible benefits can be realised quickly and continued to be experienced, leading to users having higher morale and more confidence in executing their tasks. 
48|7||Dynamic model for the system testing process|The approach for estimating and controlling the software testing effort presented in this paper is based on the theory of dynamical systems. The system testing process is modeled by a dynamical system to better understand its behaviour and to assist project and test managers in planning and tracking effort needs.The proposed model is based on worktime effort measurement and has been applied on three industrial software development projects data. In comparison to other models of the literature the worktime based system testing model describes the behaviour of the process more adequately. Consequently, decisions about the duration of system testing can be supported. Previous models from reliability modelling were selected for comparison. However, the emphasis here is placed on industrial experience on effort tracking and control. 
48|7||Automatic test data generation using genetic algorithm and program dependence graphs|The complexity of software systems has been increasing dramatically in the past decade, and software testing as a labor-intensive component is becoming more and more expensive. Testing costs often account for up to 50% of the total expense of software development; hence any techniques leading to the automatic generation of test data will have great potential to considerably reduce costs. Existing approaches of automatic test data generation have achieved some success by using evolutionary computation algorithms, but they are unable to deal with Boolean variables or enumerated types and they need to be improved in many other aspects. This paper presents a new approach utilizing program dependence analysis techniques and genetic algorithms (GAs) to generate test data. A set of experiments using the new approach is reported to show its effectiveness and efficiency based upon established criterion. 
48|7||Finding frequent itemsets over online data streams|Conventional data mining methods for finding frequent itemsets require considerable computing time to produce their results from a large data set. Due to this reason, it is almost impossible to apply them to an analysis task in an online data stream where a new transaction is continuously generated at a rapid rate. An algorithm for finding frequent itemsets over an online data stream should support flexible trade-off between processing time and mining accuracy. Furthermore, the most up-to-date resulting set of frequent itemsets should be available quickly at any moment. To satisfy these requirements, this paper proposes a data mining method for finding frequent itemsets over an online data stream. The proposed method examines each transaction one-by-one without any candidate generation process. The count of an itemset that appears in each transaction is monitored by a lexicographic tree resided in main memory. The current set of monitored itemsets in an online data stream is minimized by two major operations: delayed-insertion and pruning. The former is delaying the insertion of a new itemset in recent transactions until the itemset becomes significant enough to be monitored. The latter is pruning a monitored itemset when the itemset turns out to be insignificant. The number of monitored itemsets can be flexibly controlled by the thresholds of these two operations. As the number of monitored itemsets is decreased, frequent itemsets in the online data stream are more rapidly traced while they are less accurate. The performance of the proposed method is analyzed through a series of experiments in order to identify its various characteristics. 
48|7||Encapsulating windows-based software applications into reusable components with design patterns|Reusing software by integrating Commercial Off-The-Shelf (COTS) applications into a software system is maturing in practice. Our previous work [1] presented a component wrapping approach to convert Windows-based COTS applications into CORBA objects. A formal and generalized representation of the conversion process for a Windows-based COTS application into a reusable software component would be useful and desirable for applying such software reuse to COTS-based system development. This study addresses a pattern-based representation of our experience. The patterns in this study offer clear documentation and sufficient information for a software developer to develop a COTS-based software system rapidly. An example system, Graphic Mechanical Part Management System (GMPMS) assembling two COTS applications under MS-DOS and MS-Windows 2000/XP, respectively, is also developed in this study to reveal how the patterns are utilized. 
48|8|http://www.sciencedirect.com/science/journal/09505849/48/8|Towards a consistent terminology for software measurement|Although software measurement plays an increasingly important role in Software Engineering, there is no consensus yet on many of the concepts and terminology used in this field. Even worse, vocabulary conflicts and inconsistencies can be frequently found amongst the many sources and references commonly used by software measurement researchers and practitioners. This article presents an analysis of the current situation, and provides a comparison framework that can be used to identify and address the discrepancies, gaps, and terminology conflicts that current software measurement proposals present. A basic software measurement ontology is introduced, that aims at contributing to the harmonization of the different software measurement proposals and standards, by providing a coherent set of common concepts used in software measurement. The ontology is also aligned with the metrology vocabulary used in other more mature measurement engineering disciplines. 
48|8||A method for assigning a value to a communication protocol test case|One of the main problems in industrial testing is the enormous number of test cases derived from any complex communication protocol. Due to budget constraints and tight schedule, the number of test cases has to be within a certain limit. However, by having a limit on the number of test cases, it raises some issues. For instances, what criteria should be used for selecting the test cases? How can we ensure that important test cases have not been excluded? We are proposing that assigning a value to each of the test cases of a test suite can provide a solution. By doing so, the relative importance of each of the test cases can be ranked and an optimal test suite can then be designed. The value of a test case is to be measured in economic terms, which could be based on the probability that a particular case will occur, and the probability that an error is likely to be uncovered. This paper presents a method for assigning a value to a test case of a communication protocol; it is based on sensitivity analysis, which involves execution, infection and propagation probabilities. To illustrate the method, the results of applying it to the INRES protocol are presented. 
48|8||Design dysphasia and the pattern maintenance cycle|Software developers utilize design methods that enable them to manipulate conceptual structures that correlate to programming language features. However, programming languages and the programming paradigms they embody co-evolve over time. Within industrial and academic circles, for example, object-oriented programming has evolved and effectively replaced imperative programming. More recently, many object-oriented languages have assimilated features from other programming paradigms, evolving into multiparadigm languages we refer to as ‘object-oriented plus–plus’ or OO++. This language evolution may weaken the interface between design and implementation, introducing what we call ‘design dysphasia’—a partial disability in the use of a programming language because of incongruous design methods. Software design patterns capture elements of reusable design within a specific context. When the programming languages that are part of pattern context evolve, patterns must adapt to the language change or they may reinforce design dysphasia in the practitioner. We assert that the current ‘capture/recapture’ pattern maintenance model is suboptimal for adapting patterns to language evolution and propose a new ‘capture/modify/recapture’ maintenance cycle as a more effective approach. We then suggest a concrete ‘modify’ phase for current patterns to be adapted to object-oriented based multiparadigm language trends. We present an OO++ Iterator pattern as an example throughout. 
48|8||The collateral coverage of data flow criteria when branch testing|When exercising program code with test data in an attempt to satisfy a given testing criterion, there will be a concurrent accrual of coverage in respect of other testing criteria. Knowledge of the extent of such ‘collateral coverage’ can be used to advantage both in providing better estimates of the overheads entailed by the overall testing exercise, and in helping to determine an optimal sequence for the application of a set of testing methods.In this paper, the results deriving from a set of experiments are reported. The aim of the experiments was to investigate the extent of the collateral coverage that is achieved in respect of the data-flow testing criteria when branch testing is undertaken. 
48|8||The HWS hybrid web search|The widespread availability of machine understandable information on the Semantic Web offers some opportunities to improve traditional search. In this paper, we propose a hybrid web search architecture-HWS, which combines text search with semantic search to improve precision and recall. The components in HWS are described in detail, including several novel algorithms proposed to support the hybrid web search. 
48|8||Improving test quality using robust unique input/output circuit sequences (UIOCs)|In finite state machine (FSM) based testing, the problem of fault masking in the unique input/output (UIO) sequence may degrade the test performance of the UIO based methods. This paper investigates this problem and proposes the use of a new type of unique input/output circuit (UIOC) sequence for state verification, which may help to overcome the drawbacks that exist in the UIO based techniques. When constructing a UIOC, overlap and internal state observation schema are used to increase the robustness of a test sequence. Test quality is compared by using the forward UIO method (F-method), the backward UIO method (B-method) and the UIOC method (C-method) separately. Robustness of the UIOCs constructed by the algorithm given in this paper is also compared with those constructed by the algorithm given previously. Experimental results suggest that the C-method outperforms the F- and the B-methods and the UIOCs constructed by the algorithm given in this paper, are more robust than those constructed by other proposed algorithms. 
48|8||A high concurrency XPath-based locking protocol for XML databases|Providing efficient access to XML documents becomes crucial in XML database systems. More and more concurrency control protocols for XML database systems were proposed in the past few years. Being an important language for addressing data in XML documents, XPath expressions are the basis of several query languages, such as XQurey and XSLT. In this paper, we propose a lock-based concurrency control protocol, called XLP, for transactions accessing XML data by the XPath model. XLP is based on the XPath model and has the features of rich lock modes, low lock conflict and lock conversion. XLP is also proved to ensure conflict serializability. In sum, there are three major contributions in this paper. The proposed XLP supports most XPath axes, rather than simple path expressions only. Conflict conditions and rules in the XPath model are analyzed and derived. Moreover, a lightweighted lock mode, P-lock, is invented and integrated into XLP for better concurrency. 
48|8||Information flow control in multithread applications based on access control lists|Information flow control models prevent information leakage during the execution of an application. We developed a model OORBAC to control information flows in object-oriented systems. Soon after the development of OORBAC, we identified that the model cannot solve the problems induced by multithreaded applications. We thus adapted the concepts of OORBAC to develop a new information flow control model. It offers the features of OORBAC and solves the problems induced by multithread object-oriented applications. The new model is named MtACL (information flow control model for multithreaded object-oriented applications based on access control lists). The multithreaded problems solved by MtACL include the shared memory problem, the non-interference problem, and the combination leakage problem. This paper presents MtACL and proves that the model solves the multithreaded problems. 
48|8||A framework for anonymous but accountable self-organizing communities|In this paper we propose a novel architecture and approach to provide accountability for Web communities that require a high-level of privacy. A two-layered privacy protection architecture is proposed, that supports (i) registration of participants and enforcement of community rules, called internal accountability, and (ii) rule-based interaction with real world organizations, called external accountability. Our security protocols build upon community-based trust and limit the exposure of private data on trusted third parties.The two-layered architecture protects the mappings between real users and their virtual identities, and among the virtual users, while guaranteeing internal and external accountability. We target Web communities that are dynamic and self-organizing, i.e. roles and contributions of participants may change over time. The proposed concepts and protocols are implemented in our SyllabNet project that supports anonymous course evaluations by university students. 
48|8||A systematic review of statistical power in software engineering experiments|Statistical power is an inherent part of empirical studies that employ significance testing and is essential for the planning of studies, for the interpretation of study results, and for the validity of study conclusions. This paper reports a quantitative assessment of the statistical power of empirical software engineering research based on the 103 papers on controlled experiments (of a total of 5,453 papers) published in nine major software engineering journals and three conference proceedings in the decade 1993–2002. The results show that the statistical power of software engineering experiments falls substantially below accepted norms as well as the levels found in the related discipline of information systems research. Given this study's findings, additional attention must be directed to the adequacy of sample sizes and research designs to ensure acceptable levels of statistical power. Furthermore, the current reporting of significance tests should be enhanced by also reporting effect sizes and confidence intervals. 
48|8||Incremental mining of generator representation using border sets|Incremental frequent itemset mining refers to the maintenance and utilization of the knowledge discovered in the previous mining operations for later frequent itemset mining. This paper describes an incremental algorithm for maintaining the generator representation in dynamic datasets. The generator representation is a kind of lossless, concise representation of the set of frequent itemsets. It may be orders of magnitude smaller than the set of frequent itemsets. Furthermore, the algorithm utilizes a novel optimization based on generator borders for the first time in the literature. Generator borders are the borderline between frequent generators and other itemsets. New frequent generators can be generated through monitoring them. Extensive Experiments show that this algorithm is more efficient than previous solutions. 
48|9|http://www.sciencedirect.com/science/journal/09505849/48/9|Guest editorial for the special section on distributed software development|
48|9||An integration centric approach for the coordination of distributed software development projects|This paper presents an approach for Distributed Software Development (DSD) that is based on two foundations. The first one is an integration centric engineering process, which aims at managing crucial dependencies in DSD projects. The second foundation is a strategy for operationalizing the coordination of the engineering process. The purpose of this strategy is to simultaneously provide global information system support for coordination and achieve common understanding about what should be coordinated and how. The approach has been successfully used at Ericsson, a major supplier of telecommunication systems worldwide, for coordinating extraordinary complex projects developing nodes in the third generation of mobile systems. Although many obstacles have to be addressed, the results indicate that the approach is a viable way to manage DSD during very demanding circumstances. 
48|9||Essential communication practices for Extreme Programming in a global software development team|We conducted an industrial case study of a distributed team in the USA and the Czech Republic that used Extreme Programming. Our goal was to understand how this globally-distributed team created a successful project in a new problem domain using a methodology that is dependent on informal, face-to-face communication. We collected quantitative and qualitative data and used grounded theory to identify four key factors for communication in globally-distributed XP teams working within a new problem domain. Our study suggests that, if these critical enabling factors are addressed, methodologies dependent on informal communication can be used on global software development projects. 
48|9||Evaluation of composite object replication schemes for dependable server applications|Object oriented dependable server applications often rely on fault-tolerance schemes, which are comprised of different replication policies for the constituent objects (composite replication schemes). This paper introduces a simulation-based evaluation approach for quantifying the tradeoffs between fault-tolerance overhead and fault-tolerance effectiveness in composite replication schemes. Compared to other evaluation approaches: (a) we do not use the well-known reliability blocks based simulation, but a hybrid reliability and system's traffic simulation and (b) we make a clear distinction between the measures used for the fault-affected service response times from those used for the fault-unaffected ones. The first mentioned feature allows taking into account additional concerns other than fault-tolerance, like for example load balancing and multithreading. The second feature renders the proposed approach suitable for design studies that aim to determine either optimal replication properties for the constituent objects or quality of service (QoS) guarantees for the perceived service response times. We obtain results for a case system model, based on different assumptions on what happens when server-objects fail (loss scenarios). The presented results give insight in the design of composite method request-retry schemes with appropriate request timeouts. 
48|9||A comparison of two approaches to utilizing XML in parametric databases for temporal data|The parametric data model captures an object in terms of a single tuple. This feature eliminates unnecessary self-join operations to combine tuples scattered in a temporal relation. Despite this advantage, this model is relatively difficult to implement on top of relational databases because the sizes of attributes are unfixed. Since data boundaries are not problematic in XML, XML can be an elegant solution to implement parametric databases for temporal data. There are two approaches to implementing parametric databases using XML: (1) a native XML database with XQuery engine, and (2) an XML storage with a temporal query language. To determine which approach is appropriate in parametric databases, we consider four questions: the effectiveness of XML in modeling temporal data, the applicability of XML query languages, the user-friendliness of the query languages, and system performances of two approaches. By evaluating the four questions, we show that the latter approach is more appropriate to utilizing XML in parametric databases. 
48|9||Scenario-based multitasking for real-time object-oriented models|Contemporary embedded systems quite often employ extremely complicated software consisting of a number of interrelated components, and this has made object-oriented design methodologies widely used in practice. To implement an object-oriented model in given target hardware, it is imperative to derive a set of tasks from the designed objects. This process of determining tasks and the events they handle greatly influences the real-time performance of the resultant system including response times and real-time guarantees. However, the innate discrepancies between objects and tasks make this exceedingly difficult, and many developers are forced to find their task sets through trial and error. In this paper, we propose Scenario-based Implementation Synthesis Architecture (SISA), an architecture consisting of a method for deriving a task set from a given object-oriented model and the development tools and run-time system architecture to support the method. A system developed with SISA guarantees the optimal response time for each event while deriving the smallest possible number of tasks. We have fully implemented SISA by extending the RoseRT development tool and applied it to an existing industrial PBX (private branch exchange) system. The experimental results show that SISA outperforms the best known conventional techniques by reducing maximum response times an average of 30.3%. 
48|9||Using use case patterns to estimate reusability in software systems|The applicability of using use case patterns as a basis for software cost estimation in the early stages of software development is described. This required the construction of a use case patterns catalogue using a novel process. The catalogue has been analysed to estimate the potential reusability in different software applications. This has shown that 43% of system functions are generally application domain independent, whereas 57% are application domain dependent. Statistical tests showed that the level of specialisation in software systems could be as low as 20%, which supports the direction taken in this research to build a use case patterns catalogue as a basis for the development of use case based software cost estimation models. 
48|9||B-SCP: A requirements analysis framework for validating strategic alignment of organizational IT based on strategy, context, and process|Ensuring that organizational IT is in alignment with and provides support for an organization's business strategy is critical to business success. Despite this, business strategy and strategic alignment issues are all but ignored in the requirements engineering research literature. We present B-SCP, a requirements engineering framework for organizational IT that directly addresses an organization's business strategy and the alignment of IT requirements with that strategy. B-SCP integrates the three themes of strategy, context, and process using a requirements engineering notation for each theme. We demonstrate a means of cross-referencing and integrating the notations with each other, enabling explicit traceability between business processes and business strategy. In addition, we show a means of defining requirements problem scope as a Jackson problem diagram by applying a business modeling framework. Our approach is illustrated via application to an exemplar. The case example demonstrates the feasibility of B-SCP, and we present a comparison with other approaches. 
48|9||Online aggregation with tight error bounds in dynamic environments|OLAP is a category of database technology that allows analysts to gain insight into the aggregation of data by enabling them to gain access to a variety of different views of the information contained in a database. It is very important to provide analysts with guaranteed error bounds for approximate results to aggregation queries in enterprise applications such as decision support systems. We propose a general method of providing tight error bounds for approximate results to OLAP range-sum queries. We perform an extensive experiment on diverse data sets and examine the effectiveness of the proposed method for various data cube dimensions and query sizes. 
48|9||Verification framework and algorithms for integrating information distribution systems|When two competitive companies merge into one bigger company, reusing existing technical resources in each company to form a common technology becomes a priority integration task. One of the specific problems occurring during integration is the resulting integrated scopes' requirements specifications become faulty while integrating two sets of software systems from two participating companies. The integrated scopes refer to the domains of information software systems, business policies, business processes, business rules, interface functions, and data that are being integrated in each participating company.Using a Transition-Directed Graph (TDG) representation, specified requirements involved in the integration will be represented in a form of TDG to be analyzed for faults. Five efficient algorithms are developed to identify faults in the resulting TDG formatted requirement specifications. Four correction algorithms are also developed to correct detected faults found in the TDG formatted requirements specifications. 
48|9||Experiences on establishing software processes in small companies|In order to guide the tailoring of existing approaches for the establishment of software processes in small companies, we report our experiences on defining and implementing software processes in two small software companies. The paper describes the principal steps performed and presents information on costs and duration. We analyse, if and how process guides are used, their impacts and how they are improved. Our findings indicate that also in this specific kind of organisation, software processes can be established successfully at low cost considering typical business models, goals and characteristics of small organisations. 
48|9||Extending the UML concepts to transform natural language queries with fuzzy semantics into SQL|Database applications tend toward getting more versatile and broader to comply with the expansion of various organizations. However, naïve users usually suffer from accessing data arbitrarily by using formal query languages. Therefore, we believe that accessing databases using natural language constructs will become a popular interface in the future. The concept of object-oriented modeling makes the real world to be well represented or expressed in some kinds of logical form. Since the class diagram in UML is used to model the static relationships of databases, in this paper, we intend to study how to extend the UML class diagram representations to capture natural language queries with fuzzy semantics. By referring to the conceptual schema throughout the class diagram representation, we propose a methodology to map natural language constructs into the corresponding class diagram and employ Structured Object Model (SOM) methodology to transform the natural language queries into SQL statements for query executions. Moreover, our approach can handle queries containing vague terms specified in fuzzy modifiers, like ‘good’ or ‘bad’. By our approach, users obtain not only the query answers but also the corresponding degree of vagueness, which can be regarded as the same way we are thinking. 
48|9||The increase of productivity over timeâan industrial case study|Introducing new and specialized technology is often seen as a way of meeting increasing non-functional requirements. An example of such a technology is a software platform that provides high performance and availability. The novelty of such a platform and lack of related experience and competence among the staff may affect initial development productivity. The competence problems should disappear with time. In this paper, we present a study, which we conducted at Ericsson. The purpose of the study was to assess the impact of experience and maturity on productivity in software development on the specialized platform. We quantify the impact by comparing productivity of two projects. One represents an initial development stage while the other represents a subsequent and thus more matured development stage. Both projects resulted in large commercial products. We reveal a factor of four difference in productivity. The difference was caused by a higher code delivery rate and a lower number of code lines per functionality in the latter project. We assess the impact of both these issues on productivity and explain their nature. Based on our findings, we suggest a number of improvement suggestions and guidelines for the process of introducing a new technology. 
48|9||A multiple-depth structural index for branching query|XML structural index, which acts as a schema, plays an important role in XML query optimization and formulation. To provide a reasonable structural index for branching path query under space constraint, we propose an adaptive index of multiple local branching depths and multiple local bisimilarities, which is constructed by maximizing marginal gain for given query load. It cannot only give good support to branching path queries but also have much smaller size compared with that of same sort of index. Detailed experiments have shown that the index is effective and efficient for XML branching path query. 
48|9||Computing simple and complex matchings between XML schemas for transforming XML documents|This paper presents a schema matching method for the transformation of XML documents. The proposed method consists of two steps: computing preliminary matching relationships between leaf nodes in the two XML schemas based on proposed ontology and leaf node similarity, and extracting final matchings based on a proposed path similarity. Particularly, for a sophisticated schema matching, the proposed ontology is incrementally updated by users' feedback. Furthermore, since the ontology can describe various relationships between concepts, the proposed method can compute complex matchings as well as simple matchings. Experimental results with schemas used in various domains show that the proposed method performs better than previous methodologies, resulting in a precision of 97% and a recall of 83% on the average. 
48|9||Contents continued|
49|1|http://www.sciencedirect.com/science/journal/09505849/49/1|Most cited journal articles in software engineering|
49|1||An analysis of the most cited articles in software engineering journals - 2000|Citations and related work are crucial in any research to position the work and to build on the work of others. A high citation count is an indication of the influence of specific articles. The importance of citations means that it is interesting to analyze which articles are cited the most. Such an analysis has been conducted using the ISI Web of Science to identify the most cited software engineering journal articles published in 2000. The objective of the analysis is to identify and list the articles that have influenced others the most as measured by citation count. An understanding of which research is viewed by the research community as most valuable to build upon may provide valuable insights into what research to focus on now and in the future. Based on the analysis, a list of the 20 most cited articles is presented here. The intention of the analysis is twofold. First, to identify the most cited articles, and second, to invite the authors of the most cited articles in 2000 to contribute to a special issue of Information and Software Technology. Five authors have accepted the invitation and their articles appear in this special issue. Moreover, an analysis of the most cited software engineering journal articles in the last 20 years is presented. The presentation includes both the most cited articles in absolute numbers and the most cited articles when looking at the average number of citations per year. The article describing the SPIN model checker by G.J. Holzmann published in 1997 is first on both these lists. 
49|1||Moving architectural description from under the technology lamppost|In 2000, we published an extensive study of existing software architecture description languages (ADLs), which has served as a useful reference to software architecture researchers and practitioners. Since then, circumstances have changed. The Unified Modeling Language (UML) has gained popularity and wide adoption, and many of the ADLs we studied have been pushed into obscurity. We argue that this progression can be attributed to early ADLs’ nearly exclusive focus on technological aspects of architecture, ignoring application domain and business contexts within which software systems and development organizations exist. These three concerns – technology, domain, and business – constitute three “lampposts” needed to appropriately “illuminate” software architecture and architectural description. 
49|1||Predicting software defects in varying development lifecycles using Bayesian nets|An important decision in software projects is when to stop testing. Decision support tools for this have been built using causal models represented by Bayesian Networks (BNs), incorporating empirical data and expert judgement. Previously, this required a custom BN for each development lifecycle. We describe a more general approach that allows causal models to be applied to any lifecycle. The approach evolved through collaborative projects and captures significant commercial input. For projects within the range of the models, defect predictions are very accurate. This approach enables decision-makers to reason in a way that is not possible with regression-based models. 
49|1||Software, regulation, and domain specificity|The growing pervasiveness of computer systems is bringing with it more societal reliance on those systems, which in turn is attracting the attention of various legal and political entities. This increasing attention will, one way or another, result in more regulation. This paper discusses regulation and its various forms, its effects on software development, and the software development tools and techniques that can be used to respond effectively to the demands of regulation. In particular, the maturing of software technology is leading to domain specific solutions that fit the needs of both software developers and regulators. 
49|1||ATerms for manipulation and exchange of structured data: Itâs all about sharing|Some data types are so simple that they tend to be reimplemented over and over again. This is certainly true for terms, tree-like data structures that can represent prefix formulae, syntax trees, intermediate code, and more. We first describe the motivation to introduce Annotated Terms (ATerms): unifying several term formats, optimizing storage requirements by introducing maximal subterm sharing, and providing a language-neutral exchange format. Next, we present a brief overview of the ATerm technology itself and of its wide range of applications. A discussion of competing technologies and the future of ATerms concludes the paper. 
49|1||Autonomic resource provisioning for software business processes|Software development nowadays involves several levels of abstraction: starting from the programming of single objects, to their combination into components, to their publication as services and the overall architecture linking elements at each level. As a result, software engineering is dealing with a wider range of artifacts and concepts (i.e., in the context of this paper: services and business processes) than ever before. In this paper we explore the importance of having an adequate engine for executing business processes written as compositions of Web services. The paper shows that, independently of the composition language used, the overall scalability of the system is determined by how the run-time engine treats the process execution. This is particularly relevant at the service level because publishing a process through a Web service interface makes it accessible to an unpredictable and potentially very large number of clients. As a consequence, the process developer is confronted with the difficult question of resource provisioning. Determining the optimal configuration of the distributed engine that runs the process becomes sensitive both to the actual number of clients and to the kinds of processes to be executed. The main contribution of the paper is to show how resource provisioning for software business processes can be solved using autonomic computing techniques. The engine separates execution in two stages (navigation and dispatching) and uses a controller to allocate the node of a cluster of computers to each one of those stages as the workload changes. The controller can be configured with different policies that define how to reconfigure the system. To prove the feasibility of the concept, we have implemented the autonomic controller and evaluated its performance with an extensive set of experiments. 
49|11-12|http://www.sciencedirect.com/science/journal/09505849/49/11-12|A systematic review of effect size in software engineering experiments|An effect size quantifies the effects of an experimental treatment. Conclusions drawn from hypothesis testing results might be erroneous if effect sizes are not judged in addition to statistical significance. This paper reports a systematic review of 92 controlled experiments published in 12 major software engineering journals and conference proceedings in the decade 1993–2002. The review investigates the practice of effect size reporting, summarizes standardized effect sizes detected in the experiments, discusses the results and gives advice for improvements. Standardized and/or unstandardized effect sizes were reported in 29% of the experiments. Interpretations of the effect sizes in terms of practical importance were not discussed beyond references to standard conventions. The standardized effect sizes computed from the reviewed experiments were equal to observations in psychology studies and slightly larger than standard conventions in behavioral science. 
49|11-12||A state-based approach to integration testing based on UML models|Correct functioning of object-oriented software depends upon the successful integration of classes. While individual classes may function correctly, several new faults can arise when these classes are integrated together. In this paper, we present a technique to enhance testing of interactions among modal classes. The technique combines UML collaboration diagrams and statecharts to automatically generate an intermediate test model, called SCOTEM (State COllaboration TEst Model). The SCOTEM is then used to generate valid test paths. We also define various coverage criteria to generate test paths from the SCOTEM model. In order to assess our technique, we have developed a tool and applied it to a case study to investigate its fault detection capability. The results show that the proposed technique effectively detects all the seeded integration faults when complying with the most demanding adequacy criterion and still achieves reasonably good results for less expensive adequacy criteria. 
49|11-12||Capturing quality requirements of product family architecture|Software quality is one of the major issues with software intensive systems. Moreover, quality is a critical success factor in software product families exploiting shared architecture and common components in a set of products. Our contribution is the QRF (Quality Requirements of a software Family) method, which explicitly focuses on how quality requirements have to be defined, represented and transformed to architectural models. The method has been applied to two experiments; one in a laboratory environment and the other in industry. The use of the QRF method is exemplified by the Distribution Service Platform (DiSeP), the laboratory experiment. The lessons learned are also based on our experiences of applying the method in industrial settings. 
49|11-12||Negotiation support systems and team negotiations: The coalition formation perspective|The use of software to support negotiations has captured the attention of academics and practitioners for some three decades and the research stream of negotiation support systems (NSS) has emerged. Over the years, many NSS have been developed and used in training and research but they have been rarely deployed in organizations. Our speculation is that much existing research is confined to dyadic (i.e., one-to-one) settings which may not adequately reflect the real-world situations in which teams, rather than individuals, often engage in negotiations. To address the gap, our current research aspires to conceptualize the NSS in supporting team negotiations and to theoretically examine their impact. Coalition formation has been a prevalent organizational phenomenon that constitutes important dynamics in any negotiating team; it will be conceptualized as the mechanism through which NSS impacts upon team negotiation outcomes in our paper. Globalization has rendered cross-cultural negotiations a commonplace; at the same time, culture serves as a most salient attribute in activating coalitions. In light of the above, cultural diversity is studied as the antecedent to coalition formation and moderates the influence of NSS on the extent of coalition formation. A set of research propositions, derived from a theoretical framework, are raised and their implications discussed. 
49|11-12||Using classification techniques for informal requirements in the requirements analysis-supporting system|In order to efficiently develop large-scale and complicated software, it is important for system engineers to correctly understand users’ requirements. Most requirements in large-scale projects are collected from various stakeholders located in various regions, and they are generally written in natural language. Therefore, the initial collected requirements must be classified into various topics prior to analysis phases in order to be usable as input in several requirements analysis methods. If this classification process is manually done by analysts, it becomes a time-consuming task. To solve this problem, we propose a new bootstrapping method which can automatically classify requirements sentences into each topic category using only topic words as the representative of the analysts’ views. The proposed method is verified through experiments using two requirements data sets: one written in English and the other in Korean. The significant performances were achieved in the experiments: the 84.28 and 87.91 F1 scores for the English and Korean data sets, respectively. As a result, the proposed method can provide an effective function for an Internet-based requirements analysis-supporting system so as to efficiently gather and analyze requirements from various and distributed stakeholders by using the Internet. 
49|11-12||An ECA-based framework for decentralized coordination of ubiquitous web services|Emerging ubiquitous computing network is expected to consist of a variety of heterogeneous and distributed devices. While web services technology is increasingly being considered as a promising solution to support the inter-operability between such heterogeneous devices via well-defined protocol, currently there is no effective framework reported in the literature that can address the problem of coordinating the web services-enabled devices. This paper considers a ubiquitous computing environment that is comprised of active, autonomous devices interacting with each other through web services, and presents an ECA (Event-Condition-Action)-based framework for effective coordination of those devices. Specifically, we first present an XML-based language for describing ECA rules that are embedded in web service-enabled devices. An ECA rule, when triggered by an internal or external event to the device, can result in the invocation of appropriate web services in the system. Subsequently, we consider the situation in which the rules are introduced and managed by multiple, independent users, and propose effective mechanisms that can detect and resolve potential inconsistencies among the rules. The presented ECA-based coordination approach is expected to facilitate seamless inter-operation among the web service-enabled devices in the emerging ubiquitous computing environments. 
49|11-12||Modelling non-functional requirements of business processes|This paper presents an approach to the identification and inclusion of ‘non-functional’ aspects of a business process in modelling for business improvement. The notion of non-functional requirements (NFRs) is borrowed from software engineering, and a method developed in that field for linking NFRs to conceptual models is adapted and applied to business process modelling. Translated into this domain, NFRs are equated with the general or overall quality attributes of a business process, which, though essential aspects of any effective process, are not well captured in a functionally oriented process model. Using an example of a healthcare process (cancer registration in Jordan). We show how an analysis and evaluation of NFRs can be applied to a process model developed with role activity diagramming (RAD) to operationalise desirable quality features more explicitly in the model. This gives a useful extension to RAD and similar modelling methods, as well as providing a basis for business improvement. 
49|11-12||Call for papers|
49|2|http://www.sciencedirect.com/science/journal/09505849/49/2|Reverse-engineering 1-n associations from Java bytecode using alias analysis|1-n associations are design language constructs that represent one-to-many structural invariants for objects. To implement 1-n associations, container classes, such as Vector in Java, are frequently used as programming language constructs. Many of the current CASE tools fail to reverse-engineer 1-n associations that have been implemented via containers because sophisticated analyses are required to infer the type of elements collected in containers. This paper presents a new approach to reverse-engineering 1-n associations from Java bytecode based on alias analysis. In our approach, 1-n associations are inferred by examining the abstract heap structure that is constructed by applying an alias analysis on inter-variable relationships extracted from assignments and method invocations of containers. Our approach handles container alias problem that has been neglected by previous techniques by approximating the relationships between containers and elements at the object level rather than analyzing only the bytecode. Our prototype implementation was used with a suite of well-known Java programs. Most of the 1-n associations were successfully reverse-engineered from hundreds of class files in less than 1 minute. 
49|2||Semantic similarity-based grading of student programs|An automatic grading approach is presented based on program semantic similarity. Automatic grading of a student program is achieved by calculating semantic similarities between the student program and each correct model program after they are standardized. This approach was implemented in an on-line examination system for the programming language C. Different form other existing approaches, it can evaluate how close a student’s source code is to a correct solution and give a matching accuracy. 
49|2||Early detection of COTS component functional suitability|The adoption of COTS-based development brings with it many challenges about the identification and finding of candidate components for reuse. Particularly, the first stage in the identification of COTS candidates is commonly carried out by dealing with unstructured information on the Web, which makes the evaluation process highly costly when applying complex evaluation criteria. To facilitate this process, our proposal introduces an early measurement procedure for suitability of COTS candidates. Considering that filtering is about a first-stage selection, functionality evaluation might drive the analysis, proceeding with the evaluation of other properties only on the pre-selected candidates. In this way, a few candidates are fully evaluated making in principle the whole process more cost-effective. In this paper, we illustrate how functional measures at an initial state are calculated for an E-payment case study. 
49|2||A quality framework for developing and evaluating original software components|Component-based software development is being identified as the emerging method of developing complex applications consisting of heterogeneous systems. Although more research attention has been given to Commercial Off The Shelf (COTS) components, original software components are also widely used in the software industry. Original components are smaller in size, they have a narrower functional scope and they usually find more uses when it comes to specific and dedicated functions. Therefore, their need for interoperability is equal or greater, than that of COTS components. A quality framework for developing and evaluating original components is proposed in this paper, along with an application methodology that facilitates their evaluation. The framework is based on the ISO9126 quality model which is modified and refined so as to reflect better the notion of original components. The quality model introduced can be tailored according to the organization-reuser and the domain needs of the targeted component. The proposed framework is demonstrated and validated through real case examples, while its applicability is assessed and discussed. 
49|2||Validating the enforcement of access control policies and separation of duty principle in requirement engineering|Validating the compliance of software requirements with the access control policies during the early development life cycle improves the security of the software. It prevents authorizing unauthorized subject during the specification of requirements and analysis before proceeding to other phases where the cost of fixing defects is augmented. This paper provides a logical-based framework that analyzes the authorization requirements specified in the Unified Modeling Language (UML). It ensures that the access requirements are consistent, complete and conflict-free. The framework proposed in this paper is an extension to AuthUML framework. We refine AuthUML and extend it by expanding its analysis to validate the enforcement of the Separation of Duty (SoD) during the requirement engineering. We enhance and extend AuthUML with the necessary phase, predicates and rules. The paper shows the various types of SoD and how each type can be validated. The extension shows the flexibility and scalability of AuthUML to validate new policies. Also, the extension makes AuthUML spans to different phases of the software development process that widen the application of AuthUML. 
49|2||Automatic test case generation from UML communication diagrams|We present a method to generate cluster level test cases based on UML communication diagrams. In our approach, we first construct a tree representation of communication diagrams. We then carry out a post-order traversal of the constructed tree for selecting conditional predicates from the communication diagram. We transform the conditional predicates on the communication diagram and apply function minimization technique to generate the test data. The generated test cases achieve message paths coverage as well as boundary coverage. We have implemented our technique and tested it on several example problems. 
49|2||Generalization of strategies for fuzzy query translation in classical relational databases|Users of information systems would like to express flexible queries over the data possibly retrieving imperfect items when the perfect ones, which exactly match the selection conditions, are not available. Most commercial DBMSs are still based on the SQL for querying. Therefore, providing some flexibility to SQL can help users to improve their interaction with the systems without requiring them to learn a completely novel language. Based on the fuzzy set theory and the Î±-cut operation of fuzzy number, this paper presents the generic fuzzy queries against classical relational databases and develops the translation of the fuzzy queries. The generic fuzzy queries mean that the query condition consists of complex fuzzy terms as the operands and complex fuzzy relations as the operators in a fuzzy query. With different thresholds that the user chooses for the fuzzy query, the user’s fuzzy queries can be translated into precise queries for classical relational databases. 
49|2||State of the practice: An exploratory analysis of schedule estimation and software project success prediction|During discussions with a group of U.S. software developers we explored the effect of schedule estimation practices and their implications for software project success. Our objective is not only to explore the direct effects of cost and schedule estimation on the perceived success or failure of a software development project, but also to quantitatively examine a host of factors surrounding the estimation issue that may impinge on project outcomes. We later asked our initial group of practitioners to respond to a questionnaire that covered some important cost and schedule estimation topics. Then, in order to determine if the results are generalizable, two other groups from the US and Australia, completed the questionnaire. Based on these convenience samples, we conducted exploratory statistical analyses to identify determinants of project success and used logistic regression to predict project success for the entire sample, as well as for each of the groups separately. From the developer point of view, our overall results suggest that success is more likely if the project manager is involved in schedule negotiations, adequate requirements information is available when the estimates are made, initial effort estimates are good, take staff leave into account, and staff are not added late to meet an aggressive schedule. For these organizations we found that developer input to the estimates did not improve the chances of project success or improve the estimates. We then used the logistic regression results from each single group to predict project success for the other two remaining groups combined. The results show that there is a reasonable degree of generalizability among the different groups. 
49|2||Managing the business of software product line: An empirical investigation of key business factors|Business has been highlighted as a one of the critical dimensions of software product line engineering. This paper’s main contribution is to increase the understanding of the influence of key business factors by showing empirically that they play an imperative role in managing a successful software product line. A quantitative survey of software organizations currently involved in the business of developing software product lines over a wide range of operations, including consumer electronics, telecommunications, avionics, and information technology, was designed to test the conceptual model and hypotheses of the study. This is the first study to demonstrate the relationships between the key business factors and software product lines. The results provide evidence that organizations in the business of software product line development have to cope with multiple key business factors to improve the overall performance of the business, in addition to their efforts in software development. The conclusions of this investigation reinforce current perceptions of the significance of key business factors in successful software product line business. 
49|3|http://www.sciencedirect.com/science/journal/09505849/49/3|Working Conference on Reverse Engineering 2005|
49|3||Recovering UML class models from C++: A detailed explanation|An approach to recovering design-level UML class models from C++ source code to support program comprehension is presented. A set of mappings are given that focus on accurately identifying such elements as relationship types, multiplicities, and aggregation semantics. These mappings are based on domain knowledge of the C++ language and common programming conventions and idioms. Additionally, formal concept analysis is used to detect design-level attributes of UML classes. An application implementing these mappings is used to reverse engineer a moderately sized, open-source application and the resultant class model is compared against those produced by other UML reverse engineering tools. This comparison shows that the presented mapping rules effectively produce meaningful and semantically accurate UML models. 
49|3||Semantic clustering: Identifying topics in source code|Many of the existing approaches in Software Comprehension focus on program structure or external documentation. However, by analyzing formal information the informal semantics contained in the vocabulary of source code are overlooked. To understand software as a whole, we need to enrich software analysis with the developer knowledge hidden in the code naming. This paper proposes the use of information retrieval to exploit linguistic information found in source code, such as identifier names and comments. We introduce Semantic Clustering, a technique based on Latent Semantic Indexing and clustering to group source artifacts that use similar vocabulary. We call these groups semantic clusters and we interpret them as linguistic topics that reveal the intention of the code. We compare the topics to each other, identify links between them, provide automatically retrieved labels, and use a visualization to illustrate how they are distributed over the system. Our approach is language independent as it works at the level of identifier names. To validate our approach we applied it on several case studies, two of which we present in this paper.Note: Some of the visualizations presented make heavy use of colors. Please obtain a color copy of the article for better understanding. 
49|3||Clustering large software systems at multiple layers|Software clustering algorithms presented in the literature rarely incorporate in the clustering process dynamic information, such as the number of function invocations during runtime. Moreover, the structure of a software system is often multi-layered, while existing clustering algorithms often create flat system decompositions.This paper presents a software clustering algorithm called MULICsoft that incorporates in the clustering process both static and dynamic information. MULICsoft produces layered clusters with the core elements of each cluster assigned to the top layer. We present experimental results of applying MULICsoft to a large open-source system. Comparison with existing software clustering algorithms indicates that MULICsoft is able to produce decompositions that are close to those created by system experts. 
49|3||Automated clustering to support the reflexion method|A significant aspect in applying the Reflexion Method is the mapping of components found in the source code onto the conceptual components defined in the hypothesized architecture. To date, this mapping is established manually, which requires a lot of work for large software systems. In this paper, we present a new approach, in which clustering techniques are applied to support the user in the mapping activity. The result is a semi-automated mapping technique that accommodates the automatic clustering of the source model with the user’s hypothesized knowledge about the system’s architecture.This paper describes three case studies in which the semi-automated mapping technique, called HuGMe, has been applied successfully to extend a partial map of real-world software applications. In addition, the results of another case study from an earlier publication are summarized, which lead to comparable results. We evaluated the extended versions of two automatic software clustering techniques, namely, MQAttract and CountAttract, with oracle mappings. We closely study the influence of the degree of completeness of the existing mapping and other controlling variables of the technique to make reliable suggestions.Both clustering techniques were able to achieve a mapping quality where more than 90% of the automatic mapping decisions turned out to be correct. Moreover, the experiments indicate that the attraction function (CountAttract′) based on local coupling and cohesion is more suitable for semi-automated mapping than the approach MQAttract′ based on a global assessment of coupling and cohesion. 
49|3||Case study: Re-engineering C++ component models via automatic program transformation|Automated program transformation holds promise for a variety of software life cycle endeavors, particularly where the size of legacy systems makes manual code analysis, re-engineering, and evolution difficult and expensive. But constructing highly scalable transformation tools supporting modern languages in full generality is itself a painstaking and expensive process. This cost can be managed by developing a common transformation system infrastructure re-useable by derived tools that each address specific tasks, thus leveraging the infrastructure costs. This paper describes the Design Maintenance System (DMS1), a practical, commercial program analysis and transformation system, and discusses how it was employed to construct a custom modernization tool being applied to a large C++ avionics system. The tool transforms components developed in a 1990s-era component style to a more modern CORBA-like component framework, preserving functionality. 
49|3||An infrastructure to support interoperability in reverse engineering|The reverse engineering community has recognized the importance of interoperability, the cooperation of two or more systems to enable the exchange and utilization of data, and has noted that the current lack of interoperability is a contributing factor to the lack of adoption of available infrastructures. To address the problems of interoperability and reproducing previous results, we present an infrastructure that supports interoperability among reverse engineering tools and applications. We present the design of our infrastructure, including the hierarchy of schemas that captures the interactions among graph structures. We also develop and utilize our implementation, which is designed using a GXL-based pipe-filter architecture, to perform a case study that demonstrates the feasibility of our infrastructure. 
49|4|http://www.sciencedirect.com/science/journal/09505849/49/4|A holistic architecture assessment method for software product lines|The success of architecture-centric development of software product lines is critically dependent upon the availability of suitable architecture assessment methods. While a number of architecture assessment methods are available and some of them have been widely used in the process of evaluating single product architectures, none of them is equipped to deal with the main challenges of product line development. In this paper we present an adaptation of the Architecture Tradeoff Analysis Method (ATAM) for the task of assessing product line architectures. The new method, labeled Holistic Product Line Architecture Assessment (HoPLAA), uses a holistic approach that focuses on risks and quality attribute tradeoffs – not only for the common product line architecture, but for the individual product architectures as well. In addition, it prescribes a qualitative analytical treatment of variation points using scenarios. The use of the new method is illustrated through a case study. 
49|4||Indexing range sum queries in spatio-temporal databases|Although spatio-temporal databases have received considerable attention recently, there has been little work on processing range sum queries on the historical records of moving objects despite their importance. Since the direct access to a huge amount of data to answer range sum queries incurs prohibitive computation cost, materialization techniques based on existing index structures are suggested. A simple but effective solution is to apply the materialization technique to the MVR-tree known as the most efficient structure for window queries with spatio-temporal conditions. Aggregate structures based on other index structures such as the HR-tree and the 3DR-tree do not provide satisfactory query performance. In this paper, we propose a new index structure called the Adaptively Partitioned Aggregate R-Tree (APART) and query processing algorithms to efficiently process range sum queries in many situations. Our experimental results show that the performance of the APART is typically 1.3 times better than that of its competitor for a wide range of scenarios. 
49|4||Efficient index-based KNN join processing for high-dimensional data|In many advanced database applications (e.g., multimedia databases), data objects are transformed into high-dimensional points and manipulated in high-dimensional space. One of the most important but costly operations is the similarity join that combines similar points from multiple datasets. In this paper, we examine the problem of processing K-nearest neighbor similarity join (KNN join). KNN join between two datasets, R and S, returns for each point in R its K most similar points in S. We propose a new index-based KNN join approach using the iDistance as the underlying index structure. We first present its basic algorithm and then propose two different enhancements. In the first enhancement, we optimize the original KNN join algorithm by using approximation bounding cubes. In the second enhancement, we exploit the reduced dimensions of data space. We conducted an extensive experimental study using both synthetic and real datasets, and the results verify the performance advantage of our schemes over existing KNN join algorithms. 
49|4||Practice-driven approach for creating project-specific software development methods|Both practitioners and researchers agree that if software development methods were more adjustable to project-specific situations, this would increase their use in practice. Empirical investigations show that otherwise methods exist just on paper while in practice developers avoid them or do not follow them rigorously. In this paper we present an approach that deals with this problem. Process Configuration, as we named the approach, tells how to create a project-specific method from an existing one, taking into account the project circumstances. Compared to other approaches that deal with the creation of project-specific methods, our approach tends to be more flexible and easier to implement in practice as it introduces few simplifications. The proposed approach is practice-driven, i.e. it has been developed in cooperation with software development companies. 
49|4||Experimental evaluation of an object-oriented function point measurement procedure|This paper presents an empirical study that evaluates OO-Method Function Points (OOmFP), a functional size measurement procedure for object-oriented systems that are specified using the OO-Method approach. A laboratory experiment with students was conducted to compare OOmFP with the IFPUG – Function Point Analysis (FPA) procedure on a range of variables, including efficiency, reproducibility, accuracy, perceived ease of use, perceived usefulness and intention to use. The results show that OOmFP is more time-consuming than FPA but the measurement results are more reproducible and accurate. The results also indicate that OOmFP is perceived to be more useful and more likely to be adopted in practice than FPA in the context of OO-Method systems development. We also report lessons learned and suggest improvements to the experimental procedure employed and replications of this study using samples of industry practitioners. 
49|4||On the design of more secure software-intensive systems by use of attack patterns|Retrofitting security implementations to a released software-intensive system or to a system under development may require significant architectural or coding changes. These late changes can be difficult and more costly than if performed early in the software process. We have created regular expression-based attack patterns that show the sequential events that occur during an attack. By performing a Security Analysis for Existing Threats (SAFE-T), software engineers can match the symbols of a regular expression to their system design. An architectural analysis that identifies security vulnerabilities early in the software process can prepare software engineers for which security implementations are necessary when coding starts. A case study involving students in an upper-level undergraduate security course suggests that SAFE-T can be performed by relatively inexperienced engineers who are not experts in security. Data from the case study also suggest that the attack patterns do not restrict themselves to vulnerabilities in specific environments. 
49|4||Mutating database queries|A set of mutation operators for SQL queries that retrieve information from a database is developed and tested against a set of queries drawn from the NIST SQL Conformance Test Suite. The mutation operators cover a wide spectrum of SQL features, including the handling of null values. Additional experiments are performed to explore whether the cost of executing mutants can be reduced using selective mutation or the test suite size can be reduced by using an appropriate ordering of the mutants. The SQL mutation approach can be helpful in assessing the adequacy of database test cases and their development, and as a tool for systematically injecting faults in order to compare different database testing techniques. 
49|5|http://www.sciencedirect.com/science/journal/09505849/49/5|Software systems in-house integration: Architecture, process practices, and strategy selection|As organizations merge or collaborate closely, an important question is how their existing software assets should be handled. If these previously separate organizations are in the same business domain – they might even have been competitors – it is likely that they have developed similar software systems. To rationalize, these existing software assets should be integrated, in the sense that similar features should be implemented only once. The integration can be achieved in different ways. Success of it involves properly managing challenges such as making as well founded decisions as early as possible, maintaining commitment within the organization, managing the complexities of distributed teams, and synchronizing the integration efforts with concurrent evolution of the existing systems.This paper presents a multiple case study involving nine cases of such in-house integration processes. Based both on positive and negative experiences of the cases, we pinpoint crucial issues to consider early in the process, and suggest a number of process practices. 
49|5||Evaluation of object-oriented design patterns in game development|The use of object-oriented design patterns in game development is being evaluated in this paper. Games’ quick evolution, demands great flexibility, code reusability and low maintenance costs. Games usually differentiate between versions, in respect of changes of the same type (additional animation, terrains etc). Consequently, the application of design patterns in them can be beneficial regarding maintainability. In order to investigate the benefits of using design patterns, a qualitative and a quantitative evaluation of open source projects is being performed. For the quantitative evaluation, the projects are being analyzed by reverse engineering techniques and software metrics are calculated. 
49|5||Cognitive evaluation of information modeling methods|In the field of information system engineering, information modeling method is a technique to capture user requirements and to understand system complexity. The importance of information modeling has been recognized by practitioners and researchers, but little has been explored to analyze the available information modeling methods or to evaluate them in terms of their strengths, weaknesses, and effectiveness. This research analyzes six information-modeling methods: use case diagram, rich picture diagram, entity-relationship diagram, Trochim’s concept mapping, repertory grid, and causal mapping. These information-modeling methods are analyzed from a cognitive perspective in order to better understand their nature, the assumptions, and the important features associated. The research contributes to the understanding of the information modeling methods and their potential uses and values during IS development. 
49|5||Language subsetting in an industrial context: A comparison of MISRA C 1998 and MISRA C 2004|The MISRA (Motor Industry Software Research Association) C standard first appeared in 1998 with the object of restricting the use of features in the ISO C programming language of known undefined or otherwise dangerous behaviour in embedded control systems in the motor car industry. The first edition gained significant attention around the world and in October 2004, a further edition was issued to a wider intended target audience, with the intention of correcting ambiguous wording undermining the effectiveness of the first edition and also improving its ability to restrict features of dangerous behaviour. This paper measures how well the two versions of this document compare on the same population of software and also determines how well the 2004 version achieved its stated goals. Given its increasing influence, the results raise important concerns, specifically that the false positive rate is still unacceptably high with the accompanying danger that compliance may make things worse not better. 
49|5||Object-oriented software fault prediction using neural networks|This paper introduces two neural network based software fault prediction models using Object-Oriented metrics. They are empirically validated using a data set collected from the software modules developed by the graduate students of our academic institution. The results are compared with two statistical models using five quality attributes and found that neural networks do better. Among the two neural networks, Probabilistic Neural Networks outperform in predicting the fault proneness of the Object-Oriented modules developed. 
49|5||Tool support for iterative software process modeling|To formalize a process, its important aspects must be extracted and described in a model. This model is often written in a formal language so that the process itself can be automated. Since models are often developed iteratively, this language should support this iterative development cycle. However, many existing languages do not. In this paper, we use an existing high-level process modeling language and present a tool that we have developed for supporting iterative development. We have used our tool to develop and refine a process model of distributed software development for NetBeans. 
49|5||Software maintenance seen as a knowledge management issue|Creating and maintaining software systems is a knowledge intensive task. One needs to have a good understanding of the application domain, the problem to solve and all its requirements, the software process used, technical details of the programming language(s), the system’s architecture and how the different parts fit together, how the system interacts with its environment, etc. All this knowledge is difficult and costly to gather. It is also difficult to store and usually lives only in the mind of the software engineers who worked on a particular project.If this is a problem for development of new software, it is even more for maintenance, when one must rediscover lost information of an abstract nature from legacy source code among a swarm of unrelated details.In this paper, we submit that this lack of knowledge is one of the prominent problems in software maintenance. To try to solve this problem, we adapted a knowledge extraction technique to the knowledge needs specific to software maintenance. We explain how we explicit the knowledge discovered on a legacy software during maintenance so that it may be recorded for future use. Some applications on industry maintenance projects are reported. 
49|6|http://www.sciencedirect.com/science/journal/09505849/49/6|For the Special issue on Qualitative Software Engineering Research|
49|6||Ethnographically-informed empirical studies of software practice|Over the past decade we have performed a sustained series of qualitative studies of software development practice, focusing on social factors. Using an ethnographically-informed approach, we have addressed four areas of software practice: software quality management systems, the emergence of object technology, professional end user development and agile development. Several issues have arisen from this experience, including the nature of research questions that such studies can address, the advantages and challenges associated with being a member of the community under study, and how to maintain rigour in data collection. In this paper, we will draw on our studies to illustrate our approach and to discuss these and other issues. 
49|6||The impact of the Abilene Paradox on double-loop learning in an agile team|This paper presents a qualitative investigation of learning failures associated with the introduction of a new software development methodology by a project team. This paper illustrates that learning is more than the cognitive process of acquiring a new skill; learning also involves changes in behaviour and even beliefs. Extreme Programming (XP), like other software development methodologies, provides a set of values and guidelines as to how software should be developed. As these new values and guidelines involve behavioural changes, the study investigates the introduction of XP as a new learning experience. Researchers use the concepts of single and double-loop learning to illustrate how social actors learn to perform tasks effectively and to determine the best task to perform. The concept of triple-loop learning explains how this learning process can be ineffective, accordingly it is employed to examine why the introduction of XP was ineffective in the team studied. While XP should ideally foster double-loop learning, triple-loop learning can explain why this does not necessarily occur. Research illustrates how power factors influence learning among groups of individuals; this study focuses on one specific power factor – the power inherent in the desire to conform. The Abilene Paradox describes how groups can make ineffective decisions that are contrary to that which group members personally desire or believe. Ineffective decision-making occurs due to the desire to conform among group members; this was shown as the cause of ineffective learning in the software team studied. This desire to conform originated in how the project team cohered as a group, which was, in turn, influenced by the social values embraced by XP. 
49|6||Self-organization of teams for free/libre open source software development|This paper provides empirical evidence about how free/libre open source software development teams self-organize their work, specifically, how tasks are assigned to project team members. Following a case study methodology, we examined developer interaction data from three active and successful FLOSS projects using qualitative research methods, specifically inductive content analysis, to identify the task-assignment mechanisms used by the participants. We found that ‘self-assignment’ was the most common mechanism across three FLOSS projects. This mechanism is consistent with expectations for distributed and largely volunteer teams. We conclude by discussing whether these emergent practices can be usefully transferred to mainstream practice and indicating directions for future research. 
49|6||Revealing actual documentation usage in software maintenance through war stories|War stories are a form of qualitative data that capture informants’ specific accounts of surmounting great challenges. The rich contextual detail afforded by this approach warrants its inclusion in the methodological arsenal of empirical software engineering research. We ground this assertion in an exemplar field study that examined the use of documentation in software maintenance environments. Specific examples are unpacked to reveal a depth of insight that would not have been possible using standard interviews. This afforded a better understanding of the complex relationship between project personnel and documentation, including individuals’ roles as pointers, gatekeepers, or barriers to documentation. 
49|6||Requirements engineering challenges in market-driven software development â An interview study with practitioners|Requirements engineering for market-driven software development entails special challenges. This paper presents results from an empirical study that investigates these challenges, taking a qualitative approach using interviews with fourteen employees at eight software companies and a focus group meeting with practitioners. The objective of the study is to increase the understanding of the area of market-driven requirements engineering and provide suggestions for future research by describing encountered challenges. A number of challenging issues were found, including bridging communication gaps between marketing and development, selecting the right level of process support, basing the release plan on uncertain estimates, and managing the constant flow of requirements. 
49|6||Management competences, not tools and techniques: A grounded examination of software project management at WM-data|Traditional software project management theory often focuses on desk-based development of software and algorithms, much in line with the traditions of the classical project management and software engineering. This can be described as a tools and techniques perspective, which assumes that software project management success is dependent on having the right instruments available, rather than on the individual qualities of the project manager or the cumulative qualities and skills of the software organisation. Surprisingly, little is known about how (or whether) these tools techniques are used in practice. This study, in contrast, uses a qualitative grounded theory approach to develop the basis for an alternative theoretical perspective: that of competence. A competence approach to understanding software project management places the responsibility for success firmly on the shoulders of the people involved, project members, project leaders, managers. The competence approach is developed through an investigation of the experiences of project managers in a medium sized software development company (WM-data) in Denmark. Starting with a simple model relating project conditions, project management competences and desired project outcomes, we collected data through interviews, focus groups and one large plenary meeting with most of the company’s project managers. Data analysis employed content analysis for concept (variable) development and causal mapping to trace relationships between variables. In this way we were able to build up a picture of the competences project managers use in their daily work at WM-data, which we argue is also partly generalisable to theory. The discrepancy between the two perspectives is discussed, particularly in regard to the current orientation of the software engineering field. The study provides many methodological and theoretical starting points for researchers wishing to develop a more detailed competence perspective of software project managers’ work. 
49|6||Practical knowledge and its importance for software product quality|To achieve software product quality professional skill and knowledge is important. The way to reach software product quality is often structured approaches for software quality such as SPI and CMM which have been criticized for lack of a knowledge perspective. The view taken in this study is that software product quality is related to interpretations and understanding in practice, and thus on practical knowledge. Based on a qualitative study of practising software developers’ understanding of the concept of quality and quality assessment, it is shown why quality resists definition and why experience-based, practical knowledge is important. 
49|6||A model of design decision making based on empirical results of interviews with software designers|Despite the impact of design decisions on software design, we have little understanding about how design decisions are made. This hinders our ability to provide design metrics, processes and training that support inherent design work. By interviewing 25 software designers and using content analysis and explanation building as our analysis technique, we provide qualitative and quantitative results that highlight aspects of rational and naturalistic decision making in software design. Our qualitative multi-case study results in a model of design decision making to answer the question: how do software designers make design decisions? We find the structure of the design problem determines the aspects of rational and naturalistic decision making used. The more structured the design decision, the less a designer considers options. 
49|6||Using grounded theory to understand software process improvement: A study of Irish software product companies|Software process improvement (SPI) aims to understand the software process as it is used within an organisation and thus drive the implementation of changes to that process to achieve specific goals such as increasing development speed, achieving higher product quality or reducing costs. Accordingly, SPI researchers must be equipped with the methodologies and tools to enable them to look within organisations and understand the state of practice with respect to software process and process improvement initiatives, in addition to investigating the relevant literature. Having examined a number of potentially suitable research methodologies, we have chosen Grounded Theory as a suitable approach to determine what was happening in actual practice in relation to software process and SPI, using the indigenous Irish software product industry as a test-bed. The outcome of this study is a theory, grounded in the field data, that explains when and why SPI is undertaken by the software industry. The objective of this paper is to describe both the selection and usage of grounded theory in this study and evaluate its effectiveness as a research methodology for software process researchers. Accordingly, this paper will focus on the selection and usage of grounded theory, rather than results of the SPI study itself. 
49|6||Software process improvement as emergent change: A structurational analysis|This paper presents a framework that draws on Structuration theory and dialectical hermeneutics to explicate the dynamics of software process improvement (SPI) in a packaged software organisation. Adding to the growing body of qualitative research, this approach overcomes some of the criticisms of interpretive studies, especially the need for the research to be reflexive in nature.Our longitudinal analysis of the case study shows SPI to be an emergent rather than a deterministic activity: the design and action of the change process are shown to be intertwined and shaped by their context. This understanding is based upon a structurational perspective that highlights how the unfolding/realisation of the process improvement (intent) are enabled and constrained by their context. The work builds on the recognition that the improvements can be understood from an organisational learning perspective. Fresh insights to the improvement process are developed by recognising the role of the individual to influence the improvement through facilitating or resisting the changes. The understanding gained here can be applied by organisations to enable them to improve the effectiveness of their SPI programmes, and so improve the quality of their software. 
49|6||Interpretation, interaction and reality construction in software engineering: An explanatory model|The incorporation of social issues in software engineering is limited. Still, during the last 20 years the social element inherent in software development has been addressed in a number of publications that identified a lack of common concepts, models, and theories for discussing software development from this point of view. It has been suggested that we need to take interpretative and constructive views more seriously if we are to incorporate the social element in software engineering. Up till now we have lacked papers presenting ‘simple’ models explaining why. This article presents a model that helps us better to understand interpretation, interaction and reality construction from a natural language perspective. The concepts and categories following with the model provide a new frame of reference useful in software engineering research, teaching, and methods development. 
49|7|http://www.sciencedirect.com/science/journal/09505849/49/7|Emotional agents: A modeling and an application|This paper proposes modeling of artificial emotions through agents based on symbolic approach. The symbolic approach utilizes symbolic emotional rule-based systems (rule base that generated emotions) with continuous interactions with environment and an internal “thinking” machinery that comes as a result of series of inferences, evaluation, evolution processes, adaptation, learning, and emotions. We build two models for agent based systems; one is supported with artificial emotions and the other one without emotions. We use both in solving a bench mark problem; “The Orphanage Care Problem”. The two systems are simulated and results are compared. Our study shows that systems with proper model of emotions can perform in many cases better than systems without emotions. We try to shed the light here on how artificial emotions can be modeled in a simple rule-based agent systems and if emotions as they exist in “real intelligence” can be helpful for “artificial intelligence”. Agent architectures are presented as a generic blueprint on which the design of agents can be based. Our focus is on the functional design, including flow of information and control. With this information provided, the generic blueprints of architectures should not be difficult to implement agents, thus putting these theoretical models into practice. We build the agents using this architecture, and many experiments and analysis are shown. 
49|7||Classifying variability modeling techniques|Variability modeling is important for managing variability in software product families, especially during product derivation. In the past few years, several variability modeling techniques have been developed, each using its own concepts to model the variability provided by a product family. The publications regarding these techniques were written from different viewpoints, use different examples, and rely on a different technical background. This paper sheds light on the similarities and differences between six variability modeling techniques, by exemplifying the techniques with one running example, and classifying them using a framework of key characteristics for variability modeling. It furthermore discusses the relation between differences among those techniques, and the scope, size, and application domain of product families. 
49|7||A framework for evaluating reusability of core asset in product line engineering|Product line engineering (PLE) is a new effective approach to software reuse, where applications are generated by instantiating a core asset which is a large-grained reuse unit. Hence, a core asset is a key element of PLE, and therefore the reusability of the core asset largely determines the success of PLE projects. However, current quality models to evaluate reusability do not adequately address the unique characteristics of core assets in PLE. This paper proposes a comprehensive framework for evaluating the reusability of core assets. We first identify the key characteristics of core assets, and derive a set of quality attributes that characterizes the reusability of core assets. Then, we define metrics for each quality attribute and finally present practical guidelines for applying the evaluation framework in PLE projects. Using the proposed framework, the reusability of core assets can be more effectively and precisely evaluated. 
49|7||A method engineering approach to developing aspect-oriented modelling processes based on the OPEN process framework|Aspect-oriented software development (AOSD) is an approach to software development in which aspect-oriented techniques are integrated with traditional (mainly OO) development techniques. Identifying the appropriate method components for supporting aspect-oriented development is facilitated by the use of a method engineering approach. We demonstrate this approach by using the OPEN Process Framework (OPF) to identify previous deficiencies in the method fragments stored in the OPF repository so that the enhanced OPF repository is able to fully support AOSD. 
49|7||Identification of factors that influence defect injection and detection in development of software intensive products|The objective of this study is the identification of factors that influence defect injection and defect detection. The study is part of a broader research project with the goal to lower the number of residual defects in software intensive products, by using the influencing factors to decrease injection of defects and to increase detection of defects. As a first step, we performed an extensive literature search to find influencing factors and processed the factors to achieve consistently formulated sets of factors without duplications. As a second step, we used a cluster analysis to reduce the number influencing factors to manageable-sized sets for practical application. As a last step, final groupings of factors were obtained by expert interpretation of the cluster analysis results. These steps were separately performed for defect injection and detection influencing factors, resulting in sets of, respectively, 16 and 17 factors. Finally, the resulting factor groupings were evaluated.The findings (1) are the basis for further research focusing on a framework for lowering residual defects, (2) already provide information to enable practitioners to devise strategies for lowering residual defects, and (3) may create awareness in organizations to reconsider policies regarding development and Verification & Validation. 
49|7||A controlled empirical evaluation of a requirements abstraction model|Requirement engineers in industry are faced with the complexity of handling large amounts of requirements as development moves from traditional bespoke projects towards market-driven development. There is a need for usable and useful models that recognize this reality and support the engineers in a continuous effort of choosing which requirements to accept and which to dismiss off hand using the goals and product strategies put forward by management. This paper presents an evaluation of such a model that is built based on needs identified in industry. The evaluation’s primary goal is to test the model’s usability and usefulness in a lab environment prior to large scale industry piloting, and is a part of a large technology transfer effort. The evaluation uses 179 subjects from three different Swedish Universities, which is a large portion of the university students educated in requirements engineering in Sweden during 2004 and 2005. The results provide a strong indication that the model is indeed both useful and usable and ready for industry trials. 
49|7||Estimating nested selectivity in object-oriented and object-relational databases|A search condition in object-oriented/object-relational queries consists of nested predicates, which are predicates on path expressions. In this paper, we propose a new technique for estimating selectivity for nested predicates. Selectivity of a nested predicate, nested selectivity, is defined as the ratio of the number of the qualified objects of the starting class in the path expression to the total number of objects of the class. The new technique takes into account the effects of direct representation of the many-to-many relationship and the partial participation of objects in the relationship. These two features occur frequently in object-oriented/object-relational databases, but have not been properly handled in the conventional selectivity estimation techniques. For the many-to-many relationship, we generalize the block-hit function proposed by S.B. Yao to allow the cases that an object belongs to more than one block. For the partial participation, we propose the concept of active objects and extend our technique for total participation to handle active objects. We also propose efficient methods for obtaining statistical information needed for our estimation technique. We finally analyze the accuracy of our technique through a series of experiments and compare with the conventional ones. The experiment results showed that there was a significant inaccuracy in the estimation by the conventional ones, confirming the advantage of our technique. 
49|8|http://www.sciencedirect.com/science/journal/09505849/49/8|A new indexing method with high storage utilization and retrieval efficiency for large spatial databases|Storing and querying high-dimensional data are important problems in designing an information retrieval system. Two crucial issues, time and space efficiencies, must be considered when evaluating the performance of such a system. The KDB-tree and its variants have been reported to have good performance by using them as the index structure for retrieving multidimensional data. However, they all suffer from low storage utilization problem caused by imperfect “splitting policies.” Unnecessary splits increase the size of the index structure and deteriorate the performance of the system. In this paper, a new data insertion algorithm with a better splitting policy was proposed, which arranges data entries in the leaf nodes as many as possible. Our new index scheme can increase the storage utilization up to nearly 100% and reduce the index size to a smaller scale. As a result, both time and space efficiencies are significantly improved. Analytical and experimental results show that our indexing method outperforms the traditional KDB-tree and its variants. 
49|8||Factors affecting duration and effort estimation errors in software development projects|The purpose of this research was to fill a gap in the literature pertaining to the influence of project uncertainty and managerial factors on duration and effort estimation errors. Four dimensions were considered: project uncertainty, use of estimation development processes, use of estimation management processes, and the estimator’s experience. Correlation analysis and linear regression models were used to test the model and the hypotheses on the relations between the four dimensions and estimation errors, using a sample of 43 internal software development projects executed during the year 2002 in the IT division of a large government organization in Israel. Our findings indicate that, in general, a high level of uncertainty is associated with higher effort estimation errors while increased use of estimation development processes and estimation management processes, as well as greater estimator experience, are correlated with lower duration estimation errors. From a practical perspective, the specific findings of this study can be used as guidelines for better duration and effort estimation. Accounting for project uncertainty while managing expectations regarding estimate accuracy; investing more in detailed planning and selecting estimators based on the number of projects they have managed rather than their cumulative experience in project management, may reduce estimation errors. 
49|8||Semantic model-driven architecting of service-based software systems|Model-driven development is a software development framework that emphasises model-based abstraction and automated code generation. Service-based software architectures benefit in particular from semantic, ontology-based modelling. We present ontology-based transformation and reasoning techniques for layered semantic service architecture modelling. Integrated ontological layers support abstract domain modelling, architectural design, and interoperability aspects. Ontologies are beneficial due to their potential to formally define models, to allow reasoning about semantic models, and to automate transformations at all layers. Ontologies are suitable in particular for the Web Services platform due to their ubiquity within the Semantic Web and their application to support semantic Web services. 
49|8||Metrics for data warehouse conceptual models understandability|Due to the principal role of Data warehouses (DW) in making strategy decisions, data warehouse quality is crucial for organizations. Therefore, we should use methods, models, techniques and tools to help us in designing and maintaining high quality DWs. In the last years, there have been several approaches to design DWs from the conceptual, logical and physical perspectives. However, from our point of view, none of them provides a set of empirically validated metrics (objective indicators) to help the designer in accomplishing an outstanding model that guarantees the quality of the DW. In this paper, we firstly summarise the set of metrics we have defined to measure the understandability (a quality subcharacteristic) of conceptual models for DWs, and present their theoretical validation to assure their correct definition. Then, we focus on deeply describing the empirical validation process we have carried out through a family of experiments performed by students, professionals and experts in DWs. This family of experiments is a very important aspect in the process of validating metrics as it is widely accepted that only after performing a family of experiments, it is possible to build up the cumulative knowledge to extract useful measurement conclusions to be applied in practice. Our whole empirical process showed us that several of the proposed metrics seems to be practical indicators of the understandability of conceptual models for DWs. 
49|8||Object-based and class-based composition of transitive mixins|In object-oriented composition, classes and class inheritance are applied to realize type relationships and reusable building blocks. Unfortunately, these two goals might be contradictory in many situations, leading to classes and inheritance hierarchies that are hard to reuse. Some approaches exist to remedy this problem, such as mixins, aspects, roles, and meta-objects. However, in all these approaches, situations where the mixins, aspects, roles, or meta-objects have complex interdependencies among each other are not well solved yet. In this paper, we propose transitive mixins as an extension of the mixin concept. This approach provides a simple and reusable solution to define “mixins of mixins”. Moreover, because mixins can be easily realized on top of aspects, roles, and meta-objects, the same solution can also be applied to those other approaches. 
49|8||Testing UML designs|Early detection and correction of faults in the software design phase can reduce total cost and time to market of a software product. In this paper we describe an approach for testing UML design models to uncover inconsistencies. Our approach uses behavioral views such as Sequence Diagrams to simulate state change in an aggregate model. The aggregate model is the artifact of merging information from behavioral and structural UML views. OCL pre-conditions, post-conditions and invariants are used as a test oracle. 
49|8||Critical success factors for a customer relationship management strategy|Most organizations have perceived the customer relationship management (CRM) concept as a technological solution for problems in individual areas, accompanied by a great deal of uncoordinated initiatives. Nevertheless, CRM must be conceived as a strategy, due to its human, technological, and processes implications, at the time an organization decides to implement it. On this basis, the main goal stated in this research is to propose, justify, and validate a model based on critical success factors (CSFs) that will constitute a guide for companies in the implementation and diagnosis of a CRM strategy. The model is conformed by a set of 13 CSFs with their 55 corresponding metrics, which will serve as a guide for organizations wishing to apply this type of strategy. These factors cover the three key aspects of every CRM strategy (human factor, processes, and technology); giving a global focus and propitiating success in the implementation of a CRM strategy. These CSFs – and their metrics – were evaluated by a group of internationally experts allowing determining guidelines for a CRM implementation as well as the probable causes of the deficiencies in past projects. 
49|9-10|http://www.sciencedirect.com/science/journal/09505849/49/9-10|Consistent data for inconsistent XML document|XML document may contain inconsistencies that violate predefined integrity constraints, which causes the data inconsistency problem. In this paper, we consider how to get the consistent data from an inconsistent XML document. There are two basic concepts for this problem: Repair is the data consistent with the integrity constraints, and also minimally differs from the original one. Consistent data is the data common for every possible repair. First we give a general constraint model for XML, which can express the commonly discussed integrity constraints, including functional dependencies, keys and multivalued dependencies. Next we provide a repair framework for inconsistent XML document with three basic update operations: node insertion, node deletion and node value modification. Following this approach, we introduce the concept of repair for inconsistent XML document, discuss the chase method to generate repairs, and prove some important properties of the chase. Finally we give a method to obtain the greatest lower bound of all possible repairs, which is sufficient for consistent data. We also implement prototypes of our method, and evaluate our framework and algorithms in the experiment. 
49|9-10||Distributed priority ranking of strategic preliminary requirements for management information systems in economic organizations|The development and construction of a management information system (MIS) is a complex task. Selection of the correct requirements to be implemented in the MIS is a serious problem. The problem is made even more difficult by inadequate methods of requirements priority ranking. This paper describes instruments for distributed priority ranking of strategic preliminary requirements for MISs in organizations, profit-making or non-profit making, that are involved in the economy. The instrument consists of a metamethod that combines several methods, each accomplishing a different subtask of the priority ranking. A Web-based tool is provided to assist the requirements engineers in applying the instrument with a distributed group of stakeholders. The instrument and the tool are validated as effective by their use in an effort to create requirements for the management information system for city government. The paper reports lessons learned in this validation exercise. 
49|9-10||Method and implementation for investigating code clones in a software system|Maintaining software systems is becoming more difficult as the size and complexity of software increase. One factor that complicates software maintenance is the presence of code clones. A code clone is a code fragment that has identical or similar code fragments to it in the source code. Code clones are introduced for various reasons such as reusing code by ‘copy and paste’. If modifying a code clone with many similar code fragments, we must consider whether to modify each of them. Especially for large-scale software, such a process is very complicated and expensive. In this paper, we propose methods of visualizing and featuring code clones to support their understanding in large-scale software. The methods have been implemented as a tool called Gemini, which has applied to an open source software system. Application results show the usefulness and capability of our system. 
49|9-10||An object-oriented approach to formally analyze the UML 2.0 activity partitions|Nowadays, UML is the de-facto standard for object-oriented analysis and design. Unfortunately, the deficiency of its dynamic semantics limits the possibility of early specification analysis. UML 2.0 comes to precise and complete this semantics but it remains informal and still lacks tools for automatic validation. The main purpose of this study is to automate the formal validation, according a value-oriented approach, of the behavior of systems expressed in UML. The marriage of Petri nets with temporal logics seems a suitable formalism for translating and then validating UML state-based models. The contributions of the paper are threefold. We first, consider how UML 2.0 activity partitions can be transformed into Object Petri Nets to formalize the object dynamics, in an object-oriented context. Second, we develop an approach based on the object and sequence diagram information to initialize the derived Petri nets in terms of objects and events. Finally, to thoroughly verify if the UML model meets the system required properties, we suggest to use the OCL invariants exploiting their association end constructs. The verification is performed on a predicate/transition net explored by model checking. A case study is given to illustrate this methodology throughout the paper. 
49|9-10||Designing nesting structures of user-defined types in object-relational databases|This paper presents a methodology for designing proper nesting structures of user-defined types in object-relational databases. Briefly, we envision that users model a real-world application by using the EER model, which results in an EER schema. Our algorithm then uses the theory we developed for nested relations to generate scheme trees from the EER schema. We shall prove that the resulting scheme trees have exactly the same information content as the EER schema, and the scheme-tree instances over the resulting scheme trees do not store information redundantly. Finally, the scheme trees are transformed to Oracle Database 10g nested object types for implementation. The algorithm in this paper forms the core of a computerized object-relational database design tool we shall develop in the future. 
49|9-10||Goal-oriented test data generation for pointer programs|Automatic test data generation leads to the identification of input values on which a selected path or a selected branch is executed within a program (path-oriented vs. goal-oriented methods). In both cases, several approaches based on constraint solving exist, but in the presence of pointer variables only path-oriented methods have been proposed. Pointers are responsible for the existence of conditional aliasing problems that usually provoke the failure of the goal-oriented test data generation process. In this paper, we propose an overall constraint-based method that exploits the results of an intraprocedural points-to analysis and provides two specific constraint combinators for automatically generating goal-oriented test data. This approach correctly handles multi-levels stack-directed pointers that are mainly used in C programs. The method has been fully implemented in the test data generation tool INKA and first experiences in applying it to a variety of existing programs are presented. 
49|9-10||An objective comparison of the cost effectiveness of three testing methods|Branch testing is a well established method for exercising software. JJ-path testing, whilst employed by some practitioners, is less popular, and the testing of JJ-pairs finds few adherents. In this paper an objective, practical study of the cost-effectiveness of these three testing methods is reported. The effectiveness of each method is assessed, in the presence of infeasible paths, not only on its ability to cover the specific structural element of code that it targets, but also on its ability to cover the structural elements targeted by the other two methods – the collateral coverage it achieves. The assessment is based on the results derived from experiments in which each of the three methods is applied to 35 units of program code. 
49|9-10||The case for mesodata: An empirical investigation of an evolving database system|Database evolution can be considered a combination of schema evolution, in which the structure evolves with the addition and deletion of attributes and relations, together with domain evolution in which an attribute’s specification, semantics and/or range of allowable values changes. We present the results of an empirical investigation of the evolution of a commercial database system that measures and delineates between changes to the database that are (a) structural and (b) attribute domain related. We also estimate the impact that modelling using the mesodata approach would have on the evolving system. 
50|1-2|http://www.sciencedirect.com/science/journal/09505849/50/1-2|Editorial changes and a special issue|
50|1-2||Introduction to section most cited journal articles in software engineering|
50|1-2||An analysis of the most cited articles in software engineering journals â 2001|Citations and related work are crucial in any research to position the work and to build on the work of others. A high citation count is an indication of the influence of specific articles. The importance of citations means that it is interesting to analyze which articles are cited the most. Such an analysis has been conducted using the ISI Web of Science to identify the most cited software engineering journal articles published in 2001. The objective of the analysis is to identify and list the articles that have influenced others the most as measured by citation count. An understanding of which research is viewed by the research community as most valuable to build upon may provide valuable insights into what research to focus on now and in the future. Based on the analysis, a list of the 20 most cited articles is presented here. The intention of the analysis is twofold. First, to identify the most cited articles, and second, to invite the authors of the most cited articles in 2001 to contribute to a special section of Information and Software Technology. Three authors have accepted the invitation and their articles appear in this special section. Moreover, an analysis has been conducted regarding which authors are most productive in terms of software engineering journal publications. The latter analysis focuses on the publications in the last 20 years, which is intended as a complement to last year’s analysis focusing on the most cited articles in the last 20 years [C. Wohlin, An Analysis of the Most Cited Articles in Software Engineering Journals – 2007, Information and Software Technology 49 (1) 2–11]. The most productive author in the last 20 years is Professor Victor Basili. 
50|1-2||JADE: A software framework for developing multi-agent applications. Lessons learned|Since a number of years agent technology is considered one of the most innovative technologies for the development of distributed software systems. While not yet a mainstream approach in software engineering at large, a lot of work on agent technology has been done, many research results and applications have been presented, and some software products exists which have moved from the research community to the industrial community. One of these is JADE, a software framework that facilitates development of interoperable intelligent multi-agent systems and that is distributed under an Open Source License. JADE is a very mature product, used by a heterogeneous community of users both in research activities and in industrial applications. This paper presents JADE and its technological components together with a discussion of the possible reasons for its success and lessons learned from the somewhat detached perspective possible nine years after its inception. 
50|1-2||On adopting Content-Based Routing in service-oriented architectures|Two requirements of SOAs are the need for a global discovery agency, which assists requesters in finding their required services, and the need for new interaction paradigms, which overcome the limitations of the usual request/reply style. Content-Based Routing (CBR) holds the promise of addressing both these aspects with a single technology and a single routing infrastructure. To provide arguments for our hypothesis, we review the on going efforts for service retrieval and asynchronous communication in SOAs, identify their limitations and the advantages, and discuss how incorporating CBR into SOAs allows to solve most limitations, but also poses some interesting challenges. 
50|1-2||Engineering contextual knowledge for autonomic pervasive services|In this paper, we identify the key software engineering challenges introduced by the need of accessing and exploiting huge amount of heterogeneous contextual information. Following, we survey the relevant proposals in the area of context-aware pervasive computing, data mining and granular computing discussing their potentials and limitations with regard to their adoption in the development of context-aware pervasive services. On these bases, we propose the W4 model for contextual data and show how it can represent a simple yet effective model to enable flexible general-purpose management of contextual knowledge by pervasive services. A summarizing discussion and the identification of current limitations and open research directions conclude the paper. 
50|1-2||Editorsâ Introduction|
50|1-2||Architecting-problems rooted in requirements|Requirements permeate many parts of the software development process outside the requirements engineering (RE) process. It is thus important to determine whether software developers in these other areas of software development face any requirements-oriented (RO) problems in carrying out their tasks. Feedback so obtained can be invaluable for improving both requirements and RE technologies. In this paper, we describe an exploratory case study of requirements-oriented problems experienced by 16 architecting teams designing the same banking application. The study found that there were several different types of RO problems, of varying severity, which the architects faced in using the given requirements; those architects with RE background also faced RO problems; and about a third of all problems were RO problems. There was much concurrence of our findings with software-expert opinion from a large insurance company. There were also areas where there were relatively few RO problems. The paper also describes some implications of the findings for the RE field, particularly in the areas of: expression of quality requirements for different stakeholders; empirical studies on quality scenarios; tighter integration of RE and software architecting processes; and requirements to architecture mapping. There are opportunities for further research based on two emergent hypotheses which are also described in this paper. 
50|1-2||Requirements engineering: In search of the dependent variables|When software development teams modify their requirements engineering process as an independent variable, they often examine the implications of these process changes by assessing the quality of the products of the requirements engineering process, e.g., a software requirements specification (SRS). Using the quality of the SRS as the dependent variable is flawed. As an alternative, this paper presents a framework of dependent variables that serves as a full range for requirements engineering quality assessment. In this framework, the quality of the SRS itself is just the first level. Other higher, and more significant levels, include whether the project was successful and whether the resulting product was successful. And still higher levels include whether or not the company was successful and whether there was a positive or negative impact on society as a whole. 
50|1-2||PRiM: An iâ-based process reengineering method for information systems specification|Information system development can often be addressed as a business process reengineering practice, either because it automates some human-based processes or because it replaces an existing legacy system. Therefore, observing and analysing current processes can enable analysts to converge on the specification of the new system, generating and evaluating new system alternatives throughout. In this paper, we propose a method to support this reengineering process that analyses the strengths and weaknesses of the current process; considers the strategic needs of the organization; provides guidelines for the prescriptive construction of i∗ models; and drives the systematic generation and evaluation of alternative technological and organizational solutions for the new system. 
50|1-2||Software product release planning through optimization and what-if analysis|We present a mathematical formalization of release planning with a corresponding optimization tool that supports product and project managers during release planning. The tool is based on integer linear programming and assumes that an optimal set of requirements is the set with maximal projected revenue against available resources. The input for the optimization is twofold. The first type of input data concerns the list of candidate requirements, estimated revenues, and resources needed. Second, managerial steering mechanisms enable what-if analysis in the optimization environment. Experiments based on real-life data made a sound case for the applicability of our approach. 
50|1-2||Rigorous engineering of product-line requirements: A case study in failure management|We consider the failure detection and management function for engine control systems as an application domain where product line engineering is indicated. The need to develop a generic requirement set – for subsequent system instantiation – is complicated by the addition of the high levels of verification demanded by this safety-critical domain, subject to avionics industry standards. We present our case study experience in this area as a candidate method for the engineering, validation and verification of generic requirements using domain engineering and Formal Methods techniques and tools. For a defined class of systems, the case study produces a generic requirement set in UML and an example system instance. Domain analysis and engineering produce a validated model which is integrated with the formal specification/verification method B by the use of our UML-B profile. The formal verification both of the generic requirement set, and of a simple system instance, is demonstrated using our U2B, ProB and prototype Requirements Manager tools.This work is a demonstrator for a tool-supported method which will be an output of EU project RODIN (This work is conducted in the setting of the EU funded Research Project: IST 511599 RODIN (Rigorous Open Development Environment for Complex Systems) http://rodin.cs.ncl.ac.uk/). The use of existing and prototype formal verification and support tools is discussed. The method, developed in application to this novel combination of product line, failure management and safety-critical engineering, is evaluated and considered to be applicable to a wide range of domains. 
50|1-2||Reviewers List|
50|11|http://www.sciencedirect.com/science/journal/09505849/50/11|Knowledge management in software engineering: A systematic review of studied concepts, findings and research methods used|Software engineering is knowledge-intensive work, and how to manage software engineering knowledge has received much attention. This systematic review identifies empirical studies of knowledge management initiatives in software engineering, and discusses the concepts studied, the major findings, and the research methods used. Seven hundred and sixty-two articles were identified, of which 68 were studies in an industry context. Of these, 29 were empirical studies and 39 reports of lessons learned. More than half of the empirical studies were case studies.The majority of empirical studies relate to technocratic and behavioural aspects of knowledge management, while there are few studies relating to economic, spatial and cartographic approaches. A finding reported across multiple papers was the need to not focus exclusively on explicit knowledge, but also consider tacit knowledge. We also describe implications for research and for practice. 
50|11||Enhancing conflict detecting mechanism for Web Services composition: A business process flow model transformation approach|Businesses today are keenly aware of the competitive role of information technology. However, there is still a need for application systems to be implemented for support of daily business operations. However, these systems may be developed or acquired at different times from different vendors, and thus may be incompatible with each other. Because of this, a number of standards and technologies have been addressed to draw an integral vision in the field of enterprise application integration (EAI).Emerging Web Services has now promised enterprises that they can deploy new digital services faster than ever before by integrating the existing application systems. By integrating in this fashion, operational processes of heterogeneous systems are seamlessly controlled and integrated through a meta-processes. This integration could preserve the autonomy of original systems, as well as enhance the flexibility and agility of the business. The researchers who constructed this study have attempted to explore in detail the design of these meta-processes. Also the researchers have proposed a conflict detecting mechanism (CDM) in order to aid process designers. A prototype with a case study has been developed to evaluate the feasibility of the CDM, and the results show that it is useful in guiding process designers to correct and improve their designed meta-processes. 
50|11||Towards software process patterns: An empirical analysis of the behavior of student teams|Traditional software engineering processes are composed of practices defined by roles, activities and artifacts. Software developers have their own understanding of practices and their own ways of implementing them, which could result in variations in software development practices. This paper presents an empirical study based on six teams of five students each, involving three different projects. Their process practices are monitored by time slips based on the effort expended on various process-related activities. This study introduces a new 3-pole graphical representation to represent the process patterns of effort expended on the various discipline activities. The purpose of this study is to quantify activity patterns in the actual process, which in turn demonstrates the variability of process performance. This empirical study provides three examples of patterns based on three empirical axes (engineering, coding and V&V). The idea behind this research is to make developers aware that there is wide variability in the actual process, and that process assessments might be weakly related to actual process activities. This study suggests that in-process monitoring is required to control the process activities. In-process monitoring is likely to provide causal information between the actual process activities and the quality of the implemented components. 
50|11||The software product line architecture: An empirical investigation of key process activities|Software architecture has been a key area of concern in software industry due to its profound impact on the productivity and quality of software products. This is even more crucial in case of software product line, because it deals with the development of a line of products sharing common architecture and having controlled variability. The main contributions of this paper is to increase the understanding of the influence of key software product line architecture process activities on the overall performance of software product line by conducting a comprehensive empirical investigation covering a broad range of organizations currently involved in the business of software product lines. This is the first study to empirically investigate and demonstrate the relationships between some of the software product line architecture process activities and the overall software product line performance of an organization at the best of our knowledge. The results of this investigation provide empirical evidence that software product line architecture process activities play a significant role in successfully developing and managing a software product line. 
50|11||Exploring the underlying aspects of pair programming: The impact of personality|With the recent advent of agile software process methods, a number of seldom used and unorthodox practices have come to the forefront in the field of computer programming. One such practice is that of pair programming, which is characterized by two programmers sharing the same computer for collaborative programming purposes. The very nature of pair programming implies a psychological and social interaction between the participating programmers and thus brings into play a unique element that we do not see with the conventional individual programming model. This paper focuses on the effects that one of these psychosocial factors, a programmer’s personality type, may have on the pair programming environment. In this study, a group of university students, 68 undergraduate students and 60 master’s degree graduate students, each of whom had been personality type profiled using the Myers–Briggs Type Indicator (MBTI) model, was split into three sub-groups. One group consisted of subjects who were alike in MBTI type. Another group consisted of subjects who were opposite to each other in MBTI type, and the last group was comprised of subjects who were diverse – partially alike and partially opposite – in MBTI type. Through two pair programming sessions, the pairs in each group were assessed for their output, in code productivity. The result showed that the sub-group of subjects who were diverse in MBTI type exhibited higher productivity than both alike and opposite groups. In a comparison between alike and opposite groups, the productivity of the opposite group was greater than that of the alike group. 
50|11||A methodology for business process improvement and IS development|This paper aims at discussing business process modelling and improvement as an essential work to create a successful and competitive enterprise. To achieve this goal, we use a methodology called TAD, which consists of six phases. The first three deal with business process identification, modelling and improvement. The methodology presents a new unique way for business process identification, modelling, and improvement. The last three phases continue with the implementation of the improved business process(es) by developing its information system. The business process “Surgery” is used as an example to show the implementation of the methodology. 
50|11||Time-line based model for software project scheduling with genetic algorithms|Effective management of complex software projects depends on the ability to solve complex, subtle optimization problems. Most studies on software project management do not pay enough attention to difficult problems such as employee-to-task assignments, which require optimal schedules and careful use of resources. Commercial tools, such as Microsoft Project, assume that managers as users are capable of assigning tasks to employees to achieve the efficiency of resource utilization, while the project continually evolves. Our earlier work applied genetic algorithms (GAs) to these problems. This paper extends that work, introducing a new, richer model that is capable of more realistically simulating real-world situations. The new model is described along with a new GA that produces optimal or near-optimal schedules. Simulation results show that this new model enhances the ability of GA-based approaches, while providing decision support under more realistic conditions. 
50|11||Ontological modelling of content management and provision|Information provision to address the changing requirements can be best supported by content management. The current information technology enables information to be stored and provided from various distributed sources. To identify and retrieve relevant information requires effective mechanisms for information discovery and assembly. This paper presents a method, which enables the design of such mechanisms, with a set of techniques for articulating and profiling users’ requirements, formulating information provision specifications, realising management of information content in repositories, and facilitating response to the user’s requirements dynamically during the process of knowledge construction. These functions are represented in an ontology which integrates the capability of the mechanisms. The ontological modelling in this paper has adopted semiotics principles with embedded norms to ensure coherent course of actions represented in these mechanisms. 
50|11||The impacts of function extraction technology on program comprehension: A controlled experiment|Program comprehension is a critical, time-consuming, and highly error-prone task for software developers. Function extraction (FX) is a theory and technology that automates and supports program comprehension by calculating the semantic behaviors of programs at many levels of abstraction and displaying those behaviors in a standard, readable format in terms of the “as-built” specification of the program. In this experimental study, developers using an FX prototype tool to assist them in determining the behavior of software modules have significantly more effective program comprehension, in both increased accuracy of understanding and reduced time on task. Moreover, developers have a positive reaction toward the use of the FX technology, and use of FX does not reduce their overall comprehension of the program. 
50|12|http://www.sciencedirect.com/science/journal/09505849/50/12|An empirical study of the CobbâDouglas production function properties of software development effort|In this paper we study whether software development effort exhibits Cobb–Douglas functional form with respect to team size and software size. We empirically test this relationship using real-world software engineering data set containing over 500 software projects. The results of our experiments indicate that the hypothesized Cobb–Douglas function form for software development effort with respect to team size and software size is true. We also find increasing returns to scale relationship between software size and team size with software development effort. 
50|12||Locating dependence structures using search-based slicing|This paper introduces an approach to locating dependence structures in a program by searching the space of the powerset of the set of all possible program slices. The paper formulates this problem as a search-based software engineering problem. To evaluate the approach, the paper introduces an instance of a search-based slicing problem concerned with locating sets of slices that decompose a program into a set of covering slices that minimize inter-slice overlap. The paper reports the result of an empirical study of algorithm performance and result-similarity for Hill Climbing, Genetic, Random Search and Greedy Algorithms applied to a set of 12 C programs. 
50|12||From page-centric to portlet-centric Web development: Easing the transition using MDD|Portlet syndication is the next wave following the successful use of content syndication in current portals. Portlets can be regarded as Web components, and the portal as the component container where portlets are aggregated to provide higher-order applications. This perspective requires a departure from how current Web portals are envisaged. The portal is no longer perceived as a set of pages but as an integrated set of Web components that are now delivered through the portal. From this perspective, the portal page now acts as a mere conduit for portlets. Page and page navigation dilute in favor of portlet and portlet orchestration. However, the mapping from portlet orchestration (design time) to page navigation (implementation time) is too tedious and error prone. For instance, the fact that the same portlet can be placed in distinct pages produces code clones that are repeated along the pages that contain this portlet. This redundancy substantiates in the first place the effort to move to model-driven development. This work uses the eXo platform as the target PSM, and the PIM is based on Hypermedia Model Based on Statecharts. The paper shows how this approach accounts for portal validation/verification to be conducted earlier at the PIM level, and streamlines both design and implementation of eXo portals. A running example is used throughout the paper. 
50|12||Does software reliability growth behavior follow a non-homogeneous Poisson process|It is widely believed in software reliability community that software reliability growth behavior follows a non-homogeneous Poisson process (NHPP) based on analyzing the behavior of the mean of the cumulative number of observed software failures. In this paper we present two controlled software experiments to examine this belief. The behavior of the mean of the cumulative number of observed software failures and that of the corresponding variance are examined simultaneously. Both empirical observations and statistical hypothesis testing suggest that software reliability behavior does not follow a non-homogeneous Poisson process in general, and does not fit the Goel–Okumoto NHPP model in particular. Although this new finding should be further tested on other software experiments, it is reasonable to cast doubt on the validity of the NHPP framework for software reliability modeling. The importance of the work presented in this paper is not only for the new finding which is distinctly different from existing popular belief of software reliability modeling, but also for the adopted research approach which is to examine the behavior of the mean and that of the corresponding variance simultaneously on basis of controlled software experiments. 
50|12||Generating CAM aspect-oriented architectures using Model-Driven Development|Aspect-Oriented Software Development promotes the separation of those concerns that cut across several components and/or are tangled with the base functionality of a component, through all phases of the software lifecycle. The benefit of identifying these crosscutting concerns (aspects) at the architectural level in particular is to improve the architecture design and its subsequent evolution, before moving onto detailed design and implementation. However, software architects are not usually experts on using specific AO architecture notations. Therefore, the aim of this paper is to provide support to define and specify aspect-oriented (AO) architectures using non-AO ones as the source. We will use the Model-Driven Development approach to transform a component-based architecture model into an AO architecture model. The CAM (component and aspect model) model and the DAOP–ADL language are the proposals used for modelling and specifying AO architectures. We will show how we automated part of the process and the tool support. 
50|12||SOPHIE: Use case and evaluation|Services communicate with each other by exchanging self-contained messages. Depending on the specific requirements of the business model they serve and the application domain for which services were deployed, a number of mismatches (i.e. sequence and cardinality of messages exchanges, structure and format of messages and content semantics), can occur which prevent interoperation among a priori compatible services. This paper presents the evaluation of SOPHIE, a conceptual framework for supporting the conceptualization of ontology-based services choreographies. In doing so a three fold approach is taken that considers formal, epistemological and technical aspects. The formal evaluation tries to prove the consistency, completeness and conciseness of ontological model used. The epistemological evaluation enumerates the improvements and differentiating aspect of SOPHIE with respect to existing related work and reviews a number of application areas where the work was successfully applied. Finally, the technical feasibility evaluation tries to demonstrate the viability of the approach from the point of view of the engineering process required to allow the interaction of heterogeneous Semantic Services. 
50|12||Semantics and analysis of business process models in BPMN|The Business Process Modelling Notation (BPMN) is a standard for capturing business processes in the early phases of systems development. The mix of constructs found in BPMN makes it possible to create models with semantic errors. Such errors are especially serious, because errors in the early phases of systems development are among the most costly and hardest to correct. The ability to statically check the semantic correctness of models is thus a desirable feature for modelling tools based on BPMN. Accordingly, this paper proposes a mapping from BPMN to a formal language, namely Petri nets, for which efficient analysis techniques are available. The proposed mapping has been implemented as a tool that, in conjunction with existing Petri net-based tools, enables the static analysis of BPMN models. The formalisation also led to the identification of deficiencies in the BPMN standard specification. 
50|12||Transformation techniques can make students excited about formal methods|Formal methods have always been controversial. In spite of the fact that the disbelief about their usefulness has been corrected by a growing number of applications and even more publications, it remains a challenge to demonstrate the strengths and weaknesses of formal methods within the time constraints of a typical semester course. This article1 reports on a new course at the University of Antwerp in which the introduction of a new formalism yields a better understanding of previously taught ones. While the exercises are designed to reveal the limitations of the formalisms used, students remain convinced that their formal models have more value than conventional source code. 
50|3|http://www.sciencedirect.com/science/journal/09505849/50/3|Translating unstructured workflow processes to readable BPEL: Theory and implementation|The Business Process Execution Language for Web Services (BPEL) has emerged as the de facto standard for implementing processes. Although intended as a language for connecting web services, its application is not limited to cross-organizational processes. It is expected that in the near future a wide variety of process-aware information systems will be realized using BPEL. While being a powerful language, BPEL is difficult to use. Its XML representation is very verbose and only readable for the trained eye. It offers many constructs and typically things can be implemented in many ways, e.g., using links and the flow construct or using sequences and switches. As a result only experienced users are able to select the right construct. Several vendors offer a graphical interface that generates BPEL code. However, the graphical representations are a direct reflection of the BPEL code and not easy to use by end-users. Therefore, we provide a mapping from Workflow Nets (WF-nets) to BPEL. This mapping builds on the rich theory of Petri nets and can also be used to map other languages (e.g., UML, EPC, BPMN, etc.) onto BPEL. In addition to this we have implemented the algorithm in a tool called WorkflowNet2BPEL4WS. 
50|3||Applying static analysis for automated extraction of database interactions in web applications|Database interactions are among the most essential functional features in web applications. Therefore, for the testing and maintenance of a web application, it is important that the web engineer could identify all the database interactions in the web application. However, the highly dynamic nature of web applications makes it challenging to extract all the possible database interactions from source code.In this paper, we propose an automated approach to extract database interactions from source code by using symbolic execution and inference rules. Our approach automatically identifies all the possible database interaction points. After that, all the program paths, which pass through each interaction point, are also computed. Each of these paths is then symbolically executed following our proposed symbolic evaluation rules. We also develop inference rules to deduce the interaction types from the set of symbolic expressions derived during the symbolic execution. Experiments have been conducted to evaluate the performance and usefulness of the proposed approach. The results indicate that even with some limitations in handling function calls, pointers and polymorphism, our approach still gives an average precision of 79.2%, which is 45.4% more than that of the conservative approach. 
50|3||An evaluation and selection framework for interoperability standards|There is a wide range of standards available for the integration and interoperability of applications and information systems, both on domain-specific and domain-neutral levels. The evaluation and selection of interoperability standards are necessary in the application development and integration projects, when there is a need to assess the usefulness of existing models or to find open solutions. In addition, standards have to be evaluated when recommendations are made for a given domain or when their quality is examined. The evaluation of the scope and other aspects of interoperability standards is usually performed against project-specific requirements, but generic frameworks can be used for supporting the evaluation. In this article, we present a conceptual framework which has been developed for the systematic evaluation of interoperability standards. We also present an overview of a process for the evaluation of interoperability standards. We illustrate the use of these models with practical experience and examples. 
50|3||Scenario support for effective requirements|Scenarios are widely used as requirements, and the quality of requirements is an important factor in the efficiency and success of a development project. The informal nature of scenarios requires that analysts do much manual work with them, and much tedious and detailed effort is needed to make a collection of scenarios well-defined, relatively complete, minimal, and coherent. We discuss six aspects of scenarios having inherent structure on which automated support may be based, and the results of using such support. This automated support frees analysts to concentrate on tasks requiring human intelligence, resulting in higher-quality scenarios for better system requirements. Two studies validating the work are presented. 
50|3||Improving analogy-based software cost estimation by a resampling method|Estimation by analogy (EbA) is a well-known technique for software cost estimation. The popularity of the method is due to its straightforwardness and its intuitively appealing interpretation. However, in spite of the simplicity in application, the theoretical study of EbA is quite complicated. In this paper, we exploit the relation of EbA method to the nearest neighbor non-parametric regression in order to suggest a resampling procedure, known as iterated bagging, for reducing the prediction error. The improving effect of iterated bagging on EbA is validated using both artificial and real datasets from the literature, obtaining very promising results. 
50|3||Pair programming in software development teams â An empirical study of its benefits|We present the results of an extensive and substantial case study on pair programming, which was carried out in courses for software development at the University of Dortmund, Germany. Thirteen software development teams with about 100 students took part in the experiments. The groups were divided into two sets with different working conditions. In one set, the group members worked on their projects in pairs. Even though the paired teams could only use half of the workstations the teams of individual workers could use, the paired teams produced nearly as much code as the teams of individual workers at the same time. In addition, the code produced by the paired teams was easier to read and to understand. This facilitates finding errors and maintenance. 
50|4|http://www.sciencedirect.com/science/journal/09505849/50/4|Towards management of software as assets: A literature review with additional sources|How should and how can software be managed? What is the management concept or paradigm? Software professionals, if they think about management of software at all, think in terms of Configuration Management. This is not a method for over-all software management; it merely controls software items’ versions. This is much too fine a level of granularity.Management begins with accurate and timely information. Managers tend to view software as something (unfortunately) very necessary but troubling because, they have very little real information about it and control is still nebulous, at best. Accountants view software as an incomprehensible intangible, neither wholly an expense nor really an asset. They do not have, nor do they produce information concerning it. Their data concerning software barely touches on direct outlays and contains no element of effort.Part of this disorientation is the basic confusion between “business software” and “engineering software”. This “Gordian Knot” must be opened; it needs to be made much more clear. This article shows a direction how such clarity may be achieved. 
50|4||Micro and macro workflow variability design techniques of component|Components should provide variability in satisfying a variety of domains [C. Szyperski, Component Software: Beyond Object-Oriented Programming, Addison-Wesley, 2002.], but it is not easy to develop components which can be applied to all domains. Although components are developed by analyzing many different requirements, developing components that satisfy all requirements is difficult since unexpected requirements occur during the utilization of components. Hence, providing the variability of components becomes an important prerequisite for a successful component-based application development.In this paper, we propose a variability design technique that can satisfy the business workflow requirements of many different kinds of domains. The technique addresses a method for designing the variability of the workflow in a more detailed method and uses an object-oriented mechanism and design patterns. One of the most important goals of this technique is to provide a practical process can be effectively applied in component-based application development. 
50|4||An evaluation of the degree of agility in six agile methods and its applicability for method engineering|While agile methods are in use in industry, little research has been undertaken into what is meant by agility and how a supposed agile method can be evaluated with regard to its veracity to belong to this category of software development methodological approaches. Here, an analytical framework, called 4-DAT, is developed and applied to six well-known agile methods and, for comparison, two traditional methods. The results indicate the degree of agility to be found in each method, from which a judgement can be made as to whether the appellation of “agile” to that method is appropriate. This information is shown to be useful, for example, when constructing a methodology from method fragments (method engineering) and when comparing agile and traditional methods. 
50|4||On the interplay between inconsistency and incompleteness in multi-perspective requirements specifications|A major challenge for dealing with multi-perspective specifications, and more concretely, with merging of several descriptions or views is toleration of incompleteness and inconsistency: views may be inconclusive, and may have conflicts over the concepts being modeled. The desire of being able to tolerate both phenomena introduces the need to evaluate and quantify the significance of a detected inconsistency as well as to measure the degree of conflict and uncertainty of the merged view as the specification process evolves.We show in this paper to what extent disagreement and incompleteness are closely interrelated and play a central role to obtain a measure of the level of inconsistency and to define a merging operator whose aim is getting the model which best reflects the combined knowledge of all stakeholders. We will also propose two kinds of interesting and useful orderings among perspectives which are based on differences of behavior and inconsistency, respectively. 
50|4||Achieving Mobile Agent Systems interoperability through software layering|Interoperability is a key issue for a wider adoption of mobile agent systems (MASs) in heterogeneous and open distributed environments where agents, in order to fulfill their tasks, must interact with non-homogeneous agents and traverse different agent platforms to access remote resources. To date, while several approaches have been proposed to deal with different aspects of MAS interoperability, they all lack the necessary flexibility to provide an adequate degree of interoperability among the currently available MASs. In this paper, we propose an application-level approach grounded in the software layering concept, which enables execution, migration and communication interoperability between Java-based mobile agent systems, thus overcoming major setbacks affecting the other approaches currently proposed for supporting MAS interoperability. In particular, we define a Java-based framework, named JIMAF, which relies on an event-driven, proxy-based mobile agent model and supports interoperable mobile agents which can be easily coded and adapted to existing MASs without any modifications of the MAS infrastructures. Results from the performance evaluation of MAS interoperability was carried by using JIMAF atop Aglets, Ajanta, Grasshopper, and Voyager, demonstrating that the high-level JIMAF approach offers high efficacy while maintaining overhead at acceptable levels for target computing environments. 
50|4||Patterns and technologies for enabling supply chain traceability through collaborative e-business|Industrial traceability systems are designed to operate over complex supply chains, with a large and dynamic group of participants. These systems need to agree on processing and marketing of goods, information management, responsibility, and identification. In addition, they should guarantee context independence, scalability, and interoperability. In this paper, we first discuss the main issues emerging at different abstraction levels in developing traceability systems. Second, we introduce a data model for traceability and a set of suitable patterns to encode generic traceability semantics. Then, we discuss suitable technological standards to define, register, and enable business collaborations. Finally, we show a practical implementation of a traceability system through a real world experience on food supply chains. 
50|5|http://www.sciencedirect.com/science/journal/09505849/50/5|Self-organization process in open-source software: An empirical study|Software systems must continually evolve to adapt to new functional requirements or quality requirements to remain competitive in the marketplace. However, different software systems follow different strategies to evolve, affecting both the release plan and the quality of these systems. In this paper, software evolution is considered as a self-organization process and the difference between closed-source software and open-source software is discussed in terms of self-organization. In particular, an empirical study of the evolution of Linux from version 2.4.0 to version 2.6.13 is reported. The study shows how open-source software systems self-organize to adapt to functional requirements and quality requirements. 
50|5||Efficient mining of frequent XML query patterns with repeating-siblings|A recent approach to improve the performance of XML query evaluation is to cache the query results of frequent query patterns. Unfortunately, discovering these frequent query patterns is an expensive operation. In this paper, we develop a two-pass mining algorithm 2PXMiner that guarantees the discovery of frequent query patterns by scanning the database at most twice. By exploiting a transaction summary data structure, and an enumeration tree, we are able to determine the upper bounds of the frequencies of the candidate patterns, and to quickly prune away the infrequent patterns. We also design an index to trace the repeating candidate subtrees generated by sibling repetition, thus avoiding redundant computations. Experiments results indicate that 2PXMiner is both efficient and scalable. 
50|5||A preliminary study on various implementation approaches of domain-specific language|Various implementation approaches for developing a domain-specific language are available in literature. There are certain common beliefs about the advantages/disadvantages of these approaches. However, it is hard to be objective and speak in favor of a particular one, since these implementation approaches are normally compared over diverse application domains.The purpose of this paper is to provide empirical results from ten diverse implementation approaches for domain-specific languages, but conducted using the same representative language. Comparison shows that these discussed approaches differ in terms of the effort need to implement them, however, the effort needed by a programmer to implement a domain-specific language should not be the only factor taken into consideration. Another important factor is the effort needed by an end-user to rapidly write correct programs using the produced domain-specific language. Therefore, this paper also provides empirical results on end-user productivity, which is measured as the lines of code needed to express a domain-specific program, similarity to the original notation, and how error-reporting and debugging are supported in a given implementation. 
50|5||Constraint-driven development|To obtain the full benefits of model-driven development (MDD) approaches such as MDA, a suitable level of abstraction needs to be chosen which enables the core functionality and properties of a system to be expressed, independent of programming language or implementation platform, so that this specification can be reused for a wide variety of different environments.This paper describes how constraints, together with UML class diagrams and state machines, can be used as a precise and platform-independent specification language. We describe the use of constraints in UML-RSDS and tool support for the synthesis of executable systems from constraints. 
50|5||Accommodating mesodata into conceptual modelling methodologies|Mesodata modelling is a recently developed approach for enhancing a data model’s capabilities by providing for more advanced semantics to be associated with the domain of an attribute. Mesodata supplies both an inter-value structure to the domain and a set of operations applicable to that structure that may be used to facilitate additional functionality in a database. We argue that conceptual modelling methodologies would be semantically richer if they were able to express the semantics of complex data types for attribute domains. This paper investigates the accommodation of mesodata into the entity-relationship and object role modelling, presenting the Mesodata Entity-Relationship (MDER) model and Mesodata Object Role Modelling (MDORM), which show how the mesodata concept can be incorporated into conceptual modelling methodologies to include the semantics of complex-domain structures. 
50|5||Specifying and validating structural constraints of analysis class models using OCL|Analysis modeling focuses on functional requirements and postpone implementation specific issues until subsequent design activities are undertaken. Based on the analysis models, the design activities are performed by refining and clarifying the analysis models. Thus, the quality of analysis models has a vast impact on the design models. Therefore, much effort should be taken to build correct analysis models.In this paper, we propose structural constraints that analysis class models of information systems should satisfy, and describe an OCL-based approach to validating the analysis class models against the constraints. In addition, through a case study with four medium-sized industrial information systems, we find that the proposed approach can help to identify deficiencies in analysis models. 
50|5||An approach for the maintenance of input validation|Input validation is the enforcement of constraints that an input must satisfy before it is accepted in a program. It is an essential and important feature in a large class of systems and usually forms a major part of a data-intensive system. Currently, the design and implementation of input validation are carried out by application developers. The recovery and maintenance of input validation implemented in a system is a challenging issue. In this paper, we introduce a variant of control flow graph, called validation flow graph as a model to analyze input validation implemented in a program. We have also discovered some empirical properties that characterizing the implementation of input validation. Based on the model and the properties discovered, we then propose a method that recovers the input validation model from source and use program slicing techniques to aid the understanding and maintenance of input validation. We have also evaluated the proposed method through case studies. The results show that the method can be very useful and effective for both experienced and inexperienced developers. 
50|5||XTRON: An XML data management system using relational databases|Recently, there has been plenty of interest in XML. Since the amount of data in XML format has rapidly increased, the need for effective storage and retrieval of XML data has arisen. Many database researchers and vendors have proposed various techniques and tools for XML data storage and retrieval in recent years. In this paper, we present an XML data management system using a relational database as a repository. Our XML management system stores XML data in a schema independent manner, and translates a comprehensive subset of XQuery expressions into a single SQL statement. Also, our system does not modify the relational engine. In this paper, we also present the experimental results in order to demonstrate the efficiency and scalability of our system compared with well-known XML processing systems. 
50|6|http://www.sciencedirect.com/science/journal/09505849/50/6|A framework to analyze information systems as knowledge flow facilitators|This paper presents a framework which can be used to analyze information systems as knowledge flow facilitators in organizational processes. This framework may be useful, particularly to small organizations, for two main reasons: it can help them to start seeing the implications of KM in their current technical infrastructure, and as a result, they should be in a better position to know how to include their current working tools in part of a KM strategy, thus facilitating the alignment of such a strategy to the daily work of the organization. Second, identifying the role that their current tools play in the flow of knowledge should help such organizations to identify means by which to improve such tools as KM enablers, before becoming engaged in costly KM efforts that could require the acquisition of new tools and often also big changes in their current work processes. The applicability of the framework is illustrated with a case study conducted in a software development environment in which it was successfully applied. 
50|6||An empirical investigation of the drivers of software outsourcing decisions in Japanese organizations|Although Japan represents the single largest Asian market and 10% of the global software outsourcing market, little is understood about how Japanese companies make software project outsourcing decisions. Tried-and-tested outsourcing models consistently fail to predict the outsourcing decisions of Japanese companies, leaving global software development companies with little usable guidance in the Japanese outsourcing market. Analyses of 396 software project outsourcing decisions made by 33 IT managers in Toshiba, Hitachi, Fujitsu, IBM-Japan, and Mitsubishi provides novel insights into the drivers of Japanese software outsourcing decisions. The objective of this paper is to develop an analytic tool to predict the likelihood of a software project being outsourced by Japanese IT managers. 
50|6||Investigating Knowledge Management practices in software development organisations â An Australian experience|This study, using both quantitative and qualitative methods, investigates current practice of Knowledge Management (KM) in Software Engineering (SE) processes in two Australian companies on the basis that they both claimed to apply KM practices in their software development work. It also describes the KM activities and KM process used in SE practice, and examines the enablers of KM process for SE in terms of leadership, technology, culture, process and measurement.One of the main findings showed that software developers believe in the usefulness of knowledge sharing; however, their ability to utilise some of the KM systems was limited. The most commonly used systems included personal networks, informal networks, groupware and third-party knowledge. There is a need to formalise knowledge sharing of practices, while also supporting informal and ad-hoc knowledge sharing. While KM was considered to be important, the tools, techniques and methodologies currently employed for software development were inadequate to address effective management of knowledge in these organisations. In both organisations, a uniform model of the KM process did not exist. Among the four KM enablers, leadership was considered to be the most significant as top-down KM strategies were seemingly being pursued by management. Technology was also considered to be an obvious mechanism for KM, despite some of their current KM systems either being unsuitable or inaccessible. In addition, the crucial role that personal networks played in accessing tacit and implicit knowledge was seen as a key reason to foster a culture that encourages participants to share their knowledge with others. 
50|6||An experimental study of four typical test suite reduction techniques|In software development, developers often rely on testing to reveal bugs. Typically, a test suite should be prepared before initial testing, and new test cases may be added to the test suite during the whole testing process. This may usually cause the test suite to contain more or less redundancy. In other words, a subset of the test suite (called the representative set) may still satisfy all the test objectives. As the redundancy can increase the cost of executing the test suite, quite a few test suite reduction techniques have been brought out in spite of the NP-completeness of the general problem of finding the optimal representative set of a test suite. In the literature, there have been some experimental studies of test suite reduction techniques, but the limitations of the these experimental studies are quite obvious. Recently proposed techniques are not experimentally compared against each other, and reported experiments are mainly based on small programs or even simulation data. This paper presents a new experimental study of the four typical test suite reduction techniques, including Harrold et al.’s heuristic, and three other recently proposed techniques such as Chen and Lau’s GRE heuristic, Mansour and El-Fakin’s genetic algorithm-based approach, and Black et al.’s ILP-based approach. Based on the results of this experimental study, we also provide a guideline for choosing the appropriate test suite reduction technique and some insights into why the techniques vary in effectiveness and efficiency. 
50|6||Enhancing usability testing through datamining techniques: A novel approach to detecting usability problem patterns for a context of use|Usability is a software attribute usually associated with the “ease of use and to learn” of a given interactive system. Nowadays usability evaluation is becoming an important part of software development, providing results based on quantitative and qualitative estimations. In this context, qualitative results are usually obtained through a Qualitative Usability Testing process which includes a number of different methods focused on analyzing the interface of a particular interactive system. These methods become complex when a large number of interactive systems belonging to the same context of use have to be jointly considered to provide a general diagnosis, as a considerable amount of information must be visualized and treated simultaneously. However, diagnosing the most general usability problems of a context of use as a whole from a qualitative viewpoint is a challenge for UE nowadays. Identifying such problems can help to evaluate a new interface belonging to this context, and to prevent usability errors when a novel interactive system is being developed. From a quantitative viewpoint, condensing results in singles scores, metrics or statistical functions is an acceptable solution for processing huge amounts of usability related information. Nevertheless, QUT processes need to keep their richness by prioritizing the “what” over the “how much/how many” questions related to the detection of usability problems.To cope with the above situation, this paper presents a new approach in which two datamining techniques (association rules and decision trees) are used to extend the existing Qualitative Usability Testing process in order to provide a general usability diagnosis of a given context of use from a qualitative viewpoint. In order to validate our proposal, usability problems patterns belonging to academic webpages in Spanish-speaking countries are assessed by processing 3450 records which store qualitative information collected by means of a Heuristic Evaluation. 
50|6||On-line generation association rules over data streams|In order to efficiently trace the changes of association rules over an online data stream, this paper proposes a method of generating association rules directly over the changing set of currently frequent itemsets. While all of the currently frequent itemsets in an online data stream are monitored by the estDec method, all the association rules of every frequent itemset in the prefix tree of the estDec method are generated by the proposed method in this paper. For this purpose, a traversal stack is introduced to efficiently enumerate all association rules in the prefix tree. This online implementation can avoid the drawbacks of the conventional two-step approach. In addition, the prefix tree itself can be utilized as an index structure for finding the current support of the antecedent of an association rule. Finally, the performance of the proposed method is analyzed by a series of experiments to identify its various characteristics. 
50|6||Effectively utilizing project, product and process knowledge|Improving project management, product development and engineering processes is for many companies crucial to survive in a fast changing environment. However, these activities are rarely integrated well due to the diversity of stakeholders with individual knowledge about projects, products and processes. This case study shows how Alcatel-Lucent over time achieved effective interaction of engineering processes, tools and people on the basis of a knowledge-centric product life-cycle management (PLM). Starting from identifying project, product and process knowledge, we show how they can be effectively integrated for best possible usage across the enterprise. The case study provides insight into how to best embark on PLM and how to effectively integrate product development with supportive tools. It describes how the concepts can be transferred to software engineering teams and IT departments in other companies. Concrete results from several product lines, such as efficiency improvement and better global development underline the business value. 
50|6||Estimating the coverage of the framework application reusable cluster-based test cases|Object-oriented frameworks support both software code and design reusability. In addition, it is found that providing class-based tests with the framework reduces considerably the class-based testing time and effort of the applications developed using the frameworks. Similarly, reusable cluster-based test cases can be generated using the framework hooks, and they, too, can be provided with the framework to reduce the cluster testing time and effort of the framework applications.In this paper, we introduce a methodology to estimate the possible coverage of the cluster-based reusable test cases for framework applications prior to suggesting and applying a specific technique to produce the test cases. An experimental case study is conducted to demonstrate the practical issues in applying the introduced methodology and to give insights on the possible coverage of the framework reusable cluster-based test cases. The results of applying the methodology on five framework applications show that, on average, the reusable cluster-based test cases cover at least one-third of the cluster testing areas of the interface classes created during the framework application engineering stage. 
50|7-8|http://www.sciencedirect.com/science/journal/09505849/50/7-8|Systematic review of organizational motivations for adopting CMM-based SPI|Background: Software Process Improvement (SPI) is intended to improve software engineering, but can only be effective if used. To improve SPI’s uptake, we should understand why organizations adopt SPI. CMM-based SPI approaches are widely known and studied. Objective: We investigated why organizations adopt CMM-based SPI approaches, and how these motivations relate to organizations’ size. Method: We performed a systematic review, examining reasons reported in more than forty primary studies. Results: Reasons usually related to product quality and project performance, and less commonly, to process. Organizations reported customer reasons infrequently and employee reasons very rarely. We could not show that reasons related to size. Conclusion: Despite its origins in helping to address customer-related issues for the USAF, CMM-based SPI has mostly been adopted to help organizations improve project performance and product quality issues. This reinforces a view that the goal of SPI is not to improve process per se, but instead to provide business benefits. 
50|7-8||A test driven approach for aspectualizing legacy software using mock systems|Aspect-based refactoring, called aspectualization, involves moving program code that implements cross-cutting concerns into aspects. Such refactoring can improve the maintainability of legacy systems. Long compilation and weave times, and the lack of an appropriate testing methodology are two challenges to the aspectualization of large legacy systems. We propose an iterative test driven approach for creating and introducing aspects. The approach uses mock systems that enable aspect developers to quickly experiment with different pointcuts and advice, and reduce the compile and weave times. The approach also uses weave analysis, regression testing, and code coverage analysis to test the aspects. We developed several tools for unit and integration testing. We demonstrate the test driven approach in the context of large industrial C++ systems, and we provide guidelines for mock system creation. 
50|7-8||Heuristics-based infeasible path detection for dynamic test data generation|Automated test data generation plays an important part in reducing the cost and increasing the reliability of software testing. However, a challenging problem in path-oriented test data generation is the existence of infeasible program paths, where considerable effort may be wasted in trying to generate input data to traverse the paths. In this paper, we propose a heuristics-based approach to infeasible path detection for dynamic test data generation. Our approach is based on the observation that many infeasible program paths exhibit some common properties. Through realizing these properties in execution traces collected during the test data generation process, infeasible paths can be detected early with high accuracy. Our experiments show that the proposed approach efficiently detects most of the infeasible paths with an average precision of 96.02% and a recall of 100% of all the cases. 
50|7-8||Combining probabilistic models for explanatory productivity estimation|In this paper Association Rules (AR) and Classification and Regression Trees (CART) are combined in order to deliver an effective conceptual estimation framework. AR descriptive nature is exploited by identifying logical associations between project attributes and the required effort for the development of the project. CART method on the other hand has the benefit of acquiring general knowledge from specific examples of projects and is able to provide estimates for all possible projects. The particular methods have the ability of learning and modelling associations in data and hence they can be used to describe complex relationships in software cost data sets that are not immediately apparent. Potential benefits of combining these probabilistic methods involve the ability of the final model to reveal the way in which particular attributes can increase or decrease productivity and the fact that such assumptions vary among different ranges of productivity values. Experimental results on two data sets indicate efficient overall performance of the suggested integrated method. 
50|7-8||A new calibration for Function Point complexity weights|Function Point (FP) is a useful software metric that was first proposed 25 years ago, since then, it has steadily evolved into a functional size metric consolidated in the well-accepted Standardized International Function Point Users Group (IFPUG) Counting Practices Manual – version 4.2. While software development industry has grown rapidly, the weight values assigned to count standard FP still remain same, which raise critical questions about the validity of the weight values. In this paper, we discuss the concepts of calibrating Function Point, whose aims are to estimate a more accurate software size that fits for specific software application, to reflect software industry trend, and to improve the cost estimation of software projects. A FP calibration model called Neuro-Fuzzy Function Point Calibration Model (NFFPCM) that integrates the learning ability from neural network and the ability to capture human knowledge from fuzzy logic is proposed. The empirical validation using International Software Benchmarking Standards Group (ISBSG) data repository release 8 shows a 22% accuracy improvement of mean magnitude relative error (MMRE) in software effort estimation after calibration. 
50|7-8||A framework for ensuring consistency of Web Services Transactions|For efficiently managing Web Services (WS) transactions which are executed across multiple loosely-coupled autonomous organizations, isolation is commonly relaxed. A Web service operation of a transaction releases locks on its resources once its jobs are completed without waiting for the completions of other operations. However, those early unlocked resources can be seen by other transactions, which can spoil data integrity and cause incorrect outcomes. Existing WS transaction standards do not consider this problem. In this paper, we propose a mechanism to ensure the consistent executions of isolation-relaxing WS transactions. The mechanism effectively detects inconsistent states of transactions with a notion of an end-state dependency and recovers them to consistent states. We also propose a new Web services Transaction Dependency management Protocol (WTDP). WTDP helps organizations manage the WS transactions easily without data inconsistency. WTDP is designed to be compliant with a representative WS transaction standard, the Web Services Transactions specifications, for easy integration into existing WS transaction systems. We prototyped a WTDP-based WS transaction management system to validate our protocol. 
50|7-8||MOBMAS: A methodology for ontology-based multi-agent systems development|Ontologies offer significant benefits to multi-agent systems: interoperability, reusability, support for multi-agent system (MAS) development activities (such as system analysis and agent knowledge modeling) and support for MAS operation (such as agent communication and reasoning). This paper presents an ontology-based methodology, MOBMAS, for the analysis and design of multi-agent systems. MOBMAS is the first methodology that explicitly identifies and implements the various ways in which ontologies can be used in the MAS development process and integrated into the MAS model definitions. In this paper, we present comprehensive documentation and validation of MOBMAS. 
50|7-8||Deriving an approximation algorithm for automatic computation of ripple effect measures|The ripple effect measures impact, or how likely it is that a change to a particular module may cause problems in the rest of a program. It can also be used as an indicator of the complexity of a particular module or program. Central to this paper is a reformulation in terms of matrix arithmetic of the original ripple effect algorithm produced by Yau and Collofello in 1978. The main aim of the reformulation is to clarify the component parts of the algorithm making the calculation more explicit. The reformulated algorithm has been used to implement REST (Ripple Effect and Stability Tool) which produces ripple effect measures for C programs. This paper describes the reformulation of Yau and Collofello’s ripple effect algorithm focusing on the computation of matrix Zm which holds intramodule change propagation information. The reformulation of the ripple effect algorithm is validated using fifteen programs which have been grouped by type. Due to the approximation spurious 1s are contained within matrix Zm. It is discussed whether this has an impact on the accuracy of the reformulated algorithm. The conclusion of this research is that the approximated algorithm is valid and as such can replace Yau and Collofello’s original algorithm. 
50|7-8||Consistency in multi-viewpoint design of enterprise information systems|Different stakeholders in the design of an enterprise information system have their own view on that design. To help produce a coherent design this paper presents a framework that aids in specifying relations and consistency rules between such views. The contribution of our framework is that it provides a collection of basic concepts. These basic concepts aid in relating viewpoints by providing: (i) a common terminology that helps stakeholders to understand each others concepts; and (ii) re-usable consistency rules. We show that our framework can be applied, by performing a case study in which we specify the relations and consistency rules between three RM-ODP viewpoints. 
50|7-8||Aligning the economic modeling of software reuse with reuse practices|In contrast to current practices where software reuse is applied recursively and reusable assets are tailored trough parameterization or specialization, existing reuse economic models assume that (i) the cost of reusing a software asset depends on its size and (ii) reusable assets are developed from scratch. The contribution of this paper is that it provides modeling elements and an economic model that is better aligned with current practices. The functioning of the model is illustrated in an example. The example also shows how the model can support practitioners in deciding whether it is economically feasible to apply software reuse recursively. 
50|7-8||Initiating software process improvement in very small enterprises: Experience with a light assessment tool|The paper concerns software process improvement in Very Small Enterprises (VSEs). It presents briefly a gradual methodology to initiate software process improvement in VSE through three steps approach and develops the first and most original step. This first step is based on a light evaluation achieved by means of a dedicated Micro-Evaluation approach. It has been experimented during 7 years in 86 organizations from three countries. The experience with that utilization tends to show that such a light approach is practicable and promising, at least for the targeted enterprises. 
50|7-8||The size and effort estimates in iterative development|The quick delivery of a functionally truncated product is one of the most common results in iterative development, and has become the predominant development approach. One of its drawbacks is the appearance of incomplete artifacts between iterations. Consequently, well-known size-estimation methods can not be used in iterative development. This paper addresses the problem of size estimation in iterative development. We present a novel approach that enables early size estimation using Unified Modeling Language (UML) artifacts. The approach incorporates self-improvement steps that increase the estimation accuracy in subsequent iterations. The demonstration of its applicability and research results are also presented. The results anticipate the possibility of a significant improvement in size and effort estimates by applying the approach presented here. 
50|7-8||Predicting weekly defect inflow in large software projects based on project planning and test status|Defects discovered during the testing phase in software projects need to be removed before the software is shipped to the customers. The removal of defects can constitute a significant amount of effort in a project and project managers are faced with a decision whether to continue development or shift some resources to cope with defect removal. The goal of this research is to improve the practice of project management by providing a method for predicting the number of defects reported into the defect database in the project. In this paper we present a method for predicting the number of defects reported into the defect database in a large software project on a weekly basis. The method is based on using project progress data, in particular the information about the test progress, to predict defect inflow in the next three coming weeks. The results show that the prediction accuracy of our models is up to 72% (mean magnitude of relative error for predictions of 1 week in advance is 28%) when used in ongoing large software projects. The method is intended to support project managers in more accurate adjusting resources in the project, since they are notified in advance about the potentially large effort needed to correct defects. 
50|7-8||Using formal metamodels to check consistency of functional views in information systems specification|UML notations require adaptation for applications such as Information Systems (IS). Thus we have defined IS-UML. The purpose of this article is twofold. First, we propose an extension to this language to deal with functional aspects of IS. We use two views to specify IS transactions: the first one is defined as a combination of behavioural UML diagrams (collaboration and state diagrams), and the second one is based on the definition of specific classes of an extended class diagram. The final objective of the article is to consider consistency issues between the various diagrams of an IS-UML specification. In common with other UML languages, we use a metamodel to define IS-UML. We use class diagrams to summarize the metamodel structure and a formal language, B, for the full metamodel. This allows us to formally express consistency checks and mapping rules between specific metamodel concepts. 
50|7-8||Integrating B-SCP and MAP to manage the evolution of strategic IT requirements|This paper presents the first steps in a research project that integrates two requirements engineering methodologies, B-SCP and MAP, in order to manage the evolution of strategic IT. Our integration approach presents a mechanism to validate and verify MAP requirements against B-SCP requirements and vice versa. MAP has an inbuilt Gap Analysis process which saves the overhead of inventing a new approach to deal with requirements evolution. In addition, MAP extends B-SCP’s capability by the addition of non-deterministic process modelling. Our solution is evaluated on an exemplar case study which looks at a system built for Seven Eleven Japan. 
50|9-10|http://www.sciencedirect.com/science/journal/09505849/50/9-10|Empirical studies of agile software development: A systematic review|Agile software development represents a major departure from traditional, plan-based approaches to software engineering. A systematic review of empirical studies of agile software development up to and including 2005 was conducted. The search strategy identified 1996 studies, of which 36 were identified as empirical studies. The studies were grouped into four themes: introduction and adoption, human and social factors, perceptions on agile methods, and comparative studies. The review investigates what is currently known about the benefits and limitations of, and the strength of evidence for, agile methods. Implications for research and practice are presented. The main implication for research is a need for more and better empirical studies of agile software development within a common research agenda. For the industrial readership, the review provides a map of findings, according to topic, that can be compared for relevance to their own settings and situations. 
50|9-10||Motivation in Software Engineering: A systematic literature review|ObjectiveIn this paper, we present a systematic literature review of motivation in Software Engineering. The objective of this review is to plot the landscape of current reported knowledge in terms of what motivates developers, what de-motivates them and how existing models address motivation.MethodsWe perform a systematic literature review of peer reviewed published studies that focus on motivation in Software Engineering. Systematic reviews are well established in medical research and are used to systematically analyse the literature addressing specific research questions.ResultsWe found 92 papers related to motivation in Software Engineering. Fifty-six percent of the studies reported that Software Engineers are distinguishable from other occupational groups. Our findings suggest that Software Engineers are likely to be motivated according to three related factors: their ‘characteristics’ (for example, their need for variety); internal ‘controls’ (for example, their personality) and external ‘moderators’ (for example, their career stage). The literature indicates that de-motivated engineers may leave the organisation or take more sick-leave, while motivated engineers will increase their productivity and remain longer in the organisation. Aspects of the job that motivate Software Engineers include problem solving, working to benefit others and technical challenge. Our key finding is that the published models of motivation in Software Engineering are disparate and do not reflect the complex needs of Software Engineers in their career stages, cultural and environmental settings.ConclusionsThe literature on motivation in Software Engineering presents a conflicting and partial picture of the area. It is clear that motivation is context dependent and varies from one engineer to another. The most commonly cited motivator is the job itself, yet we found very little work on what it is about that job that Software Engineers find motivating. Furthermore, surveys are often aimed at how Software Engineers feel about ‘the organisation’, rather than ‘the profession’. Although models of motivation in Software Engineering are reported in the literature, they do not account for the changing roles and environment in which Software Engineers operate. Overall, our findings indicate that there is no clear understanding of the Software Engineers’ job, what motivates Software Engineers, how they are motivated, or the outcome and benefits of motivating Software Engineers. 
50|9-10||A comparative evaluation on the accuracies of software effort estimates from clustered data|Precision in estimating the required software development effort plays a critical factor in the success of software project management. Most existing software effort estimation models only compare the accuracies of software effort estimates from the historical data without clustering. A potential factor that can affect the accuracies of the established effort estimation models is the homogeneity of the data. However, such investigation on the effects of the accuracies of the derived effort estimates is seldom explored in software effort estimation literature. Therefore, this paper aims to explore the effects of accuracies of the software effort estimation models established from the clustered data by using the International Software Benchmarking Standards Group (ISBSG) repository. The ordinary least square (OLS) regression method is adopted to establish a respective effort estimation model in each cluster of datasets. The empirical experiment results show that the estimation accuracies do not reveal significant differences within the respective dataset clustered by each software effort driver. It also demonstrates that software effort estimation models from the clustered data present almost similar accuracy results compared to models from the entire data without clustering. 
50|9-10||The impact of software process standardization on software flexibility and project management performance: Control theory perspective|It has been assumed for years that process standardization in the development of software will improve the efficiency of the development project by the virtues of applying a learned procedure and tight controls. Past research, however, is inconclusive in the elements that must be in place to achieve the benefits. In this study, we employ the software quality principle of flexibility as a mediator variable to determine if certain design aspects play a key role in achieving the benefits to the project of process standardization. A survey of computer professionals indicates that software flexibility is a positive influence. System designers should apply standard processes but with an eye toward quality design principles. 
50|9-10||Applying UML and software simulation for process definition, verification, and validation|Process definition, verification, and validation are recognized as critical elements in software process improvement, whereas CMMI is a process improvement approach that provides organizations with the essential elements of effective processes. Organizations must define their own processes to meet the requirements of CMMI. A friendly, unambiguous process modeling language and tool are thus very important for organizations to define, verify, and validate the processes. Nevertheless, hardly has any research yet been done on how to embed CMMI process area goals into process definition stage to satisfy organization process improvement. In this research, we propose a UML-based approach to define, verify, and validate an organization’s process. Our approach can also be applied to a process learning environment for students and project members. 
50|9-10||An approach to fuzzy granule-based hierarchical polynomial networks for empirical data modeling in software engineering|Experimental software data capturing the essence of software projects (expressed e.g., in terms of their complexity and development time) have been a subject of intensive modeling. In this study, we introduce a new category of fuzzy granule-based hierarchical polynomial networks (FG-HPN) and discuss their comprehensive design methodology. The FG-HPN architecture benefits from the existence of highly synergistic linkages between fuzzy granules (referred here as granular information design phase of FG-HPN) and hierarchical polynomial networks (referred as network design phase of FG-HPN). We develop a rule-based fuzzy granules consisting of a number of “if-then” statements whose antecedents are formed in the input space and linked with the consequents (conclusion parts) formed in the output space. Hierarchical polynomial networks provide approximation of experimental data. In this framework, fuzzy granules contribute to the realization of the granular information design phase of the overall networks structure of the FG-HPN. The networks design phase is designed with the aid of genetically endowed hierarchical polynomial networks. The experiments reported in this study deal with well-known software data such as the NASA dataset concerning software cost estimation and the one describing software modules of the Medical Imaging System (MIS). In comparison with the previously discussed approaches, the proposed FG-HPN is more accurate and yield significant generalization abilities. 
50|9-10||Securing services in nomadic computing environments|This work addresses the existing research gap regarding the security of service oriented architectures and their integration in the context of nomadic computing. The state of the art of Service Oriented Architectures (SOAs) is thoroughly investigated to understand what secure service provision means for different SOAs and whether an established notion of secure SOA existed. Based on the analysis of existing SOAs, we define a set of requirements for securing services among different nomadic computing domains. Such requirements concern the security of service registration and that of the discovery and delivery phases. The surveyed SOAs are then evaluated in the light of the defined requirements, revealing interesting observations about how current SOAs address security issues. The second part of this work addresses the research issue of achieving secure service provision in a nomadic computing environment characterized by a number of heterogeneous service oriented architectures. A solution is presented in the form of an architectural model, named Secure Nomadic Computing Architecture. The model relies on a novel three-phase discovery-delivery protocol which allows the enforcement of a number of security requirements, identified as a result of the first part of the work. Finally, we present an exemplary implementation of the proposed architectural model developed within the context of a distributed management information system for the discovery of digital educational content. 
50|9-10||MARS: A metamodel recovery system using grammar inference|Domain-specific modeling (DSM) assists subject matter experts in describing the essential characteristics of a problem in their domain. When a metamodel is lost, repositories of domain models can become orphaned from their defining metamodel. Within the purview of model-driven engineering, the ability to recover the design knowledge in a repository of legacy models is needed.In this paper we describe MARS, a semi-automatic grammar-centric system that leverages grammar inference techniques to solve the metamodel recovery problem. The paper also contains an applicative case study, as well as experimental results from the recovery of several metamodels in diverse domains. 
50|9-10||Integrating aspects in software architectures: PRISMA applied to robotic tele-operated systems|Aspect-Oriented Software Development (AOSD) has emerged as a new approach to develop software systems by improving their structure, reuse, maintenance and evolution properties. It is being applied to all stages of the software life cycle. In this paper, we present the PRISMA approach, which introduces AOSD in software architectures. PRISMA is characterized by integrating aspects as first-order citizens of software architectures. This paper shows how the PRISMA methodology is applied to develop a case study of the tele-operation system domain. We illustrate how the PRISMA approach can improve the development and maintenance processes of these kinds of industrial systems. 
50|9-10||Challenges and strategies in the use of Open Source Software by Independent Software Vendors|Open Source Software (OSS) has already been adopted by a large number of organizations. An important – but sometimes neglected – group of OSS users are Independent Software Vendors (ISVs). ISVs often develop their applications on top of OSS platform software. Frequently, this requires making several extensions and modifications to these OSS components. We identify a number of challenges that ISVs face in handling these extensions and modifications. Next, we describe several strategies ISVs can follow in maintaining these modifications. Finally, we suggest an opportunity for a closer collaboration between OSS projects and ISVs which could be mutually beneficial. 
50|9-10||A catalog of architectural primitives for modeling architectural patterns|Architectural patterns are a fundamental aspect of the architecting process and subsequently the architectural documentation. Unfortunately, there is only poor support for modeling architectural patterns for two reasons. First, patterns describe recurring design solutions and hence do not directly match the elements in modeling languages. Second, they support an inherent variability in the solution space that is hard to model using a single modeling solution. This paper proposes to address this problem by finding and representing architectural primitives: fundamental, formalized modeling elements in representing patterns. In particular, we examined architectural patterns from the components and connectors architectural view, and we discovered recurring primitive abstractions among the patterns, that also demonstrate a degree of variability for each pattern. We used UML 2 as the language for representing these primitive abstractions as extensions of the standard UML elements. The contribution of this approach is that we provide a generic and extensible concept for modeling architectural patterns by means of architectural primitives. Also, we can demonstrate a first set of primitives that participate in several well-known architectural patterns. 
50|9-10||Do secure information system design methods provide adequate modeling support?|Information system development (ISD) methods lack security features. To address this problem, various secure information system (SIS) design methods have been proposed. An important feature of these methods is modeling support, which manifests itself through modeling notations. This paper explores the extent to which the alternative SIS design methods offer modeling support. The results suggest that extant SIS design methods provide only limited modeling support. No single SIS design method offers comprehensive modeling support. This result has implications for practice and research. Practitioners may need to combine different SIS design methods for the development of secure information systems (IS). In turn, scholars and SIS design method developers should ensure that future SIS design methods offer comprehensive modeling support. Finally, empirical studies should be conducted to explore the usability of the current conceptual models of secure systems design methods in practice. 
volume|issue|url|title|abstract
51|-|http://www.sciencedirect.com/science/journal/09505849/51|Quality of UML models|
51|-||A systematic review of UML model consistency management|Information System (IS) development has been beset by consistency problems since its infancy. These problems are greater still in UML software development, and are principally caused by the existence of multiple views (models) for the same system, and may involve potentially contradictory system specifications. Since a considerable amount of work takes place within the scope of model consistency management, this paper presents a systematic literature review (SLR) which was carried out to discover the various current model consistency conceptions, proposals, problems and solutions provided. To do this, a total of 907 papers related to UML model consistency published in literature and extracted from the most relevant scientific sources (IEEE Computer Society, ACM Digital Library, Google Scholar, ScienceDirect, and the SCOPUS Database) were considered, of which 42 papers were eventually analyzed. This systematic literature review resulted in the identification of the current state-of-the-art with regard to UML model consistency management research along with open issues, trends and future research within this scope. A formal approach for the handling of inconsistency problems which fulfils the identified limitations is also briefly presented. 
51|-||Definitions and approaches to model quality in model-based software development â A review of literature|More attention is paid to the quality of models along with the growing importance of modelling in software development. We performed a systematic review of studies discussing model quality published since 2000 to identify what model quality means and how it can be improved. From forty studies covered in the review, six model quality goals were identified; i.e., correctness, completeness, consistency, comprehensibility, confinement and changeability. We further present six practices proposed for developing high-quality models together with examples of empirical evidence. The contributions of the article are identifying and classifying definitions of model quality and identifying gaps for future research. 
51|-||Level of detail in UML models and its impact on model comprehension: A controlled experiment|Previous studies have shown that the style and rigor used in UML models vary widely across software projects 1, 2 and 3. However, notwithstanding the varying use of styles and rigor, little research has been conducted to investigate the drivers and effects of using different styles and rigor in modeling on software development. In this paper, we evaluate Level of Detail (LoD) in UML models as a form of style and rigor in UML modeling. Using a UML model of a library system, we experimentally investigate the impact of LoD on model comprehension. More specifically, we explore whether LoD in UML models affects the correctness and efficiency in comprehending UML models. Using two independent groups of graduate students majoring in computer science, we performed a controlled experiment. The results of the experiment confirm the significant effect of LoD in UML models on model comprehension. Nevertheless, replication of this study is necessary, especially in contexts that involve professional software engineers, to improve the generalizability of the results. 
51|-||Guidelines on the aesthetic quality of UML class diagrams|In the past, formatting guidelines have proved to be a successful method to improve the readability of source code. With the increasing success of visual specification languages such as UML for model-driven software engineering visual guidelines are needed to standardize the presentation and the exchange of modeling diagrams with respect to human communication, understandability and readability. In this article, we introduce a new and encompassing taxonomy of visual guidelines capturing the aesthetic quality of UML class diagrams. We propose these guidelines as a framework to improve the aesthetic quality and thus the understandability of UML class diagrams. To validate this claim, we describe in detail a controlled experiment carried out as a pilot study to gather preliminary insights on the effects of some of the guideline rules on the understandability of UML class diagrams. 
51|-||An investigation of use case quality in a large safety-critical software development project|Use case modelling is a much-used technique for eliciting and documenting functional requirements. The quality of the use cases may have an important impact on the development project and on the resulting software product. This paper presents an empirical study of the changes that were made to the use case models in a large software project during the analysis phase. The results show (a) which were the most difficult aspects of use case modelling in this project and (b) how the quality of the functional requirements, in particular correctness, completeness, and clarity, was improved through the use case modelling process. 
51|-||The practical application of a process for eliciting and designing security in web service systems|Best practices currently state that the security requirements and security architectures of distributed software-intensive systems should be based on security risk assessments, which have been designed from security patterns, are implemented in security standards and are tool-supported throughout their development life-cycle. Web service-based information systems uphold inter-enterprise relations through the Internet, and this technology has been revealed as the reference solution with which to implement Service-Oriented Architectures. In this paper, we present the application of the Process for Web Service Security (PWSSec), developed by the authors, to a real web service-based case study. The manner in which security in inter-organizational information systems can be analyzed, designed and implemented by applying PWSSec, which combines a risk analysis and management, along with a security architecture and a standard-based approach, is also shown. We additionally present a tool built to provide support to the PWSSec process. 
51|-||Ontology-based modelling of architectural styles|The conceptual modelling of software architectures is of central importance for the quality of a software system. A rich modelling language is required to integrate the different aspects of architecture modelling, such as architectural styles, structural and behavioural modelling, into a coherent framework. Architectural styles are often neglected in software architectures. We propose an ontological approach for architectural style modelling based on description logic as an abstract, meta-level modelling instrument. We introduce a framework for style definition and style combination. The application of the ontological framework in the form of an integration into existing architectural description notations is illustrated. 
51|1|http://www.sciencedirect.com/science/journal/09505849/51/1|Introduction to section most cited journal articles in Software Engineering|
51|1||An analysis of the most cited articles in software engineering journals â 2002|Citations and related work are crucial in any research to position the work and to build on the work of others. A high citation count is an indication of the influence of specific articles. The importance of citations means that it is interesting to analyze which articles are cited the most. Such an analysis has been conducted using the ISI Web of Science to identify the most cited software engineering journal articles published in 2002. The objective of the analysis is to identify and list the articles that have influenced others the most as measured by citation count. An understanding of which research is viewed by the research community as most valuable to build upon may provide valuable insights into what research to focus on now and in the future. Based on the analysis, a list of the 20 most cited articles is presented here. The intention of the analysis is twofold. First, to identify the most cited articles, and second, to invite the authors of the most cited articles in 2002 to contribute to a special section of Information and Software Technology. Six authors have accepted the invitation and their articles appear in this special section. 
51|1||Systematic literature reviews in software engineering â A systematic literature review|BackgroundIn 2004 the concept of evidence-based software engineering (EBSE) was introduced at the ICSE04 conference.AimsThis study assesses the impact of systematic literature reviews (SLRs) which are the recommended EBSE method for aggregating evidence.MethodWe used the standard systematic literature review method employing a manual search of 10 journals and 4 conference proceedings.ResultsOf 20 relevant studies, eight addressed research trends rather than technique evaluation. Seven SLRs addressed cost estimation. The quality of SLRs was fair with only three scoring less than 2 out of 4.ConclusionsCurrently, the topic areas covered by SLRs are limited. European researchers, particularly those at the Simula Laboratory appear to be the leading exponents of systematic literature reviews. The series of cost estimation SLRs demonstrate the potential value of EBSE for synthesising evidence and making it available to practitioners. 
51|1||Automating regression test selection based on UML designs|This paper presents a methodology and tool to support test selection from regression test suites based on change analysis in object-oriented designs. We assume that designs are represented using the Unified Modeling Language (UML) 2.0 and we propose a formal mapping between design changes and a classification of regression test cases into three categories: Reusable, Retestable, and Obsolete. We provide evidence of the feasibility of the methodology and its usefulness by using our prototype tool on an industrial case study and two student projects. 
51|1||Modes in component behavior specification via EBP and their application in product lines|The concept of software product lines (SPL) is a modern approach to software development simplifying construction of related variants of a product thus lowering development costs and shortening time-to-market. In SPL, software components play an important role. In this paper, we show how the original idea of component mode can be captured and further developed in behavior specification via the formalism of extended behavior protocols (EBP). Moreover, we demonstrate how the modes in behavior specification can be used for modeling behavior of an entire product line. The main benefits include (i) the existence of a single behavior specification capturing the behavior of all product variants, and (ii) automatic verification of absence of communication errors among the cooperating components taking the variability into account. These benefits are demonstrated on a part of a non-trivial case study. 
51|1||Performance analysis of allocation policies for interGrid resource provisioning|Several Grids have been established and used for varying science applications during the last years. Most of these Grids, however, work in isolation and with different utilisation levels. Previous work has introduced an architecture and a mechanism to enable resource sharing amongst Grids. It has demonstrated that there can be benefits for a Grid to offload requests or provide spare resources to another Grid. In this work, we address the problem of resource provisioning to Grid applications in multiple-Grid environments. The provisioning is carried out based on availability information obtained from queueing-based resource management systems deployed at the provider sites which are the participants of the Grids. We evaluate the performance of different allocation policies. In contrast to existing work on load sharing across Grids, the policies described here take into account the local load of resource providers, imprecise availability information and the compensation of providers for the resources offered to the Grid. In addition, we evaluate these policies along with a mechanism that allows resource sharing amongst Grids. Experimental results obtained through simulation show that the mechanism and policies are effective in redirecting requests thus improving the applications’ average weighted response time. 
51|1||More on graph theoretic software watermarks: Implementation, analysis, and attacks|This paper presents an implementation of the watermarking method proposed by Venkatesan et al. in their paper [R. Venkatesan, V. Vazirani, S. Sinha, A graph theoretic approach to software watermarking, in: Fourth International Information Hiding Workshop, Pittsburgh, PA, 2001]. An executable program is marked by the addition of code for which the topology of the control-flow graph encodes a watermark. We discuss issues that were identified during construction of an actual implementation that operates on Java bytecode. We present two algorithms for splitting a watermark number into a redundant set of pieces and an algorithm for turning a watermark number into a control-flow graph. We measure the size and time overhead of watermarking, and evaluate the algorithm against a variety of attacks. 
51|1||RESRES: The story behind the paper âResearch in software engineering: An analysis of the literatureâ|This article is a background report describing a comprehensive study of research in the three computing disciplines Computer Science, Software Engineering, and Information Systems. Findings relate to research topics, approaches, methods, reference disciplines, and levels of analysis. The article informally describes the process used and the research products produced. 
51|1||A systematic review of quasi-experiments in software engineering|Background:Experiments in which study units are assigned to experimental groups nonrandomly are called quasi-experiments. They allow investigations of cause–effect relations in settings in which randomization is inappropriate, impractical, or too costly.Problem outline:The procedure by which the nonrandom assignments are made might result in selection bias and other related internal validity problems. Selection bias is a systematic (not happening by chance) pre-experimental difference between the groups that could influence the results. By detecting the cause of the selection bias, and designing and analyzing the experiments accordingly, the effect of the bias may be reduced or eliminated.Research method:To investigate how quasi-experiments are performed in software engineering (SE), we conducted a systematic review of the experiments published in nine major SE journals and three conference proceedings in the decade 1993–2002.Results:Among the 113 experiments detected, 35% were quasi-experiments. In addition to field experiments, we found several applications for quasi-experiments in SE. However, there seems to be little awareness of the precise nature of quasi-experiments and the potential for selection bias in them. The term “quasi-experiment” was used in only 10% of the articles reporting quasi-experiments; only half of the quasi-experiments measured a pretest score to control for selection bias, and only 8% reported a threat of selection bias. On average, larger effect sizes were seen in randomized than in quasi-experiments, which might be due to selection bias in the quasi-experiments.Conclusion:We conclude that quasi-experimentation is useful in many settings in SE, but their design and analysis must be improved (in ways described in this paper), to ensure that inferences made from this kind of experiment are valid. 
51|1||Evaluating the validity of data instances against ontology evolution over the Semantic Web|It is natural for ontologies to evolve over time. These changes could be at structural and semantic levels. Due to changes to an ontology, its data instances may become invalid, and as a result, may become non-interpretable. In this paper, we address precisely this problem, validity of data instances due to ontological evolution. Towards this end, we make the following three novel contributions to the area of Semantic Web. First, we propose formal notions of structural validity and semantic validity of data instances, and then present approaches to ensure them. Second, we propose semantic view as part of an ontology, and demonstrate that it is sufficient to validate a data instance against the semantic view rather than the entire ontology. We discuss how the semantic view can be generated through an implication analysis, i.e., how semantic changes to one component imply semantic changes to other components in the ontology. Third, we propose a validity identification approach that employs locally maintaining a hash value of the semantic view at the data instance. 
51|1||Multiversion join index for multiversion data warehouse|The data warehouse (DW) technology is developed in order to support the integration of external data sources (EDSs) for the purpose of advanced data analysis by On-Line Analytical Processing (OLAP) applications. Since contents and structures of integrated EDSs may evolve in time, the content and schema of a DW must evolve too in order to correctly reflect the evolution of EDSs. In order to manage a DW evolution, we developed the multiversion data warehouse (MVDW) approach. In this approach, different states of a DW are represented by the sequence of persistent DW versions that correspond either to the real world state or to a simulation scenario. Typically, OLAP applications execute star queries that join multiple fact and dimension tables. An important optimization technique for this kind of queries is based on join indexes. Since in the MVDW fact and dimension data are physically distributed among multiple DW versions, standard join indexes need extensions. In this paper we present the concept of a multiversion join index (MVJI) applicable to indexing dimension and fact tables in the MVDW. The MVJI has a two-level structure, where an upper level is used for indexing attributes and a lower level is used for indexing DW versions. The paper also presents the theoretical upper bound (pessimistic) analysis of the MVJI performance characteristic with respect to I/O operations. The analysis is followed by experimental evaluation. It shows that the MVJI increases a system performance for queries addressing multiple DW versions with exact match and range predicates. 
51|1||Adaptive Agent Model: Software Adaptivity using an Agent-oriented Model-Driven Architecture|Model-Driven Architecture (MDA) promotes the development of software systems through successive building and generation of models, improving the reusability of models. Applying the same principles to the area of Agent-Oriented Software Engineering (AOSE) advances the ideas behind MDA even more significantly, due to the inherent adaptivity of software agents We describe an appropriate set of models originating from requirements specification and transformable to models understandable and executable by agents thus demonstrating an Agent-oriented Model-Driven Architecture (AMDA) approach. In AMDA, agents use hierarchical business knowledge models with business process rules at the top, business rules to control policy and logic in the middle and a base layer defining business concepts. Being externalised, knowledge is easily configurable by human beings and applied by software agents. A real case study is used to illustrate the process. The main advances over the object-oriented MDA are (i) the addition of component dynamics (ii) the use of agent-executable rule-based business models and (iii) a proposed higher level of abstraction with the direct representation of business requirements. 
51|1||Object-oriented transformations for extracting aspects|In the migration of object-oriented systems towards the aspect technology, after locating fragments of code presenting a crosscutting behavior and before extracting such code to aspects, transformations may be needed in the base program. Such transformations aim to associate crosscutting code to points of the base program that can be captured using the pointcut descriptor model of aspect-oriented languages. In this paper, we present a catalog of object-oriented transformations and demonstrate the importance of such transformations by reporting on a case study involving four systems that have been aspectized using AspectJ. 
51|1||Improving the effectiveness of root cause analysis in post mortem analysis: A controlled experiment|Retrospective analysis is a way to share knowledge following the completion of a project or major milestone. However, in the busy workday of a software project, there is rarely time for such reviews and there is a need for effective methods that will yield good results quickly without the need for external consultants or experts. Building on an existing method for retrospective analysis and theories of group involvement, we propose improvements to the root cause analysis phase of a lightweight retrospective analysis method known as post mortem analysis (PMA). In particular, to facilitate brainstorming during the root cause analysis phase of the PMA, we propose certain processual changes to facilitate more active individual participation and the use of less rigidly structured diagrams. We conducted a controlled experiment to compare this new variation of the method with the existing one, and conclude that in our setting of small software teams with no access to an experienced facilitator, the new variation is more effective when it comes to identifying possible root causes of problems and successes. The modified method also produced more specific starting points for improving the software development process. 
51|1||Dynamic project performance estimation by combining static estimation models with system dynamics|Changes in user requirements or project personnel occur frequently during project execution particularly in long-term and large-size projects. We need a tool which can estimate the effects of changing conditions to effectively manage the project.This paper proposes a simulation method for dynamic project performance in terms of effort, schedule, and defect density changes in a dynamic project environment by combining COCOMO II with system dynamics. We apply expert judgment technique to overcome the lack of empirical data on the effects of dynamic project environment. We develop a simulation tool (available on the authors’ website) which has model adjustment parameters to reflect experts’ estimation on project characteristics. The simulation experiment on a military application development project demonstrates that the developed model can show the behavioral characteristics of a project suffering unanticipated and uncontrolled requirements creep. This helps project managers understand interactions between project factors and proactively evaluate and control the effects of dynamic project environment. 
51|1||Accuracy and efficiency comparisons of single- and multi-cycled software classification models|Software classification models have been regarded as an essential support tool in performing measurement and analysis processes. Most of the established models are single-cycled in the model usage stage, and thus require the measurement data of all the model’s variables to be simultaneously collected and utilized for classifying an unseen case within only a single decision cycle. Conversely, the multi-cycled model allows the measurement data of all the model’s variables to be gradually collected and utilized for such a classification within more than one decision cycle, and thus intuitively seems to have better classification efficiency but poorer classification accuracy. Software project managers often have difficulties in choosing an appropriate classification model that is better suited to their specific environments and needs. However, this important topic is not adequately explored in software measurement and analysis literature. By using an industrial software measurement dataset of NASA KC2, this paper explores the quantitative performance comparisons of the classification accuracy and efficiency of the discriminant analysis (DA)- and logistic regression (LR)-based single-cycled models and the decision tree (DT)-based (C4.5 and ECHAID algorithms) multi-cycled models. The experimental results suggest that the re-appraisal cost of the Type I MR, the software failure cost of Type II MR and the data collection cost of software measurements should be considered simultaneously when choosing an appropriate classification model. 
51|1||Empirical investigation towards the effectiveness of Test First programming|The Test First (TF) programming, which is based on an iterative process of “setting up test cases, implementing the functionality, and having all test cases passed”, has been put forward for decades, however knowledge of the evidence of the Test First programming’s success is limited. This paper describes a controlled experiment that investigated the distinctions between the effectiveness of Test First and that of Test Last (TL) (the traditional approach). The experimental results showed that Test First teams spent a larger percentage of time on testing. The achievable minimum external quality of delivered software applications increased with the percentage of time spent on testing regardless of the testing strategy (TF or TL) applied, although there does not exist a linear correlation between them. With four years’ data, it is also found that a strong linear correlation between the amount of effort spent on testing and coding in Test First teams, while this phenomenon was not observed in Test Last teams. 
51|1||Variability assessment in software product families|Software variability management is a key factor in the success of software systems and software product families. An important aspect of software variability management is the evolution of variability in response to changing markets, business needs, and advances in technology. To be able to determine whether, when, and how variability should evolve, we have developed the COVAMOF software variability assessment method (COSVAM). The contribution of COSVAM is that it is a novel, and industry-strength assessment process that addresses the issues that are associated to the current variability assessment practice. In this paper, we present the successful validation of COSVAM in an industrial software product family. 
51|1||Models of motivation in software engineering|Motivation in software engineering is recognized as a key success factor for software projects, but although there are many papers written about motivation in software engineering, the field lacks a comprehensive overview of the area. In particular, several models of motivation have been proposed, but they either rely heavily on one particular model (the job characteristics model), or are quite disparate and difficult to combine. Using the results from our previous systematic literature review (SLR), we constructed a new model of motivation in software engineering. We then compared this new model with existing models and refined it based on this comparison. This paper summarises the SLR results, presents the important existing models found in the literature and explains the development of our new model of motivation in software engineering. 
51|10|http://www.sciencedirect.com/science/journal/09505849/51/10|Guest Editorial for the Special Issue on Source Code Analysis andManipulation, SCAM 2008|
51|10||Higher Order Mutation Testing|This paper introduces a new paradigm for Mutation Testing, which we call Higher Order Mutation Testing (HOM Testing). Traditional Mutation Testing considers only first order mutants, created by the injection of a single fault. Often these first order mutants denote trivial faults that are easily killed. Higher order mutants are created by the insertion of two or more faults. The paper introduces the concept of a subsuming HOM; one that is harder to kill than the first order mutants from which it is constructed. By definition, subsuming HOMs denote subtle fault combinations. The paper reports the results of an empirical study of HOM Testing using 10 programs, including several non-trivial real-world subjects for which test suites are available. 
51|10||Change impact graphs: Determining the impact of prior codechanges|The source code of a software system is in constant change. The impact of these changes spreads out across the software system and may lead to the sudden manifestation of failures in unchanged parts. To help developers fix such failures, we propose a method that, in a pre-processing stage, analyzes prior code changes to determine what functions have been modified. Next, given a particular period of time in the past, the functions changed during that period are propagated throughout the rest of the system using the dependence graph of the system. This information is visualized using Change Impact Graphs (CIGs). Through a case study based on the Apache Web Server, we demonstrate the benefit of using CIGs to investigate several real defects. 
51|10||Decompilation of Java bytecode to Prolog by partial evaluation|Reasoning about Java bytecode (JBC) is complicated due to its unstructured control-flow, the use of three-address code combined with the use of an operand stack, etc. Therefore, many static analyzers and model checkers for JBC first convert the code into a higher-level representation. In contrast to traditional decompilation, such representation is often not Java source, but rather some intermediate language which is a good input for the subsequent phases of the tool. Interpretive decompilation consists in partially evaluating an interpreter for the compiled language (in this case JBC) written in a high-level language with respect to the code to be decompiled. There have been proofs-of-concept that interpretive decompilation is feasible, but there remain important open issues when it comes to decompile a real language such as JBC. This paper presents, to the best of our knowledge, the first modular scheme to enable interpretive decompilation of a realistic programming language to a high-level representation, namely of JBC to Prolog. We introduce two notions of optimality which together require that decompilation does not generate code more than once for each program point. We demonstrate the impact of our modular approach and optimality issues on a series of realistic benchmarks. Decompilation times and decompiled program sizes are linear with the size of the input bytecode program. This demonstrates empirically the scalability of modular decompilation of JBC by partial evaluation. 
51|10||Fast and precise points-to analysis|Many software engineering applications require points-to analysis. These client applications range from optimizing compilers to integrated program development environments (IDEs) and from testing environments to reverse-engineering tools. Moreover, software engineering applications used in an edit-compile cycle need points-to analysis to be fast and precise.In this article, we present a new context- and flow-sensitive approach to points-to analysis where calling contexts are distinguished by the points-to sets analyzed for their call target expressions. Compared to other well-known context-sensitive techniques it is faster in practice, on average, twice as fast as the call string approach and by an order of magnitude faster than the object-sensitive technique. In fact, it shows to be only marginally slower than a context-insensitive baseline analysis. At the same time, it provides higher precision than the call string technique and is similar in precision to the object-sensitive technique. We confirm these statements with experiments using a number of abstract precision metrics and a concrete client application: escape analysis. 
51|10||May/must analysis and the DFAGen data-flow analysis generator|Data-flow analysis is a common technique for gathering program information for use in program transformations such as register allocation, dead-code elimination, common subexpression elimination, and scheduling. Current tools for generating data-flow analysis implementations enable analysis details to be specified orthogonally to the iterative analysis algorithm but still require implementation details regarding the may and must use and definition sets that occur due to the effects of pointers, side effects, arrays, and user-defined structures. This paper presents the Data-Flow Analysis Generator tool (DFAGen), which enables analysis writers to generate analyses for separable and nonseparable data-flow analyses that are pointer, aggregate, and side-effect cognizant from a specification that assumes only scalars. By hiding the compiler-specific details behind predefined set definitions, the analysis specifications for the DFAGen tool are typically less than ten lines long and similar to those in standard compiler textbooks. The main contribution of this work is the automatic determination of when to use the may or must variant of a predefined set usage in the analysis specification. 
51|10||Recovering structured data types from a legacy data model with overlays|Legacy systems are often written in programming languages that support arbitrary variable overlays. When migrating to modern languages, the data model must adhere to strict structuring rules, such as those associated with an object oriented data model, supporting classes, class attributes and inter-class relationships.In this paper, we deal with the problem of automatically transforming a data model which lacks structure and relies on the explicit layout of variables in memory as defined by programmers. We introduce an abstract syntax and a set of abstract rewrite rules to describe the proposed approach in a language neutral formalism. Then, we instantiate the approach for the proprietary programming language that was used to develop a large legacy system we are migrating to Java. 
51|10||The life and death of statically detected vulnerabilities: An empirical study|Vulnerable statements constitute a major problem for developers and maintainers of networking systems. Their presence can ease the success of security attacks, aimed at gaining unauthorized access to data and functionality, or at causing system crashes and data loss. Examples of attacks caused by source code vulnerabilities are buffer overflows, command injections, and cross-site scripting.This paper reports on an empirical study, conducted across three networking systems, aimed at observing the evolution and decay of vulnerabilities detected by three freely available static analysis tools. In particular, the study compares the decay of different kinds of vulnerabilities, characterizes the decay likelihood through probability density functions, and reports a quantitative and qualitative analysis of the reasons for vulnerability removals. The study is performed by using a framework that traces the evolution of source code fragments across subsequent commits. 
51|11|http://www.sciencedirect.com/science/journal/09505849/51/11|Advancing test automation technology to meet the challenges of model-based software testing â Guest editorsâ introduction to the special section of the Third IEEE International Workshop on Automation of Software Test (AST 2008)|
51|11||Model-based testing approaches selection for software projects|Selecting software technologies for software projects represents a challenge to software engineers. It is known that software projects differ from each other by presenting different characteristics that can complicate the selection of such technologies. This is not different when considering model-based testing. There are many approaches with different characteristics described in the technical literature that can be used in software projects. However, there is no indication as to how they can fit a software project. Therefore, a strategy to select model-based testing approaches for software projects called Porantim is fully described in this paper. Porantim is based on a body of knowledge describing model-based testing approaches and their characterization attributes (identified by secondary and primary experimental studies), and a process to guide by adequacy and impact criteria regarding the use of this sort of software technology that can be used by software engineers to select model-based testing approaches for software projects. 
51|11||Validation of SDL specifications using EFSM-based test generation|Existing methods for testing an SDL specification mainly allow for either black box simulation or conformance testing to verify that the behavior of an implementation matches its corresponding model. However, this relies on the potentially hazardous assumption that the model is completely correct. We propose a test generation method that can accomplish conformance verification as well as coverage criteria-driven white box testing of the specification itself. We first reformat a set of EFSMs equivalent to the processes in an SDL specification and identify “hot spots” – nodes or edges in the EFSM which should be prioritized during testing to effectively increase coverage. Then, we generate test sequences intended to cover selected hot spots; we address the possible infeasibility of such a test sequence by allowing for its rejection decided by a constraint solver and re-generation of an alternate test sequence to the hot spot. In this paper, we present our test generation method and tool, and provide case studies on five SDL processes demonstrating the effectiveness of our coverage-based test sequence selection. 
51|11||A methodology for evaluating test coverage criteria of high levelPetri nets|High level Petri nets have been extensively used for modeling concurrent systems; however, their strong expressive power reduces their ability to be easily analyzed. Currently there are few effective formal analysis techniques to support the validation of high level Petri nets. The executable nature of high level Petri nets means that during validation they can be analyzed using test criteria defined on the net model. Recently, theoretical test adequacy coverage criteria for concurrent systems using high level Petri nets have been proposed. However, determining the applicability of these test adequacy criteria has not yet been undertaken. In this paper, we present an approach for evaluating the proposed test adequacy criteria for high level Petri nets through experimentation. In our experiments we use the simulation functionality of the model checker SPIN to analyze various test coverage criteria on high level Petri nets. 
51|11||Test Case Evaluation and Input Domain Reduction strategies for the Evolutionary Testing of Object-Oriented software|In Evolutionary Testing, meta-heuristic search techniques are used for generating test data. The focus of our research is on employing evolutionary algorithms for the structural unit-testing of Object-Oriented programs. Relevant contributions include the introduction of novel methodologies for automation, search guidance and Input Domain Reduction; the strategies proposed were empirically evaluated with encouraging results.Test cases are evolved using the Strongly-Typed Genetic Programming technique. Test data quality evaluation includes instrumenting the test object, executing it with the generated test cases, and tracing the structures traversed in order to derive coverage metrics. The methodology for efficiently guiding the search process towards achieving full structural coverage involves favouring test cases that exercise problematic structures. Purity Analysis is employed as a systematic strategy for reducing the search space. 
51|11||Engineering quality software â Guest editorâs introduction to the special section of the Eighth International Conference on Quality Software (QSIC 2008)|
51|11||Using machine learning to refine Category-Partition test specifications and test suites|In the context of open source development or software evolution, developers often face test suites which have been developed with no apparent rationale and which may need to be augmented or refined to ensure sufficient dependability, or even reduced to meet tight deadlines. We refer to this process as the re-engineering of test suites. It is important to provide both methodological and tool support to help people understand the limitations of test suites and their possible redundancies, so as to be able to refine them in a cost effective manner. To address this problem in the case of black-box, Category-Partition testing, we propose a methodology and a tool based on machine learning that has shown promising results on a case study involving students as testers. 
51|11||Integrating top-down and scenario-based methods for constructing software specifications|How to achieve a complete and consistent software specification by construction is an important issue for software quality assurance but still remains an open problem. The difficulty lies in the fact that the assurance of the completeness needs user’s judgments and the specification keeps changing as requirements analysis progresses. To allow the user to easily make such judgments and to reduce chances for creating inconsistencies due to frequent specification modifications, in this paper we describe an intuitive, formal, and expressive specification method that integrates top-down decompositional and scenario-based compositional methods. The decompositional method is used at an informal level with the goal of achieving a complete coverage of the user’s functional requirements, while the compositional method is used to precisely define the functionality of each scenario and to construct complex scenarios by composition of simple scenarios in a formal, intuitive language called SOFL. Combination of the decompositional and compositional processes can facilitate the analyst in completing a specification in a hierarchical structure. We present an example to illustrate how the integrated method is used in practice and describe a software support tool for the method. 
51|11||Is non-parametric hypothesis testing model robust for statistical fault localization?|Fault localization is one of the most difficult activities in software debugging. Many existing statistical fault-localization techniques estimate the fault positions of programs by comparing the program feature spectra between passed runs and failed runs. Some existing approaches develop estimation formulas based on mean values of the underlying program feature spectra and their distributions alike. Our previous work advocates the use of a non-parametric approach in estimation formulas to pinpoint fault-relevant positions. It is worthy of further study to resolve the two schools of thought by examining the fundamental, underlying properties of distributions related to fault localization. In particular, we ask: Can the feature spectra of program elements be safely considered as normal distributions so that parametric techniques can be soundly and powerfully applied? In this paper, we empirically investigate this question from the program predicate perspective. We conduct an experimental study based on the Siemens suite of programs. We examine the degree of normality on the distributions of evaluation biases of the predicates, and obtain three major results from the study. First, almost all examined distributions of evaluation biases are either normal or far from normal, but not in between. Second, the most fault-relevant predicates are less likely to exhibit normal distributions in terms of evaluation biases than other predicates. Our results show that normality is not common as far as evaluation bias can represent. Furthermore, the effectiveness of our non-parametric predicate-based fault-localization technique weakly correlates with the distributions of evaluation biases, making the technique robust to this type of uncertainty in the underlying program spectra. 
51|11||Architecture compliance checking at run-time|In this paper, we report on our experiences with architecture compliance checking – the process of checking whether the planned or specified software architecture is obeyed by the running system – of an OSGi-based, dynamically evolving application in the office domain. To that end, we first show how to dynamically instrument a running system in the context of OSGi in order to collect run-time traces. Second, we explain how to bridge the abstraction gap between run-time traces and software architectures, through the construction of hierarchical Colored Petri nets (CP-nets). In addition, we demonstrate how to design reusable hierarchical CP-nets. In an industry example, we were able to extract views that helped us to identify a number of architecturally relevant issues (e.g., architectural style violations, behavior violations) that would not have been detected otherwise, and could have caused serious problems like system malfunctioning or unauthorized access to sensitive data. Finally, we package valuable experiences and lessons learned from this endeavor. 
51|11||Using coverage to automate and improve test purpose based testing|Test purposes have been presented as a solution to avoid the state space explosion when selecting test cases from formal models. Although such techniques work very well with regard to the speed of the test derivation, they leave the tester with one important task that influences the quality of the overall testing process: test purposes have to be formulated manually. In this paper, we present an approach that assists a test engineer with test purpose design in two ways: it allows automatic generation of coverage based test suites and can be used to automatically exercise those aspects of the system that are missed by hand-crafted test purposes. We consider coverage of Lotos specifications, and show how labeled transition systems derived from such specifications have to be extended in order to allow the application of logical coverage criteria to Lotos specifications. We then show how existing tools can be used to efficiently derive test cases and suggest how to use the coverage information to minimize test suites while generating them. 
51|11||Linux bugs: Life cycle, resolution and architectural analysis|Efforts to improve application reliability can be irrelevant if the reliability of the underlying operating system on which the application resides is not seriously considered. An important first step in improving the reliability of an operating system is to gain insights into why and how the bugs originate, contributions of the different modules to the bugs, their distribution across severities, the different ways in which the bugs may be resolved and the impact of bug severities on their resolution times. To acquire this insight, we conducted an extensive analysis of the publicly available bug data on the Linux kernel over a period of seven years. We also justify and explain the statistical bug occurrence trends observed from the data, using the architecture of the Linux kernel as an anchor. The statistical analysis of the Linux bug data suggests that the Linux kernel may draw significant benefits from the continual reliability improvement efforts of its developers. These efforts, however, are disproportionately targeted towards popular configurations and hardware platforms, due to which the reliability of these configurations may be better than those that are not commonly used. Thus, a key finding of our study is that it may be prudent to restrict to using common configurations and platforms when using open source systems such as Linux in applications with stringent reliability expectations. Finally, our study of the architectural properties of the bugs suggests that the dependence among the modules rather than the unreliabilities of the individual modules is the primary cause of the bugs and their impact on system reliability. 
51|2|http://www.sciencedirect.com/science/journal/09505849/51/2|A roadmap to electronic payment transaction guarantees and a Colored Petri Net model checking approach|Electronic payment systems play a vital role in modern business-to-consumer and business-to-business e-commerce. Atomicity, fault tolerance and security concerns form a problem domain of interdependent issues that are taken into account to assure the transaction guarantees of interest. We focus on the most notable payment transaction guarantees: money conservation, no double spending, goods atomicity, distributed payment atomicity, certified delivery or validated receipt and the high-level guarantees of fairness and protection of payment participants’ interests. Apart from a roadmap to the forenamed transaction guarantees, this work’s contribution is basically a full-fledged methodology for building and validating high-level protocol models and for proving payment transaction guarantees by model checking them from different participants perspectives (payer perspective, as well as payee perspective). Our approach lies on the use of Colored Petri Nets and the CPN Tools environment (i) for editing and analyzing protocol models, (ii) for proving the required transaction guarantees by CTL-based (Computation Tree Temporal Logic) model checking and (iii) for evaluating the need of candidate security requirements. 
51|2||VxBPEL: Supporting variability for Web services in BPEL|Web services provide a way to facilitate the business integration over the Internet. Flexibility is an important and desirable property of Web service-based systems due to dynamic business environments. The flexibility can be provided or addressed by incorporating variability into a system. In this study, we investigate how variability can be incorporated into service-based systems. We propose a language, VxBPEL, which is an adaptation of an existing language, BPEL, and able to capture variability in these systems. We develop a prototype to interpret this language. Finally, we illustrate our method by using it to handle variability of an example. 
51|2||Using acceptance tests as a support for clarifying requirements: A series of experiments|One of the main reasons for the failure of many software projects is the late discovery of a mismatch between the customers’ expectations and the pieces of functionality implemented in the delivered system. At the root of such a mismatch is often a set of poorly defined, incomplete, under-specified, and inconsistent requirements. Test driven development has recently been proposed as a way to clarify requirements during the initial elicitation phase, by means of acceptance tests that specify the desired behavior of the system.The goal of the work reported in this paper is to empirically characterize the contribution of acceptance tests to the clarification of the requirements coming from the customer. We focused on Fit tables, a way to express acceptance tests, which can be automatically translated into executable test cases. We ran two experiments with students from University of Trento and Politecnico of Torino, to assess the impact of Fit tables on the clarity of requirements. We considered whether Fit tables actually improve requirement understanding and whether this requires any additional comprehension effort. Experimental results show that Fit helps in the understanding of requirements without requiring a significant additional effort. 
51|2||Analysis of workflow dynamic changes based on Petri net|Dynamic adaptability has become one of the major research topics in the area of workflow management system. When adjusting a workflow process to some structural changes, there is a potential problem: the new workflow may contain errors, such as deadlock, inconsistency and even loss of instance. This paper primary addresses the issues related to workflow structural changes. It firstly defines a class of structural change called compatible change. This kind of change can be applied to the workflow process, without causing any structural errors or behavioral inconsistencies. Secondly, an algorithm is put forward to calculate the minimal region affected by the changes. Furthermore, it proves that the change regions can be used to check the compatibility of workflow changes. This approach is applicable and efficient in terms of time and space for large-scale and complex systems. Lastly, this paper discusses the problem to decide whether an active workflow instance can be smoothly evolved to the new workflow, and provides a sufficient condition for valid migration. In the end, an example is given to illustrate the effectiveness of the proposed concepts and method. 
51|2||The effect of task order on the maintainability of object-oriented software|This paper presents results from a quasi-experiment that investigates how the sequence in which maintenance tasks are performed affects the time required to perform them and the functional correctness of the changes made. Specifically, the study compares how time required and correctness are affected by (1) starting with the easiest change task and progressively performing the more difficult tasks (Easy-First), versus (2) starting with the most difficult change task and progressively performing the easier tasks (Hard-First). In both cases, the experimental tasks were performed on two alternative types of design of a Java system to assess whether the choice of the design strategy moderates the effects of task order on effort and correctness.The results show that the time spent on making the changes is not affected significantly by the task order of the maintenance tasks, regardless of the type of design. However, the correctness of the maintainability tasks is significantly higher when the task order of the change tasks is Easy-First compared to Hard-First, again regardless of design. A possible explanation for the results is that a steeper learning curve (Hard-First) causes the programmer to create software that is less maintainable overall. 
51|2||An ADL dealing with aspects at software architecture stage|Managing complex software systems is one of the most important problems to be solved by software engineering. The software engineer needs to apply new techniques that allow for their adequate manipulation. Software architecture is becoming an important part of software design, helping the designer to handle the structure and the complexity of large systems, and AOSD is a paradigm proposed to manage this complexity by considering crosscutting concerns throughout the software’s life-cycle. The suitability of the existence of an Aspect-Oriented (AO) architectural design appears when AO concepts are extended to the whole life-cycle. In order to adequately specify the AO design, aspect-oriented architecture description languages are needed. The formal basis of these will allow architects to reason about the properties of the software architecture. In this paper, a new architecture description language – AspectLEDA – is formally described in order to adequately manipulate AO concepts at the software architecture stage. The AspectLEDA translation process is also described. A toolkit assists the architect during the process. Finally, a prototype of the system can be obtained, and the correctness of the architecture obtained can be checked. 
51|2||A heuristics-based approach to reverse engineering of electronic services|Since the beginning of the electronic era, public administrations and enterprises have been developing services, through which citizens, businesses and customers can conduct their transactions with the offering entity. Each electronic service contains a substantial amount of knowledge in the form help texts, rules of use or legislation excerpts, examples, validation checks, etc. This knowledge has been extracted from domain experts when the services were developed, especially in the phases of analysis and design, and was subsequently translated into software. In the latter format though, knowledge cannot be readily used in organizational processes, such as knowledge sharing and development of new services. In this paper, we present an approach for reverse engineering electronic services in order to create knowledge items of high levels of abstraction, which can be used in knowledge sharing environments as well as in service development platforms. The proposed approach has been implemented and configured to generate artifacts for the SmartGov service development platform. Finally, an evaluation of the proposed approach is presented to assess its efficiency regarding various aspects of the reverse engineering process. 
51|2||Towards the development of privacy-aware systems|Privacy and data protection are pivotal issues in nowadays society. They concern the right to prevent the dissemination of sensitive or confidential information of individuals. Many studies have been proposed on this topic from various perspectives, namely sociological, economic, legal, and technological. We have recognized the legal perspective as being the basis of all other perspectives. Actually, data protection regulations set the legal principles and requirements that must be met by organizations when processing personal data. The objective of this work is to provide a reference base for the development of methodologies tailored to design privacy-aware systems to be compliant with data protection regulations. 
51|2||Evaluating the relationship between process improvement and schedule deviation in software maintenance|A basic proposition of process assessment models is that higher process maturity is associated with improved project performance and product quality. This study provides empirical evidence to support this proposition by testing the hypothesis that higher process maturity is negatively associated with schedule deviation in software maintenance. Next, the present study investigates whether two process context factors (organizational size and geographical region) modify the relationship between process maturity and schedule deviation by using a moderator testing method. Our results show that organizational size does not influence the relationship, while geographical region is deemed to be an independent variable. 
51|2||Efficient tracesâ collection mechanisms for passive testing of Web Services|Web Services are a novel approach for business-to-business interactions. Their management, especially fault and performance management, is becoming necessary for their success and emergence. Nowadays, this management is platform-dependent and does not allow third parties to be involved. In this paper, we consider management of Web Services by passive testing where the tester itself is a Web Service. We propose different architectures for observation of simple and composite Web Services. We also study a set of online traces collection mechanisms and discuss their performances in terms of required CPU/RAM resources and introduced network overhead. These performances are then maximized by selecting best locations of observers. Observation considers both functional and non-functional (QoS) properties of Web Services. The paper presents also our experiments using different observation architectures and traces collection mechanisms while observing a simple and a composite Web Service. 
51|2||Integrating in-process software defect prediction with association mining to discover defect pattern|Rather than detecting defects at an early stage to reduce their impact, defect prevention means that defects are prevented from occurring in advance. Causal analysis is a common approach to discover the causes of defects and take corrective actions. However, selecting defects to analyze among large amounts of reported defects is time consuming, and requires significant effort. To address this problem, this study proposes a defect prediction approach where the reported defects and performed actions are utilized to discover the patterns of actions which are likely to cause defects. The approach proposed in this study is adapted from the Action-Based Defect Prediction (ABDP), an approach uses the classification with decision tree technique to build a prediction model, and performs association rule mining on the records of actions and defects. An action is defined as a basic operation used to perform a software project, while a defect is defined as software flaws and can arise at any stage of the software process. The association rule mining finds the maximum rule set with specific minimum support and confidence and thus the discovered knowledge can be utilized to interpret the prediction models and software process behaviors. The discovered patterns then can be applied to predict the defects generated by the subsequent actions and take necessary corrective actions to avoid defects.The proposed defect prediction approach applies association rule mining to discover defect patterns, and multi-interval discretization to handle the continuous attributes of actions. The proposed approach is applied to a business project, giving excellent prediction results and revealing the efficiency of the proposed approach. The main benefit of using this approach is that the discovered defect patterns can be used to evaluate subsequent actions for in-process projects, and reduce variance of the reported data resulting from different projects. Additionally, the discovered patterns can be used in causal analysis to identify the causes of defects for software process improvement. 
51|2||Algorithms and tool support for dynamic information flow analysis|A new approach to dynamic information flow analysis (DIFA) is presented, and its applications to intrusion detection, software testing and program debugging are discussed. The approach is based on a new forward-computing algorithm that enables online analysis when fast response is not critical. A new forward-computing algorithm for dynamic slicing is also presented, which is more precise than previous forward-computing algorithms and is not restricted to programs with structured control flow. The DIFA and slicing algorithms both rely on a new, precise direct dynamic control dependence algorithm, which requires only constant time per program action. The correctness of this algorithm depends on special, graph-theoretic properties of control dependence, which are established here. A tool called DynFlow is described that implements the proposed approach in order to support analysis of Java byte code programs, and two case studies are presented to illustrate how DynFlow can be used to detect and debug insecure flows. Finally, since dynamic analysis alone is inherently unable to detect implicit information flows, an extension to our approach is described that enables it to detect most implicit information flows at runtime. 
51|2||Revising cohesion measures by considering the impact of write interactions between class members|Cohesion refers to the degree of the relatedness of the members in a class and several cohesion measures have been proposed to quantify the cohesiveness of classes in an object-oriented program. However, the existing cohesion measures do not differentiate write interactions from read interactions between class members, thus, do not properly reflect the cohesiveness of the class. This paper presents the revised versions of the existing five cohesion measures by considering the impact of write interactions between class members. In addition, we prove that the revised measures can be reduced into the original ones. To demonstrate the importance of write interactions, we have developed tools for automatic computation of the original and the revised cohesion measures and performed a case study where we found that write interactions are so commonly used in classes that they have much influence on cohesion measurement and the revised measures have stronger relations with change-proneness of classes than the original ones. 
51|2||Automatic generation of test specifications for coverage of system state transitions|Adequate system testing of present day application programs requires satisfactory coverage of system states and transitions. This can be achieved by using a system state model. However, the system state models are rarely constructed by system developers, as these are large and complex. The only state models that are constructed by the developers are those of individual objects. However test case generation for state-based system testing based on traversal of statecharts of individual objects appears to be infeasible, since system test cases would have to be specified in the form of scenario sequences rather than transitions on individual object statecharts. In this paper, we propose a novel approach to coverage of elementary transition paths of an automatically synthesized system state model. Our technique for coverage of elementary transition paths would also ensure coverage of all states and transitions of the system model. 
51|2||Evaluating legacy system migration technologies through empirical studies|We present two controlled experiments conducted with master students and practitioners and a case study conducted with practitioners to evaluate the use of MELIS (Migration Environment for Legacy Information Systems) for the migration of legacy COBOL programs to the web. MELIS has been developed as an Eclipse plug-in within a technology transfer project conducted with a small software company [16]. The partner company has developed and marketed in the last 30 years several COBOL systems that need to be migrated to the web, due to the increasing requests of the customers. The goal of the technology transfer project was to define a systematic migration strategy and the supporting tools to migrate these COBOL systems to the web and make the partner company an owner of the developed technology. The goal of the controlled experiments and case study was to evaluate the effectiveness of introducing MELIS in the partner company and compare it with traditional software development environments. The results of the overall experimentation show that the use of MELIS increases the productivity and reduces the gap between novice and expert software engineers. 
51|2||CompAS: A new approach to commonality and variability analysis with applications in computer assisted orthopaedic surgery|In rapidly evolving domains such as Computer Assisted Orthopaedic Surgery (CAOS) emphasis is often put first on innovation and new functionality, rather than in developing the common infrastructure needed to support integration and reuse of these innovations. In fact, developing such an infrastructure is often considered to be a high-risk venture given the volatility of such a domain. We present CompAS, a method that exploits the very evolution of innovations in the domain to carry out the necessary quantitative and qualitative commonality and variability analysis, especially in the case of scarce system documentation. We show how our technique applies to the CAOS domain by using conference proceedings as a key source of information about the evolution of features in CAOS systems over a period of several years. We detect and classify evolution patterns to determine functional commonality and variability. We also identify non-functional requirements to help capture domain variability. We have validated our approach by evaluating the degree to which representative test systems can be covered by the common and variable features produced by our analysis. 
51|2||Building test cases and oracles to automate the testing of web database applications|Many organizations rely on web applications that use back-end databases to store important data. Testing such applications requires significant effort. Manual testing alone is often impractical, so testers also rely on automated testing techniques. However, current automated testing techniques may produce false positives (or false negatives) even in a perfectly working system because the outcome of a test case depends on the state of the database which changes over time as data is inserted and deleted. The Automatic Database Tester (AutoDBT) generates functional test cases that account for database updates. AutoDBT takes as input a model of the application and a set of testing criteria. The model consists of a state transition diagram that shows how users navigate pages, a data specification that captures how data flows, and an update specification that shows how the database is updated. AutoDBT generates guard queries to determine whether the database is in a state conducive to performing and evaluating tests. AutoDBT also generates partial oracles to help validate whether a back-end database is updated correctly during testing. This paper describes the design of AutoDBT, a prototype implementation, several experiments with the prototype, and four case studies. 
51|2||Consistency preserving co-evolution of formal specifications and agent-oriented conceptual models|Many modelling techniques tend to address “late-phase” requirements while many critical modelling decisions (such as determining the main goals of the system, how the stakeholders depend on each other, and what alternatives exist) are taken during early-phase requirements engineering. The i∗ modelling framework is a semiformal agent-oriented conceptual modelling language that is well-suited for answering these questions. This paper addresses key challenge faced in the practical deployment of agent-oriented conceptual modelling frameworks such as i∗. Our approach to addressing this problem is based on the observation that the value of conceptual modelling in the i∗ framework lies in its use as a notation complementary to existing requirements modelling and specification languages, i.e., the expressive power of i∗ complements rather than supplants that of existing notations. The use of i∗ in this fashion requires that we define methodologies that support the co-evolution of i∗ models with more traditional specifications. This research examines how this might be done with formal specification notations (specifically Z). 
51|2||Uncertainty explicit assessment of off-the-shelf software: A Bayesian approach|Assessment of software COTS components is an essential part of component-based software development. Poorly chosen components may lead to solutions of low quality and that are difficult to maintain. The assessment may be based on incomplete knowledge about the COTS component itself and other aspects (e.g. vendor’s credentials, etc.), which may affect the decision of selecting COTS component(s). We argue in favor of assessment methods in which uncertainty is explicitly represented (‘uncertainty explicit’ methods) using probability distributions. We provide details of a Bayesian model, which can be used to capture the uncertainties in the simultaneous assessment of two attributes, thus, also capturing the dependencies that might exist between them. We also provide empirical data from the use of this method for the assessment of off-the-shelf database servers which illustrate the advantages of ‘uncertainty explicit’ methods over conventional methods of COTS component assessment which assume that at the end of the assessment the values of the attributes become known with certainty. 
51|2||Automated traceability analysis for UML model refinements|During iterative, UML-based software development, various UML diagrams, modeling the same system at different levels of abstraction are developed. These models must remain consistent when changes are performed. In this context, we refine the notion of impact analysis and distinguish horizontal impact analysis–that focuses on changes and impacts at one level of abstraction–from vertical impact analysis–that focuses on changes at one level of abstraction and their impacts on another level. Vertical impact analysis requires that some traceability links be established between model elements at the two levels of abstraction. We propose a traceability analysis approach for UML 2.0 class diagrams which is based on a careful formalization of changes to those models, refinements which are composed of those changes, and traceability links corresponding to refinements. We show how actual refinements and corresponding traceability links are formalized using the OCL. Tool support and a case study are also described. 
51|2||An approach for concurrent evaluation of technical and social aspects of software development methodologies|The paper presents an approach for evaluation of software development methodologies (SDM) that considers the aspects of a SDM’s social adoption and technical efficiency. It builds on existing evaluation models used in the field of SDM. Case study approach was used to validate the model in four software development organisations. In all four cases the management confirmed that the model provided valuable new insights into adoption and efficiency of the companies’ SDM. 
51|2||Covering code behavior on input validation in functional testing|Input validation is the enforcement built in software systems to ensure that only valid input is accepted to raise external effects. It is essential and very important to a large class of systems and usually forms a major part of a data-intensive system. Most existing methods for input validation testing are specification-based. However, to test input validation more accurately, a code-based method is also required. In this paper, we propose an approach to extract path partition and input conditions from code for testing input validation. The path partition can be used to design white-box test cases for testing input validation. It can also be used to measure the coverage of input validation testing. The valid and invalid input conditions recovered can be used to check against the specifications and aid the test suite design in black-box testing. We have also evaluated the proposed method through experimental study. 
51|3|http://www.sciencedirect.com/science/journal/09505849/51/3|Evaluating and selecting software packages: A review|Evaluating and selecting software packages that meet an organization’s requirements is a difficult software engineering process. Selection of a wrong software package can turn out to be costly and adversely affect business processes. The aim of this paper is to provide a basis to improve the process of evaluation and selection of the software packages. This paper reports a systematic review of papers published in journals and conference proceedings. The review investigates methodologies for selecting software packages, software evaluation techniques, software evaluation criteria, and systems that support decision makers in evaluating software packages. The key findings of the review are: (1) analytic hierarchy process has been widely used for evaluation of the software packages, (2) there is lack of a common list of generic software evaluation criteria and its meaning, and (3) there is need to develop a framework comprising of software selection methodology, evaluation technique, evaluation criteria, and system to assist decision makers in software selection. 
51|3||Model-checking for adventure videogames|This paper describes a model-checking approach for adventure games focusing on ãe-Adventureã, a platform for the development of adaptive educational adventure videogames. In ãe-Adventureã, games are described using a domain-specific language oriented to game writers. By defining a translation from this language to suitable state-based models, it is possible to automatically extract a verification model for each ãe-Adventureã game. In addition, temporal properties to be verified are described using an extensible assertion language, which can be tailored to each specific application scenario. When the framework determines that some of these properties do not hold, it generates an animation of a counterexample. This approach facilitates the collaboration of multidisciplinary teams of experts during the verification of the integrity of the game scripts, exchanging hours of manual verification for semi-automatic verification processes that also facilitate the diagnosis of the conditions that may potentially break the games. 
51|3||An economic model to compare the profitability of pay-per-use and fixed-fee licensing|This paper develops an economic model to compare the profitability of two strategies for the pricing of packaged software: fixed-fee and pay-per-use licensing. It is assumed that the market consists of a monopoly software vendor who is selling packaged software to customers who are homogeneous in marginal value of software use but heterogeneous in level of use. In addition to obtaining the software package from the market, customers can develop the required software in-house. When in-house development costs are constant across customers, the results show that the software vendor prefers pay-per-use licensing over fixed-fee licensing if in-house development is relatively expensive, whereas fixed-fee licensing is optimal if the cost of in-house development drops below a certain threshold value. When the assumption of a constant in-house development cost is relaxed by letting it vary among customers, it still holds that pay-per-use licensing is optimal if its average is relatively large. For low and medium values of the average cost of in-house development, however, fixed-fee licensing may no longer be optimal as the relative attractiveness of the two licensing strategies now depends on how dispersed the in-house development costs of individual customers are. 
51|3||On automated prepared statement generation to remove SQL injection vulnerabilities|Since 2002, over 10% of total cyber vulnerabilities were SQL injection vulnerabilities (SQLIVs). This paper presents an algorithm of prepared statement replacement for removing SQLIVs by replacing SQL statements with prepared statements. Prepared statements have a static structure, which prevents SQL injection attacks from changing the logical structure of a prepared statement. We created a prepared statement replacement algorithm and a corresponding tool for automated fix generation. We conducted four case studies of open source projects to evaluate the capability of the algorithm and its automation. The empirical results show that prepared statement code correctly replaced 94% of the SQLIVs in these projects. 
51|3||Customizing ISO 9126 quality model for evaluation of B2B applications|A software quality model acts as a framework for the evaluation of attributes of an application that contribute to the software quality. In this paper, a quality model is presented for evaluation of B2B applications. First, the most well-known quality models are studied, and reasons for using ISO 9126 quality model as the basis are discussed. This model, then, is customized in accordance with special characteristics of B2B applications. The customization is done by extracting the quality factors from web applications and B2B e-commerce applications, weighting these factors from the viewpoints of both developers and end users, and adding them to the model. Finally, as a case study, ISACO portal is evaluated by the proposed model. 
51|3||Complexity metrics for Workflow nets|Process modeling languages such as EPCs, BPMN, flow charts, UML activity diagrams, Petri nets, etc., are used to model business processes and to configure process-aware information systems. It is known that users have problems understanding these diagrams. In fact, even process engineers and system analysts have difficulties in grasping the dynamics implied by a process model. Recent empirical studies show that people make numerous errors when modeling complex business processes, e.g., about 20% of the EPCs in the SAP reference model have design flaws resulting in potential deadlocks, livelocks, etc. It seems obvious that the complexity of the model contributes to design errors and a lack of understanding. It is not easy to measure complexity, however. This paper presents three complexity metrics that have been implemented in the process analysis tool ProM. The metrics are defined for a subclass of Petri nets named Workflow nets, but the results can easily be applied to other languages. To demonstrate the applicability of these metrics, we have applied our approach and tool to 262 relatively complex Protos models made in the context of various student projects. This allows us to validate and compare the different metrics. It turns out that our new metric focusing on the structuredness outperforms existing metrics. 
51|3||How do personality, team processes and task characteristics relate to job satisfaction and software quality?|This article analyses the relationships between personality, team processes, task characteristics, product quality and satisfaction in software development teams. The data analysed here were gathered from a sample of 35 teams of students (105 participants). These teams applied an adaptation of an agile methodology, eXtreme Programming (XP), to develop a software product. We found that the teams with the highest job satisfaction are precisely the ones whose members score highest for the personality factors agreeableness and conscientiousness. The satisfaction levels are also higher when the members can decide how to develop and organize their work. On the other hand, the level of satisfaction and cohesion drops the more conflict there is between the team members. Finally, the teams exhibit a significant positive correlation between the personality factor extraversion and software product quality. 
51|3||Handling imprecision and uncertainty in software development effort prediction: A type-2 fuzzy logic based framework|Traditional approaches for software projects effort prediction such as the use of mathematical formulae derived from historical data, or the use of experts judgments are plagued with issues pertaining to effectiveness and robustness in their results. These issues are more pronounced when these effort prediction approaches are used during the early phases of the software development lifecycle, for example requirements development, whose effort predictors along with their relationships to effort are characterized as being even more imprecise and uncertain than those of later development phases, for example design. Recent works have demonstrated promising results using approaches based on fuzzy logic. Effort prediction systems that use fuzzy logic can deal with imprecision; they, however, can not deal with uncertainty. This paper presents an effort prediction framework that is based on type-2 fuzzy logic to allow handling imprecision and uncertainty inherent in the information available for effort prediction. Evaluation experiments have shown the framework to be promising. 
51|3||A framework for understanding creativity in requirements engineering|Creativity is important in the discovery and analysis of user and business requirements to achieve innovative uses of information and communication technologies. This paper builds a theoretical framework for understanding creativity in requirements engineering. The framework provides a systematic means of understanding creativity in requirements engineering and comprises five elements (product, process, domain, people and socio-organisational context). The framework provides researchers with a sound basis for exploring how the five elements of creativity can be incorporated within RE methods and techniques to support creative requirements engineering. It provides practitioners with a systematic means of creating environments that nurture and develop creative people, cognitive and collaborative processes and products. 
51|3||Investigating the relationship between schedules and knowledge transfer in software testing|This empirical study investigates the relationship between schedules and knowledge transfer in software testing. In our exploratory survey, statistical analysis indicated that increased knowledge transfer between testing and earlier phases of software development was associated with testing schedule over-runs. A qualitative case study was conducted to interpret this result. We found that this relationship can be explained with the size and complexity of software, knowledge management issues, and customer involvement. We also found that the primary strategies for avoiding testing schedule over-runs were reducing the scope of testing, leaving out features from the software, and allocating more resources to testing. 
51|4|http://www.sciencedirect.com/science/journal/09505849/51/4|Analysis of test suite reduction with enhanced tie-breaking techniques|
51|4||A bottom-up pointer analysis using the update history|Pointer analysis is an important part for the source code analysis of C programs. In this paper, we propose a bottom-up and flow- and context-sensitive pointer analysis algorithm, where bottom-up refers to the ability to perform the analysis from callee modules to caller modules. Our approach is based on a new modular pointer analysis domain named the update history that can abstract memory states of a procedure independently of the information on aliases between memory locations and keep the information on the order of side effects performed. Such a memory representation not only enables the analysis to be formalized as a bottom-up analysis, but also helps the analysis to effectively identify killed side effects and relevant alias contexts. The experiments performed on a pilot implementation of the method shows that our approach is effective for improving the precision of a client analysis. 
51|4||Automated test data generation using a scatter search approach|The techniques for the automatic generation of test cases try to efficiently find a small set of cases that allow a given adequacy criterion to be fulfilled, thus contributing to a reduction in the cost of software testing. In this paper we present and analyze two versions of an approach based on the scatter search metaheuristic technique for the automatic generation of software test cases using a branch coverage adequacy criterion. The first test case generator, called TCSS, uses a diversity property to extend the search of test cases to all branches of the program under test in order to generate test cases that cover these. The second, called TCSS-LS, is an extension of the previous test case generator which combines the diversity property with a local search method that allows the intensification of the search for test cases that cover the difficult branches. We present the results obtained by our generators and carry out a detailed comparison with many other generators, showing a good performance of our approach. 
51|4||A framework for developing measurement systems and its industrial evaluation|As in every engineering discipline, metrics play an important role in software development, with the difference that almost all software projects need the customization of metrics used. In other engineering disciplines, the notion of a measurement system (i.e. a tool used to collect, calculate, and report quantitative data) is well known and defined, whereas it is not as widely used in software engineering. In this paper we present a framework for developing custom measurement systems and its industrial evaluation in a software development unit within Ericsson. The results include the framework for designing measurement systems and its evaluation in real life projects at the company. The results show that with the help of ISO/IEC standards, measurement systems can be effectively used in software industry and that the presented framework improves the way of working with metrics. This paper contributes with the presentation of how automation of metrics collection and processing can be successfully introduced into a large organization and shows the benefits of it: increased efficiency of metrics collection, increased adoption of metrics in the organization, independence from individuals and standardized nomenclature for metrics in the organization. 
51|4||Comparison of estimation methods of cost and duration in IT projects|Producing accurate and reliable project cost estimations at an early stage of a project’s life cycle remains a substantial challenge in the information technology field. This research benchmarks the performance of various approaches to estimating IT project effort and duration. Empirical data were gathered from various “real-world” organizations including several prominent Israeli high-tech companies as well as from the International Software Benchmarking Standards Group (ISBSG) IT project database. The study contrasts two types of models that have been employed to estimate project duration and effort separately: linear regression estimation models and models deriving from a more novel approach based on artificial neural networks (ANNs). 
51|4||Introducing requirements traceability support in model-driven development of web applications|In this work, we present an approach that introduces requirements traceability capabilities in the context of model-driven development of Web applications. This aspect allows us to define model-to-model transformations that not only provide a software artifact of lower abstraction (as model-to-model transformations usually do) but also to provide feedback about how they are applied. This feedback helps us to validate whether transformations are correctly applied. In particular, we present a model-to-model transformation that allows us to obtain navigational models of the Web engineering method OOWS from a requirements model. This transformation is defined as a set of mappings between these two models that have been implemented by means of graph transformations. The use of graph transformations allows us to develop a tool-supported strategy for applying mappings automatically. In addition, mechanisms for tracing requirements are also included in the definition of graph transformations. These mechanisms allow us to link each conceptual element to the requirements from which it is derived. In particular, we focus on tracing requirements throughout the navigational model, which describe the navigational structure of a Web application. To take advantage of these traceability mechanisms, we have developed a tool that obtains traceability reports after applying transformations. These reports help us to study aspects such as whether requirements are all supported, the impact of changing a requirement, or how requirements are modelled. 
51|4||Supporting user-oriented analysis for multi-view domain-specific visual languages|The integration of usable and flexible analysis support in modelling environments is a key success factor in Model-Driven Development. In this paradigm, models are the core asset from which code is automatically generated, and thus ensuring model correctness is a fundamental quality control activity. For this purpose, a common approach is to transform the system models into formal semantic domains for verification. However, if the analysis results are not shown in a proper way to the end-user (e.g. in terms of the original language) they may become useless.In this paper we present a novel DSVL called BaVeL that facilitates the flexible annotation of verification results obtained in semantic domains to different formats, including the context of the original language. BaVeL is used in combination with a consistency framework, providing support for all steps in a verification process: acquisition of additional input data, transformation of the system models into semantic domains, verification, and flexible annotation of analysis results.The approach has been validated analytically by the cognitive dimensions framework, and empirically by its implementation and application to several DSVLs. Here we present a case study of a notation in the area of Digital Libraries, where the analysis is performed by transformations into Petri nets and a process algebra. 
51|4||The repertory grid technique: Its place in empirical software engineering research|Personal construct theory (applied via the repertory grid technique) supports interpretivist research in a structured manner and, as such, has relevance for researchers conducting studies focused on the human and organisational aspects of software engineering. Personal construct theory (which underpins the repertory grid technique) is introduced, and the technique and its administration is discussed. Research studies from the literature are reviewed to provide illustrative examples of its application within a software engineering context. Since any research approach needs to answer questions about its reliability and validity within a particular study, these issues are considered for repertory grid investigations and criteria are offered that can be used to judge these issues within a planned, and/or reported, study. 
51|4||Factor oriented requirement coverage based system test case prioritization of new and regression test cases|Test case prioritization involves scheduling test cases in an order that increases the effectiveness in achieving some performance goals. One of the most important performance goals is the rate of fault detection. Test cases should run in an order that increases the possibility of fault detection and also that detects the most severe faults at the earliest in its testing life cycle. In this paper, we propose to put forth a model for system level test case prioritization (TCP) from software requirement specification to improve user satisfaction with quality software that can also be cost effective and to improve the rate of severe fault detection. The proposed model prioritizes the system test cases based on the six factors: customer priority, changes in requirement, implementation complexity, completeness, traceability and fault impact. The proposed prioritization technique is validated with two different validation techniques and is experimented in three phases with student projects and two sets of industrial projects and the results show convincingly that the proposed prioritization technique improves the rate of severe fault detection. 
51|5|http://www.sciencedirect.com/science/journal/09505849/51/5|Model-Driven Development for secure information systems|
51|5||Automated analysis of security-design models|We have previously proposed SecureUML, an expressive UML-based language for constructing security-design models, which are models that combine design specifications for distributed systems with specifications of their security policies. Here, we show how to automate the analysis of such models in a semantically precise and meaningful way. In our approach, models are formalized together with scenarios that represent possible run-time instances. Queries about properties of the security policy modeled are expressed as formulas in UML’s Object Constraint Language. The policy may include both declarative aspects, i.e., static access-control information such as the assignment of users and permissions to roles, and programmatic aspects, which depend on dynamic information, namely the satisfaction of authorization constraints in a given scenario. We show how such properties can be evaluated, completely automatically, in the context of the metamodel of the security-design language. We demonstrate, through examples, that this approach can be used to formalize and check non-trivial security properties. The approach has been implemented in the SecureMOVA tool and all of the examples presented have been checked using this tool. 
51|5||A security-aware metamodel for multi-agent systems (MAS)|This paper adopts a model-based security (MBS) approach to identify security requirements during the early stages of multi-agent system development. Our adopted MBS approach is underpinned by a metamodel independent of any specific methodology. It allows for security considerations to be embedded within any situated agent methodology which then prescribes security considerations within its work products. Using a standard model-driven engineering (MDE) approach, these work products are initially constructed as high abstraction models and then transformed into more precise models until code-specific models can be produced. A multi-agent system case study is used to illustrate the applicability of the proposed security-aware metamodel. 
51|5||An aspect-oriented methodology for designing secure applications|We propose a methodology, based on aspect-oriented modeling (AOM), for incorporating security mechanisms in an application. The functionality of the application is described using the primary model and the attacks are specified using aspects. The attack aspect is composed with the primary model to obtain the misuse model. The misuse model describes how much the application can be compromised. If the results are unacceptable, then some security mechanism must be incorporated into the application. The security mechanism, modeled as security aspect, is composed with the primary model to obtain the security-treated model. The security-treated model is analyzed to give assurance that it is resilient to the attack. 
51|5||A model-based aspect-oriented framework for building intrusion-aware software systems|Security is a critical issue for software systems, especially for those systems which are connected to networks and the Internet, since most of them suffer from various malicious attacks. Intrusion detection is an approach to protect software against such attacks. However, security vulnerabilities that are exploited by intruders cut across multiple modules in software systems and are difficult to address and monitor. These kinds of concerns, called cross-cutting concerns, can be handled by aspect-oriented software development (AOSD) for better modularization. A number of works have utilized AOSD to address security issues of software systems, but none of them has employed AOSD for intrusion detection. In this paper, we propose a model-based aspect-oriented framework for building intrusion-aware software systems. We model attack scenarios and intrusion detection aspects using an aspect-oriented Unified Modeling Language (UML) profile. Based on the UML model, the intrusion detection aspects are implemented and woven into the target system. The resulting target system has the ability to detect the intrusions automatically. We present an experimental evaluation by applying this framework for some of the most common attacks included in the Web Application Security Consortium (WASC) web security threat classification. The experimental results demonstrate that the framework is effective in specifying and implementing intrusion detection and can be applied for a wide range of attacks. 
51|5||XRound: A reversible template language and its application in model-based security analysis|Successful analysis of the models used in Model-Driven Development requires the ability to synthesise the results of analysis and automatically integrate these results with the models themselves. This paper presents a reversible template language called XRound which supports round-trip transformations between models and the logic used to encode system properties. A template processor that supports the language is described, and the use of the template language is illustrated by its application in an analysis workbench, designed to support analysis of security properties of UML and MOF-based models. As a result of using reversible templates, it is possible to seamlessly and automatically integrate the results of a security analysis with a model. 
51|5||Model-Based Development of firewall rule sets: Diagnosing model inconsistencies|The design and management of firewall rule sets is a very difficult and error-prone task because of the difficulty of translating access control requirements into complex low-level firewall languages. Although high-level languages have been proposed to model firewall access control lists, none has been widely adopted by the industry. We think that the main reason is that their complexity is close to that of many existing low-level languages. In addition, none of the high-level languages that automatically generate firewall rule sets verifies the model prior to the code-generation phase. Error correction in the early stages of the development process is cheaper compared to the cost associated with correcting errors in the production phase. In addition, errors generated in the production phase usually have a huge impact on the reliability and robustness of the generated code and final system.In this paper, we propose the application of the ideas of Model-Based Development to firewall access control list modelling and automatic rule set generation. First, an analysis of the most widely used firewall languages in the industry is conducted. Next, a Platform-Independent Model for firewall ACLs is proposed. This model is the result of exhaustive analysis and of a discussion of different alternatives for models in a bottom-up methodology. Then, it is proposed that a verification stage be added in the early stages of the Model-Based Development methodology, and a polynomial time complexity process and algorithms are proposed to detect and diagnose inconsistencies in the Platform-Independent Model. Finally, a theoretical complexity analysis and empirical tests with real models were conducted, in order to prove the feasibility of our proposal in real environments. 
51|5||Experimental comparison of attack trees and misuse cases for security threat identification|A number of methods have been proposed or adapted to include security in the requirements analysis stage, but the industrial take-up has been limited and there are few empirical and comparative evaluations. This paper reports on a pair of controlled experiments that compared two methods for early elicitation of security threats, namely attack trees and misuse cases. The 28 and 35 participants in the two experiments solved two threat identification tasks individually by means of the two techniques, using a Latin-Squares design to control for technique and task order. The dependent variables were effectiveness of the techniques measured as the number of threats found, coverage of the techniques measured in terms of the types of threats found and perceptions of the techniques measured through a post-task questionnaire based on the Technology Acceptance Model. The only difference was that, in the second experiment, the participants were given a pre-drawn use-case diagram to use as a starting point for solving the tasks. In the first experiment, no pre-drawn use-case diagram was provided. The main finding was that attack trees were more effective for finding threats, in particular when there was no pre-drawn use-case diagram. However, the participants had similar opinions of the two techniques, and perception of a technique was not correlated with performance with that technique. The study underlines the need for further comparisons in a broader range of settings involving additional techniques, and it suggests several concrete experiments and other paths for further work. 
51|5||An adaptive security model using agent-oriented MDA|Model-driven architecture (MDA) supports model-centred software development via successive model transformation. In MDA, the reusability of models is improved as well as the traceability of requirements. Agent-oriented model-driven architecture (AMDA) associates adaptive agents with a business-oriented interaction model and lets agents dynamically interpret their behaviour from the continuously maintained model via which the current business needs are deployed at runtime. The continuous re-interpretation rather than discrete re-transformation of models means immediate requirements deployment after re-configuration, no system down time being required to affect changes and results in a development process that is oriented to business experts rather than developers. Adopting the adaptive agent model, an AMDA paradigm, we put forward a security–aware model-driven mechanism by using an extension of the role-based access control (RBAC) model. For this purpose, the concept of agent role proposed in agent-oriented software engineering (AOSE) is integrated with the one proposed in RBAC. Agent duties are specified in an interaction model and describe the roles that agents can play to fulfil their functional responsibilities. Agent rights are specified in a security policy rule model attached to the interaction model and describe constraints upon agent capabilities caused by their associated social roles. The role-based interaction and policy-driven model incorporates both agent rights and duties. Hence, functional requirements and non-functional security constraint requirements are put together, related by the concept of role. Consequently, agents can continuously use the re-configurable model to play their roles in order to fulfil their responsibilities, and at the same time respect the security constraints. The major contribution from the approach is a method for building adaptive and secure MAS, following model-driven architecture. The approach is illustrated with an actual British railway management system. 
51|6|http://www.sciencedirect.com/science/journal/09505849/51/6|A systematic review of search-based testing for non-functional system properties|Search-based software testing is the application of metaheuristic search techniques to generate software tests. The test adequacy criterion is transformed into a fitness function and a set of solutions in the search space are evaluated with respect to the fitness function using a metaheuristic search technique. The application of metaheuristic search techniques for testing is promising due to the fact that exhaustive testing is infeasible considering the size and complexity of software under test. Search-based software testing has been applied across the spectrum of test case design methods; this includes white-box (structural), black-box (functional) and grey-box (combination of structural and functional) testing. In addition, metaheuristic search techniques have also been applied to test non-functional properties. The overall objective of undertaking this systematic review is to examine existing work into non-functional search-based software testing (NFSBST). We are interested in types of non-functional testing targeted using metaheuristic search techniques, different fitness functions used in different types of search-based non-functional testing and challenges in the application of these techniques. The systematic review is based on a comprehensive set of 35 articles obtained after a multi-stage selection process and have been published in the time span 1996–2007. The results of the review show that metaheuristic search techniques have been applied for non-functional testing of execution time, quality of service, security, usability and safety. A variety of metaheuristic search techniques are found to be applicable for non-functional testing including simulated annealing, tabu search, genetic algorithms, ant colony methods, grammatical evolution, genetic programming (and its variants including linear genetic programming) and swarm intelligence methods. The review reports on different fitness functions used to guide the search for each of the categories of execution time, safety, usability, quality of service and security; along with a discussion of possible challenges in the application of metaheuristic search techniques. 
51|6||A UML profile for the conceptual modelling of data-mining with time-series in data warehouses|Time-series analysis is a powerful technique to discover patterns and trends in temporal data. However, the lack of a conceptual model for this data-mining technique forces analysts to deal with unstructured data. These data are represented at a low-level of abstraction and their management is expensive. Most analysts face up to two main problems: (i) the cleansing of the huge amount of potentially-analysable data and (ii) the correct definition of the data-mining algorithms to be employed. Owing to the fact that analysts’ interests are also hidden in this scenario, it is not only difficult to prepare data, but also to discover which data is the most promising. Since their appearance, data warehouses have, therefore, proved to be a powerful repository of historical data for data-mining purposes. Moreover, their foundational modelling paradigm, such as, multidimensional modelling, is very similar to the problem domain. In this article, we propose a unified modelling language (UML) extension through UML profiles for data-mining. Specifically, the UML profile presented allows us to specify time-series analysis on top of the multidimensional models of data warehouses. Our extension provides analysts with an intuitive notation for time-series analysis which is independent of any specific data-mining tool or algorithm. In order to show its feasibility and ease of use, we apply it to the analysis of fish-captures in Alicante. We believe that a coherent conceptual modelling framework for data-mining assures a better and easier knowledge-discovery process on top of data warehouses. 
51|6||Requirements-based Access Control Analysis and Policy Specification (ReCAPS)|Access control (AC) is a mechanism for achieving confidentiality and integrity in software systems. Access control policies (ACPs) express rules concerning who can access what information, and under what conditions. ACP specification is not an explicit part of the software development process and is often isolated from requirements analysis activities, leaving systems vulnerable to security breaches because policies are specified without ensuring compliance with system requirements. In this paper, we present the Requirements-based Access Control Analysis and Policy Specification (ReCAPS) method for deriving and specifying ACPs, and discuss three validation efforts. The method integrates policy specification into the software development process, ensures consistency across software artifacts, and provides prescriptive guidance for how to specify ACPs. It also improves the quality of requirements specifications and system designs by clarifying ambiguities and resolving conflicts across these artifacts during the analysis, making a significant step towards ensuring that policies are enforced in a manner consistent with a system’s requirements specifications. To date, the method has been applied within the context of four operational systems. Additionally, we have conducted an empirical study to evaluate its usefulness and effectiveness. A software tool, the Security and Privacy Requirements Analysis Tool (SPRAT), was developed to support ReCAPS analysis activities. 
51|6||Reduction rules for YAWL workflows with cancellation regions and OR-joins|As the need for concepts such as cancellation and OR-joins occurs naturally in business scenarios, comprehensive support in a workflow language is desirable. However, there is a clear trade-off between the expressive power of a language (i.e., introducing complex constructs such as cancellation and OR-joins) and ease of verification. When a workflow contains a large number of tasks and involves complex control flow dependencies, verification can take too much time or it may even be impossible. There are a number of different approaches to deal with this complexity. Reducing the size of the workflow, while preserving its essential properties with respect to a particular analysis problem, is one such approach. In this paper, we present a set of reduction rules for workflows with cancellation regions and OR-joins and demonstrate how they can be used to improve the efficiency of verification. Our results are presented in the context of the YAWL workflow language. 
51|6||Tool-supported requirements prioritization: Comparing the AHP and CBRank methods|Requirements prioritization aims at identifying the most important requirements for a software system, a crucial step when planning for system releases and deciding which requirements to implement in each release. Several prioritization methods and supporting tools have been proposed so far. How to evaluate their properties, with the aim of supporting the selection of the most appropriate method for a specific project, is considered a relevant question.In this paper, we present an empirical study aiming at evaluating two state-of-the art tool-supported requirements prioritization methods, AHP and CBRank. We focus on three measures: the ease of use, the time-consumption and the accuracy. The experiment has been conducted with 23 experienced subjects on a set of 20 requirements from a real project. Results indicate that for the first two characteristics CBRank overcomes AHP, while for the accuracy AHP performs better than CBRank, even if the resulting ranks from the two methods are very similar. The majority of the users found CBRank the “overall best” method. 
51|6||An engineering process for developing Secure Data Warehouses|We present a new approach for the elicitation and development security requirements in the entire Data Warehouse (DWs) life cycle, which we have called a Secure Engineering process for DAta WArehouses (SEDAWA). Whilst many methods for the requirements analysis phase of the DWs have been proposed, the elicitation of security requirements as non-functional requirements has not received sufficient attention. Hence, in this paper we propose a methodology for the DW design based on Model Driven Architecture (MDA) and the standard Software Process Engineering Metamodel Specification (SPEM) from the Object Management Group (OMG). We define four phases comprising of several activities and steps, an d five disciplines which cover the whole DW design. Our methodology adapts the i∗ framework to be used under MDA and the SPEM approaches in order to elicit and develop security requirements for DWs. The benefits of our proposal are shown through an example related to the management of the pharmacies consortium business. 
51|6||Malaca: A component and aspect-oriented agent architecture|The production of maintainable and reusable agents depends largely on how well the agent architecture is modularized. Most commercial agent toolkits provide an Object-Oriented (OO) framework, whose agent architecture does not facilitate separate (re)use of the domain-specific functionality of an agent from other concerns. This paper presents Malaca, an agent architecture that combines the use of Component-based Software Engineering and Aspect-Oriented Software Development, both of which promote better modularization of the agent architecture while increase at the architectural level. Malaca supports the separate (re)use of the domain-specific functionality of an agent from other communication concerns, providing explicit support for the design and configuration of agent architectures and allows the development of agent-based software so that it is easy to understand, maintain and reuse. 
51|6||Software product integration: A case study-based synthesis of reference models|In software intensive systems the integration becomes complex since both software and hardware components are integrated and run in the execution environment for the first time. Support for this stage is thus essential. Practices for Product Integration are described in different reference models. We have investigated these and compared them with activities performed in seven product development projects.Our conclusion is that descriptions of best practices in product integration are available in different reference models, but need to be merged into one set of practices. Through case studies we see that the described practices are insufficiently used in industry, and that organizations would benefit from adhering to them. Our investigations indicate that a set of practices are necessary to be successful in software product integration: define and check criteria for integration, review interface descriptions and ensure coordination of interface changes, and deliver components as agreed. In addition to these, a set of practices are supporting the integration activities, including the definition of an integration strategy, and the establishment of a suitable integration environment. 
51|6||Empirical evaluation in Computer Science research published by ACM|This paper repeats part of the analysis performed in the 1995 paper “Experimental evaluation in Computer Science: a quantitative study” by Tichy and collaborators, for 147 papers randomly selected from the ACM, published in the year 2005. The papers published in 2005 are classified in the following way: 4% theory, 17% empirical, 4.7% hypothesis testing, 3.4% other, and 70% design and modeling (using the 1995 paper categories). Within the design and modeling class, 33% of the papers have no evaluation. The numbers of the 2005 sample are very similar to the original figures for the 1995 sample, which shows that Computer Science research has not increased significantly its empirical or experimental component. 
51|7|http://www.sciencedirect.com/science/journal/09505849/51/7|A systematic literature review to identify and classify software requirement errors|Most software quality research has focused on identifying faults (i.e., information is incorrectly recorded in an artifact). Because software still exhibits incorrect behavior, a different approach is needed. This paper presents a systematic literature review to develop taxonomy of errors (i.e., the sources of faults) that may occur during the requirements phase of software lifecycle. This taxonomy is designed to aid developers during the requirement inspection process and to improve overall software quality. The review identified 149 papers from the software engineering, psychology and human cognition literature that provide information about the sources of requirements faults. A major result of this paper is a categorization of the sources of faults into a formal taxonomy that provides a starting point for future research into error-based approaches to improving software quality. 
51|7||The effectiveness of pair programming: A meta-analysis|Several experiments on the effects of pair versus solo programming have been reported in the literature. We present a meta-analysis of these studies. The analysis shows a small significant positive overall effect of pair programming on quality, a medium significant positive overall effect on duration, and a medium significant negative overall effect on effort. However, between-study variance is significant, and there are signs of publication bias among published studies on pair programming. A more detailed examination of the evidence suggests that pair programming is faster than solo programming when programming task complexity is low and yields code solutions of higher quality when task complexity is high. The higher quality for complex tasks comes at a price of considerably greater effort, while the reduced completion time for the simpler tasks comes at a price of noticeably lower quality. We conclude that greater attention should be given to moderating factors on the effects of pair programming. 
51|7||Using status messages in the distributed test architecture|If the system under test has multiple interfaces/ports and these are physically distributed then in testing we place a tester at each port. If these testers cannot directly communicate with one another and there is no global clock then we are testing in the distributed test architecture. If the distributed test architecture is used then there may be input sequences that cannot be applied in testing without introducing controllability problems. Additionally, observability problems can allow fault masking. In this paper we consider the situation in which the testers can apply a status message: an input that causes the system under test to identify its current state. We show how such a status message can be used in order to overcome controllability and observability problems. 
51|7||A data flow-based structural testing technique for FBD programs|With increased use of programmable logic controllers (PLCs) in implementing critical systems, quality assurance became an important issue. Regulation requires structural testing be performed for safety-critical systems by identifying coverage criteria to be satisfied and accomplishment measured. Classical coverage criteria, based on control flow graphs, are inadequate when applied to a data flow language function block diagram (FBD) which is a PLC programming language widely used in industry. We propose three structural coverage criteria for FBD programs, analyze relationship among them, and demonstrate their effectiveness using a real-world reactor protection system. Using test cases that had been manually prepared by FBD testing professionals, our technique found many aspects of the FBD logic that were not tested sufficiently. Domain experts, having found the approach highly intuitive, found the technique effective. 
51|7||A stock recommendation system exploiting rule discovery in stock databases|This paper addresses an approach that recommends investment types to stock investors by discovering useful rules from past changing patterns of stock prices in databases. First, we define a new rule model for recommending stock investment types. For a frequent pattern of stock prices, if its subsequent stock prices are matched to a condition of an investor, the model recommends a corresponding investment type for this stock. The frequent pattern is regarded as a rule head, and the subsequent part a rule body. We observed that the conditions on rule bodies are quite different depending on dispositions of investors while rule heads are independent of characteristics of investors in most cases. With this observation, we propose a new method that discovers and stores only the rule heads rather than the whole rules in a rule discovery process. This allows investors to impose various conditions on rule bodies flexibly, and also improves the performance of a rule discovery process by reducing the number of rules to be discovered. For efficient discovery and matching of rules, we propose methods for discovering frequent patterns, constructing a frequent pattern base, and its indexing. We also suggest a method that finds the rules matched to a query from a frequent pattern base, and a method that recommends an investment type by using the rules. Finally, we verify the effectiveness and the efficiency of our approach through extensive experiments with real-life stock data. 
51|7||Special section on Software Engineering for Secure Systems (SESS â07)|
51|7||On the secure software development process: CLASP, SDL and Touchpoints compared|Development processes for software construction are common knowledge and mainstream practice in most development organizations. Unfortunately, these processes offer little support in order to meet security requirements. Over the years, research efforts have been invested in specific methodologies and techniques for secure software engineering, yet dedicated processes have been proposed only recently.In this paper, three high-profile processes for the development of secure software, namely OWASP’s CLASP, Microsoft’s SDL and McGraw’s Touchpoints, are evaluated and compared in detail. The paper identifies the commonalities, discusses the specificity of each approach, and proposes suggestions for improvement. 
51|7||Security enforcement aware software development|In the domain of security policy enforcement, the concerns of application developers are almost completely ignored. As a consequence, it is hard to develop useful and reliable applications that will function properly under a variety of policies. This paper addresses this issue for application security policies specified as security automata, and enforced through run-time monitoring. Our solution consists of three elements: the definition of an abstract interface to the policy that is being enforced, a sound construct to query that policy, and a static verification algorithm that guarantees absence of security policy violations in critical blocks of code. 
51|8|http://www.sciencedirect.com/science/journal/09505849/51/8|Patterns-based evaluation of open source BPM systems: The cases of jBPM, OpenWFE, and Enhydra Shark|In keeping with the proliferation of free software development initiatives and the increased interest in the business process management domain, many open source workflow and business process management systems have appeared during the last few years and are now under active development. This upsurge gives rise to two important questions: What are the capabilities of these systems? and How do they compare to each other and to their closed source counterparts? In other words: What is the state-of-the-art in the area?. To gain an insight into these questions, we have conducted an in-depth analysis of three of the major open source workflow management systems – jBPM, OpenWFE, and Enhydra Shark, the results of which are reported here. This analysis is based on the workflow patterns framework and provides a continuation of the series of evaluations performed using the same framework on closed source systems, business process modelling languages, and web-service composition standards. The results from evaluations of the three open source systems are compared with each other and also with the results from evaluations of three representative closed source systems: Staffware, WebSphere MQ, and Oracle BPEL PM. The overall conclusion is that open source systems are targeted more toward developers rather than business analysts. They generally provide less support for the patterns than closed source systems, particularly with respect to the resource perspective, i.e. the various ways in which work is distributed amongst business users and managed through to completion. 
51|8||Guideline for the definition of EMF metamodels using an Entity-Relationship approach|Metamodels are a formalism for defining the abstract syntax of modeling languages. However, designing a suitable metamodel from the features intended for the language is not a trivial task. This paper presents a guideline for defining such metamodels using an Entity-Relationship approach in the Eclipse Modeling Framework. This guideline proposes to begin by determining the structural features of the language, such as types of relationships and elements with attributes. Subsequently, it offers alternative representations for these features aimed at satisfying different requirements, such as changeability or optimized model processing. Two case studies illustrate the use of the guideline and its trade-offs. 
51|8||Identifying high perceived value practices of CMMI level 2: An empirical study|ObjectiveIn this paper, we present findings from an empirical study that was aimed at identifying the relative “perceived value” of CMMI level 2 specific practices based on the perceptions and experiences of practitioners of small and medium size companies. The objective of this study is to identify the extent to which a particular CMMI practice is used in order to develop a finer-grained framework, which encompasses the notion of perceived value within specific practices.MethodWe used face-to-face questionnaire based survey sessions as the main approach to collecting data from 46 software development practitioners from Malaysia and Vietnam. We asked practitioners to choose and rank CMMI level 2 practices against the five types of assessments (high, medium, low, zero or do not know). From this, we have proposed the notion of ‘perceived value’ associated with each practice.ResultsWe have identified three ‘requirements management’ practices as having a ‘high perceived value’. The results also reveal the similarities and differences in the perceptions of Malaysian and Vietnamese practitioners with regard to the relative values of different practices of CMMI level 2 process areas.ConclusionsSmall and medium size companies should not be seen as being “at fault” for not adopting CMMI – instead the Software Process Improvement (SPI) implementation approaches and its transition mechanisms should be improved. We argue that research into “tailoring” existing process capability maturity models may address some of the issues of small and medium size companies. 
51|8||Model-driven development of composite context-aware web applications|Context-awareness constitutes an essential aspect of services, especially when interaction with end-users is involved. In this paper a solution for the context-aware development of web applications consisting of web services is presented. The methodology proposes a model based approach and advocates in favour of a complete separation of the web application functionality from the context adaptation at all development phases (analysis, design, implementation). In essence, context adaptation takes place on top of and is transparent to the web application business functionality. Starting from UML diagrams of independent web services and respective UML context models, our approach can produce a functional composite context-aware application. At execution level this independence is maintained through an adaptation framework based on message interception. 
51|8||WS-BPEL Extensions for Versioning|This article proposes specific extensions for WS-BPEL (Business Process Execution Language) to support versioning of processes and partner links. It introduces new activities and extends existing activities, including partner links, invoke, receive, import, and onmessage activities. It proposes version-related extensions to variables and introduces version handlers. The proposed extensions represent a complete solution for process-level and scope-level versioning at development, deployment, and run-time. It also provides means to version applications that consist of several BPEL processes, and to put temporal constraints on versions. The proposed approach has been tested in real-world environment. It solves major challenges in BPEL versioning. 
51|8||Utilizing domain models for application design and validation|Domain analysis enables identifying families of applications and capturing their terminology in order to assist and guide system developers to design valid applications in the domain. One major way of carrying out the domain analysis is modeling. Several studies suggest using metamodeling techniques, feature-oriented approaches, or architectural-based methods for modeling domains and specifying applications in those domains. However, these methods mainly focus on representing the domain knowledge, providing insufficient guidelines (if any) for creating application models that satisfy the domain rules and constraints. In particular, validation of the application models which include application-specific knowledge is insufficiently dealt. In order to fill these lacks, we propose a general approach, called Application-based DOmain Modeling (ADOM), which enables specifying domains and applications similarly, (re)using domain knowledge in application models, and validating the application models against the relevant domain models. In this paper we present the ADOM approach, demonstrating its application to UML 2.0 class and sequence diagrams. 
51|9|http://www.sciencedirect.com/science/journal/09505849/51/9|On the generation of requirements specifications from software engineering models: A systematic literature review|System and software requirements documents play a crucial role in software engineering in that they must both communicate requirements to clients in an understandable manner and define requirements in precise detail for system developers. The benefits of both lists of textual requirements (usually written in natural language) and software engineering models (usually specified in graphical form) can be brought together by combining the two approaches in the specification of system and software requirements documents. If, moreover, textual requirements are generated from models in an automatic or closely monitored form, the effort of specifying those requirements is reduced and the completeness of the specification and the management of the requirements traceability are improved. This paper presents a systematic review of the literature related to the generation of textual requirements specifications from software engineering models. 
51|9||A strategy-based process for effectively determining system requirements in eCRM development|Customer relationship management (CRM) is an important concept to maintain competitiveness at e-commerce. Thus, many organizations hastily implement eCRM and fail to achieve its goal. CRM concept consists of a number of compound components on product designs, marketing attributes, and consumer behaviors. This requires different approaches from traditional ones in developing eCRM. Requirements engineering is one of the important steps in software development. Without a well-defined requirements specification, developers do not know how to proceed with requirements analysis. This research proposes a strategy-based process for requirements elicitation. This framework contains three steps: define customer strategies, identify consumer and marketing characteristics, and determine system requirements. Prior literature lacks discussing the important role of customer strategies in eCRM development. Empirical findings reveal that this strategy-based view positively improves the performance of requirements elicitation. 
51|9||Empirical investigation of refactoring effect on software quality|Developers and designers always strive for quality software. Quality software tends to be robust, reliable and easy to maintain, and thus reduces the cost of software development and maintenance. Several methods have been applied to improve software quality. Refactoring is one of those methods. The goal of this paper is to validate/invalidate the claims that refactoring improves software quality. We focused this study on different external quality attributes, which are adaptability, maintainability, understandability, reusability, and testability. We found that refactoring does not necessarily improve these quality attributes. 
51|9||Sizing user stories using paired comparisons|Agile estimation approaches usually start by sizing the user stories to be developed by comparing them to one another. Various techniques, with varying degrees of formality, are used to perform the comparisons – plain contrasts, triangulation, planning poker, and voting. This article proposes the use of a modified paired comparison method in which a reduced number of comparisons is selected according to an incomplete cyclic design. Using two sets of data, the authors show that the proposed method produces good estimates, even when the number of comparisons is reduced by half those required by the original formulation of the method. 
51|9||A method for detecting the theft of Java programs through analysis of the control flow information|A software birthmark refers to the inherent characteristics of a program that can be used to identify the program. In this paper, a method for detecting the theft of Java programs through a static software birthmark is proposed that is based on the control flow information. The control flow information shows the structural characteristics and the possible behaviors during the execution of program. Flow paths (FP) and behaviors in Java programs are formally described here, and a set of behaviors of FPs is used as a software birthmark. The similarity is calculated by matching the pairs of similar behaviors from two birthmarks. Experiments centered on the proposed birthmark with respect to precision and recall. The performance was evaluated by analyzing the F-measure curves. The experimental results show that the proposed birthmark is a more effective measure compared to earlier approaches for detecting copied programs, even in cases where such programs are aggressively modified. 
51|9||Empirical evaluation of selected best practices in implementation of software process improvement|To be successfully applied in practice, software process improvement (SPI) needs not only identifying what needs to be improved, and which factors will influence its success, but also guidelines on how to implement those improvements and meet the factors. This paper proposes an SPI implementation strategy that has been developed in lines with standard SPI models and frameworks, intended to be suitable for global software development (GSD), and exploiting ideas of incremental deliveries, short iterations with frequent reviews and close collaboration with customers. Quantitative analyses of data from case studies within large GSD show that improvement teams implementing the strategy are more likely to have better progress and achieve better effectiveness in terms of improvement deployment within development teams. The results could be useful to SPI practitioners in making their SPI initiatives more successful. 
51|9||A hybrid heuristic approach to optimize rule-based software quality estimation models|Software quality is defined as the degree to which a software component or system meets specified requirements and specifications. Assessing software quality in the early stages of design and development is crucial as it helps reduce effort, time and money. However, the task is difficult since most software quality characteristics (such as maintainability, reliability and reusability) cannot be directly and objectively measured before the software product is deployed and used for a certain period of time. Nonetheless, these software quality characteristics can be predicted from other measurable software quality attributes such as complexity and inheritance. Many metrics have been proposed for this purpose. In this context, we speak of estimating software quality characteristics from measurable attributes. For this purpose, software quality estimation models have been widely used. These take different forms: statistical models, rule-based models and decision trees. However, data used to build such models is scarce in the domain of software quality. As a result, the accuracy of the built estimation models deteriorates when they are used to predict the quality of new software components. In this paper, we propose a search-based software engineering approach to improve the prediction accuracy of software quality estimation models by adapting them to new unseen software products. The method has been implemented and favorable result comparisons are reported in this work. 
52|1|http://www.sciencedirect.com/science/journal/09505849/52/1|A systematic review of domain analysis tools|The domain analysis process is used to identify and document common and variable characteristics of systems in a specific domain. In order to achieve an effective result, it is necessary to collect, organize and analyze several sources of information about different applications in this domain. Consequently, this process involves distinct phases and activities and also needs to identify which artifacts, arising from these activities, have to be traceable and consistent. In this context, performing a domain analysis process without tool support increases the risks of failure, but the used tool should support the complete process and not just a part of it. This article presents a systematic review of domain analysis tools that aims at finding out how the available tools offer support to the process. As a result, the review identified that these tools are usually focused on supporting only one process and there are still gaps in the complete process support. Furthermore, the results can provide insights for new research in the domain engineering area for investigating and defining new tools, and the study also aids in the identification of companies’ needs for a domain analysis tool. 
52|1||A systematic review on regression test selection techniques|Regression testing is verifying that previously functioning software remains after a change. With the goal of finding a basis for further research in a joint industry-academia research project, we conducted a systematic review of empirical evaluations of regression test selection techniques. We identified 27 papers reporting 36 empirical studies, 21 experiments and 15 case studies. In total 28 techniques for regression test selection are evaluated. We present a qualitative analysis of the findings, an overview of techniques for regression test selection and related empirical evidence. No technique was found clearly superior since the results depend on many varying factors. We identified a need for empirical studies where concepts are evaluated rather than small variations in technical implementations. 
52|1||Characterizing software architecture changes: A systematic review|With today’s ever increasing demands on software, software developers must produce software that can be changed without the risk of degrading the software architecture. One way to address software changes is to characterize their causes and effects. A software change characterization mechanism allows developers to characterize the effects of a change using different criteria, e.g. the cause of the change, the type of change that needs to be made, and the part of the system where the change must take place. This information then can be used to illustrate the potential impact of the change. This paper presents a systematic literature review of software architecture change characteristics. The results of this systematic review were used to create the Software Architecture Change Characterization Scheme (SACCS). This report addresses key areas involved in making changes to software architecture. SACCS’s purpose is to identify the characteristics of a software change that will have an impact on the high-level software architecture. 
52|1||Scalability issues with using FSMWeb to test web applications|Web applications are fast becoming more widespread, larger, more interactive, and more essential to the international use of computers. It is well understood that web applications must be highly dependable, and as a field we are just now beginning to understand how to model and test Web applications. One straightforward technique is to model Web applications as finite state machines. However, large numbers of input fields, input choices and the ability to enter values in any order combine to create a state space explosion problem. This paper evaluates a solution that uses constraints on the inputs to reduce the number of transitions, thus compressing the FSM. The paper presents an analysis of the potential savings of the compression technique and reports actual savings from two case studies. 
52|1||Framework for application management with dynamic aspects J-EARS case study|The cost of computer system maintenance rises together with the increasing complexity of such systems. The use of an autonomic system architecture saves money by delegating some forms of maintenance to the systems themselves. The aim of this paper is to describe the results of creating a tool which introduces elements of adaptivity to Java applications using dynamic aspects. The impact of introducing aspects on the performance of various Application Servers is also discussed. Finally, benefits and problems arising from the use of the tool are presented, basing on sample use cases. 
52|1||Business-oriented software process improvement based on CMMI using QFD|Software Process Improvement (SPI) has become the key to the survival of many software development organizations. As a follow-up of a previous paper on SPI for the CMM using Quality Function Deployment (QFD), a new SPI framework integrating QFD with the CMMI is developed. Similar to the earlier SPI framework for the CMM, the proposed SPI framework based on the CMMI using QFD aims to achieve three objectives: (1) to map process requirements, including business requirements, to the CMMI with the help of QFD; (2) to develop a method based on QFD for the integration and prioritization of requirements from multiple perspectives (groups); and (3) to be able to prioritize software process improvement actions based on process requirements.Process requirements from multiples groups of stakeholders (perspectives), including business goals, are integrated and prioritized. SPI actions are linked to these process requirements using QFD. Thus, the priorities of actions reflect the priorities of process requirements. By executing the actions with the highest priorities, the highest satisfaction level of process requirements can be achieved. One unique feature of the framework for the CMMI Continuous representation is that the priority values of these actions can be compared across PAs, even when the PAs attempt to reach different capability levels. 
52|1||Software development effort prediction: A study on the factors impacting the accuracy of fuzzy logic systems|Reliable effort prediction remains an ongoing challenge to software engineers. Traditional approaches to effort prediction such as the use of models derived from historical data, or the use of expert opinion are plagued with issues pertaining to their effectiveness and robustness. These issues are more pronounced when the effort prediction is used during the early phases of the software development lifecycle. Recent works have demonstrated promising results obtained with the use of fuzzy logic. Fuzzy logic based effort prediction systems can deal better with imprecision, which characterizes the early phases of most software development projects, for example requirements development, whose effort predictors along with their relationships to effort are characterized as being even more imprecise and uncertain than those of later development phases, for example design. Fuzzy logic based prediction systems could produce further better estimates provided that various parameters and factors pertaining to fuzzy logic are carefully set. In this paper, we present an empirical study, which shows that the prediction accuracy of a fuzzy logic based effort prediction system is highly dependent on the system architecture, the corresponding parameters, and the training algorithms. 
52|1||Change profiles of a reused class framework vs. two of its applications|Software reuse is expected to improve software productivity and quality. Although many empirical studies have investigated the benefits and challenges of software reuse from development viewpoints, few studies have explored reuse from the perspective of maintenance. This paper reports on a case study that compares software changes during the maintenance and evolution phases of a reused Java class framework with two applications that are reusing the framework. The results reveal that: (1) The reused framework is more stable, in terms of change density, than the two applications that are reusing it. (2) The reused framework has profiles for change types that are similar to those of the applications, where perfective changes dominate. (3) The maintenance and evolution lifecycle of both the reused framework and its applications is the same: initial development, followed by a stage with extending capabilities and functionality to meet user needs, then a stage in which only minor defect repairs are made, and finally, phase-out. However, the reused framework goes faster from the stage of extending capabilities to the stage in which only minor defect repairs are made than its applications. (4) We have validated that several factors, such as are functionalities, development practice, complexity, size, and age, have affected the change densities and change profiles of the framework and applications. Thus, all these factors must be considered to predict change profiles in the maintenance and evolution phase of software. 
52|10|http://www.sciencedirect.com/science/journal/09505849/52/10|Investigating ERP systems procurement practice: Hong Kong and Australian experiences|ContextIntegration of information systems is now commonly recognized to be a powerful strategic weapon that sharpens the competitive edge of a firm in today’s highly volatile business environment. Such integration can be achieved by replacing the disconnected and incompatible legacy applications by enterprise resource planning (ERP) systems. Along with the remarkable growth of the ERP market, we have seen a number of failure cases of ERP adoption. Such failure cases indicate that not all firms know how to adopt an ERP solution effectively.ObjectiveTo explore and identify crucial practices from real experiences in the Asia–Pacific region that may explain a firm’s success in ERP procurement, with an overt intention toward the formulation of useful lessons that inform practitioners and contribution to advances in software development practices in organizations.MethodA multiple-case design involving three Chinese firms based in Hong Kong and a local firm in Australia was employed. We collected, verified, and analyzed the information about the ERP procurement practice in each subject firm by means of semi-structured interviews, archive reviews, and member checks.ResultsWe summarized our results in the form of 10 lessons learned, together with observations of how culture seems to have played a part in shaping the practice.ConclusionOur results offer practical guidelines originated from real cases that are of use for practitioners to improve the ERP procurement process. 
52|10||WSDL and BPEL extensions for Event Driven Architecture|ContextService Oriented Architecture (SOA) and Event Driven Architecture (EDA) are two acknowledged architectures for the development of business applications and information systems, which have evolved separately over the years.ObjectiveThis paper proposes a solution for extending the SOA/Web Services Platform Architecture (WSPA) with support for business events and EDA concepts. Our solution enables services to act as event producers and event consumers. It also enables event-driven service orchestrations in business processes.MethodBased on a comparison of SOA and EDA, we have identified and designed the required extensions to enable support for events and event-driven process orchestration in WSPA.ResultsWe propose specific extensions to WSDL and BPEL, and a flexible XML representation of the event payload data. We introduce event sinks, sources, and triggers to WSDL. We extend BPEL with new activities to trigger and catch events, and extend fault and event handlers, variables, and correlation properties to accommodate events.ConclusionAs a proof-of-concept, we have developed a prototype implementation and assessed the extensions on three pilot projects. We have shown that our proposed extensions work on real projects and that combining event-driven and service-oriented semantics makes sense in many business applications and can considerably reduce the development effort. 
52|10||Assessment methodology for software process improvement in small organizations|ContextDiagnosing processes in a small company requires process assessment practices which give qualitative and quantitative results; these should offer an overall view of the process capability. The purpose is to obtain relevant information about the running of processes, for use in their control and improvement. However, small organizations have some problems in running process assessment, due to their specific characteristics and limitations.ObjectiveThis paper presents a methodology for assessing software processes which assist the activity of software process diagnosis in small organizations. There is an attempt to address issues such as the fact that: (i) process assessment is expensive and typically requires major company resources and (ii) many light assessment methods do not provide information that is detailed enough for diagnosing and improving processes.MethodTo achieve all this, the METvalCOMPETISOFT assessment methodology was developed. This methodology: (i) incorporates the strategy of internal assessments known as rapid assessment, meaning that these assessments do not take up too much time or use an excessive quantity of resources, nor are they too rigorous and (ii) meets all the requirements described in the literature for an assessment proposal which is customized to the typical features of small companies.ResultsThis paper also describes the experience of the application of this methodology in eight small software organizations that took part in the COMPETISOFT project. The results obtained show that this approach allows us to obtain reliable information about the strengths and weaknesses of software processes, along with information to companies on opportunities for improvement.ConclusionThe assessment methodology proposed sets out the elements needed to assist with diagnosing the process in small organizations step-by-step while seeking to make its application economically feasible in terms of resources and time. From the initial application it may be seen that this assessment methodology can be useful, practical and suitable for diagnosing processes in this type of organizations. 
52|10||Coproduction in successful software development projects|ContextCoproduction of new products has been deemed successful in organizational partnerships by adding to the quality and scope of the product. Techniques that involve users during the development of software tend to mimic this environment, but differ in the type of product and internal client roles. The question is thus, whether coproduction improves the outcomes of a software development project as it has in other disciplines.ObjectiveThis paper evaluates how the coproduction relationship between software developers and users improves the outcomes of a development project. Coproduction is believed to improve outcomes when available knowledge is accessible and applicable to the objective of the development project. Should the relationships hold, coproduction approaches to development can be approached with confidence and improvements made by attention to the development and deployment of expertise.MethodA quantitative questionnaire related to the coproduction environment was developed for four variables to include coproduction, applying expertise, locating expertise, and project success. 128 users from development teams responded to the survey and represent a variety of industries, individual characteristics, and project sizes.ResultsExpertise is crucial to the success of a software development project and coproduction improves the ability to access and apply the needed expertise. In addition, coproduction directly improves outcomes.ConclusionCoproduction can be an effective approach to the development of systems in terms of meeting project goals. Additionally, the assembly of expertise on the team is an important contributor to successful outcomes that may be enhanced through effective selection of team members. The ability to locate the available expertise is crucial, indicating the value of team building functions to promote awareness of expertise location. 
52|10||A method for forecasting defect backlog in large streamline software development projects and its industrial evaluation|ContextPredicting a number of defects to be resolved in large software projects (defect backlog) usually requires complex statistical methods and thus is hard to use on a daily basis by practitioners in industry. Making predictions in simpler and more robust way is often required by practitioners in software engineering industry.ObjectiveThe objective of this paper is to present a simple and reliable method for forecasting the level of defect backlog in large, lean-based software development projects.MethodThe new method was created as part of an action research project conducted at Ericsson. In order to create the method we have evaluated multivariate linear regression, expert estimations and analogy-based predictions w.r.t. their accuracy and ease-of-use in industry. We have also evaluated the new method in a life project at one of the units of Ericsson during a period of 21 weeks (from the beginning of the project until the release of the product).ResultsThe method for forecasting the level of defect backlog uses an indicator of the trend (an arrow) as a basis to forecast the level of defect backlog. Forecasts are based on moving average which combined with the current level of defect backlog was found to be the best prediction method (Mean Magnitude of Relative Error of 16%) for the level of future defect backlog.ConclusionWe have found that ease-of-use and accuracy are the main aspects for practitioners who use predictions in their work. In this paper it is concluded that using the simple moving average provides a sufficiently-good accuracy (much appreciated by practitioners involved in the study). We also conclude that using the indicator (forecasting the trend) instead of the absolute number of defects in the backlog increases the confidence in our method compared to our previous attempts (regression, analogy-based, and expert estimates). 
52|10||Improving device-aware Web services and their mobile clients through an aspect-oriented, model-driven approach|ContextMobile devices have become an essential element in our daily lives, even for connecting to the Internet. Consequently, Web services have become extremely important when offering services through the Internet. However, current Web services are very inflexible as regards their invocation from different types of device, especially if we consider the need for them to be adaptable when being invoked from mobile devices.ObjectiveIn this paper, we provide an approach for the creation of flexible Web services which can be invoked transparently from different device types and which return subsequent responses, as well as providing the client’s adaptation as a result of the particular device characteristics and end-user preferences in a completely decoupled way.MethodAspect-Oriented Programming and model-driven development have been used to reduce both the impact of service and client code adaptation for multiple devices as well as to facilitate the developer’s task.ResultsA model-driven methodology can be followed from system models to code, providing the Web service developer with the option of marking which services should be adapted to mobile devices in the UML models, and obtaining the decoupled adaptation code automatically from the models.ConclusionWe can conclude that the approach presented in this paper provides us with the possibility of following the development of mobile-aware Web services in an integrated platform, benefiting from the use of aspect-oriented techniques not only for maintaining device-related code completely decoupled from the main functionality one, but also allowing a modularized non-intrusive adaptation of mobile clients to the specific device characteristics as well as to final user preferences. 
52|10||Security requirements engineering framework for software product lines|ContextThe correct analysis and understanding of security requirements are important because they assist in the discovery of any security or requirement defects or mistakes during the early stages of development. Security requirements engineering is therefore both a central task and a critical success factor in product line development owing to the complexity and extensive nature of software product lines (SPL). However, most of the current SPL practices in requirements engineering do not adequately address security requirements engineering.ObjectiveThe aim of this approach is to describe a holistic security requirements engineering framework with which to facilitate the development of secure SPLs and their derived products. It will conform with the most relevant security standards with regard to the management of security requirements, such as ISO/IEC 27001 and ISO/IEC 15408.ResultsThis framework is composed of: a security requirements engineering process for SPL (SREPPLine) driven by security standards; a Security Reference Meta Model to manage the variability of those SPL artefacts related to security requirements; and a tool (SREPPLineTool) which implements the meta-model and supports the process.MethodA complete explanation of the framework will be provided. The process will be formally specified with SPEM 2.0 and the repository will be formally specified with an XML grammar. The application of SREPPLine and SREPPLineTool will be illustrated through a description of a simple example as a preliminary validation.ConclusionAlthough there have been several attempts to fill the gap between requirements engineering and SPL requirements engineering, no systematic approach with which to define security quality requirements and to manage their variability and their related security artefacts in SPL models is, as yet, available. The contribution of this work is that of providing a systematic approach for the management of the security requirements and their variability from the early stages of product line development in order to facilitate the conformance of SPL products with the most relevant security standards. 
52|10||Introducing knowledge redundancy practice in software development: Experiences with job rotation in support work|ContextJob rotation is a widely known approach to increase knowledge redundancy but empirical evidence regarding introduction and adoption in software development is scant. A lack of knowledge redundancy is a limiting factor for collaboration, flexibility, and coordination within teams and within the organization.ObjectiveThe scientific objective of this investigation was to explore benefits and challenges with improving knowledge redundancy among developers participating in job rotation. There were two practical objectives; (a) to establish customer support as a legitimate organizational function that would shield developers from support enquiries, and (b) to contribute to improved flexibility in project staffing by enabling overlapping product experience among developers.MethodWe used action research to integrate organizational change with scientific inquiry. During a period of eighteen weeks, nine developers rotated to customer support. We collected data throughout the period of collaboration; in meetings, from comprehensive interviews, and from customer support work logs.ResultsPerceptions of reduced efficiency and unnecessary redundancy outweighed benefits of shielding and learning about different products. Although there were strong indications of increased knowledge redundancy, the benefits were not sufficient to justify job rotation. Job rotation was abandoned after the trial period.ConclusionsJob rotation can contribute to improved knowledge redundancy. Benefits of knowledge redundancy include innovation stemming from integration of different knowledge domains and improved appreciation of organizational concerns. However, knowledge redundancy incurs a collective cost that must be amortized and legitimized by the organization. An adoption process that accommodates open and trustful discussion among all involved stakeholders is therefore encouraged. 
52|11|http://www.sciencedirect.com/science/journal/09505849/52/11|Adoption of open source software in software-intensive organizations â A systematic literature review|ContextOpen source software (OSS) is changing the way organizations develop, acquire, use, and commercialize software.ObjectiveThis paper seeks to identify how organizations adopt OSS, classify the literature according to these ways of adopting OSS, and with a focus on software development evaluate the research on adoption of OSS in organizations.MethodBased on the systematic literature review method we reviewed publications from 24 journals and seven conference and workshop proceedings, published between 1998 and 2008. From a population of 24,289 papers, we identified 112 papers that provide empirical evidence on how organizations actually adopt OSS.ResultsWe show that adopting OSS involves more than simply using OSS products. We moreover provide a classification framework consisting of six distinctly different ways in which organizations adopt OSS. This framework is used to illustrate some of the opportunities and challenges organizations meet when approaching OSS, to show that OSS can be adopted successfully in different ways, and to organize and review existing research. We find that existing research on OSS adoption does not sufficiently describe the context of the organizations studied, and it fails to benefit fully from related research fields. While existing research covers a large number of topics, it contains very few closely related studies. To aid this situation, we offer directions for future research.ConclusionThe implications of our findings are twofold. On the one hand, practitioners should embrace the many opportunities OSS offers, but consciously evaluate the consequences of adopting it in their own context. They may use our framework and the success stories provided by the literature in their own evaluations. On the other hand, researchers should align their work, and perform more empirical research on topics that are important to organizations. Our framework may be used to position this research and to describe the context of the organization they are studying. 
52|11||GA-based method for feature selection and parameters optimization for machine learning regression applied to software effort estimation|ContextIn software industry, project managers usually rely on their previous experience to estimate the number men/hours required for each software project. The accuracy of such estimates is a key factor for the efficient application of human resources. Machine learning techniques such as radial basis function (RBF) neural networks, multi-layer perceptron (MLP) neural networks, support vector regression (SVR), bagging predictors and regression-based trees have recently been applied for estimating software development effort. Some works have demonstrated that the level of accuracy in software effort estimates strongly depends on the values of the parameters of these methods. In addition, it has been shown that the selection of the input features may also have an important influence on estimation accuracy.ObjectiveThis paper proposes and investigates the use of a genetic algorithm method for simultaneously (1) select an optimal input feature subset and (2) optimize the parameters of machine learning methods, aiming at a higher accuracy level for the software effort estimates.MethodSimulations are carried out using six benchmark data sets of software projects, namely, Desharnais, NASA, COCOMO, Albrecht, Kemerer and Koten and Gray. The results are compared to those obtained by methods proposed in the literature using neural networks, support vector machines, multiple additive regression trees, bagging, and Bayesian statistical models.ResultsIn all data sets, the simulations have shown that the proposed GA-based method was able to improve the performance of the machine learning methods. The simulations have also demonstrated that the proposed method outperforms some recent methods reported in the recent literature for software effort estimation. Furthermore, the use of GA for feature selection considerably reduced the number of input features for five of the data sets used in our analysis.ConclusionsThe combination of input features selection and parameters optimization of machine learning methods improves the accuracy of software development effort. In addition, this reduces model complexity, which may help understanding the relevance of each input feature. Therefore, some input parameters can be ignored without loss of accuracy in the estimations. 
52|11||Generating a catalog of unanticipated schemas in class hierarchies using Formal Concept Analysis|ContextInheritance is the cornerstone of object-oriented development, supporting conceptual modeling, subtype polymorphism and software reuse. But inheritance can be used in subtle ways that make complex systems hard to understand and extend, due to the presence of implicit dependencies in the inheritance hierarchy.ObjectiveAlthough these dependencies often specify well-known schemas (i.e., recurrent design or coding patterns, such as hook and template methods), new unanticipated dependency schemas arise in practice, and can consequently be hard to recognize and detect. Thus, a developer making changes or extensions to an object-oriented system needs to understand these implicit contracts defined by the dependencies between a class and its subclasses, or risk that seemingly innocuous changes break them.MethodTo tackle this problem, we have developed an approach based on Formal Concept Analysis. Our Formal Concept Analysis based-Reverse Engineering methodology (FoCARE) identifies undocumented hierarchical dependencies in a hierarchy by taking into account the existing structure and behavior of classes and subclasses.ResultsWe validate our approach by applying it to a large and non-trivial case study, yielding a catalog of hierarchy schemas, each one composed of a set of dependencies over methods and attributes in a class hierarchy. We show how the discovered dependency schemas can be used not only to identify good design practices, but also to expose bad smells in design, thereby helping developers in initial reengineering phases to develop a first mental model of a system. Although some of the identified schemas are already documented in existing literature, with our approach based on Formal Concept Analysis (FCA), we are also able to identify previously unidentified schemas.ConclusionsFCA is an effective tool because it is an ideal classification mining tool to identify commonalities between software artifacts, and usually these commonalities reveal known and unknown characteristics of the software artifacts. We also show that once a catalog of useful schemas stabilizes after several runs of FoCARE, the added cost of FCA is no longer needed. 
52|11||A family of experiments to validate measures for UML activity diagrams of ETL processes in data warehouses|In data warehousing, Extract, Transform, and Load (ETL) processes are in charge of extracting the data from the data sources that will be contained in the data warehouse. Their design and maintenance is thus a cornerstone in any data warehouse development project. Due to their relevance, the quality of these processes should be formally assessed early in the development in order to avoid populating the data warehouse with incorrect data. To this end, this paper presents a set of measures with which to evaluate the structural complexity of ETL process models at the conceptual level. This study is, moreover, accompanied by the application of formal frameworks and a family of experiments whose aim is to theoretical and empirically validate the proposed measures, respectively. Our experiments show that the use of these measures can aid designers to predict the effort associated with the maintenance tasks of ETL processes and to make ETL process models more usable. Our work is based on Unified Modeling Language (UML) activity diagrams for modeling ETL processes, and on the Framework for the Modeling and Evaluation of Software Processes (FMESP) framework for the definition and validation of the measures. 
52|11||Software Process Improvement barriers: A cross-cultural comparison|ContextSoftware Process Improvement initiatives have been around for many years with the growing globalisation of software development is making them increasingly important.ObjectiveThe objective of this exploratory research is to gain an in-depth understanding of barriers that can undermine SPI, in the context of Global Software Development, from the perspective of software development practitioners; this will enable SPI managers to better manage SPI initiatives. We intend to discover if the barriers to SPI initiatives in a developed country are different to those in a developing country.MethodIn an empirical study, Vietnamese software practitioners’ experiences of SPI barriers are compared with barriers identified by Australian practitioners. Face-to-face questionnaire-based survey sessions with 23 Vietnamese SPI practitioners were conducted. Our survey included barriers to SPI improvement initiatives identified in previous research. We asked the participants to rank each SPI barrier on a three-point scale (high, medium, low) to determine the importance of each barrier. We then compare our results, with results (identified in previous work), from 34 Australian software development practitioners.ResultsWe identify (1) lack of project management, (2) lack of resources, (3) lack of sponsorship, (4) inexperienced staff/lack of knowledge, and (5) lack of SPI awareness as ‘high’ value SPI barriers in Vietnam. The results also reveal similarities and differences between the experiences of Australian and Vietnamese practitioners regarding the importance of the SPI barriers identified. While the Australian practitioners were also concerned with (1) lack of SPI awareness, they were even more concerned with (2) organisational politics, and (3) lack of support.ConclusionsPractitioners identify SPI barriers based on previous SPI implementation experience. Their role(s) in their different organisations have helped them to understand the importance of that barrier. Vietnamese software practitioners cited more SPI barriers than their counterparts in Australia. The Vietnamese SPI barriers relate to project management, resources, and sponsorship while the Australian barriers are concerned with organisational politics and lack of support. 
52|11||Representing the behaviour of software projects using multi-dimensional timelines|ContextThere are few empirical studies in the empirical software engineering research community that describe software projects, at the level of the project, as they progress over time.ObjectiveTo investigate how to coherently represent a large volume of qualitative and quantitative data on a range of project-level attributes as those attributes change over time.MethodDevelop a modelling technique, multi-dimensional timelines (MDTs) and undertake a preliminary appraisal of the technique using examples from a longitudinal case study of a project at IBM Hursley Park.ResultsMDTs can represent project-level attributes as they change over time, provided these attributes, and the empirical data about them, can be located in time (an analytical requirement) and can be represented in terms of the simple geometrical structures of points, lines and planes (a graphical requirement). Changes in attributes are documented at the point in time at which the change occurs. There are a number of ways in which an attribute can be represented on the MDT: as a quantitative time series, as an event, as an event with label containing complex qualitative information, or as a schedule. The MDT technique is currently not capable of representing relationships between different attributes e.g. a causal relationship.ConclusionThe initial appraisal of MDTs is encouraging, but further work is needed on the development of the MDT technique and on its evaluation. 
52|11||Special Section on Best Papers PROMISE 2009|
52|11||A Bayesian network approach to assess and predict software quality using activity-based quality models|ContextSoftware quality is a complex concept. Therefore, assessing and predicting it is still challenging in practice as well as in research. Activity-based quality models break down this complex concept into concrete definitions, more precisely facts about the system, process, and environment as well as their impact on activities performed on and with the system. However, these models lack an operationalisation that would allow them to be used in assessment and prediction of quality. Bayesian networks have been shown to be a viable means for this task incorporating variables with uncertainty.ObjectiveThe qualitative knowledge contained in activity-based quality models are an abundant basis for building Bayesian networks for quality assessment. This paper describes a four-step approach for deriving systematically a Bayesian network from an assessment goal and a quality model.MethodThe four steps of the approach are explained in detail and with running examples. Furthermore, an initial evaluation is performed, in which data from NASA projects and an open source system is obtained. The approach is applied to this data and its applicability is analysed.ResultsThe approach is applicable to the data from the NASA projects and the open source system. However, the predictive results vary depending on the availability and quality of the data, especially the underlying general distributions.ConclusionThe approach is viable in a realistic context but needs further investigation in case studies in order to analyse its predictive validity. 
52|11||Practical considerations in deploying statistical methods for defect prediction: A case study within the Turkish telecommunications industry|ContextBuilding defect prediction models in large organizations has many challenges due to limited resources and tight schedules in the software development lifecycle. It is not easy to collect data, utilize any type of algorithm and build a permanent model at once. We have conducted a study in a large telecommunications company in Turkey to employ a software measurement program and to predict pre-release defects. Based on our prior publication, we have shared our experience in terms of the project steps (i.e. challenges and opportunities). We have further introduced new techniques that improve our earlier results.ObjectiveIn our previous work, we have built similar predictors using data representative for US software development. Our task here was to check if those predictors were specific solely to US organizations or to a broader class of software.MethodWe have presented our approach and results in the form of an experience report. Specifically, we have made use of different techniques for improving the information content of the software data and the performance of a Naïve Bayes classifier in the prediction model that is locally tuned for the company. We have increased the information content of the software data by using module dependency data and improved the performance by adjusting the hyper-parameter (decision threshold) of the Naïve Bayes classifier. We have reported and discussed our results in terms of defect detection rates and false alarms. We also carried out a cost–benefit analysis to show that our approach can be efficiently put into practice.ResultsOur general result is that general defect predictors, which exist across a wide range of software (in both US and Turkish organizations), are present. Our specific results indicate that concerning the organization subject to this study, the use of version history information along with code metrics decreased false alarms by 22%, the use of dependencies between modules further reduced false alarms by 8%, and the decision threshold optimization for the Naïve Bayes classifier using code metrics and version history information further improved false alarms by 30% in comparison to a prediction using only code metrics and a default decision threshold.ConclusionImplementing statistical techniques and machine learning on a real life scenario is a difficult yet possible task. Using simple statistical and algorithmic techniques produces an average detection rate of 88%. Although using dependency data improves our results, it is difficult to collect and analyze such data in general. Therefore, we would recommend optimizing the hyper-parameter of the proposed technique, Naïve Bayes, to calibrate the defect prediction model rather than employing more complex classifiers. We also recommend that researchers who explore statistical and algorithmic methods for defect prediction should spend less time on their algorithms and more time on studying the pragmatic considerations of large organizations. 
52|12|http://www.sciencedirect.com/science/journal/09505849/52/12|Intelligent distributed control systems|ContextThe paper2 deals with distributed reconfigurable embedded control systems following the component-based International Industrial Standard IEC61499 in which a Function Block (abbreviated by FB) is an event-triggered software component owning data and a control application is a distributed network of Function Blocks. Nowadays, limited related works have been proposed to address particular cases of reconfigurations without considering distributed architectures. Our first problem is to be able to handle all possible forms of reconfigurations that can be applied at run-time to distributed Function Blocks. In this case, a coordination between devices of the execution environment should be applied to guarantee safe and coherent distributed reconfigurations. A second problem is to find the sufficient solutions for the correct implementation of this reconfigurable distributed architecture.ObjectiveThe paper defines an implementable multi-agent architecture for automatic and coherent reconfigurations of distributed Function Blocks.MethodTo address all possible industrial forms, we classify the reconfiguration scenarios into three levels. The first level deals with additions–removals of Function Blocks to-from the system’s implementation. The second deals with updates of compositions of blocks, and the third deals with updates of data. We define a Reconfiguration Agent for each device of the execution environment, and a unique Coordination Agent for coordinations between devices. Each Reconfiguration Agent to be modelled by nested state machines applies local reconfiguration scenarios in the corresponding device after coordinations with the Coordination Agent. We propose an Inter-Agents Communication Protocol to support correct and coherent reconfigurations of distributed devices. This protocol is based on Coordination Matrices to be handled by the Coordination Agent in order to define all reconfiguration scenarios that should be simultaneously applied in distributed devices. We propose XML-based implementations for both kinds of agents where XML code blocks are exchanged between devices to guarantee safety distributed reconfigurations. The contributions of the paper are applied to two Benchmark Production Systems available in our research laboratory.ResultsThe communication protocol is successfully applied to our platforms where simulations are executed to check distributed and coherent reconfiguration scenarios. The Reconfiguration and Coordination Agents are implemented in this platform by following the International Standard IEC61499. We show in addition XML-based successful interactions between devices when distributed reconfigurations are applied.ConclusionThe paper successfully defines a multi-agent architecture for IEC61499 distributed reconfigurable embedded systems where Coordination and Reconfiguration agents are proposed to allow feasible and coherent distributed reconfigurations by using a defined communication protocol. This architecture is implemented in XML and applied to real industrial platforms. 
52|12||WISDOM: A website design method based on reusing design and software solutions|ContextWebsites are increasingly important for advertising and announcing information and they have become virtual business operations support tools. Thus, website designers have to take an increasing number of criteria (cost, delay, quality, security, maintenance) into account during design process to satisfy the needs.ObjectiveThe objective of this paper was to present our WISDOM method that: guides the designer through the website design process, proposes design solutions based on already existing solutions for online website design, facilitates the choice of software components to implement specific services, and speeds up website construction.MethodThe originality of our method is that it links the design process to formalized experience and a software component characterization that allows both functional and non-functional aspects to be considered.ResultsThis method relies on the state-of-the-art strengths in the website design process, modeling dimensions, Model-Driven Engineering and the patterns approach. We propose an implementation of our method as a dedicated website which helps website design and provides a website analysis catalog and a software component analysis catalog.ConclusionOur analysis of the method’s use highlights that formalizing the steps of the design process helps designers, especially novice designers, to design a website; our analysis of the tool’s use highlights its efficiency for rapid website development and its use of the “website family” concept. The results are so very encouraging for both method and tool; both facilitate website design by reusing existing solutions and components. 
52|12||FSM-based conformance testing methods: A survey annotated with experimental evaluation|The development of test cases is an important issue for testing software, communication protocols and other reactive systems. A number of methods are known for the development of a test suite based on a formal specification given in the form of a finite state machine. In this paper, we overview and experiment with these methods to assess their complexity, applicability, completeness, fault detection capability, length and derivation time of their test suites. The experiments are conducted on randomly generated specifications and on two realistic protocols called the Simple Connection Protocol and the ITU-T V.76 Recommendation. 
52|12||A novel composite model approach to improve software quality prediction|Context:How can quality of software systems be predicted before deployment? In attempting to answer this question, prediction models are advocated in several studies. The performance of such models drops dramatically, with very low accuracy, when they are used in new software development environments or in new circumstances.ObjectiveThe main objective of this work is to circumvent the model generalizability problem. We propose a new approach that substitutes traditional ways of building prediction models which use historical data and machine learning techniques.MethodIn this paper, existing models are decision trees built to predict module fault-proneness within the NASA Critical Mission Software. A genetic algorithm is developed to combine and adapt expertise extracted from existing models in order to derive a “composite” model that performs accurately in a given context of software development. Experimental evaluation of the approach is carried out in three different software development circumstances.ResultsThe results show that derived prediction models work more accurately not only for a particular state of a software organization but also for evolving and modified ones.ConclusionOur approach is considered suitable for software data nature and at the same time superior to model selection and data combination approaches. It is then concluded that learning from existing software models (i.e., software expertise) has two immediate advantages; circumventing model generalizability and alleviating the lack of data in software-engineering. 
52|12||Package Fingerprints: A visual summary of package interface usage|ContextObject-oriented languages such as Java, Smalltalk, and C++ structure their programs using packages. Maintainers of large systems need to understand how packages relate to each other, but this task is complex because packages often have multiple clients and play different roles (class container, code ownership, etc.). Several approaches have been proposed, among which the use of cohesion and coupling metrics. Such metrics help identify candidate packages for restructuring; however, they do not help maintainers actually understand the structure and interrelationships between packages.ObjectivesIn this paper, we use pre-attentive processing as the basis for package visualization and see to what extent it could be used in package understanding.MethodWe present the Package Fingerprint, a 2D visualization of the references made to and from a package. The proposed visualization offers a semantically rich, but compact and zoomable views centered on packages. We focus on two views (incoming and outgoing references) that help users understand how the package under analysis is used by the system and how it uses the system.ResultsWe applied these views on four large systems: Squeak, JBoss, Azureus, and ArgoUML. We obtained several interesting results, among which, the identification of a set of recurring visual patterns that help maintainers: (a) more easily identify the role of and the way a package is used within the system (e.g., the package under analysis provides a set of layered services), and (b) detect either problematic situations (e.g., a single package that groups together a large number of basic services) or opportunities for better package restructuring (e.g., removing cyclic dependencies among packages). The visualization generally scaled well and the detection of different patterns was always possible.ConclusionThe proposed visualizations and patterns proved to be useful in understanding and maintaining the different systems we addressed. To generalize to other contexts and systems, a real user study is required. 
52|12||Quantification of interacting runtime qualities in software architectures: Insights from transaction processing in clientâserver architectures|ContextArchitecture is fundamental for fulfilling requirements related to the non-functional behavior of a software system such as the quality requirement that response time does not degrade to a point where it is noticeable. Approaches like the Architecture Tradeoff Analysis Method (ATAM) combine qualitative analysis heuristics (e.g. scenarios) for one or more quality metrics with quantitative analyses. A quantitative analysis evaluates a single metric such as response time. However, since quality metrics interact with each other, a change in the architecture can affect unpredictably multiple quality metrics.ObjectiveThis paper introduces a quantitative method that determines the impact of a design change on multiple metrics, thus reducing the risks in architecture design. As a proof of concept, the method is applied on a simulation model of transaction processing in client server architecture.MethodFactor analysis is used to unveil latent (i.e. not directly measurable) quality features represented by new variables that reflect architecture-specific correlations between metrics. Separate Analyses of Variance (ANOVA) are then applied to these variables, for interpreting the tradeoffs detected by factor analysis in terms of the quantified metrics.ResultsThe results for the examined transaction processing architecture show three latent quality features, the corresponding groups of strongly correlated quality metrics and the impact of architecture characteristics on the latent quality features.ConclusionThe proposed method is a systematic way for relating the variability of quality metrics and the implied tradeoffs to specific architecture characteristics. 
52|12||An object-oriented high-level design-based class cohesion metric|ContextClass cohesion is an important object-oriented software quality attribute. Assessing class cohesion during the object-oriented design phase is one important way to obtain more comprehensible and maintainable software. In practice, assessing and controlling cohesion in large systems implies measuring it automatically. One issue with the few existing cohesion metrics targeted at the high-level design phase is that they are not based on realistic assumptions and do not fulfill expected mathematical properties.ObjectiveThis paper proposes a High-Level Design (HLD) class cohesion metric, which is based on realistic assumptions, complies with expected mathematical properties, and can be used to automatically assess design quality at early stages using UML diagrams.MethodThe notion of similarity between pairs of methods and pairs of attribute types in a class is introduced and used as a basis to introduce a novel high-level design class cohesion metric. The metric considers method–method, attribute–attribute, and attribute–method direct and transitive interactions. We validate this Similarity-based Class Cohesion (SCC) metric theoretically and empirically. The former includes a careful study of the mathematical properties of the metric whereas the latter investigates, using four open source software systems and 10 cohesion metrics, whether SCC is based on realistic assumptions and whether it better explains the presence of faults, from a statistical standpoint, than other comparable cohesion metrics, considered individually or in combination.ResultsResults confirm that SCC is based on clearly justified theoretical principles, relies on realistic assumptions, and is an early indicator of quality (fault occurrences).ConclusionIt is concluded that SCC is both theoretically valid and supported by empirical evidence. It is a better alternative to measure class cohesion than existing HLD class cohesion metrics. 
52|2|http://www.sciencedirect.com/science/journal/09505849/52/2|Seven process modeling guidelines (7PMG)|Business process modeling is heavily applied in practice, but important quality issues have not been addressed thoroughly by research. A notorious problem is the low level of modeling competence that many casual modelers in process documentation projects have. Existing approaches towards model quality might be of benefit, but they suffer from at least one of the following problems. On the one hand, frameworks like SEQUAL and the Guidelines of Modeling are too abstract to be applicable for novices and non-experts in practice. On the other hand, there are collections of pragmatic hints that lack a sound research foundation. In this paper, we analyze existing research on relationships between model structure on the one hand and error probability and understanding on the other hand. As a synthesis we propose a set of seven process modeling guidelines (7PMG). Each of these guidelines builds on strong empirical insights, yet they are formulated to be intuitive to practitioners. Furthermore, we analyze how the guidelines are prioritized by industry experts. In this regard, the seven guidelines have the potential to serve as an important tool of knowledge transfer from academia into modeling practice. 
52|2||A pattern-based outlier detection method identifying abnormal attributes in software project data|Despite the importance of the quality of software project data, problematic data inevitably occurs during data collection. These data are the outliers with abnormal values on certain attributes, which we call the abnormal attributes of outliers. Manually detecting outliers and their abnormal attributes is laborious and time consuming. Although few existing approaches identify outliers and their abnormal attributes, these approaches are not effective in (1) identifying the abnormal attributes when the outlier has abnormal values on more than the specific number of its attributes or (2) discovering accurate rules to detect outliers and their abnormal attributes. In this paper, we propose a pattern-based outlier detection method that identifies abnormal attributes in software project data: after discovering the reliable frequent patterns that reflect the typical characteristics of the software project data, outliers and their abnormal attributes are detected by matching the software project data with those patterns. Empirical studies were performed on three industrial data sets and 48 artificial data sets with injected outliers. The results demonstrate that our approach outperforms five other approaches by an average of 35.27% and 107.5% in detecting the outliers and abnormal attributes, respectively, on the industrial data sets, and an average of 35.44% and 46.57%, respectively on the artificial data sets. 
52|2||Identification of design motifs with pattern matching algorithms|Design patterns are important in software maintenance because they help in understanding and re-engineering systems. They propose design motifs, solutions to recurring design problems. The identification of occurrences of design motifs in large systems consists of identifying classes whose structure and organization match exactly or approximately the structure and organization of classes as suggested by the motif. We adapt two classical approximate string matching algorithms based on automata simulation and bit-vector processing to efficiently identify exact and approximate occurrences of motifs. We then carry out two case studies to show the performance, precision, and recall of our algorithms. In the first case study, we assess the performance of our algorithms on seven medium-to-large systems. In the second case study, we compare our approach with three existing approaches (an explanation-based constraint approach, a metric-enhanced explanation-based constraint approach, and a similarity scoring approach) by applying the algorithms on three small-to-medium size systems, JHotDraw, Juzzle, and QuickUML. Our studies show that approximate string matching based on bit-vector processing provides efficient algorithms to identify design motifs. 
52|2||The impact of Test-First programming on branch coverage and mutation score indicator of unit tests: An experiment|BackgroundTest-First programming is regarded as one of the software development practices that can make unit tests to be more rigorous, thorough and effective in fault detection. Code coverage measures can be useful as indicators of the thoroughness of unit test suites, while mutation testing turned out to be effective at finding faults.ObjectiveThis paper presents an experiment in which Test-First vs. Test-Last programming practices are examined with regard to branch coverage and mutation score indicator of unit tests.MethodStudent subjects were randomly assigned to Test-First and Test-Last groups. In order to further reduce pre-existing differences among subjects, and to get a more sensitive measure of our experimental effect, multivariate analysis of covariance was performed.ResultsMultivariate tests results indicate that there is no statistically significant difference between Test-First and Test-Last practices on the combined dependent variables, i.e. branch coverage and mutation score indicator, (F(2,9)=.52F(2,9)=.52, p>.05p>.05), even if we control for the pre-test results, the subjects’ experience, and when the subjects who showed deviations from the assigned programming technique are excluded from the analysis.ConclusionAccording to the preliminary results presented in this paper, the benefits of the Test-First practice in this specific context can be considered minor.LimitationIt is probably the first-ever experimental evaluation of the impact of Test-First programming on mutation score indicator of unit tests and further experimentation is needed to establish evidence. 
52|2||An embeddable mobile agent platform supporting runtime code mobility, interaction and coordination of mobile agents and host systems|Agent technology is emerging as an important concept for the development of distributed complex systems. A number of mobile agent systems have been developed in the last decade. However, most of them were developed to support only Java mobile agents. In order to provide distributed applications with code mobility, this article presents a library, the Mobile-C library, that allows a mobile agent platform, Mobile-C, to be embeddable in an application to support mobile C/C++ codes carried by mobile agents. Mobile-C uses a C/C++ interpreter as its Agent Execution Engine (AEE). Through the Mobile-C library, Mobile-C can be embedded into an application to support mobile C/C++ codes carried by mobile agents. Using mobile C/C++ codes, it is easy to interface a variety of low-level hardware devices and legacy systems. Through the Mobile-C library, Mobile-C can run on heterogeneous platforms with various operating systems. The Mobile-C library has a small footprint to meet the stringent memory capacity for applications in mechatronic and embedded systems. The Mobile-C library contains different categories of Application Programming Interfaces (APIs) in both binary and agent spaces to facilitate the design of mobile agent based applications. In addition, a rich set of existing APIs for the C/C++ interpreter employed as the AEE allows an application to have complete information and control over the mobile C/C++ codes residing in Mobile-C. With the synchronization mechanism provided by the Mobile-C library for both binary and agent spaces, simultaneous processes across both spaces can be coordinated to get correct runtime order and avoid unexpected race condition. The study of performance comparisons indicates that Mobile-C is about two times faster than JADE in agent migration. The application of the Mobile-C library is illustrated by dynamic runtime control of a mobile robot’s behavior using mobile agents. 
52|2||An effort prediction framework for software defect correction|This article tackles the problem of predicting effort (in person–hours) required to fix a software defect posted on an Issue Tracking System. The proposed method is inspired by the Nearest Neighbour Approach presented by the pioneering work of Weiss et al. (2007)  [1]. We propose four enhancements to Weiss et al. (2007)  [1]: Data Enrichment, Majority Voting, Adaptive Threshold and Binary Clustering. Data Enrichment infuses additional issue information into the similarity-scoring procedure, aiming to increase the accuracy of similarity scores. Majority Voting exploits the fact that many of the similar historical issues have repeating effort values, which are close to the actual. Adaptive Threshold automatically adjusts the similarity threshold to ensure that we obtain only the most similar matches. We use Binary Clustering if the similarity scores are very low, which might result in misleading predictions. This uses common properties of issues to form clusters (independent of the similarity scores) which are then used to produce the predictions. Numerical results are presented showing a noticeable improvement over the method proposed in Weiss et al. (2007)  [1]. 
52|2||Filtering false alarms of buffer overflow analysis using SMT solvers|Buffer overflow detection using static analysis can provide a powerful tool for software programmers to find difficult bugs in C programs. Sound static analysis based on abstract interpretation, however, often suffers from false alarm problem. Although more precise abstraction can reduce the number of the false alarms in general, the cost to perform such analysis is often too high to be practical for large software. On the other hand, less precise abstraction is likely to be scalable in exchange for the increased false alarms. In order to attain both precision and scalability, we present a method that first applies less precise abstraction to find buffer overflow alarms fast, and selectively applies a more precise analysis only to the limited areas of code around the potential false alarms. In an attempt to develop the precise analysis of alarm filtering for large C programs, we perform a symbolic execution over the potential alarms found in the previous analysis, which is based on the abstract interpretation. Taking advantage of a state-of-art SMT solver, our precise analysis efficiently filters out a substantial number of false alarms. Our experiment with the test cases from three open source programs shows that our filtering method can reduce about 68% of false alarms on average. 
52|2||Intelligent security and access control framework for service-oriented architecture|One of the most significant difficulties with developing Service-Oriented Architecture (SOA) involves meeting its security challenges, since the responsibilities of SOA security are based on both the service providers and the consumers. In recent years, many solutions to these challenges have been implemented, such as the Web Services Security Standards, including WS-Security and WS-Policy. However, those standards are insufficient for the new generation of Web technologies, including Web 2.0 applications. In this research, we propose an intelligent SOA security framework by introducing its two most promising services: the Authentication and Security Service (NSS), and the Authorization Service (AS). The suggested autonomic and reusable services are constructed as an extension of WS-∗ security standards, with the addition of intelligent mining techniques, in order to improve performance and effectiveness. In this research, we apply three different mining techniques: the Association Rules, which helps to predict attacks, the Online Analytical Processing (OLAP) Cube, for authorization, and clustering mining algorithms, which facilitate access control rights representation and automation. Furthermore, a case study is explored to depict the behavior of the proposed services inside an SOA business environment. We believe that this work is a significant step towards achieving dynamic SOA security that automatically controls the access to new versions of Web applications, including analyzing and dropping suspicious SOAP messages and automatically managing authorization roles. 
52|3|http://www.sciencedirect.com/science/journal/09505849/52/3|A systematic review on strategic release planning models|ContextStrategic release planning (sometimes referred to as road-mapping) is an important phase of the requirements engineering process performed at product level. It is concerned with selection and assignment of requirements in sequences of releases such that important technical and resource constraints are fulfilled.ObjectivesIn this study we investigate which strategic release planning models have been proposed, their degree of empirical validation, their factors for requirements selection, and whether they are intended for a bespoke or market-driven requirements engineering context.MethodsIn this systematic review a number of article sources are used, including Compendex, Inspec, IEEE Xplore, ACM Digital Library, and Springer Link. Studies are selected after reading titles and abstracts to decide whether the articles are peer reviewed, and relevant to the subject.ResultsTwenty four strategic release planning models are found and mapped in relation to each other, and a taxonomy of requirements selection factors is constructed.ConclusionsWe conclude that many models are related to each other and use similar techniques to address the release planning problem. We also conclude that several requirement selection factors are covered in the different models, but that many methods fail to address factors such as stakeholder value or internal value. Moreover, we conclude that there is a need for further empirical validation of the models in full scale industry trials. 
52|3||Model-driven development for early aspects|Currently, non-functional requirements (NFRs) consume a considerable part of the software development effort. The good news is that most of them appear time and again during system development and, luckily, their solutions can be often described as a pattern independently from any specific application or domain. A proof of this are the current application servers and middleware platforms that can provide configurable prebuilt services for managing some of these crosscutting concerns, or aspects. Nevertheless, these reusable pattern solutions presents two shortcomings, among others: (1) they need to be applied manually; and (2) most of these pattern solutions do not use aspect-orientation, and, since NFRs are often crosscutting concerns, this leads to scattered and tangled representations of these concerns. Our approach aims to overcome these limitations by: (1) using model-driven techniques to reduce the development effort associated to systematically apply reusable solutions for satisfying NFRs; and (2) using aspect-orientation to improve the modularization of these crosscutting concerns. Regarding the first contribution, since the portion of a system related to NFRs is usually significant, the reduction on the development effort associated to these NFRs is also significant. Regarding the second contribution, the use aspect-orientation improves maintenance and evolution of the non-functional requirements that are managed as aspects. An additional contribution of our work is to define a mapping and transition from aspectual requirements to aspect-oriented software architectures, which, in turn, contributes to improve the general issue of systematically relating requirements to architecture. Our approach is illustrated by applying it to a Toll Gate case study. 
52|3||Automated verification of security pattern compositions|Software security becomes a critically important issue for software development when more and more malicious attacks explore the security holes in software systems. To avoid security problems, a large software system design may reuse good security solutions by applying security patterns. Security patterns document expert solutions to common security problems and capture best practices on secure software design and development. Although each security pattern describes a good design guideline, the compositions of these security patterns may be inconsistent and encounter problems and flaws. Therefore, the compositions of security patterns may be even insecure. In this paper, we present an approach to automated verification of the compositions of security patterns by model checking. We formally define the behavioral aspect of security patterns in CCS through their sequence diagrams. We also prove the faithfulness of the transformation from a sequence diagram to its CCS representation. In this way, the properties of the security patterns can be checked by a model checker when they are composed. Composition errors and problems can be discovered early in the design stage. We also use two case studies to illustrate our approach and show its capability to detect composition errors. 
52|3||Analysis of virtual communities supporting OSS projects using social network analysis|This paper analyses the behaviour of virtual communities for Open Source Software (OSS) projects. The development of OSS projects relies on virtual communities, which are built on relationships among members, being their final objective sharing knowledge and improving the underlying project. This study addresses the interactive collaboration in these kinds of communities applying social network analysis (SNA). In particular, SNA techniques will be used to identify those members playing a middle-man role among other community members. Results will illustrate the importance of this role to achieve successful virtual communities. 
52|3||A pattern-based approach to protocol mediation for web services composition|ContextWith the increasing popularity of Service Oriented Architecture (SOA), service composition is gaining momentum as the potential silver bullet for application integration. However, services are not always perfectly compatible and therefore cannot be directly composed. Service mediation, roughly classified into signature and protocol ones, thus becomes one key working area in SOA.ObjectiveAs a challenging problem, protocol mediation is still open and existing approaches only provide partial solutions. Further investigation on a systematic approach is needed.MethodsIn this paper, an approach based on mediator patterns is proposed to generate executable mediators and glue partially compatible services together. The mediation process and its main steps are introduced. By utilizing message mapping, a heuristic technique for identifying protocol mismatches and selecting appropriate mediator patterns is presented. The corresponding BPEL templates of these patterns are also developed.ResultsA prototype system, namely Service Mediation Toolkit (SMT), has been implemented to validate the feasibility and effectiveness of the proposed approach.ConclusionThe approach along with the prototype system facilitate the existing practice of protocol mediation for Web services composition. 
52|3||Requirements for product derivation support: Results from a systematic literature review and an expert survey|ContextAn increasing number of publications in product line engineering address product derivation, i.e., the process of building products from reusable assets. Despite its importance, there is still no consensus regarding the requirements for product derivation support.ObjectiveOur aim is to identify and validate requirements for tool-supported product derivation.MethodWe identify the requirements through a systematic literature review and validate them with an expert survey.ResultsWe discuss the resulting requirements and provide implementation examples from existing product derivation approaches.ConclusionsWe conclude that key requirements are emerging in the research literature and are also considered relevant by experts in the field. 
52|3||An experimental study on the conversion between IFPUG and COSMIC functional size measurement units|The adoption of functional size measurement (FSM) methods in software organizations is growing. In particular, special attention is being paid to the COSMIC method, because of its novelties against 1st generation FSM methods such as IFPUG FPA. One of the main problems facing organizations wanting to use COSMIC is how to properly convert the software functional size of the projects in their portfolio measured by the previously adopted FSM method to the size measured by the new method.The objective of this paper is to find a sound mathematical basis for converting an IFPUG measurement to a COSMIC measurement.In the light of previously published researches, parallel measurements were performed to establish three new datasets (respectively composed by 21, 14 and 35 data points) and verified by an expert measurer, certified on both techniques. In order to obtain a more precise solution, the search for a mathematical relationship has been run using new nonlinear equation types.Results from the analysis confirmed an approximated conversion factor of 1:1, within a range between 0.9 and 1.1, but moving from a larger number of data points analyzed then in past studies.These results can be very useful for those companies starting to use their benchmarking databases populated in IFPUG FP units to projects measured in COSMIC FP. 
52|4|http://www.sciencedirect.com/science/journal/09505849/52/4|Experimental program analysis|Program analysis techniques are used by software engineers to deduce and infer characteristics of software systems. Recent research has suggested that certain program analysis techniques can be formulated as formal experiments. This article reports the results of research exploring this suggestion. Building on principles and methodologies underlying the use of experimentation in other fields, we provide descriptive and operational definitions of experimental program analysis, illustrate them by example, and describe several differences between experimental program analysis and experimentation in other fields. We also explore the applicability of experimental program analysis to three software engineering problems: program transformation, program debugging, and program understanding. Our findings indicate that experimental program analysis techniques can provide new and potentially improved solutions to these problems, and suggest that experimental program analysis offers a promising new direction for program analysis research. 
52|4||AAOP-based dynamically reconfigurable monitoring system|A key aspect of resource management is efficient and effective deployment of available resources whenever needed. The issue typically covers two areas: monitoring of resources used by software systems and managing the consumption of resources. A key aspect of each monitoring system is its reconfigurability – the ability of a system to limit the number of resources monitored at a given time to those that are really necessary at any particular moment. The authors of this article propose a fully dynamic and reconfigurable monitoring system based on the concept of Adaptable Aspect-Oriented Programming (AAOP) in which a set of AOP aspects is used to run an application in a manner specified by the adaptability strategy. The model can be used to implement systems that are able to monitor an application and its execution environment and perform actions such as changing the current set of resource management constraints applied to an application if the application/environment conditions change. Any aspect that implements a predefined interface may be used by the AAOP-based monitoring system as a source of information. The system utilizes the concept of dynamic AOP, meaning that the aspects (which are sources of information) may be dynamically enabled/disabled. 
52|4||A tool for IT process construction|ContextThe field of IT processes lacks a scientifically-based tool that constructs organisation-specific IT processes according to the organisation’s socio-technical characteristics.ObjectiveIn this paper we propose a solution to this problem in the form of IT process engineering (ITPE). ITPE is based on established method engineering principles which we have adapted to IT process construction.MethodThe tool was demonstrated by having three organisations use ITPE to each construct two IT processes.ResultsITPE provided useful guidance in all three cases.ConclusionsThe study demonstrates that method engineering principles can be applied in research fields other than information system development. 
52|4||Current state and potential of variability management practices in software-intensive SMEs: Results from a regional industrial survey|ContextMore and more, small and medium-sized enterprises (SMEs) are using software to augment the functionality of their products and offerings. Variability management of software is becoming an interesting topic for SMEs with expanding portfolios and increasingly complex product structures. While the use of software product lines to resolve high variability is well known in larger organizations, there is less known about the practices in SMEs.ObjectiveThis paper presents results from a survey of software developing SMEs. The purpose of the paper is to provide a snapshot of the current awareness and practices of variability modeling in organizations that are developing software with the constraints present in SMEs.MethodA survey with questions regarding the variability practices was distributed to software developing organizations in a region of Sweden that has many SMEs. The response rate was 13% and 25 responses are used in this analysis.ResultsWe find that, although there are SMEs that develop implicit software product lines and have relatively sophisticated variability structures for their solution space, the structures of the problem space and the product space have room for improvement.ConclusionsThe answers in the survey indicate that SMEs are in situations where they can benefit from more structured variability management, but the awareness need to be raised. Even though the problem space similarity is high, there is little reuse and traceability activities performed. The existence of SMEs with qualified variability management and product line practices indicates that small organizations are capable to apply such practices. 
52|4||A framework for the definition of metamodels for Computer-Aided Software Engineering tools|Computer-Aided Software Engineering (CASE) tools support modeling-related activities in development projects. Given the variety of tools and functionalities, it is quite common to work with several tools in the same project. However, data cannot usually be exchanged between these tools without loss of information. Recent approaches address this model interchange problem using metamodels to characterize the involved information and transformations to export/import it. Nevertheless, most of these solutions focus on the abstract syntax of models. They fail to consider aspects such as the presentation of models or tool-specific information, which are either disregarded or represented in ad-hoc ways that make difficult their processing. In order to overcome these limitations, this paper introduces a framework to define metamodels of CASE tools and a process to carry out the model interchange using them. The proposed metamodels have a modular organization with several internal metamodels. Each of them is aimed at describing some specific information about content, structure and presentation for both models and tools. The paper illustrates this approach with a case study used for comparison with existing works for this problem. 
52|4||Identification of non-functional requirements in textual specifications: A semi-supervised learning approach|ContextEarly detection of non-functional requirements (NFRs) is crucial in the evaluation of architectural alternatives starting from initial design decisions. The application of supervised text categorization strategies for requirements expressed in natural language has been proposed in several works as a method to help analysts in the detection and classification of NFRs concerning different aspects of software. However, a significant number of pre-categorized requirements are needed to train supervised text classifiers, which implies that analysts have to manually assign categories to numerous requirements before being able of accurately classifying the remaining ones.ObjectiveWe propose a semi-supervised text categorization approach for the automatic identification and classification of non-functional requirements. Therefore, a small number of requirements, possibly identified by the requirement team during the elicitation process, enable learning an initial classifier for NFRs, which could successively identify the type of further requirements in an iterative process. The goal of the approach is the integration into a recommender system to assist requirement analysts and software designers in the architectural design process.MethodDetection and classification of NFRs is performed using semi-supervised learning techniques. Classification is based on a reduced number of categorized requirements by taking advantage of the knowledge provided by uncategorized ones, as well as certain properties of text. The learning method also exploits feedback from users to enhance classification performance.ResultsThe semi-supervised approach resulted in accuracy rates above 70%, considerably higher than the results obtained with supervised methods using standard collections of documents.ConclusionEmpirical evidence showed that semi-supervision requires less human effort in labeling requirements than fully supervised methods, and can be further improved based on feedback provided by analysts. Our approach outperforms previous supervised classification proposals and can be further enhanced by exploiting feedback provided by analysts. 
52|4||Studying the impact of uncertainty in operational release planning â An integrated method and its initial evaluation|ContextUncertainty is an unavoidable issue in software engineering and an important area of investigation. This paper studies the impact of uncertainty on total duration (i.e., make-span) for implementing all features in operational release planning.ObjectiveThe uncertainty factors under investigation are: (1) the number of new features arriving during release construction, (2) the estimated effort needed to implement features, (3) the availability of developers, and (4) the productivity of developers.MethodAn integrated method is presented combining Monte-Carlo simulation (to model uncertainty in the operational release planning (ORP) process) with process simulation (to model the ORP process steps and their dependencies as well as an associated optimization heuristic representing an organization-specific staffing policy for make-span minimization). The method allows for evaluating the impact of uncertainty on make-span. The impact of uncertainty factors both in isolation and in combination are studied in three different pessimism levels through comparison with a baseline plan. Initial evaluation of the method is done by an explorative case study at Chartwell Technology Inc. to demonstrate its applicability and its usefulness.ResultsThe impact of uncertainty on release make-span increases – both in terms of magnitude and variance – with an increase of pessimism level as well as with an increase of the number of uncertainty factors. Among the four uncertainty factors, we found that the strongest impact stems from the number of new features arriving during release construction. We have also demonstrated that for any combination of uncertainty factors their combined (i.e., simultaneous) impact is bigger than the addition of their individual impacts.ConclusionThe added value of the presented method is that managers are able to study the impact of uncertainty on existing (i.e., baseline) operational release plans pro-actively. 
52|5|http://www.sciencedirect.com/science/journal/09505849/52/5|Does the technology acceptance model predict actual use? A systematic literature review|ContextThe technology acceptance model (TAM) was proposed in 1989 as a means of predicting technology usage. However, it is usually validated by using a measure of behavioural intention to use (BI) rather than actual usage.ObjectiveThis review examines the evidence that the TAM predicts actual usage using both subjective and objective measures of actual usage.MethodWe performed a systematic literature review based on a search of six digital libraries, along with vote-counting meta-analysis to analyse the overall results.ResultsThe search identified 79 relevant empirical studies in 73 articles. The results show that BI is likely to be correlated with actual usage. However, the TAM variables perceived ease of use (PEU) and perceived usefulness (PU) are less likely to be correlated with actual usage.ConclusionCare should be taken using the TAM outside the context in which it has been validated. 
52|5||A teamwork model for understanding an agile team: A case study of a Scrum project|ContextSoftware development depends significantly on team performance, as does any process that involves human interaction.ObjectiveMost current development methods argue that teams should self-manage. Our objective is thus to provide a better understanding of the nature of self-managing agile teams, and the teamwork challenges that arise when introducing such teams.MethodWe conducted extensive fieldwork for 9 months in a software development company that introduced Scrum. We focused on the human sensemaking, on how mechanisms of teamwork were understood by the people involved.ResultsWe describe a project through Dickinson and McIntyre’s teamwork model, focusing on the interrelations between essential teamwork components. Problems with team orientation, team leadership and coordination in addition to highly specialized skills and corresponding division of work were important barriers for achieving team effectiveness.ConclusionTransitioning from individual work to self-managing teams requires a reorientation not only by developers but also by management. This transition takes time and resources, but should not be neglected. In addition to Dickinson and McIntyre’s teamwork components, we found trust and shared mental models to be of fundamental importance. 
52|5||HCI and business practices in a collaborative method for augmented reality systems|ContextEvery interactive system is composed of a functional core and a user interface. However, the software engineering (SE) and human–computer interaction (HCI) communities do not share the same methods, models or tools. This usually induces a large work overhead when specialists from the two domains try to connect their applicative studies, especially when developing augmented reality systems that feature complex interaction cores.ObjectiveWe present in this paper the essential activities and concepts of a development method integrating the SE and HCI development practices, from the specifications down to the design, as well as their application on a case study.MethodThe efficiency of the method was tested in a qualitative study involving four pairs of SE and HCI experts in the design of an application for which an augmented reality interaction would provide better user performance than a classic interactive system. The effectivity of the method was evaluated in a qualitative study comparing the quality of three implementations of the same application fragment (based on the same analysis model), using software engineering metrics.ResultsThe first evaluation confirmed the ease of use of our method and the relevance of our tools for guiding the design process, but raised concerns on the handling of conflicting collaborative activities. The second evaluation gave indications that the structure of the analysis model facilitates the implementation of quality software (in terms of coupling, stability and complexity).ConclusionIt is concluded that our method enables design teams with different backgrounds in application development to collaborate for integrating augmented reality applications with information systems. Areas of improvement are also described. 
52|5||Identification of more risks can lead to increased over-optimism of and over-confidence in software development effort estimates|Software professionals are, on average, over-optimistic about the required effort usage and over-confident about the accuracy of their effort estimates. A better understanding of the mechanisms leading to the over-optimism and over-confidence may enable better estimation processes and, as a consequence, better managed software development projects. We hypothesize that there are situations where more work on risk identification leads to increased over-optimism and over-confidence in software development effort estimates, instead of the intended improvement of realism. Four experiments with software professionals are conducted to test the hypothesis. All four experiments provide results in support of the hypothesis. Possible explanations of the counter-intuitive finding relate to results from cognitive science on “illusion-of-control”, “cognitive accessibility”, “the peak-end rule” and “risk as feeling.” Thorough work on risk identification is essential for many purposes and our results should not lead to less emphasis on this activity. Our results do, however, suggest that it matters how risk identification and judgment-based effort estimation processes are combined. A simple approach for better combination of risk identification work and effort estimation is suggested. 
52|5||Analysis of Secure Mobile Grid Systems: A systematic approach|Developing software through systematic processes is becoming more and more important due to the growing complexity of software development. It is important that the development process used integrates security aspects from the first stages at the same level as other functional and non-functional requirements. Systems which are based on Grid Computing are a kind of systems that have clear differentiating features in which security is a highly important aspect. The Mobile Grid, which is relevant to both Grid and Mobile Computing, is a full inheritor of the Grid with the additional feature that it supports mobile users and resources. A development methodology for Secure Mobile Grid Systems is proposed in which the security aspects are considered from the first stages of the life-cycle and in which the mobile Grid technological environment is always present in each activity. This paper presents the analysis activity, in which the requirements (focusing on the grid, mobile and security requirements) of the system are specified and which is driven by reusable use cases through which the requirements and needs of these systems can be defined. These use cases have been defined through a UML-extension for security use cases and Grid use cases which capture the behaviour of this kind of systems. The analysis activity has been applied to a real case. 
52|5||TAIC-PART 2008 â Testing: Academic & Industrial conference â Practice and research techniques, special section editorial|
52|5||Exploring the relationship of a fileâs history and its fault-proneness: An empirical method and its application to open source programs|ContextThe knowledge about particular characteristics of software that are indicators for defects is very valuable for testers because it helps them to focus the testing effort and to allocate their limited resources appropriately.ObjectiveIn this paper, we explore the relationship between several historical characteristics of files and their defect count.MethodFor this purpose, we propose an empirical approach that uses statistical procedures and visual representations of the data in order to determine indicators for a file’s defect count. We apply this approach to nine open source Java projects across different versions.ResultsOnly 4 of 9 programs show moderate correlations between a file’s defects in previous and in current releases in more than half of the analysed releases. In contrast to our expectations, the oldest files represent the most fault-prone files. Additionally, late changes correlate with a file’s defect count only partly. The number of changes, the number of distinct authors performing changes to a file as well as the file’s age are good indicators for a file’s defect count in all projects.ConclusionOur results show that a software’s history is a good indicator for ist quality. We did not find one indicator that persists across all projects in an equal manner. Nevertheless, there are several indicators that show significant strong correlations in nearly all projects: DA (number of distinct authors) and FC (frequency of change). In practice, for each software, statistical analyses have to be performed in order to evaluate the best indicator(s) for a file’s defect count. 
52|5||Iterative execution-feedback model-directed GUI testing|Current fully automatic model-based test-case generation techniques for GUIs employ a static model. Therefore they are unable to leverage certain state-based relationships between GUI events (e.g., one enables the other, one alters the other’s execution) that are revealed at run-time and non-trivial to infer statically. We present ALT – a new technique to generate GUI test cases in batches. Because of its “alternating” nature, ALT enhances the next batch by using GUI run-time information from the current batch. An empirical study on four fielded GUI-based applications demonstrated that ALT was able to detect new 4- and 5-way GUI interaction faults; in contrast, previous techniques, due to their requirement of too many test cases, were unable to even test 4- and 5-way GUI interactions. 
52|6|http://www.sciencedirect.com/science/journal/09505849/52/6|Knowledge based quality-driven architecture design and evaluation|Modelling and evaluating quality properties of software is of high importance, especially when our every day life depends on the quality of services produced by systems and devices embedded into our surroundings. This paper contributes to the body of research in quality and model driven software engineering. It does so by introducing; (1) a quality aware software architecting approach and (2) a supporting tool chain. The novel approach with supporting tools enables the systematic development of high quality software by merging benefits of knowledge modelling and management, and model driven architecture design enhanced with domain-specific quality attributes. The whole design flow of software engineering is semi-automatic; specifying quality requirements, transforming quality requirements to architecture design, representing quality properties in architectural models, predicting quality fulfilment from architectural models, and finally, measuring quality aspects from implemented source code. The semi-automatic design flow is exemplified by the ongoing development of a secure middleware for peer-to-peer embedded systems. 
52|6||Determinants of software quality: A survey of information systems project managers|Software quality is important for the success of any information systems (IS). In this research, we find the determinants of software quality. We used five attributes for software quality: system reliability, maintainability, ease of use, usefulness, and relevance. By surveying 112 IS project managers, we collected data about their perceptions on the software quality attributes and their determinants. We arrived at six factors through exploratory factor analysis. We determined the individual factors that impacted the software quality attributes; for example, reliability is associated with responsiveness of IS department; ease of use is influenced by the capabilities of users and attitude of management; and usefulness is impacted by capabilities of IS department and responsiveness of IS department. We show that organizational factors are more important than technical factors in impacting software quality in IS projects. We provide implications of our research to practice and to future research. 
52|6||Links between the personalities, views and attitudes of software engineers|Context:Successful software development and management depends not only on the technologies, methods and processes employed but also on the judgments and decisions of the humans involved. These, in turn, are affected by the basic views and attitudes of the individual engineers.Objective:The objective of this paper is to establish if these views and attitudes can be linked to the personalities of software engineers.Methods:We summarize the literature on personality and software engineering and then describe an empirical study on 47 professional engineers in ten different Swedish software development companies. The study evaluated the personalities of these engineers via the IPIP 50-item five-factor personality test and prompted them on their attitudes towards and basic views on their professional activities.Results:We present extensive statistical analyses of their responses to show that there are multiple, significant associations between personality factors and software engineering attitudes. The tested individuals are more homogeneous in personality than a larger sample of individuals from the general population.Conclusion:Taken together, the methodology and personality test we propose and the associated statistical analyses can help find and quantify relations between complex factors in software engineering projects in both research and practice. 
52|6||Experience and challenges with UML-driven performance engineering of a Distributed Real-Time System|ContextPerformance-related failures of Distributed and Real-Time Software Systems (DRTS’s) can be very costly, e.g., explosion of a nuclear reactor. We reported in a previous work a stress testing methodology to detect performance-related Real-Time (RT) faults in DRTS’s based on the design UML model of a System Under Test (SUT). The stress methodology aimed at increasing the chances of RT failures (violations in RT constraints).ObjectiveAfter stress testing a SUT and finding RT faults, an important immediate question is how to fix (debug) those RT faults and prevent the same RT violations in the future and after deployment. If appropriate solutions to this challenge cannot be found, stress testing and its findings (detection of RT faults) will be of no or little use to the quality assurance goals of the development team.MethodTo move towards systematically solving performance-related problems causing RT faults, we develop a customized version of the standard Software Performance Engineering process and conduct an experiment on a DRTS. The process is iteratively applied to a SUT, while results from stress testing reveal that there are still scenarios in which RT constraints are violated.ResultsApplication of the performance engineering paradigm in this context on a real DRTS enables systematic analysis of performance-related defects and their fixations.ConclusionThe contributions of this work are an initial approach to software performance engineering based on stress testing, and an analysis, based on experimentation, of the open issues that need to be addressed in order to improve the approach. 
52|6||Improving component selection and monitoring with controlled experimentation and automated measurements|Context: A number of approaches have been proposed for the general problem of software component evaluation and selection. Most approaches come from the field of Component-Based Software Development (CBSD), tackle the problem of Commercial-off-the-shelf component selection and use goal-oriented requirements modelling and multi-criteria decision making techniques. Evaluation of the suitability of components is carried out largely manually and partly relies on subjective judgement. However, in dynamic, distributed environments with high demands for transparent selection processes leading to trustworthy, auditable decisions, subjective judgements and vendor claims are not considered sufficient. Furthermore, continuous monitoring and re-evaluation of components after integration is sometimes needed.Objective: This paper describes how an evidence-based approach to component evaluation can improve repeatability and reproducibility of component selection under the following conditions: (1) Functional homogeneity of candidate components and (2) High number of components and selection problem instances.Method: Our evaluation and selection method and tool empirically evaluate candidate components in controlled experiments by applying automated measurements. By analysing the differences to system-development-oriented scenarios, the paper shows how the process of utility analysis can be tailored to fit the problem space, and describes a method geared towards automated evaluation in an empirical setting. We describe tool support and a framework for automated measurements.We further present a taxonomy of decision criteria for the described scenario and discuss the data collection means needed for each category of criteria.Results: To evaluate our approach, we discuss a series of case studies in the area of digital preservation. We analyse the criteria defined in these case studies, classify them according to the taxonomy, and discuss the quantitative coverage of automated measurements.Conclusion: The results of the analysis show that an automated measurement, evaluation and selection framework is necessary and feasible to ensure trusted and repeatable decisions. 
52|6||Using the DEMO methodology for modeling open source software development processes|ContextOpen source software development (OSSD) process modeling has received increasing interest in recent years. These efforts aim to identify common elements in the development process between multiple open source software (OSS) projects. However, the complexity inherent to OSSD process modeling puts significant demands on the modeling language.ObjectiveIn this paper, we propose that the Design and Engineering Methodology for Organizations (DEMO) may provide an interesting alternative to develop OSSD process models. DEMO exhibits two unique features within the context of OSSD process modeling. First, DEMO analyzes processes at the ontological level and provides high-level process descriptions, instead of focusing on the implementation level. Second, DEMO studies the communication patterns between human actors, instead of the sequences in which activities are performed.MethodWe investigate the feasibility of using DEMO to construct OSSD process models by means of a case study. DEMO models were constructed to describe the NetBeans Requirements and Release process. In addition, the quality of these DEMO models was evaluated using a quality framework for conceptual modeling.ResultsOur results showed that our DEMO models exhibited a high level of abstraction, thereby reducing the complexity of the OSSD process models. In addition, the evaluation of the models developed in this paper by using the quality framework for conceptual modeling showed that the models were of high quality.ConclusionsWe have shown that the DEMO methodology can be successfully used to model OSSD processes and to obtain abstract and high-quality OSSD process models. However, given some potential drawbacks with respect to understandability and implementability, we primarily propose the use of DEMO within OSSD process modeling as an analysis tool that should be complemented with other techniques and models for communication and reenactment purposes. 
52|6||User commitment and collaboration: Motivational antecedents and project performance|ContextPrior research into the success of information system development projects views user commitment and collaboration as unrelated concepts in models that take either a perspective of mediators or one of processes. This perspective is limiting in that mediators and processes may interact during the course of an information system development project.ObjectiveIn this work, we model both mediators and processes as important to project outcomes and propose that processes will also be impacted by affective mediators, specifically the behavioral mediator of user commitment and the project process of collaboration. The model also allows behavioral antecedents to be considered in relation to the mediation variable, specifically the ability of the users and the extrinsic motivators perceived by the users.MethodA questionnaire containing constructs of collaboration processes, user commitment, abilities, and extrinsic motivation are completed by users in a development project and project success is measured by the IS staff for a matching independent variable. 128 matching pairs were collected and the model analyzed using partial least squares regression.ResultsResults indicate that the affective mediator can be influenced by the tested antecedents showing that IS project managers should be able to choose users with essential abilities and also establish sufficient rewards to employees, even those who may not be direct subordinates. Similarly, collaboration is still important to the success of a project, indicating that procedures to encourage collaboration be installed from the beginning of the project. However, commitment alone is sufficient to predict collaboration, meaning that motivation outside the processes in place may not be necessary to encourage collaboration between the users and IS staff.ConclusionIS researchers should consider both process mediators and affective states in future work when considering the link between antecedent inputs of software projects to the success of outputs. IS managers should promote commitment among users beyond placing collaboration mechanisms in place. This might require project managers have more decision authority in the rewards provided to user participants. 
52|6||Empirical validation of the Classic Change Curve on a software technology change project|ContextNew processes, tools, and practices are being introduced into software companies at an increasing rate. With each new advance in technology, software managers need to consider not only whether it is time to change the technologies currently used, but also whether an evolutionary change is sufficient or a revolutionary change is required.ObjectiveIn this paper, we approach this dilemma from the organizational and technology research points of view to see whether they can help software companies in initiating and managing technology change. In particular, we explore the fit of the technology S-curve, the Classic Change Curve, and a technological change framework to a software technology change project and examine the insights that such frameworks can bring.MethodThe descriptive case study described in this paper summarizes a software technology change project in which a 30-year old legacy information system running on a mainframe was replaced by a network server system at the same time as the individual-centric development practices were replaced with organization-centric ones. The study is based on a review of the company’s annual reports, in conjunction with other archival documents, five interviews and collaboration with a key stakeholder in the company.ResultsAnalyses of the collected data suggest that software technology change follows the general change research findings as characterized by the technology S-curve and the Classic Change Curve. Further, that such frameworks present critical questions for management to address when embarking on and then running such projects.ConclusionsWe describe how understanding why a software technology change project is started, the way in which it unfolds, and how different factors affect it, are essential tools for project leaders in preparing for change projects and for keeping them under control. Moreover, we show how it is equally important to understand how software technology change can work as a catalyst in revitalizing a stagnated organization, facilitating other changes and thereby helping an organization to redefine its role in the marketplace. 
52|6||Test case generation for the task tree type of architecture|ContextEmerging multicores and clusters of multicores that may operate in parallel have set a new challenge – development of massively parallel software composed of thousands of loosely coupled or even completely independent threads/processes, such as MapReduce and Java 3.0 workers, or Erlang processes, respectively. Testing and verification is a critical phase in the development of such software products.ObjectiveGenerating test cases based on operational profiles and certifying declared operational reliability figure of the given software product is a well-established process for the sequential type of software. This paper proposes an adaptation of that process for a class of massively parallel software – large-scale task trees.MethodThe proposed method uses statistical usage testing and operational reliability estimation based on operational profiles and novel test suite quality indicators, namely the percentage of different task trees and the percentage of different paths.ResultsAs an example, the proposed method is applied to operational reliability certification of a parallel software infrastructure named the TaskTreeExecutor. The paper proposes an algorithm for generating random task trees to enable that application. Test runs in the experiments involved hundreds and thousands of Win32/Linux threads thus demonstrating scalability of the proposed approach. For practitioners, the most useful result presented is the method for determining the number of task trees and the number of paths, which are needed to certify the given operational reliability of a software product. The practitioners may also use the proposed coverage metrics to measure the quality of automatically generated test suite.ConclusionThis paper provides a useful solution for the test case generation that enables the operational reliability certification process for a class of massively parallel software called the large-scale task trees. The usefulness of this solution was demonstrated by a case study – operational reliability certification of the real parallel software product. 
52|7|http://www.sciencedirect.com/science/journal/09505849/52/7|A longitudinal study of development and maintenance|ContextThe information systems we see around us today are at first sight very different from those that were developed 15 years ago and more. On the other hand, it seems that we are still struggling with many of the same problems, such as late projects and unfulfilled customer demands.ObjectiveThe paper presents finding relative to the distribution of work between maintenance and development tasks, comparing to the results reported earlier to assess the stability of important metrics within the area.MethodThis paper presents the main results of a survey-investigation performed in 2008 in 67 Norwegian organizations comparing the distribution of work to results from similar investigations performed in Norway in 1993, 1998, and 2003. Some comparisons to similar investigations performed in USA before this is also provided.ResultsThe amount of application portfolio upkeep (work made to keep up the functional coverage of the application system portfolio of the organization, including the development of replacement systems), is at the same level as reported in 1998 and 2003. The level of application maintenance is also on the same level as the similar investigations conducted in 2003 and 1998. There was a significant increase in both maintenance and application portfolio upkeep from 1993 to 1998, which could partly be attributed to be the extra maintenance and replacement-oriented work necessary to deal with the “year 2000 problem”, but this seemed to be reversed in 2003 and 2008. As for the 2003 investigation, the slow IT-market in general seemed to have influenced the results negatively seen from the point of view of application systems support efficiency in organization. No similar explanation can be used for the 2008 numbers.ConclusionBased on the last surveys it seems than a stable level of work distribution both on maintenance and application portfolio upkeep have been reached, although the underlying development technologies are still undergoing large changes. This is contrary to others claiming that the amount of maintenance is still increasing. 
52|7||Incremental method evolution in global software product management: A retrospective case study|Company growth in a global setting causes challenges in the adaptation and maintenance of an organization’s methods. In this paper, we will analyze incremental method evolution in software product management in a global environment. We validate a method increment approach, based on method engineering principles, by applying it to a retrospective case study conducted at a large ERP vendor. The results show that the method increment types cover all increments that were found in the case study. Also, we identified the following lessons learned for company growth in a global software product management context: method increment drivers, such as the change of business strategy, vary during evolution; a shared infrastructure is critical for rollout; small increments facilitate gradual process improvement; and global involvement is critical. We then claim that method increments enable software companies to accommodate evolutionary adaptations of development process in agreement with the overall company expansion. 
52|7||A DSL toolkit for deferring architectural decisions in DSL-based software design|A number of mature toolkits and language workbenches for DSL-based design have been proposed, making DSL-based design attractive for many projects. These toolkits preselect many architectural decision options. However, in many cases it would be beneficial for DSL-based design to decide for the DSL’s architecture later on in a DSL project, once the requirements and the domain have been sufficiently understood. We propose a language and a number of DSLs for DSL-based design and development that combine important benefits of different DSL toolkits in a unique way. Our approach specifically targets at deferring architectural decisions in DSL-based design. As a consequence, the architect can choose, even late in a DSL project, for options such as whether to provide the DSL as one or more external or embedded DSLs and whether to use an explicit language model or not . 
52|7||Best practice fusion of CMMI-DEV v1.2 (PP, PMC, SAM) and PMBOK 2008|ContextThe establishment of effective and efficient project management practices still remains a challenge to software organizations. In striving to address these needs, “best practice” models, such as, CMMI or PMBOK, are being developed to assist organizations interested in improving project management. And, although, those models share overlapping content, there are still differences and, therefore, each of the models offers different advantages.ObjectiveThis paper proposes a set of unified project management best practices by integrating and harmonizing on a high-level perspective PMBOK (4th ed.) processes and CMMI-DEV v1.2 specific practices of the basic project management process areas PP, PMC and SAM.MethodBased on the analysis of both models, a unified set of best practices has been defined by a group of researchers with theoretical and practical expertise on the CMMI framework and software process improvement as well as project management and the PMBOK. The proposed set has been revised by different researchers from different institutions in several review rounds until consensus was achieved.ResultsAs a result, a set of unified best practices is defined and explicitly mapped to the correspondent PMBOK processes and CMMI specific practices of the current versions of both models.ConclusionWe can conclude that an integration and harmonization of both models is possible and may help to implement and assess project management processes more effectively and efficiently, optimizing software process improvement investments. 
52|7||Simulating evolution in model-based product line engineering|ContextNumerous approaches are available for modeling product lines and their variability. However, the long-term impacts of model-based development on maintenance effort and model complexity can hardly be investigated due to a lack of empirical data. Conducting empirical research in product line engineering is difficult as companies are typically reluctant to provide access to data from their product lines. Also, many benefits of product lines can be measured only in longitudinal studies, which are difficult to perform in most environments.ObjectiveIn this paper, we thus aim to explore the benefit of simulation to investigate the evolution of model-based product lines.MethodWe present a simulation approach for exploring the effects of product line evolution on model complexity and maintenance effort. Our simulation considers characteristics of product lines (e.g., size, dependencies in models) and we experiment with different evolution profiles (e.g., technical refactoring vs. placement of new products).ResultsWe apply the approach in a simulation experiment that uses data from real-world product lines from the domain of industrial automation systems to demonstrate its feasibility.ConclusionOur results demonstrate that simulation contributes to understanding the effects of maintenance and evolution in model-based product lines. 
52|7||Case-based analysis in user requirements modelling for knowledge construction|ContextLearning can be regarded as knowledge construction in which prior knowledge and experience serve as basis for the learners to expand their knowledge base. Such a process of knowledge construction has to take place continuously in order to enhance the learners’ competence in a competitive working environment. As the information consumers, the individual users demand personalised information provision which meets their own specific purposes, goals, and expectations.ObjectivesThe current methods in requirements engineering are capable of modelling the common user’s behaviour in the domain of knowledge construction. The users’ requirements can be represented as a case in the defined structure which can be reasoned to enable the requirements analysis. Such analysis needs to be enhanced so that personalised information provision can be tackled and modelled. However, there is a lack of suitable modelling methods to achieve this end. This paper presents a new ontological method for capturing individual user’s requirements and transforming the requirements onto personalised information provision specifications. Hence the right information can be provided to the right user for the right purpose.MethodAn experiment was conducted based on the qualitative method. A medium size of group of users participated to validate the method and its techniques, i.e. articulates, maps,configures, and learning content. The results were used as the feedback for the improvement.ResultThe research work has produced an ontology model with a set of techniques which support the functions for profiling user’s requirements, reasoning requirements patterns, generating workflow from norms, and formulating information provision specifications.ConclusionThe current requirements engineering approaches provide the methodical capability for developing solutions. Our research outcome, i.e. the ontology model with the techniques, can further enhance the RE approaches for modelling the individual user’s needs and discovering the user’s requirements. 
52|8|http://www.sciencedirect.com/science/journal/09505849/52/8|Process models in the practice of distributed software development: A systematic review of the literature|ContextDistributed Software Development (DSD) has recently become an active research area. Although considerable research effort has been made in this area, as yet, no agreement has been reached as to an appropriate process model for DSD.PurposeThis paper is intended to identify and synthesize papers that describe process models for distributed software development in the context of overseas outsourcing, i.e. “offshoring”.MethodWe used a systematic review methodology to search seven digital libraries and one topic-specific conference.ResultsWe found 27 primary studies describing stage-related DSD process models. Only five of such studies looked into outsourcing to a subsidiary company (i.e. “internal offshoring”). Nineteen primary studies addressed the need for DSD process models. Eight primary studies and three literature surveys described stage-based DSD process models, but only three of such models were empirically evaluated.ConclusionWe need more research aimed at internal offshoring. Furthermore, proposed models need to be empirically validated. 
52|8||Systematic literature reviews in software engineering â A tertiary study|ContextIn a previous study, we reported on a systematic literature review (SLR), based on a manual search of 13 journals and conferences undertaken in the period 1st January 2004 to 30th June 2007.ObjectiveThe aim of this on-going research is to provide an annotated catalogue of SLRs available to software engineering researchers and practitioners. This study updates our previous study using a broad automated search.MethodWe performed a broad automated search to find SLRs published in the time period 1st January 2004 to 30th June 2008. We contrast the number, quality and source of these SLRs with SLRs found in the original study.ResultsOur broad search found an additional 35 SLRs corresponding to 33 unique studies. Of these papers, 17 appeared relevant to the undergraduate educational curriculum and 12 appeared of possible interest to practitioners. The number of SLRs being published is increasing. The quality of papers in conferences and workshops has improved as more researchers use SLR guidelines.ConclusionSLRs appear to have gone past the stage of being used solely by innovators but cannot yet be considered a main stream software engineering research methodology. They are addressing a wide range of topics but still have limitations, such as often failing to assess primary study quality. 
52|8||Requirements engineering for software product lines: A systematic literature review|ContextSoftware product line engineering (SPLE) is a growing area showing promising results in research and practice. In order to foster its further development and acceptance in industry, it is necessary to assess the quality of the research so that proper evidence for adoption and validity are ensured. This holds in particular for requirements engineering (RE) within SPLE, where a growing number of approaches have been proposed.ObjectiveThis paper focuses on RE within SPLE and has the following goals: assess research quality, synthesize evidence to suggest important implications for practice, and identify research trends, open problems, and areas for improvement.MethodA systematic literature review was conducted with three research questions and assessed 49 studies, dated from 1990 to 2009.ResultsThe evidence for adoption of the methods is not mature, given the primary focus on toy examples. The proposed approaches still have serious limitations in terms of rigor, credibility, and validity of their findings. Additionally, most approaches still lack tool support addressing the heterogeneity and mostly textual nature of requirements formats as well as address only the proactive SPLE adoption strategy.ConclusionsFurther empirical studies should be performed with sufficient rigor to enhance the body of evidence in RE within SPLE. In this context, there is a clear need for conducting studies comparing alternative methods. In order to address scalability and popularization of the approaches, future research should be invested in tool support and in addressing combined SPLE adoption strategies. 
52|8||A language-independent and formal approach to pattern-based modelling with support for composition and analysis|ContextPatterns are used in different disciplines as a way to record expert knowledge for problem solving in specific areas. Their systematic use in Software Engineering promotes quality, standardization, reusability and maintainability of software artefacts. The full realisation of their power is however hindered by the lack of a standard formalization of the notion of pattern.ObjectiveOur goal is to provide a language-independent formalization of the notion of pattern, so that it allows its application to different modelling languages and tools, as well as generic methods to enable pattern discovery, instantiation, composition, and conflict analysis.MethodFor this purpose, we present a new visual and formal, language-independent approach to the specification of patterns. The approach is formulated in a general way, based on graphs and category theory, and allows the specification of patterns in terms of (nested) variable submodels, constraints on their allowed variance, and inter-pattern synchronization across several diagrams (e.g. class and sequence diagrams for UML design patterns).ResultsWe provide a formal notion of pattern satisfaction by models and propose mechanisms to suggest model transformations so that models become consistent with the patterns. We define methods for pattern composition, and conflict analysis. We illustrate our proposal on UML design patterns, and discuss its generality and applicability on different types of patterns, e.g. workflow patterns, enterprise integration patterns and interaction patterns.ConclusionThe approach has proven to be powerful enough to formalize patterns from different domains, providing methods to analyse conflicts and dependencies that usually are expressed only in textual form. Its language independence makes it suitable for integration in meta-modelling tools and for use in Model-Driven Engineering. 
52|8||Traceability-centric model-driven object-oriented engineering|ContextObject-oriented (OO) development method is a popular paradigm in developing target systems. However, the current practices of OO analysis and design (OOAD) and implementation largely rely on human developers’ experience and expertise, making it possible less efficient and more error-prone. Hence, there is room for improving the development efficiency while preserving high quality of programs.ObjectiveModel-driven development (MDD) is a promising approach to developing programs by machine-assisted model transformation, saving human efforts and reducing the possibility of introducing program faults. Hence, it is appealing to apply key disciplines of MDD in developing OO programs.MethodIn this paper, we propose a comprehensive framework for applying MDD on OO program engineering in a rigorous and formal fashion. The framework consists of: (1) a hybrid engineering model of human and machine, (2) meta-models of OOAD artifacts, (3) traceability map with trace links, and (4) transformation rules.ResultsWe identified five platform independent models and two platform specific models, and defined formal representations for them. We identified 16 traceability links and accordingly 16 transformation rules among the eight artifacts. Through the case study, we showed that our work is feasible and applicable. We assessed our work and concluded that our work is sound, complete, and extendable. Our work established the foundation toward automatic generation of OO programs based on the traceability framework.ConclusionIt is concluded that it is essential to identify the OOAD artifacts, traceability links, and transformation rules for automatic generation of OO programs. It is also important to understand the human involvement nature in MDD and to explicitly treat them in the model transformation. 
52|9|http://www.sciencedirect.com/science/journal/09505849/52/9|A systematic review of comparative evidence of aspect-oriented programming|ContextAspect-oriented programming (AOP) promises to improve many facets of software quality by providing better modularization and separation of concerns, which may have system wide affect. There have been numerous claims in favor and against AOP compared with traditional programming languages such as Objective Oriented and Structured Programming Languages. However, there has been no attempt to systematically review and report the available evidence in the literature to support the claims made in favor or against AOP compared with non-AOP approaches.ObjectiveThis research aimed to systematically identify, analyze, and report the evidence published in the literature to support the claims made in favor or against AOP compared with non-AOP approaches.MethodWe performed a systematic literature review of empirical studies of AOP based development, published in major software engineering journals and conference proceedings.ResultsOur search strategy identified 3307 papers, of which 22 were identified as reporting empirical studies comparing AOP with non-AOP approaches. Based on the analysis of the data extracted from those 22 papers, our findings show that for performance, code size, modularity, and evolution related characteristics, a majority of the studies reported positive effects, a few studies reported insignificant effects, and no study reported negative effects; however, for cognition and language mechanism, negative effects were reported.ConclusionAOP is likely to have positive effect on performance, code size, modularity, and evolution. However its effect on cognition and language mechanism is less likely to be positive. Care should be taken using AOP outside the context in which it has been validated. 
52|9||Software engineering research for computer games: A systematic review|ContextCurrently, computer game development is one of the fastest growing industries in the worldwide economy. In addition to that, computer games are rapidly evolving in the sense that newer game versions arrive in a very short interval. Thus, software engineering techniques are needed for game development in order to achieve greater flexibility and maintainability, less cost and effort, better design, etc. In addition, games present several characteristics that differentiate their development from classical software development.ObjectiveThis study aims to assess the state of the art on research concerning software engineering for computer games and discuss possible important areas for future research.MethodWe employed a standard methodology for systematic literature reviews using four well known digital libraries.ResultsSoftware engineering for computer games is a research domain that has doubled its research activity during the last 5 years. The dominant research topic has proven to be requirements engineering, while topics such as software verification and maintenance have been neglected up to now.ConclusionThe results of the study suggest that software engineering for computer games is a field that embraces many techniques and methods from conventional software engineering and adapts them so as to fit the specific requirements of game development. In addition to that, the study proposes the employment of more elaborate empirical methods, i.e. controlled experiments and case studies, in game software engineering research, which, have not been extensively used up to now. 
52|9||Survival analysis on the duration of open source projects|ContextOpen source (FLOSS) project survivability is an important piece of information for many open source stakeholders. Coordinators of open source projects would like to know the chances for the survival of the projects they coordinate. Companies are also interested in knowing how viable a project is in order to either participate or invest in it, and volunteers want to contribute to vivid projects.ObjectiveThe purpose of this article is the application of survival analysis techniques for estimating the future development of a FLOSS project.MethodIn order to apply such approach, duration data regarding FLOSS projects from the FLOSSMETRICS (This work was partially supported by the European Community’s Sixth Framework Program under the Contract FP6-033982) database were collected. Such database contains metadata for thousands of FLOSS projects, derived from various forges. Subsequently, survival analysis methods were employed to predict the survivability of the projects, i.e. their probability of continuation in the future, by examining their duration, combined with other project characteristics such as their application domain and number of committers.ResultsIt was shown how probability of termination or continuation may be calculated and how a prediction model may be built to upraise project future. In addition, the benefit of adding more committers to FLOSS projects was quantified.ConclusionAnalysis results demonstrate the usefulness of the proposed framework for assessing the survival probability of a FLOSS project. 
52|9||Investigating the impact of a measurement program on software quality|ContextMeasurement programs have been around for several decades but have been often misused or misunderstood by managers and developers. This misunderstanding prevented their adoption despite their many advantages.ObjectiveIn this paper, we present the results of an empirical study on the impact of a measurement program, MQL (“Mise en Qualité du Logiciel”, French for “Quality Software Development”), in an industrial context.MethodWe analyzed data collected on 44 industrial systems of different sizes: 22 systems were developed using MQL while the other 22 used ad-hoc approaches to assess and control quality (control group, referred to as “ad-hoc systems”). We studied the impact of MQL on a set of nine variables: six quality factors (maintainability, evolvability, reusability, robustness, testability, and architecture quality), corrective-maintenance effort, code complexity, and the presence of comments.ResultsOur results show that MQL had a clear positive impact on all the studied indicators. This impact is statistically significant for all the indicators but corrective-maintenance effort.ConclusionWe bring concrete evidence that a measurement program can have a significant, positive impact on the quality of software systems if combined with appropriate decision making procedures and corrective actions. 
52|9||Evaluating logistic regression models to estimate software project outcomes|ContextSoftware has been developed since the 1960s but the success rate of software development projects is still low. During the development of software, the probability of success is affected by various practices or aspects. To date, it is not clear which of these aspects are more important in influencing project outcome.ObjectiveIn this research, we identify aspects which could influence project success, build prediction models based on the aspects using data collected from multiple companies, and then test their performance on data from a single organization.MethodA survey-based empirical investigation was used to examine variables and factors that contribute to project outcome. Variables that were highly correlated to project success were selected and the set of variables was reduced to three factors by using principal components analysis. A logistic regression model was built for both the set of variables and the set of factors, using heterogeneous data collected from two different countries and a variety of organizations. We tested these models by using a homogeneous hold-out dataset from one organization. We used the receiver operating characteristic (ROC) analysis to compare the performance of the variable and factor-based models when applied to the homogeneous dataset.ResultsWe found that using raw variables or factors in the logistic regression models did not make any significant difference in predictive capability. The prediction accuracy of these models is more balanced when the cut-off is set to the ratio of success to failures in the datasets used to build the models. We found that the raw variable and factor-based models predict significantly better than random chance.ConclusionWe conclude that an organization wishing to estimate whether a project will succeed or fail may use a model created from heterogeneous data derived from multiple organizations. 
52|9||Semi-formal transformation of secure business processes into analysis class and use case models: An MDA approach|ContextModel-Driven Development (MDD) is an alternative approach for information systems development. The basic underlying concept of this approach is the definition of abstract models that can be transformed to obtain models near implementation. One fairly widespread proposal in this sphere is that of Model Driven Architecture (MDA). Business process models are abstract models which additionally contain key information about the tasks that are being carried out to achieve the company’s goals, and two notations currently exist for modelling business processes: the Unified Modelling Language (UML), through activity diagrams, and the Business Process Modelling Notation (BPMN).ObjectiveOur research is particularly focused on security requirements, in such a way that security is modelled along with the other aspects that are included in a business process. To this end, in earlier works we have defined a metamodel called secure business process (SBP), which may assist in the process of developing software as a source of highly valuable requirements (including very abstract security requirements), which are transformed into models with a lower abstraction level, such as analysis class diagrams and use case diagrams through the approach presented in this paper.MethodWe have defined all the transformation rules necessary to obtain analysis class diagrams and use case diagrams from SBP, and refined them through the characteristic iterative process of the action-research method.ResultsWe have obtained a set of rules and a checklist that make it possible to automatically obtain a set of UML analysis classes and use cases, starting from SBP models. Our approach has additionally been applied in a real environment in the area of the payment of electrical energy consumption.ConclusionsThe application of our proposal shows that our semi-automatic process can be used to obtain a set of useful artifacts for software development processes. 
52|9||Bug localization using latent Dirichlet allocation|ContextSome recent static techniques for automatic bug localization have been built around modern information retrieval (IR) models such as latent semantic indexing (LSI). Latent Dirichlet allocation (LDA) is a generative statistical model that has significant advantages, in modularity and extensibility, over both LSI and probabilistic LSI (pLSI). Moreover, LDA has been shown effective in topic model based information retrieval. In this paper, we present a static LDA-based technique for automatic bug localization and evaluate its effectiveness.ObjectiveWe evaluate the accuracy and scalability of the LDA-based technique and investigate whether it is suitable for use with open-source software systems of varying size, including those developed using agile methods.MethodWe present five case studies designed to determine the accuracy and scalability of the LDA-based technique, as well as its relationships to software system size and to source code stability. The studies examine over 300 bugs across more than 25 iterations of three software systems.ResultsThe results of the studies show that the LDA-based technique maintains sufficient accuracy across all bugs in a single iteration of a software system and is scalable to a large number of bugs across multiple revisions of two software systems. The results of the studies also indicate that the accuracy of the LDA-based technique is not affected by the size of the subject software system or by the stability of its source code base.ConclusionWe conclude that an effective static technique for automatic bug localization can be built around LDA. We also conclude that there is no significant relationship between the accuracy of the LDA-based technique and the size of the subject software system or the stability of its source code base. Thus, the LDA-based technique is widely applicable. 
52|9||Specification of personalization in web application design|Personalization of websites has become an important issue in Web modeling methods due to their big and heterogeneous audience. However, due to the existence of too many notations to represent the same design concepts in different methodologies, personalization specifications cannot be used out of the scope of a single tool or method. Moreover, in some cases, personalization is not defined as a separate aspect, being difficult to maintain and update. This paper tackles the aforementioned problems presenting a generic modeling technique to facilitate the specification of the personalization. Personalization specifications can be reused across different websites and different development environments. 
53|1|http://www.sciencedirect.com/science/journal/09505849/53/1|Taking novelty to a new level|
53|1||Software product line testing â A systematic mapping study|ContextSoftware product lines (SPL) are used in industry to achieve more efficient software development. However, the testing side of SPL is underdeveloped.ObjectiveThis study aims at surveying existing research on SPL testing in order to identify useful approaches and needs for future research.MethodA systematic mapping study is launched to find as much literature as possible, and the 64 papers found are classified with respect to focus, research type and contribution type.ResultsA majority of the papers are of proposal research types (64%). System testing is the largest group with respect to research focus (40%), followed by management (23%). Method contributions are in majority.ConclusionsMore validation and evaluation research is needed to provide a better foundation for SPL testing. 
53|1||A formal approach for the development of reactive systems|ContextThis paper deals with the development and verification of liveness properties on reactive systems using the Event-B method. By considering the limitation of the Event-B method to invariance properties, we propose to apply the language TLA+ to verify liveness properties on Event-B models.ObjectiveThis paper deals with the use of two verification approaches: theorem proving and model-checking, in the construction and verification of safe reactive systems. The theorem prover concerned is part of the Click_n_Prove tool associated to the Event-B method and the model checker is TLC for TLA+ models.MethodTo verify liveness properties on Event-B systems, we extend first the expressivity and the semantics of a B model (called temporal B model) to deal with the specification of fairness and eventuality properties. Second, we propose semantics of the extension over traces, in the same spirit as TLA+ does. Third, we give verification rules in the axiomatic way of the Event-B method. Finally, we give transformation rules from a temporal B model into a TLA+ module. We present in particular, our prototype system called B2TLA+, that we have developed to support this transformation; then we can verify liveness properties thanks to the model checker TLC on finite state systems. For the verification of infinite-state systems, we propose the use of the predicate diagrams and its associated tool DIXIT. As the B refinement preserves invariance properties through refinement steps, we propose some rules to get the preservation of liveness properties by the B refinement.ResultsThe proposed approach is applied for the development of some reactive systems examples and our prototype system B2TLA+ is successfully used to transform a temporal B model into a TLA+ module.ConclusionThe paper successfully defines an approach for the specification and verification of safety and liveness properties for the development of reactive systems using the Event-B method, the language TLA+ and the predicate diagrams with their associated tools. The approach is illustrated on a case study of a parcel sorting system. 
53|1||Evolution of XML schemas and documents from stereotyped UML class models: A traceable approach|ContextUML and XML are two of the most commonly used languages in software engineering processes. One of the most critical of these processes is that of model evolution and maintenance. More specifically, when an XML schema is modified, the changes should be propagated to the corresponding XML documents, which must conform with the new, modified schema.ObjectiveThe goal of this paper is to provide an evolution framework by which the XML schema and documents are incrementally updated according to the changes in the conceptual model (expressed as a UML class model). In this framework, we include the transformation and evolution of UML profiles specified in UML class models because they are widely used to capture domain specific semantics.MethodWe have followed a metamodeling approach which allowed us to achieve a language independent framework, not tied to the specific case of UML–XML. Besides, our proposal considers a traceability setting as a key aspect of the transformation process which allows changes to be propagated from UML class models to both XML schemas and documents.ResultsAs a general framework, we propose a Generic Evolution Architecture (GEA) for the model-driven engineering context. Within this architecture and for the particular case of the UML-to-XML setting, our contribution is a UML-to-XML framework that, to our knowledge, is the only approach that incorporates the following four characteristics. Firstly, the evolution tasks are carried out in a conceptual model. Secondly, our approach includes the transformation to XML of UML profiles. Thirdly, the proposal allows stereotyped UML class models to be evolved, propagating changes to XML schemas and documents in such a way that the different elements are kept in synch. Finally, we propose a traceability setting that enables evolution tasks to be performed seamlessly.ConclusionsGeneric frameworks such as that proposed in this paper help to reduce the work overload experienced by software engineers in keeping different software artifacts synchronized. 
53|1||Change the face of software engineering education: A field report from Taiwan|Context: In Taiwan, the supply of software engineers provided by universities has suffered from both a quantity problem and a quality problem. An effort to change the software engineering education is in need.Objective: The Software Engineering Consortium (SEC) of Taiwan sets its objective to increase the number of college graduates that are better prepared for filling software development and maintenance jobs.Method: Four dysfunctions: avoidance of process, inattention to modeling, lack of awareness to software quality, and chasm between application domains and software engineering, of the current situation are identified. The effort to correct the dysfunctions involves design of a module-oriented software engineering curriculum, and organization of people, resource, and activities.Results: In the academic years from 2003 to 2008, both the number of software engineering courses offered and the enrollment size increased significantly by a space of some 250 courses and 5000 enrollments, respectively.Conclusion: The SEC effort to establishing software engineering modules has been received with enthusiasm by faculty members and students of the participating institutes. Inspired by the important foundational work such as SWEBOK and SE2004, we believe that the adopted strategy of identifying dysfunctions and then designing remedies to address these dysfunctions contributed significantly to the success of the SEC effort. 
53|1||The agile requirements refinery: Applying SCRUM principles to software product management|ContextAlthough agile software development methods such as SCRUM and DSDM are gaining popularity, the consequences of applying agile principles to software product management have received little attention until now.ObjectiveIn this paper, this gap is filled by the introduction of a method for the application of SCRUM principles to software product management.MethodA case study research approach is employed to describe and evaluate this method.ResultsThis has resulted in the ‘agile requirements refinery’, an extension to the SCRUM process that enables product managers to cope with complex requirements in an agile development environment. A case study is presented to illustrate how agile methods can be applied to software product management.ConclusionsThe experiences of the case study company are provided as a set of lessons learned that will help others to apply agile principles to their software product management process. 
53|1||Migration of information systems in the Italian industry: A state of the practice survey|ContextSoftware migration—and in particular migration towards the Web and towards distributed architectures—is a challenging and complex activity, and has been particularly relevant in recent years, due to the large number of migration projects the industry had to face off because of the increasing pervasiveness of the Web and of mobile devices.ObjectiveThis paper reports a survey aimed at identifying the state-of-the-practice of the Italian industry for what concerns the previous experiences in software migration projects—specifically concerning information systems—the adopted tools and the emerging needs and problems.MethodThe study has been carried out among 59 Italian Information Technology companies, and for each company a representative person had to answer an on-line questionnaire concerning migration experiences, pieces of technology involved in migration projects, adopted tools, and problems occurred during the project.ResultsIndicate that migration—especially towards the Web—is highly relevant for Italian IT companies, and that companies tend to increasingly adopt free and open source solutions rather than commercial ones. Results also indicate that the adoption of specific tools for migration is still very limited, either because of the lack of skills and knowledge, or due to the lack of mature and adequate options.ConclusionsFindings from this survey suggest the need for further technology transfer between academia and industry for the purpose of favoring the adoption of software migration techniques and tools. 
53|1||Applying CIM-to-PIM model transformations for the service-oriented development of information systems|ContextModel-driven approaches deal with the provision of models, transformations between them and code generators to address software development. This approach has the advantage of defining a conceptual structure, where the models used by business managers and analysts can be mapped into more detailed models used by software developers. This alignment between high-level business specifications and the lower-level information technologies (ITs) models is crucial to the field of service-oriented development, where meaningful business services and process specifications are those relevant to real business scenarios.ObjectiveThis paper presents a model-driven approach which, starting from high-level computational-independent business models (CIMs) – the business view – sets out guidelines for obtaining lower-level platform-independent behavioural models (PIMs) – the information system view. A key advantage of our approach is the use of real high-level business models, not just requirements models, which, by means of model transformations, helps software developers to make the most of the business knowledge for specifying and developing business services.MethodThis proposal is framed in a method for service-oriented development of information systems whose main characteristic is the use of services as first-class objects. The method follows an MDA-based approach, proposing a set of models at different levels of abstraction and model transformations to connect them.ResultsThe paper present the complete set of CIM and PIM metamodels and the specification of the mappings between them, which clear advantage is the support for the alignment between high-level business view and ITs. The proposed model-driven process is being implemented in an MDA tool. A first prototype has been used to develop a travel agency case study that illustrates the proposal.ConclusionThis study shows how a model-driven approach helps to solve the alignment problem between the business view and the information system view that arises when adopting service-oriented approaches for software development. 
53|1||A profiling method by PCB hooking and its application for memory fault detection in embedded system operational test|ContextAn operational test means a system test that examines whether or not all software or hardware components comply with the requirements given to a system which is deployed in an operational environment.ObjectiveIt is a necessary lightweight-profiling method for embedded systems with severe resource restrictions to conduct operational testing.MethodWe focus on the Process Control Block as the optimal location to monitor the execution of all processes. We propose a profiling method to collect the runtime execution information of the processes without interrupting the system’s operational environment by hacking the Process Control Block information. Based on the proposed method applied to detect runtime memory faults, we develop the operational testing tool AMOS v1.0 which is currently being used in the automobile industry.ResultsAn industrial field study on 23 models of car-infotainment systems revealed a total of 519 memory faults while only slowing down the system by 0.084–0.132×. We conducted a comparative analysis on representative runtime memory fault detection tools. This analysis result shows our proposed method that has relatively low overhead meets the requirements for operational testing, while other methods failed to satisfy the operational test conditions.ConclusionWe conclude that a lightweight-profiling method for embedded system operational testing can be built around the Process Control Block. 
53|10|http://www.sciencedirect.com/science/journal/09505849/53/10|Business process archeology using MARBLE|ContextLegacy information systems age over time. These systems cannot be thrown away because they store a significant amount of valuable business knowledge over time, and they cannot be entirely replaced at an acceptable cost. This circumstance is similar to that of the monuments of ancient civilizations, which have aged but still hold meaningful information about their civilizations. Evolutionary maintenance is the most suitable mechanism to deal with the software ageing problem since it preserves business knowledge. But first, recovering the underlying business knowledge in legacy systems is necessary in order to preserve this vital heritage.ObjectiveThis paper proposes and validates a method for recovering and rebuilding business processes from legacy information systems. This method, which can be considered a business process archeology, makes it possible to preserve the business knowledge in legacy information systems.MethodThe business process archeology method is framed in MARBLE, a generic framework based on Architecture-Driven Modernization (ADM), which uses the Knowledge Discovery Metamodel (KDM) standard. The proposed method is validated using a case study that involves a real-life legacy system. The case study is conducted following the case study protocol proposed by Brereton et al.ResultsThe study reports that the proposed method makes it possible to obtain business process models from legacy systems with adequate levels of accuracy. In addition, the effectiveness of the proposed method is also validated positively.ConclusionThe proposed method semi-automatically rebuilds the hidden business processes embedded in a legacy system. Therefore, the business process archeology method quickly allows business experts to have a meaningful understanding of the organization’s business processes. This proposal is less time-consuming and more exhaustive (since it considers the embedded business knowledge) than a manual process redesign by experts from scratch. In addition, it helps maintainers to extract the business knowledge needed for the system to evolve. 
53|10||Development and evaluation of a lightweight root cause analysis method (ARCA method) â Field studies at four software companies|ContextThe key for effective problem prevention is detecting the causes of a problem that has occurred. Root cause analysis (RCA) is a structured investigation of the problem to identify which underlying causes need to be fixed. The RCA method consists of three steps: target problem detection, root cause detection, and corrective action innovation. Its results can help with process improvement.ObjectiveThis paper presents a lightweight RCA method, named the ARCA method, and its empirical evaluation. In the ARCA method, the target problem detection is based on a focus group meeting. This is in contrast to prior RCA methods, where the target problem detection is based on problem sampling, requiring heavy startup investments.MethodThe ARCA method was created with the framework of design science. We evaluated it through field studies at four medium-sized software companies using interviews and query forms to collect feedback from the case attendees. A total of five key representatives of the companies were interviewed, and 30 case participants answered the query forms. The output of the ARCA method was also evaluated by the case attendees, i.e., a total 757 target problem causes and 124 related corrective actions.ResultsThe case attendees considered the ARCA method useful and easy to use, which indicates that it is beneficial for process improvement and problem prevention. In each case, 24–77 target problem root causes were processed and 13–40 corrective actions were developed. The effort of applying the method was 89 man-hours, on average.ConclusionThe ARCA method required an acceptable level of effort and resulted in numerous high-quality corrective actions. In contrast to the current company practices, the method is an efficient method to detect new process improvement opportunities and develop new process improvement ideas. Additionally, it is easy to use. 
53|10||Empirical evaluation of the fault detection effectiveness and test effort efficiency of the automated AOP testing approaches|ContextTesting process is a time-consuming, expensive, and labor-intensive activity in any software setting including aspect-oriented programming (AOP). To reduce the testing costs, human effort, and to achieve the improvements in both quality and productivity of AOP, it is desirable to automate testing of aspect-oriented programs as much as possible.ObjectiveIn recent past years, a lot of research effort has been devoted to testing aspect-oriented programs but less effort has been dedicated to the automated AOP testing. This denotes that the current research on automated AOP testing is not sufficient and is still in a stage of infancy. In order to advance the state of the research in this area and to provide testers of AOP-based projects with a comparison basis, a detailed evaluation of the current automated AOP testing approaches in a thorough and experimental manner is required. Thus, the objective of this paper is to provide such evaluation of the current approaches.MethodIn this paper, we carry out an empirical study based on mutation analysis to examine four (namely Wrasp, Aspectra, Raspect, and EAT) existing automated AOP testing approaches, particularly their underlying test input generation and selection strategies, with regard to fault detection effectiveness. In addition, the approaches are compared in terms of required effort in detecting faults as part of efficiency evaluation.ResultsThe experimental results and comparison provided insights into the effectiveness and efficiency of automated AOP testing with their respective strengths and weaknesses. Results showed that EAT is more effective than the other automated AOP testing approaches but not significant for all approaches. EAT was found to be significantly better than Wrasp at 95% confidence level (i.e. p < 0.05), but not significantly better than Aspectra or Raspect. Concerning the test effort efficiency, Wrasp was significantly (p < 0.05) efficient with requiring the lowest amount of test effort compared to the other approaches. Whereas, EAT showed to be not very efficient by recording the highest amount of test effort.ConclusionThis implies that EAT can currently be the most effective automated AOP testing approach but perhaps less efficient. More generally, search-based testing (as underlying strategy of EAT approach) might achieve better effectiveness but at the cost of greater test effort compared to random testing (as underlying strategy of other approaches). 
53|10||A method for assessing confidence in requirements analysis|ContextDuring development managers, analysts and designers often need to know whether enough requirements analysis work has been done and whether or not it is safe to proceed to the design stage.ObjectiveThis paper describes a new, simple and practical method for assessing our confidence in a set of requirements.MethodWe identified four confidence factors and used a goal oriented framework with a simple ordinal scale to develop a method for assessing confidence. We illustrate the method and show how it has been applied to a real systems development project.ResultsWe show how assessing confidence in the requirements could have revealed problems in this project earlier and so saved both time and money.ConclusionOur meta-level assessment of requirements provides a practical and pragmatic method that can prove useful to managers, analysts and designers who need to know when sufficient requirements analysis has been performed. 
53|10||Guest Editorial for Special Section on Mutation Testing|
53|10||A mutation carol: Past, present and future|ContextThe field of mutation analysis has been growing, both in the number of published papers and the number of active researchers. This special issue provides a sampling of recent advances and ideas. But do all the new researchers know where we started?ObjectiveTo imagine where we are going, we must first know where we are. To understand where we are, we must know where we have been. This paper reviews past mutation analysis research, considers the present, then imagines possible future directions.MethodA retrospective study of past trends lets us the ability to see the current state of mutation research in a clear context, allowing us to imagine and then create future vectors.ResultsThe value of mutation has greatly expanded since the early view of mutation as an expensive way to unit test subroutines. Our understanding of what mutation is and how it can help has become much deeper and broader.ConclusionMutation analysis has been around for 35 years, but we are just now beginning to see its full potential. The papers in this issue and future mutation workshops will eventually allow us to realize this potential. 
53|10||Evolutionary mutation testing|ContextMutation testing is a testing technique that has been applied successfully to several programming languages. However, it is often regarded as computationally expensive, so several refinements have been proposed to reduce its cost. Moreover, WS-BPEL compositions are being widely adopted by developers, but present new challenges for testing, since they can take much longer to run than traditional programs of the same size. Therefore, it is interesting to reduce the number of mutants required.ObjectiveWe present Evolutionary Mutation Testing (EMT), a novel mutant reduction technique for finding mutants that help derive new test cases that improve the quality of the initial test suite. It uses evolutionary algorithms to reduce the number of mutants that are generated and executed with respect to the exhaustive execution of all possible mutants, keeping as many difficult to kill and potentially equivalent mutants (strong mutants) as possible in the reduced set.MethodTo evaluate EMT we have developed GAmera, a mutation testing system powered by a co-evolutive genetic algorithm. We have applied this system to three WS-BPEL compositions to estimate its effectiveness, comparing it with random selection.ResultsThe results obtained experimentally show that EMT can select all strong mutants generating 15% less mutants than random selection in over 20% less time for complex compositions. When generating a percentage of all mutants, EMT finds on average more strong mutants than random selection. This has been confirmed to be statistically significant within a 99.9% confidence interval.ConclusionsEMT has reduced for the three tested compositions the number of mutants required to select those which are useful to derive new test cases that improve the quality of the test suite. The directed search performed by EMT makes it more effective than random selection, especially as compositions become more complex and the search space widens. 
53|10||Mutation testing on an object-oriented framework: An experience report|ContextThe increasing presence of Object-Oriented (OO) programs in industrial systems is progressively drawing the attention of mutation researchers toward this paradigm. However, while the number of research contributions in this topic is plentiful, the number of empirical results is still marginal and mostly provided by researchers rather than practitioners.ObjectiveThis article reports our experience using mutation testing to measure the effectiveness of an automated test data generator from a user perspective.MethodIn our study, we applied both traditional and class-level mutation operators to FaMa, an open source Java framework currently being used for research and commercial purposes. We also compared and contrasted our results with the data obtained from some motivating faults found in the literature and two real tools for the analysis of feature models, FaMa and SPLOT.ResultsOur results are summarized in a number of lessons learned supporting previous isolated results as well as new findings that hopefully will motivate further research in the field.ConclusionWe conclude that mutation testing is an effective and affordable technique to measure the effectiveness of test mechanisms in OO systems. We found, however, several practical limitations in current tool support that should be addressed to facilitate the work of testers. We also missed specific techniques and tools to apply mutation testing at the system level. 
53|10||A logic mutation approach to selective mutation for programs and queries| ContextProgram mutation testing is a technique for measuring and generating high quality test data. However, traditional mutation operators are not necessarily efficient or effective. We address three specific issues. One, test data that kills all mutants generated by current mutation tools can still miss detection of some common logic faults because such tools lack appropriate logic mutation operators. Two, the number of mutants generated is often unnecessarily large. Three, many equivalent mutants can be generated and these can be difficult to eliminate.ObjectiveThis paper explores the idea of addressing these issues by selectively generating only specially engineered subsuming higher order logic mutants. However, such an approach is only useful if a test set that kills all such mutants also kills a high percentage of general mutants.MethodAn empirical study was conducted using a tool that generates only subsuming higher order logic mutants and tools that generate general mutants. Both Java code and SQL were used as the source under test.Results and conclusionsFor both the software and queries, tests killing all the subsuming higher order mutants killed a high percentage of general mutants while reducing both the number of mutants and the number of equivalent mutants. The conclusion is that, for the test subjects studied, subsuming higher order logic mutation is an effective approach to selective mutation for programs and queries. 
53|10||Mutant generation for embedded systems using kernel-based software and hardware fault simulation|ContextMutation testing is a fault-injection-based technique to help testers generate test cases for detecting specific and predetermined types of faults.ObjectiveBefore mutation testing can be effectively applied to embedded systems, traditional mutation testing needs to be modified. To inject a fault into an embedded system without causing any system failure or hardware damage is a challenging task as it requires some knowledge of the underlying layers such as the kernel and the corresponding hardware.MethodWe propose a set of mutation operators for embedded systems using kernel-based software and hardware fault simulation. These operators are designed for software developers so that they can use the mutation technique to test the entire system after the software is integrated with the kernel and hardware devices.ResultsA case study on a programmable logic controller for a digital reactor protection system in a nuclear power plant is conducted. Our results suggest that the proposed mutation operators are useful for fault-injection and this is evidenced by the fact that faults not injected by us were discovered in the subject software as a result of the case study.ConclusionWe conclude that our mutation operators are useful for integration testing of an embedded system. 
53|11|http://www.sciencedirect.com/science/journal/09505849/53/11|Specifying aspect-oriented architectures in AO-ADL|ContextArchitecture description languages (ADLs) are a well-accepted approach to software architecture representation. The majority of well-known ADLs are defined by means of components and connectors. Architectural connectors are mainly used to model interactions among components, specifying component communication and coordination separately. However, there are other properties that cut across several components and also affect component interactions (e.g. security).ObjectiveIt seems reasonable therefore to model how such crosscutting properties affect component interactions as part of connectors.MethodUsing an aspect-oriented approach, the AO-ADL architecture description language extends the classical connector semantics with enough expressiveness to model the influences of such crosscutting properties on component interactions (defined as ‘aspectual compositions’ in connectors).ResultsThis paper describes the AO-ADL language putting special emphasis on the extended connectors used to specify aspectual and non-aspectual compositions between concrete components. The contributions of AO-ADL are validated using concern-oriented metrics available in the literature.ConclusionThe measured indicators show that using AO-ADL it is possible to specify more reusable and scalable software architectures. 
53|11||Data warehouse testing: A prototype-based methodology|ContextTesting is an essential part of the development life-cycle of any software product. While most phases of data warehouse design have received considerable attention in the literature, not much has been written about data warehouse testing.ObjectiveIn this paper we propose a comprehensive approach to testing data warehouse systems. Its main features are earliness with respect to the life-cycle, modularity, tight coupling with design, scalability, and measurability through proper metrics.MethodWe introduce a number of specific testing activities, we classify them in terms of what is tested and how it is tested, and we show how they can be framed within a prototype-based methodology. We apply our approach to a real case study for a large retail company.ResultsThe case study we faced, based on an iterative prototype-based medium-size project, confirmed the validity of our approach. In particular, the main benefits were obtained in terms of project transparency, coordination of the development team, and organization of design activities.ConclusionThough some general-purpose testing techniques can be applied to data warehouse projects, the effectiveness of testing can be largely improved by applying specifically-devised techniques and metrics. 
53|11||Determinants of software quality in offshore development â An empirical study of an Indian vendor|ContextCost advantage has been one of the primary drivers of successful offshoring engagements of Indian software and services companies. However, the emphasis has shifted to the ability of the vendors to provide high quality over cost advantage in delivering software products and services. Meeting high quality requirements of the clients is a challenge due to the very nature of development and delivery of software through offshoring.ObjectiveThe objective of this research paper is to identify and evaluate the key determinants of quality in the case of software projects delivered through offshoring model.MethodA detailed survey was conducted among project managers/project leaders (leads) of a leading midsize Indian IT services company to evaluate the relationship of the determinants on the attributes of quality.ResultsOut of six determinants, our research reveals requirements uncertainty has significant association with all the attributes of quality. While process maturity and trained personnel have moderate association, communication and control, knowledge transfer and integration and technical infrastructure have relatively low association on software quality attributes in the case of offshoring.ConclusionIt is concluded that the complexities in offshoring necessitates proper capturing of requirements. In addition high level of process maturity and availability of trained personnel to the project will help vendors to achieve software quality. The paper provides a set of implications for practice and directions for further research. 
53|11||Path dependent stochastic models to detect planned and actual technology use: A case study of OpenOffice|ContextAdopting IT innovation in organizations is a complex decision process driven by technical, social and economic issues. Thus, those organizations that decide to adopt innovation take a decision of uncertain success of implementation, as the actual use of a new technology might not be the one expected. The misalignment between planned and effective use of innovation is called assimilation gap.ObjectiveThis research aims at defining a quantitative instrument for measuring the assimilation gap and applying it to the case of the adoption of OSS.MethodIn this paper, we use the theory of path dependence and increasing returns of Arthur. In particular, we model the use of software applications (planned or actual) by stochastic processes defined by the daily amounts of files created with the applications. We quantify the assimilation gap by comparing the resulting models by measures of proximity.ResultsWe apply and validate our method to a real case study of introduction of OpenOffice. We have found a gap between the planned and the effective use despite well-defined directives to use the new OS technology. These findings suggest a need of strategy re-calibration that takes into account environmental factors and individual attitudes.ConclusionsThe theory of path dependence is a valid instrument to model the assimilation gap provided information on strategy toward innovation and quantitative data on actual use are available. 
53|11||Quantitative release planning in extreme programming|ContextExtreme Programming (XP) is one of the most popular agile software development methodologies. XP is defined as a consistent set of values and practices designed to work well together, but lacks practices for project management and especially for supporting the customer role. The customer representative is constantly under pressure and may experience difficulties in foreseeing the adequacy of a release plan.ObjectiveTo assist release planning in XP by structuring the planning problem and providing an optimization model that suggests a suitable release plan.MethodWe develop an optimization model that generates a release plan taking into account story size, business value, possible precedence relations, themes, and uncertainty in velocity prediction. The running-time feasibility is established through computational tests. In addition, we provide a practical heuristic approach to velocity estimation.ResultsComputational tests show that problems with up to six themes and 50 stories can be solved exactly. An example provides insight into uncertainties affecting velocity, and indicates that the model can be applied in practice.ConclusionAn optimization model can be used in practice to enable the customer representative to take more informed decisions faster. This can help adopting XP in projects where plan-driven approaches have traditionally been used. 
53|11||The value of software sizing|ContextOne of the difficulties faced by software development Project Managers is estimating the cost and schedule for new projects. Previous industry surveys have concluded that software size and cost estimation is a significant technical area of concern. In order to estimate cost and schedule it is important to have a good understanding of the size of the software product to be developed. There are a number of techniques used to derive software size, with function points being amongst the most documented.ObjectiveIn this paper we explore the utility of function point software sizing techniques when applied to two levels of software requirements documentation in a commercial software development organisation. The goal of the research is to appraise the value (cost/benefit) which functional sizing techniques can bring to the project planning and management of software projects within a small-to-medium sized software development enterprise (SME).MethodFunctional counts were made at the bid and detailed functional specification stages for each of five commercial projects used in the research. Three variants of the NESMA method were used to determine these function counts. Through a structured interview session, feedback on the sizing results was obtained to evaluate its feasibility and potential future contribution to the company.ResultsThe results of our research suggest there is value in performing size estimates at two appropriate stages in the software development lifecycle, with simplified methods providing the optimal return on effort expended.ConclusionThe ‘Estimated NESMA’ is the most appropriate tool for use in size estimation for the company studied. The use of software sizing provides a valuable contribution which would augment, but not replace, the company’s existing cost estimation approach. 
53|11||Guest Editorial: Special Section from 6th Workshop on Advances in Model-Based Testing (A-MOST 2010)|
53|11||Generating asynchronous test cases from test purposes|ContextInput/output transition system (IOTS) models are commonly used when next input can arrive even before outputs are produced. The interaction between the tester and an implementation under test (IUT) is usually assumed to be synchronous. However, as the IUT can produce outputs at any moment, the tester should be prepared to accept all outputs from the IUT, or else be able to block (refuse) outputs of the implementation. Testing distributed, remote applications under the assumptions that communication is synchronous and actions can be blocked is unrealistic, since synchronous communication for such applications can only be achieved if special protocols are used. In this context, asynchronous tests can be more appropriate, reflecting the underlying test architecture which includes queues.ObjectiveIn this paper, we investigate the problem of constructing test cases for given test purposes and specification input/output transition systems, when the communication between the tester and the implementation under test is assumed to be asynchronous, performed via multiple queues.MethodWhen issuing verdicts, asynchronous tests should take into account a distortion caused by the queues in the observed interactions. First, we investigate how the test purpose can be transformed to account for this distortion when there are a single input queue and a single output queue. Then, we consider a more general problem, when there may be multiple queues.ResultsWe propose an algorithm which constructs a sound test case, by transforming the test purpose prior to composing it with the specification without queues.ConclusionThe proposed algorithm mitigates the state explosion problem which usually occurs when queues are directly involved in the composition. Experimental results confirm the resulting state space reduction. 
53|11||Generating minimal fault detecting test suites for general Boolean specifications|ContextBoolean expressions are a central aspect of specifications and programs, but they also offer dangerously many ways to introduce faults. To counter this effect, various criteria to generate and evaluate tests have been proposed. These are traditionally based on the structure of the expressions, but are not directly related to the possible faults. Often, they also require expressions to be in particular formats such as disjunctive normal form (DNF), where a strict hierarchy of faults is available to prove fault detection capability.ObjectiveThis paper describes a method that generates test cases directly from an expression’s possible faults, guaranteeing that faults of any chosen class will be detected. In contrast to many previous criteria, this approach does not require the Boolean expressions to be in DNF, but allows expressions in any format, using any deliberate fault classes.MethodThe presented approach is based on creating test objectives for individual faults, such that efficient, modern satisfiability solvers can be used to derive test cases that directly address the faults. Although the number of such test objectives can be high depending on the considered fault classes, a number of optimizations can be applied to reduce the test generation effort.ResultsEvaluation on a set of commonly used benchmarks shows that despite guaranteeing fault coverage, the number of test cases can be reduced even further than that produced by other state of the art strategies. At the same time, the fault detection capability is not affected negatively, and clearly improves over state of the art criteria for general form Boolean expressions.ConclusionThe approach presented in this paper is shown to improve over the state of the art with respect to the types of expressions that can be handled, the fault classes that are guaranteed to be covered, and the sizes of test suites generated automatically. This has implications for several fields of software testing: A main application is specification based testing, but Boolean expressions also exist in normal source code and need to be tested there as well. 
53|12|http://www.sciencedirect.com/science/journal/09505849/53/12|Static and dynamic adaptations for service-based systems|ContextIn service-oriented computing (SOC), service providers publish reusable services, and service consumers subscribe them. However, there exist potential problems in reusing services. Mismatch is a problem that occurs when a candidate service does not fully match to the feature expected. Fault is a problem that occurs when an invocation of services results in some abnormality at runtime. Without remedying mismatch problems, services would not be reusable. Without remedying fault problems, service invocations at runtime would result in failures. Static and dynamic adaptations are practical approaches to remedying the problems.ObjectiveOur objective is to define a comprehensive framework which includes a design of service adaptation framework (SAF), and design of static and dynamic adapters.MethodWe design the SAF which governs dynamic adaptations, and define a service life-cycle with adaptation-related activities. Based on causal–effect relationships among mismatch, fault, cause, and adapter, we derive mismatches and faults, from which their relevant causes are identified. For the causes, we define six static adapters and five dynamic adapters. We specify instructions for designing static adapters, and provide step-wise algorithms for designing dynamic adapters based on enterprise service bus (ESB). And, we show a proof-of-concept (POC) of implementation to show applicability of the methods.ResultsThe paper presents service life-cycle with adaptation-related activities, SAF design, and design of static and dynamic adapters.ConclusionMismatch and fault problems in utilizing services present threats to high reusability of services. Static adaptations can remedy mismatch problems, and dynamic adaptations can remedy fault problems. In this paper, we presented technical insights of service adaption, SAF design, and definitions of static and dynamic adapters. By utilizing the proposed SAF and service adapters, reusability of services can be greatly enhanced. 
53|12||An integrated search-based approach for automatic testing from extended finite state machine (EFSM) models|ContextThe extended finite state machine (EFSM) is a modelling approach that has been used to represent a wide range of systems. When testing from an EFSM, it is normal to use a test criterion such as transition coverage. Such test criteria are often expressed in terms of transition paths (TPs) through an EFSM. Despite the popularity of EFSMs, testing from an EFSM is difficult for two main reasons: path feasibility and path input sequence generation. The path feasibility problem concerns generating paths that are feasible whereas the path input sequence generation problem is to find an input sequence that can traverse a feasible path.ObjectiveWhile search-based approaches have been used in test automation, there has been relatively little work that uses them when testing from an EFSM. In this paper, we propose an integrated search-based approach to automate testing from an EFSM.MethodThe approach has two phases, the aim of the first phase being to produce a feasible TP (FTP) while the second phase searches for an input sequence to trigger this TP. The first phase uses a Genetic Algorithm whose fitness function is a TP feasibility metric based on dataflow dependence. The second phase uses a Genetic Algorithm whose fitness function is based on a combination of a branch distance function and approach level.ResultsExperimental results using five EFSMs found the first phase to be effective in generating FTPs with a success rate of approximately 96.6%. Furthermore, the proposed input sequence generator could trigger all the generated feasible TPs (success rate = 100%).ConclusionThe results derived from the experiment demonstrate that the proposed approach is effective in automating testing from an EFSM. 
53|12||A comparative study of challenges in integrating Open Source Software and Inner Source Software|ContextSeveral large software-developing organizations have adopted Open Source Software development (OSSD) practices to develop in-house components that are subsequently integrated into products. This phenomenon is also known as “Inner Source”. While there have been several reports of successful cases of this phenomenon, little is known about the challenges that practitioners face when integrating software that is developed in such a setting.ObjectiveThe objective of this study was to shed light on challenges related to building products with components that have been developed within an Inner Source development environment.MethodFollowing an initial systematic literature review to generate seed category data constructs, we performed an in-depth exploratory case study in an organization that has a significant track record in the implementation of Inner Source. Data was gathered through semi-structured interviews with participants from a range of divisions across the organization. Interviews were transcribed and analyzed using qualitative data analysis techniques.ResultsWe have identified a number of challenges and approaches to address them, and compared the findings to challenges related to development with OSS products reported in the literature. We found that many challenges identified in the case study could be mapped to challenges related to integration of OSS.ConclusionThe results provide important insights into common challenges of developing with OSS and Inner Source and may help organizations to understand how to improve their software development practices by adopting certain OSSD practices. The findings also identify the areas that need further research. 
53|12||Automating image segmentation verification and validation by learning test oracles|An image segmentation algorithm delineates (an) object(s) of interest in an image. Its output is referred to as a segmentation. Developing these algorithms is a manual, iterative process involving repetitive verification and validation tasks. This process is time-consuming and depends on the availability of experts, who may be a scarce resource (e.g., medical experts). We propose a framework referred to as Image Segmentation Automated Oracle (ISAO) that uses machine learning to construct an oracle, which can then be used to automatically verify the correctness of image segmentations, thus saving substantial resources and making the image segmentation verification and validation task significantly more efficient. The framework also gives informative feedback to the developer as the segmentation algorithm evolves and provides a systematic means of testing different parametric configurations of the algorithm. During the initial learning phase, segmentations from the first few (optimally two) versions of the segmentation algorithm are manually verified by experts. The similarity of successive segmentations of the same images is also measured in various ways. This information is then fed to a machine learning algorithm to construct a classifier that distinguishes between consistent and inconsistent segmentation pairs (as determined by an expert) based on the values of the similarity measures associated with each segmentation pair. Once the accuracy of the classifier is deemed satisfactory to support a consistency determination, the classifier is then used to determine whether the segmentations that are produced by subsequent versions of the algorithm under test, are (in)consistent with already verified segmentations from previous versions. This information is then used to automatically draw conclusions about the correctness of the segmentations. We have successfully applied this approach to 3D segmentations of the cardiac left ventricle obtained from CT scans and have obtained promising results (accuracies of 95%). Even though more experiments are needed to quantify the effectiveness of the approach in real-world applications, ISAO shows promise in increasing the quality and testing efficiency of image segmentation algorithms. 
53|12||Taking context into account in conceptual models using a Model Driven Engineering approach|ContextIn public transport, travelers (considered as information systems users) do not have the same objectives and/or concerns at the same time. For this reason it is not always easy to provide them with the right information at the right time. If personalizing the information to the user allows to do this to some extend it is not enough since the information could also depend on the use of the context and the environment (e.g., place, time, etc.).ObjectiveThis paper proposes a solution allowing the context to be managed inside an application’s conceptual models in order to provide more flexible web applications from the user’s point of view.MethodOur work is based on a modeling method using the Model Driven Engineering (MDE) approach and on practical field experiences permitting us to validate our solution. Our domain of application is personalized transport information.ResultsIntroducing the notion of context into rules and decision trees that are used inside conceptual models allowed us to incorporate context as important information for personalizing web applications.ConclusionThe context should be integrated into an application during the modeling phase to allow a smooth integration inside the application and to facilitate the evolution over time. Our solution offers a better user’s experience through an extended personalization of web applications. 
53|12||A relaxable service selection algorithm for QoS-based web service composition| ContextWeb Services are emerging technologies that enable application to application communication and reuse of autonomous services over Web. Composition of web services is a concept of integrating individual web services to conduct complex business transactions based on functionality and performance constraintsObjectiveTo satisfy user requirements, technologies of Quality of service (QoS)-based web service composition (QWSC) are widely used to build complex applications by discovering the best-fit web services in term of QoS.MethodIn this paper, a QoS-based service selection (RQSS) algorithm is proposed to help composite web application development by discovering feasible web services based on functionalities and QoS criteria of user requirements. The RQSS recommends prospective service candidates to users by relaxing QoS constraints if no suitable or available web service could exactly fulfill user requirements.ResultsA generic framework is implemented to demonstrate the feasibility and performance of RQSS by adapting WS-BPEL standards, and can be reused for QoS-based web composition applications.ConclusionThe experimental results show that the RQSS algorithm indeed performs well and increases the system availability and reliability. 
53|12||Contrasting ideal and realistic conditions as a means to improve judgment-based software development effort estimation|ContextThe effort estimates of software development work are on average too low. A possible reason for this tendency is that software developers, perhaps unconsciously, assume ideal conditions when they estimate the most likely use of effort. In this article, we propose and evaluate a two-step estimation process that may induce more awareness of the difference between idealistic and realistic conditions and as a consequence more realistic effort estimates. The proposed process differs from traditional judgment-based estimation processes in that it starts with an effort estimation that assumes ideal conditions before the most likely use of effort is estimated.ObjectiveThe objective of the paper is to examine the potential of the proposed method to induce more realism in the judgment-based estimates of work effort.MethodThree experiments with software professionals as participants were completed. In all three experiments there was one group of participants which followed the proposed and another group which followed the traditional estimation process. In one of the experiments there was an additional group which started with a probabilistically defined estimate of minimum effort before estimating the most likely effort.ResultsWe found, in all three experiments, that estimation of most likely effort seems to assume rather idealistic assumptions and that the use of the proposed process seems to yield more realistic effort estimates. In contrast, starting with an estimate of the minimum effort, rather than an estimate based on ideal conditions, did not have the same positive effect on the subsequent estimate of the most likely effort.ConclusionThe empirical results from our studies together with similar results from other domains suggest that the proposed estimation process is promising for the improvement of the realism of software development effort estimates. 
53|12||Assessing the influence of stereotypes on the comprehension of UML sequence diagrams: A family of experiments|ContextThe conventional wisdom states that stereotypes are used to clarify or extend the meaning of model elements and consequently should be helpful in comprehending the diagram semantics.ObjectiveThe main goal of this work is to present a family of experiments that we have carried out to investigate whether the use of stereotypes improves the comprehension of UML sequence diagrams.MethodThe family of experiments consists of an experiment and two replications carried out with 78, 29 and 36 undergraduate Computer Science students, respectively. The comprehension of UML sequence diagrams with and without stereotypes was analyzed from three different perspectives borrowed from the Cognitive Theory of Multimedia Learning (CTML): semantic comprehension, retention and transfer. In addition, we carried out a meta-analysis study to integrate the different data samples.ResultsThe statistical analysis and meta-analysis of the data obtained from each experiment separately indicates that the use of the proposed stereotypes helps improving the comprehension of the diagrams, especially when the subjects are not familiar with the domain.ConclusionsThe set of stereotypes presented in this work seem to be helpful for a better comprehension of UML sequence diagrams, especially with not well-known domains. Although further research is necessary for strengthening these results, introducing these stereotypes both in academia and industry could be an interesting practice for checking the validity of the results. 
53|2|http://www.sciencedirect.com/science/journal/09505849/53/2|Components meet aspects: Assessing design stability of a software product line|ContextIt is important for Product Line Architectures (PLA) to remain stable accommodating evolutionary changes of stakeholder’s requirements. Otherwise, architectural modifications may have to be propagated to products of a product line, thereby increasing maintenance costs. A key challenge is that several features are likely to exert a crosscutting impact on the PLA decomposition, thereby making it more difficult to preserve its stability in the presence of changes. Some researchers claim that the use of aspects can ameliorate instabilities caused by changes in crosscutting features. Hence, it is important to understand which aspect-oriented (AO) and non-aspect-oriented techniques better cope with PLA stability through evolution.ObjectiveThis paper evaluates the positive and negative change impact of component and aspect based design on PLAs. The objective of the evaluation is to assess how aspects and components promote PLA stability in the presence of various types of evolutionary change. To support a broader analysis, we also evaluate the PLA stability of a hybrid approach (i.e. combined use of aspects and components) against the isolated use of component-based, OO, and AO approaches.MethodAn quantitative and qualitative analysis of PLA stability which involved four different implementations of a PLA: (i) an OO implementation, (ii) an AO implementation, (iii) a component-based implementation, and (iv) a hybrid implementation where both components and aspects are employed. Each implementation has eight releases and they are functionally equivalent. We used conventional metrics suites for change impact and modularity to measure the architecture stability evaluation of the 4 implementations.ResultsThe combination of aspects and components promotes superior PLA resilience than the other PLAs in most of the circumstances.ConclusionIt is concluded that the combination of aspects and components supports the design of high cohesive and loosely coupled PLAs. It also contributes to improve modularity by untangling feature implementation. 
53|2||Assessing PSP effect in training disciplined software development: A PlanâTrackâReview model|ContextIn training disciplined software development, the PSP is said to result in such effect as increased estimation accuracy, better software quality, earlier defect detection, and improved productivity. But a systematic mechanism that can be easily adopted to assess and interpret PSP effect is scarce within the existing literature.ObjectiveThe purpose of this study is to explore the possibility of devising a feasible assessment model that ties up critical software engineering values with the pertinent PSP metrics.MethodA systematic review of the literature was conducted to establish such an assessment model (we called a Plan–Track–Review model). Both mean and median approaches along with a set of simplified procedures were used to assess the commonly accepted PSP training effects. A set of statistical analyses further followed to increase understanding of the relationships among the PSP metrics and to help interpret the application results.ResultsBased on the results of this study, PSP training effect on the controllability, manageability, and reliability of a software engineer is quite positive and largely consistent with the literature. However, its effect on one’s predictability on project in general (and on project size in particular) is not implied as said in the literature. As for one’s overall project efficiency, our results show a moderate improvement. Our initial finding also suggests that a prior stage PSP effect could have an impact on later stage training outcomes.ConclusionIt is concluded that this Plan–Track–Review model with the associated framework can be used to assess PSP effect regarding a disciplined software development. The generated summary report serves to provide useful feedback for both PSP instructors and students based on internal as well as external standards. 
53|2||Discriminative effect of user influence and user responsibility on information system development processes and project management|ContextUser participation in information system (IS) development has received much research attention. However, prior empirical research regarding the effect of user participation on IS success is inconclusive. This might be because previous studies overlook the effect of the particular components of user participation and other possible mediating factors.ObjectiveThe objective of this study is to empirically examine how user influence and user responsibility affect IS project performance. We inspect whether user influence and user responsibility improve the quality of the IS development process and in turn leads to project success, or if they have a direct positive influence on project success.MethodWe conducted a survey of 151 IS project managers in order to understand the impact of user influence and user responsibility on IS project performance. Regression analysis was conducted to assess the relationship among user influence, user responsibility, organizational technology learning, project control, user–developer interaction, and IS project management performance.ResultsThis study shows that user responsibility and user influence have a positive effect on project performance through the promotion of IS development processes as mediators, including organizational technology learning, project control, and user–IS interaction.ConclusionOur results suggest that user responsibility and user influence respectively play an important role in indirectly and directly impacting project management performance. Results of the analysis imply that organizations and project managers should use both user participation and user influence to improve processes performance, and in turn, increase project success. 
53|2||A unit test approach for database schema evolution|ContextThe constant changes in today’s business requirements demand continuous database revisions. Hence, database structures, not unlike software applications, deteriorate during their lifespan and thus require refactoring in order to achieve a longer life span. Although unit tests support changes to application programs and refactoring, there is currently a lack of testing strategies for database schema evolution.ObjectiveThis work examines the challenges for database schema evolution and explores the possibility of using various testing strategies to assist with schema evolution. Specifically, the work proposes a novel unit test approach for the application code that accesses databases with the objective of proactively evaluating the code against the altered database.MethodThe approach was validated through the implementation of a testing framework in conjunction with a sample application and a relatively simple database schema. Although the database schema in this study was simple, it was nevertheless able to demonstrate the advantages of the proposed approach.ResultsAfter changes in the database schema, the proposed approach found all SELECT statements as well as the majority of other statements requiring modifications in the application code. Due to its efficiency with SELECT statements, the proposed approach is expected to be more successful with database warehouse applications where SELECT statements are dominant.ConclusionThe unit test approach that accesses databases has proven to be successful in evaluating the application code against the evolved database. In particular, the approach is simple and straightforward to implement, which makes it easily adoptable in practice. 
53|3|http://www.sciencedirect.com/science/journal/09505849/53/3|Testing in Service Oriented Architectures with dynamic binding: A mapping study|ContextService Oriented Architectures (SOA) have emerged as a new paradigm to develop interoperable and highly dynamic applications.ObjectiveThis paper aims to identify the state of the art in the research on testing in Service Oriented Architectures with dynamic binding.MethodA mapping study has been performed employing both manual and automatic search in journals, conference/workshop proceedings and electronic databases.ResultsA total of 33 studies have been reviewed in order to extract relevant information regarding a previously defined set of research questions. The detection of faults and the decision making based on the information gathered from the tests have been identified as the main objectives of these studies. To achieve these goals, monitoring and test case generation are the most proposed techniques testing both functional and non-functional properties. Furthermore, different stakeholders have been identified as participants in the tests, which are performed in specific points in time during the life cycle of the services. Finally, it has been observed that a relevant group of studies have not validated their approach yet.ConclusionsAlthough we have only found 33 studies that address the testing of SOA where the discovery and binding of the services are performed at runtime, this number can be considered significant due to the specific nature of the reviewed topic. The results of this study have contributed to provide a body of knowledge that allows identifying current gaps in improving the quality of the dynamic binding in SOA using testing approaches. 
53|3||An effective sequential statistical test for probabilistic monitoring|ContextA monitor checks if a system behaves according to a specified property at runtime. This is required for quality assurance purposes. Currently several approaches exist to monitor standard and real-time properties. However, a current challenge is to provide a comprehensive approach for monitoring probabilistic properties, as they are used to formulate quality of service requirements like performance, reliability, safety, and availability. The main problem of these probabilistic properties is that there is no binary acceptance condition.ObjectiveTo overcome this problem, this article presents an improved and generic statistical decision procedure based on acceptance sampling and sequential hypothesis testing.MethodThe developed decision procedure is validated using several experiments that determine the operating characteristic, runtime overhead as well as the expected sample sizes.Results and conclusionThe experimental validation provides evidence that the developed testing procedure reduces the runtime overhead and improves the accuracy of classification. Thus, the statistical decision procedure is superior to the existing statistical tests currently used in probabilistic monitoring. 
53|3||Simplifying effort estimation based on Use Case Points|ContextThe Use Case Points (UCP) method can be used to estimate software development effort based on a use-case model and two sets of adjustment factors relating to the environmental and technical complexity of a project. The question arises whether all of these components are important from the effort estimation point of view.ObjectiveThis paper investigates the construction of UCP in order to find possible ways of simplifying it.MethodThe cross-validation procedure was used to compare the accuracy of the different variants of UCP (with and without the investigated simplifications). The analysis was based on data derived from a set of 14 projects for which effort ranged from 277 to 3593 man-hours. In addition, the factor analysis was performed to investigate the possibility of reducing the number of adjustment factors.ResultsThe two variants of UCP – with and without unadjusted actor weights (UAW) provided similar prediction accuracy. In addition, a minor influence of the adjustment factors on the accuracy of UCP was observed. The results of the factor analysis indicated that the number of adjustment factors could be reduced from 21 to 6 (2 environmental factors and 4 technical complexity factors). Another observation was made that the variants of UCP calculated based on steps were slightly more accurate than the variants calculated based on transactions. Finally, a recently proposed use-case-based size metric TTPoints provided better accuracy than any of the investigated variants of UCP.ConclusionThe observation in this study was that the UCP method could be simplified by rejecting UAW; calculating UCP based on steps instead of transactions; or just counting the total number of steps in use cases. Moreover, two recently proposed use-case-based size metrics Transactions and TTPoints could be used as an alternative to UCP to estimate effort at the early stages of software development. 
53|3||Empirical extension of a classification framework for addressing consistency in model based development|ContextConsistency constitutes an important aspect in practical realization of modeling ideas in the process of software development and in the related research which is diverse. A classification framework has been developed, in order to aid the model based software construction by categorizing research problems related to consistency. However, the framework does not include information on the importance of classification elements.ObjectiveThe aim was to extend the classification framework with information about the relative importance of the elements constituting the classification. The research question was how to express and obtain this information.MethodA survey was conducted on a sample of 24 stakeholders from academia and industry, with different roles, who answered a quantitative questionnaire. Specifically, the respondents prioritized perspectives and issues using an extended hierarchical voting scheme based on the hundred dollar test. The numerical data obtained were first weighted and normalized and then they were analyzed by descriptive statistics and bar charts.ResultsThe detailed analysis of the data revealed the relative importance of consistency perspectives and issues under different views, allowing for the desired extension of the classification framework with empirical information. The most highly valued issues come from the pragmatics perspective. These issues are the most important for tool builders and practitioners from industry, while for the responders from academia theory group some issues from the concepts perspective are equally important.ConclusionThe method of using empirical data from a hierarchical cumulative voting scheme for extending existing research classification framework is useful for including information regarding the importance of the classification elements. 
53|3||A two-stage framework for UML specification matching|ContextSpecification matching techniques are crucial for effective retrieval processes. Despite the prevalence for object-oriented methodologies, little attention has been given to Unified Modeling Language (UML) for matching.ObjectiveThis paper presents a two-stage framework for matching two UML specifications and quantifying the results based on the systematic integration of their structural and behavioral similarities in order to identify the candidate component set for reuse.MethodThe first stage in the framework is an evaluation of the similarities between UML class diagrams using the Structure-Mapping Engine (SME), a simulation of the analogical reasoning approach known as the structure-mapping theory. The second stage, performed on the components identified in the first stage, is based on a graph-similarity scoring algorithm in which UML class diagrams and sequence diagrams are transformed into an SME representation and a Message-Object-Order Graph (MOOG). The effectiveness of the proposed framework was evaluated using a case study.ResultsThe experimental results showed a reduction in potential mismatches and an overall high precision and recall.ConclusionIt is concluded that the two-stage framework is capable of performing more precise matching compared to those of other single-stage matching frameworks. Moreover, the two-stage framework could be utilized within a reuse process, bypassing the need for extra information for retrieval of the components described by UML. 
53|3||Automated metamorphic testing on the analyses of feature models|ContextA feature model (FM) represents the valid combinations of features in a domain. The automated extraction of information from FMs is a complex task that involves numerous analysis operations, techniques and tools. Current testing methods in this context are manual and rely on the ability of the tester to decide whether the output of an analysis is correct. However, this is acknowledged to be time-consuming, error-prone and in most cases infeasible due to the combinatorial complexity of the analyses, this is known as the oracle problem.ObjectiveIn this paper, we propose using metamorphic testing to automate the generation of test data for feature model analysis tools overcoming the oracle problem. An automated test data generator is presented and evaluated to show the feasibility of our approach.MethodWe present a set of relations (so-called metamorphic relations) between input FMs and the set of products they represent. Based on these relations and given a FM and its known set of products, a set of neighbouring FMs together with their corresponding set of products are automatically generated and used for testing multiple analyses. Complex FMs representing millions of products can be efficiently created by applying this process iteratively.ResultsOur evaluation results using mutation testing and real faults reveal that most faults can be automatically detected within a few seconds. Two defects were found in FaMa and another two in SPLOT, two real tools for the automated analysis of feature models. Also, we show how our generator outperforms a related manual suite for the automated analysis of feature models and how this suite can be used to guide the automated generation of test cases obtaining important gains in efficiency.ConclusionOur results show that the application of metamorphic testing in the domain of automated analysis of feature models is efficient and effective in detecting most faults in a few seconds without the need for a human oracle. 
53|3||Formal model for assigning human resources to teams in software projects|ContextHuman resources play a critical role in software project success. However, people are still the least formalized factor in today’s process models. Generally, people are assigned to roles and project teams are formed on the basis of project leaders’ experience of people, constraints (e.g. availability) and skill requirements. Yet this process has to take multiple factors into account. Few works in the literature model this process. Most of these are informal proposals focusing on the individual assignment of people to project tasks and do not consider other aspects like team formation as a whole.ObjectiveIn this paper we formulate a formal model for assigning human resources to software project teams. Additionally, we describe the key results of the knowledge management process enacted to output the elements of the model.MethodThe model elements were identified using the Delphi expert consultation method and applying psychological tests. The proposed model was implemented in a software tool and validated on two software development organization assignment scenarios.ResultsWe built a formal model for the process of assigning human resources to software project teams. This model takes into account as many factors as possible and aids the assignment of individuals to project roles, as well as the formation of the team as a whole.We found that the rules that were identified to form software development project teams are useful. From the tests we found that model implementation was feasible (all the executions of the implemented problem-solving algorithms output feasible solutions in response times that can be considered as acceptable).ConclusionUsing the Delphi method we were able to propose software project roles and competences. Psychological tests and data mining tools identified useful rules for forming software project teams. These were used to build a formal model. This model was built into a tool that returns role assignments in acceptable response times. This decision support tool helps managers assign people to roles and to form teams. Using the tool, project leaders can flexibly evaluate different team make-ups, taking into account several factors, as well as different constraints and objectives. 
53|3||Agile methods rapidly replacing traditional methods at Nokia: A survey of opinions on agile transformation|ContextMany organizations have started to deploy agile methods, but so far there exist only a few studies on organization-wide transformations. Are agile methods here to stay? Some claim that agile software development methods are in the mainstream adoption phase in the software industry, while others hope that those are a passing fad. The assumption here is that if agile would not provide real improvement, adopters would be eager at first but turn pessimistic after putting it into practice.ObjectiveDespite the growing amount of anecdotal evidence on the success of agile methods across a wide range of different real-life development settings, scientific studies remain scarce. Even less is known about the perception of the impacts of agile transformation when it is deployed in a very large software development environment, and whether agile methods are here to stay. This study aims to fill that gap by providing evidence from a large-scale agile transformation within Nokia. While we have yet to confirm these findings with solid quantitative data, we believe that the perception of the impacts already pinpoints the direction of the impacts of large-scale agile transformation.MethodThe data were collected using a questionnaire. The population of the study contains more than 1000 respondents in seven different countries in Europe, North America, and Asia.ResultsThe results reveal that most respondents agree on all accounts with the generally claimed benefits of agile methods. These benefits include higher satisfaction, a feeling of effectiveness, increased quality and transparency, increased autonomy and happiness, and earlier detection of defects. Finally, 60% of respondents would not like to return to the old way of working.ConclusionWhile the perception of the impact of agile methods is predominantly positive, several challenge areas were discovered. However, based on this study, agile methods are here to stay. 
53|3||Corrigendum to âA novel composite model approach to improve software quality predictionâ [Information and Software Technology 52 (12) (2010) 1298â1311]|
53|4|http://www.sciencedirect.com/science/journal/09505849/53/4|Special section editorial â Software Engineering track of the 24th Annual Symposium on Applied Computing|
53|4||A test-driven approach to code search and its application to the reuse of auxiliary functionality|ContextSoftware developers spend considerable effort implementing auxiliary functionality used by the main features of a system (e.g., compressing/decompressing files, encryption/decription of data, scaling/rotating images). With the increasing amount of open source code available on the Internet, time and effort can be saved by reusing these utilities through informal practices of code search and reuse. However, when this type of reuse is performed in an ad hoc manner, it can be tedious and error-prone: code results have to be manually inspected and integrated into the workspace.ObjectiveIn this paper we introduce and evaluate the use of test cases as an interface for automating code search and reuse. We call our approach Test-Driven Code Search (TDCS). Test cases serve two purposes: (1) they define the behavior of the desired functionality to be searched; and (2) they test the matching results for suitability in the local context. We also describe CodeGenie, an Eclipse plugin we have developed that performs TDCS using a code search engine called Sourcerer.MethodOur evaluation consists of two studies: an applicability study with 34 different features that were searched using CodeGenie; and a performance study comparing CodeGenie, Google Code Search, and a manual approach.ResultsBoth studies present evidence of the applicability and good performance of TDCS in the reuse of auxiliary functionality.ConclusionThis paper presents an approach to source code search and its application to the reuse of auxiliary functionality. Our exploratory evaluation shows promising results, which motivates the use and further investigation of TDCS. 
53|4||Pattern-based framework for modularized software development and evolution robustness|ContextSoftware development is now facing much more challenges than ever before due to the intrinsic high complexity and the increasing demands of the quick-service-ready paradigm.ObjectiveAs the developers are now called for more quality software systems from the industries, there is insufficient guidance from the methodologies and standards of software engineering that can provide assistance to the rapid development of qualified business software.MethodIn this work, we discuss the advantages of the pattern-based software development. We verify the benefits using a pattern-based software framework called OS2F, and a corresponding system design architecture that is intended for the rapid development of web applications.ResultsThe objective of the framework/architecture is that, through software patterns, developers should be able to separate the work of system development from the business rules so as to reduce the problems caused by a developer’s lack of business experiences.ConclusionThrough a suitable pattern-based software framework, the quality of the product can thus be enhanced, software development time and cost decreased, and software evolution robustness improved. 
53|4||Measuring and predicting software productivity: A systematic map and review|ContextSoftware productivity measurement is essential in order to control and improve the performance of software development. For example, by identifying role models (e.g. projects, individuals, tasks) when comparing productivity data. The prediction is of relevance to determine whether corrective actions are needed, and to discover which alternative improvement action would yield the best results.ObjectiveIn this study we identify studies for software productivity prediction and measurement. Based on the identified studies we first create a classification scheme and map the studies into the scheme (systematic map). Thereafter, a detailed analysis and synthesis of the studies is conducted.MethodAs a research method for systematically identifying and aggregating the evidence of productivity measurement and prediction approaches systematic mapping and systematic review have been used.ResultsIn total 38 studies have been identified, resulting in a classification scheme for empirical research on software productivity. The mapping allowed to identify the rigor of the evidence with respect to the different productivity approaches. In the detailed analysis the results were tabulated and synthesized to provide recommendations to practitioners.ConclusionRisks with simple ratio-based measurement approaches were shown. In response to the problems data envelopment analysis seems to be a strong approach to capture multivariate productivity measures, and allows to identify reference projects to which inefficient projects should be compared. Regarding simulation no general prediction model can be identified. Simulation and statistical process control are promising methods for software productivity prediction. Overall, further evidence is needed to make stronger claims and recommendations. In particular, the discussion of validity threats should become standard, and models need to be compared with each other. 
53|4||A systematic review of evaluation of variability management approaches in software product lines|ContextVariability management (VM) is one of the most important activities of software product-line engineering (SPLE), which intends to develop software-intensive systems using platforms and mass customization. VM encompasses the activities of eliciting and representing variability in software artefacts, establishing and managing dependencies among different variabilities, and supporting the exploitation of the variabilities for building and evolving a family of software systems. Software product line (SPL) community has allocated huge amount of effort to develop various approaches to dealing with variability related challenges during the last two decade. Several dozens of VM approaches have been reported. However, there has been no systematic effort to study how the reported VM approaches have been evaluated.ObjectiveThe objectives of this research are to review the status of evaluation of reported VM approaches and to synthesize the available evidence about the effects of the reported approaches.MethodWe carried out a systematic literature review of the VM approaches in SPLE reported from 1990s until December 2007.ResultsWe selected 97 papers according to our inclusion and exclusion criteria. The selected papers appeared in 56 publication venues. We found that only a small number of the reviewed approaches had been evaluated using rigorous scientific methods. A detailed investigation of the reviewed studies employing empirical research methods revealed significant quality deficiencies in various aspects of the used quality assessment criteria. The synthesis of the available evidence showed that all studies, except one, reported only positive effects.ConclusionThe findings from this systematic review show that a large majority of the reported VM approaches have not been sufficiently evaluated using scientifically rigorous methods. The available evidence is sparse and the quality of the presented evidence is quite low. The findings highlight the areas in need of improvement, i.e., rigorous evaluation of VM approaches. However, the reported evidence is quite consistent across different studies. That means the proposed approaches may be very beneficial when they are applied properly in appropriate situations. Hence, it can be concluded that further investigations need to pay more attention to the contexts under which different approaches can be more beneficial. 
53|4||A systematic literature review of actionable alert identification techniques for automated static code analysis|ContextAutomated static analysis (ASA) identifies potential source code anomalies early in the software development lifecycle that could lead to field failures. Excessive alert generation and a large proportion of unimportant or incorrect alerts (unactionable alerts) may cause developers to reject the use of ASA. Techniques that identify anomalies important enough for developers to fix (actionable alerts) may increase the usefulness of ASA in practice.ObjectiveThe goal of this work is to synthesize available research results to inform evidence-based selection of actionable alert identification techniques (AAIT).MethodRelevant studies about AAITs were gathered via a systematic literature review.ResultsWe selected 21 peer-reviewed studies of AAITs. The techniques use alert type selection; contextual information; data fusion; graph theory; machine learning; mathematical and statistical models; or dynamic detection to classify and prioritize actionable alerts. All of the AAITs are evaluated via an example with a variety of evaluation metrics.ConclusionThe selected studies support (with varying strength), the premise that the effective use of ASA is improved by supplementing ASA with an AAIT. Seven of the 21 selected studies reported the precision of the proposed AAITs. The two studies with the highest precision built models using the subject program’s history. Precision measures how well a technique identifies true actionable alerts out of all predicted actionable alerts. Precision does not measure the number of actionable alerts missed by an AAIT or how well an AAIT identifies unactionable alerts. Inconsistent use of evaluation metrics, subject programs, and ASAs in the selected studies preclude meta-analysis and prevent the current results from informing evidence-based selection of an AAIT. We propose building on an actionable alert identification benchmark for comparison and evaluation of AAIT from literature on a standard set of subjects and utilizing a common set of evaluation metrics. 
53|4||An ant colony optimization algorithm to improve software quality prediction models: Case of class stability|ContextAssessing software quality at the early stages of the design and development process is very difficult since most of the software quality characteristics are not directly measurable. Nonetheless, they can be derived from other measurable attributes. For this purpose, software quality prediction models have been extensively used. However, building accurate prediction models is hard due to the lack of data in the domain of software engineering. As a result, the prediction models built on one data set show a significant deterioration of their accuracy when they are used to classify new, unseen data.ObjectiveThe objective of this paper is to present an approach that optimizes the accuracy of software quality predictive models when used to classify new data.MethodThis paper presents an adaptive approach that takes already built predictive models and adapts them (one at a time) to new data. We use an ant colony optimization algorithm in the adaptation process. The approach is validated on stability of classes in object-oriented software systems and can easily be used for any other software quality characteristic. It can also be easily extended to work with software quality predictive problems involving more than two classification labels.ResultsResults show that our approach out-performs the machine learning algorithm C4.5 as well as random guessing. It also preserves the expressiveness of the models which provide not only the classification label but also guidelines to attain it.ConclusionOur approach is an adaptive one that can be seen as taking predictive models that have already been built from common domain data and adapting them to context-specific data. This is suitable for the domain of software quality since the data is very scarce and hence predictive models built from one data set is hard to generalize and reuse on new data. 
53|4||Problems in the interplay of development and IT operations in system development projects: A Delphi study of Norwegian IT experts|ContextThe assumption of the presented work is that the ability of system developers and IT operations personnel to cooperate effectively in system development projects has great impact on the quality of the final system solution, as well as on the service level of its subsequent operation.ObjectiveThe present research explores the interplay of system development and IT operations and the challenges they are meeting. We are especially interested in identifying problems encountered between these two parties in system development projects.MethodWe identify and rank problems by using a ranking-type Delphi study. We involved 42 Norwegian IT experts and split them into three expert panels: system developers, IT operations personnel and system owners. We then guided them through the three phases of the Delphi method – brainstorming, reduction and ranking.ResultsA comprehensive list of 66 problems, organized into seven groups, is compiled. Through a selection and ranking procedure, the panels found the following to be the six most serious problems in the interplay of system development and IT operations: (1) IT operations not being involved in the requirements specification; (2) poor communication and information flow; (3) unsatisfactory test environments; (4) lack of knowledge transfer; (5) systems being put into production before they are complete; and (6) operational routines not being established prior to deployment.ConclusionThe sheer amount and variety of problems mentioned and the respondents’ explanations confirm that this interplay needs attention; the parties agree that they do not cooperate effectively in development projects. The results imply that IT operations should be regarded as an important stakeholder throughout several systems development activities, especially requirements analysis, testing and deployment. Moreover, such involvement should be facilitated by an increased focus on enhancing cooperation and communication. 
53|5|http://www.sciencedirect.com/science/journal/09505849/53/5|A systematic mapping study of software product lines testing|ContextIn software development, Testing is an important mechanism both to identify defects and assure that completed products work as specified. This is a common practice in single-system development, and continues to hold in Software Product Lines (SPL). Even though extensive research has been done in the SPL Testing field, it is necessary to assess the current state of research and practice, in order to provide practitioners with evidence that enable fostering its further development.ObjectiveThis paper focuses on Testing in SPL and has the following goals: investigate state-of-the-art testing practices, synthesize available evidence, and identify gaps between required techniques and existing approaches, available in the literature.MethodA systematic mapping study was conducted with a set of nine research questions, in which 120 studies, dated from 1993 to 2009, were evaluated.ResultsAlthough several aspects regarding testing have been covered by single-system development approaches, many cannot be directly applied in the SPL context due to specific issues. In addition, particular aspects regarding SPL are not covered by the existing SPL approaches, and when the aspects are covered, the literature just gives brief overviews. This scenario indicates that additional investigation, empirical and practical, should be performed.ConclusionThe results can help to understand the needs in SPL Testing, by identifying points that still require additional investigation, since important aspects regarding particular points of software product lines have not been addressed yet. 
53|5||Process models for service-based applications: A systematic literature review|ContextService-Oriented Computing (SOC) is a promising computing paradigm which facilitates the development of adaptive and loosely coupled service-based applications (SBAs). Many of the technical challenges pertaining to the development of SBAs have been addressed, however, there are still outstanding questions relating to the processes required to develop them.ObjectiveThe objective of this study is to systematically identify process models for developing service-based applications (SBAs) and review the processes within them. This will provide a useful starting point for any further research in the area. A secondary objective of the study is to identify process models which facilitate the adaptation of SBAs.MethodIn order to achieve this objective a systematic literature review (SLR) of the existing software engineering literature is conducted.ResultsDuring this research 722 studies were identified using a predefined search strategy, this number was narrowed down to 57 studies based on a set of strict inclusion and exclusion criteria. The results are reported both quantitatively in the form of a mapping study, as well as qualitatively in the form of a narrative summary of the key processes identified.ConclusionThere are many process models reported for the development of SBAs varying in detail and maturity, this review has identified and categorised the processes within those process models. The review has also identified and evaluated process models which facilitate the adaptation of SBAs. 
53|5||Research synthesis in software engineering: A tertiary study|ContextComparing and contrasting evidence from multiple studies is necessary to build knowledge and reach conclusions about the empirical support for a phenomenon. Therefore, research synthesis is at the center of the scientific enterprise in the software engineering discipline.ObjectiveThe objective of this article is to contribute to a better understanding of the challenges in synthesizing software engineering research and their implications for the progress of research and practice.MethodA tertiary study of journal articles and full proceedings papers from the inception of evidence-based software engineering was performed to assess the types and methods of research synthesis in systematic reviews in software engineering.ResultsAs many as half of the 49 reviews included in the study did not contain any synthesis. Of the studies that did contain synthesis, two thirds performed a narrative or a thematic synthesis. Only a few studies adequately demonstrated a robust, academic approach to research synthesis.ConclusionWe concluded that, despite the focus on systematic reviews, there is limited attention paid to research synthesis in software engineering. This trend needs to change and a repertoire of synthesis methods needs to be an integral part of systematic reviews to increase their significance and utility for research and practice. 
53|5||Modeling process-related RBAC models with extended UML activity models|ContextBusiness processes are an important source for the engineering of customized software systems and are constantly gaining attention in the area of software engineering as well as in the area of information and system security. While the need to integrate processes and role-based access control (RBAC) models has been repeatedly identified in research and practice, standard process modeling languages do not provide corresponding language elements.ObjectiveIn this paper, we are concerned with the definition of an integrated approach for modeling processes and process-related RBAC models – including roles, role hierarchies, statically and dynamically mutual exclusive tasks, as well as binding of duty constraints on tasks.MethodWe specify a formal metamodel for process-related RBAC models. Based on this formal model, we define a domain-specific extension for a standard modeling language.ResultsOur formal metamodel is generic and can be used to extend arbitrary process modeling languages. To demonstrate our approach, we present a corresponding extension for UML2 activity models. The name of our extension is Business Activities. Moreover, we implemented a library and runtime engine that can manage Business Activity runtime models and enforce the different policies and constraints in a software system.ConclusionThe definition of process-related RBAC models at the modeling-level is an important prerequisite for the thorough implementation and enforcement of corresponding policies and constraints in a software system. We identified the need for modeling support of process-related RBAC models from our experience in real-world role engineering projects and case studies. The Business Activities approach presented in this paper is successfully applied in role engineering projects. 
53|5||Modeling software requirement with timing diagram and Simulink Stateflow|ContextA methodology is needed to model software specification with both timing diagram and Simulink/Stateflow (SL/SF) and to convert timing diagram model into SL/SF model.ObjectivesThis paper aims to propose a timing diagram drawing method and the algorithm to convert a timing diagram model into the equivalently behaving SL/SF model.MethodWe add a few extra features to those of the typical timing diagrams. The extra features were chosen by the field engineers’ requests and the survey of many different commercial models. This paper proposes the formal method to describe the timing diagram. Based on the formal description, the converting algorithm translates a timing diagram into the SL/SF model.ResultsBy providing a path from timing diagram to SL/SF, system specifications can be described with both SL/SF and timing diagram. This paper addresses the details of outcomes that the proposed method was successfully applied to modeling “Theft watch system” and “Automotive power window controller. The proposed method has been successfully applied to other commercial systems, and to the models provided by Mathworks.ConclusionThis paper proposed a methodology to describe system specification with both timing diagram and SL/SF. The strategy might help designers more efficiently describe specifications. In addition, the mixed specification can be simulated in SL/SF, and the specification can utilize other third party SL/SF tools such as test case generation or model check utilities. 
53|5||Evaluating software engineering techniques for developing complex systems with multiagent approaches|ContextMultiagent systems (MAS) allow complex systems to be developed in which autonomous and heterogeneous entities interact. Currently, there are a great number of methods and frameworks for developing MAS. The selection of one or another development environment is a crucial part of the development process. Therefore, the evaluation and comparison of MAS software engineering techniques is necessary in order to make the selection of the development environment easier.ObjectiveThe main goal of this paper is to define an evaluation framework that will help in facilitating, standardizing, and simplifying the evaluation, analysis, and comparison of MAS development environments. Moreover, the final objective of the proposed tool is to provide a repository of the most commonly used MAS software engineering methods and tools.MethodThe proposed framework analyzes methods and tools through a set of criteria that are related to both system engineering dimensions and MAS features. Also, the support for developing organizational and service-oriented MAS is studied. This framework is implemented as an online application to improve its accessibility.ResultsIn this paper, we present Masev, which is an evaluation framework for MAS software engineering. It allows MAS methods, techniques and environments to be analyzed and compared. A case study of the analysis of four methodologies is presented.ConclusionIt is concluded that Masev simplifies the evaluation and comparison task and summarizes the most important issues for developing MAS, organizational MAS, and service-oriented MAS. Therefore, it could help developers to select the most appropriate MAS method and tools for developing a specific system, and it could be used for MAS software engineering developers to detect and deficiencies in their methods and tools. Also, developers of new tools can understand this application as a way to publish their tools and demonstrate what their contributions are to the state of the art. 
53|5||Special Section on Best Papers from XP2010|
53|5||The relationship between organizational culture and the deployment of agile methods|ContextSystems development normally takes place in a specific organizational context, including organizational culture. Previous research has identified organizational culture as a factor that potentially affects the deployment systems development methods.ObjectiveThe purpose is to analyze the relationship between organizational culture and the post-adoption deployment of agile methods.MethodThis study is a theory development exercise. Based on the Competing Values Model of organizational culture, the paper proposes a number of hypotheses about the relationship between organizational culture and the deployment of agile methods.ResultsInspired by the agile methods thirteen new hypotheses are introduced and discussed. They have interesting implications, when contrasted with ad hoc development and with traditional systems development methods.ConclusionBecause of the conceptual richness of organizational culture and the ambiguity of the concept of agility the relationship between organizational culture and the deployment of agile systems development forms a rich and interesting research topic. Recognizing that the Competing Values Model represents just one view of organizational culture, the paper introduces a number of alternative conceptions and identifies several interesting paths for future research into the relationship between organizational culture and agile methods deployment. 
53|5||The impact of inadequate customer collaboration on self-organizing Agile teams|ContextCustomer collaboration is a vital feature of Agile software development.ObjectiveThis article addresses the importance of adequate customer involvement on Agile projects, and the impact of different levels of customer involvement on real-life Agile projects.MethodWe conducted a Grounded Theory study involving 30 Agile practitioners from 16 software development organizations in New Zealand and India, over a period of 3 years.ResultsWe discovered that Lack of Customer Involvement was one of the biggest challenges faced by Agile teams. Customers were not as involved on these Agile projects as Agile methods demand. We describe the causes of inadequate customer collaboration, its adverse consequences on self-organizing Agile teams, and Agile Undercover — a set of strategies used by the teams to practice Agile despite insufficient or ineffective customer involvement.ConclusionCustomer involvement is important on Agile projects. Inadequate customer involvement causes adverse problems for Agile teams. The Agile Undercover strategies we’ve identified can assist Agile teams facing similar lack of customer involvement. 
53|5||Beyond the customer: Opening the agile systems development process|ContextA particular strength of agile systems development approaches is that they encourage a move away from ‘introverted’ development, involving the customer in all areas of development, leading to more innovative and hence more valuable information system. However, a move toward open innovation requires a focus that goes beyond a single customer representative, involving a broader range of stakeholders, both inside and outside the organisation in a continuous, systematic way.ObjectiveThis paper provides an in-depth discussion of the applicability and implications of open innovation in an agile environment.MethodWe draw on two illustrative cases from industry.ResultsWe highlight some distinct problems that arose when two project teams tried to combine agile and open innovation principles. For example, openness is often compromised by a perceived competitive element and lack of transparency between business units. In addition, minimal documentation often reduce effective knowledge transfer while the use of short iterations, stand-up meetings and presence of on-site customer reduce the amount of time for sharing ideas outside the team.ConclusionA clear understanding of the inter- and intra-organisational applicability and implications of open innovation in agile systems development is required to address key challenges for research and practice. 
53|5||Post-agility: What follows a decade of agility?|ContextAgile information systems development (ISD) has received much attention from both the practitioner and researcher community over the last 10–15 years. However, it is still unclear what precisely constitutes agile ISD.ObjectiveBased on four empirical studies conducted over a 10-year time period from 1999 to 2008 the objective of this paper is to show how the meaning and practice of agile ISD has evolved over time and on this basis to speculate about what comes next.MethodFour phases of research has been conducted, using a grounded theory approach. For each research phase qualitative interviews were held in American and/or Danish companies and a grounded theory was inductively discovered by careful data analysis. Subsequently, the four unique theories have been analyzed for common themes, and a global theory was identified across the empirical data.ResultsIn 1999 companies were developing software at high-speed in a desperate rush to be first-to-market. In 2001 a new high-speed/quick results development process had become established practice. In 2003 changes in the market created the need for a more balanced view on speed and quality, and in 2008 companies were successfully combining agile and plan-driven approaches to achieve the benefits of both. The studies reveal a two-stage pattern in which dramatic changes in the market causes disruption of established practices and process adaptations followed by consolidation of lessons learnt into a once again stable software development process.ConclusionThe cyclical history of punctuated process evolution makes it possible to distinguish pre-agility from current practices (agility), and on this basis, to speculate about post-agility: a possible next cycle of software process evolution concerned with proactively pursuing the dual goal of agility and alignment through a diversity of means. 
53|6|http://www.sciencedirect.com/science/journal/09505849/53/6|Impact of test-driven development on productivity, code and tests: A controlled experiment|ContextTest-driven development is an approach to software development, where automated tests are written before production code in highly iterative cycles. Test-driven development attracts attention as well as followers in professional environment; however empirical evidence of its superiority regarding its effect on productivity, code and tests compared to test-last development is still fairly limited. Moreover, it is not clear if the supposed benefits come from writing tests before code or maybe from high iterativity/short development cycles.ObjectiveThis paper describes a family of controlled experiments comparing test-driven development to micro iterative test-last development with emphasis on productivity, code properties (external quality and complexity) and tests (code coverage and fault-finding capabilities).MethodSubjects were randomly assigned to test-driven and test-last groups. Controlled experiments were conducted for two years, in an academic environment and in different developer contexts (pair programming and individual programming contexts). Number of successfully implemented stories, percentage of successful acceptance tests, McCabe’s code complexity, code coverage and mutation score indicator were measured.ResultsExperimental results and their selective meta-analysis show no statistically significant differences between test-driven development and iterative test-last development regarding productivity (Ï2(6) = 4.799, p = 1.0, r = .107, 95% CI (confidence interval): −.149 to .349), code complexity (Ï2(6) = 8.094, p = .46, r = .048, 95% CI: −.254 to .341), branch coverage (Ï2(6) = 13.996, p = .059, r = .182, 95% CI: −.081 to .421), percentage of acceptance tests passed (one experiment, Mann–Whitney U = 125.0, p = .98, r = .066) and mutation score indicator (Ï2(4) = 3.807, p = .87, r = .128, 95% CI: −.162 to .398).ConclusionAccording to our findings, the benefits of test-driven development compared to iterative test-last development are small and thus in practice relatively unimportant, although effects are positive. There is an indication of test-driven development endorsing better branch coverage, but effect size is considered small. 
53|6||Conceptual scheduling model and optimized release scheduling for agile environments|ContextRelease scheduling deals with the selection and assignment of deliverable features to a sequence of consecutive product deliveries while several constraints are fulfilled. Although agile software development represents a major approach to software engineering, there is no well-established conceptual definition and sound methodological support of agile release scheduling.ObjectiveTo propose a solution, we present, (1) a conceptual model for agile scheduling, and (2) a novel multiple knapsack-based optimization model with (3) a branch-and-bound optimization algorithm for agile release scheduling.MethodTo evaluate our model simulations were carried out seven real life and several generated data sets.ResultsThe developed algorithm strives to prevent resource overload and resource underload, and mitigates risks of delivery slippage.ConclusionThe results of the experiment suggest that this approach can provide optimized semi-automatic release schedule generations and more informed and established decisions utilizing what-if-analysis on the fly to tailor the best schedule for the specific project context. 
53|6||External social capital and information systems development team flexibility|ContextISD research based on the socio-technical perspective suggests that two sources of socio-technical change have a bearing on the performance of information systems development (ISD) projects: business requirements and development technology. To enhance project effectiveness, ISD teams need to enhance their flexibility in the face of the constant changes taking place from business and technical environments in which they operate. Flexibility is conceptualized as an outcome of capability development through constantly integrating and reconfiguring available resources within and outside of the organization where the team is embedded.ObjectiveThe purpose of this study is to examine the relationship between a team’s external social capital and team flexibility. More specifically, based on social capital theory, this study argues that external social capital leads to IS team flexibility, which in turn contributes to the successful project performance.MethodA survey design was selected to collect data and test the proposed model. A snowballing strategy was employed to collect data. 118 information systems developers participated in the survey and the model was analyzed using partial least squares regression. Results: The findings show that, in general, the ISD teams’ external social capital do contribute to the team’s response to changes. However, they exhibit unique impacts on ISD team flexibility respectively.ResultsThe findings show that, in general, the ISD teams’ external social capital does contribute to the team’s response to changes. However, they exhibit unique impacts on ISD team flexibility respectively.ConclusionThe various external social capitals have distinctive effects on a team’s flexibility. Specifically, horizontal relationships are positively associated with both business and technology flexibility. Vertical relationships are positively associated with business flexibility and market relationships technology flexibility. ISD managers should establish robust relationships with its high-ups, other lateral units, and third parties outside in the market. They also need to be more adaptive to the increasingly volatile socio-technical environment, and proactively search, exploit, upgrade, and integrate resources that are essential to the development of system development team flexibility. 
53|6||Applying agglomerative hierarchical clustering algorithms to component identification for legacy systems|ContextComponent identification, the process of evolving legacy system into finely organized component-based software systems, is a critical part of software reengineering. Currently, many component identification approaches have been developed based on agglomerative hierarchical clustering algorithms. However, there is a lack of thorough investigation on which algorithm is appropriate for component identification.ObjectiveThis paper focuses on analyzing agglomerative hierarchical clustering algorithms in software reengineering, and then identifying their respective strengths and weaknesses in order to apply them effectively for future practical applications.MethodA series of experiments were conducted for 18 clustering strategies combined according to various similarity measures, weighting schemes and linkage methods. Eleven subject systems with different application domains and source code sizes were used in the experiments. The component identification results are evaluated by the proposed size, coupling and cohesion criteria.ResultsThe experimental results suggested that the employed similarity measures, weighting schemes and linkage methods can have various effects on component identification results with respect to the proposed size, coupling and cohesion criteria, so the hierarchical clustering algorithms produced quite different clustering results.ConclusionsAccording to the experimental results, it can be concluded that it is difficult to produce perfectly satisfactory results for a given clustering algorithm. Nevertheless, these algorithms demonstrated varied capabilities to identify components with respect to the proposed size, coupling and cohesion criteria. 
53|6||Guest Editorial: Special section on the selected papers from 14th International Conference on Evaluation and Assessment in Software Engineering (EASE 2010)|
53|6||A systematic review of research on open source software in commercial software product development|ContextThe popularity of the open source software development in the last decade, has brought about an increased interest from the industry on how to use open source components, participate in the open source community, build business models around this type of software development, and learn more about open source development methodologies. There is a need to understand the results of research in this area.ObjectiveSince there is a need to understand conducted research, the aim of this study is to summarize the findings of research that has ben carried out on usage of open source components and development methodologies by the industry, as well as companies’ participation in the open source community.MethodSystematic review through searches in library databases and manual identification of articles from the open source conference. The search was first carried out in May 2009 and then once again in May 2010.ResultsIn 2009, 237 articles were first found, from which 19 were selected based on content and quality, and in 2010, 76 new articles were found from which four were selected. Twenty three articles were identified in total.ConclusionsThe articles could be divided into four categories: open source as part of component based software engineering, business models with open source in commercial organization, company participation in open source development communities, and usage of open source processes within a company. 
53|6||Identifying relevant studies in software engineering|ContextSystematic literature review (SLR) has become an important research methodology in software engineering since the introduction of evidence-based software engineering (EBSE) in 2004. One critical step in applying this methodology is to design and execute appropriate and effective search strategy. This is a time-consuming and error-prone step, which needs to be carefully planned and implemented. There is an apparent need for a systematic approach to designing, executing, and evaluating a suitable search strategy for optimally retrieving the target literature from digital libraries.ObjectiveThe main objective of the research reported in this paper is to improve the search step of undertaking SLRs in software engineering (SE) by devising and evaluating systematic and practical approaches to identifying relevant studies in SE.MethodWe have systematically selected and analytically studied a large number of papers (SLRs) to understand the state-of-the-practice of search strategies in EBSE. Having identified the limitations of the current ad-hoc nature of search strategies used by SE researchers for SLRs, we have devised a systematic and evidence-based approach to developing and executing optimal search strategies in SLRs. The proposed approach incorporates the concept of ‘quasi-gold standard’ (QGS), which consists of collection of known studies, and corresponding ‘quasi-sensitivity’ into the search process for evaluating search performance.ResultsWe conducted two participant–observer case studies to demonstrate and evaluate the adoption of the proposed QGS-based systematic search approach in support of SLRs in SE research.ConclusionWe report their findings based on the case studies that the approach is able to improve the rigor of search process in an SLR, as well as it can serve as a supplement to the guidelines for SLRs in EBSE. We plan to further evaluate the proposed approach using a series of case studies on varying research topics in SE. 
53|6||Using mapping studies as the basis for further research â A participant-observer case study|ContextWe are strong advocates of evidence-based software engineering (EBSE) in general and systematic literature reviews (SLRs) in particular. We believe it is essential that the SLR methodology is used constructively to support software engineering research.ObjectiveThis study aims to assess the value of mapping studies which are a form of SLR that aims to identify and categorise the available research on a broad software engineering topic.MethodWe used a multi-case, participant-observer case study using five examples of studies that were based on preceding mapping studies. We also validated our results by contacting two other researchers who had undertaken studies based on preceding mapping studies and by assessing review comments related to our follow-on studies.ResultsOur original case study identified 11 unique benefits that can accrue from basing research on a preceding mapping study of which only two were case specific. We also identified nine problems associated with using preceding mapping studies of which two were case specific. These results were consistent with the information obtained from the validation activities. We did not find an example of an independent research group making use of a mapping study produced by other researchers.ConclusionMapping studies can save time and effort for researchers and provide baselines to assist new research efforts. However, they must be of high quality in terms of completeness and rigour if they are to be a reliable basis for follow-on research. 
53|6||Guest Editorsâ introduction to the special section of the 16th AsiaâPacific Software Engineering Conference (APSEC2009)|
53|6||Contribution-based call stack abstraction for call string based pointer analysis|ContextDifferent method calls may have different contributions to the precision of the final application when abstracted into the call strings. The existing call string based pointer analysis algorithms do not consider such contribution difference and hence may not achieve best cost-effectiveness.ObjectiveTo be more cost-effective, we try to leverage the contribution information of each method call in call string based pointer analysis.MethodThe paper firstly proposes a contribution-based call stack abstraction method which abstracts the call stacks into call strings with the contribution information under consideration. Then, we apply the new call stack abstraction method to the pointer analysis of AspectJ programs and propose a concern-sensitive points-to analysis method. Besides, the new abstraction method is also applied to multi-threaded Java programs and results in a thread-sensitive pointer analysis method.ResultsThe experimental results show that the two pointer analysis methods with contribution-based call stack abstraction can be more cost-effective than the ordinary call string based approaches for an application that detects harmful advices and an application that detects inter-thread data flow.ConclusionThese pointer analysis methods more concretely and more clearly show that the contribution-based call stack abstraction can lead to better cost-effectiveness for the given applications. 
53|6||An approach to identifying causes of implied scenarios using unenforceable orders|ContextThe implied scenarios are unexpected behaviors in the scenario specifications. Detecting and handling them is essential for the correctness of the scenario specifications. To handle such implied scenarios, identifying the causes of implied scenarios is also essential. Most recent researches focus on detecting those implied scenarios, themselves or limited causes of implied scenarios.ObjectiveThe purpose of this research is to provide an approach to detecting the causes of implied scenarios.MethodThe scenario specification is a set of events and a set of relative orders between the events, and enforces them for its implementation. Among the orders, a set of orders that cannot be inherently enforced is the unenforceable orders. Obviously, existence of unenforceable orders leads the implied scenarios. To obtain the unenforceable orders, we first provide a method to represent each of the specification and its implementation as a set of orders between events, called the causal order graph. Then, the differences between them are the unenforceable orders.ResultsBecause the unenforceable orders consist of events and their order relation that are specified in the scenario specification, they can point out which part of the scenario specification should be considered to handle the implied scenarios. In addition, our approach supports the synchronous, asynchronous, and FIFO communication styles without the state explosion or heavy computational overhead. To validate our approach, we provide two case studies.ConclusionsThis approach helps a designer to effectively correct the scenario specification by identifying where to be fixed, especially in large cases and under the various communication styles. 
53|6||A controlled experiment in assessing and estimating software maintenance tasks|ContextSoftware maintenance is an important software engineering activity that has been reported to account for the majority of the software total cost. Thus, understanding the factors that influence the cost of software maintenance tasks helps maintainers to make informed decisions about their work.ObjectiveThis paper describes a controlled experiment of student programmers performing maintenance tasks on a C++ program. The objective of the study is to assess the maintenance size, effort, and effort distributions of three different maintenance types and to describe estimation models to predict the programmer’s effort spent on maintenance tasks.MethodTwenty-three graduate students and a senior majoring in computer science participated in the experiment. Each student was asked to perform maintenance tasks required for one of the three task groups. The impact of different LOC metrics on maintenance effort was also evaluated by fitting the data collected into four estimation models.ResultsThe results indicate that corrective maintenance is much less productive than enhancive and reductive maintenance and program comprehension activities require as much as 50% of the total effort in corrective maintenance. Moreover, the best software effort model can estimate the time of 79% of the programmers with the error of or less than 30%.ConclusionOur study suggests that the LOC added, modified, and deleted metrics are good predictors for estimating the cost of software maintenance. Effort estimation models for maintenance work may use the LOC added, modified, deleted metrics as the independent parameters instead of the simple sum of the three. Another implication is that reducing business rules of the software requires a sizable proportion of the software maintenance effort. Finally, the differences in effort distribution among the maintenance types suggest that assigning maintenance tasks properly is important to effectively and efficiently utilize human resources. 
53|7|http://www.sciencedirect.com/science/journal/09505849/53/7|Barriers in the selection of offshore software development outsourcing vendors: An exploratory study using a systematic literature review|ContextSoftware development outsourcing is a contract-based relationship between client and vendor organisations in which a client contracts out all or part of its software development activities to a vendor, who provides agreed services for remuneration.ObjectiveThe objective is to identify various barriers that have a negative impact on software outsourcing clients in the selection process of offshore software development outsourcing vendors.MethodWe have performed a systematic literature review (SLR) process for the identification of barriers. We have performed all the SLR steps such as the protocol development, initial selection, final selection, quality assessment, data extraction and data synthesis.ResultsWe have identified barriers such as ‘language and cultural barriers’, ‘country instability’, ‘lack of project management’, ‘lack of protection for intellectual property rights’ and ‘lack of technical capability’ that generally have a negative impact on outsourcing clients. We have identified only one common frequently cited barrier in three types of organisations (i.e. small, medium and large) which is ‘language and cultural barriers’. We did not identify any common frequently cited barrier in three continents (Asia, North America and Europe) and in two decades (1990–1999 and 2000–mid 2008). The results also reveal the similarities and differences in the barriers identified through different study strategies.ConclusionsVendors should address frequently cited barriers such as ‘language and cultural barriers’, ‘country instability’, ‘lack of project management’, ‘lack of protection for intellectual property rights’ and ‘lack of technical capability’ in order to compete in the offshore outsourcing business. 
53|7||Analyzing evolution of variability in a software product line: From contexts and requirements to features|ContextIn the long run, features of a software product line (SPL) evolve with respect to changes in stakeholder requirements and system contexts. Neither domain engineering nor requirements engineering handles such co-evolution of requirements and contexts explicitly, making it especially hard to reason about the impact of co-changes in complex scenarios.ObjectiveIn this paper, we propose a problem-oriented and value-based analysis method for variability evolution analysis. The method takes into account both kinds of changes (requirements and contexts) during the life of an evolving software product line.MethodThe proposed method extends the core requirements engineering ontology with the notions to represent variability-intensive problem decomposition and evolution. On the basis of problemorientation, the analysis method identifies candidate changes, detects influenced features, and evaluates their contributions to the value of the SPL.Results and ConclusionThe process of applying the analysis method is illustrated using a concrete case study of an evolving enterprise software system, which has confirmed that tracing back to requirements and contextual changes is an effective way to understand the evolution of variability in the software product line. 
53|7||Partnering effects on userâdeveloper conflict and role ambiguity in information system projects|ContextInformation system development (ISD) has been plagued with high failure rates. This is partially due to the activities being a combination of both a technical and social processes involving stakeholders with conflicting interests.ObjectiveExisting software risk management theories and frameworks offer limited suggestions for actions that can be taken to reduce the chance of failure of ISD projects. Our objective is to examine the connections among some of the more important user related risks in order to shed light on how specific strategies enhance the chance of project success.MethodWe conducted a sample of information systems project managers to test a multivariate model to explain the impact of pursuing a partnership with users on the conflicts that arise between users and developers, role ambiguity, and subsequent impact on project performance.ResultsThe proposed model was supported, suggesting that user–developer conflict and role ambiguity have a negative impact on performance estimation difficulty, which negatively affects project performance.ConclusionPursuit of project partnering yields a number of significant relationships in the model indicating an organization can implement practices that reduce risks associated with role ambiguity and conflict in system development projects. 
53|7||The longitudinal, chronological case study research strategy: A definition, and an example from IBM Hursley Park|ContextThere is surprisingly little empirical software engineering research (ESER) that has analysed and reported the rich, fine-grained behaviour of phenomena over time using qualitative and quantitative data. The ESER community also increasingly recognises the need to develop theories of software engineering phenomena e.g. theories of the actual behaviour of software projects at the level of the project and over time.ObjectiveTo examine the use of the longitudinal, chronological case study (LCCS) as a research strategy for investigating the rich, fine-grained behaviour of phenomena over time using qualitative and quantitative data.MethodReview the methodological literature on longitudinal case study. Define the LCCS and demonstrate the development and application of the LCCS research strategy to the investigation of Project C, a software development project at IBM Hursley Park. Use the study to consider prospects for LCCSs, and to make progress on a theory of software project behaviour.ResultsLCCSs appear to provide insights that are hard to achieve using existing research strategies, such as the survey study. The LCCS strategy has basic requirements that data is time-indexed, relatively fine-grained and collected contemporaneous to the events to which the data refer. Preliminary progress is made on a theory of software project behaviour.ConclusionLCCS appears well suited to analysing and reporting rich, fine-grained behaviour of phenomena over time. 
53|7||An experimental assessment of module documentation-based testing|ContextTesting a module that has memory using the black-box approach has been found to be expensive and relatively ineffective. Instead, testing without knowledge of the specifications (white-box approach) may not be effective in showing whether a program has been properly implemented as stated in its specifications. We propose instead a grey-box approach called Module Documentation-based Testing or MD-Test, the heart of which is an automatic generation of the test oracle from the external and internal views of the module.ObjectiveThis paper presents an empirical analysis and comparison of MD-Test against three existing testing tools.MethodThe experiment was conducted using a mutation-testing approach, in two phases that assess the capability of MD-Test in general and its capability of evaluating test results in particular.ResultsThe results of the general assessment indicate that MD-Test is more effective than the other three tools under comparison, where it is able to detect all faults. The second phase of the experiment, which is significant to this study, compares the capabilities of MD-Test and JUnit-black using the test evaluation results. Likewise, an analysis of the test evaluation results shows that MD-Test is more effective and efficient, where MD-Test is able to detect at least the same number of faults as, or is at par with, the black-box approach.ConclusionIt is concluded that test evaluation using grey-box approach is more effective and efficient that the black-box approach when testing a module that has memory. 
53|7||Comparing the performance of metaheuristics for the analysis of multi-stakeholder tradeoffs in requirements optimisation|ContextIn requirements engineering, there will be many different stake holders. Often the requirements engineer has to find a set of requirements that reflect the needs of several different stake holders, while remaining within budget.ObjectiveThis paper introduces an optimisation-based approach to the automated analysis of requirements assignments when multiple stake holders are to be satisfied by a single choice of requirements.MethodThe paper reports on experiments using two different multi-objective evolutionary optimisation algorithms with real world data sets as well as synthetic data sets. This empirical validation includes a statistical analysis of the performance of the two algorithms.ResultsThe results reveal that the Two-Archive algorithm outperformed the others in convergence as the scale of problems increase. The paper also shows how both traditional and animated Kiviat diagrams can be used to visualise the tensions between the stake holders’ competing requirements in the presence of increasing budgetary pressure.ConclusionThis paper presented the concept of internal tensioning among multi-stakeholder in requirements analysis and optimisation for the first time. This analysis may be useful in internal negotiations over budgetary allowance for the project. 
53|7||An automated framework for software test oracle|ContextOne of the important issues of software testing is to provide an automated test oracle. Test oracles are reliable sources of how the software under test must operate. In particular, they are used to evaluate the actual results that produced by the software. However, in order to generate an automated test oracle, oracle challenges need to be addressed. These challenges are output-domain generation, input domain to output domain mapping, and a comparator to decide on the accuracy of the actual outputs.ObjectiveThis paper proposes an automated test oracle framework to address all of these challenges.MethodI/O Relationship Analysis is used to generate the output domain automatically and Multi-Networks Oracles based on artificial neural networks are introduced to handle the second challenge. The last challenge is addressed using an automated comparator that adjusts the oracle precision by defining the comparison tolerance. The proposed approach was evaluated using an industry strength case study, which was injected with some faults. The quality of the proposed oracle was measured by assessing its accuracy, precision, misclassification error and practicality. Mutation testing was considered to provide the evaluation framework by implementing two different versions of the case study: a Golden Version and a Mutated Version. Furthermore, a comparative study between the existing automated oracles and the proposed one is provided based on which challenges they can automate.ResultsResults indicate that the proposed approach automated the oracle generation process 97% in this experiment. Accuracy of the proposed oracle was up to 98.26%, and the oracle detected up to 97.7% of the injected faults.ConclusionConsequently, the results of the study highlight the practicality of the proposed oracle in addition to the automation it offers. 
53|8|http://www.sciencedirect.com/science/journal/09505849/53/8|Usability evaluation methods for the web: A systematic mapping study|ContextIn recent years, many usability evaluation methods (UEMs) have been employed to evaluate Web applications. However, many of these applications still do not meet most customers’ usability expectations and many companies have folded as a result of not considering Web usability issues. No studies currently exist with regard to either the use of usability evaluation methods for the Web or the benefits they bring.ObjectiveThe objective of this paper is to summarize the current knowledge that is available as regards the usability evaluation methods (UEMs) that have been employed to evaluate Web applications over the last 14 years.MethodA systematic mapping study was performed to assess the UEMs that have been used by researchers to evaluate Web applications and their relation to the Web development process. Systematic mapping studies are useful for categorizing and summarizing the existing information concerning a research question in an unbiased manner.ResultsThe results show that around 39% of the papers reviewed reported the use of evaluation methods that had been specifically crafted for the Web. The results also show that the type of method most widely used was that of User Testing. The results identify several research gaps, such as the fact that around 90% of the studies applied evaluations during the implementation phase of the Web application development, which is the most costly phase in which to perform changes. A list of the UEMs that were found is also provided in order to guide novice usability practitioners.ConclusionsFrom an initial set of 2703 papers, a total of 206 research papers were selected for the mapping study. The results obtained allowed us to reach conclusions concerning the state-of-the-art of UEMs for evaluating Web applications. This allowed us to identify several research gaps, which subsequently provided us with a framework in which new research activities can be more appropriately positioned, and from which useful information for novice usability practitioners can be extracted. 
53|8||Cycle elimination for invocation graph-based context-sensitive pointer analysis|ContextPointer analysis is an important building block of optimizing compilers and program analyzers for C language. Various methods with precision and performance trade-offs have been proposed. Among them, cycle elimination has been successfully used to improve the scalability of context-insensitive pointer analyses without losing any precision.ObjectiveIn this article, we present a new method on context-sensitive pointer analysis with an effective application of cycle elimination.MethodTo obtain similar benefits of cycle elimination for context-sensitive analysis, we propose a novel constraint-based formulation that uses sets of contexts as annotations. Our method is not based on binary decision diagram (BDD). Instead, we directly use invocation graphs to represent context sets and apply a hash-consing technique to deal with the exponential blow-up of contexts.ResultExperimental results on C programs ranging from 20,000 to 290,000 lines show that applying cycle elimination to our new formulation results in 4.5 ×speedup over the previous BDD-based approach.ConclusionWe showed that cycle elimination is an effective method for improving the scalability of context-sensitive pointer analysis. 
53|8||Design guidelines for software processes knowledge repository development|ContextStaff turnover in organizations is an important issue that should be taken into account mainly for two reasons:1.Employees carry an organization’s knowledge in their heads and take it with them wherever they go2.Knowledge accessibility is limited to the amount of knowledge employees want to shareObjectiveThe aim of this work is to provide a set of guidelines to develop knowledge-based Process Asset Libraries (PAL) to store software engineering best practices, implemented as a wiki.MethodFieldwork was carried out in a 2-year training course in agile development. This was validated in two phases (with and without PAL), which were subdivided into two stages: Training and Project.ResultsThe study demonstrates that, on the one hand, the learning process can be facilitated using PAL to transfer software process knowledge, and on the other hand, products were developed by junior software engineers with a greater degree of independence.ConclusionPAL, as a knowledge repository, helps software engineers to learn about development processes and improves the use of agile processes. 
53|8||Exploring a Bayesian and linear approach to requirements traceability|ContextFor large software projects it is important to have some traceability between artefacts from different phases (e.g.requirements, designs, code), and between artefacts and the involved developers. However, if the capturing of traceability information during the project is felt as laborious to developers, they will often be sloppy in registering the relevant traceability links so that the information is incomplete. This makes automated tool-based collection of traceability links a tempting alternative, but this has the opposite challenge of generating too many potential trace relationships, not all of which are equally relevant.ObjectiveThis paper evaluates how to rank such auto-generated trace relationships.MethodWe present two approaches for such a ranking: a Bayesian technique and a linear inference technique. Both techniques depend on the interaction event trails left behind by collaborating developers while working within a development tool.ResultsThe outcome of a preliminary study suggest the advantage of the linear approach, we also explore the challenges and potentials of the two techniques.ConclusionThe advantage of the two techniques is that they can be used to provide traceability insights that are contextual and would have been much more difficult to capture manually. We also present some key lessons learnt during this research. 
53|8||Guest editorial: Advances in functional size measurement and effort estimation â Extended best papers|
53|8||Convertibility of Function Points into COSMIC Function Points: A study using Piecewise Linear Regression|BackgroundCOSMIC Function Points and traditional Function Points (i.e., IFPUG Function Points and more recent variation of Function Points, such as NESMA and FISMA) are probably the best known and most widely used Functional Size Measurement methods. The relationship between the two kinds of Function Points still needs to be investigated. If traditional Function Points could be accurately converted into COSMIC Function Points and vice versa, then, by measuring one kind of Function Points, one would be able to obtain the other kind of Function Points, and one might measure one or the other kind interchangeably. Several studies have been performed to evaluate whether a correlation or a conversion function between the two measures exists. Specifically, it has been suggested that the relationship between traditional Function Points and COSMIC Function Points may not be linear, i.e., the value of COSMIC Function Points seems to increase more than proportionally to an increase of traditional Function Points.ObjectiveThis paper aims at verifying this hypothesis using available datasets that collect both FP and CFP size measures.MethodRigorous statistical analysis techniques are used, specifically Piecewise Linear Regression, whose applicability conditions are systematically checked. The Piecewise Linear Regression curve is a series of interconnected segments. In this paper, we focused on Piecewise Linear Regression curves composed of two segments. We also used Linear and Parabolic Regressions, to check if and to what extent Piecewise Linear Regression may provide an advantage over other regression techniques. We used two categories of regression techniques: Ordinary Least Squares regression is based on the usual minimization of the sum of squares of the residuals, or, equivalently, on the minimization of the average squared residual; Least Median of Squares regression is a robust regression technique that is based on the minimization of the median squared residual. Using a robust regression technique helps filter out the excessive influence of outliers.ResultsIt appears that the analysis of the relationship between traditional Function Points and COSMIC Function Points based on the aforementioned data analysis techniques yields valid significant models. However, different results for the various available datasets are achieved. In practice, we obtained statistically valid linear, piecewise linear, and non-linear conversion formulas for several datasets. In general, none of these is better than the others in a statistically significant manner.ConclusionsPractitioners interested in the conversion of FP measures into CFP (or vice versa) cannot just pick a conversion model and be sure that it will yield the best results. All the regression models we tested provide good results with some datasets. In practice, all the models described in the paper – in particular, both linear and non-linear ones – should be evaluated in order to identify the ones that are best suited for the specific dataset at hand. 
53|8||Improving the reliability of transaction identification in use cases| ContextThe concept of transactions is used in Use Case Points (UCP), and in many other functional size measurement methods, to capture the smallest unit of functionality that should be considered while measuring the size of a system. Unfortunately, in the case of the UCP method at least four methods for use-case transaction identification have been proposed so far. The different approaches to transaction identification and difficulties related to the analysis of requirements expressed in natural language can lead to problems in the reliability of functional size measurement.ObjectiveThe goal of this study was to evaluate reliability of transaction identification in use cases (with the methods mentioned in the literature), analyze their weaknesses, and propose some means for their improvement.MethodA controlled experiment on a group of 120 students was performed to investigate if the methods for transaction identification, known from the literature, provide similar results. In addition, a qualitative analysis of the experiment data was performed to investigate the potential problems related to transaction identification in use cases. During the experiment a use-case benchmark specification was used. The automatic methods for transaction identification, proposed in the paper have been validated using the same benchmark by comparing the outcomes provided by these methods to on-average number of transactions identified by the participants of the experiment.ResultsA significant difference in the median number of transactions was observed between groups using different methods of transaction identification. The Kruskal–Wallis test was performed with the significance level Î± set to 0.05 and followed by the post-hoc analysis performed according to the procedure proposed by Conover. Also a large intra-method variability was observed. The ratios between the maximum and minimum number of transactions identified by the participants using the same method were equal to 1.96, 3.83, 2.03, and 2.21. The proposed automatic methods for transaction identification provided results consistent with those provided by the participants of the experiment and functional measurement experts. The relative error between the number of transaction identified by the tool and on-average number of transactions identified by the participants of the experiment ranged from 3% to 7%.ConclusionsHuman-performed transaction identification is error prone and quite subjective. Its reliability can be improved by automating the process with the use of natural language processing techniques. 
53|9|http://www.sciencedirect.com/science/journal/09505849/53/9|Six years of systematic literature reviews in software engineering: An updated tertiary study|ContextSince the introduction of evidence-based software engineering in 2004, systematic literature review (SLR) has been increasingly used as a method for conducting secondary studies in software engineering. Two tertiary studies, published in 2009 and 2010, identified and analysed 54 SLRs published in journals and conferences in the period between 1st January 2004 and 30th June 2008.ObjectiveIn this article, our goal was to extend and update the two previous tertiary studies to cover the period between 1st July 2008 and 31st December 2009. We analysed the quality, coverage of software engineering topics, and potential impact of published SLRs for education and practice.MethodWe performed automatic and manual searches for SLRs published in journals and conference proceedings, analysed the relevant studies, and compared and integrated our findings with the two previous tertiary studies.ResultsWe found 67 new SLRs addressing 24 software engineering topics. Among these studies, 15 were considered relevant to the undergraduate educational curriculum, and 40 appeared of possible interest to practitioners. We found that the number of SLRs in software engineering is increasing, the overall quality of the studies is improving, and the number of researchers and research organisations worldwide that are conducting SLRs is also increasing and spreading.ConclusionOur findings suggest that the software engineering research community is starting to adopt SLRs consistently as a research method. However, the majority of the SLRs did not evaluate the quality of primary studies and fail to provide guidelines for practitioners, thus decreasing their potential impact on software engineering practice. 
53|9||Improving the applicability of object-oriented class cohesion metrics|ContextClass cohesion is an important object-oriented quality attribute. It refers to the degree of relatedness between the methods and attributes of a class. Several metrics have been proposed to measure the extent to which the class members are related. Most of these metrics have undefined values for a relatively high percentage of classes, which limits their applicability. The classes that have undefined values lack methods, attributes, or parameter types, or they include only a single method.ObjectiveWe improve the applicability of the class cohesion metrics by defining their values for such special classes. In addition, we theoretically and empirically validate the improved metrics.MethodWe theoretically examine whether the defined values satisfy the key cohesion properties. In addition, we empirically validate the metrics before and after the improvements to test whether the defined values improve the ability of the metrics to evaluate class cohesion. We also explore the correlation between the metrics and the presence of faulty classes to indirectly determine the strength or weakness of the metrics in indicating class quality.ResultsThe results show that our assigned values for the undefined cases do not violate the key cohesion properties and considerably improve the ability of the metrics to explain the presence of faulty classes and may therefore improve their ability to indicate the quality of the class design.ConclusionsHaving the class cohesion metrics defined for all possible cases improves the applicability of the metrics and potentially increases their precision in indicating class quality. 
53|9||Reliability analysis and optimal version-updating for open source software|ContextAlthough reliability is a major concern of most open source projects, research on this problem has attracted attention only recently. In addition, the optimal version-dating for open source software considering its special properties is not yet discussed.ObjectiveIn this paper, the reliability analysis and optimal version-updating for open source software are studied.MethodA modified non-homogeneous Poisson process model is developed for open source software reliability modeling and analysis. Based on this model, optimal version-updating for open source software is investigated as well. In the decision process, the rapid release strategy and the level of reliability are the two most important factors. However, they are essentially contradicting with each other. In order to consider these two conflicting factors simultaneously, a new decision model based on multi-attribute utility theory is proposed.ResultsOur models are tested on the real world data sets from two famous open source projects: Apache and GNOME. It is found that traditional software reliability models provide overestimations of the reliability of open source software. In addition, the proposed decision model can help management to make a rational decision on the optimal version-updating for open source software.ConclusionEmpirical results reveal that the proposed model for open source software reliability can describe the failure process more accurately. Furthermore, it can be seen that the proposed decision model can assist management to appropriately determine the optimal version-update time for open source software. 
53|9||Identifying refactoring opportunities in process model repositories|ContextIn order to ensure high quality of a process model repository, refactoring operations can be applied to correct anti-patterns, such as overlap of process models, inconsistent labeling of activities and overly complex models. However, if a process model collection is created and maintained by different people over a longer period of time, manual detection of such refactoring opportunities becomes difficult, simply due to the number of processes in the repository. Consequently, there is a need for techniques to detect refactoring opportunities automatically.ObjectiveThis paper proposes a technique for automatically detecting refactoring opportunities.MethodWe developed the technique based on metrics that can be used to measure the consistency of activity labels as well as the extent to which processes overlap and the type of overlap that they have. We evaluated it, by applying it to two large process model repositories.ResultsThe evaluation shows that the technique can be used to pinpoint the approximate location of three types of refactoring opportunities with high precision and recall and of one type of refactoring opportunity with high recall, but low precision.ConclusionWe conclude that the technique presented in this paper can be used in practice to automatically detect a number of anti-patterns that can be corrected by refactoring. 
53|9||Guest editorial: Studying work practices in Global Software Engineering|
53|9||Sociomaterial bricolage: The creation of location-spanning work practices by global software developers|ContextStudies on global software development have documented severe coordination and communication problems among coworkers due to geographic dispersion and consequent dependency on technology. These problems are exacerbated by increase in the complexity of work undertaken by global teams. However, despite these problems, global software development is on the rise and firms are adopting global practices across the board, raising the question: What does successful global software development look like and what can we learn from its practitioners?ObjectiveThis study draws on practice-based studies of work to examine successful work practices of global software developers. The primary aim of this study was to understand how workers develop practices that allow them to function effectively across geographically dispersed locations.MethodAn ethnographically-informed field study was conducted with data collection at two international locations of a firm. Interview, observation and archival data were collected. A total of 42 interviews and 3 weeks of observations were conducted.ResultsTeams spread across different locations around the world developed work practices through sociomaterial bricolage. Two facets of technology use were necessary for the creation of these practices: multiplicity of media and relational personalization at dyadic and team levels. New practices were triggered by the need to achieve a work-life balance, which was disturbed by global development. Reflecting on my role as a researcher, I underscore the importance of understanding researchers’ own frames of reference and using research practices that mirror informants’ work practices.ConclusionSoftware developers on global teams face unique challenges which necessitate a shift in their work practices. Successful teams are able to create practices that span locations while still being tied to location based practices. Inventive use of material and social resources is central to the creation of these practices. 
53|9||Methodological reflections on a field study of a globally distributed software project| ContextWe describe the methodology of a field study of a globally distributed software development project in a multinational corporation. The project spanned four sites in the US and one in India, and is a representative example of the complexities and intricacies of global corporate software development.ObjectiveOur goal is to provide the rationale behind the methodological choices and derive insights to inform the methodology of future studies of global software engineering teams. The paper also aims to provide an illustrative case of a typical geographically distributed corporate software project, through an in-depth description that emerged by applying the methods.MethodWe reflect upon the reasons for choosing each of our methods, viz., non-participant observation, site visits, interviews, and an online questionnaire. We then discuss what we learned from the experience of applying the methods.ResultsDuring and after the study, the discussions surrounding our methodological choices yielded important insights. The dynamics of software engineering practice and the geographical distribution of the project impacted factors such as access, costs, and cultural and linguistic diversity, and influenced the choice of methods. Our experience makes a case for methodological breadth and plurality as a means to a broad understanding of a global project. This understanding could then be linked to the specific research questions under consideration.ConclusionThe in-depth contextual description of the project that emerged from our methods highlights the utility of our methodological approach and provides an illustration of the complex nature of these projects. Our systematic reflection also yielded several methodological insights and provides important implications for future empirical studies of global corporate software development. Our experience can serve as a useful resource in methodological choices for research on globally distributed software engineering teams, or collaborative knowledge work in general. 
53|9||On qualitative methodologies and dispersed communities: Reflections on the process of investigating an open source community|ContextQualitative methodologies hold much potential for building an understanding of the principles and practices of free and open source software (FOSS) communities. Yet there is a scarcity in the literature of discussions focused on the practical and methodological challenges of this particular research context.ObjectiveThis paper formulates and addresses a number of questions regarding the applicability of qualitative methodologies for the study of FOSS communities. It reflects on the challenges of such approaches as seen in previous research efforts and discusses how they manifest in research practice through a thorough description of a case study of a community called PyPy.MethodThe paper primarily discusses interpretive research approaches which are based on ethnographic data collection methods. The study under discussion was an exploratory case study utilizing multiple methods, including participant observation, virtual ethnography, and open-ended questionnaires. Grounded Theory was used for data analysis.ResultsTwo broad sets of challenges are highlighted in relation to the multidimensionality of the FOSS phenomenon and the difficulty of qualitative analysis of activities in long-term context. Additional issues identified relate to potential problems with focus and the need for reflexivity, but also to the extent of the study and the importance of maintaining an active relationship with the core community group.ConclusionThis paper provides an overview – grounded in practical research experience and linked to insights from the literature – of methodological issues in the specific research area of qualitative studies of FOSS communities, which up until now has been lacking. 
53|9||Understanding technology use in global virtual teams: Research methodologies and methods|ContextThe globalisation of activities associated with software development and use has introduced many challenges in practice, and also (therefore) many for research. While the predominant approach to research in software engineering has followed a positivist science model, this approach may be sub-optimal when addressing problems with a dominant social or cultural dimension, such as those frequently encountered when studying work practices in a globally distributed team setting.The investigation of such a team reported in this paper provides one example of an alternative approach to research in a global context, through a longitudinal interpretive field study seeking to understand how global virtual teams mediated the use of technology. The study involved a large collective of faculty and support staff plus student members based in the geographically and temporally distant locations of New Zealand, the United States of America and Sweden.ObjectiveOur focus in this paper is on the conduct of research in the context of global software activities, and in particular, as applied to the actions and interactions of global virtual teams. We consider the appropriateness of various methodologies and methods in enabling such issues to be addressed.MethodWe describe how we undertook a substantial field study of global virtual teams, and highlight how the adopted structuration theory, action research and grounded theory methodologies applied to the analysis of email data, enabled us to deliver effectively against our goals.ResultsWe believe that the approach taken suited a research context in which situated practices were occurring over time in a highly complex domain, ensuring that our results were both strongly grounded and relevant to practice. It has resulted in the generation of substantive theory and techniques that have been adapted and applied on a pilot basis in further field settings.ConclusionWe conclude that globally distributed teamwork presents a complex context which demands new research approaches, beyond the limited set customarily applied by software engineering researchers. We advocate experimenting with different research methodologies and methods so that we have a more rounded repertoire to address the most important and relevant issues in global software development research, with the forms of rigour that suit the chosen approach. 
53|9||Conducting a Business Ethnography in Global Software Development projects of small German enterprises|ContextStudying work practices in the context of Global Software Development (GSD) projects entails multiple opportunities and challenges for the researchers. Understanding and tackling these challenges requires a careful and rigor application of research methods.ObjectiveWe want to contribute to the understanding of the challenges of studying GSD by reflecting on several obstacles we had to deal with when conducting ethnographically-informed research on offshoring in German small to medium-sized enterprises.MethodThe material for this paper is based on reflections and field notes from two research projects: an exploratory ethnographic field study, and a study that was framed as a Business Ethnography. For the analysis, we took a Grounded Theory-oriented coding and analysis approach in order to identify issues and challenges documented in our research notes.ResultsWe introduce the concept of Business Ethnography and discuss our experiences of adapting and implementing this action research concept for our study. We identify and discuss three primary issues: understanding complex global work practices from a local perspective, adapting to changing interests of the participants, and dealing with micro-political frictions between the cooperating sites.ConclusionsWe identify common interests between the researchers and the companies as a challenge and chance for studies on offshoring. Building on our experiences from the field, we argue for an active conceptualization of struggles and conflicts in the field as well as for extending the role of the ethnographer to that of a learning mediator. 
54|1|http://www.sciencedirect.com/science/journal/09505849/54/1|A systematic mapping study on the combination of static and dynamic quality assurance techniques|ContextA lot of different quality assurance techniques exist to ensure high quality products. However, most often they are applied in isolation. A systematic combination of different static and dynamic quality assurance techniques promises to exploit synergy effects, such as higher defect detection rates or reduced quality assurance costs. However, a systematic overview of such combinations and reported evidence about achieving synergy effects with such kinds of combinations is missing.ObjectiveThe main goal of this article is the classification and thematic analysis of existing approaches that combine different static and dynamic quality assurance technique, including reported effects, characteristics, and constraints. The result is an overview of existing approaches and a suitable basis for identifying future research directions.MethodA systematic mapping study was performed by two researchers, focusing on four databases with an initial result set of 2498 articles, covering articles published between 1985 and 2010.ResultsIn total, 51 articles were selected and classified according to multiple criteria. The two main dimensions of a combination are integration (i.e., the output of one quality assurance technique is used for the second one) and compilation (i.e., different quality assurance techniques are applied to ensure a common goal, but in isolation). The combination of static and dynamic analyses is one of the most common approaches and usually conducted in an integrated manner. With respect to the combination of inspection and testing techniques, this is done more often in a compiled way than in an integrated way.ConclusionThe results show an increased interest in this topic in recent years, especially with respect to the integration of static and dynamic analyses. Inspection and testing techniques are currently mostly performed in an isolated manner. The integration of inspection and testing techniques is a promising research direction for the exploitation of additional synergy effects. 
54|1||A systematic review of software architecture evolution research|ContextSoftware evolvability describes a software system’s ability to easily accommodate future changes. It is a fundamental characteristic for making strategic decisions, and increasing economic value of software. For long-lived systems, there is a need to address evolvability explicitly during the entire software lifecycle in order to prolong the productive lifetime of software systems. For this reason, many research studies have been proposed in this area both by researchers and industry practitioners. These studies comprise a spectrum of particular techniques and practices, covering various activities in software lifecycle. However, no systematic review has been conducted previously to provide an extensive overview of software architecture evolvability research.ObjectiveIn this work, we present such a systematic review of architecting for software evolvability. The objective of this review is to obtain an overview of the existing approaches in analyzing and improving software evolvability at architectural level, and investigate impacts on research and practice.MethodThe identification of the primary studies in this review was based on a pre-defined search strategy and a multi-step selection process.ResultsBased on research topics in these studies, we have identified five main categories of themes: (i) techniques supporting quality consideration during software architecture design, (ii) architectural quality evaluation, (iii) economic valuation, (iv) architectural knowledge management, and (v) modeling techniques. A comprehensive overview of these categories and related studies is presented.ConclusionThe findings of this review also reveal suggestions for further research and practice, such as (i) it is necessary to establish a theoretical foundation for software evolution research due to the fact that the expertise in this area is still built on the basis of case studies instead of generalized knowledge; (ii) it is necessary to combine appropriate techniques to address the multifaceted perspectives of software evolvability due to the fact that each technique has its specific focus and context for which it is appropriate in the entire software lifecycle. 
54|1||Systematic literature review of machine learning based software development effort estimation models|ContextSoftware development effort estimation (SDEE) is the process of predicting the effort required to develop a software system. In order to improve estimation accuracy, many researchers have proposed machine learning (ML) based SDEE models (ML models) since 1990s. However, there has been no attempt to analyze the empirical evidence on ML models in a systematic way.ObjectiveThis research aims to systematically analyze ML models from four aspects: type of ML technique, estimation accuracy, model comparison, and estimation context.MethodWe performed a systematic literature review of empirical studies on ML model published in the last two decades (1991–2010).ResultsWe have identified 84 primary studies relevant to the objective of this research. After investigating these studies, we found that eight types of ML techniques have been employed in SDEE models. Overall speaking, the estimation accuracy of these ML models is close to the acceptable level and is better than that of non-ML models. Furthermore, different ML models have different strengths and weaknesses and thus favor different estimation contexts.ConclusionML models are promising in the field of SDEE. However, the application of ML models in industry is still limited, so that more effort and incentives are needed to facilitate the application of ML models. To this end, based on the findings of this review, we provide recommendations for researchers as well as guidelines for practitioners. 
54|1||Is software âgreenâ? Application development environments and energy efficiency in open source applications|ContextThe energy efficiency of IT systems, also referred to as Green IT, is attracting more and more attention. While several researchers have focused on the energy efficiency of hardware and embedded systems, the role of application software in IT energy consumption still needs investigation.ObjectiveThis paper aims to define a methodology for measuring software energy efficiency and to understand the consequences of abstraction layers and application development environments for the energy efficiency of software applications.MethodWe first develop a measure of energy efficiency that is appropriate for software applications. We then examine how the use of application development environments relates to this measure of energy efficiency for a sample of 63 open source software applications.ResultsOur findings indicate that a greater use of application development environments – specifically, frameworks and external libraries – is more detrimental in terms of energy efficiency for larger applications than for smaller applications. We also find that different functional application types have distinctly different levels of energy efficiency, with text and image editing and gaming applications being the most energy inefficient due to their intense use of the processor.ConclusionWe conclude that different designs can have a significant impact on the energy efficiency of software applications. We have related the use of software application development environments to software energy efficiency suggesting that there may be a trade-off between development efficiency and energy efficiency. We propose new research to further investigate this topic. 
54|1||Towards an ontology-based retrieval of UML Class Diagrams|ContextSoftware Reuse has always been an important area amongst software companies in order to increase their productivity and the quality of their products, but code reuse is not the only answer for this. Nowadays, reuse techniques proposals include software designs or even software specifications. Therefore, this research focuses on software design, specifically on UML Class Diagrams. A semantic technology has been applied to facilitate the retrieval process for an effective reuse.ObjectiveThis research proposes an ontology-based retrieval technique by semantic similarity in order to support effective retrieval process for UML Class Diagrams. Since UML Class Diagrams are a de facto standard in the design stages of a Software Development Process, a good technique is needed to reuse them, i.e. reusing during the design stage instead of just the coding stages.MethodAn application ontology modeled using UML specifications was designed to compare UML Class Diagram element types. To measure their similarity, a survey was conducted amongst UML experts. Query expansion was improved by a domain ontology supporting the retrieval phase. The calculus of minimal distances in ontologies was solved using a shortest path algorithm.ResultsThe case study shows the domain ontology importance in the UML Class Diagram retrieval process as well as the importance of an element type expansion method, such as an application ontology. A correlation between the query complexity and retrieved elements has been identified, by analyzing results. Finally, a positive Return of Investment (ROI) was estimated using Poulin’s Model.ConclusionBecause Software Reuse has not to be limited to the coding stage, approaches to reuse design stage must be developed, i.e. UML Class Diagrams reuse. This approach proposes a technique for UML Class Diagrams retrieval, which is one important step towards reuse. Semantic technology combined with information retrieval improves the retrieval results. 
54|1||A systematic approach to integrate common timed security rules within a TEFSM-based system specification|ContextFormal methods are very useful in the software industry and are becoming of paramount importance in practical engineering techniques. They involve the design and modeling of various system aspects expressed usually through different paradigms. These different formalisms make the verification of global developed systems more difficult.ObjectiveIn this paper, we propose to combine two modeling formalisms, in order to express both functional and security timed requirements of a system to obtain all the requirements expressed in a unique formalism.MethodFirst, the system behavior is specified according to its functional requirements using Timed Extended Finite State Machine (TEFSM) formalism. Second, this model is augmented by applying a set of dedicated algorithms to integrate timed security requirements specified in Nomad language. This language is adapted to express security properties such as permissions, prohibitions and obligations with time considerations.ResultsThe proposed algorithms produce a global TEFSM specification of the system that includes both its functional and security timed requirements.ConclusionIt is concluded that it is possible to merge several requirement aspects described with different formalisms into a global specification that can be used for several purposes such as code generation, specification correctness proof, model checking or automatic test generation. In this paper, we applied our approach to a France Telecom Travel service to demonstrate its scalability and feasibility. 
54|1||Human and program factors affecting the maintenance of programs with deployed design patterns|ContextPractitioners may use design patterns to organize program code. Various empirical studies have investigated the effects of pattern deployment and work experience on the effectiveness and efficiency of program maintenance. However, results from these studies are not all consistent. Moreover, these studies have not considered some interesting factors, such as a maintainer’s prior exposure to the program under maintenance.ObjectiveThis paper aims at identifying what factors may contribute to the productivity of maintainers in the context of making correct software changes when they work on programs with deployed design patterns.MethodWe performed an empirical study involving 118 human subjects with three change tasks on a medium-sized program to explore the possible effects of a suite of six human and program factors on the productivity of maintainers, measured by the time taken to produce a correctly revised program in a course-based setting. The factors we studied include the deployment of design patterns and the presence of pattern-unaware solutions, as well as the maintainer’s prior exposure to design patterns, the subject program and the programming language, and prior work experience.ResultsAmong the factors under examination, we find that the deployment of design patterns, prior exposure to the program and the presence of pattern-unaware solutions are strongly correlated with the time taken to correctly complete maintenance tasks. We also report some interesting observations from the experiment.ConclusionA new factor, namely, the presence of pattern-unaware solutions, contributes to the efficient completion of maintenance tasks of programs with deployed design patterns. Moreover, we conclude from the study that neither prior exposure to design patterns nor prior exposure to the programming language is supported by sufficient evidences to be significant factors, whereas the subjects’ exposure to the program under maintenance is notably more important. 
54|1||Controlled composition and abstraction for bottom-up integration and verification of abstract components|This work proposes a method for improving the scalability of model-checking compositions in the bottom-up construction of abstract components. The approach uses model checking in the model construction process for testing the composite behaviors of components, including process deadlock and inconsistency in inter-component call sequences. Assuming a single processor model, the scalability issue is addressed by introducing operational models for synchronous/asynchronous inter-component message passing, which are designed to reduce spurious behaviors caused by typical parallel compositions. Together with two abstraction techniques, synchronized abstraction and projection abstraction, that hide verified internal communication behavior, this operational model helps to reduce the complexity of composition and verification.The approach is supported by the Marmot development framework, where the soundness of the approach is assured through horizontal verification as well as vertical verification. Application of the approach on a wireless sensor network application shows promising performance improvement with linear growth in memory usage for the vertically incremental verification of abstract components. 
54|10|http://www.sciencedirect.com/science/journal/09505849/54/10|A systematic review of code generation proposals from state machine specifications|ContextModel Driven Development (MDD) encourages the use of models for developing complex software systems. Following a MDD approach, modelling languages are used to diagrammatically model the structure and behaviour of object-oriented software, among which state-based languages (including UML state machines, finite state machines and Harel statecharts) constitute the most widely used to specify the dynamic behaviour of a system. However, generating code from state machine models as part of the final system constitutes one of the most challenging tasks due to its dynamic nature and because many state machine concepts are not supported by the object-oriented programming languages. Therefore, it is not surprising that such code generation has received great attention over the years.ObjectiveThe overall objective of this paper is to plot the landscape of published proposals in the field of object oriented code generation from state machine specifications, restricting the search neither to a specific context nor to a particular programming language.MethodWe perform a systematic, accurate literature review of published studies focusing on the object oriented implementation of state machine specifications.ResultsThe systematic review is based on a comprehensive set of 53 resources in all, which we have classified into two groups: pattern-based and not pattern-based. For each proposal, we have analysed both the state machine specification elements they support and the means the authors propose for their implementation. Additionally, the review investigates which proposals take into account desirable features to be considered in software development such as maintenance or reusability.ConclusionsOne of the conclusions drawn from the review is that most of the analysed works are based on a software design pattern. Another key finding is that many papers neither support several of the main components of the expressive richness of state machine specifications nor provide an implementation strategy that considers relevant qualitative aspects in software development. 
54|10||Impact of physical ambiance on communication, collaboration and coordination in agile software development: An empirical evaluation|ContextCommunication, collaboration and coordination are key enablers of software development and even more so in agile methods. The physical environment of the workspace plays a significant role in effective communication, collaboration, and coordination among people while developing software.ObjectiveIn this paper, we have studied and further evaluated empirically the effect of different constituents of physical environment on communication, coordination, and collaboration, respectively. The study aims to provide a guideline for prospective agile software developers.MethodA survey was conducted among software developers at a software development organization. To collect data, a survey was carried out along with observations, and interviews.ResultsIt has been found that half cubicles are ‘very effective’ for the frequency of communication. Further, half cubicles were discovered ‘effective’ but not ‘very effective’ for the quality/effectiveness of communication. It is found that half-height cubicles and status boards are ‘very effective’ for the coordination among team members according to the survey. Communal/discussion space is found to be ‘effective’ but not ‘very effective’ for coordination among team members. Our analysis also reveals that half-height glass barriers are ‘very effective’ during the individuals problem-solving activities while working together as a team. Infact, such a physically open environment appears to improve communication, coordination, and collaboration.ConclusionAccording to this study, an open working environment with only half-height glass barriers and communal space plays a major role in communication among team members. The presence of status boards significantly help in reducing unnecessary communication by providing the required information to individuals and therefore, in turn reduce distractions a team member may confront in their absence. As communication plays a significant role in improving coordination and collaboration, it is not surprising to find the effect of open working environment and status boards in improving coordination and collaboration. An open working environment increases the awareness among software developers e.g. who is doing what, what is on the agenda, what is taking place, etc. That in turn, improves coordination among them. A communal/discussion space helps in collaboration immensely. 
54|10||A visual analysis approach to validate the selection review of primary studies in systematic reviews|ContextSystematic Literature Reviews (SLRs) are an important component to identify and aggregate research evidence from different empirical studies. Despite its relevance, most of the process is conducted manually, implying additional effort when the Selection Review task is performed and leading to reading all studies under analysis more than once.ObjectiveWe propose an approach based on Visual Text Mining (VTM) techniques to assist the Selection Review task in SLR. It is implemented into a VTM tool (Revis), which is freely available for use.MethodWe have selected and implemented appropriate visualization techniques into our approach and validated and demonstrated its usefulness in performing real SLRs.ResultsThe results have shown that employment of VTM techniques can successfully assist in the Selection Review task, speeding up the entire SLR process in comparison to the conventional approach.ConclusionVTM techniques are valuable tools to be used in the context of selecting studies in the SLR process, prone to speed up some stages of SLRs. 
54|10||Reducing test effort: A systematic mapping study on existing approaches|ContextQuality assurance effort, especially testing effort, is often a major cost factor during software development, which sometimes consumes more than 50% of the overall development effort. Consequently, one major goal is often to reduce testing effort.ObjectiveThe main goal of the systematic mapping study is the identification of existing approaches that are able to reduce testing effort. Therefore, an overview should be presented both for researchers and practitioners in order to identify, on the one hand, future research directions and, on the other hand, potential for improvements in practical environments.MethodTwo researchers performed a systematic mapping study, focusing on four databases with an initial result set of 4020 articles.ResultsIn total, we selected and categorized 144 articles. Five different areas were identified that exploit different ways to reduce testing effort: approaches that predict defect-prone parts or defect content, automation, test input reduction approaches, quality assurance techniques applied before testing, and test strategy approaches.ConclusionThe results reflect an increased interest in this topic in recent years. A lot of different approaches have been developed, refined, and evaluated in different environments. The highest attention was found with respect to automation and prediction approaches. In addition, some input reduction approaches were found. However, in terms of combining early quality assurance activities with testing to reduce test effort, only a small number of approaches were found. Due to the continuous challenge of reducing test effort, future research in this area is expected. 
54|10||Are you biting off more than you can chew? A case study on causes and effects of overscoping in large-scale software engineering|ContextScope management is a core part of software release management and often a key factor in releasing successful software products to the market. In a market-driven case, when only a few requirements are known a priori, the risk of overscoping may increase.ObjectiveThis paper reports on findings from a case study aimed at understanding overscoping in large-scale, market-driven software development projects, and how agile requirements engineering practices may affect this situation.MethodBased on a hypothesis of which factors that may be involved in an overscoping situation, semi-structured interviews were performed with nine practitioners at a large, market-driven software company. The results from the interviews were validated by six (other) practitioners at the case company via a questionnaire.ResultsThe results provide a detailed picture of overscoping as a phenomenon including a number of causes, root causes and effects, and indicate that overscoping is mainly caused by operating in a fast-moving market-driven domain and how this ever-changing inflow of requirements is managed. Weak awareness of overall goals, in combination with low development involvement in early phases, may contribute to ‘biting off’ more than a project can ‘chew’. Furthermore, overscoping may lead to a number of potentially serious and expensive consequences, including quality issues, delays and failure to meet customer expectations. Finally, the study indicates that overscoping occurs also when applying agile requirements engineering practices, though the overload is more manageable and perceived to result in less wasted effort when applying a continuous scope prioritization, in combination with gradual requirements detailing and a close cooperation within cross-functional teams.ConclusionThe results provide an increased understanding of scoping as a complex and continuous activity, including an analysis of the causes, effects, and a discussion on possible impact of agile requirements engineering practices to the issue of overscoping. The results presented in this paper can be used to identify potential factors to address in order to achieve a more realistic project scope. 
54|10||Constructing models for predicting extract subclass refactoring opportunities using object-oriented quality metrics|ContextRefactoring is a maintenance task that refers to the process of restructuring software source code to enhance its quality without affecting its external behavior. Inspecting and analyzing the source code of the system under consideration to identify the classes in need of extract subclass refactoring (ESR) is a time consuming and costly process.ObjectiveThis paper explores the abilities of several quality metrics considered individually and in combination to predict the classes in need of ESR.MethodFor a given a class, this paper empirically investigates, using univariate logistic regression analysis, the abilities of 25 existing size, cohesion, and coupling metrics to predict whether the class is in need of restructuring by extracting a subclass from it. In addition, models of combined metrics based on multivariate logistic regression analysis were constructed and validated to predict the classes in need of ESR, and the best model is justifiably recommended. We explored the statistical relations between the values of the selected metrics and the decisions of the developers of six open source Java systems with respect to whether the classes require ESR.ResultsThe results indicate that there was a strong statistical relation between some of the quality metrics and the decision of whether ESR activity was required. From a statistical point of view, the recommended model of metrics has practical thresholds that lead to an outstanding classification of the classes into those that require ESR and those that do not.ConclusionThe proposed model can be applied to automatically predict the classes in need of ESR and present them as suggestions to developers working to enhance the system during the maintenance phase. In addition, the model is capable of ranking the classes of the system under consideration according to their degree of need of ESR. 
54|10||Requirements engineering tools: Capabilities, survey and assessment|ContextThere is a significant number of requirements engineering (RE) tools with different features and prices. However, existing RE tool lists do not provide detailed information about the features of the tools that they catalogue. It would therefore be interesting for both practitioners and tool developers to be aware of the state-of-the-art as regards RE tools.ObjectiveThis paper presents the results of a survey answered by RE tool vendors. The purpose of the survey was to gain an insight into how current RE tools support the RE process by means of concrete capabilities, and to what degree.MethodThe ISO/IEC TR 24766:2009 is a framework for assessing RE tools’ capabilities. A 146-item questionnaire based principally on the features covered by this international guideline was sent to major tool vendors worldwide. A descriptive statistical study was then carried out to provide comparability, and bivariate correlation tests were also applied to measure the association between different variables. A sample of the tools was subjected to neutral assessment and an interrater reliability analysis was performed to ensure the reliability of the results.ResultsThe 38 participants sent back their answers. Most tools are delivered under a proprietary license, and their licenses are not free. A growing number of them facilitate Web access. Moreover, requirements elicitation exemplifies the best supported category of features in this study, whereas requirements modeling and management are the most badly supported categories.ConclusionThe RE process seems to be well covered by current RE tools, but there is still a certain margin for amelioration, principally with regard to requirements modeling, open data model and data integration features. These subjects represent areas for improvement for RE tool developers. Practitioners might also obtain useful ideas from the study to be taken into account when selecting an appropriate RE tool to be successfully applied to their work. 
54|11|http://www.sciencedirect.com/science/journal/09505849/54/11|Quality indicators for business process models from a gateway complexity perspective|ContextQuality assurance of business process models has been recognized as an important factor for modeling success at an enterprise level. Since quality of models might be subject to different interpretations, it should be addressed in the most objective way, by the application of measures. That said, however, assessment of measurement results is not a straightforward task: it requires the identification of relevant threshold values, which are able to distinguish different levels of process model quality.ObjectiveSince there is no consensual technique for obtaining these values, this paper proposes the definition of thresholds for gateway complexity measures based on the application of statistical techniques on empirical data.MethodTo this end, we conducted a controlled experiment that evaluates quality characteristics of understandability and modifiability of process models in two different runs. The thresholds obtained were validated in a replication of the experiment.ResultsThe thresholds for gateway complexity measures are instrumental as guidelines for novice modelers. A tool for supporting business process model measurement and improvement is described, based on the automatic application of measurement, and assessment as well as derivation of advice about how to improve the quality of the model.ConclusionIt is concluded that thresholds classified business process models in the specific level of understandability and modifiability, so these thresholds were good and useful for decision-making. 
54|11||A Process Framework for Global Software Engineering Teams|ContextGlobal Software Engineering (GSE) continues to experience substantial growth and is fundamentally different to collocated development. As a result, software managers have a pressing need for support in how to successfully manage teams in a global environment. Unfortunately, de facto process frameworks such as the Capability Maturity Model Integration (CMMI®) do not explicitly cater for the complex and changing needs of global software management.ObjectiveTo develop a Global Teaming (GT) process area to address specific problems relating to temporal, cultural, geographic and linguistic distance which will meet the complex and changing needs of global software management.MethodWe carried out three in-depth case studies of GSE within industry from 1999 to 2007. To supplement these studies we conducted three literature reviews. This allowed us to identify factors which are important to GSE. Based on a gap analysis between these GSE factors and the CMMI®, we developed the GT process area. Finally, the literature and our empirical data were used to identify threats to software projects if these processes are not implemented.ResultsOur new GT process area brings together practices drawn from the GSE literature and our previous empirical work, including many socio-technical factors important to global software development. The GT process area presented in this paper encompasses recommended practices that can be used independently or with existing models. We found that if managers are not proactive in implementing new GT practices they are putting their projects under threat of failure. We therefore include a list of threats that if ignored could have an adverse effect on an organization’s competitive advantage, employee satisfaction, timescales, and software quality.ConclusionThe GT process area and associated threats presented in this paper provides both a guide and motivation for software managers to better understand how to manage technical talent across the globe. 
54|11||Static analysis of Android programs|ContextAndroid is a programming language based on Java and an operating system for embedded and mobile devices, whose upper layers are written in the Android language itself. As a language, it features an extended event-based library and dynamic inflation of graphical views from declarative XML layout files. A static analyzer for Android programs must consider such features, for correctness and precision.ObjectiveOur goal is to extend the Julia static analyzer, based on abstract interpretation, to perform formally correct analyses of Android programs. This article is an in-depth description of such an extension, of the difficulties that we faced and of the results that we obtained.MethodWe have extended the class analysis of the Julia analyzer, which lies at the heart of many other analyses, by considering some Android key specific features such as the potential existence of many entry points to a program and the inflation of graphical views from XML through reflection. We also have significantly improved the precision of the nullness analysis on Android programs.ResultsWe have analyzed with Julia most of the Android sample applications by Google and a few larger open-source programs. We have applied tens of static analyses, including classcast, dead code, nullness and termination analysis. Julia has found, automatically, bugs, flaws and inefficiencies both in the Google samples and in the open-source applications.ConclusionJulia is the first sound static analyzer for Android programs, based on a formal basis such as abstract interpretation. Our results show that it can analyze real third-party Android applications, without any user annotation of the code, yielding formally correct results in at most 7 min and on standard hardware. Hence it is ready for a first industrial use. 
54|11||Automated refactoring to the Strategy design pattern|ContextThe automated identification of code fragments characterized by common design flaws (or “code smells”) that can be handled through refactoring, fosters refactoring activities, especially in large code bases where multiple developers are engaged without a detailed view on the whole system. Automated refactoring to design patterns enables significant contributions to design quality even from developers with little experience on the use of the required patterns.ObjectiveThis work targets the automated identification of refactoring opportunities to the Strategy design pattern and the elimination through polymorphism of respective “code smells” that are related to extensive use of complex conditional statements.MethodAn algorithm is introduced for the automated identification of refactoring opportunities to the Strategy design pattern. Suggested refactorings comprise conditional statements that are characterized by analogies to the Strategy design pattern, in terms of the purpose and selection mode of strategies. Moreover, this work specifies the procedure for refactoring to Strategy the identified conditional statements. For special cases of these statements, a technique is proposed for total replacement of conditional logic with method calls of appropriate concrete Strategy instances. The identification algorithm and the refactoring procedure are implemented and integrated in the JDeodorant Eclipse plug-in. The method is evaluated on a set of Java projects, in terms of quality of the suggested refactorings and run-time efficiency. The relevance of the identified refactoring opportunities is verified by expert software engineers.ResultsThe identification algorithm recalled, from the projects used during evaluation, many of the refactoring candidates that were identified by the expert software engineers. Its execution time on projects of varying size confirmed the run-time efficiency of this method.ConclusionThe proposed method for automated refactoring to Strategy contributes to simplification of conditional statements. Moreover, it enhances system extensibility through the Strategy design pattern. 
54|11||Analyzing the understandability of Requirements Engineering languages for CSCW systems: A family of experiments|ContextA collaborative system is a special kind of software whose users can perform collaboration, communication and collaboration tasks. These systems usually have a high number of non-functional requirements, resulting from the users’ need of being aware of other users with whom to collaborate, that is, the workspace awareness.ObjectiveThis paper aims at evaluating two Requirements Engineering languages i* and CSRML (an extension of i*) in order to determine which is the most suitable one to specify requirements of collaborative systems, taking into account their special characteristics regarding collaboration and awareness.MethodWe performed a family of experiments comprising an original experiment and two replicas. They were performed by 30, 45 and 9 Computer Science students, respectively, from Spain and Argentina. These subjects filled in two understandability questionnaires once they analyzed the requirements models of two systems: an e-learning collaborative system and a conference review system with some collaborative aspects support. Both models were specified by using the evaluated languages.ResultsThe statistical analysis of the family of experiments showed that the understandability was higher for the models specified with CSRML than for those specified with i*, especially for collaborative aspects. This result was also confirmed by the meta-analysis conducted.ConclusionsCSRML surpasses i* when modeling collaborative systems requirements models due to the specific expressiveness introduced to represent collaboration between users and awareness and the new resorts to manage actors and roles. 
54|11||Software quality assurance economics|ContextSoftware companies invest in quality assurance in order to lower software development and maintenance cost, and to increase revenue and profit margins. To contribute to increase of net income, a quality assurance organization has to consider cost and value of the testware involved in assuring quality of software artifacts, such as requirements, specifications, designs, and code.ObjectiveThis paper proposes a set of economic metrics: testware return on investment, inflation, and cost and value sensitivity to artifact changes and time passage. The paper proposes a set of guidelines on lowering testware cost, on increasing value, on maximizing return on investment, and on when to release.MethodThis paper presents an industrial case study data on the relation between test case cost and value, and on cost and value sensitivity to time passage and artifact changes.ResultsThe industrial case study showed return on investment on test cases of up to 200%, deflation of up to −2% per month, undesirable economic effects, such as test case cost outpacing test case value and rapid test case value depreciation based on time passage.ConclusionA viable QA organization should measure and improve test case return on investment, inflation, and cost and value sensitivity to artifact changes and time passage. 
54|11||Domain model-driven software engineering: A method for discovery of dependency links|ContextDependency management often suffers from labor intensity and complexity in creating and maintaining the dependency relations in practice. This is even more critical in a distributed development, in which developers are geographically distributed and a wide variety of tools is used. In those settings, different interpretations of software requirements or usage of different terminologies make it challenging to predict the change impact.Objectiveis (a) to describe a method facilitating change management in geographically distributed software engineering by effective discovery and establishment of dependency links using domain models; (b) to evaluate the effectiveness of the proposed method.MethodA domain model, providing a common reference point, is used to manage development objects and to automatically support dependency discovery. We propose to associate (annotate) development objects with the concepts from the model. These associations are used to compute dependency among development objects, and are stepwise refined to direct dependency links (i.e. enabling product traceability). To evaluate the method, we conducted a laboratory-based randomized experiment on two real cases. Six participants were using an implemented prototype and two comparable tools to perform simulated tasks.ResultsIn the paper we elaborate on the proposed method discussing its functional steps. Results from the experiment show that the method can be effectively used to assist in discovery of dependency links. Users have discovered on average fourteen percent more dependency links than by using the comparable tools.ConclusionsThe proposed method advocates the use of domain models throughout the whole development life-cycle and is apt to facilitate multi-site software engineering. The experimental study and results suggest that the method is effective in the discovery of dependencies among development objects. 
54|11||An exploratory study on the accuracy of FPA to COSMIC measurement method conversion types|BackgroundFunctional size measurement methods are increasingly being adopted by software organizations due to the benefits they provide to software project managers. The Function Point Analysis (FPA) measurement method has been used extensively and globally in software organizations. The COSMIC measurement method is considered a second generation FSM method, because of the novel aspects it brings to the FSM field. After the COSMIC method was proposed, the issue of convertibility from FPA to COSMIC method arose, the main problem being the ability to convert FPA historical data to the corresponding COSMIC Function Point (CFP) data with a high level of accuracy, which would give organizations the ability to use the data in their future planning. Almost all the convertibility studies found in the literature involve converting FPA measures to COSMIC measures statistically, based on the final size generated by both methods.ObjectivesThis paper has three main objectives. The first is to explore the accuracy of the conversion type that converts FPA measures to COSMIC measures statistically, and that of the type that converts FPA transaction function measures to COSMIC measures. The second is to propose a new conversion type that predicts the number of COSMIC data movements based on the number of file type references referenced by all the elementary processes in a single application. The third is to compare the accuracy of our proposed conversion type with the other two conversion types found in the literature.MethodOne dataset from the management information systems domain was used to compare the accuracy of all three conversion types using a systematic conversion approach that applies three regression models: Ordinary Least Squares, Robust Least Trimmed Squares, and logarithmic transformation were used. Four datasets from previous studies were used to evaluate the accuracy of the three conversion types, to which the Leave One Out Cross Validation technique was applied to obtain the measures of fitting accuracy.ResultsThe conversion type most often used as well as the conversion type based on transaction function size were found to generate nonlinear, inaccurate and invalid results according to measurement theory. In addition, they produce a loss of measurement information in the conversion process, because of the FPA weighting system and FPA structural problems, such as illegal scale transformation. Our proposed conversion type avoids the problems inherent in the other two types but not the nonlinearity problem. Furthermore, the proposed conversion type has been found to be more accurate than the other types when the COSMIC functional processes comprise dataset applications that are systematically larger than their corresponding FPA elementary processes, or when the processes vary from small to large. Finally, our proposed conversion type delivered better results over the tested datasets, whereas, in general, there is no statistical significant difference between the accuracy of the conversion types examined for every dataset, particularly the conversion type most often used is not the most accurate.ConclusionsOur proposed conversion type achieves accurate results over the tested datasets. However, the lack of knowledge needed to use it over all the datasets in the literature limits the value of this conclusion. Consequently, practitioners converting from FPA to COSMIC should not stay with only one conversion type, assuming that it is the best. In order to achieve a high level of accuracy in the conversion process, all three conversion types must be tested via a systematic conversion approach. 
54|11||Quality evaluation for Model-Driven Web Engineering methodologies|ContextThere are lots of approaches or methodologies in the Model-Driven Web Engineering (MDWE) context to develop Web Applications without reaching a consensus on the use of standards and scarcity of both, practical experience and tool support.ObjectiveModel-Driven Web Engineering (MDWE) methodologies are constantly evolving. Moreover, Quality is a very important factor to identify within a methodology as it defines processes, techniques and artifacts to develop Web Applications. For this reason, when analyzing a methodology, it is not only necessary to evaluate quality, but also to find out how to improve it. The main goal of this paper is to develop a set of Quality Characteristics and Sub-Characteristics for MDWE approaches based on ISO/IEC standards.MethodFrom the software products context, some widely standards proposed, such as ISO/IEC 9126 or ISO/IEC 25000, suggest a Quality Model for software products, although up to now, there are no standard methods to assess quality on MDWE methodologies. Such methodologies can be organized into Properties, thus, a methodology has artifacts, processes and techniques. Then, each item is evaluated through a set of appropriate Quality Characteristics, depending on its nature. This paper proposes to evaluate a methodology as a product itself.ResultsThis paper recommends a set of Quality Characteristics and Sub-Characteristics based on these standards in order to evaluate MDWE methodologies quality. Additionally, it defines an agile way to relate these Quality Sub-Characteristics to Properties with the sole purpose of not only analyzing, but also assessing and improving MDWE methodologies.ConclusionsThe application of these Quality Characteristics and Sub-Characteristics could promote efficiency in methodologies since this kind of assessment enhances both the understanding of strengths and weaknesses of approaches. 
54|11||A framework for pathologies of message sequence charts|ContextIt is known that a Message Sequence Chart (MSC) specification can contain different types of pathology. However, definitions of different types of pathology and the problems caused by pathologies are unclear, let alone the relationships between them. In this circumstance, it can be problematic for software engineers to accurately predict the possible problems that may exist in implementations of MSC specifications and to trace back to the design problems in MSC specifications from the observed problems of an implementation.ObjectiveWe focus on generating a clearer view on MSC pathologies and building formal relationships between pathologies and the problems that they may cause.MethodBy concentrating on the problems caused by pathologies, a categorisation of problems that a distributed system may suffer is first introduced. We investigate the different types of problems and map them to categories of pathologies. Thus, existing concepts related to pathology are refined and necessary concepts in the pathology framework are identified. Finally, we formally prove the relationships between the concepts in the framework.ResultsA pathology framework is established as desired based on a restriction that considers problematic scenarios with a single undesirable event. In this framework, we define disjoint categories of both pathologies and the problems caused; the identified types of pathology are successfully mapped to the problems that they may cause.ConclusionThe framework achieved in this paper introduces taxonomies into and clarifies relationships between concepts in research on MSC pathologies. The taxonomies and relationships in the framework can help software engineers to predict problems and verify MSC specifications. The single undesirable event restriction not only enables a categorisation of pathological scenarios, but also has the potential practical benefit that a software engineer can concentrate on key problematic scenarios. This may make it easier to either remove pathologies from an MSC specification MM or test an implementation developed from MM for potential problems resulting from such pathologies. 
54|12|http://www.sciencedirect.com/science/journal/09505849/54/12|Increasing clone maintenance support by unifying clone detection and refactoring activities|ContextClone detection tools provide an automated mechanism to discover clones in source code. On the other side, refactoring capabilities within integrated development environments provide the necessary functionality to assist programmers in refactoring. However, we have observed a gap between the processes of clone detection and refactoring.ObjectiveIn this paper, we describe our work on unifying the code clone maintenance process by bridging the gap between clone detection and refactoring.MethodThrough an Eclipse plug-in called CeDAR (Clone Detection, Analysis, and Refactoring), we forward clone detection results to the refactoring engine in Eclipse. In this case, the refactoring engine is supplied with information about the detected clones to which it can then determine those clones that can be refactored. We describe the extensions to Eclipse’s refactoring engine to allow clones with additional similarity properties to be refactored.ResultsOur evaluation of open source artifacts shows that this process yields considerable increases in the instances of clone groups that may be suggested to the programmer for refactoring within Eclipse.ConclusionBy unifying the processes of clone detection and refactoring, in addition to providing extensions to the refactoring engine of an IDE, the strengths of both processes (i.e., more significant detection capabilities and an established framework for refactoring) can be garnered. 
54|12||Has open source software been institutionalized in organizations or not?|ContextAlmost a decade ago, researchers in information systems and analysts of the information technology (IT) industry were predicting a bright future for open source software (OSS). Recent examples appear to lend support to this, but there exist many detractors of OSS and resistance to the transformation it creates. Thus, it is relevant to take a closer look at the institutionalization of OSS.ObjectiveThis paper evaluates the extent of OSS institutionalization in organizations. A practice or innovation is said to be institutionalized when it is taken-for-granted and its use becomes the norm.MethodDrawing on institutional theory, the underlying concept of organizing vision and the rhetorical theory of diffusion of innovations, we analyze OSS institutionalization through the observation of the evolution of the public discourse about OSS and, simultaneously, the observation of the rate of adoption or diffusion of OSS in organizations.ResultsOSS has become institutionalized for many back-end applications and is gradually becoming institutionalized for some front-end applications, mainly in small and medium enterprises but also in organizations in the financial, publishing, education, government and public sectors. Using the rhetorical theory of diffusion of innovations in tandem with the concept of organizing vision, we provide a deep understanding of the institutionalization of OSS by showing that it has not only diffused among organizations, but is also taken-for-granted in thought and social action. The positive tone and prominence of the public discourse on OSS have an important role to play in its institutionalization.ConclusionThe institutionalization of OSS in organizations cannot be underestimated by IT and business executives as well as key players in the IT industry. Future research efforts should be pursued and directed toward the institutionalization of particular OSS applications in a variety of industries and geographic regions. 
54|12||The maturity of maturity model research: A systematic mapping study|ContextMaturity models offer organizations a simple but effective possibility to measure the quality of their processes. Emerged out of software engineering, the application fields have widened and maturity model research is becoming more important. During the last two decades the publication amount steadily rose as well. Until today, no studies have been available summarizing the activities and results of the field of maturity model research.ObjectiveThe objective of this paper is to structure and analyze the available literature of the field of maturity model research to identify the state-of-the-art research as well as research gaps.MethodA systematic mapping study was conducted. It included relevant publications of journals and IS conferences. Mapping studies are a suitable method for structuring a broad research field concerning research questions about contents, methods, and trends in the available publications.ResultsThe mapping of 237 articles showed that current maturity model research is applicable to more than 20 domains, heavily dominated by software development and software engineering. The study revealed that most publications deal with the development of maturity models and empirical studies. Theoretical reflective publications are scarce. Furthermore, the relation between conceptual and design-oriented maturity model development was analyzed, indicating that there is still a gap in evaluating and validating developed maturity models. Finally, a comprehensive research framework was derived from the study results and implications for further research are given.ConclusionThe mapping study delivers the first systematic summary of maturity model research. The categorization of available publications helps researchers gain an overview of the state-of-the-art research and current research gaps. The proposed research framework supports researchers categorizing their own projects. In addition, practitioners planning to use a maturity model may use the study as starting point to identify which maturity models are suitable for their domain and where limitations exist. 
54|12||Model-Driven Engineering as a new landscape for traceability management: A systematic literature review|ContextModel-Driven Engineering provides a new landscape for dealing with traceability in software development.ObjectiveOur goal is to analyze the current state of the art in traceability management in the context of Model-Driven Engineering.MethodWe use the systematic literature review based on the guidelines proposed by Kitchenham. We propose five research questions and six quality assessments.ResultsOf the 157 relevant studies identified, 29 have been considered primary studies. These studies have resulted in 17 proposals.ConclusionThe evaluation shows that the most addressed operations are storage, CRUD and visualization, while the most immature operations are exchange and analysis traceability information. 
54|12||A HCI technique for improving requirements elicitation|ContextTo develop usable software we need to understand the users that will interact with the system. Personas is a HCI technique that gathers information about users in order to comprehend their characteristics. This information is used to define fictitious persons on which development should focus. Personas provides an understanding of the user, often overlooked in SE developments.ObjectiveThe goal of our research is to modify Personas to readily build the technique into the requirements stage of regular SE developments.MethodWe tried to apply Cooper’s version of the Personas technique and we found shortcomings in both the definition of the procedure to be enacted and the formalization of the product resulting from the execution of each step of the Personas technique. For each of these limitations (up to a total of 11), we devised an improvement to be built into Personas. We have incorporated these improvements into a SE version of Personas. The improved Personas avoid the weaknesses encountered by an average software developer unfamiliar with HCI techniques applying the original Personas.ResultsWe aim to improve requirements elicitation through the use of Personas. We have systematized and formalized Personas in the SE tradition in order to build this new version of the technique into the requirements stage. We have applied our proposal in an application example.ConclusionThe integration of Personas into the SE requirements stage might improves the understanding of what the software product should do and how it should behave. We have modified the HCI Personas technique to comply with the levels of systematization required by SE. We have enriched the SE requirements process by incorporating Personas activities into requirements activities. Requirements elicitation and requirements analysis are the RE activities most affected by incorporating Personas. 
54|12||Guest Editorial: Special section on software reliability and security|
54|12||Assessing practical usefulness and performance of the PREDIQT method: An industrial case study|ContextWhen adapting a system to new usage patterns, processes or technologies, it is necessary to foresee the implications of the architectural design changes on system quality. Examination of quality outcomes through implementation of the different architectural design alternatives is often unfeasible. We have developed a method called PREDIQT with the aim to facilitate model-based prediction of impacts of architectural design changes on system quality. A recent case study indicated feasibility of the PREDIQT method when applied on a real-life industrial system. The promising results encouraged further and more structured evaluation of PREDIQT.ObjectiveThis paper reports on the experiences from applying the PREDIQT method in a second and more recent case study – on a real-life industrial system from another domain and with different system characteristics, as compared to the previous case study. The objective was to evaluate the method in a fully realistic setting and with respect to carefully defined criteria.MethodThe case study conducted the first two phases of PREDIQT in their entirety, while the last (third) phase was partially covered. In addition, the method was assessed through a thought experiment-based evaluation of predictions and a postmortem review. All prediction models were developed during the analysis and the entire target system was analyzed in a fully realistic setting.ResultsThe evaluation argues that the prediction models are sufficiently expressive and comprehensible. It is furthermore argued that PREDIQT: facilitates predictions such that informed decisions can be made; is cost-effective; and facilitates knowledge management.ConclusionThe experiences and results obtained indicate that the PREDIQT method can be carried out with limited resources, on a real-life system, and result in useful predictions. Furthermore, the observations indicate that the method, particularly its process, facilitates understanding of the system architecture and its quality characteristics, and contributes to structured knowledge management. 
54|12||Comprehensive two-level analysis of role-based delegation and revocation policies with UML and OCL|ContextRole-based access control (RBAC) has become the de facto standard for access management in various large-scale organizations. Often role-based policies must implement organizational rules to satisfy compliance or authorization requirements, e.g., the principle of separation of duty (SoD). To provide business continuity, organizations should also support the delegation of access rights and roles, respectively. This, however, makes access control more complex and error-prone, in particular, when delegation concepts interplay with SoD rules.ObjectiveA systematic way to specify and validate access control policies consisting of organizational rules such as SoD as well as delegation and revocation rules shall be developed. A domain-specific language for RBAC as well as delegation concepts shall be made available.MethodIn this paper, we present an approach to the precise specification and validation of role-based policies based on UML and OCL. We significantly extend our earlier work, which proposed a UML-based domain-specific language for RBAC, by supporting delegation and revocation concepts.ResultWe show the appropriateness of our approach by applying it to a banking application. In particular, we give three scenarios for validating the interplay between SoD rules and delegation/revocation.ConclusionTo the best of our knowledge, this is the first attempt to formalize advanced RBAC concepts, such as history-based SoD as well as various delegation and revocation schemes, with UML and OCL. With the rich tool support of UML, we believe our work can be employed to validate and implement real-world role-based policies. 
54|12||Validation of SDL-based architectural design models using communication-based coverage criteria|ContextAs the capability to automatically generate code from different models becomes more sophisticated, it is critical that these models be adequately tested for quality assurance prior to code generation.ObjectiveAlthough simulation-based black-box testing strategies exist for these models, it is important that we also employ white-box testing strategies similar to those used to test implementation code to further validate the quality of these models.MethodWe apply coverage testing to architectural design models represented by SDL, a Specification and Description Language. Our previous study defined a methodology for automatic test generation with respect to two structural-based criteria, all-node and all-edge, for each individual SDL process. In this paper, we present new coverage criteria such as n-step-message-transfer and sender–receiver-round-trip, aiming at the communication between different SDL processes.ResultsA test generator using innovative backward tracking and forward validation has been implemented to support these criteria, guiding test generation to detect bugs which could not be revealed by test cases generated only with respect to the all-node and all-edge criteria.ConclusionsThe results of our case study support the feasibility of using our test generator to create test cases satisfying the proposed communication-based criteria. 
54|12||Verification and analysis of domain-specific models of physical characteristics in embedded control software|ContextA considerable portion of the software systems today are adopted in the embedded control domain. Embedded control software deals with controlling a physical system, and as such models of physical characteristics become part of the embedded control software.ObjectiveDue to the evolution of system properties and increasing complexity, faults can be left undetected in these models of physical characteristics. Therefore, their accuracy must be verified at runtime. Traditional runtime verification techniques that are based on states/events in software execution are inadequate in this case. The behavior suggested by models of physical characteristics cannot be mapped to behavioral properties of software. Moreover, implementation in a general-purpose programming language makes these models hard to locate and verify. Therefore, this paper proposes a novel approach to perform runtime verification of models of physical characteristics in embedded control software.MethodThe development of an approach for runtime verification of models of physical characteristics and the application of the approach to two industrial case studies from the printing systems domain.ResultsThis paper presents a novel approach to specify models of physical characteristics using a domain-specific language, to define monitors that detect inconsistencies by exploiting redundancy in these models, and to realize these monitors using an aspect-oriented approach. We complement runtime verification with static analysis to verify the composition of domain-specific models with the control software written in a general-purpose language.ConclusionsThe presented approach enables runtime verification of implemented models of physical characteristics to detect inconsistencies in these models, as well as broken hardware components and wear and tear of hardware in the physical system. The application of declarative aspect-oriented techniques to realize runtime verification monitors increases modularity and provides the ability to statically verify this realization. The complementary static and runtime verification techniques increase the reliability of embedded control software. 
54|2|http://www.sciencedirect.com/science/journal/09505849/54/2|Drivers of agile software development use: Dialectic interplay between benefits and hindrances|ContextAgile software development with its emphasis on producing working code through frequent releases, extensive client interactions and iterative development has emerged as an alternative to traditional plan-based software development methods. While a number of case studies have provided insights into the use and consequences of agile, few empirical studies have examined the factors that drive the adoption and use of agile.ObjectiveWe draw on intention-based theories and a dialectic perspective to identify factors driving the use of agile practices among adopters of this software development methodology.MethodData for the study was gathered through an anonymous online survey of software development professionals. We requested participation from members of a selected list of online discussion groups, and received 98 responses.ResultsOur analyses reveal that subjective norm and training play a significant role in influencing software developers’ use of agile processes and methods, while perceived benefits and perceived limitations are not primary drivers of agile use among adopters. Interestingly, perceived benefit emerges as a significant predictor of agile use only if adopters face hindrances to their agile practices.ConclusionWe conclude that research in the adoption of software development innovations should examine the effects of both enabling and detracting factors and the interactions between them. Since training, subjective norm, and the interplay between perceived benefits and perceived hindrances appear to be key factors influencing the adoption of agile methods, researchers can focus on how to (a) perform training on agile methods more effectively, (b) facilitate the dialog between developers and managers about perceived benefits and hindrances, and (c) capitalize on subjective norm to publicize the benefits of agile methods within an organization. Further, when managing the transition to new software development methods, we recommend that practitioners adapt their strategies and tactics contingent on the extent of perceived hindrances to the change. 
54|2||Conceptual framework for business processes compositional verification|ContextTo guarantee the success of Business Process Modelling (BPM) it is necessary to check whether the activities and tasks described by Business Processes (BPs) are sound and well coordinated.ObjectiveThis article describes and validates a Formal Compositional Verification Approach (FCVA) that uses a Model-Checking (MC) technique to specify and verify BPs.MethodThis is performed using the Communicating Sequential Processes +Time (CSP+T) process calculus, which adds new constructions to timed Business Process Model and Notation (BPMN) modelling entities for non- functional requirement specification.ResultsUsing our proposal we are able to specify the BP Task Model (BPTM) associated with BPs by formalising the timed BPMN notational elements. The proposal also allows us to apply MC to BPTM verification. A real-life example of verifying a BPTM in the field of Customer Relationship Management (CRM) is discussed as a practical application of FCVA.ConclusionThis approach facilitates the verification of complex BPs from independently verified local processes, and establishes a feasible way to use process calculi to verify BPs using state-of-the-art MC tools. 
54|2||Field study on requirements engineering: Investigation of artefacts, project parameters, and execution strategies|ContextRequirements Engineering (RE) is a critical discipline mostly driven by uncertainty, since it is influenced by the customer domain or by the development process model used. Volatile project environments restrict the choice of methods and the decision about which artefacts to produce in RE.ObjectiveWe aim to investigate RE processes in successful project environments to discover characteristics and strategies that allow us to elaborate RE tailoring approaches in the future.MethodWe perform a field study on a set of projects at one company. First, we investigate by content analysis which RE artefacts were produced in each project and to what extent they were produced. Second, we perform qualitative analysis of semi-structured interviews to discover project parameters that relate to the produced artefacts. Third, we use cluster analysis to infer artefact patterns and probable RE execution strategies, which are the responses to specific project parameters. Fourth, we investigate by statistical tests the effort spent in each strategy in relation to the effort spent in change requests to evaluate the efficiency of execution strategies.ResultsWe identified three artefact patterns and corresponding execution strategies. Each strategy covers different project parameters that impact the creation of certain artefacts. The effort analysis shows that the strategies have no significant differences in their effort and efficiency.ConclusionsIn contrast to our initial assumption that an increased effort in requirements engineering lowers the probability of change requests or project failures in general, our results show no statistically significant difference between the efficiency of the strategies. In addition, it turned out that many parameters considered as the main causes for project failures can be successfully handled. Hence, practitioners can apply the artefact patterns and related project parameters to tailor the RE process according to individual project characteristics. 
54|2||Fault-based test suite prioritization for specification-based testing|ContextExisting test suite prioritization techniques usually rely on code coverage information or historical execution data that serve as indicators for estimating the fault-detecting ability of test cases. Such indicators are primarily empirical in nature and not theoretically driven; hence, they do not necessarily provide sound estimates. Also, these techniques are not applicable when the source code is not available or when the software is tested for the first time.ObjectiveWe propose and develop the novel notion of fault-based prioritization of test cases which directly utilizes the theoretical knowledge of their fault-detecting ability and the relationships among the test cases and the faults in the prescribed fault model, based on which the test cases are generated.MethodWe demonstrate our approach of fault-based prioritization by applying it to the testing of the implementation of logical expressions against their specifications. We then validate our proposal by an empirical study that evaluates the effectiveness of prioritization techniques using two different metrics.ResultsA theoretically guided fault-based prioritization technique generally outperforms other techniques under study, as assessed by two different metrics. Our empirical results also show that the technique helps to reveal all target faults by executing only about 72% of the prioritized test suite, thereby reducing the effort required in testing.ConclusionsThe fault-based prioritization approach is not only applicable to the instance empirically validated in this paper, but should also be adaptable to other fault-based testing strategies. We also envisage new research directions to be opened up by our work. 
54|2||Code churn estimation using organisational and code metrics: An experimental comparison|ContextSource code revision control systems contain vast amounts of data that can be exploited for various purposes. For example, the data can be used as a base for estimating future code maintenance effort in order to plan software maintenance activities. Previous work has extensively studied the use of metrics extracted from object-oriented source code to estimate future coding effort. In comparison, the use of other types of metrics for this purpose has received significantly less attention.ObjectiveThis paper applies machine learning techniques to unveil predictors of yearly cumulative code churn of software projects on the basis of metrics extracted from revision control systems.MethodThe study is based on a collection of object-oriented code metrics, XML code metrics, and organisational metrics. Several models are constructed with different subsets of these metrics. The predictive power of these models is analysed based on a dataset extracted from eight open-source projects.ResultsThe study shows that a code churn estimation model built purely with organisational metrics is superior to one built purely with code metrics. However, a combined model provides the highest predictive power.ConclusionThe results suggest that code metrics in general, and XML metrics in particular, are complementary to organisational metrics for the purpose of estimating code churn. 
54|2||On the relationship of concern metrics and requirements maintainability|ContextMaintainability has become one of the most essential attributes of software quality, as software maintenance has shown to be one of the most costly and time-consuming tasks of software development. Many studies reveal that maintainability is not often a major consideration in requirements and design stages, and software maintenance costs may be reduced by a more controlled design early in the software life cycle. Several problem factors have been identified as harmful for software maintainability, such as lack of upfront consideration of proper modularity choices. In that sense, the presence of crosscutting concerns is one of such modularity anomalies that possibly exert negative effects on software maintainability. However, to the date there is little or no knowledge about how characteristics of crosscutting concerns, observable in early artefacts, are correlated with maintainability.ObjectiveIn this setting, this paper introduces an empirical analysis where the correlation between crosscutting properties and two ISO/IEC 9126 maintainability attributes, namely changeability and stability, is presented.MethodThis correlation is based on the utilization of a set of concern metrics that allows the quantification of crosscutting, scattering and tangling.ResultsOur study confirms that a change in a crosscutting concern is more difficult to be accomplished and that artefacts addressing crosscutting concerns are found to be less stable later as the system evolves. Moreover, our empirical analysis reveals that crosscutting properties introduce non-syntactic dependencies between software artefacts, thereby decreasing the quality of software in terms of changeability and stability as well. These subtle dependencies cannot be easily detected without the use of concern metrics.ConclusionThe correlation provides evidence that the presence of certain crosscutting properties negatively affects to changeability and stability. The whole analysis is performed using as target cases three software product lines, where maintainability properties are of upmost importance not only for individual products but also for the core architecture of the product line. 
54|3|http://www.sciencedirect.com/science/journal/09505849/54/3|IT Service Management Process Improvement based on ISO/IEC 15504: A systematic review|ContextIn recent years, many software companies have considered Software Process Improvement (SPI) as essential for successful software development. These companies have also shown special interest in IT Service Management (ITSM). SPI standards have evolved to incorporate ITSM best practices.ObjectiveThis paper presents a systematic literature review of ITSM Process Improvement initiatives based on the ISO/IEC 15504 standard for process assessment and improvement.MethodA systematic literature review based on the guidelines proposed by Kitchenham and the review protocol template developed by Biolchini et al. is performed.ResultsTwenty-eight relevant studies related to ITSM Process Improvement have been found. From the analysis of these studies, nine different ITSM Process Improvement initiatives have been detected. Seven of these initiatives use ISO/IEC 15504 conformant process assessment methods.ConclusionDuring the last decade, in order to satisfy the on-going demand of mature software development companies for assessing and improving ITSM processes, different models which use the measurement framework of ISO/IEC 15504 have been developed. However, it is still necessary to define a method with the necessary guidelines to implement both software development processes and ITSM processes reducing the amount of effort, especially because some processes of both categories are overlapped. 
54|3||Transfer learning for cross-company software defect prediction|ContextSoftware defect prediction studies usually built models using within-company data, but very few focused on the prediction models trained with cross-company data. It is difficult to employ these models which are built on the within-company data in practice, because of the lack of these local data repositories. Recently, transfer learning has attracted more and more attention for building classifier in target domain using the data from related source domain. It is very useful in cases when distributions of training and test instances differ, but is it appropriate for cross-company software defect prediction?ObjectiveIn this paper, we consider the cross-company defect prediction scenario where source and target data are drawn from different companies. In order to harness cross company data, we try to exploit the transfer learning method to build faster and highly effective prediction model.MethodUnlike the prior works selecting training data which are similar from the test data, we proposed a novel algorithm called Transfer Naive Bayes (TNB), by using the information of all the proper features in training data. Our solution estimates the distribution of the test data, and transfers cross-company data information into the weights of the training data. On these weighted data, the defect prediction model is built.ResultsThis article presents a theoretical analysis for the comparative methods, and shows the experiment results on the data sets from different organizations. It indicates that TNB is more accurate in terms of AUC (The area under the receiver operating characteristic curve), within less runtime than the state of the art methods.ConclusionIt is concluded that when there are too few local training data to train good classifiers, the useful knowledge from different-distribution training data on feature level may help. We are optimistic that our transfer learning method can guide optimal resource allocation strategies, which may reduce software testing cost and increase effectiveness of software testing process. 
54|3||API2MoL: Automating the building of bridges between APIs and Model-Driven Engineering|ContextA software artefact typically makes its functionality available through a specialized Application Programming Interface (API) describing the set of services offered to client applications. In fact, building any software system usually involves managing a plethora of APIs, which complicates the development process. In Model-Driven Engineering (MDE), where models are the key elements of any software engineering activity, this API management should take place at the model level. Therefore, tools that facilitate the integration of APIs and MDE are clearly needed.ObjectiveOur goal is to automate the implementation of API–MDE bridges for supporting both the creation of models from API objects and the generation of such API objects from models. In this sense, this paper presents the API2MoL approach, which provides a declarative rule-based language to easily write mapping definitions to link API specifications and the metamodel that represents them. These definitions are then executed to convert API objects into model elements or vice versa. The approach also allows both the metamodel and the mapping to be automatically obtained from the API specification (bootstrap process).MethodAfter implementing the API2MoL engine, its correctness was validated using several APIs. Since APIs are normally large, we then developed a tool to implement the bootstrap process, which was also validated.ResultsWe provide a toolkit (language and bootstrap tool) for the creation of bridges between APIs and MDE. The current implementation focuses on Java APIs, although its adaptation to other statically typed object-oriented languages is straightforward. The correctness, expressiveness and completeness of the approach have been validated with the Swing, SWT and JTwitter APIs.ConclusionAPI2MoL frees developers from having to manually implement the tasks of obtaining models from API objects and generating such objects from models. This helps to manage API models in MDE-based solutions. 
54|3||Defect proneness estimation and feedback approach for software design quality improvement|ContextModern software engineering demands professionals and researchers to proactively and collectively work towards exploring and experimenting viable and valuable mechanisms in order to extract all kinds of degenerative bugs, security holes, and possible deviations at the initial stage. Having understood the real need here, we have introduced a novel methodology for the estimation of defect proneness of class structures in object oriented (OO) software systems at design stage.ObjectiveThe objective of this work is to develop an estimation model that provides significant assessment of defect proneness of object oriented software packages at design phase of SDLC. This frame work enhances the efficiency of SDLC through design quality improvement.MethodThis involves a data driven methodology which is based on the empirical study of the relationship existing between design parameters and defect proneness. In the first phase, a mapping of the relationship between the design metrics and normal occurrence pattern of defects are carried out. This is represented as a set of non linear multifunctional regression equations which reflects the influence of individual design metrics on defect proneness. The defect proneness estimation model is then generated by weighted linear combination of these multifunctional regression equations. The weighted coefficients are evaluated through GQM (Goal Question Metric) paradigm.ResultsThe model evaluation and validation is carried out with a selected set of cases which is found to be promising. The current study is successfully dealt with three projects and it opens up the opportunity to extend this to a wide range of projects across industries.ConclusionThe defect proneness estimation at design stage facilitates an effective feedback to the design architect and enabling him to identify and reduce the number of defects in the modules appropriately. This results in a considerable improvement in software design leading to cost effective products. 
54|3||DELIVER! â An educational game for teaching Earned Value Management in computing courses|ContextTo meet the growing need for education in Software Project Management, educational games have been introduced as a beneficial instructional strategy. However, there are no low-cost board games openly available to teach Earned Value Management (EVM) in computing programs.ObjectiveThis paper presents an educational board game to reinforce and teach the application of EVM concepts in the context of undergraduate computing programs complementing expository lessons on EVM basics.MethodThe game has been developed based on project management fundamentals and teaching experience in this area. So far, it has been applied in two project management courses in undergraduate computing programs at the Federal University of Santa Catarina. We evaluated motivation, user experience and the game’s contribution to learning through case studies on Kirkpatrick’s level one based on the perception of the students.ResultsFirst results of the evaluation of the game indicate a perceived potential of the game to contribute to the learning of EVM concepts and their application. The results also point out a very positive effect of the game on social interaction, engagement, immersion, attention and relevance to the course objectives.ConclusionWe conclude that the game DELIVER! can contribute to the learning of the EVM on the cognitive levels of remembering, understanding and application. The illustration of the application of EVM through the game can motivate its usefulness. The game has proven to be an engaging instructional strategy, keeping students on the task and attentive. In this respect, the game offers a possibility to complement traditional instructional strategies for teaching EVM. In order to further generalize and to strengthen the validity of the results, it is important to obtain further evaluations. 
54|3||SOAdapt: A process reference model for developing adaptable service-based applications|ContextThe loose coupling of services and Service-Based Applications (SBAs) have made them the ideal platform for context-based run-time adaptation. There has been a lot of research into implementation techniques for adapting SBAs, without much effort focused on the software process required to guide the adaptation.ObjectiveThis paper aims to bridge that gap by providing an empirically grounded software process model that can be used by software practitioners who want to build adaptable SBAs. The process model will focus only on the adaptation specific issues.MethodThe process model presented in this paper is based on data collected through interviews with 10 practitioners occupying various roles within eight different companies. The data was analyzed using qualitative data analysis techniques. We used the output to develop a set of activities, tasks, stakeholders and artifacts that were used to construct the process model.ResultsThe outcome of the data analysis process was a process model identifying nine sets of adaptation process attributes. These can be used in conjunction with an organisation’s existing development life-cycle or another reference life-cycle.ConclusionThe process model developed in this paper provides a solid reference for practitioners who are planning to develop adaptable SBAs. It has advantages over similar approaches in that it focuses on software process rather than the specific adaptation mechanism implementation techniques. 
54|3||Damon: A distributed AOP middleware for large-scale scenarios|ContextThe development of distributed applications in large-scale environments has always been a complex task. In order to guarantee non-functional properties like scalability or availability, developers are usually faced with the same problems over and over again. These problems can be separated in distributed concerns, as for example, distribution, load-balancing or replication, just to name a few. Nevertheless, none of the current solutions in adaptive middleware area, like Aspect-Oriented Programming (AOP), is capable of implementing these distributed concerns transparently.ObjectiveIn this article, we present a distributed AOP middleware for large-scale development, called Damon. Its main goal is to implement true distributed aspects, which enables the use of distributed concerns in applications that were not specifically designed for distributed or large-scale scenarios.MethodOur middleware comprises two main layers: a distributed composition model and a scalable deployment platform. The distributed composition model envisages separation of distributed aspects, taking the necessary features from component models, like distribution facilities and connectors, and from computational reflection, like introspection and meta-levels. This recursive and fully distributed model provides its own Architecture Description Language (ADL), and thus allows low dependency and high cohesion among distributed aspects. Additionally, our model is built on top of a deployment platform where distributed aspects are disseminated and activated in individual or grouped hosts. This platform benefits from peer-to-peer and dynamic AOP substrates to implement its services in a decentralized, decoupled, and efficient way.ResultsTherefore, our middleware solution reduces the complexity of distributed application development, managing separated functionalities, and enabling necessary services like transparent reconfiguration and deployment at runtime. Finally, we have implemented a functional prototype and we conducted several experiments using the PlanetLab testbed.ConclusionOur distributed AOP approach fulfills the large-scale scenarios requirements, and represents a solid building block for future distributed transparent infrastructures. 
54|4|http://www.sciencedirect.com/science/journal/09505849/54/4|A methodology to assess the impact of design patterns on software quality|ContextSoftware quality is considered to be one of the most important concerns of software production teams. Additionally, design patterns are documented solutions to common design problems that are expected to enhance software quality. Until now, the results on the effect of design patterns on software quality are controversial.AimsThis study aims to propose a methodology for comparing design patterns to alternative designs with an analytical method. Additionally, the study illustrates the methodology by comparing three design patterns with two alternative solutions, with respect to several quality attributes.MethodThe paper introduces a theoretical/analytical methodology to compare sets of “canonical” solutions to design problems. The study is theoretical in the sense that the solutions are disconnected from real systems, even though they stem from concrete problems. The study is analytical in the sense that the solutions are compared based on their possible numbers of classes and on equations representing the values of the various structural quality attributes in function of these numbers of classes. The exploratory designs have been produced by studying the literature, by investigating open-source projects and by using design patterns. In addition to that, we have created a tool that helps practitioners in choosing the optimal design solution, according to their special needs.ResultsThe results of our research suggest that the decision of applying a design pattern is usually a trade-off, because patterns are not universally good or bad. Patterns typically improve certain aspects of software quality, while they might weaken some other.ConclusionsConcluding the proposed methodology is applicable for comparing patterns and alternative designs, and highlights existing threshold that when surpassed the design pattern is getting more or less beneficial than the alternative design. More specifically, the identification of such thresholds can become very useful for decision making during system design and refactoring. 
54|4||Resolving unwanted couplings through interactive exploration of co-evolving software entities â An experience report|ContextFrequent changes to groups of software entities belonging to different parts of the system may indicate unwanted couplings between those parts. Visualizations of co-changing software entities have been proposed to help developers identify unwanted couplings. Identifying unwanted couplings, however, is only the first step towards an important goal of a software architect: to improve the decomposition of the software system. An in-depth analysis of co-changing entities is needed to understand the underlying reasons for co-changes, and also determine how to resolve the issues.ObjectiveIn this paper we discuss how interactive visualizations can support the process of analyzing the identified unwanted couplings.MethodWe applied a tool that interactively visualizes software evolution in 10 working sessions with architects and developers of a large embedded software system having a development history of more than a decade.ResultsThe participants of the working sessions were overall very positive about their experiences with the interactive visualizations. In 70% of the cases investigated, a decision could be taken on how to resolve the unwanted couplings.ConclusionOur experience suggests that interactive visualization not only helps to identify unwanted couplings but it also helps experts to reason about and resolve them. 
54|4||Hybrid methodology for data warehouse conceptual design by UML schemas|ContextData warehouse conceptual design is based on the metaphor of the cube, which can be derived from either requirement-driven or data-driven methodologies. Each methodology has its own advantages. The first allows designers to obtain a conceptual schema very close to the user needs but it may be not supported by the effective data availability. On the contrary, the second ensures a perfect traceability and consistence with the data sources—in fact, it guarantees the presence of data to be used in analytical processing—but does not preserve from missing business user needs. To face this issue, the necessity emerged in the last years to define hybrid methodologies for conceptual design.ObjectiveThe objective of the paper is to use a hybrid methodology based on different multidimensional models in order to gather all advantages of each of them.MethodThe proposed methodology integrates the requirement-driven strategy with the data-driven one, in that order, possibly performing alterations of functional dependencies on UML multidimensional schemas reconciled with data sources.ResultsAs case study, we illustrate how our methodology can be applied to the university environment. Furthermore, we evaluate quantitatively the benefits of this methodology by comparing it with some popular and conventional methodologies.ConclusionIn conclusion, we highlight how the hybrid methodology improves the conceptual schema quality. Finally, we outline our present work devoted to introduce automatic design techniques in the methodology on the basis of the logical programming. 
54|4||Business process model repositories â Framework and survey|ContextLarge organizations often run hundreds or even thousands of different business processes. Managing such large collections of business process models is a challenging task. Software can assist in performing that task, by supporting common management functions such as storage, search and version management of models. It can also provide advanced functions that are specific for managing collections of process models, such as managing the consistency of public and private processes. Software that supports the management of large collections of business process models is called: business process model repository software.ObjectiveThis paper contributes to the development of business process model repositories, by analyzing the state of the art.MethodTo perform the analysis a literature survey and a comparison of existing (business process model) repository technology is performed.ResultThe results of the state of the art analysis are twofold. First, a framework for business process model repositories is presented, which consists of a management model and a reference architecture. The management model lists the functionality that can be provided and the reference architecture presents the components that provide that functionality. Second, an analysis is presented of the extent to which existing business process model repositories implement the functionality from the framework.ConclusionThe results presented in the paper are valuable as a comprehensive overview of business process model repository functionality. In addition they form a basis for a future research agenda. We conclude that existing repositories focus on traditional functionality rather than exploiting the full potential of information management tools, thus we show that there is a strong basis for further research. 
54|4||Fault prediction and the discriminative powers of connectivity-based object-oriented class cohesion metrics|ContextSeveral metrics have been proposed to measure the extent to which class members are related. Connectivity-based class cohesion metrics measure the degree of connectivity among the class members.ObjectiveWe propose a new class cohesion metric that has higher discriminative power than any of the existing cohesion metrics. In addition, we empirically compare the connectivity and non-connectivity-based cohesion metrics.MethodThe proposed class cohesion metric is based on counting the number of possible paths in a graph that represents the connectivity pattern of the class members. We theoretically and empirically validate this path connectivity class cohesion (PCCC) metric. The empirical validation compares seven connectivity-based metrics, including PCCC, and 11 non-connectivity-based metrics in terms of discriminative and fault detection powers. The discriminative-power study explores the probability that a cohesion metric will incorrectly determine classes to be cohesively equal when they have different connectivity patterns. The fault detection study investigates whether connectivity-based metrics, including PCCC, better explain the presence of faults from a statistical standpoint in comparison to other non-connectivity-based cohesion metrics, considered individually or in combination.ResultsThe theoretical validation demonstrates that PCCC satisfies the key cohesion properties. The results of the empirical studies indicate that, in contrast to other connectivity-based cohesion metrics, PCCC is much better than any comparable cohesion metric in terms of its discriminative power. In addition, the results also indicate that PCCC measures cohesion aspects that are not captured by other metrics, wherein it is considerably better than other connectivity-based metrics but slightly worse than some other non-connectivity-based cohesion metrics in terms of its ability to predict faulty classes.ConclusionPCCC is more useful in practice for the applications in which practitioners need to distinguish between the quality of different classes or the quality of different implementations of the same class. 
54|4||A framework for analysis and design of software reference architectures|ContextA software reference architecture is a generic architecture for a class of systems that is used as a foundation for the design of concrete architectures from this class. The generic nature of reference architectures leads to a less defined architecture design and application contexts, which makes the architecture goal definition and architecture design non-trivial steps, rooted in uncertainty.ObjectiveThe paper presents a structured and comprehensive study on the congruence between context, goals, and design of software reference architectures. It proposes a tool for the design of congruent reference architectures and for the analysis of the level of congruence of existing reference architectures.MethodWe define a framework for congruent reference architectures. The framework is based on state of the art results from literature and practice. We validate our framework and its quality as analytical tool by applying it for the analysis of 24 reference architectures. The conclusions from our analysis are compared to the opinions of experts on these reference architectures documented in literature and dedicated communication.ResultsOur framework consists of a multi-dimensional classification space and of five types of reference architectures that are formed by combining specific values from the multi-dimensional classification space. Reference architectures that can be classified in one of these types have better chances to become a success. The validation of our framework confirms its quality as a tool for the analysis of the congruence of software reference architectures.ConclusionThis paper facilitates software architects and scientists in the inception, design, and application of congruent software reference architectures. The application of the tool improves the chance for success of a reference architecture. 
54|5|http://www.sciencedirect.com/science/journal/09505849/54/5|The situational factors that affect the software development process: Towards a comprehensive reference framework|ContextAn optimal software development process is regarded as being dependent on the situational characteristics of individual software development settings. Such characteristics include the nature of the application(s) under development, team size, requirements volatility and personnel experience. However, no comprehensive reference framework of the situational factors affecting the software development process is presently available.ObjectiveThe absence of such a comprehensive reference framework of the situational factors affecting the software development process is problematic not just because it inhibits our ability to optimise the software development process, but perhaps more importantly, because it potentially undermines our capacity to ascertain the key constraints and characteristics of a software development setting.MethodTo address this deficiency, we have consolidated a substantial body of related research into an initial reference framework of the situational factors affecting the software development process. To support the data consolidation, we have applied rigorous data coding techniques from Grounded Theory and we believe that the resulting framework represents an important contribution to the software engineering field of knowledge.ResultsThe resulting reference framework of situational factors consists of eight classifications and 44 factors that inform the software process. We believe that the situational factor reference framework presented herein represents a sound initial reference framework for the key situational elements affecting the software process definition.ConclusionIn addition to providing a useful reference listing for the research community and for committees engaged in the development of standards, the reference framework also provides support for practitioners who are challenged with defining and maintaining software development processes. Furthermore, this framework can be used to develop a profile of the situational characteristics of a software development setting, which in turn provides a sound foundation for software development process definition and optimisation. 
54|5||Assisting conformance checks between architectural scenarios and implementation|
54|5||Automated removal of cross site scripting vulnerabilities in web applications|ContextCross site scripting (XSS) vulnerability is among the top web application vulnerabilities according to recent surveys. This vulnerability occurs when a web application uses inputs received from users in web pages without properly checking them. This allows an attacker to inject malicious scripts in web pages via such inputs such that the scripts perform malicious actions when a client visits the exploited web pages. Such an attack may cause serious security violations such as account hijacking and cookie theft. Current approaches to mitigate this problem mainly focus on effective detection of XSS vulnerabilities in the programs or prevention of real time XSS attacks. As more sophisticated attack vectors are being discovered, vulnerabilities if not removed could be exploited anytime.ObjectiveTo address this issue, this paper presents an approach for removing XSS vulnerabilities in web applications.MethodBased on static analysis and pattern matching techniques, our approach identifies potential XSS vulnerabilities in program source code and secures them with appropriate escaping mechanisms which prevent input values from causing any script execution.ResultsWe developed a tool, saferXSS, to implement the proposed approach. Using the tool, we evaluated the applicability and effectiveness of the proposed approach based on the experiments on five Java-based web applications.ConclusionOur evaluation has shown that the tool can be applied to real-world web applications and it automatically removed all the real XSS vulnerabilities in the test subjects. 
54|5||Software process improvement success factors for small and medium Web companies: A qualitative study|ContextThe context of this research is software process improvement (SPI) in small and medium Web companies.ObjectiveThe primary objective of this paper is to identify software process improvement (SPI) success factors for small and medium Web companies.MethodTo achieve this goal, we conducted semi-structured, open-ended interviews with 21 participants representing 11 different companies in Pakistan, and analyzed the data qualitatively using the Glaserian strand of grounded theory research procedures. The key steps of these procedures that were employed in this research included open coding, focused coding, theoretical coding, theoretical sampling, constant comparison, and scaling up.ResultsAn initial framework of key SPI success factors for small and medium Web companies was proposed, which can be of use for small and medium Web companies engaged in SPI. The paper also differentiates between small and medium Web companies and analyzes crucial SPI requirements for companies operating in the Web development domain.ConclusionThe results of this work, in particular the use of qualitative techniques – allowed us to obtain rich insight into SPI success factors for small and medium Web companies. Future work comprises the validation of the SPI success factors with small and medium Web companies. 
54|5||MC Sandbox: Devising a tool for method-user-centered method configuration|ContextMethod engineering approaches are often based on the assumption that method users are able to explicitly express their situational method requirements. Similar to systems requirements, method requirements are often vague and hard to explicate. In this paper we address the issue of involving method users early in method configuration. This is done through borrowing ideas from user-centered design and prototyping, and implementing them on the method engineering layer.ObjectiveWe design a computerized tool, MC Sandbox, to capture method requirements through the use of method-user-centered method configuration, hence bridging the gap between systems developers’ and method engineers’ understanding of and expectations on a situational method.MethodThe research method adopted can be characterized as multi-grounded action research. Our implementation of multi-grounded action research follows the traditional ‘canonical’ action research method, which has cycles of diagnosing, action planning, action taking, evaluating, and specifying learning. The research project comprised three such action research cycles where 10 action cases were performed.ResultsMC Sandbox has proven useful in eliciting and negotiating method requirements in a continuously ongoing dialog between the method users and the method engineers during configuration workshops. The results also show that the method engineer role rotated among the systems developers and that they were indeed committed to the negotiated methods during the systems development projects.ConclusionIt is possible for method users to actively participate and construct suitable situational methods if they are provided with appropriate high-level modelling concepts, such as method components, configuration packages and configuration templates. This way, the project members’ understanding of the current development practice develops incrementally, both in terms of understanding the needs and available method support. In addition, both method requirements and commitments are made explicit, which are important aspects when working with method configuration from a collaboration point of view. 
54|6|http://www.sciencedirect.com/science/journal/09505849/54/6|Comparing alternatives for analyzing requirements trade-offs â In the absence of numerical data|ContextChoosing a design solution most often involves dealing with trade-offs and conflicts among requirements and design objectives. Making such trade-offs during early stages of requirements and design is challenging because costs and benefits of alternatives are often hard to quantify.ObjectiveThe objective of this work is to develop a decision analysis method that assists in making trade-offs in the absence of quantitative data.MethodIn this method, stakeholders qualitatively compare consequences of alternatives on decision criteria. We propose an algorithm that generates all possible consequences of alternatives on requirements, according to the rough qualitative comparisons that stakeholders made. The possible consequences generated by the algorithm are then analyzed by the Even Swaps Multi-Criteria Decision Analysis method to determine the best solution. The Even Swaps method is a technique developed in management science to assist in multi-criteria decision making when explicit value trade-offs are not available.Results and conclusionsOur algorithm teases out the need to accurately measure or estimate costs and benefits of alternative design solutions. The algorithm automates the Even Swap process, and reuses stakeholders’ value trade-offs throughout the Even Swaps process. We applied the prototype tool in several case studies to evaluate the utility of the method. The results of case studies provide evidence that our decision aid method selects the optimum solution correctly compared to results of other similar quantitative methods, while our method does not rely on detailed numerical assessment of alternatives and importance weights of criteria. 
54|6||Compliance in service-oriented architectures: A model-driven and view-based approach|ContextEnsuring software systems conforming to multiple sources of relevant policies, laws, and regulations is significant because the consequences of infringement can be serious. Unfortunately, this goal is hardly achievable due to the divergence and frequent changes of compliance sources and the differences in perception and expertise of the involved stakeholders. In the long run, these issues lead to problems regarding complexity, understandability, maintainability, and reusability of compliance concerns.ObjectiveIn this article, we present a model-driven and view-based approach for addressing problems related to compliance concerns.MethodCompliance concerns are represented using separate view models. This is achieved using domain-specific languages (DSLs) that enable non-technical and technical experts to formulate only the excerpts of the system according to their expertise and domain knowledge. The compliance implementations, reports, and documentation can be automatically generated from the models. The applicability of our approach has been validated using an industrial case study.ResultsOur approach supports stakeholders in dealing with the divergence of multiple compliance sources. The compliance controls and relevant reports and documentation are generated from the models and hence become traceable, understandable, and reusable. Because the generated artifacts are associated with the models, the compliance information won’t be lost as the system evolves. DSLs and view models convey compliance concerns to each stakeholder in a view that is most appropriate for his/her current work task.ConclusionsOur approach lays a solid foundation for ensuring conformance to relevant laws and regulations. This approach, on the one hand, aims at addressing the variety of expertise and domain knowledge of stakeholders. On the other hand, it also aims at ensuring the explicit links between compliance sources and the corresponding implementations, reports, and documents for conducting many important tasks such as root cause analysis, auditing, and governance. 
54|6||Design and implementation of a harmony-search-based variable-strength t-way testing strategy with constraints support|ContextAlthough useful, AI-based variable strength t-way strategies are lacking in terms of the support for high interaction strength. Additionally, most AI-based strategies generally do not address the support for constraints. Addressing the aforementioned issues, this paper elaborates the design, implementation, and evaluation of a novel variable-strength-based on harmony search algorithm, called Harmony Search Strategy (HSS).ObjectiveThe objective of this work is to investigate the adoption of harmony search algorithm for constructing variable-strength t-way strategy.MethodImplemented in Java, HSS integrates the harmony search algorithm as parts of its search engine.ResultBenchmarking results demonstrate that HSS gives competitive results against most existing AI-based (and pure computational) counterparts. However, unlike other AI-based counterparts, HSS addresses the support for high interaction strength and permits the support for constraints.ConclusionAI-based t-way strategies tend to outperform the pure computational-based strategies in terms of test size. 
54|6||A SysML-based approach to traceability management and design slicing in support of safety certification: Framework, tool support, and case studies|ContextTraceability is one of the basic tenets of all safety standards and a key prerequisite for software safety certification. In the current state of practice, there is often a significant traceability gap between safety requirements and software design. Poor traceability, in addition to being a non-compliance issue on its own, makes it difficult to determine whether the design fulfills the safety requirements, mainly because the design aspects related to safety cannot be clearly identified.ObjectiveThe goal of this article is to develop a framework for specifying and automatically extracting design aspects relevant to safety requirements. This goal is realized through the combination of two components: (1) A methodology for establishing traceability between safety requirements and design, and (2) an algorithm that can extract for any given safety requirement a minimized fragment (slice) of the design that is sound, and yet easy to understand and inspect.MethodWe ground our framework on System Modeling Language (SysML). The framework includes a traceability information model, a methodology to establish traceability, and mechanisms for model slicing based on the recorded traceability information. The framework is implemented in a tool, named SafeSlice.ResultsWe prove that our slicing algorithm is sound for temporal safety properties, and argue about the completeness of slices based on our practical experience. We report on the lessons learned from applying our approach to two case studies, one benchmark and one industrial case. Both studies indicate that our approach substantially reduces the amount of information that needs to be inspected for ensuring that a given (behavioral) safety requirement is met by the design. 
54|6||Special section on engineering complex software systems through multi-agent systems and simulation|
54|6||On the combination of top-down and bottom-up methodologies for the design of coordination mechanisms in self-organising systems|In resource-flow systems, e.g. production lines, agents are processing resources by applying capabilities to them in a given order. Such systems benefit from self-organization as they become easier to manage and more robust against failures. In this paper, we demonstrate the conception of a decentralized coordination process for resource-flow systems and its integration into an agent-based software system. This process restores a system’s functionality after a failure by propagating information about the error through the system until a fitting agent is found that is able to perform the required function. The mechanism has been designed by combining a top-down design approach for self-organizing resource-flow system and a systemic development framework for the development of decentralized, distributed coordination processes. Using the latter framework, a process is designed and integrated in a system realization that follows the former conceptual model. Evaluations of convergence as well as performance of the mechanism and the required amount of redundancy of the system are performed by simulations. 
54|6||ELDAMeth: An agent-oriented methodology for simulation-based prototyping of distributed agent systems|In application domains, such as distributed information retrieval, content management and distribution, e-Commerce, the agent-based computing paradigm has been demonstrated to be effective for the analysis, design and implementation of distributed software systems. In particular, several agent-oriented methodologies, incorporating suitable agent models, frameworks and tools, have been to date defined to support the development lifecycle of distributed agent systems (DAS). However, few of them provide effective validation methods to analyze design objects at different degrees of refinement before their actual implementation and deployment. In this paper, ELDAMeth, a simulation-based methodology for DAS, which enables rapid prototyping based on visual programming, validation, and automatic code generation for JADE-based DAS, is presented. ELDAMeth can be used both stand-alone for the modeling and evaluation of DAS and coupled with other agent-oriented methodologies for enhancing them with simulation-based validation. In particular, the proposed methodology, which is based on the ELDA (Event-driven Lightweight Distilled StateCharts-based Agents) agent model, provides key programming abstractions (event-driven computation, multi-coordination, and coarse-grained strong mobility) very suitable for highly dynamic distributed computing and is supported by a CASE tool-driven iterative process seamlessly covering the detailed design, simulation, and implementation phases of DAS. A simple yet effective case study in the distributed information retrieval domain is used to illustrate the proposed methodology. 
54|6||On the engineering of agent-based simulations of social activities with social networks|ContextModels of how people move around cities play a role in making decisions about urban and land-use planning. Previous models have been based on space and time, and have neglected the social aspect of travel. Recent work on agent-based modelling shows promise as a new approach, especially for models with both social and spatial elements.ObjectiveThis paper demonstrates the design and implementation of an agent-based model of social activity generation and scheduling for experimental purposes to explore the effects of social space in addition to physical space. As a side-effect, the paper discusses the need for and requirements on structured design of agent-based models and simulations.MethodModel design was based on the MASQ meta-model and implemented in Python. The model was then tested against several hypotheses with several initial networks.ResultsThe model allowed us to investigate the effects of social networks. We found that the model was most sensitive to the pair attributes of the network, rather than the global or personal attributes.ConclusionAs demonstrated, a structured approach to model development is important in order to be able to understand and apply the results, and for the model to be extensible in the future. Agent-based modelling approaches allow for inclusion of social elements. For models incorporating social networks, testing the sensitivity to the initial network is important to ensure the model performs as expected. 
54|6||Generating inspiration for agent design by reinforcement learning|One major challenge in developing multiagent systems is to find the appropriate agent design that is able to generate the intended overall dynamics, but does not contain unnecessary features. In this article we suggest to use agent learning for supporting the development of an agent model during an analysis phase in agent-based software engineering.Hereby, the designer defines the environmental model and the agent interfaces. A reward function captures a description of the overall agent performance with respect to the intended outcome of the agent behavior. Based on this setup, reinforcement learning techniques can be used for learning rules that are optimally governing the agent behavior. However, for really being useful for analysis, the human developer must be able to review and fully understand the learnt behavior program. We propose to use additional learning mechanisms for a post-processing step supporting the usage of the learnt model. 
54|7|http://www.sciencedirect.com/science/journal/09505849/54/7|Software quality trade-offs: A systematic map|BackgroundSoftware quality is complex with over investment, under investment and the interplay between aspects often being overlooked as many researchers aim to advance individual aspects of software quality.AimThis paper aims to provide a consolidated overview the literature that addresses trade-offs between aspects of software product quality.MethodA systematic literature map is employed to provide an overview of software quality trade-off literature in general. Specific analysis is also done of empirical literature addressing the topic.ResultsThe results show a wide range of solution proposals being considered. However, there is insufficient empirical evidence to adequately evaluate and compare these proposals. Further a very large vocabulary has been found to describe software quality.ConclusionGreater empirical research is required to sufficiently evaluate and compare the wide range of solution proposals. This will allow researchers to focus on the proposals showing greater signs of success and better support industrial practitioners. 
54|7||Tools used in Global Software Engineering: A systematic mapping review|ContextThis systematic mapping review is set in a Global Software Engineering (GSE) context, characterized by a highly distributed environment in which project team members work separately in different countries. This geographic separation creates specific challenges associated with global communication, coordination and control.ObjectiveThe main goal of this study is to discover all the available communication and coordination tools that can support highly distributed teams, how these tools have been applied in GSE, and then to describe and classify the tools to allow both practitioners and researchers involved in GSE to make use of the available tool support in GSE.MethodWe performed a systematic mapping review through a search for studies that answered our research question, “Which software tools (commercial, free or research based) are available to support Global Software Engineering?” Applying a range of related search terms to key electronic databases, selected journals, and conferences and workshops enabled us to extract relevant papers. We then used a data extraction template to classify, extract and record important information about the GSD tools from each paper. This information was synthesized and presented as a general map of types of GSD tools, the tool’s main features and how each tool was validated in practice.ResultsThe main result is a list of 132 tools, which, according to the literature, have been, or are intended to be, used in global software projects. The classification of these tools includes lists of features for communication, coordination and control as well as how the tool has been validated in practice. We found that out the total of 132, the majority of tools were developed at research centers, and only a small percentage of tools (18.9%) are reported as having been tested outside the initial context in which they were developed.ConclusionThe most common features in the GSE tools included in this study are: team activity and social awareness, support for informal communication, Support for Distributed Knowledge Management and Interoperability with other tools. Finally, there is the need for an evaluation of these tools to verify their external validity, or usefulness in a wider global environment. 
54|7||Problems and their mitigation in system and software architecting|ContextToday, software and embedded systems act as enablers for developing new functionality in traditional industries such as the automotive, process automation, and manufacturing automation domains. This differs from 25–30 years ago when these systems where based on electronics and electro-mechanical solutions. The architecture of the embedded system and of the software is important to ensure the qualities of these applications. However, the effort of designing and evolving the architecture is in practice often neglected during system development, whilst development efforts are centered on implementing new functionality.ObjectiveWe present problems and success factors that are central to the architectural development of software intensive systems in the domain of automotive and automation products as judged by practitioners.MethodThe method consisted of three steps. First, we used semi-structured interviews to collect data in an exploratory manner. As a second step, a survey based on problems extracted from the interview data was used to investigate the occurrence of these problems at a wider range of organizations. In order to identify and suggest how to mitigate the problems that were considered important, we finally performed root cause analysis workshops, and from these a number of success factors were elicited.ResultsA total of 21 problems have been identified based on the interview data, and these are related to the technical, organizational, project, and agreement processes. Based on the survey results, the following four problems were selected for a root cause analysis: (1) there is a lack of process for architecture development, (2) there is a lack of method or model to evaluate the business value when choosing the architecture, (3) there is a lack of clear long-term architectural strategy, and (4) processes and methods are less valued than knowledge and competence of individuals.ConclusionIn conclusion, the following identified success factors are crucial components to be successful in developing software intensive systems: (1) define an architectural strategy, (2) implement a process for architectural work, (3) ensure authority for architects, (4) clarify the business impact of the architecture, and (5) optimize on the project portfolio level instead of optimizing each project. 
54|7||Interactive specification and verification of behavioral adaptation contracts|ContextAdaptation is a crucial issue when building new applications by reusing existing software services which were not initially designed to interoperate with each other. Adaptation contracts describe composition constraints and adaptation requirements among these services. The writing of this specification by a designer is a difficult and error-prone task, especially when interaction protocols are considered in service interfaces.ObjectiveIn this article, we propose a tool-based, interactive approach to support the contract design process.MethodOur approach includes: (i) a graphical notation to define port bindings, and an interface compatibility measure to compare protocols and suggest some port connections to the designer, (ii) compositional and hierarchical techniques to facilitate the specification of adaptation contracts by building them incrementally, (iii) validation and verification techniques to check that the contract will make the involved services work correctly and as expected by the designer.ResultsOur results show a reduction both in the amount of effort that the designer has to put into building the contract, as well as in the number of errors present in the final result (noticeably higher in the case of manual specification).ConclusionWe conclude that it is important to provide integrated tool support for the specification and verification of adaptation contracts, since their incorrect specification induces erroneous executions of the system. To the best of our knowledge, such tool support has not been provided by any other approach so far, and hence we consider the techniques described in this paper as an important contribution to the area of behavioral software adaptation. 
54|7||Improving the effectiveness of test suite reduction for user-session-based testing of web applications|ContextTest suite reduction is the problem of creating and executing a set of test cases that are smaller in size but equivalent in effectiveness to an original test suite. However, reduced suites can still be large and executing all the tests in a reduced test suite can be time consuming.ObjectiveWe propose ordering the tests in a reduced suite to increase its rate of fault detection. The ordered reduced test suite can be executed in time constrained situations, where, even if test execution is stopped early, the best test cases from the reduced suite will already be executed.MethodIn this paper, we present several approaches to order reduced test suites using experimentally verified prioritization criteria for the domain of web applications. We conduct an empirical study with three subject applications and user-session-based test cases to demonstrate how ordered reduced test suites often make a practical contribution. To enable comparison between test suites of different sizes, we develop Mod_APFD_C, a modification of the traditional prioritization effectiveness measure.ResultsWe find that by ordering the reduced suites, we create test suites that are more effective than unordered reduced suites. In each of our subject applications, there is at least one ordered reduced suite that outperforms the best unordered reduced suite and the best prioritized original suite.ConclusionsOur results show that when a tester does not have enough time to execute the entire reduced suite, executing an ordered reduced suite often improves the rate of fault detection. By coupling the underlying system’s characteristics with observations from our study on the criteria that produce the best ordered reduced suites, a tester can order their reduced test suites to obtain increased testing effectiveness. 
54|7||How well does test case prioritization integrate with statistical fault localization?|ContextEffective test case prioritization shortens the time to detect failures, and yet the use of fewer test cases may compromise the effectiveness of subsequent fault localization.ObjectiveThe paper aims at finding whether several previously identified effectiveness factors of test case prioritization techniques, namely strategy, coverage granularity, and time cost, have observable consequences on the effectiveness of statistical fault localization techniques.MethodThis paper uses a controlled experiment to examine these factors. The experiment includes 16 test case prioritization techniques and four statistical fault localization techniques using the Siemens suite of programs as well as grep, gzip, sed, and flex as subjects. The experiment studies the effects of the percentage of code examined to locate faults from these benchmark subjects after a given number of failures have been observed.ResultsWe find that if testers have a budgetary concern on the number of test cases for regression testing, the use of test case prioritization can save up to 40% of test case executions for commit builds without significantly affecting the effectiveness of fault localization. A statistical fault localization technique using a smaller fraction of a prioritized test suite is found to compromise its effectiveness seriously. Despite the presence of some variations, the inclusion of more failed test cases will generally improve the fault localization effectiveness during the integration process. Interestingly, during the variation periods, adding more failed test cases actually deteriorates the fault localization effectiveness. In terms of strategies, Random is found to be the most effective, followed by the ART and Additional strategies, while the Total strategy is the least effective. We do not observe sufficient empirical evidence to conclude that using different coverage granularity levels have different overall effects.ConclusionThe paper empirically identifies that strategy and time–cost of test case prioritization techniques are key factors affecting the effectiveness of statistical fault localization, while coverage granularity is not a significant factor. It also identifies a mid-range deterioration in fault localization effectiveness when adding more test cases to facilitate debugging. 
54|7||Towards a framework to characterize ubiquitous software projects|ContextUbiquitous Computing (or UbiComp) represents a paradigm in which information processing is thoroughly integrated into everyday objects and activities. From a Software Engineering point of view this development scenario brings new challenges in tailoring or building software processes, impacting current software technologies. However, it has not yet been explicitly shown how to characterize a software project with the perception of ubiquitous computing.ObjectiveThis paper presents a conceptual framework to support the characterization of ubiquitous software projects according to their ubiquity adherence level. It also intends to apply such characterization approach to some projects, aiming at observing their adherence with ubiquitous computing principles.MethodTo follow a research strategy based on systematic reviews and surveys to acquire UbiComp knowledge and organize a conceptual framework regarding ubiquitous computing, which can be used to characterize UbiComp software projects. Besides, to demonstrate its application by characterizing some software projects.ResultsUbiquitous computing encapsulates at least 11 different high abstraction level characteristics represented by 123 functional and 45 restrictive factors. Based on this a checklist was organized to allow the characterization of ubiquitous software projects, which has been applied on 26 ubiquitous software projects from four different application domains (ambient intelligence, pervasive healthcare, U-learning, and urban space). No project demonstrated to support more than 65% of the characteristics set. Service omnipresence was observed in all of these projects. However, some characteristics, although identified as necessary in the checklist, were not identified in any of them.ConclusionThere are characteristics that identify a software project as ubiquitous. However, a ubiquitous software project does not necessarily have to implement all of them. The application domain can influence the appearing of UbiComp characteristics in software projects, promoting an increase of their adherence to UbiComp and, thus, for additional software technologies to deal with these ubiquitous requirements. 
54|7||Utilizing architectural styles to enhance the adaptation support of middleware platforms|ContextModern middleware platforms provide the applications deployed on top of them with facilities for their adaptation. However, the level of adaptation support provided by the state-of-the-art middleware solutions is often limited to dynamically loading and off-loading of software components. Therefore, it is left to the application developers to handle the details of change such that the system’s consistency is not jeopardized.ObjectiveWe aim to change the status quo by providing the middleware facilities necessary to ensure the consistency of software after adaptation. We would like these facilities to be reusable across different applications, such that the middleware can streamline the process of achieving safe adaptation.MethodOur approach addresses the current shortcomings by utilizing the information encoded in a software system’s architectural style. This information drives the development of reusable adaptation patterns. The patterns specify both the exact sequence of changes and the time at which those changes need to occur. We use the patterns to provide advanced adaptation support on top of an existing architectural middleware platform.ResultsOur experience shows the feasibility of deriving detailed adaptation patterns for several architectural styles. Applying the middleware to adapt two real-world software systems shows the approach is effective in consistently adapting these systems without jeopardizing their consistency.ConclusionWe conclude the approach is effective in alleviating the application developers from the responsibility of managing the adaptation process at the application-level. Moreover, we believe this study provides the foundation for changing the way adaptation support is realized in middleware solutions. 
54|7||Corrigendum to: âA systematic mapping study of software product lines testingâ [Inf. Softw. Technol. 53 (5) (2011) 407â423]|
54|8|http://www.sciencedirect.com/science/journal/09505849/54/8|Editorial: Voice of the editorial board|
54|8||Three empirical studies on the agreement of reviewers about the quality of software engineering experiments|ContextDuring systematic literature reviews it is necessary to assess the quality of empirical papers. Current guidelines suggest that two researchers should independently apply a quality checklist and any disagreements must be resolved. However, there is little empirical evidence concerning the effectiveness of these guidelines.AimsThis paper investigates the three techniques that can be used to improve the reliability (i.e. the consensus among reviewers) of quality assessments, specifically, the number of reviewers, the use of a set of evaluation criteria and consultation among reviewers. We undertook a series of studies to investigate these factors.MethodTwo studies involved four research papers and eight reviewers using a quality checklist with nine questions. The first study was based on individual assessments, the second study on joint assessments with a period of inter-rater discussion. A third more formal randomised block experiment involved 48 reviewers assessing two of the papers used previously in teams of one, two and three persons to assess the impact of discussion among teams of different size using the evaluations of the “teams” of one person as a control.ResultsFor the first two studies, the inter-rater reliability was poor for individual assessments, but better for joint evaluations. However, the results of the third study contradicted the results of Study 2. Inter-rater reliability was poor for all groups but worse for teams of two or three than for individuals.ConclusionsWhen performing quality assessments for systematic literature reviews, we recommend using three independent reviewers and adopting the median assessment. A quality checklist seems useful but it is difficult to ensure that the checklist is both appropriate and understood by reviewers. Furthermore, future experiments should ensure participants are given more time to understand the quality checklist and to evaluate the research papers. 
54|8||Evaluating prediction systems in software project estimation|ContextSoftware engineering has a problem in that when we empirically evaluate competing prediction systems we obtain conflicting results.ObjectiveTo reduce the inconsistency amongst validation study results and provide a more formal foundation to interpret results with a particular focus on continuous prediction systems.MethodA new framework is proposed for evaluating competing prediction systems based upon (1) an unbiased statistic, Standardised Accuracy, (2) testing the result likelihood relative to the baseline technique of random ‘predictions’, that is guessing, and (3) calculation of effect sizes.ResultsPreviously published empirical evaluations of prediction systems are re-examined and the original conclusions shown to be unsafe. Additionally, even the strongest results are shown to have no more than a medium effect size relative to random guessing.ConclusionsBiased accuracy statistics such as MMRE are deprecated. By contrast this new empirical validation framework leads to meaningful results. Such steps will assist in performing future meta-analyses and in providing more robust and usable recommendations to practitioners. 
54|8||A systematic review and an expert survey on capabilities supporting multi product lines|ContextComplex software-intensive systems comprise many subsystems that are often based on heterogeneous technological platforms and managed by different organizational units. Multi product lines (MPLs) are an emerging area of research addressing variability management for such large-scale or ultra-large-scale systems. Despite the increasing number of publications addressing MPLs the research area is still quite fragmented.ObjectiveThe aims of this paper are thus to identify, describe, and classify existing approaches supporting MPLs and to increase the understanding of the underlying research issues. Furthermore, the paper aims at defining success-critical capabilities of infrastructures supporting MPLs.MethodUsing a systematic literature review we identify and analyze existing approaches and research issues regarding MPLs. Approaches described in the literature support capabilities needed to define and operate MPLs. We derive capabilities supporting MPLs from the results of the systematic literature review. We validate and refine these capabilities based on a survey among experts from academia and industry.ResultsThe paper discusses key research issues in MPLs and presents basic and advanced capabilities supporting MPLs. We also show examples from research approaches that demonstrate how these capabilities can be realized.ConclusionsWe conclude that approaches supporting MPLs need to consider both technical aspects like structuring large models and defining dependencies between product lines as well as organizational aspects such as distributed modeling and product derivation by multiple stakeholders. The identified capabilities can help to build, enhance, and evaluate MPL approaches. 
54|8||Challenges of shared decision-making: A multiple case study of agile software development|ContextAgile software development changes the nature of collaboration, coordination, and communication in software projects.ObjectiveOur objective was to understand the challenges of shared decision-making in agile software development teams.MethodWe designed a multiple case study consisting of four projects in two software product companies that recently adopted Scrum. We collected data in semi-structured interviews, through participant observations, and from process artifacts.ResultsWe identified three main challenges to shared decision-making in agile software development: alignment of strategic product plans with iteration plans, allocation of development resources, and performing development and maintenance tasks in teams.ConclusionAgile software development requires alignment of decisions on the strategic, tactical, and operational levels in order to overcome these challenges. Agile development also requires a transition from specialized skills to redundancy of functions and from rational to naturalistic decision-making. This takes time; the case companies needed from one to two years to change from traditional, hierarchical decision-making to shared decision-making in software development projects. 
54|8||Exploratory case study research: Outsourced project failure|ContextIT plays an increasingly strategic role in the business performance of organizations, however, the development of strategic IT systems involves a high degree of risk and outsourcing the development of such systems increases the risk.ObjectiveUsing a case study approach we build on research that identifies risk factors leading to the failure of outsourced strategic IT development projects. We investigate the BSkyB project, a strategic development project, which was the subject of recent litigation in the British High Court. We wish to discover what factors led to the failure of such a high profile project; in particular we wish to identify which factors were under the control of the client. We also review the usefulness of the case study methodology when it is not possible to interview any of the people involved with a project.MethodDetailed-step-by-step guidelines designed for multiple industrial case studies are used to investigate the failure factors of the BSkyB project. We use transcripts of court proceedings and media reports to determine the failure factors. We compare the factors identified with those in a conceptual risk framework developed in prior research thus providing an initial validation of that framework.ResultsThe following factors were identified as problems in the BSkyB project: contract, requirements, project complexity, planning and control, execution, and team. A time and materials contract was a risk not originally included in the risk framework that we used.ConclusionThe BSkyB project failed because of problems that can be traced to both client and vendor. According to the judge’s summing up the major fault was with the vendor, although some problems did emanate from the client side. We found that many sections in the case study methodology we used were unnecessary for a single case study based on court proceedings and media reports. The risk framework helped with risk identification. 
54|8||Critical role of measures in decision processes: Managerial and technical measures in the context of large software development organizations|ContextToday, many software development organizations struggle to establish measurement programs to monitor their projects, products and units. After overcoming the initial threshold of establishing the measurement program organizations stand before the questions of which measures should be collected in order to lead to actions or at least effectively trigger decision processes.ObjectiveThe objective of this paper is to investigate how to use measures in an effective way in decision processes. This dependency is examined through a case study – Ericsson in Sweden. Two models of these dependencies are recognized a priori – metrics-push and metric-pull – and in the study the models are used to describe how measures affect decisions and vice versa.MethodThe research method is a case study of the measurement program of one of the product development units of Ericsson in Sweden. The participants are carefully selected from the management teams at different levels of organizations. The objects are measures and decisions at these management levels. The instruments are interviews and observations. The results obtained at Ericsson are validated through interviews at another company – RUAG Space.ResultsThe results show that effective use of measures as evidence for decision processes does not require a large number of measures (ca. 20 at the top management level). It was found that there are four types of measures which are used in different ways in the context of decision formulation and implementation (which we call decision-measures dependency model). The critical aspects of effective measures in decision-making context are completeness, reliability and providing early warnings. It was also found that the time between the decision and when its results can be observed via measures (length of the feedback loop) is a crucial aspect determining at which organizational level a measure should be placed.ConclusionsAfter overcoming the initial threshold of establishing measurement programs the organizations demand non-functional properties from the measures. These non-functional properties like completeness, providing early-warning or trust determine whether decision processes are triggered by measures or not. 
54|8||On generating mutants for AspectJ programs|ContextMutation analysis has been widely used in research studies to evaluate the effectiveness of test suites and testing techniques. Faulty versions (i.e., mutants) of a program are generated such that each mutant contains one seeded fault. The mutation score provides a measure of effectiveness.ObjectiveWe study three problems with the use of mutation analysis for testing AspectJ programs:•The manual identification and removal of equivalent mutants is difficult and time consuming. We calculate the percentage of equivalent mutants generated for benchmark AspectJ programs using available mutation tools.•The generated mutants need to cover the various fault types described in the literature on fault models for AspectJ programs. We measure the distribution of the mutants generated using available mutation tools with respect to the AspectJ fault types.•We measure the difficulty of killing the generated mutants.We propose the use of simple analysis of the subject programs to prevent the generation of some equivalent mutants.MethodWe revised existing AspectJ fault models and presented a fault model that removes the problems in existing fault models, such as overlapping between fault types and missing fault types. We also defined three new fault types that occur due to incorrect data-flow interactions occurring in AspectJ programs. We used three mutation tools: AjMutator, Proteum/AJ, and MuJava on three AspectJ programs. To measure the difficulty of killing the mutants created using a mutation operator, we compared the average number of the mutants killed by 10 test suites that satisfy block coverage criterion.ResultsA high percentage of the mutants are equivalent. The mutation tools do not cover all the fault types. Only 4 out of 27 operators generated mutants that were easy to kill.ConclusionsOur analysis approach removed about 80% of the equivalent mutants. Higher order mutation is needed to cover all the fault types. 
54|9|http://www.sciencedirect.com/science/journal/09505849/54/9|Mutation based test case generation via a path selection strategy|ContextGenerally, mutation analysis has been identified as a powerful testing method. Researchers have shown that its use as a testing criterion exercises quite thoroughly the system under test while it achieves to reveal more faults than standard structural testing criteria. Despite its potential, mutation fails to be adopted in a widespread practical use and its popularity falls significantly short when compared with other structural methods. This can be attributed to the lack of thorough studies dealing with the practical problems introduced by mutation and the assessment of the effort needed when applying it. Such an incident, masks the real cost involved preventing the development of easy and effective to use strategies to circumvent this problem.ObjectiveIn this paper, a path selection strategy for selecting test cases able to effectively kill mutants when performing weak mutation testing is presented and analysed.MethodThe testing effort is highly correlated with the number of attempts the tester makes in order to generate adequate test cases. Therefore, a significant influence on the efficiency associated with a test case generation strategy greatly depends on the number of candidate paths selected in order to achieve a predefined coverage goal. The effort can thus be related to the number of infeasible paths encountered during the test case generation process.ResultsAn experiment, investigating well over 55 million of program paths is conducted based on a strategy that alleviates the effects of infeasible paths. Strategy details, along with a prototype implementation are reported and analysed through the experimental results obtained by its employment to a set of program units.ConclusionThe results obtained suggest that the strategy used can play an important role in making the mutation testing method more appealing and practical. 
54|9||Issue-based variability management|ContextVariability management is a key activity in software product line engineering. This paper focuses on managing rationale information during the decision-making activities that arise during variability management. By decision-making we refer to systematic problem solving by considering and evaluating various alternatives. Rationale management is a branch of science that enables decision-making based on the argumentation of stakeholders while capturing the reasons and justifications behind these decisions.ObjectiveDecision-making should be supported to identify variability in domain engineering and to resolve variation points in application engineering. We capture the rationale behind variability management decisions. The captured rationale information is useful to evaluate future changes of variability models as well as to handle future instantiations of variation points. We claim that maintaining rationale will enhance the longevity of variability models. Furthermore, decisions should be performed using a formal communication between domain engineering and application engineering.MethodWe initiate the novel area of issue-based variability management (IVM) by extending variability management with rationale management. The key contributions of this paper are: (i) an issue-based variability management methodology (IVMM), which combines questions, options and criteria (QOC) and a specific variability approach; (ii) a meta-model for IVMM and a process for variability management and (iii) a tool for the methodology, which was developed by extending an open source rationale management tool.ResultsRationale approaches (e.g. questions, options and criteria) guide distributed stakeholders when selecting choices for instantiating variation points. Similarly, rationale approaches also aid the elicitation of variability and the evaluation of changes. The rationale captured within the decision-making process can be reused to perform future decisions on variability.ConclusionIVMM was evaluated comparatively based on an experimental survey, which provided evidence that IVMM is more effective than a variability modeling approach that does not use issues. 
54|9||Composing web services enacted by autonomous agents through agent-centric contract net protocol|ContextAgents are considered as one of the fundamental technologies underlying open and dynamic systems that are largely enabled by the semantic web and web services. Recently, there is a trend to introduce the notion of autonomy empowered by agents into web services. However, it has been argued that the characteristics of autonomy will make agents become available intermittently and behave variedly over time, which therefore increase the complexity on devising mechanisms for composing services enacted by autonomous agents.ObjectiveIn this work, we propose an extension to Contract Net protocol, called Agent-centric Contract Net Protocol (ACNP), as a negotiation mechanism with three key features for composing web services enacted by autonomous agents.Method(1) A matchmaking mechanism embedded in a middle agent (as a service matchmaker) for discovering web services that are available intermittently is presented based on the concept of agent roles; (2) A selection algorithm based on risk-enabled reputation model (REAL) embedded in a manager agent (as a service composer) is introduced to serve a basis for selecting web services with variant performance; and (3) A negotiation mechanism between a manager agent and contractor agents (as atomic services) is devised and enables both a service composer and the atomic services to request, refuse or agree on adapting changes of services.ResultsThe problem of assembling a computer is discussed in this paper.ConclusionIt is increasingly recognised that web services would become more autonomous by introducing diverse agent technologies to better constitute more complex systems in open and dynamic environments. As web service technologies are best exploited by composite services, it is imperative to devise mechanisms for composing services of autonomy. 
54|9||Making the leap to a software platform strategy: Issues and challenges|ContextWhile there are many success stories of achieving high reuse and improved quality using software platforms, there is a need to investigate the issues and challenges organizations face when transitioning to a software platform strategy.ObjectiveThis case study provides a comprehensive taxonomy of the challenges faced when a medium-scale organization decided to adopt software platforms. The study also reveals how new trends in software engineering (i.e. agile methods, distributed development, and flat management structures) interplayed with the chosen platform strategy.MethodWe used an ethnographic approach to collect data by spending time at a medium-scale company in Scandinavia. We conducted 16 in-depth interviews with representatives of eight different teams, three of which were working on three separate platforms. The collected data was analyzed using Grounded Theory.ResultsThe findings identify four classes of challenges, namely: business challenges, organizational challenges, technical challenges, and people challenges. The article explains how these findings can be used to help researchers and practitioners identify practical solutions and required tool support.ConclusionThe organization’s decision to adopt a software platform strategy introduced a number of challenges. These challenges need to be understood and addressed in order to reap the benefits of reuse. Researchers need to further investigate issues such as supportive organizational structures for platform development, the role of agile methods in software platforms, tool support for testing and continuous integration in the platform context, and reuse recommendation systems. 
54|9||The role of social interaction in software effort estimation: Unpacking the âmagic stepâ between reasoning and decision-making|ContextSoftware effort estimation is a core task regarding planning, budgeting and controlling software development projects. However, providing accurate effort estimates is challenging. Estimation work is increasingly group based, and to support it, there is a need to reveal how work practices are carried out as collaborative efforts.ObjectiveThis paper examines the use of concepts in software effort estimation by analysing group work as communicative practice. The objective is to improve our understanding of how software professionals invoke different types of knowledge when talking, reasoning and reaching a decision on a software effort estimate.MethodEstimation meetings in the industry where planning poker was used as the estimation method have been video recorded and analysed by means of the interaction analysis technique, focusing on the communicative and collaborative aspects of the group work.ResultsThe user story mediates the types of resources and knowledge needed to solve the task. Concepts from the knowledge domain are used to frame the task and allow the participants to reach consensus, sufficient to take the next step in the problem-solving activity. Individual knowledge seems to be the dominating orientation when it comes to specifying the work needed for solving the tasks.ConclusionThe step from reasoning to decision-making has been called the “magic step” in software effort estimation. We argue that the magic step is found in the analysis of the social interaction in which the concepts used are anchored in the knowledge domain of software engineering and in the historical experiences of the participants and subsequently become activated. We propose that by taking a socio-cultural perspective on concepts in activities, the ways in which software professionals reach a decision can be unpacked. The paper contributes to an understanding of the role of concepts in group work and of software effort estimation as a specific work practice. 
54|9||An advanced approach for modeling and detecting software vulnerabilities|ContextPassive testing is a technique in which traces collected from the execution of a system under test are examined for evidence of flaws in the system.ObjectiveIn this paper we present a method for detecting the presence of security vulnerabilities by detecting evidence of their causes in execution traces. This is a new approach to security vulnerability detection.MethodOur method uses formal models of vulnerability causes, known as security goal models and vulnerability detection conditions (VDCs). The former are used to identify the causes of vulnerabilities and model their dependencies, and the latter to give a formal interpretation that is suitable for vulnerability detection using passive testing techniques. We have implemented modeling tools for security goal models and vulnerability detection conditions, as well as TestInv-Code, a tool that checks execution traces of compiled programs for evidence of VDCs.ResultsWe present the full definitions of security goal models and vulnerability detection conditions, as well as structured methods for creating both. We describe the design and implementation of TestInv-Code. Finally we show results obtained from running TestInv-Code to detect typical vulnerabilities in several open source projects. By testing versions with known vulnerabilities, we can quantify the effectiveness of the approach.ConclusionAlthough the current implementation has some limitations, passive testing for vulnerability detection works well, and using models as the basis for testing ensures that users of the testing tool can easily extend it to handle new vulnerabilities. 
54|9||The Pro-PD Process Model for Product Derivation within software product lines|BackgroundThe derivation of products from a software product line is a time consuming and expensive activity. Despite recognition that an effective process could alleviate many of the difficulties associated with product derivation, existing approaches have different scope, emphasise different aspects of the derivation process and are frequently too specialised to serve as a general solution.ObjectiveTo define a systematic process that will provide a structured approach to the derivation of products from a software product line, based on a set of tasks, roles and artefacts.MethodThrough a series of research stages using sources in industry and academia, this research has developed a Process Model for Product Derivation (Pro-PD). We document the evidence for the construction of Pro-PD and the design decisions taken. We evaluate Pro-PD through comparison with prominent existing approaches and standards.ResultsThis research presents a Process Model for Product Derivation (Pro-PD). Pro-PD describes the tasks, roles and work artefacts used to derive products from a software product line.ConclusionIn response to a need for methodological support, we developed Pro-PD (Process Model for Product Derivation). Pro-PD was iteratively developed and evaluated through four research stages. Our research is a first step toward an evidence-based methodology for product derivation and a starting point for the definition of a product derivation approach. 
54|9||Evaluation of the Pattern-based method for Secure Development (PbSD): A controlled experiment|ContextSecurity in general, and database protection from unauthorized access in particular, are crucial for organizations. Although it has been long accepted that the important system requirements should be considered from the early stages of the development process, non-functional requirements such as security tend to get neglected or dealt with only at later stages of the development process.ObjectiveWe present an empirical study conducted to evaluate a Pattern-based method for Secure Development – PbSD – that aims to help developers, in particular database designers, to design database schemata that comply with the organizational security policies regarding authorization, from the early stages of development. The method provides a complete framework to guide, enforce and verify the correct implementation of security policies within a system design, and eventually generate a database schema from that design.MethodThe PbSD method was evaluated in comparison with a popular existing method that directly specifies the security requirements in SQL and Oracle’s VPD. The two methods were compared with respect to the quality of the created access control specifications, the time it takes to complete the specification, and the perceived quality of the methods.ResultsWe found that the quality of the access control specifications using the PbSD method for secure development were better with respect to privileges granted in the table, column and row granularity levels. Moreover, subjects who used the PbSD method completed the specification task in less time compared to subjects who used SQL. Finally, the subjects perceived the PbSD method clearer and more easy to use.ConclusionThe pattern-based method for secure development can enhance the quality of security specification of databases, and decrease the software development time and cost. The results of the experiment may also indicate that the use of patterns in general has similar benefits; yet this requires further examinations. 
55|1|http://www.sciencedirect.com/science/journal/09505849/55/1|A systematic review of software robustness|ContextWith the increased use of software for running key functions in modern society it is of utmost importance to understand software robustness and how to support it. Although there have been many contributions to the field there is a lack of a coherent and summary view.ObjectiveTo address this issue, we have conducted a literature review in the field of robustness.MethodThis review has been conducted by following guidelines for systematic literature reviews. Systematic reviews are used to find and classify all existing and available literature in a certain field.ResultsFrom 9193 initial papers found in three well-known research databases, the 144 relevant papers were extracted through a multi-step filtering process with independent validation in each step. These papers were then further analyzed and categorized based on their development phase, domain, research, contribution and evaluation type. The results indicate that most existing results on software robustness focus on verification and validation of Commercial of the shelf (COTS) or operating systems or propose design solutions for robustness while there is a lack of results on how to elicit and specify robustness requirements. The research is typically solution proposals with little to no evaluation and when there is some evaluation it is primarily done with small, toy/academic example systems.ConclusionWe conclude that there is a need for more software robustness research on real-world, industrial systems and on software development phases other than testing and design, in particular on requirements engineering. 
55|1||An architectural model for software testing lesson learned systems|ContextSoftware testing is a key aspect of software reliability and quality assurance in a context where software development constantly has to overcome mammoth challenges in a continuously changing environment. One of the characteristics of software testing is that it has a large intellectual capital component and can thus benefit from the use of the experience gained from past projects. Software testing can, then, potentially benefit from solutions provided by the knowledge management discipline. There are in fact a number of proposals concerning effective knowledge management related to several software engineering processes.ObjectiveWe defend the use of a lesson learned system for software testing. The reason is that such a system is an effective knowledge management resource enabling testers and managers to take advantage of the experience locked away in the brains of the testers. To do this, the experience has to be gathered, disseminated and reused.MethodAfter analyzing the proposals for managing software testing experience, significant weaknesses have been detected in the current systems of this type. The architectural model proposed here for lesson learned systems is designed to try to avoid these weaknesses. This model (i) defines the structure of the software testing lessons learned; (ii) sets up procedures for lesson learned management; and (iii) supports the design of software tools to manage the lessons learned.ResultsA different approach, based on the management of the lessons learned that software testing engineers gather from everyday experience, with two basic goals: usefulness and applicability.ConclusionThe architectural model proposed here lays the groundwork to overcome the obstacles to sharing and reusing experience gained in the software testing and test management. As such, it provides guidance for developing software testing lesson learned systems. 
55|1||Reasoning with contextual requirements: Detecting inconsistency and conflicts|ContextThe environment in which the system operates, its context, is variable. The autonomous ability of a software to adapt to context has to be planned since the requirements analysis stage as a strong mutual influence between requirements and context does exist. On the one hand, context is a main factor to decide whether to activate a requirement, the applicable alternatives to meet an activated requirement as well as their qualities. On the other hand, the system actions to reach requirements could cause changes in the context.ObjectivesModelling the relationship between requirements and context is a complex task and developing error-free models is hard to achieve without an automated support. The main objective of this paper is to develop a set of automated analysis mechanisms to support the requirements engineers to detect and analyze modelling errors in contextual requirements models.MethodWe study the analysis of the contextual goal model which is a requirements model that weaves together the variability of both context and requirements. Goal models are used during the early stages of software development and, thus, our analysis detects errors early in the development process. We develop two analysis mechanisms to detect two kinds of modelling errors. The first mechanism concerns the detection of inconsistent specification of contexts in a goal model. The second concerns the detection of conflicting context changes that arise as a consequence of the actions performed by the system to meet different requirements simultaneously. We support our analysis with a CASE tool and provide a systematic process that guides the construction and analysis of contextual goal models. We illustrate and evaluate our framework via a case study on a smart-home system for supporting the life of people having dementia problems.ResultsThe evaluation showed a significant ability of our analysis mechanisms to detect errors which were not notable by requirements engineers. Moreover, the evaluation showed acceptable performance of these mechanisms when processing up to medium-sized contextual goal models. The modelling constructs which we proposed as an input to enable the analysis were found easy to understand and capture.ConclusionsOur developed analysis for the detection of inconsistency and conflicts in contextual goal models is an essential step for the entire system correctness. It avoids us developing unusable and unwanted functionalities and functionalities which lead to conflicts when they operate together. Further research to improve our analysis to scale with large-sized models and to consider other kinds of errors is still needed. 
55|1||A pattern-based approach for the verification of business process descriptions|ContextEmpirical studies indicate that companies mostly adopt business process management to support organizational concerns. Consequently, descriptions of business processes usually rely on natural languages or tables. Nevertheless, many companies also intend to support process execution using software (e.g. ERP systems, databases, office applications). This involves significant implementation effort, and therefore the underlying business process description should be correct.ObjectiveThis paper presents an approach for verifying business process descriptions that can be presented in any style, e.g. as text, tables or graphical process model created with some or other process modelling language.MethodBecause of its generality and the availability of tools, model checking is used here to verify business process descriptions. In particular, the SPIN model checker was chosen because it is well-established and continuously improved. The proposed composition-based approach permits the semi-automatic implementation of any kind of business process description in the SPIN tool and the verification of numerous correctness properties, which refer to workflow control flow patterns, safety and liveness.ResultsBecause of systemizing, the proposed approach represents all 43 workflow control-flow patterns by 20 basic ones, six fragments and their relationships. For the basic patterns and fragments, PROMELA inline-constructs are provided, and the set of applicable correctness properties is suggested. The correctness properties are specified as templates in linear temporal logic. Implementing a business process description consists of assembling the inline-constructs and associating business semantics with the symbols in the logical formulae of the correctness properties. For verification, the SPIN algorithms are used.ConclusionBy using the approach presented, business process descriptions can be checked for correctness even if the original business process description is vague. The control structures allowed in the business process descriptions are not restricted, and the verifiable correctness properties go beyond soundness. 
55|1||Model-driven performance analysis of rule-based domain specific visual models|ContextDomain-Specific Visual Languages (DSVLs) play a crucial role in Model-Driven Engineering (MDE). Most DSVLs already allow the specification of the structure and behavior of systems. However, there is also an increasing need to model, simulate and reason about their non-functional properties. In particular, QoS usage and management constraints (performance, reliability, etc.) are essential characteristics of any non-trivial system.ObjectiveVery few DSVLs currently offer support for modeling these kinds of properties. And those which do, tend to require skilled knowledge of specialized notations, which clashes with the intuitive nature of DSVLs. In this paper we present an alternative approach to specify QoS properties in a high-level and platform-independent manner.MethodWe propose the use of special objects (observers) that can be added to the graphical specification of a system for describing and monitoring some of its non-functional properties.ResultsObservers allow extending the global state of the system with the variables that the designer wants to analyze, being able to capture the performance properties of interest. A performance evaluation tool has also been developed as a proof of concept for the proposal.ConclusionThe results show how non-functional properties can be specified in DSVLs using observers, and how the performance of systems specified in this way can be evaluated in a flexible and effective way. 
55|1||Guest Editorial: Special section of the best papers from the 2nd International Symposium on Search Based Software Engineering 2010|
55|1||AUSTIN: An open source tool for search based software testing of C programs|ContextDespite the large number of publications on Search-Based Software Testing (SBST), there remain few publicly available tools. This paper introduces AUSTIN, a publicly available open source SBST tool for the C language.1 The paper is an extension of previous work [1]. It includes a new hill climb algorithm implemented in AUSTIN and an investigation into the effectiveness and efficiency of different pointer handling techniques implemented by AUSTIN’s test data generation algorithms.ObjectiveTo evaluate the different search algorithms implemented within AUSTIN on open source systems with respect to effectiveness and efficiency in achieving branch coverage. Further, to compare AUSTIN against a non-publicly available, state-of-the-art Evolutionary Testing Framework (ETF).MethodFirst, we use example functions from open source benchmarks as well as common data structure implementations to check if the decision procedure for pointer inputs, introduced in this paper, differs in terms of effectiveness and efficiency compared to a simpler alternative that generates random memory graphs. A second empirical study formulates two alternate hypotheses regarding the effectiveness and efficiency of AUSTIN compared to the ETF. These hypotheses are tested using a paired Wilcoxon test.Results and ConclusionThe first study highlights some practical problems with the decision procedure for pointer inputs described in this paper. In particular, if the code under test contains insufficient guard statements to enforce constraints over pointers, then using a constraint solver for pointer inputs may be suboptimal compared to a method that generates random memory graphs. The programs used in the second study do not require any constraint solving for pointer inputs and consist of eight non-trivial, real-world C functions drawn from three embedded automotive software modules. For these functions, AUSTIN is competitive compared to the ETF, achieving an equal or higher branch coverage for six of the functions. In addition, for functions where AUSTIN’s branch coverage is equal or higher, AUSTIN is more efficient than the ETF. 
55|1||Empirical evaluation of search based requirements interaction management|ContextRequirements optimization has been widely studied in the Search Based Software Engineering (SBSE) literature. However, previous approaches have not handled requirement interactions, such as the dependencies that may exist between requirements, and, or, precedence, cost- and value-based constraints.ObjectiveTo introduce and evaluate a Multi-Objective Search Based Requirements Selection technique, using chromosome repair and to evaluate it on both synthetic and real world data sets, in order to assess its effectiveness and scalability. The paper extends and improves upon our previous conference paper on requirements interaction management.1MethodThe popular multi-objective evolutionary algorithm NSGA-II was used to produce baseline data for each data set in order to determine how many solutions on the Pareto front fail to meet five different requirement interaction constraints. The results for this baseline data are compared to those obtained using the archive based approach previously studied and the repair based approach introduced in this paper.ResultsThe repair based approach was found to produce more solutions on the Pareto front and better convergence and diversity of results than the previously studied NSGA-II and archive-based NSGA-II approaches based on Kruskal–Wallis test in most cases. The repair based approach was also found to scale almost as well as the previous approach.ConclusionThere is evidence to indicate that the repair based algorithm introduced in this paper is a suitable technique for extending previous work on requirements optimization to handle the requirement interaction constraints inherent in requirement interactions arising from dependencies, and, or, precedence, cost- and value-based constraints. 
55|1||An identification of program factors that impact crossover performance in evolutionary test input generation for the branch coverage of C programs|Context: Genetic Algorithms are a popular search-based optimisation technique for automatically generating test inputs for structural coverage of a program, but there has been little work investigating the class of programs for which they will perform well.Objective: This paper presents and evaluates a series of program factors that are hypothesised to affect the performance of crossover, a key search operator in Genetic Algorithms, when searching for inputs that cover the branching structure of a C function.Method: Each program factor is evaluated with example programs using Genetic Algorithms with and without crossover. Experiments are also performed to test whether crossover is acting as macro-mutation operator rather than usefully recombining the component parts of input vectors when searching for test data.Results: The results show that crossover has an impact for each of the program factors studied.Conclusion: It is concluded crossover plays an increasingly important role for programs with large, multi-dimensional input spaces, where the target structure’s input condition breaks down into independent sub-problems for which solutions may be sought in parallel. Furthermore, it is found that crossover can be inhibited when the program under test is unstructured or involves nested conditional statements; and when intermediate variables are used in branching conditions, as opposed to direct input values. 
55|1||Interactive requirements prioritization using a genetic algorithm|ContextThe order in which requirements are implemented affects the delivery of value to the end-user, but it also depends on technical constraints and resource availability. The outcome of requirements prioritization is a total ordering of requirements that best accommodates the various kinds of constraints and priorities. During requirements prioritization, some decisions on the relative importance of requirements or the feasibility of a given implementation order must necessarily resort to a human (e.g., the requirements analyst), possessing the involved knowledge.ObjectiveIn this paper, we propose an Interactive Genetic Algorithm (IGA) that includes incremental knowledge acquisition and combines it with the existing constraints, such as dependencies and priorities. We also assess the performance of the proposed algorithm.MethodThe validation of IGA was conducted on a real case study, by comparing the proposed algorithm with the state of the art, interactive prioritization technique Incomplete Analytic Hierarchy Process (IAHP).ResultsThe proposed method outperforms IAHP in terms of effectiveness, efficiency and robustness to decision maker errors.ConclusionIGA produces a good approximation of the reference requirements ranking, requiring an acceptable manual effort and tolerating a reasonable human error rate. 
55|10|http://www.sciencedirect.com/science/journal/09505849/55/10|Graphical user interface (GUI) testing: Systematic mapping and repository|ContextGUI testing is system testing of a software that has a graphical-user interface (GUI) front-end. Because system testing entails that the entire software system, including the user interface, be tested as a whole, during GUI testing, test cases—modeled as sequences of user input events—are developed and executed on the software by exercising the GUI’s widgets (e.g., text boxes and clickable buttons). More than 230 articles have appeared in the area of GUI testing since 1991.ObjectiveIn this paper, we study this existing body of knowledge using a systematic mapping (SM).MethodThe SM is conducted using the guidelines proposed by Petersen et al. We pose three sets of research questions. We define selection and exclusion criteria. From the initial pool of 230 articles, published in years 1991–2011, our final pool consisted of 136 articles. We systematically develop a classification scheme and map the selected articles to this scheme.ResultsWe present two types of results. First, we report the demographics and bibliometrics trends in this domain, including: top-cited articles, active researchers, top venues, and active countries in this research area. Moreover, we derive the trends, for instance, in terms of types of articles, sources of information to derive test cases, types of evaluations used in articles, etc. Our second major result is a publicly-accessible repository that contains all our mapping data. We plan to update this repository on a regular basis, making it a “live” resource for all researchers.ConclusionOur SM provides an overview of existing GUI testing approaches and helps spot areas in the field that require more attention from the research community. For example, much work is needed to connect academic model-based techniques with commercially available tools. To this end, studies are needed to compare the state-of-the-art in GUI testing in academic techniques and industrial tools. 
55|10||The state of the art in automated requirements elicitation|ContextIn large software development projects a huge number of unstructured text documents from various stakeholders becomes available and needs to be analyzed and transformed into structured requirements. This elicitation process is known to be time-consuming and error-prone when performed manually by a requirements engineer. Consequently, substantial research has been done to automate the process through a plethora of tools and technologies.ObjectiveThis paper aims to capture the current state of automated requirements elicitation and derive future research directions by identifying gaps in the existing body of knowledge and through relating existing works to each other. More specifically, we are investigating the following research question: What is the state of the art in research covering tool support for automated requirements elicitation from natural language documents?MethodA systematic review of the literature in automated requirements elicitation is performed. Identified works are categorized using an analysis framework comprising tool categories, technological concepts and evaluation approaches. Furthermore, the identified papers are related to each other through citation analysis to trace the development of the research field.ResultsWe identified, categorized and related 36 relevant publications. Summarizing the observations we made, we propose future research to (1) investigate alternative elicitation paradigms going beyond a pure automation approach (2) compare the effects of different types of knowledge on elicitation results (3) apply comparative evaluation methods and multi-dimensional evaluation measures and (4) strive for a closer integration of research activities across the sub-fields of automatic requirements elicitation.ConclusionThrough the results of our paper, we intend to contribute to the Requirements Engineering body of knowledge by (1) conceptualizing an analysis framework for works in the area of automated requirements elicitation, going beyond former classifications (2) providing an extensive overview and categorization of existing works in this area (3) formulating concise directions for future research. 
55|10||AREION: Software effort estimation based on multiple regressions with adaptive recursive data partitioning|ContextAlong with expert judgment, analogy-based estimation, and algorithmic methods (such as Function point analysis and COCOMO), Least Squares Regression (LSR) has been one of the most commonly studied software effort estimation methods. However, an effort estimation model using LSR, a single LSR model, is highly affected by the data distribution. Specifically, if the data set is scattered and the data do not sit closely on the single LSR model line (do not closely map to a linear structure) then the model usually shows poor performance. In order to overcome this drawback of the LSR model, a data partitioning-based approach can be considered as one of the solutions to alleviate the effect of data distribution. Even though clustering-based approaches have been introduced, they still have potential problems to provide accurate and stable effort estimates.ObjectiveIn this paper, we propose a new data partitioning-based approach to achieve more accurate and stable effort estimates via LSR. This approach also provides an effort prediction interval that is useful to describe the uncertainty of the estimates.MethodEmpirical experiments are performed to evaluate the performance of the proposed approach by comparing with the basic LSR approach and clustering-based approaches, based on industrial data sets (two subsets of the ISBSG (Release 9) data set and one industrial data set collected from a banking institution).ResultsThe experimental results show that the proposed approach not only improves the accuracy of effort estimation more significantly than that of other approaches, but it also achieves robust and stable results according to the degree of data partitioning.ConclusionCompared with the other considered approaches, the proposed approach shows a superior performance by alleviating the effect of data distribution that is a major practical issue in software effort estimation. 
55|10||An object-oriented implementation of concurrent and hierarchical state machines|ContextState machine diagrams are a powerful means to describe the behavior of reactive systems. Unfortunately, the implementation of state machines is difficult, because state machine concepts, like states, events and transitions, are not directly supported in commonly used programming languages. Most of the implementation approaches known so far have one or more serious drawbacks: they are difficult to understand and maintain, lack in performance, depend on the properties of a specific programming language or do not implement the more advanced state machine features like hierarchy, concurrency or history.ObjectiveThis paper proposes and examines an approach to implement state machines, where both states and events are objects. Because the reaction of the state machine depends on two objects (state and event), a method known as double-dispatch is used to invoke the transition between the states. The aim of this work is to explore this approach in detail.MethodTo prove the usefulness of the proposed approach, an example was implemented with the proposed approach as well as with other commonly known approaches. The implementation strategies are then compared with each other with respect to run-time, code size, maintainability and portability.ResultsThe presented approach executes fast but needs slightly more memory than other approaches. It supports hierarchy, concurrency and history, is human authorable, easy to understand and easy to modify. Because of its pure object-oriented nature depending only on inheritance and late binding, it is extensible and can be implemented with a wide variety of programming languages.ConclusionThe results show that the presented approach is a useful way to implement state machines, even on small micro-controllers. 
55|10||An investigation of âbuild vs. buyâ decision for software acquisition by small to medium enterprises|ContextThe prevalence of computing and communication technologies, combined with the availability of sophisticated and highly specialised software packages from software vendors has made package acquisition a viable option for many organisations. While some research has addressed the factors that influence the selection of the software acquisition method in large organisations, little is known about the factors affecting SMEs.ObjectiveTo provide an understanding of factors that affect the decision process of software acquisition for SMEs. It is expected that results from this study: (i) will assist the SME decision process for software acquisition, and (ii) will assist policy makers in terms of developing appropriate guidelines for SME software acquisition.MethodA positivist research perspective has been adopted involving semi-structured interviews in eight SMEs in Thailand with the interviewees assigning to each of the potential factors.ResultsThe study found that the following factors affect both SMEs and large organisations: requirements fit, cost, scale and complexity, commoditization/flexibility, time, in-house experts, support structure, and operational factors. Factors mainly applying to large organisations were strategic role of the software, intellectual property concerns, and risk, Factors particularly relevant to SMEs (ubiquitous systems, availability of free download, and customizable to specific government/tax regulations).ConclusionThe results suggest that: (i) when deciding on their software acquisition method, SMEs are generally less likely to pursue a long-term vision compared with larger organisations, possibly because SMEs mainly serve their local markets; and (ii) contrary to the large organisations, the role that the IT plays in SMEs may not be as vital to the SMEs’ core business processes, to their supply chains, and/or to the management of their customer relationship. Furthermore, neither the level of technological intensity nor size of the SME appears to affect the ranks given by the interviewees for the various factors. 
55|10||Context aware exception handling in business process execution language|ContextFault handling represents a very important aspect of business process functioning. However, fault handling has thus far been solved statically, requiring the definition of fault handlers and handling logic to be defined at design time, which requires a great deal of effort, is error-prone and relatively difficult to maintain and extend. It is sometimes even impossible to define all fault handlers at design time.ObjectiveTo address this issue, we describe a novel context-aware architecture for fault handling in executable business process, which enables dynamic fault handling during business process execution.MethodWe performed analysis of existing fault handling disadvantages of WS-BPEL. We designed the artifact which complements existing statically defined fault handling in such a way that faults can be defined dynamically during business process run-time. We evaluated the artifact with analysis of system performance and performed a comparison against a set of well known workflow exception handling patterns.ResultsWe designed an artifact, that comprises an Observer component, Exception Handler Bus, Exception Knowledge Base and Solution Repository. A system performance analysis shows a significantly decreased repair time with the use of context aware activities. We proved that the designed artifact extends the range of supported workflow exception handling patterns.ConclusionThe artifact presented in this research considerably improves static fault handling, as it enables the dynamic fault resolution of semantically similar faults with continuous enhancement of fault handling in run-time. It also results in broader support of workflow exception handling patterns. 
55|10||Predicting SQL injection and cross site scripting vulnerabilities through mining input sanitization patterns|ContextSQL injection (SQLI) and cross site scripting (XSS) are the two most common and serious web application vulnerabilities for the past decade. To mitigate these two security threats, many vulnerability detection approaches based on static and dynamic taint analysis techniques have been proposed. Alternatively, there are also vulnerability prediction approaches based on machine learning techniques, which showed that static code attributes such as code complexity measures are cheap and useful predictors. However, current prediction approaches target general vulnerabilities. And most of these approaches locate vulnerable code only at software component or file levels. Some approaches also involve process attributes that are often difficult to measure.ObjectiveThis paper aims to provide an alternative or complementary solution to existing taint analyzers by proposing static code attributes that can be used to predict specific program statements, rather than software components, which are likely to be vulnerable to SQLI or XSS.MethodFrom the observations of input sanitization code that are commonly implemented in web applications to avoid SQLI and XSS vulnerabilities, in this paper, we propose a set of static code attributes that characterize such code patterns. We then build vulnerability prediction models from the historical information that reflect proposed static attributes and known vulnerability data to predict SQLI and XSS vulnerabilities.ResultsWe developed a prototype tool called PhpMinerI for data collection and used it to evaluate our models on eight open source web applications. Our best model achieved an averaged result of 93% recall and 11% false alarm rate in predicting SQLI vulnerabilities, and 78% recall and 6% false alarm rate in predicting XSS vulnerabilities.ConclusionThe experiment results show that our proposed vulnerability predictors are useful and effective at predicting SQLI and XSS vulnerabilities. 
55|10||Test case selection for black-box regression testing of database applications|ContextThis paper presents an approach for selecting regression test cases in the context of large-scale database applications. We focus on a black-box (specification-based) approach, relying on classification tree models to model the input domain of the system under test (SUT), in order to obtain a more practical and scalable solution. We perform an experiment in an industrial setting where the SUT is a large database application in Norway’s tax department.ObjectiveWe investigate the use of similarity-based test case selection for supporting black box regression testing of database applications. We have developed a practical approach and tool (DART) for functional black-box regression testing of database applications. In order to make the regression test approach scalable for large database applications, we needed a test case selection strategy that reduces the test execution costs and analysis effort. We used classification tree models to partition the input domain of the SUT in order to then select test cases. Rather than selecting test cases at random from each partition, we incorporated a similarity-based test case selection, hypothesizing that it would yield a higher fault detection rate.MethodAn experiment was conducted to determine which similarity-based selection algorithm was the most suitable in selecting test cases in large regression test suites, and whether similarity-based selection was a worthwhile and practical alternative to simpler solutions.ResultsThe results show that combining similarity measurement with partition-based test case selection, by using similarity-based test case selection within each partition, can provide improved fault detection rates over simpler solutions when specific conditions are met regarding the partitions.ConclusionsUnder the conditions present in the experiment the improvements were marginal. However, a detailed analysis concludes that the similarity-based selection strategy should be applied when a large number of test cases are contained in each partition and there is significant variability within partitions. If these conditions are not present, incorporating similarity measures is not worthwhile, since the gain is negligible over a random selection within each partition. 
55|10||Towards a simplified definition of Function Points|BackgroundThe measurement of Function Points is based on Base Functional Components. The process of identifying and weighting Base Functional Components is hardly automatable, due to the informality of both the Function Point method and the requirements documents being measured. So, Function Point measurement generally requires a lengthy and costly process.ObjectivesWe investigate whether it is possible to take into account only subsets of Base Functional Components so as to obtain functional size measures that simplify Function Points with the same effort estimation accuracy as the original Function Points measure. Simplifying the definition of Function Points would imply a reduction of measurement costs and may help spread the adoption of this type of measurement practices. Specifically, we empirically investigate the following issues: whether available data provide evidence that simplified software functionality measures can be defined in a way that is consistent with Function Point Analysis; whether simplified functional size measures by themselves can be used without any appreciable loss in software development effort prediction accuracy; whether simplified functional size measures can be used as software development effort predictors in models that also use other software requirements measures.MethodWe analyze the relationships between Function Points and their Base Functional Components. We also analyze the relationships between Base Functional Components and development effort. Finally, we built effort prediction models that contain both the simplified functional measures and additional requirements measures.ResultsSignificant statistical models correlate Function Points with Base Functional Components. Basic Functional Components can be used to build models of effort that are equivalent, in terms of accuracy, to those based on Function Points. Finally, simplified Function Points measures can be used as software development effort predictors in models that also use other requirements measures.ConclusionThe definition and measurement processes of Function Points can be dramatically simplified by taking into account a subset of the Base Functional Components used in the original definition of the measure, thus allowing for substantial savings in measurement effort, without sacrificing the accuracy of software development effort estimates. 
55|10||A study of subgroup discovery approaches for defect prediction|ContextAlthough many papers have been published on software defect prediction techniques, machine learning approaches have yet to be fully explored.ObjectiveIn this paper we suggest using a descriptive approach for defect prediction rather than the precise classification techniques that are usually adopted. This allows us to characterise defective modules with simple rules that can easily be applied by practitioners and deliver a practical (or engineering) approach rather than a highly accurate result.MethodWe describe two well-known subgroup discovery algorithms, the SD algorithm and the CN2-SD algorithm to obtain rules that identify defect prone modules. The empirical work is performed with publicly available datasets from the Promise repository and object-oriented metrics from an Eclipse repository related to defect prediction. Subgroup discovery algorithms mitigate against characteristics of datasets that hinder the applicability of classification algorithms and so remove the need for preprocessing techniques.ResultsThe results show that the generated rules can be used to guide testing effort in order to improve the quality of software development projects. Such rules can indicate metrics, their threshold values and relationships between metrics of defective modules.ConclusionsThe induced rules are simple to use and easy to understand as they provide a description rather than a complete classification of the whole dataset. Thus this paper represents an engineering approach to defect prediction, i.e., an approach which is useful in practice, easily understandable and can be applied by practitioners. 
55|10||Comparing the comprehensibility of requirements models expressed in Use Case and Tropos: Results from a family of experiments|ContextOver the years, several modeling languages for requirements have been proposed. These languages employ different conceptual approaches, including scenario-based and goal-oriented ones. Empirical studies providing evidence about requirements model comprehensibility are rare, especially when addressing languages that belong to different modeling approaches.ObjectiveThis work aims to compare the comprehensibility of requirements models expressed in different but comparable modeling approaches from a requirements analysts’ perspective. In particular, in this paper we compare the comprehensibility of requirements models expressed in two visual modeling languages: Use Case, which is scenario-based, and Tropos, which exploits goal-oriented modeling. We further compare the effort required for comprehending the different models, and the derived productivity in each case.MethodRequirements model comprehensibility is measured here in the context of three types of tasks that analysts usually perform, namely mapping between textual description and the model elements, reading and understanding the model irrespectively of the original textual description, and modifying the model. This experimental evaluation has been conducted within a family of controlled experiments aiming at comparing the comprehensibility of Use Case and Tropos requirements models. Three runs of the experiment were performed, including a first experiment and two replications, involving 79 subjects overall (all of which were information systems students). The data for each experiment was separately analyzed, followed by a meta-analysis of the three experiments.ResultsThe experimental results show that Tropos models seem to be more comprehensible with respect to the three types of requirements analysis tasks, although more time consuming than Use Case models.ConclusionsMeasuring model comprehensibility by means of controlled experiments is feasible and provides a basis for comparing Tropos and Use Case models, although these languages belong to different modeling approaches. Specifically, Tropos outperformed Use Case in terms of comprehensibility, but required more effort leading to a similar productivity of the two languages. 
55|11|http://www.sciencedirect.com/science/journal/09505849/55/11|Combining service-orientation and software product line engineering: A systematic mapping study|ContextService-Orientation (SO) is a rapidly emerging paradigm for the design and development of adaptive and dynamic software systems. Software Product Line Engineering (SPLE) has also gained attention as a promising and successful software reuse development paradigm over the last decade and proven to provide effective solutions to deal with managing the growing complexity of software systems.ObjectiveThis study aims at characterizing and identifying the existing research on employing and leveraging SO and SPLE.MethodWe conducted a systematic mapping study to identify and analyze related literature. We identified 81 primary studies, dated from 2000–2011 and classified them with respect to research focus, types of research and contribution.ResultThe mapping synthesizes the available evidence about combining the synergy points and integration of SO and SPLE. The analysis shows that the majority of studies focus on service variability modeling and adaptive systems by employing SPLE principles and approaches.In particular, SPLE approaches, especially feature-oriented approaches for variability modeling, have been applied to the design and development of service-oriented systems. While SO is employed in software product line contexts for the realization of product lines to reconcile the flexibility, scalability and dynamism in product derivations thereby creating dynamic software product lines.ConclusionOur study summarizes and characterizes the SO and SPLE topics researchers have investigated over the past decade and identifies promising research directions as due to the synergy generated by integrating methods and techniques from these two areas. 
55|11||Software evolution visualization: A systematic mapping study|BackgroundSoftware evolution is an important topic in software engineering. It generally deals with large amounts of data, as one must look at whole project histories as opposed to their current snapshot. Software visualization is the field of software engineering that aims to help people to understand software through the use of visual resources. It can be effectively used to analyze and understand the large amount of data produced during software evolution.ObjectiveThis study investigates Software Evolution Visualization (SEV) approaches, collecting evidence about how SEV research is structured, synthesizing current evidence on the goals of the proposed approaches and identifying key challenges for its use in practice.MethodsA mapping study was conducted to analyze how the SEV area is structured. Selected primary studies were classified and analyzed with respect to nine research questions.ResultsSEV has been used for many different purposes, especially for change comprehension, change prediction and contribution analysis. The analysis identified gaps in the studies with respect to their goals, strategies and approaches. It also pointed out to a widespread lack of empirical studies in the area.ConclusionResearchers have proposed many SEV approaches during the past years, but some have failed to clearly state their goals, tie them back to concrete problems, or formally validate their usefulness. The identified gaps indicate that there still are many opportunities to be explored in the area. 
55|11||Enforcement of entailment constraints in distributed service-based business processes|ContextA distributed business process is executed in a distributed computing environment. The service-oriented architecture (SOA) paradigm is a popular option for the integration of software services and execution of distributed business processes. Entailment constraints, such as mutual exclusion and binding constraints, are important means to control process execution. Mutually exclusive tasks result from the division of powerful rights and responsibilities to prevent fraud and abuse. In contrast, binding constraints define that a subject who performed one task must also perform the corresponding bound task(s).ObjectiveWe aim to provide a model-driven approach for the specification and enforcement of task-based entailment constraints in distributed service-based business processes.MethodBased on a generic metamodel, we define a domain-specific language (DSL) that maps the different modeling-level artifacts to the implementation-level. The DSL integrates elements from role-based access control (RBAC) with the tasks that are performed in a business process. Process definitions are annotated using the DSL, and our software platform uses automated model transformations to produce executable WS-BPEL specifications which enforce the entailment constraints. We evaluate the impact of constraint enforcement on runtime performance for five selected service-based processes from existing literature.ResultsOur evaluation demonstrates that the approach correctly enforces task-based entailment constraints at runtime. The performance experiments illustrate that the runtime enforcement operates with an overhead that scales well up to the order of several ten thousand logged invocations. Using our DSL annotations, the user-defined process definition remains declarative and clean of security enforcement code.ConclusionOur approach decouples the concerns of (non-technical) domain experts from technical details of entailment constraint enforcement. The developed framework integrates seamlessly with WS-BPEL and the Web services technology stack. Our prototype implementation shows the feasibility of the approach, and the evaluation points to future work and further performance optimizations. 
55|11||AiOLoS: A model for assessing organizational learning in software development organizations|ContextIn an industry in which technological developments are rapid, in order to keep up with the continuously increasing competition and to obtain competitive advantage, the software development organizations (SDOs) need to obtain the correct knowledge, use it efficiently and pass it to future projects evolving it accordingly.ObjectiveThe main aim of this paper is to propose a novel model, AiOLoS, for assessing the level and characteristics of organizational learning (OL) in SDOs.MethodThe primary contributions of this two-legged AiOLoS model are the identification of the major process areas and the core processes that a learning software organization (LSO) follows during its OL process and to provide the necessary measures and the corresponding definitions/interpretations for the assessment of the learning characteristics of the SDO. The research is supported with a multiple case-study work to identify the mapping of the core processes and the applicability of the AiOLoS model to SDOs, its utilization as a tool for assessing OL and providing a basis for software process improvement (SPI).ResultsThe case studies have shown that not only the AiOLoS measures are applicable to SDOs but also that they measure in great extent the actual OL that is realized in the organization and that the major process areas and core processes are actually related to the OL process of SDOs.ConclusionAiOLoS has been designed to provide a starting point for the enhancement of OL capabilities of SDOs, which in turn should provide a basis to conduct SPI activities. Therefore, it is also important to investigate a possible binding of AiOLoS to SPICE and the inclusion of a maturity dimension to AiOLoS. 
55|11||A formal framework for software product lines|ContextA Software Product Line is a set of software systems that are built from a common set of features. These systems are developed in a prescribed way and they can be adapted to fit the needs of customers. Feature models specify the properties of the systems that are meaningful to customers. A semantics that models the feature level has the potential to support the automatic analysis of entire software product lines.ObjectiveThe objective of this paper is to define a formal framework for Software Product Lines. This framework needs to be general enough to provide a formal semantics for existing frameworks like FODA (Feature Oriented Domain Analysis), but also to be easily adaptable to new problems.MethodWe define an algebraic language, called SPLA, to describe Software Product Lines. We provide the semantics for the algebra in three different ways. The approach followed to give the semantics is inspired by the semantics of process algebras. First we define an operational semantics, next a denotational semantics, and finally an axiomatic semantics. We also have defined a representation of the algebra into propositional logic.ResultsWe prove that the three semantics are equivalent. We also show how FODA diagrams can be automatically translated into SPLA. Furthermore, we have developed our tool, called AT, that implements the formal framework presented in this paper. This tool uses a SAT-solver to check the satisfiability of an SPL.ConclusionThis paper defines a general formal framework for software product lines. We have defined three different semantics that are equivalent; this means that depending on the context we can choose the most convenient approach: operational, denotational or axiomatic. The framework is flexible enough because it is closely related to process algebras. Process algebras are a well-known paradigm for which many extensions have been defined. 
55|11||Towards the automatic and optimal selection of risk treatments for business processes using a constraint programming approach|ContextThe use of Business Process Management Systems (BPMS) has emerged in the IT arena for the automation of business processes. In the majority of cases, the issue of security is overlooked by default in these systems, and hence the potential cost and consequences of the materialization of threats could produce catastrophic loss for organizations. Therefore, the early selection of security controls that mitigate risks is a real and important necessity. Nevertheless, there exists an enormous range of IT security controls and their configuration is a human, manual, time-consuming and error-prone task. Furthermore, configurations are carried out separately from the organization perspective and involve many security stakeholders. This separation makes difficult to ensure the effectiveness of the configuration with regard to organizational requirements.ObjectiveIn this paper, we strive to provide security stakeholders with automated tools for the optimal selection of IT security configurations in accordance with a range of business process scenarios and organizational multi-criteria.MethodAn approach based on feature model analysis and constraint programming techniques is presented, which enable the automated analysis and selection of optimal security configurations.ResultsA catalogue of feature models is determined by analyzing typical IT security controls for BPMSs for the enforcement of the standard goals of security: integrity, confidentiality, availability, authorization, and authentication. These feature models have been implemented through constraint programs, and Constraint Programming techniques based on optimized and non-optimized searches are used to automate the selection and generation of configurations. In order to compare the results of the determination of configuration a comparative analysis is given.ConclusionIn this paper, we present innovative tools based on feature models, Constraint Programming and multi-objective techniques that enable the agile, adaptable and automatic selection and generation of security configurations in accordance with the needs of the organization. 
55|11||Modeling optimal release policy under fuzzy paradigm in imperfect debugging environment|ContextIn this study, a software optimal release time with cost-reliability criteria has been discussed in an imperfect debugging environment.ObjectiveThe motive of this study is to model uncertainty involved in estimated parameters of the software reliability growth model (SRGM).MethodInitially the reliability parameters of SRGM are estimated using least square estimation (LSE). Considering the uncertainty involved in the estimated parameters due to human behavior being subjective in nature and the dynamism of the testing environment, the concept of fuzzy set theory is applicable in developing SRGM. Finally, using arithmetic operations on fuzzy numbers, the reliability and total software cost are calculated.ResultsVarious reliability measures have been computed at different levels of uncertainties, and a comparison has been made with the existing results reported in the literature.ConclusionIt is evident from the results that a better prediction of reliability measures, namely, software reliability and total software cost can be made under the fuzzy paradigm. 
55|11||Is lines of code a good measure of effort in effort-aware models?|ContextEffort-aware models, e.g., effort-aware bug prediction models aim to help practitioners identify and prioritize buggy software locations according to the effort involved with fixing the bugs. Since the effort of current bugs is not yet known and the effort of past bugs is typically not explicitly recorded, effort-aware bug prediction models are forced to use approximations, such as the number of lines of code (LOC) of the predicted files.ObjectiveAlthough the choice of these approximations is critical for the performance of the prediction models, there is no empirical evidence on whether LOC is actually a good approximation. Therefore, in this paper, we investigate the question: is LOC a good measure of effort for use in effort-aware models?MethodWe perform an empirical study on four open source projects, for which we obtain explicitly-recorded effort data, and compare the use of LOC to various complexity, size and churn metrics as measures of effort.ResultsWe find that using a combination of complexity, size and churn metrics are a better measure of effort than using LOC alone. Furthermore, we examine the impact of our findings on previous effort-aware bug prediction work and find that using LOC as a measure for effort does not significantly affect the list of files being flagged, however, using LOC under-estimates the amount of effort required compared to our best effort predictor by approximately 66%.ConclusionStudies using effort-aware models should not assume that LOC is a good measure of effort. For the case of effort-aware bug prediction, using LOC provides results that are similar to combining complexity, churn, size and LOC as a proxy for effort when prioritizing the most risky files. However, we find that for the purpose of effort-estimation, using LOC may under-estimate the amount of effort required. 
55|11||Efficient software clustering technique using an adaptive and preventive dendrogram cutting approach|ContextSoftware clustering is a key technique that is used in reverse engineering to recover a high-level abstraction of the software in the case of limited resources. Very limited research has explicitly discussed the problem of finding the optimum set of clusters in the design and how to penalize for the formation of singleton clusters during clustering.ObjectiveThis paper attempts to enhance the existing agglomerative clustering algorithms by introducing a complementary mechanism. To solve the architecture recovery problem, the proposed approach focuses on minimizing redundant effort and penalizing for the formation of singleton clusters during clustering while maintaining the integrity of the results.MethodAn automated solution for cutting a dendrogram that is based on least-squares regression is presented in order to find the best cut level. A dendrogram is a tree diagram that shows the taxonomic relationships of clusters of software entities. Moreover, a factor to penalize clusters that will form singletons is introduced in this paper. Simulations were performed on two open-source projects. The proposed approach was compared against the exhaustive and highest gap dendrogram cutting methods, as well as two well-known cluster validity indices, namely, Dunn’s index and the Davies-Bouldin index.ResultsWhen comparing our clustering results against the original package diagram, our approach achieved an average accuracy rate of 90.07% from two simulations after the utility classes were removed. The utility classes in the source code affect the accuracy of the software clustering, owing to its omnipresent behavior. The proposed approach also successfully penalized the formation of singleton clusters during clustering.ConclusionThe evaluation indicates that the proposed approach can enhance the quality of the clustering results by guiding software maintainers through the cutting point selection process. The proposed approach can be used as a complementary mechanism to improve the effectiveness of existing clustering algorithms. 
55|11||Defining a test coverage criterion for model-level testing of FBD programs|ContextThe Programmable Logic Controller (PLC) is being integrated into the automation and control of computer systems in safety–critical domains at an increasing rate. Thoroughly testing such software to ensure safety is crucial. Function Block Diagram (FBD) is a popular data-flow programming language for PLC. Current practice often involves translating an FBD program into an equivalent C program for testing. Little research has been conducted on coverage of direct testing a data-flow program, such as an FBD program, at the model level. There are no commonly accepted structural test coverage criteria for data-flow programs. The objective of this study is to develop effective structural test coverage criterion for testing model-level FBD programs. The proposed testing scheme can be used to detect mutation errors at the logical function level.ObjectiveThe purpose of this study is to design a new test coverage criterion that can directly test FBD programs and effectively detect logical function mutation errors.MethodA complete test set for each function and function block in an FBD program are defined. Moreover, this method augments the data-flow path concept with a sensitivity check to avoid fault masking and effectively detect logical function mutation errors.ResultsPreliminary experiments show that this test coverage criterion is comprehensive and effective for error detection.ConclusionThe proposed coverage criterion is general and can be applied to real cases to improve the quality of data-flow program design. 
55|11||Object-oriented class maintainability prediction using internal quality attributes|ContextClass maintainability is the likelihood that a class can be easily modified. Before releasing an object-oriented software system, it is impossible to know with certainty when, where, how, and how often a class will be modified. At that stage, this likelihood can be estimated using the internal quality attributes of a class, which include cohesion, coupling, and size. To reduce the future class maintenance efforts and cost, developers are encouraged to carefully test and well document low maintainability classes before releasing the object-oriented system.ObjectiveWe empirically study the relationship between internal class quality attributes (size, cohesion, and coupling) and an external quality attribute (class maintainability). Using statistical techniques, we also construct models based on the selected internal attributes to predict class maintainability.MethodWe consider classes of three open-source systems. For each class, we account for two actual maintainability indicators, the number of revised lines of code and the number of revisions in which the class was involved. Using 19 internal quality measures, we empirically explore the impact of size, cohesion, and coupling on class maintainability. We also empirically investigate the abilities of the measures, considered both individually and combined, to estimate class maintainability. Statistically based prediction models are constructed and validated.ResultsOur results demonstrate that classes with better qualities (i.e., higher cohesion values and lower size and coupling values) have better maintainability (i.e., are more likely to be easily modified) than those of worse qualities. Most of the considered measures are shown to be predictors of the considered maintainability indicators to some degree. The abilities of the considered internal quality measures to predict class maintainability are improved when the measures are combined using optimized multivariate statistical models.ConclusionThe prediction models can help software engineers locate classes with low maintainability. These classes must be carefully tested and well documented. 
55|12|http://www.sciencedirect.com/science/journal/09505849/55/12|A systematic review of systematic review process research in software engineering|ContextMany researchers adopting systematic reviews (SRs) have also published papers discussing problems with the SR methodology and suggestions for improving it. Since guidelines for SRs in software engineering (SE) were last updated in 2007, we believe it is time to investigate whether the guidelines need to be amended in the light of recent research.ObjectiveTo identify, evaluate and synthesize research published by software engineering researchers concerning their experiences of performing SRs and their proposals for improving the SR process.MethodWe undertook a systematic review of papers reporting experiences of undertaking SRs and/or discussing techniques that could be used to improve the SR process. Studies were classified with respect to the stage in the SR process they addressed, whether they related to education or problems faced by novices and whether they proposed the use of textual analysis tools.ResultsWe identified 68 papers reporting 63 unique studies published in SE conferences and journals between 2005 and mid-2012. The most common criticisms of SRs were that they take a long time, that SE digital libraries are not appropriate for broad literature searches and that assessing the quality of empirical studies of different types is difficult.ConclusionWe recommend removing advice to use structured questions to construct search strings and including advice to use a quasi-gold standard based on a limited manual search to assist the construction of search stings and evaluation of the search process. Textual analysis tools are likely to be useful for inclusion/exclusion decisions and search string construction but require more stringent evaluation. SE researchers would benefit from tools to manage the SR process but existing tools need independent validation. Quality assessment of studies using a variety of empirical methods remains a major problem. 
55|12||Mutation-oriented test data augmentation for GUI software fault localization|ContextFault localization lies at the heart of program debugging and often proceeds by contrasting the statistics of program constructs executed by passing and failing test cases. A vital issue here is how to obtain these “suitable” test cases. Techniques presented in the literature mostly assume the existence of a large test suite a priori. However, developers often encounter situations where a failure occurs, but where no or no appropriate test suite is available for use to localize the fault.ObjectiveThis paper aims to alleviate this key limitation of traditional fault localization techniques for GUI software particularly, namely, it aims at enabling cost-effective fault localization process for GUI software in the described scenario.MethodTo address this scenario, we propose a mutation-oriented test data augmentation technique, which actually is directed by the “similarity” criterion in GUI software’s test case context towards the generation of test suite with excellent fault localization capabilities. More specifically, the technique mainly uses four proposed novel mutation operators to iteratively mutate some failing GUI test cases’ event sequences to derive new test cases potentially useful to localize the specific encountered fault. We then compare the fault localization performance of the test suite generated using this technique with that of an original provided large event-pair adequate test suite on some GUI applications.ResultsThe results indicate that the proposed technique is capable of generating a test suite that has comparable, if not better, fault localization effectiveness to the event-pair adequate test suite, but it is much smaller and it is generated immediately once a failure is encountered by developers.ConclusionIt is concluded that the proposed technique can truly enable quick-start cost-effective fault localization process under the investigated all-too-common scenario, greatly alleviating one key limitation of traditional fault localization techniques and prompting the test–diagnose–repair cycle. 
55|12||Ontological and linguistic metamodelling revisited: A language use approach|ContextAlthough metamodelling is generally accepted as important for our understanding of software and systems development, arguments about the validity and utility of ontological versus linguistic metamodelling continue.ObjectiveThe paper examines the traditional, metamodel-focused construction of modelling languages in the context of language use, and particularly speech act theory. These concepts are then applied to the problems introduced by the “Orthogonal Classification Architecture” that is often called the ontological/linguistic paradox. The aim of the paper is to show how it is possible to overcome these problems.MethodThe paper adopts a conceptual–analytical approach by revisiting the published arguments and developing an alternative metamodelling architecture based on language use.ResultsThe analysis shows that when we apply a language use perspective of meaning to traditional modelling concepts, a number of incongruities and misconceptions in the traditional approaches are revealed – issues that are not evident in previous work based primarily on set theory. Clearly differentiating between the extensional and intensional aspects of class concepts (as sets) and also between objects (in the social world) and things (in the physical world) allows for a deeper understanding to be gained of the relationship between the ontological and linguistic views promulgated in the modelling world.ConclusionsWe propose that a viewpoint that integrates language use ideas into traditional modelling (and metamodelling) is vital, and stress that meaning is not inherent in the physical world; meaning, and thus socially valid objects, are constructed by use of language, which may or may not establish a one-to-one correspondence relationship between objects and physical things. 
55|12||Estimating software testing complexity|ContextComplexity measures provide us some information about software artifacts. A measure of the difficulty of testing a piece of code could be very useful to take control about the test phase.ObjectiveThe aim in this paper is the definition of a new measure of the difficulty for a computer to generate test cases, we call it Branch Coverage Expectation (BCE). We also analyze the most common complexity measures and the most important features of a program. With this analysis we are trying to discover whether there exists a relationship between them and the code coverage of an automatically generated test suite.MethodThe definition of this measure is based on a Markov model of the program. This model is used not only to compute the BCE, but also to provide an estimation of the number of test cases needed to reach a given coverage level in the program. In order to check our proposal, we perform a theoretical validation and we carry out an empirical validation study using 2600 test programs.ResultsThe results show that the previously existing measures are not so useful to estimate the difficulty of testing a program, because they are not highly correlated with the code coverage. Our proposed measure is much more correlated with the code coverage than the existing complexity measures.ConclusionThe high correlation of our measure with the code coverage suggests that the BCE measure is a very promising way of measuring the difficulty to automatically test a program. Our proposed measure is useful for predicting the behavior of an automatic test case generator. 
55|12||BPELDebugger: An effective BPEL-specific fault localization framework|ContextBusiness Process Execution Language (BPEL) is a widely recognized executable service composition language, which is significantly different from typical programming languages in both syntax and semantics, and especially shorter in program scale. How to effectively locate faults in BPEL programs is an open and challenging problem.ObjectiveIn this paper, we propose a fault localization framework for BPEL programs.MethodBased on BPEL program characteristics, we propose two fault localization guidelines to locate the integration and interaction faults in BPEL programs. Our framework formulates the BPEL fault localization problem using the popular fault localization problem settings, and synthesizes BPEL-specific fault localization techniques by reuse of existing fault localization formulas. We use two realistic BPEL programs and three existing fault localization formulas to evaluate the feasibility and effectiveness of the proposed fault localization framework and guidelines.ResultExperiment results show that faults can be located with the fewest code examining efforts. That is, the fault-relevant basic block is assigned the highest suspiciousness score by our fault localization method. The experiment results also show that with the use of the proposed fault localization guidelines, the code examining efforts to locate faults are extraordinarily reduced.ConclusionWe conclude that the proposed framework is feasible in synthesizing effective fault localization techniques, and our fault localization guidelines are very effective to enhance existing fault localization techniques in locating faults in BPEL programs. 
55|12||When agile meets the enterprise|ContextWhile renowned agile methods like XP and Scrum were initially intended for projects with small teams, traditional enterprise environments, i.e. environments where plan-driven development is prevalent, have also become attracted by the promises of a faster time to market through agility. Agile software development methods emphasize lightweight software development. Projects within enterprise environments, however, are typically confronted with a large and complex IT landscape, where mission-critical information is at play whose criticality requires prudence regarding design and development. In many an organization, both approaches are used simultaneously.ObjectiveFind out which challenges the co-existence of agile methods and plan-driven development brings, and how organizations deal with those challenges.MethodWe present a grounded theory of the challenges of using agile methods in traditional enterprise environments, based on a Grounded Theory research involving 21 agile practitioners from two large enterprise organizations in the Netherlands.ResultsWe organized the challenges under two factors: Increased landscape complexity and Lack of business involvement. For both factors, we identify successful mitigation strategies. These mitigation strategies concern the communication between the agile and traditional part of the organization, and the timing of that communication.ConclusionAgile practices can coexist with plan-driven development. One should, however, keep in mind the context and take actions to mitigate the challenges incurred. 
55|12||A metric towards evaluating understandability of state machines: An empirical study|ContextState machines are widely used to describe the dynamic behavior of objects, components, and systems. As a communication tool between various stakeholders, it is essential that state machines be easily and correctly comprehensible. Poorly understood state machines can lead to misunderstandings and communication overhead, thus adversely affecting the quality of the final product. Nevertheless, there is a lack of measurement research for state machines.ObjectiveIn this paper, we propose a metric, called SUM, to evaluate the understandability of state machines. SUM is defined on the basis of cohesion and coupling concepts.MethodTo validate SUM as a state machine understandability indicator, we performed an empirical study using five systems. We constructed five different state machines for each system, resulting in a total of 25 state machines being prepared. Two aspects of understandability, efficiency (UEff) and correctness (UCor), were obtained from 40 participants for the state machines. We then performed correlation and consistency analyses between the SUMs and the measured understandability values.ResultsThe results of the correlation analysis indicated that SUM was significantly correlated with UEff (p = 0.003) and UCor (p = 0.027). The consistency analysis results indicated that SUM was positively correlated with UEff in four of the systems and UCor in all five systems.ConclusionThese results confirm the possibility that SUM can be a useful understandability indicator for SMs. We believe that the proposed metric can be used as a guideline to construct quality state machines. 
55|12||A formal approach for run-time verification of web applications using scope-extended LTL|ContextIn the past decade, the World Wide Web has been subject to rapid changes. Web sites have evolved from static information pages to dynamic and service-oriented applications that are used for a broad range of activities on a daily basis. For this reason, thorough analysis and verification of Web Applications help assure the deployment of high quality applications.ObjectivesIn this paper, an approach is presented to the formal verification and validation of existing web applications. The approach consists of using execution traces of a web application to automatically generate a communicating automata model. The obtained model is used to model checking the application against predefined properties, to perform regression testing, and for documentation.MethodsTraces used in the proposed approach are collected by monitoring a web application while it is explored by a user or a program. An automata-based model is derived from the collected traces by mapping the pages of the application under test into states and the links and forms used to browse the application into transitions between the states. Properties, meanwhile, express correctness and quality requirements on web applications and might concern all states of the model; in many cases, these properties concern only a proper subset of the states, in which case the model is refined to designate the subset of the global states of interest. A related problem of property specification in Linear Temporal Logic (LTL) over only a subset of states of a system is solved by means of specialized operators that facilitate specifying properties over propositional scopes in a concise and intuitive way. Each scope constitutes a subset of states that satisfy a propositional logic formula.ResultsAn implementation of the verification approach that uses the model checker Spin is presented where an integrated toolset is developed and empirical results are shown. Also, Linear Temporal Logic is extended with propositional scopes.Conclusiona formal approach is developed to build a finite automata model tuned to features of web applications that have to be validated, while delegating the task of property verification to an existing model checker. Also, the problem of property specification in LTL over a subset of the states of a given system is addressed, and a generic and practical solution is proposed which does not require any changes in the system model by defining specialized operators in LTL using scopes. 
55|12||Comparison and integration of genetic algorithms and dynamic symbolic execution for security testing of cross-site scripting vulnerabilities|ContextCross-site scripting (XSS for short) is considered one of the major threat to the security of web applications. Static analysis supports manual security review in mitigating the impact of XSS-related issues, by suggesting a set of potential problems, expressed in terms of candidate vulnerabilities. A security problem spotted by static analysis, however, consists of a list of (possibly complicated) conditions that should be satisfied to concretely exploit a vulnerability. Static analysis, instead, does not provide examples of what input values must be used to make the application execute the (sometimes complex) execution path that causes a XSS vulnerability. Runnable test cases, however, consist of an executable and reproducible evidence of the vulnerability mechanics. Test cases represent a valuable support for developers who should concretely understand security problems in detail before fixing them.ObjectiveThis paper evaluates various strategies to automatically generate security test cases, i.e. test cases that expose a vulnerability by making the application control flow satisfy vulnerability conditions.MethodA combination of genetic algorithms and concrete symbolic execution is presented for the automatic generation of security test cases. This combined strategy is compared with genetic algorithms and with concrete symbolic execution alone, in terms of coverage and productivity on four case study web applications.ResultWhile genetic algorithms require less time to generate security test cases, those generated by concrete symbolic execution cover a higher number of vulnerabilities. The highest coverage, however, is achieved when the two approaches are combined and integrated.ConclusionThe integrated approach that we propose has shown to be effective for security testing. In fact, genetic algorithms have shown to be able to generate test cases only for few and simple vulnerabilities when not combined with other approaches. However, their contribution is fundamental to improve the coverage of test cases generated by concrete symbolic execution. 
55|12||To what extent can maintenance problems be predicted by code smell detection? â An empirical study|ContextCode smells are indicators of poor coding and design choices that can cause problems during software maintenance and evolution.ObjectiveThis study is aimed at a detailed investigation to which extent problems in maintenance projects can be predicted by the detection of currently known code smells.MethodA multiple case study was conducted, in which the problems faced by six developers working on four different Java systems were registered on a daily basis, for a period up to four weeks. Where applicable, the files associated to the problems were registered. Code smells were detected in the pre-maintenance version of the systems, using the tools Borland Together and InCode. In-depth examination of quantitative and qualitative data was conducted to determine if the observed problems could be explained by the detected smells.ResultsFrom the total set of problems, roughly 30% percent were related to files containing code smells. In addition, interaction effects were observed amongst code smells, and between code smells and other code characteristics, and these effects led to severe problems during maintenance. Code smell interactions were observed between collocated smells (i.e., in the same file), and between coupled smells (i.e., spread over multiple files that were coupled).ConclusionsThe role of code smells on the overall system maintainability is relatively minor, thus complementary approaches are needed to achieve more comprehensive assessments of maintainability. Moreover, to improve the explanatory power of code smells, interaction effects amongst collocated smells and coupled smells should be taken into account during analysis. 
55|2|http://www.sciencedirect.com/science/journal/09505849/55/2|MDD vs. traditional software development: A practitionerâs subjective perspective|ContextToday’s project managers have a myriad of methods to choose from for the development of software applications. However, they lack empirical data about the character of these methods in terms of usefulness, ease of use or compatibility, all of these being relevant variables to assess the developer’s intention to use them.ObjectiveTo compare three methods, each following a different paradigm (Model-Driven, Model-Based and Code-Centric) with respect to their adoption potential by junior software developers engaged in the development of the business layer of a Web 2.0 application.MethodWe have conducted a quasi-experiment with 26 graduate students of the University of Alicante. The application developed was a Social Network, which was organized around a fixed set of modules. Three of them, similar in complexity, were used for the experiment. Subjects were asked to use a different method for each module, and then to answer a questionnaire that gathered their perceptions during such use.ResultsThe results show that the Model-Driven method is regarded as the most useful, although it is also considered the least compatible with previous developers’ experiences. They also show that junior software developers feel comfortable with the use of models, and that they are likely to use them if the models are accompanied by a Model-Driven development environment.ConclusionsDespite their relatively low level of compatibility, Model-Driven development methods seem to show a great potential for adoption. That said, however, further experimentation is needed to make it possible to generalize the results to a different population, different methods, other languages and tools, different domains or different application sizes. 
55|2||Action-based discovery of satisfying subsets: A distributed method for model correction|ContextUnderstanding and resolving counterexamples in model checking is a difficult task that often takes a significant amount of resources and many rounds of regression model checking after any fix. As such, it is desirable to have algorithmic methods that correct finite-state models when their model checking for a specific property fails without undermining the correctness of the rest of the properties, called the model correction problem.ObjectiveThe objective of this paper is to mitigate time and space complexity of correction.MethodTo achieve the objective, this paper presents a distributed method that solves the model correction problem using the concept of satisfying subsets, where a satisfying subset is a subset of model computations that meets a new property while preserving existing properties. The proposed method automates the elimination of superfluous non-determinism in models of concurrent computing systems, thereby generating models that are correct by construction.ResultsWe have implemented the proposed method in a distributed software tool, called the Model Corrector (ModCor). Due to the distributed nature of the correction algorithms, ModCor exploits the processing power of computer clusters to mitigate the space and time complexity of correction. Our experimental results are promising as we have used a small cluster of five regular PCs to automatically correct large models (with about 3159 reachable states) in a few hours. Such corrections would have been impossible without using ModCor.ConclusionsThe results of this paper illustrate that partitioning finite-state models based on their transition relations and distributing them across a computer cluster facilitates the automated correction of models when their model checking fails. 
55|2||A process for managing interaction between experimenters to get useful similar replications|ContextA replication is the repetition of an experiment. Several efforts have been made to adopt replication as a common practice in software engineering. There are different types of replications, depending on their purpose. Similar replications keep the experimental conditions as alike as possible to the original ones. External similar replications, where the replicating experimenters are not the same people as the original experimenters, have been a stumbling block. Several attempts at combining the results of replications have resulted in failure. Software engineering does not appear to be well suited to such replications, because it works with complex experimentally immature contexts. Software engineering settings have a large number of variables, and the role that many of them play is unknown. A successful (or useful) similar replication helps to better understand the phenomenon under study by verifying results and/or identifying contextual variables that could influence (or not) the results, through the combination of experimental results.ObjectiveTo be able to get successful similar replications, there needs to be interaction between original and replicating experimenters. In this paper, we propose an interaction process for achieving successful similar replications.MethodThis process consists of: an adaptation meeting, where experimenters tailor the experiment to the new setting; querying, to settle occasional inquiries while the experiment is being run; and a combination meeting, where experimenters meet to discuss the combination of replication outcomes with previous results. To check its effectiveness, the process has been tested on three different replications of the same experiment.ResultsThe proposed interaction process has helped to identify new contextual variables that could potentially influence (or not) the experimental results in the three replications run. Additionally, the interaction process has helped to uncover certain problems and deviations that occurred during some of the replications that we would have not been aware of otherwise.ConclusionsThere are signs that suggest that it is possible to get successful similar replications in software engineering experimentation, when there is appropriate interaction among experimenters. 
55|2||How influential has academic and industrial research been in current software life cycles? A retrospective analysis of four mainstream activities|ContextKnowledge transfer is an important responsibility of universities and research institutes as part of their contribution to society. In the field of software engineering, several studies have been performed to show the influence of research in popular technologies such as middleware systems. However, there is no scholarly analysis of the influence that research has had in mainstream activities of current software life cycles.ObjectiveWe analyse how methodological research has influenced activities of widespread use in current software life cycles. To keep this goal into manageable bounds, we focus on four very successful trends of current practice: iterative development, architecture-centric development, requirements-driven development, and coherent method integration.MethodWe follow different forms of evidence backwards in time. As signs of influence we admit the following categories: citations included in papers and standards, interviews, historical essays, people movement, and acquisitions of companies.ResultsFor each one of the mentioned activities, we obtain a trace diagram showing the indirect influence that pieces of research have had in the selected activities of software life cycles.ConclusionsOur results support the following claims: (1) mainstream dissemination of the analysed methodological research has taken on the order of 20–40 years; (2) interdisciplinarity has been important in the research that influenced some very popular activities of current software life cycles; (3) research on life cycles is more influential when it originates from large development projects; and (4) probably the best results can be obtained if industrial research goes hand in hand with academic research. 
55|2||Probabilistic size proxy for software effort prediction: A framework|Software effort prediction is an important and challenging activity that takes place during the early stages of software development, where costing is needed. Software size estimate is one of the most popular inputs for software effort prediction models. Accordingly, providing a size estimate with good accuracy early in the lifecycle is very important; it is equally challenging too. Estimates that are computed early in the development lifecycle, when it is needed the most, are typically associated with uncertainty. However, none of the prominent software effort prediction techniques or software size metrics addresses this issue satisfactorily. In this paper, we propose a framework for developing probabilistic size proxies for software effort prediction using information from conceptual UML models created early in the software development lifecycle. The framework accounts for uncertainty in software size and effort prediction by providing the estimate as a probability density function instead of a certain value. We conducted a case study using open source datasets and the results were encouraging. 
55|2||Usage and testability of AOP: An empirical study of AspectJ|ContextBack in 2001, the MIT announced aspect-oriented programming as a key technology in the next 10 years. Nowadays, 10 years later, AOP is still not widely adopted.ObjectiveThe objective of this work is to understand the current status of AOP practice through the analysis of open-source project which use AspectJ.MethodFirst we analyze different dimensions of AOP usage in 38 AspectJ projects. We investigate the degree of coupling between aspects and base programs, and the usage of the pointcut description language. A second part of our study focuses on testability as an indicator of maintainability. We also compare testability metrics on Java and AspectJ implementations of the HealthWatcher aspect-oriented benchmark.ResultsThe first part of the analysis reveals that the number of aspects does not increase with the size of the base program, that most aspects are woven in every places in the base program and that only a small portion of the pointcut language is used. The second part about testability reveals that AspectJ reduces the size of modules, increases their cohesion but also increases global coupling, thus introducing a negative impact on testability.ConclusionThese observations and measures reveal a major trend: AOP is currently used in a very cautious way. This cautious usage could come from a partial failure of AspectJ to deliver all promises of AOP, in particular an increased software maintainability. 
55|2||Equality in cumulative voting: A systematic review with an improvement proposal|ContextPrioritization is an essential part of requirements engineering, software release planning and many other software engineering disciplines. Cumulative Voting (CV) is known as a relatively simple method for prioritizing requirements on a ratio scale. Historically, CV has been applied in decision-making in government elections, corporate governance, and forestry. However, CV prioritization results are of a special type of data—compositional data.ObjectivesThe purpose of this study is to aid decision-making by collecting knowledge on the empirical use of CV and develop a method for detecting prioritization items with equal priority.MethodsWe present a systematic literature review of CV and CV analysis methods. The review is based on searching electronic databases and snowball sampling of the found primary studies. Relevant studies are selected based on titles, abstracts, and full text inspection. Additionally, we propose Equality of Cumulative Votes (ECVs)—a CV result analysis method that identifies prioritization items with equal priority.ResultsCV has been used in not only requirements prioritization and release planning but also in e.g. software process improvement, change impact analysis and model driven software development. The review presents a collection of state of the practice studies and CV result analysis methods. In the end, ECV was applied to 27 prioritization cases from 14 studies and identified nine groups of equal items in three studies.ConclusionsWe believe that the analysis of the collected studies and the CV result analysis methods can help in the adoption of CV prioritization method. The evaluation of ECV indicates that it is able to detect prioritization items with equal priority and thus provide the practitioner with a more fine-grained analysis. 
55|2||Enhancing software reliability estimates using modified adaptive testing|ContextMost software reliability models are based on a binary notion of correctness, i.e. “successful” or “failed.” However, in several instances, it is important to account of failure severity to obtain more descriptive and accurate estimates of the reliability of the software.ObjectiveIn this paper, we develop a set of extended metrics based on the Nelson’s software reliability model to account for information gained from a user’s point of view regarding the severity of the observed failures. Model formulation based on multi-granularity failure severity is provided, and the proposed metrics are proved to be backward compatible.MethodIn order to estimate the software reliability through testing, an extended adaptive testing strategy, namely Modified Adaptive Testing (MAT) is proposed. The use of test history information allows the resulting test process to be adaptive in the selection of tests under limited test budget. Simulations and experiments on real-life programs are conducted to evaluate the effectiveness of MAT.ResultsData show that the reliability estimates obtained using MAT (a) are closer to the “true” reliability than those obtained using random testing and (b) lead to lower variance than the techniques used for comparison, which means MAT can be applied to help testers and reliability engineers better understand the reliability of their programs.ConclusionIt is concluded that the proposed approach can enhance the software reliability estimation testing by guiding the test case selection process by providing more descriptive and accurate results. 
55|2||Automated generation of test oracles using a model-driven approach|ContextSoftware development time has been reduced with new development tools and paradigms, testing must accompany these changes. In order to release software products in a timely manner as well as to minimise the impact of possible errors introduced during maintenance interventions, testing automation has become a central goal. Whilst research has produced significant results in test case generation and tools for test case (re)-execution, one of the most important open problems in testing is the automation of oracle generation. The oracle decides whether the program under test has or has not behaved correctly and then issues a pass/fail verdict. In most cases, writing the oracle is a time-consuming activity that, moreover, is manual in most cases.ObjectiveThis article automates two important steps in the test oracle: obtention of expected output and its comparison with the actual output, using a model-driven approach.MethodThe oracle automation problem is resolved using a model-driven framework, based on OMG standards: UML is used as metamodel and QVT and MOF2Text as transformation languages. The automated testing framework takes the models that describe the system as input, using UML notation and derives from them the test model and then the test code, following a model-driven approach. Test oracle procedures are obtained from a UML state machine.ResultsA complete executable test case at functional test level is obtained, composed of a test procedure with parametrized input test data and expected result automation.ConclusionThe oracle automation is obtained using a model-driven approach, test cases are obtained automatically from UML models. The model-driven testing framework was applied to an industrial application and has been useful to testing automation for the main functionalities in the system. 
55|2||Variability in quality attributes of service-based software systems: A systematic literature review|ContextVariability is the ability of a software artifact (e.g., a system, component) to be adapted for a specific context, in a preplanned manner. Variability not only affects functionality, but also quality attributes (e.g., security, performance). Service-based software systems consider variability in functionality implicitly by dynamic service composition. However, variability in quality attributes of service-based systems seems insufficiently addressed in current design practices.ObjectiveWe aim at (a) assessing methods for handling variability in quality attributes of service-based systems, (b) collecting evidence about current research that suggests implications for practice, and (c) identifying open problems and areas for improvement.MethodA systematic literature review with an automated search was conducted. The review included studies published between the year 2000 and 2011. We identified 46 relevant studies.ResultsCurrent methods focus on a few quality attributes, in particular performance and availability. Also, most methods use formal techniques. Furthermore, current studies do not provide enough evidence for practitioners to adopt proposed approaches. So far, variability in quality attributes has mainly been studied in laboratory settings rather than in industrial environments.ConclusionsThe product line domain as the domain that traditionally deals with variability has only little impact on handling variability in quality attributes. The lack of tool support, the lack of practical research and evidence for the applicability of approaches to handle variability are obstacles for practitioners to adopt methods. Therefore, we suggest studies in industry (e.g., surveys) to collect data on how practitioners handle variability of quality attributes in service-based systems. For example, results of our study help formulate hypotheses and questions for such surveys. Based on needs in practice, new approaches can be proposed. 
55|2||Systematic scenario test case generation for nuclear safety systems|ContextThe current validation tests for nuclear software are routinely performed by random testing, which leads to uncertain test coverage. Moreover, validation tests should directly verify the system’s compliance with the original user’s needs. Unlike current model-based testing methods, which are generally based on requirements or design models, the proposed model is derived from the original user’s needs in text through domain-specific ontology, and then used to generate validation tests systematically.ObjectiveOur first goal is to develop an objective, repeatable, and efficient systematic validation test scheme that is effective for large systems, with analyzable test coverage. Our second goal is to provide a new model-based validation testing method that reflects the user’s original safety needs.MethodA model-based scenario test case generation for nuclear digital safety systems was designed. This was achieved by converting the scenarios described in natural language in a Safety Analysis Report (SAR) prepared by the power company for licensing review, to Unified Modeling Language (UML) sequence diagrams based on a proposed ontology of a related regulatory standard. Next, we extracted the initial environmental parameters and the described operational sequences. We then performed variations on these data to systematically generate a sufficient number of scenario test cases.ResultsTest coverage criteria, which are the equivalence partition coverage of initial environment, the condition coverage, the action coverage and the scenario coverage, were met using our method.ConclusionThe proposed model-based scenario testing can provide improved testing coverage than random testing. A test suite based on user needs can be provided. 
55|2||System integration by developing adapters using a database abstraction|ContextLarge software systems are usually developed by integrating several smaller systems, which may have been developed independently. The integration of such systems often requires the development of a custom adapter (sometimes called mediator or glue logic) for bridging any technical incompatibilities between the systems.Adapter development often focuses on how to respond to events from the external interfaces, e.g., by applying data conversions and performing events on (other) external interfaces. Such an operational focus is closely related to an implementation of the adapter, but it makes it complicated to reason about complex adapters. For example, it requires a lot of effort to understand the relation that the adapter establishes between the systems to be integrated, and to avoid any internal inconsistencies.ObjectiveThis article investigates a way to develop adapters in terms of a more abstract, model-based specification. Experts from the application domain should be able to reason about the specification, and the specification should contain enough details to generate an implementation.MethodBased on a few industrial adapters from the domain of Maritime Safety and Security, we study ways to specify them conveniently, and ways to generate them efficiently. On this basis, we identify an approach for developing adapters. In turn, this approach is validated using an additional set of adapters.ResultsAfter abstracting from the details of the interface technologies, the studied adapters could be generated using techniques for incremental view maintenance. This requires an adapter specification in terms of database views to relate the main semantic concepts in the application domain.ConclusionFor developing adapters, it can be useful to model all interface technologies as database operations. Thus adapters can be specified using database views, which improve the conceptual understanding of the adapters. Publish/subscribe-style adapters can then be generated using incremental view maintenance. 
55|2||A visual token-based formalization of BPMN 2.0 based on in-place transformations|ContextThe Business Process Model and Notation (BPMN) standard informally defines a precise execution semantics. It defines how process instances should be updated in a model during execution. Existing formalizations of the standard are incomplete and rely on mappings to other languages.ObjectiveThis paper provides a BPMN 2.0 semantics formalization that is more complete and intuitive than existing formalizations.MethodThe formalization consists of in-place graph transformation rules that are documented visually using BPMN syntax. In-place transformations update models directly and do not require mappings to other languages. We have used a mature tool and test-suite to develop a reference implementation of all rules.ResultsOur formalization is a promising complement to the standard, in particular because all rules have been extensively verified and because conceptual validation is facilitated (the informal semantics also describes in-place updates).ConclusionSince our formalization has already revealed problems with the standard and since the BPMN is still evolving, the maintainers of the standard can benefit from our results. Moreover, tool vendors can use our formalization and reference implementation for verifying conformance to the standard. 
55|2||Aspect-oriented model-driven code generation: A systematic mapping study|ContextModel-driven code generation is being increasingly applied to enhance software development from perspectives of maintainability, extensibility and reusability. However, aspect-oriented code generation from models is an area that is currently underdeveloped.ObjectiveIn this study we provide a survey of existing research on aspect-oriented modeling and code generation to discover current work and identify needs for future research.MethodA systematic mapping study was performed to find relevant studies. Classification schemes have been defined and the 65 selected primary studies have been classified on the basis of research focus, contribution type and research type.ResultsThe papers of solution proposal research type are in a majority. All together aspect-oriented modeling appears being the most focused area divided into modeling notations and process (36%) and model composition and interaction management (26%). The majority of contributions are methods.ConclusionAspect-oriented modeling and composition mechanisms have been significantly discussed in existing literature while more research is needed in the area of model-driven code generation. Furthermore, we have observed that previous research has frequently focused on proposing solutions and thus there is need for research that validates and evaluates the existing proposals in order to provide firm foundations for aspect-oriented model-driven code generation. 
55|2||Interpretative case studies on agile team productivity and management|ContextThe management of software development productivity is a key issue in software organizations, where the major drivers are lower cost and shorter time-to-market. Agile methods, including Extreme Programming and Scrum, have evolved as “light” approaches that simplify the software development process, potentially leading to increased team productivity. However, little empirical research has examined which factors do have an impact on productivity and in what way, when using agile methods.ObjectiveOur objective is to provide a better understanding of the factors and mediators that impact agile team productivity.MethodWe have conducted a multiple-case study for 6 months in three large Brazilian companies that have been using agile methods for over 2 years. We have focused on the main productivity factors perceived by team members through interviews, documentation from retrospectives, and non-participant observation.ResultsWe developed a novel conceptual framework, using thematic analysis to understand the possible mechanisms behind such productivity factors. Agile team management was found to be the most influential factor in achieving agile team productivity. At the intra-team level, the main productivity factors were team design (structure and work allocation) and member turnover. At the inter-team level, the main productivity factors were how well teams could be effectively coordinated by proper interfaces and other dependencies and avoiding delays in providing promised software to dependent teams.ConclusionTeams should be aware of the influence and magnitude of turnover, which has been shown negative for agile team productivity. Team design choices remain an important factor impacting team productivity, even more pronounced on agile teams that rely on teamwork and people factors. The intra-team coordination processes must be adjusted to enable productive work by considering priorities and pace between teams. Finally, the revised conceptual framework for agile team productivity supports further tests through confirmatory studies. 
55|2||Constraints for the design of variability-intensive service-oriented reference architectures â An industrial case study|ContextService-oriented architecture has become a widely used concept in software industry. However, we currently lack support for designing variability-intensive service-oriented systems. Such systems could be used in different environments, without the need to design them from scratch. To support the design of variability-intensive service-oriented systems, reference architectures that facilitate variability in instantiated service-oriented architectures can help.ObjectiveThe design of variability-intensive service-oriented reference architectures is subject to specific constraints. Architects need to know these constraints when designing such reference architectures. Our objective is to identify these constraints.MethodAn exploratory case study was performed in the context of local e-government in the Netherlands to study constraints from the perspective of (a) the users of a variability-intensive service-oriented system (municipalities that implement national laws), and (b) the implementing organizations (software vendors). We collected data through interviews with representatives from five organizations, document analyses and expert meetings.ResultsWe identified ten constraints (e.g., organizational constraints, integration-related constraints) which affect the process of designing reference architectures for variability-intensive service-oriented systems. Also, we identified how stakeholders are affected by these constraints, and how constraints are specific to the case study domain.ConclusionsOur results help design variability-intensive service-oriented reference architectures. Furthermore, our results can be used to define processes to design such reference architectures. 
55|2||Guest Editorial for Special Section from Component-based Software Engineering (CBSE) 2011|
55|2||Testing component compatibility in evolving configurations|Software components are increasingly assembled from other components. Each component may further depend on others, and each may have multiple active versions. The total number of configurations—combinations of components and their versions—in use can be very large. Moreover, components are constantly being enhanced and new versions are being released. Component developers, therefore, spend considerable time and effort doing compatibility testing—determining whether their components can be built correctly for all deployed configurations–both for existing active component versions and new releases. In previous work we developed Rachet, a distributed, cache-aware mechanism to support large-scale compatibility testing of component-based software with a fixed set of component versions.In this paper, we observe that it is too expensive to perform compatibility testing from scratch each time a new version of a component is released. We thus add a new dimension to Rachet: to perform incremental and prioritized compatibility testing. We describe algorithms to compute differences in component compatibilities between current and previous component builds, a formal test adequacy criterion based on covering the differences, and cache-aware configuration sampling and testing methods that attempt to reuse effort from previous testing sessions. Because testers are often interested in focusing test effort on newly released and modified components and their versions, we have developed a prioritization mechanism that enhances compatibility testing by examining the configurations that test new or modified component versions first, while also distributing the work over a cluster of machines. We evaluate our approach using the 5-year evolution history of a scientific middleware component. Our results show that our methods can increase performance significantly over Rachet’s previous retest-all approach and also tests important component compatibilities early in the overall testing process, making the process of compatibility testing practical for evolving components. 
55|2||A modular package manager architecture|ContextThe success of modern software distributions in the Free and Open Source world can be explained, among other factors, by the availability of a large collection of software packages and the possibility to easily install and remove those components using state-of-the-art package managers. However, package managers are often built using a monolithic architecture and hard-wired and ad-hoc dependency solvers implementing some customized heuristics.ObjectiveWe aim at laying the foundation for improving on existing package managers. Package managers should be complete, that is find a solution whenever there exists one, and allow the user to specify complex criteria that define how to pick the best solution according to the user’s preferences.MethodIn this paper we propose a modular architecture relying on precise interface formalisms that allows the system administrator to choose from a variety of dependency solvers and backends.ResultsWe have built a working prototype–called MPM–following the design advocated in this paper, and we show how it largely outperforms a variety of current package managers.ConclusionWe argue that a modular architecture, allowing for delegating the task of constraint solving to external solvers, is the path that leads to the next generation of package managers that will deliver better results, offer more expressive preference languages, and be easily adaptable to new platforms. 
55|2||Efficient and deterministic application deployment in component-based enterprise distributed real-time and embedded systems| ContextComponent-based middleware, such as the Lightweight CORBA Component Model, is increasingly used to implement enterprise distributed real-time and embedded (DRE) systems. In addition to supporting the quality-of-service (QoS) requirements of individual DRE systems, component technologies must also support bounded latencies when effecting deployment changes to DRE systems in response to changing environmental conditions and operational requirements.ObjectiveThe goals of this paper are to (1) study sources of inefficiencies and non-deterministic performance in deployment capabilities for DRE systems and (2) devise solutions to overcome these performance problems.MethodThe paper makes two contributions to the study of the deployment and configuration of distributed component based applications. First, we analyze how conventional implementations of the OMG’s Deployment and Configuration (D&C) specification for component-based systems can significantly degrade deployment latencies. Second, we describe architectural changes and performance optimizations implemented within the Locality-Enhanced Deployment and Configuration Engine (LE-DAnCE) implementation of the D&C specification to obtain efficient and deterministic deployment latencies.ResultsWe analyze the performance of LE-DAnCE in the context of component deployments on 10 nodes for a representative DRE system consisting of 1000 components and in a cluster environment with up to 100 nodes. Our results show LE-DAnCE’s optimizations provide a bounded deployment latency of less than 2 s for the 1000 component scenario with just a 4 percent jitter.ConclusionThe improvements contained in the LE-DAnCE infrastructure provide an efficient and scaleable standards-based deployment system for component-based enterprise DRE systems. In particular, deployment time parallelism can improve deployment latency significantly, both during pre-deployment analysis of the deployment plan and during the process of installing and activating components. 
55|3|http://www.sciencedirect.com/science/journal/09505849/55/3|Guest Editorsâ Introduction: Special Issue on Software Reuse and Product Lines|
55|3||Scalable prediction of non-functional properties in software product lines: Footprint and memory consumption|ContextA software product line is a family of related software products, typically created from a set of common assets. Users select features to derive a product that fulfills their needs. Users often expect a product to have specific non-functional properties, such as a small footprint or a bounded response time. Because a product line may have an exponential number of products with respect to its features, it is usually not feasible to generate and measure non-functional properties for each possible product.ObjectiveOur overall goal is to derive optimal products with respect to non-functional requirements by showing customers which features must be selected.MethodWe propose an approach to predict a product’s non-functional properties based on the product’s feature selection. We aggregate the influence of each selected feature on a non-functional property to predict a product’s properties. We generate and measure a small set of products and, by comparing measurements, we approximate each feature’s influence on the non-functional property in question. As a research method, we conducted controlled experiments and evaluated prediction accuracy for the non-functional properties footprint and main-memory consumption. But, in principle, our approach is applicable for all quantifiable non-functional properties.ResultsWith nine software product lines, we demonstrate that our approach predicts the footprint with an average accuracy of 94%, and an accuracy of over 99% on average if feature interactions are known. In a further series of experiments, we predicted main memory consumption of six customizable programs and achieved an accuracy of 89% on average.ConclusionOur experiments suggest that, with only few measurements, it is possible to accurately predict non-functional properties of products of a product line. Furthermore, we show how already little domain knowledge can improve predictions and discuss trade-offs between accuracy and required number of measurements. With this technique, we provide a basis for many reasoning and product-derivation approaches. 
55|3||Model-based verification of quantitative non-functional properties for software product lines|Evaluating quality attributes of a design model in the early stages of development can significantly reduce the cost and risks of developing a low quality product. To make this possible, software designers should be able to predict quality attributes by reasoning on a model of the system under development. Although there exists a variety of quality-driven analysis techniques for software systems, only a few work address software product lines. This paper describes how probabilistic model checking techniques and tools can be used to verify non-functional properties of different configurations of a software product line. We propose a model-based approach that enables software engineers to assess their design solutions for software product lines in the early stages of development. Furthermore, we discuss how the analysis time can be surprisingly reduced by applying parametric model checking instead of classic model checking. The results show that the parametric approach is able to substantially alleviate the verification time and effort required to analyze non-functional properties of software product lines. 
55|3||Trusted Product Lines|ContextThe paper addresses the use of a Software Product Line approach in the context of developing software for a high-integrity, regulated domain such as civil aerospace. The success of a Software Product Line approach must be judged on whether useful products can be developed more effectively (lower cost, reduced schedule) than with traditional single-system approaches. When developing products for regulated domains, the usefulness of the product is critically dependent on the ability of the development process to provide approval evidence for scrutiny by the regulating authority.ObjectiveThe objective of the work described is to propose a framework for arguing that a product instantiated using a Software Product Line approach can be approved and used within a regulated domain, such that the development cost of that product would be less than if it had been developed in isolation.MethodThe paper identifies and surveys the issues relating the adoption of Software Product Lines as currently understood (including related technologies such as feature modelling, component-based development and model transformation) when applied to high-integrity software development. We develop an argument framework using Goal Structuring Notation to structure the claims made and the evidence required to support the approval of an instantiated product in such domains. Any unsubstantiated claims or missing/sub-standard evidence is identified, and we propose potential approaches or pose research questions to help address this.ResultsThe paper provides an argument framework supporting the use of a Software Product Line approach within a high-integrity regulated domain. It shows how lifecycle evidence can be collected, managed and used to credibly support a regulatory approval process, and provides a detailed example showing how claims regarding model transformation may be supported. Any attempt to use a Software Product Line approach in a regulated domain will need to provide evidence to support their approach in accordance with the argument outlined in the paper.ConclusionProduct Line practices may complicate the generation of convincing evidence for approval of instantiated products, but it is possible to define a credible Trusted Product Line approach. 
55|3||Improving software product line configuration: A quality attribute-driven approach|ContextDuring the definition of software product lines (SPLs) it is necessary to choose the components that appropriately fulfil a product’s intended functionalities, including its quality requirements (i.e., security, performance, scalability). The selection of the appropriate set of assets from many possible combinations is usually done manually, turning this process into a complex, time-consuming, and error-prone task.ObjectiveOur main objective is to determine whether, with the use of modeling tools, we can simplify and automate the definition process of a SPL, improving the selection process of reusable assets.MethodWe developed a model-driven strategy based on the identification of critical points (sensitivity points) inside the SPL architecture. This strategy automatically selects the components that appropriately match the product’s functional and quality requirements. We validated our approach experimenting with different real configuration and derivation scenarios in a mobile healthcare SPL where we have worked during the last three years.ResultsThrough our SPL experiment, we established that our approach improved in nearly 98% the selection of reusable assets when compared with the unassisted analysis selection. However, using our approach there is an increment in the time required for the configuration corresponding to the learning curve of the proposed tools.ConclusionWe can conclude that our domain-specific modeling approach significantly improves the software architect’s decision making when selecting the most suitable combinations of reusable components in the context of a SPL. 
55|3||Architectural evolution of FamiWare using cardinality-based feature models|ContextAmbient Intelligence systems domain is an outstanding example of modern systems that are in permanent evolution, as new devices, technologies or facilities are continuously appearing. This means it would be desirable to have a mechanism that helps with the propagation of evolution changes in deployed systems.ObjectiveWe present a software product line engineering process to manage the evolution of FamiWare, a family of middleware for ambient intelligence environments. This process drives the evolution of FamiWare middleware configurations using cardinality-based feature models, which are especially well suited to express the structural variability of ambient intelligence systems.MethodFamiWare uses cardinality-based feature models and clonable features to model the structural variability present in ambient intelligence systems, composed of a large variety of heterogeneous devices. Since the management evolution of configurations with clonable features is manually untreatable due to the high number of features, our process automates it and propagates changes made at feature level to the architectural components of the FamiWare middleware. This is a model driven development process as the evolution management, the propagation of evolution changes and the code generation are performed using some kind of model mappings and transformations. Concretely we present a variability modelling language to map the selection of features to the corresponding FamiWare middleware architectural components.ResultsOur process is able to manage the evolution of cardinality-based feature models with thousands of features, something which is not possible to tackle manually. Thanks to the use of the variability language and the automatic code generation it is possible to propagate and maintain a correspondence between the FamiWare architectural model and the code. The process is then able to calculate the architectural differences between the evolved configuration and the previous one. Checking these differences, our process helps to calculate the effort needed to perform the evolution changes in the customized products. To perform those tasks we have defined two operators, one to calculate the differences between two feature model configurations and another to create a new configuration from a previous one.ConclusionOur process automatically propagates the evolution changes of the middleware family into the existing configurations where the middleware is already deployed and also helps us to calculate the effort in performing the changes in every configuration. Finally, we validated our approach, demonstrating the functioning of the defined operators and showing that by using our tool we can generate evolved configurations for FamiWare with thousands of cloned features, for several case studies. 
55|3||Test overlay in an emerging software product line â An industrial case study|ContextIn large software organizations with a product line development approach, system test planning and scope selection is a complex task. Due to repeated testing: across different testing levels, over time (test for regression) as well as of different variants, the risk of redundant testing is large as well as the risk of overlooking important tests, hidden by the huge amount of possible tests.AimsThis study assesses the amount and type of overlaid manual testing across feature, integration and system test in such context, it explores the causes of potential redundancy and elaborates on how to provide decision support in terms of visualization for the purpose of avoiding redundancy.MethodAn in-depth case study was launched including both qualitative and quantitative observations.ResultsA high degree of test overlay is identified originating from distributed test responsibilities, poor documentation and structure of test cases, parallel work and insufficient delta analysis. The amount of test overlay depends on which level of abstraction is studied.ConclusionsAvoiding redundancy requires tool support, e.g. visualization of test design coverage, test execution progress, priorities of coverage items as well as visualized priorities of variants to support test case selection. 
55|3||Experience from model and software reuse in aircraft simulator product line engineering|Context“Reuse” and “Model Based Development” are two prominent trends for improving industrial development efficiency. Product lines are used to reduce the time to create product variants by reusing components. The model based approach provides the opportunity to enhance knowledge capture for a system in the early stages in order to be reused throughout its lifecycle. This paper describes how these two trends are combined to support development and support of a simulator product line for the SAAB 39 Gripen fighter aircraft.ObjectiveThe work aims at improving the support (in terms of efficiency and quality) when creating simulation model configurations. Software based simulators are flexible so variants and versions of included models may easily be exchanged. The objective is to increase the reuse when combining models for usage in a range of development and training simulators.MethodThe research has been conducted with an interactive approach using prototyping and demonstrations, and the evaluation is based on an iterative and a retrospective method.ResultsA product line of simulator models for the SAAB 39 Gripen aircraft has been analyzed and defined in a Product Variant Master. A configurator system has been implemented for creation, integration, and customization of stringent simulator model configurations. The system is currently under incorporation in the standard development process at SAAB Aeronautics.ConclusionThe explicit and visual description of products and their variability through a configurator system enables better insights and a common understanding so that collaboration on possible product configurations improves and the potential of software reuse increases. The combination of application fields imposes constraints on how traditional tools and methods may be utilized. Solutions for Design Automation and Knowledge Based Engineering are available, but their application has limitations for Software Product Line engineering and the reuse of simulation models. 
55|3||SimPL: A product-line modeling methodology for families of integrated control systems|ContextIntegrated control systems (ICSs) are heterogeneous systems where software and hardware components are integrated to control and monitor physical devices and processes. A family of ICSs share the same software code base, which is configured differently for each product to form a unique installation. Due to the complexity of ICSs and inadequate automation support, product configuration in this context is typically error-prone and costly.ObjectiveAs a first step to overcome these challenges, we propose a UML-based product-line modeling methodology that provides a foundation for semi-automated product configuration in the specific context of ICSs.MethodWe performed a comprehensive domain analysis to identify characteristics of ICS families, and their configuration challenges. Based on this, we formulated the characteristics of an adequate configuration solution, and derived from them a set of modeling requirements for a model-based solution to configuration. The SimPL methodology is proposed to fulfill these requirements.ResultsTo evaluate the ability of SimPL to fulfill the modeling requirements, we applied it to a large-scale industrial case study. Our experience with the case study shows that SimPL is adequate to provide a model of the product family that meets the modeling requirements. Further evaluation is still required to assess the applicability and scalability of SimPL in practice. Doing this requires conducting field studies with human subjects and is left for future work.ConclusionWe conclude that configuration in ICSs requires better automation support, and UML-based approaches to product family modeling can be tailored to provide the required foundation. 
55|3||MOD2-SCM: A model-driven product line for software configuration management systems|ContextSoftware Configuration Management (SCM) is the discipline of controlling the evolution of large and complex software systems. Over the years many different SCM systems sharing similar concepts have been implemented from scratch. Since these concepts usually are hard-wired into the respective program code, reuse is hardly possible.ObjectiveOur objective is to create a model-driven product line for SCM systems. By explicitly describing the different concepts using models, reuse can be performed on the modeling level. Since models are executable, the need for manual programming is eliminated. Furthermore, by providing a library of loosely coupled modules, we intend to support flexible composition of SCM systems.MethodWe developed a method and a tool set for model-driven software product line engineering which we applied to the SCM domain. For domain analysis, we applied the FORM method, resulting in a layered feature model for SCM systems. Furthermore, we developed an executable object-oriented domain model which was annotated with features from the feature model. A specific SCM system is configured by selecting features from the feature model and elements of the domain model realizing these features.ResultsDue to the orthogonality of both feature model and domain model, a very large number of SCM systems may be configured. We tested our approach by creating instances of the product line which mimic wide-spread systems such as CVS, GIT, Mercurial, and Subversion.ConclusionThe experiences gained from this project demonstrate the feasibility of our approach to model-driven software product line engineering. Furthermore, our work advances the state of the art in the domain of SCM systems since it support the modular composition of SCM systems at the model rather than the code level. 
55|4|http://www.sciencedirect.com/science/journal/09505849/55/4|Concept location using program dependencies and information retrieval (DepIR)|ContextThe functionality of a software system is most often expressed in terms of concepts from its problem or solution domains. The process of finding where these concepts are implemented in the source code is known as concept location and it is a prerequisite of software change.ObjectiveWe investigate a static approach to concept location named DepIR that combines program dependency search (DepS) with information retrieval-based search (IR). In this approach, programmers explore the static program dependencies of the source code components retrieved by the IR search engine.MethodThe paper presents an empirical study that compares DepIR with its constituent techniques. The evaluation is based on an empirical method of reenactment that emulates the steps of concept location for 50 past changes mined from software repositories of five software systems.ResultsThe results of the study indicate that DepIR significantly outperforms both DepS and IR.ConclusionDepIR allows developers to perform concept location efficiently. It allows finding concepts even with queries that do not rank the relevant software components highly. Since formulating a good query is not always easy, this tolerance of lower-quality queries significantly broadens the usability of DepIR compared to the traditional IR. 
55|4||Successful extreme programming: Fidelity to the methodology or good teamworking?|ContextDeveloping a theory of agile technology, in combination with empirical work, must include assessing its performance effects, and whether all or some of its key ingredients account for any performance advantage over traditional methods. Given the focus on teamwork, is the agile technology what really matters, or do general team factors, such as cohesion, primarily account for a team’s success? Perhaps the more specific software engineering team factors, for example the agile development method’s collective ownership and code management, are decisive.ObjectiveTo assess the contribution of agile methodology, agile-specific team methods, and general team factors in the performance of software teams.MethodWe studied 40 small-scale software development teams which used Extreme Programming (XP). We measured (1) the teams’ adherence to XP methods, (2) their use of XP-specific team practices, and (3) standard team attributes, as well as the quality of the project’s outcomes. We used Williams et al.’s (2004a) [33] Shodan measures of XP methods, and regression analysis.ResultsAll three types of variables are associated with the project’s performance. Teamworking is important but it is the XP-specific team factor (continuous integration, coding standards, and collective code ownership) that is significant. Only customer planning (release planning/planning game, customer access, short releases, and stand-up meeting) is positively related to performance. A negative relationship between foundations (automated unit tests, customer acceptance tests, test-first design, pair programming, and refactoring) is found and is moderated by craftsmanship (sustainable pace, simple design, and metaphor/system of names). Of the general team factors only cooperation is related to performance. Cooperation mediates the relationship between the XP-specific team factor and performance.ConclusionClient and team foci of the XP method are its critical active ingredients. 
55|4||Suitability assessment framework of agent-based software architectures|ContextA common distributed intelligent system architecture is Multi Agent Systems (MASs). Creating systems with this architecture has been recently supported by Agent Oriented Software Engineering (AOSE) methodologies. But two questions remain: how do we determine the suitability of a MAS implementation for a particular problem? And can this be determined without AOSE expertise?ObjectiveGiven the relatively small number of software engineers that are AOSE experts, many problems that could be better solved with a MAS system are solved using more commonly known but not necessarily as suitable development approaches (e.g. object-oriented). The paper aims to empower software engineers, who are not necessarily AOSE experts, in deciding whether or not they should advocate the use of an MAS technology for a given project.MethodThe paper will construct a systematic framework to identify key criteria in a problem requirement definition to assess the suitability of a MAS solution. The criteria are first identified using an iterative process. The features are initially identified from MAS implementations, and then validated against related work. This is followed by a statistical analysis of 25 problems that characterise agent-oriented solutions previously developed to group features into key criteria.ResultsKey criteria were sufficiently prominent using factor analysis to construct a framework which provides a process that identifies within the requirements the criteria discovered. This framework is then evaluated for assessing suitability of a MAS architecture, by non-AOSE experts, on two real world problems: an electricity market simulation and a financial accounting system.ConclusionSubstituting a software engineer’s personal inclination to (or not to) use a MAS, our framework provides an objective mechanism. It can supplant current practices where the decision to use a MAS architecture for a given problem remains an informal process. It was successfully illustrated on two real world problems to assess the suitability of a MAS implementation. This paper will potentially facilitate the take up of MAS technology. 
55|4||Vertical software industry evolution: The impact of software costs and limited customer base|ContextSoftware systems are commonly used in a variety of industries as a means of automating organizational business processes. Initially, such software is often developed in-house by the vertical organizations possibly with the support of professional IT service providers; however, in many cases, internally developed software is eventually replaced with the software products provided by independent software vendors. These vendors often use license fees to recover their software development investments, as well as to gain some margin. However, if the vendor’s customer base for a specific type of software is limited, then either the license fees are too high and hence the customers may prefer to develop the software internally, or the margin has to be decreased. As a result, the market for software products of that type may not materialize.ObjectiveThe paper introduces an analytical model that defines the minimum number of customers that the software vendor should have for its software to be less expensive as compared to the in-house software.MethodFollowing a conceptual-analytical approach, a model is constructed wherein the minimum number of a vendor’s customers is represented as a function of other factors affecting software development costs. This model is verified by applying it to estimate the minimum customer base in the segment of telecommunications billing mediation software.ResultsUsing the proposed analytical model, the minimum number of customers and the maximum number of software vendors in this segment are evaluated. The obtained results are found to be in line with the information available from a telecommunications software market database.ConclusionsBased on the model, a preliminary conclusion is made that in industries with high software development costs, heterogeneous legacy systems to integrate with, and a limited pool of potential customers, the number of software vendors is unlikely to be significant, and hence the in-house or custom-made software is unlikely to be superseded by the software products. 
55|4||Applying MDE to the (semi-)automatic development of model transformations|ContextModel transformations play a key role in any software development project based on Model-Driven Engineering principles. However, despite the inherent complexity of developing model transformations, little attention has been paid to the application of MDE principles to the development of model transformations.ObjectiveIn order to: (a) address the inherent complexity of model transformation development and (b) alleviate the problem of the diversity of the languages that are available for model transformation, this paper proposes the application of MDE principles to the development of model transformations. In particular, we have adopted the idea of handling model transformations as transformation models in order to be able to model, transform and generate model transformations.MethodThe proposal follows an MDA-based approach that entails the modeling of model transformations at different abstraction levels and the connection of these models by means of model transformations. It has been empirically validated by conducting a set of case studies following a systematic research methodology.ResultsThe proposal was supported by the introduction of MeTAGeM, a methodological and technical framework for the model-driven development of model transformations that bundles a set of Domain-Specific Languages for modeling model transformations with a set of model transformations in order to bridge these languages and (semi-)automate model transformations development.ConclusionThis paper serves to show that a semi-automatic development process for model transformations is not only desirable but feasible. This process, based on MDE principles, helps to ease the task of developing model transformations and to alleviate interoperability issues between model transformation languages. 
55|4||Guest Editorial: Special Section on International Conference on Program Comprehension, 2011|
55|4||Aspect-orientation is a rewarding investment into future code changes â As long as the aspects hardly change|ContextAspect-Oriented Programming (AOP) is often described as a technique which improves the resulting software’s modularity. However, previous experiments seem to indicate that AOP is a technique which potentially increases the development or maintenance time. A possible reason why previous experiments were not able to show such a benefit is that those experiments did not consider situations where AOP has its strength: situations where aspects change.ObjectivesOur objective is to analyze whether initial higher development times caused by aspect-oriented programming can be compensated by frequent changes on the aspect code.MethodThis study is an empirical study with 15 subjects based on a within-subject design (repeated measurement) using two randomized groups. The measurement is development time until programming tasks completion. Additionally, an exploratory study is performed which analyzed (a) in separation out- and under-performing subjects with respect to development time, (b) the possible influence of the lines of code, (c) the possible influence of the number of advice and pointcuts, and finally (d) the possible impact of if-statements in the aspect-oriented solutions.ResultsThe experiment shows that frequent changes in the crosscutting code which do not change the aspect’s underlying structure compensate an initial higher development time for those concerns. But it also shows that changes, which do alter the concern’s structure again, result in higher development times when using AOP. Furthermore, the paper shows that the results are similar for out- and under-performing subjects in the experiment. Finally it shows that if changes are required in an aspect’s structure, subjects tend to perform such structural changes in a non-uniform way.ConclusionAOP is a potential rewarding investment into future code changes – but it has risks. 
55|4||Applying a smoothing filter to improve IR-based traceability recovery processes: An empirical investigation|ContextTraceability relations among software artifacts often tend to be missing, outdated, or lost. For this reason, various traceability recovery approaches—based on Information Retrieval (IR) techniques—have been proposed. The performances of such approaches are often influenced by “noise” contained in software artifacts (e.g., recurring words in document templates or other words that do not contribute to the retrieval itself).AimAs a complement and alternative to stop word removal approaches, this paper proposes the use of a smoothing filter to remove “noise” from the textual corpus of artifacts to be traced.MethodWe evaluate the effect of a smoothing filter in traceability recovery tasks involving different kinds of artifacts from five software projects, and applying three different IR methods, namely Vector Space Models, Latent Semantic Indexing, and Jensen–Shannon similarity model.ResultsOur study indicates that, with the exception of some specific kinds of artifacts (i.e., tracing test cases to source code) the proposed approach is able to significantly improve the performances of traceability recovery, and to remove “noise” that simple stop word filters cannot remove.ConclusionsThe obtained results not only help to develop traceability recovery approaches able to work in presence of noisy artifacts, but also suggest that smoothing filters can be used to improve performances of other software engineering approaches based on textual analysis. 
55|4||Answering software evolution questions: An empirical evaluation|ContextDevelopers often need to find answers to questions regarding the evolution of a system when working on its code base. While their information needs require data analysis pertaining to different repository types, the source code repository has a pivotal role for program comprehension tasks. However, the coarse-grained nature of the data stored by commit-based software configuration management systems often makes it challenging for a developer to search for an answer.ObjectiveWe present Replay, an Eclipse plug-in that allows developers to explore the change history of a system by capturing the changes at a finer granularity level than commits, and by replaying the past changes chronologically inside the integrated development environment, with the source code at hand.MethodWe conducted a controlled experiment to empirically assess whether Replay outperforms a baseline (SVN client in Eclipse) on helping developers to answer common questions related to software evolution.ResultsThe experiment shows that Replay leads to a decrease in completion time with respect to a set of software evolution comprehension tasks.ConclusionWe conclude that there are benefits in using Replay over the state of the practice tools for answering questions that require fine-grained change information and those related to recent changes. 
55|5|http://www.sciencedirect.com/science/journal/09505849/55/5|Application of knowledge-based approaches in software architecture: A systematic mapping study|ContextKnowledge management technologies have been employed across software engineering activities for more than two decades. Knowledge-based approaches can be used to facilitate software architecting activities (e.g., architectural evaluation). However, there is no comprehensive understanding on how various knowledge-based approaches (e.g., knowledge reuse) are employed in software architecture.ObjectiveThis work aims to collect studies on the application of knowledge-based approaches in software architecture and make a classification and thematic analysis on these studies, in order to identify the gaps in the existing application of knowledge-based approaches to various architecting activities, and promising research directions.MethodA systematic mapping study is conducted for identifying and analyzing the application of knowledge-based approaches in software architecture, covering the papers from major databases, journals, conferences, and workshops, published between January 2000 and March 2011.ResultsFifty-five studies were selected and classified according to the architecting activities they contribute to and the knowledge-based approaches employed. Knowledge capture and representation (e.g., using an ontology to describe architectural elements and their relationships) is the most popular approach employed in architecting activities. Knowledge recovery (e.g., documenting past architectural design decisions) is an ignored approach that is seldom used in software architecture. Knowledge-based approaches are mostly used in architectural evaluation, while receive the least attention in architecture impact analysis and architectural implementation.ConclusionsThe study results show an increased interest in the application of knowledge-based approaches in software architecture in recent years. A number of knowledge-based approaches, including knowledge capture and representation, reuse, sharing, recovery, and reasoning, have been employed in a spectrum of architecting activities. Knowledge-based approaches have been applied to a wide range of application domains, among which “Embedded software” has received the most attention. 
55|5||A taxonomy of design methods process models|ContextDesigners and developers are increasingly expected to deliver high quality systems, i.e. systems that are usable, robust, consistent as well as evolutionary, and that fulfill users’ needs. To produce such systems, Design Methods suggest many approaches. However, the important number of existing approaches makes the choice of a method among the others particularly difficult. In addition to this, and because of the time required for understanding (and then operationalizing) new methods, designers tend to use already known methods, even though those which sometimes may not really be adapted to their needs.ObjectiveThis paper proposes a classification of characteristics of design methods process models. In other terms, it proposes a taxonomy that aims to facilitate the discovery and the choice of methods for designers and developers.MethodFrom a study of process models of several design methods, we identify six main axes, namely Cycle, Collaboration, Artifacts, Recommended Use, Maturity and Flexibility, which are in turn divided into 34 characteristics.ResultsThis paper provides a deep theorical insight. For each characteristic identified from relevant literature, a definition and a gradation, illustrated using examples, are given. Moreover, it presents a web site that offers various tools for exploring the axes of our taxonomy. This web site provides an overview of process models as well as means for comparing them, textually or graphically. Finally, the paper relates the first evaluation conducted in order to estimate designers’ adhesion to the taxonomy in terms of easiness of learning, completeness and intention to use.ConclusionWe show, based on evaluation results, that our taxonomy of process models facilitates the discovery of new methods and helps designers in choosing suitable methods, really adapted to their needs. Therefore, it enhances chances to conduct high quality projects. 
55|5||A survey of experienced user perceptions about software design patterns|ContextAlthough the concept of the software design pattern is well-established, there is relatively little empirical knowledge about the patterns that experienced users consider to be most valuable.AimTo identify which patterns from the set catalogued by the ‘Gang of Four’ are considered to be useful by experienced users, which ones are considered as not being useful, and why this is so.MethodWe undertook a web-based survey of experienced pattern users, seeking information about their experiences as software developers and maintainers. Our sampling frame consisted of the authors of all of the pattern papers that we had identified in a preceding systematic review of studies of patterns.ResultsWe received 206 usable responses, corresponding to a response rate of 19% from the original sampling frame. Most respondents were involved with software development rather than maintenance.ConclusionWhile patterns can provide a means of sharing ‘knowledge schemas’ between designers, only three patterns were widely regarded as valuable. Around one quarter of the patterns gained very low approval or worse. These observations need to be considered when using patterns; teaching students about the pattern concept; and planning empirical studies about patterns. 
55|5||Supporting the verification of compliance to safety standards via model-driven engineering: Approach, tool-support and empirical validation|ContextMany safety–critical systems are subject to safety certification as a way to provide assurance that these systems cannot unduly harm people, property or the environment. Creating the requisite evidence for certification can be a challenging task due to the sheer size of the textual standards based on which certification is performed and the amenability of these standards to subjective interpretation.ObjectiveThis paper proposes a novel approach to aid suppliers in creating the evidence necessary for certification according to standards. The approach is based on Model-Driven Engineering (MDE) and addresses the challenges of using certification standards while providing assistance with compliance.MethodGiven a safety standard, a conceptual model is built that provides a succinct and explicit interpretation of the standard. This model is then used to create a UML profile that helps system suppliers in relating the concepts of the safety standard to those of the application domain, in turn enabling the suppliers to demonstrate how their system development artifacts comply with the standard.ResultsWe provide a generalizable and tool-supported solution to support the verification of compliance to safety standards. Empirical validation of the work is presented via an industrial case study that shows how the concepts of a sub-sea production control system can be aligned with the evidence requirements of the IEC61508 standard. A subsequent survey examines the perceptions of practitioners about the solution.ConclusionThe case study indicates that the supplier company where the study was performed found the approach useful in helping them prepare for certification of their software. The survey indicates that practitioners found our approach easy to understand and that they would be willing to adopt it in practice. Since the IEC61508 standard applies to multiple domains, these results suggest wider applicability and usefulness of our work. 
55|5||Guest Editorial: Special Section from the 11th International Conference on Quality Software (QSIC 2011)|
55|5||Metamorphic slice: An application in spectrum-based fault localization|ContextBecause of its simplicity and effectiveness, Spectrum-Based Fault Localization (SBFL) has been one of the popular approaches towards fault localization. It utilizes the execution result of failure or pass, and the corresponding coverage information (such as program slice) to estimate the risk of being faulty for each program entity (such as statement). However, all existing SBFL techniques assume the existence of a test oracle to determine the execution result of a test case. But, it is common that test oracles do not exist, and hence the applicability of SBFL has been severely restricted.ObjectiveWe aim at developing a framework that can extend the application of SBFL to the common situations where test oracles do not exist.MethodOur approach uses a new concept of metamorphic slice resulting from the integration of metamorphic testing and program slicing. In SBFL, instead of using the program slice and the result of failure or pass for an individual test case, a metamorphic slice and the result of violation or non-violation of a metamorphic relation are used. Since we need not know the execution result for an individual test case, the existence of a test oracle is no longer a requirement to apply SBFL.ResultsAn experimental study involving nine programs and three risk evaluation formulas was conducted. The results show that our proposed solution delivers a performance comparable to the performance observed by existing SBFL techniques for the situations where test oracles exist.ConclusionWith respect to the problem that SBFL is only applicable to programs with test oracles, we propose an innovative solution. Our solution is not only intuitively appealing and conceptually feasible, but also practically effective. Consequently, test oracles are no longer mandatory for SBFL, and hence the applicability of SBFL is significantly extended. 
55|5||A general noise-reduction framework for fault localization of Java programs|ContextExisting fault-localization techniques combine various program features and similarity coefficients with the aim of precisely assessing the similarities among the dynamic spectra of these program features to predict the locations of faults. Many such techniques estimate the probability of a particular program feature causing the observed failures. They often ignore the noise introduced by other features on the same set of executions that may lead to the observed failures. It is unclear to what extent such noise can be alleviated.ObjectiveThis paper aims to develop a framework that reduces the noise in fault-failure correlation measurements.MethodWe develop a fault-localization framework that uses chains of key basic blocks as program features and a noise-reduction methodology to improve on the similarity coefficients of fault-localization techniques. We evaluate our framework on five base techniques using five real-life median-scaled programs in different application domains. We also conduct a case study on subjects with multiple faults.ResultsThe experimental result shows that the synthesized techniques are more effective than their base techniques by almost 10%. Moreover, their runtime overhead factors to collect the required feature values are practical. The case study also shows that the synthesized techniques work well on subjects with multiple faults.ConclusionWe conclude that the proposed framework has a significant and positive effect on improving the effectiveness of the corresponding base techniques. 
55|5||On the adoption of MC/DC and control-flow adequacy for a tight integration of program testing and statistical fault localization|ContextTesting and debugging consume a significant portion of software development effort. Both processes are usually conducted independently despite their close relationship with each other. Test adequacy is vital for developers to assure that sufficient testing effort has been made, while finding all the faults in a program as soon as possible is equally important. A tight integration between testing and debugging activities is essential.ObjectiveThe paper aims at finding whether three factors, namely, the adequacy criterion to gauge a test suite, the size of a prioritized test suite, and the percentage of such a test suite used in fault localization, have significant impacts on integrating test case prioritization techniques with statistical fault localization techniques.MethodWe conduct a controlled experiment to investigate the effectiveness of applying adequate test suites to locate faults in a benchmark suite of seven Siemens programs and four real-life UNIX utility programs using three adequacy criteria, 16 test case prioritization techniques, and four statistical fault localization techniques. We measure the proportion of code needed to be examined in order to locate a fault as the effectiveness of statistical fault localization techniques. We also investigate the integration of test case prioritization and statistical fault localization with postmortem analysis.ResultThe main result shows that on average, it is more effective for a statistical fault localization technique to utilize the execution results of a MC/DC-adequate test suite than those of a branch-adequate test suite, and is in turn more effective to utilize the execution results of a branch-adequate test suite than those of a statement-adequate test suite. On the other hand, we find that none of the fault localization techniques studied can be sufficiently effective in suggesting fault-relevant statements that can fit easily into one debug window of a typical IDE.ConclusionWe find that the adequacy criterion and the percentage of a prioritized test suite utilized are major factors affecting the effectiveness of statistical fault localization techniques. In our experiment, the adoption of a stronger adequacy criterion can lead to more effective integration of testing and debugging. 
55|6|http://www.sciencedirect.com/science/journal/09505849/55/6|Systematic literature reviews in software engineering|
55|6||Obsolete software requirements|ContextCoping with rapid requirements change is crucial for staying competitive in the software business. Frequently changing customer needs and fierce competition are typical drivers of rapid requirements evolution resulting in requirements obsolescence even before project completion.ObjectiveAlthough the obsolete requirements phenomenon and the implications of not addressing them are known, there is a lack of empirical research dedicated to understanding the nature of obsolete software requirements and their role in requirements management.MethodIn this paper, we report results from an empirical investigation with 219 respondents aimed at investigating the phenomenon of obsolete software requirements.ResultsOur results contain, but are not limited to, defining the phenomenon of obsolete software requirements, investigating how they are handled in industry today and their potential impact.ConclusionWe conclude that obsolete software requirements constitute a significant challenge for companies developing software intensive products, in particular in large projects, and that companies rarely have processes for handling obsolete software requirements. Further, our results call for future research in creating automated methods for obsolete software requirements identification and management, methods that could enable efficient obsolete software requirements management in large projects. 
55|6||Transforming and tracing reused requirements models to home automation models|ContextModel-Driven Software Development (MDSD) has emerged as a very promising approach to cope with the inherent complexity of modern software-based systems. Furthermore, it is well known that the Requirements Engineering (RE) stage is critical for a project’s success. Despite the importance of RE, MDSD approaches commonly leave textual requirements specifications to one side.ObjectiveOur aim is to integrate textual requirements specifications into the MDSD approach by using the MDSD techniques themselves, including metamodelling and model transformations. The proposal is based on the assumption that a reuse-based Model-Driven Requirements Engineering (MDRE) approach will improve the requirements engineering stage, the quality of the development models generated from requirements models, and will enable the traces from requirements to other development concepts (such as analysis or design) to be maintained.MethodThe approach revolves around the Requirements Engineering Metamodel, denominated as REMM, which supports the definition of the boilerplate based textual requirements specification languages needed for the definition of model transformation from application requirements models to platform-specific application models and code.ResultsThe approach has been evaluated through its application to Home Automation (HA) systems. The HA Requirement Specification Language denominated as HAREL is used to define application requirements models which will be automatically transformed and traced to the application model conforming to the HA Domain Specific Language.ConclusionsAn anonymous online survey has been conducted to evaluate the degree of acceptance by both HA application developers and MDSD practitioners. The main conclusion is that 66.7% of the HA experts polled strongly agree that the automatic transformation of the requirements models to HA models improves the quality of the HA models. Moreover, 58.3% of the HA participants strongly agree with the usefulness of the traceability matrix which links requirements to HA functional units in order to discover which devices are related to a specific requirement. We can conclude that the experts we have consulted agree with the proposal we are presenting here, since the average mark given is 4 out of 5. 
55|6||Dynamic profiling-based approach to identifying cost-effective refactorings|ContextObject-oriented software undergoes continuous changes—changes often made without consideration of the software’s overall structure and design rationale. Hence, over time, the design quality of the software degrades causing software aging or software decay. Refactoring offers a means of restructuring software design to improve maintainability. In practice, efforts to invest in refactoring are restricted; therefore, the problem calls for a method for identifying cost-effective refactorings that efficiently improve maintainability. Cost-effectiveness of applied refactorings can be explained as maintainability improvement over invested refactoring effort (cost). For the system, the more cost-effective refactorings are applied, the greater maintainability would be improved. There have been several studies of supporting the arguments that changes are more prone to occur in the pieces of codes more frequently utilized by users; hence, applying refactorings in these parts would fast improve maintainability of software. For this reason, dynamic information is needed for identifying the entities involved in given scenarios/functions of a system, and within these entities, refactoring candidates need to be extracted.ObjectiveThis paper provides an automated approach to identifying cost-effective refactorings using dynamic information in object-oriented software.MethodTo perform cost-effective refactoring, refactoring candidates are extracted in a way that reduces dependencies; these are referred to as the dynamic information. The dynamic profiling technique is used to obtain the dependencies of entities based on dynamic method calls. Based on those dynamic dependencies, refactoring-candidate extraction rules are defined, and a maintainability evaluation function is established. Then, refactoring candidates are extracted and assessed using the defined rules and the evaluation function, respectively. The best refactoring (i.e., that which most improves maintainability) is selected from among refactoring candidates, then refactoring candidate extraction and assessment are re-performed to select the next refactoring, and the refactoring identification process is iterated until no more refactoring candidates for improving maintainability are found.ResultsWe evaluate our proposed approach in three open-source projects. The first results show that dynamic information is helpful in identifying cost-effective refactorings that fast improve maintainability; and, considering dynamic information in addition to static information provides even more opportunities to identify cost-effective refactorings. The second results show that dynamic information is helpful in extracting refactoring candidates in the classes where real changes had occurred; in addition, the results also offer the promising support for the contention that using dynamic information helps to extracting refactoring candidates from highly-ranked frequently changed classes.ConclusionOur proposed approach helps to identify cost-effective refactorings and supports an automated refactoring identification process. 
55|6||More testers â The effect of crowd size and time restriction in software testing|ContextThe questions of how many individuals and how much time to use for a single testing task are critical in software verification and validation. In software review and usability evaluation contexts, positive effects of using multiple individuals for a task have been found, but software testing has not been studied from this viewpoint.ObjectiveWe study how adding individuals and imposing time pressure affects the effectiveness and efficiency of manual testing tasks. We applied the group productivity theory from social psychology to characterize the type of software testing tasks.MethodWe conducted an experiment where 130 students performed manual testing under two conditions, one with a time restriction and pressure, i.e., a 2-h fixed slot, and another where the individuals could use as much time as they needed.ResultsWe found evidence that manual software testing is an additive task with a ceiling effect, like software reviews and usability inspections. Our results show that a crowd of five time-restricted testers using 10 h in total detected 71% more defects than a single non-time-restricted tester using 9.9 h. Furthermore, we use F-score measure from the information retrieval domain to analyze the optimal number of testers in terms of both effectiveness and validity of testing results. We suggest that future studies on verification and validation practices use F-score to provide a more transparent view of the results.ConclusionsThe results seem promising for the time-pressured crowds by indicating that multiple time-pressured individuals deliver superior defect detection effectiveness in comparison to non-time-pressured individuals. However, caution is needed, as the limitations of this study need to be addressed in future works. Finally, we suggest that the size of the crowd used in software testing tasks should be determined based on the share of duplicate and invalid reports produced by the crowd and by the effectiveness of the duplicate handling mechanisms. 
55|6||Software verification and graph similarity for automated evaluation of studentsâ assignments|ContextThe number of students enrolled in universities at standard and on-line programming courses is rapidly increasing. This calls for automated evaluation of students assignments.ObjectiveWe aim to develop methods and tools for objective and reliable automated grading that can also provide substantial and comprehensible feedback. Our approach targets introductory programming courses, which have a number of specific features and goals. The benefits are twofold: reducing the workload for teachers, and providing helpful feedback to students in the process of learning.MethodFor sophisticated automated evaluation of students’ programs, our grading framework combines results of three approaches (i) testing, (ii) software verification, and (iii) control flow graph similarity measurement. We present our tools for software verification and control flow graph similarity measurement, which are publicly available and open source. The tools are based on an intermediate code representation, so they could be applied to a number of programming languages.ResultsEmpirical evaluation of the proposed grading framework is performed on a corpus of programs written by university students in programming language C within an introductory programming course. Results of the evaluation show that the synergy of proposed approaches improves the quality and precision of automated grading and that automatically generated grades are highly correlated with instructor-assigned grades. Also, the results show that our approach can be trained to adapt to teacher’s grading style.ConclusionsIn this paper we integrate several techniques for evaluation of student’s assignments. The obtained results suggest that the presented tools can find real-world applications in automated grading. 
55|6||Translation of Z specifications to executable code: Application to the database domain|ContextIt is well-known that the use of formal methods in the software development process results in high-quality software products. Having specified the software requirements in a formal notation, the question is how they can be transformed into an implementation. There is typically a mismatch between the specification and the implementation, known as the specification-implementation gap.ObjectiveThis paper introduces a set of translation functions to fill the specification-implementation gap in the domain of database applications. We only present the formal definition, not the implementation, of the translation functions.MethodWe chose Z, SQL and Delphi languages to illustrate our methodology. Because the mathematical foundation of Z has many properties in common with SQL, the translation functions from Z to SQL are derived easily. For the translation of Z to Delphi, we extend Delphi libraries to support Z mathematical structures such as sets and tuples. Then, based on these libraries, we derive the translation functions from Z to Delphi. Therefore, we establish a formal relationship between Z specifications and Delphi/SQL code. To prove the soundness of the translation from a Z abstract schema to the Delphi/SQL code, we define a Z design-level schema. We investigate the consistency of the Z abstract schema with the Z design-level schema by using Z refinement rules. Then, by the use of the laws of Morgan refinement calculus, we prove that the Delphi/SQL code refines the Z design-level schema.ResultsThe proposed approach can be used to build the correct prototype of a database application from its specification. This prototype can be evolved, or may be used to validate the software requirements specification against user requirements.ConclusionTherefore, the work presented in this paper reduces the overall cost of the development of database applications because early validation reveals requirement errors sooner in the software development cycle. 
55|6||Evaluating test suite characteristics, cost, and effectiveness of FSM-based testing methods|ContextTesting from finite state machines has been investigated due to its well-founded and sound theory as well as its practical application. There has been a recurrent interest in developing methods capable of generating test suites that detect all faults in a given fault domain. However, the proposal of new methods motivates the comparison with traditional methods.ObjectiveWe compare the methods that generate complete test suites from finite states machines. The test suites produced by the W, HSI, H, SPY, and P methods are analyzed in different configurations.MethodComplete and partial machines were randomly generated varying numbers of states, inputs, outputs, and transitions. These different configurations were used to compare test suite characteristics (number of resets, test case length) and the test suite length (i.e., the sum of the length of its test cases). The fault detection ratio was evaluated using mutation testing to produce faulty implementations with an extra state.ResultsOn average, the recent methods (H, SPY, and P) produced longer test cases but smaller test suites than the traditional methods (W, HSI). The recent methods generated test suites of similar length, though P produced slightly smaller test suites. The SPY and P methods had the highest fault detection ratios and HSI had the lowest. For all methods, there was a positive correlation between the number of resets and the test suite length and between the test case length and the fault detection ratio.ConclusionThe recent methods rely on fewer and longer test cases to reduce the overall test suite length, while the traditional methods produce more and shorter test cases. Longer test cases are correlated to fault detection ratio which favored SPY, though all methods have a ratio of over 92%. 
55|6||Guest Editorial for the Special Section on the Euromicro 2011 Conference on Software Engineering and Advanced Applications (SEAA)|
55|6||Retainment policies â A formal framework for change retainment for trace-based model transformations| ContextModel-to-model (M2M) transformations play an important role within model-driven development. Modern M2M approaches support incremental updates to the target model according to changes in the source model(s). Bidirectional transformation approaches even allow to incrementally translate target model changes back to the source model.ObjectiveA model transformation’s target model may need to be refined later on either manually or automatically. Therefore, modellers may want to specify that target model changes are not overwritten if the original transformation is re-executed. There is currently only weak support for this kind of retainment by transformation engines.MethodIn many transformation engines a transformation trace is available which keeps record of a transformation’s actions. In this paper, we exploit this information and define patterns which allow transformation engineers to trim transformations to facilitate the handling of target model changes.ResultsWe describe a formal framework which serves as basis for realizing the patterns as what we call retainment policies. Based on this framework we present an implementation of the retainment policy approach for QVT Relations.ConclusionsThe retainment policies which allow a transformation developer to develop transformation rules that will retain manual changes to the target model. The implementation of the approach for QVT Relations shows that it is realisable in state-of-the art transformation techniques. However, being defined on a theoretical level, also other transformation approaches will benefit from our work. 
55|6||Round-trip support for extra-functional property management in model-driven engineering of embedded systems|ContextIn order for model-driven engineering to succeed, automated code generation from models through model transformations has to guarantee that extra-functional properties specified at design level are preserved at code level.ObjectiveThe goal of this research work is to provide a full round-trip engineering approach in order to evaluate quality attributes of the embedded system by code execution monitoring as well as code static analysis and then provide back-propagation of the resulting values to modelling level. In this way, properties that can only be roughly estimated statically are evaluated against observed values and this consequently allows to refine the design models for ensuring preservation of analysed extra-functional properties at code level.MethodFollowing the model-driven engineering vision, (meta-) models and transformations are used as main artefacts for the realisation of the round-trip support which is finally validated against an industrial case study.ResultThis article presents an approach to support the whole round-trip process starting from the generation of source code for a target platform, passing through the monitoring of selected system quality attributes at code level, and finishing with the back-propagation of observed values to modelling level. The technique is validated against an industrial case study in the telecommunications applicative domain.ConclusionPreservation of extra-functional properties through appropriate description, computation and evaluation makes it possible to reduce final product verification and validation effort and costs by generating correct-by-construction code. The proposed round-trip support aids a model-driven component-based development process in ensuring a desired level of extra-functional properties preservation from the source modelling artefacts to the generated code. 
55|6||Empirical evaluation of the effects of mixed project data on learning defect predictors|ContextDefect prediction research mostly focus on optimizing the performance of models that are constructed for isolated projects (i.e. within project (WP)) through retrospective analyses. On the other hand, recent studies try to utilize data across projects (i.e. cross project (CP)) for building defect prediction models for new projects. There are no cases where the combination of within and cross (i.e. mixed) project data are used together.ObjectiveOur goal is to investigate the merits of using mixed project data for binary defect prediction. Specifically, we want to check whether it is feasible, in terms of defect detection performance, to use data from other projects for the cases (i) when there is an existing within project history and (ii) when there are limited within project data.MethodWe use data from 73 versions of 41 projects that are publicly available. We simulate the two above-mentioned cases, and compare the performances of naive Bayes classifiers by using within project data vs. mixed project data.ResultsFor the first case, we find that the performance of mixed project predictors significantly improves over full within project predictors (p-value < 0.001), however the effect size is small (Hedges′ g = 0.25). For the second case, we found that mixed project predictors are comparable to full within project predictors, using only 10% of available within project data (p-value = 0.002, g = 0.17).ConclusionWe conclude that the extra effort associated with collecting data from other projects is not feasible in terms of practical performance improvement when there is already an established within project defect predictor using full project history. However, when there is limited project history, e.g. early phases of development, mixed project predictions are justifiable as they perform as good as full within project models. 
55|7|http://www.sciencedirect.com/science/journal/09505849/55/7|Empirical studies concerning the maintenance of UML diagrams and their use in the maintenance of code: A systematic mapping study|ContextThe Unified Modelling Language (UML) has, after ten years, become established as the de facto standard for the modelling of object-oriented software systems. It is therefore relevant to investigate whether its use is important as regards the costs involved in its implantation in industry being worthwhile.MethodWe have carried out a systematic mapping study to collect the empirical studies published in order to discover “What is the current existing empirical evidence with regard to the use of UML diagrams in source code maintenance and the maintenance of the UML diagrams themselves?ResultsWe found 38 papers, which contained 63 experiments and 3 case studies.ConclusionAlthough there is common belief that the use of UML is beneficial for source code maintenance, since the quality of the modifications is greater when UML diagrams are available, only 3 papers concerning this issue have been published. Most research (60 empirical studies) concerns the maintainability and comprehensibility of the UML diagrams themselves which form part of the system’s documentation, since it is assumed that they may influence source code maintainability, although this has not been empirically validated. Moreover, the generalizability of the majority of the experiments is questionable given the material, tasks and subjects used. There is thus a need for more experiments and case studies to be performed in industrial contexts, i.e., with real systems and using maintenance tasks conducted by practitioners under real conditions that truly show the utility of UML diagrams in maintaining code, and that the fact that a diagram is more comprehensible or modifiable influences the maintainability of the code itself. This utility should also be studied from the viewpoint of cost and productivity, and the consistent and simultaneous maintenance of diagrams and code must also be considered in future empirical studies. 
55|7||Empirical studies on the use of social software in global software development â A systematic mapping study|BackgroundIn Global Software Development (GSD), informal communication and knowledge sharing play an important role. Social Software (SoSo) has the potential to support and foster this key responsibility. Research on the use of SoSo in GSD is still at an early stage: although a number of empirical studies on the usage of SoSo are available in related fields, there exists no comprehensive overview of what has been investigated to date across them.ObjectiveThe aim of this review is to map empirical studies on the usage of SoSo in Software Engineering projects and in distributed teams, and to highlight the findings of research works which could prove to be beneficial for GSD researchers and practitioners.MethodA Systematic Mapping Study is conducted using a broad search string that allows identifying a variety of studies which can be beneficial for GSD. Papers have been retrieved through a combination of automatic search and snowballing, hence a wide quantitative map of the research area is provided. Additionally, text extracts from the studies are qualitatively synthesised to investigate benefits and challenges of the use of SoSo.ResultsSoSo is reported as being chiefly used as a support for collaborative work, fostering awareness, knowledge management and coordination among team members. Contrary to the evident high importance of the social aspects offered by SoSo, socialisation is not the most important usage reported.ConclusionsThis review reports how SoSo is used in GSD and how it is capable of supporting GSD teams. Four emerging themes in global software engineering were identified: the appropriation and development of usage structures; understanding how an ecology of communication channels and tools are used by teams; the role played by SoSo either as a subtext or as an explicit goal; and finally, the surprising low percentage of observational studies. 
55|7||Software clone detection: A systematic review|ContextReusing software by means of copy and paste is a frequent activity in software development. The duplicated code is known as a software clone and the activity is known as code cloning. Software clones may lead to bug propagation and serious maintenance problems.ObjectiveThis study reports an extensive systematic literature review of software clones in general and software clone detection in particular.MethodWe used the standard systematic literature review method based on a comprehensive set of 213 articles from a total of 2039 articles published in 11 leading journals and 37 premier conferences and workshops.ResultsExisting literature about software clones is classified broadly into different categories. The importance of semantic clone detection and model based clone detection led to different classifications. Empirical evaluation of clone detection tools/techniques is presented. Clone management, its benefits and cross cutting nature is reported. Number of studies pertaining to nine different types of clones is reported. Thirteen intermediate representations and 24 match detection techniques are reported.ConclusionWe call for an increased awareness of the potential benefits of software clone management, and identify the need to develop semantic and model clone detection techniques. Recommendations are given for future research. 
55|7||Applying Q-methodology to analyse the success factors in GSD|ContextThe context of this paper is Global Software Development (GSD) which is a current trend concerning the development of software in a distributed manner throughout different countries. This paradigm has several advantages, but unfortunately there are a number of challenges that hinder projects’ successful development.ObjectiveThe main goal of this paper is to discover which factors affect the success of GSD projects and how these are ranked by researchers and practitioners.MethodThis paper analyses the relevant success factors reported in literature. These were collected by conducting a literature review, as a result of which 39 GSD success factors were selected. Q-methodology was then followed to conduct a survey from which the opinions of 21 experts in GSD were collected.ResultsThe data indicated that the best ranked GSD success factors are staff motivation, skilled human resources and the identification of roles and responsibilities. The lowest scores were, surprisingly, language barriers, time zone differences between sites, cultural differences and geographical distance which, to date, have frequently been considered by researchers as the most influential factors in GSD. This study additionally shows the results according to the different points of view of the respondents involved and the context of the projects.ConclusionThis study indicates that there are different points of view as regards which issues are most important to success when setting up a GSD project. For instance, some experts prefer a knowledge focus, while others prefer a project management approach in which the most important issues are those related to management (risks, coordination) and so on.The results obtained have also shown that the challenges of GSD are changing, since the critical issues were initially related to the various types of distances (geographical, temporal, socio-cultural, language). However, there is now a greater concern for the team members’ features and skills. 
55|7||Architecture-based testing of service-oriented applications in distributed systems|ContextTesting distributed service-oriented applications (SOAs) is more challenging than testing monolithic applications since these applications have complex interactions between participant services. Test engineers can observe test results only through a front service that handles request messages sent by test engineers. Message exchanges between participant services are hidden behind the front service and cannot be easily observed or controlled through the front service. For this reason, testing SOAs suffer from limited observability and controllability problem.ObjectiveThis paper proposes a new test method that is architecture-based and exploits interaction architecture of a SOA. The proposed test method alleviates the limited observability and controllability problem by employing test architecture, thereby facilitating test execution and analysis through monitoring and controlling message exchanges.MethodOur proposed method derives an interaction architecture from the specification of a SOA. Test architectures can be designed from the derived interaction architecture by extending it with additional test elements. At the same time, architecture-neutral test scenarios are automatically generated from the test model that is constructed from the specification. Our method combines test architecture information with the test scenarios to obtain architecture-enabled test scenarios under the selected test architectures. Finally, architecture-enabled test execution and analysis are conducted in the real network environment.ResultsThe efficacy of the proposed method is demonstrated with an industrial case study, which shows that it is practical and effective for testing SOAs. Even though our method increases an additional test generation effort owing to test architecture, it is counterbalanced by higher fault detection rate and faster fault locating time.ConclusionThe main benefit of our approach is that using test architecture it enhances testability of SOA by increasing observability and controllability through monitoring and controlling message exchanges. Our architecture-based test method enables test engineers to detect faults efficiently and also reduce fault locating time significantly. 
55|7||An investigation of how quality requirements are specified in industrial practice|ContextThis paper analyses a sub-contractor specification in the mobile handset domain.ObjectiveThe objective is to understand how quality requirements are specified and which types of requirements exist in a requirements specification from industry.MethodThe case study is performed in the mobile handset domain, where a requirements specification was analyzed by categorizing and characterizing the pertaining requirements.ResultsThe requirements specification is written in structured natural language with unique identifiers for the requirements. Of the 2178 requirements, 827 (38%) are quality requirements. Of the quality requirements, 56% are quantified, i.e., having a direct metric in the requirement. The variation across the different sub-domains within the requirements specification is large.ConclusionThe findings from this study suggest that methods for quality requirements need to encompass many aspects to comprehensively support working with quality requirements. Solely focusing on, for example, quantification of quality requirements might overlook important requirements since there are many quality requirements in the studied specification where quantification is not appropriate. 
55|7||Analyzing an automotive testing process with evidence-based software engineering|ContextEvidence-based software engineering (EBSE) provides a process for solving practical problems based on a rigorous research approach. The primary focus so far was on mapping and aggregating evidence through systematic reviews.ObjectivesWe extend existing work on evidence-based software engineering by using the EBSE process in an industrial case to help an organization to improve its automotive testing process. With this we contribute in (1) providing experiences on using evidence based processes to analyze a real world automotive test process and (2) provide evidence of challenges and related solutions for automotive software testing processes.MethodsIn this study we perform an in-depth investigation of an automotive test process using an extended EBSE process including case study research (gain an understanding of practical questions to define a research scope), systematic literature review (identify solutions through systematic literature), and value stream mapping (map out an improved automotive test process based on the current situation and improvement suggestions identified). These are followed by reflections on the EBSE process used.ResultsIn the first step of the EBSE process we identified 10 challenge areas with a total of 26 individual challenges. For 15 out of those 26 challenges our domain specific systematic literature review identified solutions. Based on the input from the challenges and the solutions, we created a value stream map of the current and future process.ConclusionsOverall, we found that the evidence-based process as presented in this study helps in technology transfer of research results to industry, but at the same time some challenges lie ahead (e.g. scoping systematic reviews to focus more on concrete industry problems, and understanding strategies of conducting EBSE with respect to effort and quality of the evidence). 
55|7||Quality evaluation of floss projects: Application to ERP systems|ContextThe selection and adoption of open source software can significantly influence the competitiveness of organisations. Open source software solutions offer great opportunities for cost reduction and quality improvement, especially for small and medium enterprises that typically have to address major difficulties due to the limited resources available for selecting and adopting a new software system.ObjectiveThis paper aims to provide support for selecting the open source software that is most suitable to the specific needs of an enterprise from among the options offering equivalent or overlapping functionality.MethodThis paper proposes a framework for evaluating the quality and functionality of open source software systems. The name of the framework is EFFORT (Evaluation Framework for Free/Open souRce projecTs). It supports the evaluation of product quality, community trustworthiness and product attractiveness. The framework needs to be customised to the analysis of software systems for a specific context.ResultsThe paper presents the customisation of EFFORT for evaluating Enterprise Resource Planning (ERP) open source software systems. The customised framework was applied to the evaluation and comparison of five ERP open source software systems. The results obtained permitted both the refinement of the measurement framework and the identification of the ERP open source software system that achieved the highest score for each chosen characteristic.ConclusionEFFORT is a useful tool for evaluating and selecting an open source software system. It may significantly reduce the amount of negotiation conducted among an enterprise’s members and reduce the time and cost required for gathering and interpreting data. The EFFORT framework also considers the users’ opinions by introducing relevance markers associated with the metrics and questions in the data aggregation process. 
55|7||Guest Editorial for Special Section from Empirical Software Engineering & Measurement (ESEM) 2011|
55|7||A comparison of the efficiency and effectiveness of vulnerability discovery techniques|ContextSecurity vulnerabilities discovered later in the development cycle are more expensive to fix than those discovered early. Therefore, software developers should strive to discover vulnerabilities as early as possible. Unfortunately, the large size of code bases and lack of developer expertise can make discovering software vulnerabilities difficult. A number of vulnerability discovery techniques are available, each with their own strengths.ObjectiveThe objective of this research is to aid in the selection of vulnerability discovery techniques by comparing the vulnerabilities detected by each and comparing their efficiencies.MethodWe conducted three case studies using three electronic health record systems to compare four vulnerability discovery techniques: exploratory manual penetration testing, systematic manual penetration testing, automated penetration testing, and automated static analysis.ResultsIn our case study, we found empirical evidence that no single technique discovered every type of vulnerability. We discovered that the specific set of vulnerabilities identified by one tool was largely orthogonal to that of other tools. Systematic manual penetration testing found the most design flaws, while automated static analysis found the most implementation bugs. The most efficient discovery technique in terms of vulnerabilities discovered per hour was automated penetration testing.ConclusionThe results show that employing a single technique for vulnerability discovery is insufficient for finding all types of vulnerabilities. Each technique identified only a subset of the vulnerabilities, which, for the most part were independent of each other. Our results suggest that in order to discover the greatest variety of vulnerability types, at least systematic manual penetration testing and automated static analysis should be performed. 
55|7||Discovering how end-user programmers and their communities use public repositories: A study on Yahoo! Pipes|ContextEnd-user programmers are numerous, write software that matters to an increasingly large number of users, and face software engineering challenges that are similar to their professionals counterparts. Yet, we know little about how these end-user programmers create and share artifacts in repositories as part of a community.ObjectiveThis work aims to gain a better understanding of end-user programmer communities, the characteristics of artifacts in community repositories, and how authors evolve over time.MethodAn artifact-based analysis of 32,000 mashups from the Yahoo! Pipes repository was performed. The popularity, configurability, complexity, and diversity of the artifacts were measured. Additionally, for the most prolific authors, we explore their submission trends over time.ResultsSimilar to other online communities, there is great deal of attrition but authors who persevere tend to improve over time, creating pipes that are more configurable, diverse, complex, and popular. We also discovered, however, that end-user programmers do not effectively reuse existing programs, submit pipes that are highly similar to others already in the repository, and in most cases do not have an awareness of the community or the richness of artifacts that exist in repositories.ConclusionThere is a need for better end-user programmer support in several stages of the software lifecycle, including development, maintenance, search, and program understanding. Without such support, the community repositories will continue to be cluttered with highly-similar artifacts and authors may not be able to take full advantage of the community resources. 
55|7||Design of an empirical study for comparing the usability of concurrent programming languages|Context: Developing concurrent software has long been recognized as a difficult and error-prone task. To support developers, a multitude of language proposals exist that promise to make concurrent programming easier. Empirical studies are needed to support the claim that a language is more usable than another.Objective: This paper presents the design of a study to compare concurrent programming languages with respect to comprehending and debugging existing programs and writing correct new programs. The design is applied to a comparison of two object-oriented languages for concurrency, multithreaded Java and SCOOP.Method: A critical challenge for such a study is avoiding the bias that might be introduced during the training phase and when interpreting participants’ solutions. We address these issues by the use of self-study material and an evaluation scheme that exposes any subjective decisions of the corrector, or eliminates them altogether.Results: The study template consisting of the experimental design and the structure of the self-study and evaluation material is demonstrated to work successfully in an academic setting. The concrete instantiation of the study template shows results in favor of SCOOP even though the study participants had previous training in writing multithreaded Java programs.Conclusion: It is concluded that the proposed template of a small but therefore easy-to-implement empirical study with a focus on core language constructs is helpful in characterizing the usability of concurrent programming paradigms. Applying the template to further languages could shed light on which approaches are promising and hence drive language research into the right direction. 
55|7||Team building criteria in software projects: A mix-method replicated study|ContextThe internal composition of a work team is an important antecedent of team performance and the criteria used to select team members play an important role in determining team composition. However, there are only a handful of empirical studies about the use of team building criteria in the software industry.ObjectiveThe goal of this article is to identify criteria used in industrial practice to select members of a software project team, and to look for relationships between the use of these criteria and project success. In addition, we expect to contribute with findings about the use of replication in empirical studies involving human factors in software engineering.MethodOur research was based on an iterative mix-method, replication strategy. In the first iteration, we used qualitative research to identify team-building criteria interviewing software project managers from industry. Then, we performed a cross-sectional survey to assess the correlations of the use of these criteria and project success. In the second iteration, we used the results of a systematic mapping study to complement the set of team building criteria. Finally, we performed a replication of the survey research with variations to verify and improve the results.ResultsOur results showed that the consistent use team building criteria correlated significantly with project success, and the criteria related to human factors, such as personality and behavior, presented the strongest correlations. The results of the replication did not reproduce the results of the original survey with respect to the correlations between criteria and success goals. Nevertheless, the variations in the design and the difference in the sample of projects allowed us to conclude that the two results were compatible, increasing our confidence on the existence of the correlations.ConclusionOur findings indicated that carefully selecting team member for software teams is likely to positively influence the projects in which these teams participate. Besides, it seems that the type of development method used can moderate (increase or decrease) this influence. In addition, our study showed that the choice of sampling technique is not straightforward given the many interacting factors affecting this type of investigation. 
55|7||Systematic reviews in software engineering: An empirical investigation| BackgroundSystematic Literature Reviews (SLRs) have gained significant popularity among Software Engineering (SE) researchers since 2004. Several researchers have also been working on improving the scientific and methodological infrastructure to support SLRs in SE. We argue that there is also an apparent and essential need for evidence-based body of knowledge about different aspects of the adoption of SLRs in SE.ObjectiveThe main objective of this research is to empirically investigate the adoption, value, and use of SLRs in SE research from various perspectives.MethodWe used mixed-methods approach (systematically integrating tertiary literature review, semi-structured interviews and questionnaire-based survey) as it is based on a combination of complementary research methods which are expected to compensate each others’ limitations.ResultsA large majority of the participants are convinced of the value of using a rigourous and systematic methodology for literature reviews in SE research. However, there are concerns about the required time and resources for SLRs. One of the most important motivators for performing SLRs is new findings and inception of innovative ideas for further research. The reported SLRs are more influential compared to the traditional literature reviews in terms of number of citations. One of the main challenges of conducting SLRs is drawing a balance between methodological rigour and required effort.ConclusionsSLR has become a popular research methodology for conducting literature review and evidence aggregation in SE. There is an overall positive perception about this relatively new methodology to SE research. The findings provide interesting insights into different aspects of SLRs. We expect that the findings can provide valuable information to readers about what can be expected from conducting SLRs and the potential impact of such reviews. 
55|8|http://www.sciencedirect.com/science/journal/09505849/55/8|The use of software product lines for business process management: A systematic literature review|ContextBusiness Process Management (BPM) is a potential domain in which Software Product Line (PL) can be successfully applied. Including the support of Service-oriented Architecture (SOA), BPM and PL may help companies achieve strategic alignment between business and IT.ObjectivePresenting the results of a study undertaken to seek and assess PL approaches for BPM through a Systematic Literature Review (SLR). Moreover, identifying the existence of dynamic PL approaches for BPM.MethodA SLR was conducted with four research questions formulated to evaluate PL approaches for BPM.Results63 papers were selected as primary studies according to the criteria established. From these primary studies, only 15 papers address the specific dynamic aspects in the context evaluated. Moreover, it was found that PLs only partially address the BPM lifecycle since the last business process phase is not a current concern on the found approaches.ConclusionsThe found PL approaches for BPM only cover partially the BPM lifecycle, not taking into account the last phase which restarts the lifecycle. Moreover, no wide dynamic PL proposal was found for BPM, but only the treatment of specific dynamic aspects. The results indicate that PL approaches for BPM are still at an early stage and gaining maturity. 
55|8||A systematic mapping study of web application testing|ContextThe Web has had a significant impact on all aspects of our society. As our society relies more and more on the Web, the dependability of web applications has become increasingly important. To make these applications more dependable, for the past decade researchers have proposed various techniques for testing web-based software applications. Our literature search for related studies retrieved 147 papers in the area of web application testing, which have appeared between 2000 and 2011.ObjectiveAs this research area matures and the number of related papers increases, it is important to systematically identify, analyze, and classify the publications and provide an overview of the trends in this specialized field.MethodWe review and structure the body of knowledge related to web application testing through a systematic mapping (SM) study. As part of this study, we pose two sets of research questions, define selection and exclusion criteria, and systematically develop and refine a classification schema. In addition, we conduct a bibliometrics analysis of the papers included in our study.ResultsOur study includes a set of 79 papers (from the 147 retrieved papers) published in the area of web application testing between 2000 and 2011. We present the results of our systematic mapping study. Our mapping data is available through a publicly-accessible repository. We derive the observed trends, for instance, in terms of types of papers, sources of information to derive test cases, and types of evaluations used in papers. We also report the demographics and bibliometrics trends in this domain, including top-cited papers, active countries and researchers, and top venues in this research area.ConclusionWe discuss the emerging trends in web application testing, and discuss the implications for researchers and practitioners in this area. The results of our systematic mapping can help researchers to obtain an overview of existing web application testing approaches and indentify areas in the field that require more attention from the research community. 
55|8||Software fault prediction metrics: A systematic literature review|ContextSoftware metrics may be used in fault prediction models to improve software quality by predicting fault location.ObjectiveThis paper aims to identify software metrics and to assess their applicability in software fault prediction. We investigated the influence of context on metrics’ selection and performance.MethodThis systematic literature review includes 106 papers published between 1991 and 2011. The selected papers are classified according to metrics and context properties.ResultsObject-oriented metrics (49%) were used nearly twice as often compared to traditional source code metrics (27%) or process metrics (24%). Chidamber and Kemerer’s (CK) object-oriented metrics were most frequently used. According to the selected studies there are significant differences between the metrics used in fault prediction performance. Object-oriented and process metrics have been reported to be more successful in finding faults compared to traditional size and complexity metrics. Process metrics seem to be better at predicting post-release faults compared to any static code metrics.ConclusionMore studies should be performed on large industrial software systems to find metrics more relevant for the industry and to answer the question as to which metrics should be used in a given context. 
55|8||Applying a selection method to choose Quality Attribute Techniques|ContextSoftware products have requirements on software quality attributes such as safety and performance. Development teams use various specific techniques to achieve these quality requirements. We call these “Quality Attribute Techniques” (QATs). QATs are used to identify, analyse and control potential product quality problems. Although QATs are widely used in practice, there is no systematic approach to represent, select, and integrate them in existing approaches to software process modelling and tailoring.ObjectiveThis research aims to provide a systematic approach to better select and integrate QATs into tailored software process models for projects that develop products with specific product quality requirements.MethodA selection method is developed to support the choice of appropriate techniques for any quality attribute, across the lifecycle. The selection method is based on three perspectives: (1) risk management; (2) process integration; and (3) cost/benefit using Analytic Hierarchy Process (AHP). An industry case study is used to validate the feasibility and effectiveness of applying the selection method.ResultsThe case study demonstrates that the selection method provides a more methodological and effective approach to choose QATs for projects that target a specific quality attribute, compared to the ad hoc selection performed by development teams.ConclusionThe proposed selection method can be used to systematically choose QATs for projects to target specific product qualities throughout the software development lifecycle. 
55|8||FCAâCIA: An approach of using FCA to support cross-level change impact analysis for object oriented Java programs|BackgroundSoftware Change Impact Analysis (CIA) is an essential technique in software engineering to identifying the potential influences of a change, or determining change entities to accomplish such a change. The results derived, in many cases, ambiguous for the software maintainers, introduces the problem of unclear starting point of these impacted entities.ObjectiveIn an attempt to address this issue, this work proposes a novel approach for cross-level CIA, producing a ranked list of potentially impacted methods derived from class-level changes. Moreover, the approach of ranking the impact results is expected to be effective for maintainers to distinguish the probability of the impacted methods to be false-positives. Such results provide an eclectic approach for CIA.MethodThe approach, FCA–CIA, uses formal concept analysis (FCA) to produce an intermediate representation of the program based on the static analysis of the source code. The representation is called Lattice of Class and Method Dependence (LoCMD). FCA–CIA takes the changed classes in the change set as a whole, and determines the reachable set from the changed classes on the LoCMD. Based on the hierarchical property of the LoCMD, the impacted methods are ranked according to the impact factor metric which corresponds to the priority of these methods to be inspected.ResultEmpirical evaluations on four real-world software projects demonstrate the effectiveness of the impact factor metric and the FCA–CIA technique. The result shows the predicted impacted methods with higher impact factor values are more likely to be affected by the changes. Our study also shows that the FCA–CIA technique generates more accurate impact set than the JRipples and ICP coupling based CIA technique. 
55|8||Investigating measurement scales and aggregation methods in SPICE assessment method|ContextThis study identified three issues in SPICE (Software Process Assessment and Capability dEtermination) assessment method based on ISO/IEC 15504-2 (Performing an assessment). The issues include a lack of a measurement scale for characterizing the extent to which an outcome (practice) is achieved (implemented) and two shortcomings of the aggregation methods in order to generate a process attribute (PA) rating. Such issues may be weaknesses to the needs of retaining consistent assessment results comparable within and across assessed organizations.ObjectiveThe purpose of this study is to identify issues, such as the measurement scale and aggregation methods, in SPICE assessment methods and to provide candidate solutions based on measurement theories, while the rating scales of the current PA and capability are retained.MethodFor those purposes, the present study reviews scale types based on a measurement theory and uses the reflective and formative measurement models in order to find the relationships between PAs and practices. Composite measure development methods that are dependent on the relationships are then proposed on the basis of appropriate aggregation methods by using multiple attribute decision making (MADM) methods.ResultsSix candidate solutions are presented along with their strengths and weaknesses based on practical and theoretical perspectives. Two examples are given to illustrate and interpret six candidate solutions for the issues identified. By applying six candidate solutions to the examples shows that the measurement scale and the aggregation methods influence the PA rating.ConclusionThe process community, including the SPICE standardization group, can initiate discussions in order to determine the measurement scale and the aggregation methods with our six candidate solutions. The rationale and methods addressed in this study can also be applied to other domains in order to derive a composite (aggregate) value or rating. 
55|8||Static analysis of source code security: Assessment of tools against SAMATE tests|ContextStatic analysis tools are used to discover security vulnerabilities in source code. They suffer from false negatives and false positives. A false positive is a reported vulnerability in a program that is not really a security problem. A false negative is a vulnerability in the code which is not detected by the tool.ObjectiveThe main goal of this article is to provide objective assessment results following a well-defined and repeatable methodology that analyzes the performance detecting security vulnerabilities of static analysis tools. The study compares the performance of nine tools (CBMC, K8-Insight, PC-lint, Prevent, Satabs, SCA, Goanna, Cx-enterprise, Codesonar), most of them commercials tools, having a different design.MethodWe executed the static analysis tools against SAMATE Reference Dataset test suites 45 and 46 for C language. One includes test cases with known vulnerabilities and the other one is designed with specific vulnerabilities fixed. Afterwards, the results are analyzed by using a set of well known metrics.ResultsOnly SCA is designed to detect all vulnerabilities considered in SAMATE. None of the tools detect “cross-site scripting” vulnerabilities. The best results for F-measure metric are obtained by Prevent, SCA and K8-Insight. The average precision for analyzed tools is 0.7 and the average recall is 0.527. The differences between all tools are relevant, detecting different kinds of vulnerabilities.ConclusionsThe results provide empirical evidences that support popular propositions not objectively demonstrated until now. The methodology is repeatable and allows ranking strictly the analyzed static analysis tools, in terms of vulnerabilities coverage and effectiveness for detecting the highest number of vulnerabilities having few false positives. Its use can help practitioners to select appropriate tools for a security review process of code. We propose some recommendations for improving the reliability and usefulness of static analysis tools and the process of benchmarking. 
55|8||Guest editorial for the Special Section on BEST PAPERS from the 2011 conference on Predictive Models in Software Engineering (PROMISE)|
55|8||Predicting failure-proneness in an evolving software product line|ContextPrevious work by researchers on 3 years of early data for an Eclipse product has identified some predictors of failure-prone files that work well. Eclipse has also been used previously by researchers to study characteristics of product line software.ObjectiveThe work reported here investigates whether classification-based prediction of failure-prone files improves as the product line evolves.MethodThis investigation first repeats, to the extent possible, the previous study and then extends it by including four more recent years of data, comparing the prominent predictors with the previous results. The research then looks at the data for three additional Eclipse products as they evolve over time. The analysis compares results from three different types of datasets with alternative data collection and prediction periods.ResultsOur experiments with a variety of learners show that the difference between the performance of J48, used in this work, and the other top learners is not statistically significant. Furthermore, new results show that the effectiveness of classification significantly depends on the data collection period and prediction period. The study identifies change metrics that are prominent predictors across all four releases of all four products in the product line for the three different types of datasets. From the product line perspective, prediction of failure-prone files for the four products studied in the Eclipse product line shows statistically significant improvement in accuracy but not in recall across releases.ConclusionAs the product line matures, the learner performance improves significantly for two of the three datasets, but not for prediction of post-release failure-prone files using only pre-release change data. This suggests that it may be difficult to detect failure-prone files in the evolving product line. At least in part, this may be due to the continuous change, even for commonalities and high-reuse variation components, which we previously have shown to exist. 
55|8||Analyzing and handling local bias for calibrating parametric cost estimation models|ContextParametric cost estimation models need to be continuously calibrated and improved to assure more accurate software estimates and reflect changing software development contexts. Local calibration by tuning a subset of model parameters is a frequent practice when software organizations adopt parametric estimation models to increase model usability and accuracy. However, there is a lack of understanding about the cumulative effects of such local calibration practices on the evolution of general parametric models over time.ObjectiveThis study aims at quantitatively analyzing and effectively handling local bias associated with historical cross-company data, thus improves the usability of cross-company datasets for calibrating and maintaining parametric estimation models.MethodWe design and conduct three empirical studies to measure, analyze and address local bias in cross-company dataset, including: (1) defining a method for measuring the local bias associated with individual organization data subset in the overall dataset; (2) analyzing the impacts of local bias on the performance of an estimation model; (3) proposing a weighted sampling approach to handle local bias. The studies are conducted on the latest COCOMO II calibration dataset.ResultsOur results show that the local bias largely exists in cross company dataset, and the local bias negatively impacts the performance of parametric model. The local bias based weighted sampling technique helps reduce negative impacts of local bias on model performance.ConclusionLocal bias in cross-company data does harm model calibration and adds noisy factors to model maintenance. The proposed local bias measure offers a means to quantify degree of local bias associated with a cross-company dataset, and assess its influence on parametric model performance. The local bias based weighted sampling technique can be applied to trade-off and mitigate potential risk of significant local bias, which limits the usability of cross-company data for general parametric model calibration and maintenance. 
55|8||Ensembles and locality: Insight on improving software effort estimation|ContextEnsembles of learning machines and locality are considered two important topics for the next research frontier on Software Effort Estimation (SEE).ObjectivesWe aim at (1) evaluating whether existing automated ensembles of learning machines generally improve SEEs given by single learning machines and which of them would be more useful; (2) analysing the adequacy of different locality approaches; and getting insight on (3) how to improve SEE and (4) how to evaluate/choose machine learning (ML) models for SEE.MethodA principled experimental framework is used for the analysis and to provide insights that are not based simply on intuition or speculation. A comprehensive experimental study of several automated ensembles, single learning machines and locality approaches, which present features potentially beneficial for SEE, is performed. Additionally, an analysis of feature selection and regression trees (RTs), and an investigation of two tailored forms of combining ensembles and locality are performed to provide further insight on improving SEE.ResultsBagging ensembles of RTs show to perform well, being highly ranked in terms of performance across different data sets, being frequently among the best approaches for each data set and rarely performing considerably worse than the best approach for any data set. They are recommended over other learning machines should an organisation have no resources to perform experiments to chose a model. Even though RTs have been shown to be more reliable locality approaches, other approaches such as k-Means and k-Nearest Neighbours can also perform well, in particular for more heterogeneous data sets.ConclusionCombining the power of automated ensembles and locality can lead to competitive results in SEE. By analysing such approaches, we provide several insights that can be used by future research in the area. 
55|9|http://www.sciencedirect.com/science/journal/09505849/55/9|Automated reasoning on UML conceptual schemas with derived information and queries|ContextIt is critical to ensure the quality of a software system in the initial stages of development, and several approaches have been proposed to ensure that a conceptual schema correctly describes the user’s requirements.ObjectiveThe main goal of this paper is to perform automated reasoning on UML schemas containing arbitrary constraints, derived roles, derived attributes and queries, all of which must be specified by OCL expressions.MethodThe UML/OCL schema is encoded in a first order logic formalisation, and an existing reasoning procedure is used to check whether the schema satisfies a set of desirable properties. Due to the undecidability of reasoning in highly expressive schemas, such as those considered here, we also provide a set of conditions that, if satisfied by the schema, ensure that all properties can be checked in a finite period of time.ResultsThis paper extends our previous work on reasoning on UML conceptual schemas with OCL constraints by considering derived attributes and roles that can participate in the definition of other constraints, queries and derivation rules. Queries formalised in OCL can also be validated to check their satisfiability and to detect possible equivalences between them. We also provide a set of conditions that ensure finite reasoning when they are satisfied by the schema under consideration.ConclusionThis approach improves upon previous work by allowing automated reasoning for more expressive UML/OCL conceptual schemas than those considered so far. 
55|9||Using argumentation to evaluate software assurance standards|ContextMany people and organisations rely upon software safety and security standards to provide confidence in software intensive systems. For example, people rely upon the Common Criteria for Information Technology Security Evaluation to establish justified and sufficient confidence that an evaluated information technology product’s contributions to security threats and threat management are acceptable. Is this standard suitable for this purpose?ObjectiveWe propose a method for assessing whether conformance with a software safety or security standard is sufficient to support a conclusion such as adequate safety or security. We hypothesise that our method is feasible and capable of revealing interesting issues with the proposed use of the assessed standard.MethodThe software safety and security standards with which we are concerned require evidence and discuss the objectives of that evidence. Our method is to capture a standard’s evidence and objectives as an argument supporting the desired conclusion and to subject this argument to logical criticism. We have evaluated our method by case study application to the Common Criteria standard.ResultsWe were able to capture and criticise an argument from the Common Criteria standard. Review revealed 121 issues with the analysed use of the standard. These range from vagueness in its text to failure to require evidence that would substantially increase confidence in the security of evaluated software.ConclusionOur method was feasible and revealed interesting issues with using a Common Criteria evaluation to support a conclusion of adequate software security. Considering the structure of similar assurance standards, we see no reason to believe that our method will not prove similarly valuable in other applications. 
55|9||ODEP-DPS: Ontology-driven engineering process for the collaborative development of semantic data providing services|ContextData services are services that handle operations involving the management of data. A problem with data services is that their interfaces are defined by their syntax alone. Consequently, Data Providing Services (DPSs) have been proposed to explicitly define semantics using ontologies for services that only retrieve data. However, the semantic annotations of DPSs are developed as afterthoughts to deployed data services.ObjectiveThe objective of this work is to present a DPS development process that considers all of a DPS’s dimensions including its data acquisition logic, syntax and semantics thus addressing the issue of semantic annotations developed as afterthoughts. This shall decrease the cost of deploying and maintaining DPSs.MethodThis paper contributes a holistic and collaborative process – ODEP-DPS – for the development of DPSs. It is holistic as it considers both semantics and syntax from requirements to implementation. And it is collaborative as it separates responsibilities between the roles of those who require the data and those who own them. The process is to be ontology-driven as an ontological model shall be utilized through each phase of the process; it shall formalize the requirements domain, be used as a basis for the syntactic data model, and serve as the domain ontology for annotating the deployed DPSs.ResultsThis paper proposes the ODEP-DPS development process, in addition to defining three artefacts used throughout the process. In particular, a message descriptor is defined that binds semantics and syntax into a single reusable unit. A comprehensive definition of a DPS is also provided. ODEP-DPS is evaluated using a real-life case study from a mental health institution.ConclusionThis study contributes a holistic and collaborative development process that provides an end-to-end solution for the development of semantic data providing services. It addresses semantics being developed as afterthoughts by tightly coupling semantics and syntax. 
55|9||Standardizing the narrative of use cases: A controlled vocabulary of web user tasks|ContextFor user interfaces design, the use of appropriate terminology in writing use case narratives may determine the effectiveness of the design process, facilitating communication within multidisciplinary web development teams and leading to agreed designs.ObjectiveThis paper proposes a user task vocabulary for web user interface design. This vocabulary compiles terms related to the use of web applications, regardless of the application domain, as a way of standardizing the terminology used in the elaboration of use case narrative. The use of the vocabulary would help to reduce misunderstandings within multidisciplinary web development teams.MethodThe construction of the vocabulary is based on the combination of both committee and empirical approaches. Committee approaches rely on experts in designing web applications, while empirical ones are focused on analyzing content objects, such as interaction design patterns and use case narratives.ResultsThe final version of the monolingual controlled vocabulary of web user tasks compiles a total of 40 tasks; each of them has a key term and a definition of the interaction carried out by users. Additionally, 41 semantic relationships were collected and represented as synonyms. The vocabulary has been assessed through an expert evaluation, proving its correctness and completeness, and an usability evaluation checking the efficacy of the vocabulary.ConclusionThe language employed in use case narratives can determine the value of use cases for designing user interfaces. The usage of a controlled vocabulary may allow designers to elaborate unambiguous use case narratives as a way of creating consistent task models for designing web user interfaces. 
55|9||Determining the effectiveness of three software evaluation techniques through informal aggregation|ContextAn accepted fact in software engineering is that software must undergo verification and validation process during development to ascertain and improve its quality level. But there are too many techniques than a single developer could master, yet, it is impossible to be certain that software is free of defects. So, it is crucial for developers to be able to choose from available evaluation techniques, the one most suitable and likely to yield optimum quality results for different products. Though, some knowledge is available on the strengths and weaknesses of the available software quality assurance techniques but not much is known yet on the relationship between different techniques and contextual behavior of the techniques.ObjectiveThis research investigates the effectiveness of two testing techniques – equivalence class partitioning and decision coverage and one review technique – code review by abstraction, in terms of their fault detection capability. This will be used to strengthen the practical knowledge available on these techniques.MethodThe results of eight experiments conducted over 5 years to investigate the effectiveness of three techniques – code reading by stepwise abstraction, equivalence class partitioning and decision (branch) coverage were aggregated using a less rigorous aggregation process proposed during the course of this work.ResultsIt was discovered that the equivalence class partitioning and the decision coverage techniques behaved similarly in terms of fault detection capacity (and type of faults caught) based on the programs and fault classification used in the experiments. They both behaved better than the code reading by stepwise abstraction technique.ConclusionOverall, it can be deducted from the aggregation results that the equivalence class partitioning and the decision coverage techniques used are actually equally capable in terms of the type and number of faults detected. Nevertheless, more experiments is still required in this field so that this result can be verified using a rigorous aggregation technique. 
55|9||The impact of distributed programming abstractions on application energy consumption|With battery capacities remaining a key physical constraint for mobile devices, energy efficiency has become an important software design consideration. Distributed programming abstractions (e.g., sockets, RPC, messages, etc.) are an essential component of modern software, but their energy consumption characteristics are poorly understood. The programmer has few practical guidelines to choose the right abstraction for energy-constrained scenarios. In this article, we report on the findings of a systematic study we conducted to compare and contrast major distributed programming abstractions in terms of their energy consumption patterns. By varying the abstractions with the rest of the functionality fixed, we measure and analyze the impact of distributed programming abstractions on application energy consumption. Based on our findings, we present a set of practical guidelines for the programmer to select an abstraction that satisfies the energy consumption constraints in place. Our other guidelines can steer future efforts in creating energy efficient distributed programming abstractions. 
55|9||Acquiring and sharing tacit knowledge in software development teams: An empirical study|ContextSharing expert knowledge is a key process in developing software products. Since expert knowledge is mostly tacit, the acquisition and sharing of tacit knowledge along with the development of a transactive memory system (TMS) are significant factors in effective software teams.ObjectiveWe seek to enhance our understanding human factors in the software development process and provide support for the agile approach, particularly in its advocacy of social interaction, by answering two questions: How do software development teams acquire and share tacit knowledge? What roles do tacit knowledge and transactive memory play in successful team performance?MethodA theoretical model describing the process for acquiring and sharing tacit knowledge and development of a TMS through social interaction is presented and a second predictive model addresses the two research questions above. The elements of the predictive model and other demographic variables were incorporated into a larger online survey for software development teams, completed by 46 software SMEs, consisting of 181 individual team members.ResultsOur results show that team tacit knowledge is acquired and shared directly through good quality social interactions and through the development of a TMS with quality of social interaction playing a greater role than transactive memory. Both TMS and team tacit knowledge predict effectiveness but not efficiency in software teams.ConclusionIt is concluded that TMS and team tacit knowledge can differentiate between low- and high-performing teams in terms of effectiveness, where more effective teams have a competitive advantage in developing new products and bringing them to market. As face-to-face social interaction is key, collocated, functionally rich, domain expert teams are advocated rather than distributed teams, though arguably the team manager may be in a separate geographic location provided that there is frequent communication and effective use of issue tracking tools as in agile teams. 
55|9||Multidimensional characterization of evolutionary clusters: An experience report|Context: Software architects try to decompose software systems such that their constituent parts can evolve independently from one another. During the actual evolution, identifying groups of software entities from different parts which frequently changed together, is one way to help architects evaluate how independently the different parts can evolve. However, there may be many such groups from which the architects have to select the ones denoting issues in the decomposition worth addressing.ObjectiveIn this paper, we use a number of properties of those groups of entities, such as whether they involve more than one site, how many entities are involved, how often these entities are changed, and so on, to identify a subset of groups indicating issues the architect does want to address.MethodWe describe (1) a number of properties of groups of co-changing entities, (2) scenarios expressing issues to be addressed, in terms of values for the different properties, and (3) the mapping of such scenarios to queries on a set of groups of software entities that changed together. Executing that query results in a subset pointing to issues worth addressing according to that scenario.ResultsWe apply our method to a large embedded software system having a development history of more than a decade. For several scenarios worked out, the number of issues selected for further investigation by the architect is less than half the number of issues selected using only one or two properties.ConclusionOur experience suggests that using multiple properties of groups of co-changing entities is a useful way to accurately identify the set of issues an architect has to address when improving the decomposition of a system. 
55|9||The influence of selection bias on effort overruns in software development projects|ContextA potentially important, but neglected, reason for effort overruns in software projects is related to selection bias. Selection bias–induced effort overruns occur when proposals are more likely to be accepted and lead to actual projects when based on effort estimates that are too low rather than on realistic estimates or estimates that are too high. The effect of this bias may be particularly important in bidding rounds, but is potentially relevant in all situations where there is effort or cost-based selection between alternatives.ObjectiveTo better understand the relevance and management of selection bias effects in software development contexts.MethodFirst, we present a statistical model illustrating the relation between selection bias in bidding and other contexts and effort overruns. Then, we examine this relation in an experiment with software professionals who estimated and completed a set of development tasks and examine relevant field study evidence. Finally, we use a selection bias scenario to assess awareness of the effect of selection bias among software providers.ResultsThe results from the statistical model and the experiment demonstrated that selection bias is capable of explaining much of the effort overruns. The field evidence was also consistent with a substantial effect of selection bias on effort overruns, although there are alternative explanations for the findings. We found a low awareness of selection bias among the software providers.ConclusionSelection bias is likely to be an important source of effort overruns and should be addressed to reduce problems related to over-optimistic effort estimates. 
55|9||Development of Secure XML Data Warehouses with QVT|ContextData warehouses are systems which integrate heterogeneous sources to support the decision making process. Data from the Web is becoming increasingly more important as sources for these systems, which has motivated the extensive use of XML to facilitate data and metadata interchange among heterogeneous data sources from the Web and the data warehouse. However, the business information that data warehouses manage is highly sensitive and must, therefore, be carefully protected. Security is thus a key issue in the design of data warehouses, regardless of the implementation technology. It is important to note that the idiosyncrasy of the unstructured and semi-structured data requires particular security rules that have been specifically tailored to these systems in order to permit their particularities to be captured correctly. Unfortunately, although security issues have been considered in the development of traditional data warehouses, current research lacks approaches with which to consider security when the target platform is based on XML technology.ObjectiveWe shall focus on defining transformations to obtain a secure XML Schema from the conceptual multidimensional model of a data warehouse.MethodWe have first defined the rationale behind the transformation rules and how they have been developed in natural language, and we have then established them clearly and formally by using the QVT language. Finally, in order to validate our proposal we have carried out a case study.ResultsWe have proposed an approach for the model driven development of Secure XML Data Warehouses, defining a set of QVT transformation rules.ConclusionThe main benefit of our proposal is that it is possible to model security requirements together with the conceptual model of the data warehouse during the early stages of a project, and automatically obtain the corresponding implementation for XML. 
56|1|http://www.sciencedirect.com/science/journal/09505849/56/1|Global software engineering: Identifying challenges is important and providing solutions is even better|
56|1||Global software testing under deadline pressure: Vendor-side experiences|ContextIn the era of globally-distributed software engineering, the practice of global software testing (GST) has witnessed increasing adoption. Although there have been ethnographic studies of the development aspects of global software engineering, there have been fewer studies of GST, which, to succeed, can require dealing with unique challenges.ObjectiveTo address this limitation of existing studies, we conducted, and in this paper, report the findings of, a study of a vendor organization involved in one kind of GST practice: outsourced, offshored software testing.MethodWe conducted an ethnographically-informed study of three vendor-side testing teams over a period of 2 months. We used methods, such as interviews and participant observations, to collect the data and the thematic-analysis approach to analyze the data.FindingsOur findings describe how the participant test engineers perceive software testing and deadline pressures, the challenges that they encounter, and the strategies that they use for coping with the challenges. The findings reveal several interesting insights. First, motivation and appreciation play an important role for our participants in ensuring that high-quality testing is performed. Second, intermediate onshore teams increase the degree of pressure experienced by the participant test engineers. Third, vendor team participants perceive productivity differently from their client teams, which results in unproductive-productivity experiences. Lastly, participants encounter quality-dilemma situations for various reasons.ConclusionThe study findings suggest the need for (1) appreciating test engineers’ efforts, (2) investigating the team structure’s influence on pressure and the GST practice, (3) understanding culture’s influence on other aspects of GST, and (4) identifying and addressing quality-dilemma situations. 
56|1||Software quality across borders: Three case studies on company internal alignment|ContextSoftware quality issues are commonly reported when offshoring software development. Value-based software engineering addresses this by ensuring key stakeholders have a common understanding of quality.ObjectiveThis work seeks to understand the levels of alignment between key stakeholder groups within a company on the priority given to aspects of software quality developed as part of an offshoring relationship. Furthermore, the study aims to identify factors impacting the levels of alignment identified.MethodThree case studies were conducted, with representatives of key stakeholder groups ranking aspects of software quality in a hierarchical cumulative exercise. The results are analysed using Spearman rank correlation coefficients and inertia. The results were discussed with the groups to gain a deeper understanding of the issues impacting alignment.ResultsVarious levels of alignment were found between the various groups. The reasons for misalignment were found to include cultural factors, control of quality in the development process, short-term versus long-term orientations, understanding of cost-benefits of quality improvements, communication and coordination.ConclusionsThe factors that negatively affect alignment can vary greatly between different cases. The work emphasises the need for greater support to align company internal success-critical stakeholder groups in their understanding of quality when offshoring software development. 
56|1||Guest Editorial: Special section of the best papers from the 16th International Conference on Evaluation & Assessment in Software Engineering|
56|1||Investigating dependencies in software requirements for change propagation analysis|ContextThe dependencies between individual requirements have an important influence on software engineering activities e.g., project planning, architecture design, and change impact analysis. Although dozens of requirement dependency types were suggested in the literature from different points of interest, there still lacks an evaluation of the applicability of these dependency types in requirements engineering.ObjectiveUnderstanding the effect of these requirement dependencies to software engineering activities is useful but not trivial. In this study, we aimed to first investigate whether the existing dependency types are useful in practise, in particular for change propagation analysis, and then suggest improvements for dependency classification and definition.MethodWe conducted a case study that evaluated the usefulness and applicability of two well-known generic dependency models covering 25 dependency types. The case study was conducted in a real-world industry project with three participants who offered different perspectives.ResultsOur initial evaluation found that there exist a number of overlapping and/or ambiguous dependency types among the current models; five dependency types are particularly useful in change propagation analysis; and practitioners with different backgrounds possess various viewpoints on change propagation. To improve the state-of-the-art, a new dependency model is proposed to tackle the problems identified from the case study and the related literature. The new model classifies dependencies into intrinsic and additional dependencies on the top level, and suggests nine dependency types with precise definitions as its initial set.ConclusionsOur case study provides insights into requirement dependencies and their effects on change propagation analysis for both research and practise. The resulting new dependency model needs further evaluation and improvement. 
56|1||Risks and risk mitigation in global software development: A tertiary study|ContextThere is extensive interest in global software development (GSD) which has led to a large number of papers reporting on GSD. A number of systematic literature reviews (SLRs) have attempted to aggregate information from individual studies.ObjectiveWe wish to investigate GSD SLR research with a focus on discovering what research has been conducted in the area and to determine if the SLRs furnish appropriate risk and risk mitigation advice to provide guidance to organizations involved with GSD.MethodWe performed a broad automated search to identify GSD SLRs. Data extracted from each study included: (1) authors, their affiliation and publishing venue, (2) SLR quality, (3) research focus, (4) GSD risks, (5) risk mitigation strategies and, (6) for each SLR the number of primary studies reporting each risk and risk mitigation strategy.ResultsWe found a total of 37 papers reporting 24 unique GSD SLR studies. Major GSD topics covered include: (1) organizational environment, (2) project execution, (3) project planning and control and (4) project scope and requirements. We extracted 85 risks and 77 risk mitigation advice items and categorized them under four major headings: outsourcing rationale, software development, human resources, and project management. The largest group of risks was related to project management. GSD outsourcing rationale risks ranked highest in terms of primary study support but in many cases these risks were only identified by a single SLR.ConclusionsThe focus of the GSD SLRs we identified is mapping the research rather than providing evidence-based guidance to industry. Empirical support for the majority of risks identified is moderate to low, both in terms of the number of SLRs identifying the risk, and in the number of primary studies providing empirical support. Risk mitigation advice is also limited, and empirical support for these items is low. 
56|1||Motivation in software engineering industrial practice: A cross-case analysis of two software organisations|ContextThe research about motivation in software engineering has provided important insights into characterizing factors and outcomes related to motivation. However, the complex relationships among these factors, including the moderating and mediating effects of organisational and individual characteristics, still require deeper explanatory investigation.ObjectiveOur general goal is to build explanatory theories of motivation in different software organisations and to integrate these local theories towards a comprehensive understanding of the role of motivation in the effectiveness of the individuals and the teams in which they work. In this article, we describe the integrative synthesis of the results of two case studies performed with software organisations in different business contexts.MethodWe performed two case studies using a multiple-case, replication design, focusing on the software engineers as the unit of analysis. For 13 months, we conducted semi structured interviews, diary studies, and document analyses, and analysed the collected data using grounded theory procedures. The results of the two cases were synthesized using a meta-ethnography supported process.ResultsWe built translations of the concepts and propositions from the two studies into one another. We then used the translations to build a central story of motivation that synthesizes the individual stories. This synthesis is contextualized by the differences in organisational and individual characteristics.ConclusionThe differences in organisational contexts and in the characteristics of the software engineers in each study provided rich explanations for contrasts in perceptions and feelings about motivation in both organisations. The theory that emerged from the synthesis, supported by these explanations, provides a deeper understanding of the interplay between motivators and outcomes, and the needs and personal goals of the software engineers. This theory also characterises the role of team cohesion in motivation, advancing previous models about motivation in software engineering. 
56|10|http://www.sciencedirect.com/science/journal/09505849/56/10|Quality models for web services: A systematic mapping|ContextQuality of Service (QoS) is a major issue in various web service related activities. Quality models have been proposed as the engineering artefact to provide a common framework of understanding for QoS, by defining the quality factors that apply to web service usage.ObjectiveThe goal of this study is to evaluate the current state of the art of the proposed quality models for web services, specifically: (1) which are these proposals and how are they related; (2) what are their structural characteristics; (3) what quality factors are the most and least addressed; and (4) what are their most consolidated definitions.MethodWe have conducted a systematic mapping by defining a robust protocol that combines automatic and manual searches from different sources. We used a rigorous method to elicitate the keywords from the research questions and a selection criteria to retrieve the final papers to evaluate. We have adopted the ISO/IEC 25010 standard to articulate our analysis.ResultsWe have evaluated 47 different quality models from 65 papers that fulfilled the selection criteria. By analyzing in depth these quality models, we have: (1) distributed the proposals along the time dimension and identified their relationships; (2) analyzed their size (visualizing the number of nodes and levels) and definition coverage (as indicator of quality of the proposals); (3) quantified the coverage of the different ISO/IEC 25010 quality factors by the proposals; (4) identified the quality factors that appeared in at least 30% of the surveyed proposals and provided the most consolidated definitions for them.ConclusionsWe believe that this panoramic view on the anatomy of the quality models for web services may be a good reference for prospective researchers and practitioners in the field and especially may help avoiding the definition of new proposals that do not align with current research. 
56|10||On strategies for testing software product lines: A systematic literature review|ContextTesting plays an important role in the quality assurance process for software product line engineering. There are many opportunities for economies of scope and scale in the testing activities, but techniques that can take advantage of these opportunities are still needed.ObjectiveThe objective of this study is to identify testing strategies that have the potential to achieve these economies, and to provide a synthesis of available research on SPL testing strategies, to be applied towards reaching higher defect detection rates and reduced quality assurance effort.MethodWe performed a literature review of two hundred seventy-six studies published from the year 1998 up to the 1st1st semester of 2013. We used several filters to focus the review on the most relevant studies and we give detailed analyses of the core set of studies.ResultsThe analysis of the reported strategies comprised two fundamental aspects for software product line testing: the selection of products for testing, and the actual test of products. Our findings indicate that the literature offers a large number of techniques to cope with such aspects. However, there is a lack of reports on realistic industrial experiences, which limits the inferences that can be drawn.ConclusionThis study showed a number of leveraged strategies that can support both the selection of products, and the actual testing of products. Future research should also benefit from the problems and advantages identified in this study. 
56|10||Software development in startup companies: A systematic mapping study|ContextSoftware startups are newly created companies with no operating history and fast in producing cutting-edge technologies. These companies develop software under highly uncertain conditions, tackling fast-growing markets under severe lack of resources. Therefore, software startups present a unique combination of characteristics which pose several challenges to software development activities.ObjectiveThis study aims to structure and analyze the literature on software development in startup companies, determining thereby the potential for technology transfer and identifying software development work practices reported by practitioners and researchers.MethodWe conducted a systematic mapping study, developing a classification schema, ranking the selected primary studies according their rigor and relevance, and analyzing reported software development work practices in startups.ResultsA total of 43 primary studies were identified and mapped, synthesizing the available evidence on software development in startups. Only 16 studies are entirely dedicated to software development in startups, of which 10 result in a weak contribution (advice and implications (6); lesson learned (3); tool (1)). Nineteen studies focus on managerial and organizational factors. Moreover, only 9 studies exhibit high scientific rigor and relevance. From the reviewed primary studies, 213 software engineering work practices were extracted, categorized and analyzed.ConclusionThis mapping study provides the first systematic exploration of the state-of-art on software startup research. The existing body of knowledge is limited to a few high quality studies. Furthermore, the results indicate that software engineering work practices are chosen opportunistically, adapted and configured to provide value under the constrains imposed by the startup context. 
56|10||Testing scientific software: A systematic literature review|ContextScientific software plays an important role in critical decision making, for example making weather predictions based on climate models, and computation of evidence for research publications. Recently, scientists have had to retract publications due to errors caused by software faults. Systematic testing can identify such faults in code.ObjectiveThis study aims to identify specific challenges, proposed solutions, and unsolved problems faced when testing scientific software.MethodWe conducted a systematic literature survey to identify and analyze relevant literature. We identified 62 studies that provided relevant information about testing scientific software.ResultsWe found that challenges faced when testing scientific software fall into two main categories: (1) testing challenges that occur due to characteristics of scientific software such as oracle problems and (2) testing challenges that occur due to cultural differences between scientists and the software engineering community such as viewing the code and the model that it implements as inseparable entities. In addition, we identified methods to potentially overcome these challenges and their limitations. Finally we describe unsolved challenges and how software engineering researchers and practitioners can help to overcome them.ConclusionsScientific software presents special challenges for testing. Specifically, cultural differences between scientist developers and software engineers, along with the characteristics of the scientific software make testing more difficult. Existing techniques such as code clone detection can help to improve the testing process. Software engineers should consider special challenges posed by scientific software such as oracle problems when developing testing techniques. 
56|10||Knowledge transfer, translation and transformation in the work of information technology architects|ContextInformation Technology (IT) architects are the professionals responsible for designing the information systems for an organization. In order to do that, they take into account many aspects and stakeholders, including customers, software developers, the organization’s business, and its current IT infrastructure. Therefore, different aspects influence their work.ObjectiveThis paper presents results of research into how IT architects perform their work in practice and how different aspects are taken into account when an information system is developed. An understanding of IT architects’ activities allows us to better support their work. This paper extends our own previous work (Figueiredo et al., 2012) [30] by discussing aspects of knowledge management and tool support.MethodA qualitative study was conducted using semi-structured interviews for data collection and grounded theory methods (Strauss and Corbin, 1998) [5] for data analysis. Twenty-seven interviews were conducted with twenty-two interviewees from nine different companies through four cycles of data collection and analysis.ResultsCompanies divide IT architecture activities among different roles. Although these roles receive different names in different organizations, all organizations follow a similar pattern based on 3 roles: enterprise, solutions and software architects. These architects perform both the technical activities related to the IT architecture and the social activities regarding the communication and coordination with other stakeholders and among themselves. Furthermore, current tools used by IT architects lack adequate support for all these aspects.ConclusionThe activities of the different IT architects are highly interconnected and have a huge influence in the way the requirements are handled in every phase of the development of an information system. The activities of IT architects are also important for knowledge transfer, translation and transformation, since they receive from and spread information to different groups of stakeholders. We also conclude that they lack appropriate tool support, especially regarding support for their collaborative work. 
56|10||Model-based early and rapid estimation of COSMIC functional size â An experimental evaluation|ContextFunctional size measurement methods are widely used but have two major shortcomings: they require a complete and detailed knowledge of user requirements, and they involve relatively expensive and lengthy processes.ObjectiveUML is routinely used in the software industry to effectively describe software requirements in an incremental way, so UML models grow in detail and completeness through the requirements analysis phase. Here, we aim at defining the characteristics of increasingly more refined UML requirements models that support increasingly more sophisticated – hence presumably more accurate – size estimation processes.MethodWe consider the COSMIC method and three alternative processes (two of which are proposed in this paper) to estimate COSMIC size measures that can be applied to UML diagrams at progressive stages of the requirements definition phase. Then, we check the accuracy of the estimates by comparing the results obtained on a set of projects to the functional size values obtained with the standard COSMIC method.ResultsOur analysis shows that it is possible to write increasingly more detailed and complete UML models of user requirements that provide the data required by COSMIC size estimation methods, which in turn yield increasingly more accurate size measure estimates of the modeled software. Initial estimates are based on simple models and are obtained quickly and with little effort. The estimates increase their accuracy as models grow in completeness and detail, i.e., as the requirements definition phase progresses.ConclusionDevelopers that use UML for requirements modeling can obtain an early estimation of the application size at the beginning of the development process, when only a very simple UML model has been built for the application, and can obtain increasingly more accurate size estimates while the knowledge of the product increases and UML models are refined accordingly. 
56|10||A language-independent approach to the extraction of dependencies between source code entities|ContextSoftware networks are directed graphs of static dependencies between source code entities (functions, classes, modules, etc.). These structures can be used to investigate the complexity and evolution of large-scale software systems and to compute metrics associated with software design. The extraction of software networks is also the first step in reverse engineering activities.ObjectiveThe aim of this paper is to present SNEIPL, a novel approach to the extraction of software networks that is based on a language-independent, enriched concrete syntax tree representation of the source code.MethodThe applicability of the approach is demonstrated by the extraction of software networks representing real-world, medium to large software systems written in different languages which belong to different programming paradigms. To investigate the completeness and correctness of the approach, class collaboration networks (CCNs) extracted from real-world Java software systems are compared to CCNs obtained by other tools. Namely, we used Dependency Finder which extracts entity-level dependencies from Java bytecode, and Doxygen which realizes language-independent fuzzy parsing approach to dependency extraction. We also compared SNEIPL to fact extractors present in language-independent reverse engineering tools.ResultsOur approach to dependency extraction is validated on six real-world medium to large-scale software systems written in Java, Modula-2, and Delphi. The results of the comparative analysis involving ten Java software systems show that the networks formed by SNEIPL are highly similar to those formed by Dependency Finder and more precise than the comparable networks formed with the help of Doxygen. Regarding the comparison with language-independent reverse engineering tools, SNEIPL provides both language-independent extraction and representation of fact bases.ConclusionSNEIPL is a language-independent extractor of software networks and consequently enables language-independent network-based analysis of software systems, computation of design software metrics, and extraction of fact bases for reverse engineering activities. 
56|10||Model-driven specification and enforcement of RBAC break-glass policies for process-aware information systems|ContextIn many organizational environments critical tasks exist which – in exceptional cases such as an emergency – must be performed by a subject although he/she is usually not authorized to perform these tasks. Break-glass policies have been introduced as a sophisticated exception handling mechanism to resolve such situations. They enable certain subjects to break or override the standard access control policies of an information system in a controlled manner.ObjectiveIn the context of business process modeling a number of approaches exist that allow for the formal specification and modeling of process-related access control concepts. However, corresponding support for break-glass policies is still missing. In this paper, we aim at specifying a break-glass extension for process-related role-based access control (RBAC) models.MethodWe use model-driven development (MDD) techniques to provide an integrated, tool-supported approach for the definition and enforcement of break-glass policies in process-aware information systems. In particular, we provide modeling support on the computation independent model (CIM) layer as well as on the platform independent model (PIM) and platform specific model (PSM) layers.ResultsOur approach is generic in the sense that it can be used to extend process-aware information systems or process modeling languages with support for process-related RBAC and corresponding break-glass policies. Based on the formal CIM layer metamodel, we present a UML extension on the PIM layer that allows for the integrated modeling of processes and process-related break-glass policies via extended UML Activity diagrams. We evaluated our approach in a case study on real-world processes. Moreover, we implemented our approach at the PSM layer as an extension to the BusinessActivity library and runtime engine.ConclusionOur integrated modeling approach for process-related break-glass policies allows for specifying break-glass rules in process-aware information systems. 
56|10||The effect of governance on global software development: An empirical research in transactive memory systems|ContextThe way global software development (GSD) activities are managed impacts knowledge transactions between team members. The first is captured in governance decisions, and the latter in a transactive memory system (TMS), a shared cognitive system for encoding, storing and retrieving knowledge between members of a group.ObjectiveWe seek to identify how different governance decisions (such as business strategy, team configuration, task allocation) affect the structure of transactive memory systems as well as the processes developed within those systems.MethodWe use both a quantitative and a qualitative approach. We collect quantitative data through an online survey to identify transactive memory systems. We analyze transactive memory structures using social network analysis techniques and we build a latent variable model to measure transactive memory processes. We further support and triangulate our results by means of interviews, which also help us examine the GSD governance modes of the participating projects. We analyze governance modes, as set of decisions based on three aspects; business strategy, team structure and composition, and task allocation.ResultsOur results suggest that different governance decisions have a different impact on transactive memory systems. Offshore insourcing as a business strategy, for instance, creates tightly-connected clusters, which in turn leads to better developed transactive memory processes. We also find that within the composition and structure of GSD teams, there are boundary spanners (formal or informal) who have a better overview of the network’s activities and become central members within their network. An interesting mapping between task allocation and the composition of the network core suggests that the way tasks are allocated among distributed teams is an indicator of where expertise resides.ConclusionWe present an analytical method to examine GSD governance decisions and their effect on transactive memory systems. Our method can be used from both practitioners and researchers as a “cause and effect” tool for improving collaboration of global software teams. 
56|10||Test suite reduction methods that decrease regression testing costs by identifying irreplaceable tests|ContextIn software development and maintenance, a software system may frequently be updated to meet rapidly changing user requirements. New test cases will be designed to ensure the correctness of new or modified functions, thus gradually increasing the test suite’s size. Test suite reduction techniques aim to decrease the cost of regression testing by removing the redundant test cases from the test suite and then obtaining a representative set of test cases that still yield a high level of code coverage.ObjectiveMost of the existing reduction algorithms focus on decreasing the test suite’s size. Yet, the differences in execution costs among test cases are usually significant and it may take a lot of execution time to run a test suite consisting of a few long-running test cases. This paper presents and empirically evaluates cost-aware algorithms that can produce the representative sets with lower execution costs.MethodWe first use a cost-aware test case metric, called Irreplaceability, and its enhanced version, called EIrreplaceability, to evaluate the possibility that each test case can be replaced by others during test suite reduction. Furthermore, we construct a cost-aware framework that incorporates the concept of test irreplaceability into some well-known test suite reduction algorithms.ResultsThe effectiveness of the cost-aware framework is evaluated via the subject programs and test suites collected from the Software-artifact Infrastructure Repository — frequently chosen benchmarks for experimentally evaluating test suite reduction methods. The empirical results reveal that the presented algorithms produce representative sets that normally incur a low cost to yield a high level of test coverage.ConclusionThe presented techniques indeed enhance the capability of the traditional reduction algorithms to reduce the execution cost of a test suite. Especially for the additional Greedy algorithm, the presented techniques decrease the costs of the representative sets by 8.10–46.57%. 
56|10||Study of advanced separation of concerns approaches using the GoF design patterns: A quantitative and qualitative comparison|ContextSince the emergence of the aspect oriented paradigm, several studies have been conducted to test the contribution of this new paradigm compared to the object paradigm. However, in addition to this type of studies, we need also comparative studies that assess the aspect approaches mutually. The motivations of the latter include the enhancement of each aspect approach, devising hybrid approaches or merely helping developers choosing the suitable approach according to their needs. Comparing advanced separation of concerns approaches is the context of our work.ObjectiveWe aim at making an assessment of how the aspect approaches deal with crosscutting concerns. This assessment is based on quantitative attributes such as coupling and cohesion that evaluate the modularity as well as on qualitative observations.MethodWe selected three of well-known aspect approaches: AspectJ, JBoss AOP and CaesarJ, all the three based on Java. We conducted then, a comparative study using the GoF design patterns. In order to be fair we asked a group of Master students to achieve the implementation of all patterns with the three approaches. The use of these implementations as hypothetical benchmarks allowed us to achieve two kinds of comparison: a quantitative one based on structural and performance metrics, and qualitative one based on observations collected during the implementation phase.ResultsThe quantitative comparison shows some advantages like the using of fewer components with AspectJ and the strong cohesion with CaesarJ and weaknesses, as the high internal coupling caused by the inner classes of CaesarJ. The qualitative comparison gives comments about the approach understandability and others qualitative concepts.ConclusionThis comparison highlighted strengths and weaknesses of each approach, and provided a referential work that can help choosing the right approach during software development, enhancing aspect approaches or devising hybrid approaches that combine best features. 
56|10||Automatic test case generation for structural testing of function block diagrams|ContextFunction Block Diagram (FBD) is increasingly used in safety-critical applications. Test coverage issues for FBDs are frequently raised by regulators and users. However, there is little work at this aspect on testing FBD at model level. Our previous study has designed a new data-flow test coverage criterion, FB-Path Complete Condition Test Coverage (FPCC), that can directly test FBD structures and effectively detect function mutation errors. Nevertheless, because FPCC scheme involves several data-flow concepts and thus it is somewhat complicated to comprehend and to generate FPCC-complied test cases. An automatic test suite generator for FPCC is highly desirable.ObjectiveThis study designs an automatic test case generator, FPCCTestGen, for FPCC so as to enhance the practicability and acceptance of the FPCC approach.MethodFirst, a supporting infrastructure for performing automatic FBD-to-UPPAAL-for-FPCC transformation is designed. The supporting infrastructure includes templates, declarations, and functions as building blocks for transformation. Then, for each input FBD, represented in PLCopen XML format, FPCCTestGen performs parsing and converts FBD components into corresponding UPPAAL model components using aforementioned building blocks. After that, queries related to FPCC characteristics are submitted to UPPAAL model checker for verification. Finally, the verification traces are analyzed to obtain a FPCC-complied test suite.ResultsA safety injection system is used as a case study. Preliminary results show that the generated test suite achieves the highest FPCC percentage with a near optimal number of test cases.ConclusionThis automatic test case generation tool is effective and thus, can promote the use of the new test coverage criterion. Methodology used in FPCCTestGen is generic and can be applied to test suite generation for other test criteria on data-flow programs. 
56|10||Why software repositories are not used for defect-insertion circumstance analysis more often: A case study|ContextRoot-cause analysis is a data-driven technique for developing software process improvements in mature software organizations. The search for individual process correlates of high defect densities, which we call defect insertion circumstance analysis (DICA), is potentially both effective and cost-efficient as one approach to be used when attempting a general defect root cause analysis. In DICA, data from existing repositories (version archive, bug tracker) is evaluated largely automatically in order to determine conditions (such as the people, roles, components, or time-periods involved) that correlate with higher-than-normal defect insertion frequencies. Nevertheless, no reports of industrial use of DICA have been published.ObjectiveDetermine the reasons why DICA is not used more often by practitioners.MethodWe use a single-case, typical-case, revelatory-type case study to evaluate in parallel the importance of six plausible reasons (R1–R6). The case is based on 11 years of repository data from a small but mature software company building a product in the high-end content management system domain and describes a four person-months effort to make use of these data.ResultsWhile DICA required non-negligible effort (R3) and some degree of inventiveness (R2), the most relevant roadblock was insufficient reliability of the results (R6) combined with the difficulty of assessing this reliability (R5). We identify three difficulties that led to this outcome.ConclusionCurrent repository mining methods are too immature for successful DICA. Gradual improvements are unlikely to help; different principles of operation will be required. Even with such different techniques, issues with input data quality may continue to make good results difficult-to-have. 
56|10||Evaluating the productivity of a reference-based programming approach: A controlled experiment|ContextDomain engineering aims at facilitating software development in an efficient and economical way. One way to measure that is through productivity indicators, which refer to the ability of creating a quality software product in a limited period and with limited resources. Many approaches have been devised to increase productivity; however, these approaches seem to suffer from a tension between expressiveness on the one hand, and applicability (or the lack of it) in providing guidance for developers.ObjectiveThis paper evaluates the applicability and efficiency of adopting a domain engineering approach, called Application-based DOmain Modeling (ADOM), in the context of the programming task with Java, and thus termed ADOM-Java, for improving productivity in terms of code quality and development time.MethodTo achieve that objective we have qualitatively evaluate the approach using questionnaires and following a text analysis procedure. We also set a controlled experiment in which 50 undergraduate students performed a Java-based programming task using either ADOM-Java or Java alone.ResultsThe qualitative evaluation reveal that the approach is easy to uses and provides valuable guidance. Nevertheless, it requires training. The outcomes of the experiment indicate that the approach is applicable and that the students that used ADOM-Java achieved better code quality, as well as better functionality and within less time than the students who used only Java.ConclusionThe results of the experiments imply that by providing a code base equipped with reuse guidelines for programmers can increase programming productivity in terms of quality and development time. These guidelines may also enforce coding standards and architectural design. 
56|10||Reasons for bottlenecks in very large-scale system of systems development|ContextSystem of systems (SoS) is a set or arrangement of systems that results when independent and useful systems are to be incorporated into a larger system that delivers unique capabilities. Our investigation showed that the development life cycle (i.e. the activities transforming requirements into design, code, test cases, and releases) in SoS is more prone to bottlenecks in comparison to single systems.ObjectiveThe objective of the research is to identify reasons for bottlenecks in SoS, prioritize their significance according to their effect on bottlenecks, and compare them with respect to different roles and different perspectives, i.e. SoS view (concerned with integration of systems), and systems view (concerned with system development and delivery).MethodThe research method used is a case study at Ericsson AB.ResultsResults show that the most significant reasons for bottlenecks are related to requirements engineering. All the different roles agree on the significance of requirements related factors. However, there are also disagreements between the roles, in particular with respect to quality related reasons. Quality related hinders are primarily observed and highly prioritized by quality assurance responsibles. Furthermore, SoS view and system view perceive different hinders, and prioritize them differently.ConclusionWe conclude that solutions for requirements engineering in SoS context are needed, quality awareness in the organization has to be achieved end to end, and views between SoS and system view need to be aligned to avoid sub optimization in improvements. 
56|11|http://www.sciencedirect.com/science/journal/09505849/56/11|Special issue editorial: Understanding software ecosystems|
56|11||Joining a smartphone ecosystem: Application developersâ motivations and decision criteria|ContextThe ecosystems surrounding current smartphones operating systems, especially the application markets, provide significant value for customers and therefore possibilities for provider differentiation. Why do independent application developers and innovators join these ecosystems, and which factors influence their choice between different platform options?ObjectiveThis paper evaluates why innovators publish applications for smartphone operating systems, and which factors influence their choice between the two most common platforms, Android and Apple iOS, and leading them to join this respective ecosystem.MethodA quantitative questionnaire containing questions related to demographics, motivational factors, factors impacting choice of platform and application publishing history. 113 Application developers from all over the world responded, relatively evenly split between the two operating systems.ResultsThe main motivations are the experience of fun and intellectual stimulation during the process itself and learning of new skills or know-how. Financial gain, on the other hand, was found to be of less importance. There are also significant differences between the developers according to their preferred platform, e.g. iOS developers are more often motivated by financial gain. In choosing which ecosystem to join, network size, openness and entry barriers play major roles, and according to these criteria the two ecosystems are perceived to be significantly different. In addition, the toolkit quality is one of the most important factors for developers, but not a point of differentiation between the two ecosystems.ConclusionThis paper complements prior research with the viewpoint of the developers themselves, focusing on how they perceive both technical and economic, as well as governance related aspects. In addition, most research so far has focused on one aspect only, mostly application markets, while this study assess the perception of the whole ecosystem, including end user device, markets and OS. It is concluded that the motivations and perceptions of developers comprise a major component in the creation and management of a large and diverse ecosystem, and that there exists a significant group of user innovators, who are not motivated by financial gain. There are several other aspects besides network size that drive the choice of platform. In addition, developers perceive hardware, software, marketplaces and other aspects as one single construct. Any platform owner therefore has to carefully consider and manage their expectations and perceptions. 
56|11||Software engineering beyond the project â Sustaining software ecosystems|ContextThe main part of software engineering methods, tools and technologies has developed around projects as the central organisational form of software development. A project organisation depends on clear bounds regarding scope, participants, development effort and lead-time. What happens when these conditions are not given? The article claims that this is the case for software product specific ecosystems. As software is increasingly developed, adopted and deployed in the form of customisable and configurable products, software engineering as a discipline needs to take on the challenge to support software ecosystems.ObjectiveThe article provides a holistic understanding of the observed and reported practices as a starting point to device specific support for the development in software ecosystems.MethodA qualitative interview study was designed based on previous long-term ethnographical inspired research.ResultsThe analysis results in a set of common features of product development and evolution despite differences in size, kind of software and business models. Design is distributed and needs to be coordinated across heterogeneous design constituencies that, together with the software, build a product specific socio-technical ecosystem. The technical design has to support the deference of part of the development not only to 3rd-party developers but also to local designers tailoring the software in the use organisation. The technical interfaces that separate the work of different design constituencies are contested and need to be maintained permanently. Development takes place as cycles within cycles – overlaying development cycles with different rhythms to accommodate different evolution drivers.ConclusionThe reported practices challenge some of the very core assumptions of traditional software engineering, but makes perfect sense, considering that the frame of reference for product development is not a project but continuous innovation across the respective ecosystem. The article provides a number of concrete points for further research. 
56|11||Characteristics of software ecosystems for Federated Embedded Systems: A case study|ContextTraditionally, Embedded Systems (ES) are tightly linked to physical products, and closed both for communication to the surrounding world and to additions or modifications by third parties. New technical solutions are however emerging that allow addition of plug-in software, as well as external communication for both software installation and data exchange. These mechanisms in combination will allow for the construction of Federated Embedded Systems (FES). Expected benefits include the possibility of third-party actors developing add-on functionality; a shorter time to market for new functions; and the ability to upgrade existing products in the field. This will however require not only new technical solutions, but also a transformation of the software ecosystems for ES.ObjectiveThis paper aims at providing an initial characterization of the mechanisms that need to be present to make a FES ecosystem successful. This includes identification of the actors, the possible business models, the effects on product development processes, methods and tools, as well as on the product architecture.MethodThe research was carried out as an explorative case study based on interviews with 15 senior staff members at 9 companies related to ES that represent different roles in a future ecosystem for FES. The interview data was analyzed and the findings were mapped according to the Business Model Canvas (BMC).ResultsThe findings from the study describe the main characteristics of a FES ecosystem, and identify the challenges for future research and practice.ConclusionsThe case study indicates that new actors exist in the FES ecosystem compared to a traditional supply chain, and that their roles and relations are redefined. The business models include new revenue streams and services, but also create the need for trade-offs between, e.g., openness and dependability in the architecture, as well as new ways of working. 
56|11||Analysis and design of software ecosystem architectures â Towards the 4S telemedicine ecosystem|ContextTelemedicine, the provision of health care at a distance, is arguably an effective way of increasing access to, reducing cost of, and improving quality of care. However, the deployment of telemedicine is faced with standards that are hard to use, application-specific data models, and application stove-pipes that inhibit the adoption of telemedical solutions. To which extent can a software ecosystem approach to telemedicine alleviate this?ObjectiveIn this article, we define the concept of software ecosystem architecture as the structure(s) of a software ecosystem comprising elements, relations among them, and properties of both. Our objective is to show how this concept can be used (i) in the analysis of existing software ecosystems and (ii) in the design of new software ecosystems.MethodWe performed a mixed-method study that consisted of a case study and an experiment. For (i), we performed a descriptive, revelatory case study of the Danish telemedicine ecosystem and for (ii), we experimentally designed, implemented, and evaluated the architecture of 4S.ResultsWe contribute in three areas. First, we define the software ecosystem architecture concept that captures organization, business, and software aspects of software ecosystems. Secondly, we apply this concept in our case study and demonstrate that it is a viable concept for software ecosystem analysis. Finally, based on our experiments, we discuss the practice of software engineering for software ecosystems drawn from experience in creating and evolving the 4S telemedicine ecosystem.ConclusionThe concept of software ecosystem architecture can be used analytically and constructively in respectively the analysis and design of software ecosystems. 
56|11||Bridges and barriers to hardware-dependent software ecosystem participation â A case study| abstractBackgroundSoftware ecosystems emerged as means for several actors to jointly provide more value to the market than any of them can do on its own. Recently, software ecosystems are more often used to support the development of hardware-dependent solutions.ObjectivesThis work aims at studying barriers and bridges to participation in an ecosystem with substantial hardware dependencies.MethodWe conducted an interview-based case study of an ecosystem around Axis’ network video surveillance systems, interviewing 10 internal experts and 8 external representatives of 6 companies, complemented by document studies at Axis.ResultsMajor bridges to the ecosystem include end customer demands, open and transparent communication and relationship, as well as internal and external standardizations. Barriers include the two-tier business model, entry barriers and execution performance issues. Approximately half of the identified bridges and barriers could be considered hardware-dependent ecosystems specific.ConclusionOur results suggest that ecosystem leaders should share their sales channels with the ecosystem participants and focus on good communication and relationships as the dominant factors for the ecosystem participation. Moreover, we report that internal and external standardization can play a dual role, not only ease the development but also enable additional sales channels and new opportunities for the ecosystem participants. At the same time, the business model selected by the ecosystem leaders and performance, are identified as the main barriers to ecosystem participation. We believe that the business model barrier may be much more important for similar hardware-dependent software ecosystems. 
56|11||Measuring the health of open source software ecosystems: Beyond the scope of project health|BackgroundThe livelihood of an open source ecosystem is important to different ecosystem participants: software developers, end-users, investors, and participants want to know whether their ecosystem is healthy and performing well. Currently, there exists no working operationalization available that can be used to determine the health of open source ecosystems. Health is typically looked at from a project scope, not from an ecosystem scope.ObjectivesWith such an operationalization, stakeholders can make better decisions on whether to invest in an ecosystem: developers can select the healthiest ecosystem to join, keystone organizers can establish which governance techniques are effective, and end-users can select ecosystems that are robust, will live long, and prosper.MethodDesign research is used to create the health operationalization. The evaluation step is done using four ecosystem health projects from literature.ResultsThe Open Source Ecosystem Health Operationalization is provided, which establishes the health of a complete software ecosystem, using the data from collections of open source projects that belong to the ecosystem.ConclusionThe groundwork is done, by providing a summary of research challenges, for more research in ecosystem health. With the operationalization in hand, researchers no longer need to start from scratch when researching open source ecosystems’ health. 
56|11||Variability mechanisms in software ecosystems|ContextSoftware ecosystems are increasingly popular for their economic, strategic, and technical advantages. Application platforms such as Android or iOS allow users to highly customize a system by selecting desired functionality from a large variety of assets. This customization is achieved using variability mechanisms.ObjectiveVariability mechanisms are well-researched in the context of software product lines. Although software ecosystems are often seen as conceptual successors, the technology that sustains their success and growth is much less understood. Our objective is to improve empirical understanding of variability mechanisms used in successful software ecosystems.MethodWe analyze five ecosystems, ranging from the Linux kernel through Eclipse to Android. A qualitative analysis identifies and characterizes variability mechanisms together with their organizational context. This analysis leads to a conceptual framework that unifies ecosystem-specific aspects using a common terminology. A quantitative analysis investigates scales, growth rates, and—most importantly—dependency structures of the ecosystems.ResultsIn all the studied ecosystems, we identify rich dependency languages and variability descriptions that declare many direct and indirect dependencies. Indirect dependencies to abstract capabilities, as opposed to concrete variability units, are used predominantly in fast-growing ecosystems. We also find that variability models—while providing system-wide abstractions over code—work best in centralized variability management and are, thus, absent in ecosystems with large free markets. These latter ecosystems tend to emphasize maintaining capabilities and common vocabularies, dynamic discovery, and binding with strong encapsulation of contributions, together with uniform distribution channels.ConclusionThe use of specialized mechanisms in software ecosystems with large free markets, as opposed to software product lines, calls for recognition of a new discipline—variability encouragement. 
56|12|http://www.sciencedirect.com/science/journal/09505849/56/12|Human factors in software development: On its underlying theories and the value of learning from related disciplines. A guest editorial introduction to the special issue|
56|12||Who to follow recommendation in large-scale online development communities|ContextOpen source development allows a large number of people to reuse and contribute source code to the community. Social networking features open opportunities for information discovery, social collaborations, and improved recommendations of potential collaborators.ObjectiveOnline community and development platforms rely on social network features to increase awareness and attention among community members for improved collaborations. The objective of this work is to introduce an approach for recommending relevant users to follow. Follower networks provide means for informal information propagation. The efficiency and effectiveness of such information flows is impacted by the network structure. Here, we aim to understand the resilience of networks against random or strategic node removal.MethodSocial network features of online software development communities present a new opportunity to enhance online collaboration. Our approach is based on the automatic analysis of user behavior and network structure. The proposed ‘who to follow’ recommendation algorithm can be parametrized for specific contexts. Link-analysis techniques such as PageRank/HITS provide the basis for a novel ‘who to follow’ recommendation model.ResultsWe tested the approach using a GitHub-based dataset. Currently, users follow popular community members to get updates regarding their activities instead of maintaining personal relations. Thus, social network features require further improvements to increase reciprocity. The application of our ‘who to follow’ recommendation model using the GitHub dataset shows excellent results with respect to context-sensitive following recommendations. The sensitivity of GitHub’s follower network to random node removal is comparable with other social networks but more sensitive to follower authority based node removal.ConclusionLink-based algorithm can be used for context-sensitive ‘who to follow’ recommendations. GitHub is highly sensitive to authority based node removal. Information flow established through follower relations will be strongly impacted if many authorities are removed from the network. This underpins the importance of ‘central’ users and the validity of focusing the ‘who to follow’ recommendations on those users. 
56|12||Communities of practice in a large distributed agile software development organization â Case Ericsson|ContextCommunities of practice—groups of experts who share a common interest or topic and collectively want to deepen their knowledge—can be an important part of a successful lean and agile adoption in particular in large organizations.ObjectiveIn this paper, we present a study on how a large organization within Ericsson with 400 persons in 40 Scrum teams at three sites adopted the use of Communities of Practice (CoP) as part of their transformation from a traditional plan-driven organization to lean and agile.MethodsWe collected data by 52 semi-structured interviews on two sites, and longitudinal non-participant observation of the transformation during over 20 site visits over a period of two years.ResultsThe organization had over 20 CoPs, gathering weekly, bi-weekly or on a need basis. CoPs had several purposes including knowledge sharing and learning, coordination, technical work, and organizational development. Examples of CoPs include Feature Coordination CoPs to coordinate between teams working on the same feature, a Coaching CoP to discuss agile implementation challenges and successes and to help lead the organizational continuous improvement, an end-to-end CoP to remove bottlenecks from the flow, and Developers CoPs to share good development practices. Success factors of well-functioning CoPs include having a good topic, passionate leader, proper agenda, decision making authority, open community, supporting tools, suitable rhythm, and cross-site participation when needed. Organizational support include creating a supportive atmosphere and providing a suitable infrastructure for CoPs.ConclusionsIn the case organization, CoPs were initially used to support the agile transformation, and as part of the distributed Scrum implementation. As the transformation progressed, the CoPs also took on the role of supporting continuous organizational improvements. CoPs became a central mechanism behind the success of the large-scale agile implementation in the case organization that helped mitigate some of the most pressing problems of the agile transformation. 
56|12||Understanding the attitudes, knowledge sharing behaviors and task performance of core developers: A longitudinal study|ContextPrior research has established that a few individuals generally dominate project communication and source code changes during software development. Moreover, this pattern has been found to exist irrespective of task assignments at project initiation.ObjectiveWhile this phenomenon has been noted, prior research has not sought to understand these dominant individuals. Previous work considering the effect of team structures on team performance has found that core communicators are the gatekeepers of their teams’ knowledge, and the performance of these members was correlated with their teams’ success. Building on this work, we have employed a longitudinal approach to study the way core developers’ attitudes, knowledge sharing behaviors and task performance change over the course of their project, based on the analysis of repository data.MethodWe first used social network analysis (SNA) and standard statistical analysis techniques to identify and select artifacts from ten different software development teams. These procedures were also used to select central practitioners among these teams. We then applied psycholinguistic analysis and directed content analysis (CA) techniques to interpret the content of these practitioners’ messages. Finally, we inspected these core developers’ activities as recorded in system change logs at various points in time during systems’ development.ResultsAmong our findings, we observe that core developers’ attitudes and knowledge sharing behaviors were linked to their involvement in actual software development and the demands of their wider project teams. However, core developers appeared to naturally possess high levels of insightful characteristics, which became evident very early during teamwork.ConclusionsProject performance would likely benefit from strategies aimed at surrounding core developers with other competent communicators. Core developers should also be supported by a wider team who are willing to ask questions and challenge their ideas. Finally, the availability of adequate communication channels would help with maintaining positive team climate, and this is likely to mitigate the negative effects of distance during distributed developments. 
56|12||How are software defects found? The role of implicit defect detection, individual responsibility, documents, and knowledge|ContextPrior research has focused heavily on explicit defect detection, such as formal testing and reviews. However, in reality, humans find software defects in various activities. Implicit defect detection activities, such as preparing a product demonstration or updating a user manual, are not designed for defect detection, yet through such activities defects are discovered. In addition, the type of documentation, and knowledge used, in defect detection is diverse.ObjectiveTo understand how defect detection is affected by the perspectives of responsibility, activity, knowledge, and document use. To provide illustrative numbers concerning the multidimensionality of defect detection in an industrial context.MethodThe data were collected with a survey on four software development organizations in three different companies. We designed the survey based on our prior extensive work with these companies.ResultsWe found that among our subjects (n = 105), implicit defect detection made a higher contribution than explicit defect detection in terms of found defects, 62% vs. 38%. We show that defect detection was performed by subjects in various roles supporting the earlier reports of testing being a cross-cutting activity in software development organizations. We found a low use of test cases (18%), but a high use of other documents in software defect detection, and furthermore, we found that personal knowledge was applied as an oracle in defect detection much more often than documented oracles. Finally, we recognize that contextual factors largely affect the transferability of our results, and we provide elaborate discussion about the most important contextual factors. Furthermore, we must be cautious as the results were obtained with a survey, and come from a small number of organizations.ConclusionsIn this paper, we show the large impact of implicit defect detection activities in four case organizations. Implicit defect detection has a large contribution to defect detection in practice, and can be viewed as an extremely low-cost way of detecting defects. Thus, harnessing and supporting it better may increase quality without increasing costs. For example, if an employee can update the user manual, and simultaneously detect defects from the software, then the defect detection part of this activity can be seen as cost-free. Additionally, further research is needed on how diverse types of useful documentation and knowledge can be utilized in defect detection. 
56|12||Understanding reuse of software examples: A case study of prejudice in a community of practice|ContextThe context of this research is software developers’ perceptions about the use of code examples in professional software development.ObjectiveThe primary objective of this paper is to identify the human factors that dominate example usage among professional software developers, and to provide a theory that explains these factors.MethodTo achieve this goal, we analyzed the perceptions of professional software developers as manifested on LinkedIn online community. We analyzed the data qualitatively using adapted grounded theory research procedures.ResultsThe research yields an initial framework of key factors that dominate professional developers’ perception regarding example usage. We use the theoretical lens of prejudice theory to put these factors in a broader context, and outline initial recommendations to address these factors in professional organizational context.ConclusionThe results of this work, in particular the use of qualitative techniques – allowed us to obtain rich insight into key human factors that affect professional software developers, and enrich the body of literature on the issues of reuse. These factors need to be taken into account as part of an organizational reuse strategy. 
56|2|http://www.sciencedirect.com/science/journal/09505849/56/2|Software process modeling languages: A systematic literature review|ContextOrganizations working in software development are aware that processes are very important assets as well as they are very conscious of the need to deploy well-defined processes with the goal of improving software product development and, particularly, quality. Software process modeling languages are an important support for describing and managing software processes in software-intensive organizations.ObjectiveThis paper seeks to identify what software process modeling languages have been defined in last decade, the relationships and dependencies among them and, starting from the current state, to define directions for future research.MethodA systematic literature review was developed. 1929 papers were retrieved by a manual search in 9 databases and 46 primary studies were finally included.ResultsSince 2000 more than 40 languages have been first reported, each of which with a concrete purpose. We show that different base technologies have been used to define software process modeling languages. We provide a scheme where each language is registered together with the year it was created, the base technology used to define it and whether it is considered a starting point for later languages. This scheme is used to illustrate the trend in software process modeling languages. Finally, we present directions for future research.ConclusionThis review presents the different software process modeling languages that have been developed in the last ten years, showing the relevant fact that model-based SPMLs (Software Process Modeling Languages) are being considered as a current trend. Each one of these languages has been designed with a particular motivation, to solve problems which had been detected. However, there are still several problems to face, which have become evident in this review. This let us provide researchers with some guidelines for future research on this topic. 
56|2||An empirical study on the implementation and evaluation of a goal-driven software development risk management model|ContextBuilding a quality software product in the shortest possible time to satisfy the global market demand gives an enterprise a competitive advantage. However, uncertainties and risks exist at every stage of a software development project. These can have an extremely high influence on the success of the final software product. Early risk management practice is effective to manage such risks and contributes effectively towards the project success.ObjectiveDespite risk management approaches, a detailed guideline that explains where to integrate risk management activities into the project is still missing. Little effort has been directed towards the evaluation of the overall impact of a risk management method. We present a Goal-driven Software Development Risk Management Model (GSRM) and its explicit integration into the requirements engineering phase and an empirical investigation result of applying GSRM into a project.MethodWe combine the case study method with action research so that the results from the case study directly contribute to manage the studied project risks and to identify ways to improve the proposed methodology. The data is collected from multiple sources and analysed both in a qualitative and quantitative way.ResultsWhen risk factors are beyond the control of the project manager and project environment, it is difficult to control these risks. The project scope affects all the dimensions of risk. GSRM is a reasonable risk management method that can be employed in an industrial context. The study results have been compared against other study results in order to generalise findings and identify contextual factors.ConclusionA formal early stage risk management practice provides early warning related to the problems that exists in a project, and it contributes to the overall project success. It is not necessary to always consider budget and schedule constraints as top priority. There exist issues such as requirements, change management, and user satisfaction which can influence these constraints. 
56|2||An integrated approach based on execution measures for the continuous improvement of business processes realized by services|ContextOrganizations are rapidly adopting Business Process Management (BPM) as they focus on their business processes (BPs), seeing them to be key elements in controlling and improving the way they perform their business. Business Process Intelligence (BPI) takes as its focus the collection and analysis of information from the execution of BPs for the support of decision making, based on the discovery of improvement opportunities. Realizing BPs by services introduces an intermediate service layer that enables us to separate the specification of BPs in terms of models from the technologies implementing them, thus improving their modifiability by decoupling the model from its implementation.ObjectiveTo provide an approach for the continuous improvement of BPs, based on their realization with services and execution measurement. It comprises an improvement process to integrate the improvements into the BPs and services, an execution measurement model defining and categorizing several measures for BPs and service execution, and tool support for both.MethodWe carried out a systematic literature review, to collect existing proposals related to our research work. Then, in close collaboration with business experts from the Hospital General de Ciudad Real (HGCR), Spain, and following design science principles, we developed the methods and artifacts described in this paper, which were validated by means of a case study.ResultsWe defined an improvement process extending the BP lifecycle with measurement and improvement activities, integrating an execution measurement model comprising a set of execution measures. Moreover, we developed a plug-in for the ProM framework to visualize the measurement results as a proof-of-concept prototype. The case study with the HGCR has shown its feasibility.ConclusionsOur improvement vision, based on BPs realized by services and on measurement of their execution, in conjunction with a systematic approach to integrate the detected improvements, provides useful guidance to organizations. 
56|2||Enhancing software artefact traceability recovery processes with link count information|ContextThe intensive human effort needed to manually manage traceability information has increased the interest in using semi-automated traceability recovery techniques. In particular, Information Retrieval (IR) techniques have been largely employed in the last ten years to partially automate the traceability recovery process.AimPrevious studies mainly focused on the analysis of the performances of IR-based traceability recovery methods and several enhancing strategies have been proposed to improve their accuracy. Very few papers investigate how developers (i) use IR-based traceability recovery tools and (ii) analyse the list of suggested links to validate correct links or discard false positives. We focus on this issue and suggest exploiting link count information in IR-based traceability recovery tools to improve the performances of the developers during a traceability recovery process.MethodTwo empirical studies have been conducted to evaluate the usefulness of link count information. The two studies involved 135 University students that had to perform (with and without link count information) traceability recovery tasks on two software project repositories. Then, we evaluated the quality of the recovered traceability links in terms of links correctly and erroneously traced by the students.ResultsThe results achieved indicate that the use of link count information significantly increases the number of correct links identified by the participants.ConclusionsThe results can be used to derive guidelines on how to effectively use traceability recovery approaches and tools proposed in the literature. 
56|2||Data stream mining for predicting software build outcomes using source code metrics|ContextSoftware development projects involve the use of a wide range of tools to produce a software artifact. Software repositories such as source control systems have become a focus for emergent research because they are a source of rich information regarding software development projects. The mining of such repositories is becoming increasingly common with a view to gaining a deeper understanding of the development process.ObjectiveThis paper explores the concepts of representing a software development project as a process that results in the creation of a data stream. It also describes the extraction of metrics from the Jazz repository and the application of data stream mining techniques to identify useful metrics for predicting build success or failure.MethodThis research is a systematic study using the Hoeffding Tree classification method used in conjunction with the Adaptive Sliding Window (ADWIN) method for detecting concept drift by applying the Massive Online Analysis (MOA) tool.ResultsThe results indicate that only a relatively small number of the available measures considered have any significance for predicting the outcome of a build over time. These significant measures are identified and the implication of the results discussed, particularly the relative difficulty of being able to predict failed builds. The Hoeffding Tree approach is shown to produce a more stable and robust model than traditional data mining approaches.ConclusionOverall prediction accuracies of 75% have been achieved through the use of the Hoeffding Tree classification method. Despite this high overall accuracy, there is greater difficulty in predicting failure than success. The emergence of a stable classification tree is limited by the lack of data but overall the approach shows promise in terms of informing software development activities in order to minimize the chance of failure. 
56|2||A hybrid class- and prototype-based object model to support language-neutral structural intercession|ContextDynamic languages have turned out to be suitable for developing specific applications where runtime adaptability is an important issue. Although .Net and Java platforms have gradually incorporated features to improve their support of dynamic languages, they do not provide intercession for every object or class. This limitation is mainly caused by the rigid class-based object model these platforms implement, in contrast to the flexible prototype-based model used by most dynamic languages.ObjectiveOur approach is to provide intercession for any object or class by defining a hybrid class- and prototype-based object model that efficiently incorporates structural intercession into the object model implemented by the widespread .Net and Java platforms.MethodIn a previous work, we developed and evaluated an extension of a shared-source implementation of the .Net platform. In this work, we define the formal semantics of the proposed reflective model, and modify the existing implementation to include the hybrid model. Finally, we assess its runtime performance and memory consumption, comparing it to existing approaches.ResultsOur platform shows a competitive runtime performance compared to 9 widespread systems. On average, it performs 73% and 61% better than the second fastest system for short- and long-running applications, respectively. Besides, it is the JIT-compiler approach that consumes less average memory. The proposed approach of including a hybrid object-model into the virtual machine involves a 444% performance improvement (and 65% less memory consumption) compared to the existing alternative of creating an extra software layer (the DLR). When none of the new features are used, our platform requires 12% more execution time and 13% more memory than the original .Net implementation.ConclusionOur proposed hybrid class- and prototype-based object model supports structural intercession for any object or class. It can be included in existing JIT-compiler class-based platforms to support common dynamic languages, providing competitive runtime performance and low memory consumption. 
56|2||Source code size estimation approaches for object-oriented systems from UML class diagrams: A comparative study|BackgroundSource code size in terms of SLOC (source lines of code) is the input of many parametric software effort estimation models. However, it is unavailable at the early phase of software development.ObjectiveWe investigate the accuracy of early SLOC estimation approaches for an object-oriented system using the information collected from its UML class diagram available at the early software development phase.MethodWe use different modeling techniques to build the prediction models for investigating the accuracy of six types of metrics to estimate SLOC. The used techniques include linear models, non-linear models, rule/tree-based models, and instance-based models. The investigated metrics are class diagram metrics, predictive object points, object-oriented project size metric, fast&&serious class points, objective class points, and object-oriented function points.ResultsBased on 100 open-source Java systems, we find that the prediction model built using object-oriented project size metric and ordinary least square regression with a logarithmic transformation achieves the highest accuracy (mean MMRE = 0.19 and mean Pred(25) = 0.74).ConclusionWe should use object-oriented project size metric and ordinary least square regression with a logarithmic transformation to build a simple, accurate, and comprehensible SLOC estimation model. 
56|2||Generating profile-based signatures for online intrusion and failure detection|ContextProgram execution profiles have been extensively and successfully used in several dynamic analysis fields such as software testing and fault localization.ObjectiveThis paper presents a pattern-matching approach implemented as an application-based intrusion (and failure) detection system that operates on signatures generated from execution profiles. Such signatures are not descriptions of exploits, i.e. they do not depend on the syntax or semantics of the exploits, but instead are descriptions of program events that correlate with the exploitation of program vulnerabilities.MethodA vulnerability exploit is generally correlated with the execution of a combination of program elements, such as statements, branches, and definition–use pairs. In this work we first analyze the execution profiles of a vulnerable application in order to identify such suspicious combinations, define signatures that describe them, and consequently deploy these signatures within an intrusion detection system that performs online signature matching.ResultsTo evaluate our approach, which is also applicable to online failure detection, we implemented it for the Java platform and applied it onto seven open-source applications containing 30 vulnerabilities/defects for the purpose of the online detection of attacks/ failures. Our results showed that our approach worked very well for 26 vulnerabilities/defects (86.67%) and the overhead imposed by the system is somewhat acceptable as it varied from 46% to 102%. The exhibited average rates of false negatives and false positives were 0.43% and 1.03%, respectively.ConclusionUsing profile-based signatures for online intrusion and failure detection was shown to be effective. 
56|2||Understanding the characteristics of quality for software engineering processes: A Grounded Theory investigation|ContextSoftware engineering organizations routinely define and implement processes to support, guide and control project execution. An assumption underlying this process-centric approach to business improvement is that the quality of the process will influence the quality, cost and time-to-release of the software produced. A critical question thus arises of what constitutes quality for software engineering processes.ObjectiveTo identify criteria used by experienced practitioners to judge the quality of software engineering processes and to understand how knowledge of these criteria and their relationships may be useful for those undertaking software process improvement activities.MethodInterviews were conducted with 17 experienced software engineering practitioners from a range of geographies, roles and industry sectors. Published reports from 30 software process improvement case-studies were selected from multiple peer-reviewed software journals. A qualitative Grounded Theory-based methodology was employed to systematically analyze the collected data to synthesize a model of quality for software engineering processes.ResultsThe synthesized model suggests that practitioners perceive the overall quality of a software engineering process based on four quality attributes: suitability, usability, manageability and evolvability. Furthermore, these judgments are influenced by key properties related to the semantic content, structure, representation and enactment of that process. The model indicates that these attributes correspond to particular organizational perspectives and that these differing views may explain role-based conflicts in the judgement of process quality.ConclusionConsensus exists amongst practitioners about which characteristics of software engineering process quality most influence project outcomes. The model presented provides a terminological framework that can facilitate precise discussion of software engineering process issues and a set of criteria that can support activities for software process definition, evaluation and improvement. The potential exists for further development of this model to facilitate optimization of process properties to match organizational needs. 
56|3|http://www.sciencedirect.com/science/journal/09505849/56/3|A systematic review on security in Process-Aware Information Systems â Constitution, challenges, and future directions|ContextSecurity in Process-Aware Information Systems (PAIS) has gained increased attention in current research and practice. However, a common understanding and agreement on security is still missing. In addition, the proliferation of literature makes it cumbersome to overlook and determine state of the art and further to identify research challenges and gaps. In summary, a comprehensive and systematic overview of state of the art in research and practice in the area of security in PAIS is missing.ObjectiveThis paper investigates research on security in PAIS and aims at establishing a common understanding of terminology in this context. Further it investigates which security controls are currently applied in PAIS.MethodA systematic literature review is conducted in order to classify and define security and security controls in PAIS. From initially 424 papers, we selected in total 275 publications that related to security and PAIS between 1993 and 2012. Furthermore, we analyzed and categorized the papers using a systematic mapping approach which resulted into 5 categories and 12 security controls.ResultsIn literature, security in PAIS often centers on specific (security) aspects such as security policies, security requirements, authorization and access control mechanisms, or inter-organizational scenarios. In addition, we identified 12 security controls in the area of security concepts, authorization and access control, applications, verification, and failure handling in PAIS. Based on the results, open research challenges and gaps are identified and discussed with respect to possible solutions.ConclusionThis survey provides a comprehensive review of current security practice in PAIS and shows that security in PAIS is a challenging interdisciplinary research field that assembles research methods and principles from security and PAIS. We show that state of the art provides a rich set of methods such as access control models but still several open research challenges remain. 
56|3||Comparing attack trees and misuse cases in an industrial setting|The last decade has seen an increasing focus on addressing security already during the earliest stages of system development, such as requirements determination. Attack trees and misuse cases are established techniques for representing security threats along with their potential mitigations. Previous work has compared attack trees and misuse cases in two experiments with students. The present paper instead presents an experiment where industrial practitioners perform the experimental tasks in their workplace. The industrial experiment confirms a central finding from the student experiments: that attack trees tend to help identifying more threats than misuse cases. It also presents a new result: that misuse cases tend to encourage identification of threats associated with earlier development stages than attack trees. The two techniques should therefore be considered complementary and should be used together in practical requirements work. 
56|3||Facilitating contagion trust through tools in Global Systems Engineering teams|ContextIn Global Systems Engineering teams, researchers have found that trust can be transitive to some degree or imported (swift trust) under certain conditions. We argue that trust can be contagion and seeded by tools (spread from one individual to another through tools).ObjectiveWe sought to investigate the potential for using tools to support the development of trust in such teams and facilitate contagion trust. Specifically, we sought to investigate whether any existing tools support the development of trust in such teams and which information helps such development, whether the visualization of past collaborations would help developing trust, and what tools or features practitioners would wish for, if they had a magic wand.MethodWe interviewed 71 employees from five multinational organizations. We focused on gaining an understanding of the tools that are currently used to engender trust and the information needed to facilitate contagion, in which conditions visualizations of past collaborations are helpful, and what software tool features could help develop trust. Our analysis was guided by grounded theory.ResultsWe found evidence that supports the theory of contagion trust and tools can be used to initiate the development of trust. These tools include software tools, office technologies, or organizational structures. Practitioners’ needs were functional (e.g. audio channel with remote colleagues) and/or non-functional (e.g. can be adopted in sites with poor infrastructure).ConclusionOur study illustrates that tools can be used to facilitate contagion trust and provides three main contributions. First, our exploration of how existing tools are used provides a guide to effective practices in such teams. Second, the descriptions of features that can facilitate contagion trust provide useful design implications for future tools. Third, the identification of the kind of information that facilitates contagion trust provides an understanding of practitioners’ underlying needs that can be used to develop collaboration tools. 
56|3||Model-based requirements verification method: Conclusions from two controlled experiments|ContextRequirements engineering is one of the most important and critical phases in the software development life cycle, and should be carefully performed to build high quality and reliable software. However, requirements are typically gathered through various sources and are represented in natural language (NL), making requirements engineering a difficult, fault prone, and a challenging task.ObjectiveTo ensure high-quality software, we need effective requirements verification methods that can clearly handle and address inherently ambiguous nature of NL specifications. The objective of this paper is to propose a method that can address the challenges with NL requirements verification and to evaluate our proposed method through controlled experiments.MethodWe propose a model-based requirements verification method, called NLtoSTD, which transforms NL requirements into a State Transition Diagram (STD) that can help to detect and to eliminate ambiguities and incompleteness. The paper describes the NLtoSTD method to detect requirement faults, thereby improving the quality of the requirements. To evaluate the NLtoSTD method, we conducted two controlled experiments at North Dakota State University in which the participants employed the NLtoSTD method and a traditional fault checklist during the inspection of requirement documents to identify the ambiguities and incompleteness of the requirements.ResultsTwo experiment results show that the NLtoSTD method can be more effective in exposing the missing functionality and, in some cases, more ambiguous information than the fault-checklist method. Our experiments also revealed areas of improvement that benefit the method’s applicability in the future.ConclusionWe presented a new approach, NLtoSTD, to verify requirements documents and two controlled experiments assessing our approach. The results are promising and have motivated the refinement of the NLtoSTD method and future empirical evaluation. 
56|3||A UML profile for the conceptual modelling of structurally complex data: Easing human effort in the KDD process|ContextDomains where data have a complex structure requiring new approaches for knowledge discovery from data are on the increase. In such domains, the information related to each object under analysis may be composed of a very broad set of interrelated data instead of being represented by a simple attribute table. This further complicates their analysis.ObjectiveIt is becoming more and more necessary to model data before analysis in order to assure that they are properly understood, stored and later processed. On this ground, we have proposed a UML extension that is able to represent any set of structurally complex hierarchically ordered data. Conceptually modelled data are human comprehensible and constitute the starting point for automating other data analysis tasks, such as comparing items or generating reference models.MethodThe proposed notation has been applied to structurally complex data from the stabilometry field. Stabilometry is a medical discipline concerned with human balance. We have organized the model data through an implementation based on XML syntax.ResultsWe have applied data mining techniques to the resulting structured data for knowledge discovery. The sound results of modelling a domain with such complex and wide-ranging data confirm the utility of the approach.ConclusionThe conceptual modelling and the analysis of non-conventional data are important challenges. We have proposed a UML profile that has been tested on data from a medical domain, obtaining very satisfactory results. The notation is useful for understanding domain data and automating knowledge discovery tasks. 
56|3||Semantic-based automatic service composition with functional and non-functional requirements in design time: A genetic algorithm approach|ContextIn recent years, the composition of ready-made and loosely coupled services into desired systems is a common industrial approach and a widely followed research topic in academia. In the field, the current research trend is to automate this composition; however, each of the existing efforts automates only a component of the entire problem. Therefore, a real automation process that addresses all composition concerns is lacking.ObjectiveThe objective is to first identify the present composition concerns and subsequently to devise a compositional approach that covers all concerns. Ultimately, we conduct a number of experiments to investigate the proposed approach.MethodWe identify the current composition concerns by surveying and briefly describing the existing approaches. To include all of the identified concerns, the solution space that must be searched is highly dimensioned. Thus, we adopt a genetic algorithm (GA) due to its ability to solve problems with such characteristics. Proposed GA-based approach is designed with four unusual independent fitness functions. Additionally, experiments are carried out and discussions are presented for verification of the design, including the necessity for and correctness of the independence and priority of the four fitness functions.ResultsThe case studies demonstrate that our approach can automatically generate the required composite services and considers all identified concerns simultaneously. The results confirm the need for the independence of the fitness function and also identify a more efficient priority for these functions.ConclusionsIn this study, we present an all-inclusive automatic composer that does not require human intervention and effort during the composition process and is designed for users who must address multiple composition concerns simultaneously, including requirements for overall functionality, internally workable dataflow, and non-functional transaction and quality-of-service considerations. Such multiple and complex composition requirements cannot be satisfied by any of the previous single-concern composition approaches. 
56|4|http://www.sciencedirect.com/science/journal/09505849/56/4|Considering rigor and relevance when evaluating test driven development: A systematic review|ContextTest driven development (TDD) has been extensively researched and compared to traditional approaches (test last development, TLD). Existing literature reviews show varying results for TDD.ObjectiveThis study investigates how the conclusions of existing literature reviews change when taking two study quality dimension into account, namely rigor and relevance.MethodIn this study a systematic literature review has been conducted and the results of the identified primary studies have been analyzed with respect to rigor and relevance scores using the assessment rubric proposed by Ivarsson and Gorschek 2011. Rigor and relevance are rated on a scale, which is explained in this paper. Four categories of studies were defined based on high/low rigor and relevance.ResultsWe found that studies in the four categories come to different conclusions. In particular, studies with a high rigor and relevance scores show clear results for improvement in external quality, which seem to come with a loss of productivity. At the same time high rigor and relevance studies only investigate a small set of variables. Other categories contain many studies showing no difference, hence biasing the results negatively for the overall set of primary studies. Given the classification differences to previous literature reviews could be highlighted.ConclusionStrong indications are obtained that external quality is positively influenced, which has to be further substantiated by industry experiments and longitudinal case studies. Future studies in the high rigor and relevance category would contribute largely by focusing on a wider set of outcome variables (e.g. internal code quality). We also conclude that considering rigor and relevance in TDD evaluation is important given the differences in results between categories and in comparison to previous reviews. 
56|4||Dynamic stopping criteria for search-based test data generation for path testing|ContextEvolutionary algorithms have proved to be successful for generating test data for path coverage testing. However in this approach, the set of target paths to be covered may include some that are infeasible. It is impossible to find test data to cover those paths. Rather than searching indefinitely, or until a fixed limit of generations is reached, it would be desirable to stop searching as soon it seems likely that feasible paths have been covered and all remaining un-covered target paths are infeasible.ObjectiveThe objective is to develop criteria to halt the evolutionary test data generation process as soon as it seems not worth continuing, without compromising testing confidence level.MethodDrawing on software reliability growth models as an analogy, this paper proposes and evaluates a method for determining when it is no longer worthwhile to continue searching for test data to cover un-covered target paths. We outline the method, its key parameters, and how it can be used as the basis for different decision rules for early termination of a search. Twenty-one test programs from the SBSE path testing literature are used to evaluate the method.ResultsCompared to searching for a standard number of generations, an average of 30–75% of total computation was avoided in test programs with infeasible paths, and no feasible paths were missed due to early termination. The extra computation in programs with no infeasible paths was negligible.ConclusionsThe method is effective and efficient. It avoids the need to specify a limit on the number of generations for searching. It can help to overcome problems caused by infeasible paths in search-based test data generation for path testing. 
56|4||A tool supporting root cause analysis for synchronous retrospectives in distributed software teams|ContextRoot cause analysis (RCA) is a useful practice for software project retrospectives, and is typically carried out in synchronous collocated face-to-face meetings. Conducting RCA with distributed teams is challenging, as face-to-face meetings are infeasible. Lack of adequate real-time tool support exacerbates this problem. Furthermore, there are no empirical studies on using RCA in synchronous retrospectives of geographically distributed teams.ObjectiveThis paper presents a real-time cloud-based software tool (ARCA-tool) we developed to support RCA in distributed teams and its initial empirical evaluation. The feasibility of using RCA with distributed teams is also evaluated.MethodWe compared our tool with 35 existing RCA software tools. We conducted field studies of four distributed agile software teams at two international software product companies. The teams conducted RCA collaboratively in synchronous retrospective meetings by using the tool we developed. We collected the data using observations, interviews and questionnaires.ResultsComparison revealed that none of the existing 35 tools matched all the features of our ARCA-tool. The team members found ARCA-tool to be an essential part of their distributed retrospectives. They considered the software as efficient and very easy to learn and use. Additionally, the team members perceived RCA to be a vital part of the retrospectives. In contrast to the prior retrospective practices of the teams, the introduced RCA method was evaluated as efficient and easy to use.ConclusionRCA is a useful practice in synchronous distributed retrospectives. However, it requires software tool support for enabling real-time view and co-creation of a cause-effect diagram. ARCA-tool supports synchronous RCA, and includes support for logging problems and causes, problem prioritization, cause-effect diagramming, and logging of process improvement proposals. It enables conducting RCA in distributed retrospectives. 
56|4||Simulating upgrades of complex systems: The case of Free and Open Source Software|ContextThe upgrade of complex systems is intrinsically difficult and requires techniques, algorithms, and methods which are both expressive and computationally feasible in order to be used in practice. In the case of FOSS (Free and Open Source Software) systems, many upgrade errors cannot be discovered by current upgrade managers and then a system upgrade can potentially lead the system to an inconsistent and incoherent state.ObjectiveThe objective of this paper is to propose an approach to simulate the upgrade of complex systems in order to predict errors before they affect the real system.MethodThe approach promotes the use of model-driven engineering techniques to simulate the upgrade of complex systems. The basic idea is to have a model-based description of the system to be upgraded and to make use of model transformations to perform the upgrade on a source model so to obtain a target model representing the state of the upgraded system.ResultsWe provide an implementation of the simulator, which is tailored to FOSS systems. The architecture of the simulator is distribution independent so that it can be easily instantiated to specific distributions. The simulator takes into account also pre and post-installation scripts that equip each distribution package. This feature is extremely important since maintainer scripts are full-fledged programs that are run with system administration rights.ConclusionsThe paper shows the kind of errors the simulator is able to predict before upgrading the real system, and how the approach improves the state of the art of package managers while integrated in real Linux distribution installations. 
56|5|http://www.sciencedirect.com/science/journal/09505849/56/5|Performance in software development â Special issue editorial|
56|5||Analysing ISD performance using narrative networks, routines and mindfulness|ContextWhile the ISD process and in particular Requirement Elicitation has been defined as a collaborative social interaction, visualisations fail to accurately capture the multifaceted nature of the social process. Instead, ISD visualisations focus on presenting a more mechanical/technical perspective, ultimately restricting an opportunity to better analyse the process.ObjectiveWith particular focus on Requirements Elicitation this study utilises the narrative network technique to visualise the ISD process as a live routine with the aim to detail the ideal and actual aspects of ISD. The ideal aspect consists of the abstract, generalised understandings of the human actors regarding enacting a routine. Being a live routine, the reality is that adjustments/variations are a common occurrence and need to be taken into account. Enabling the opportunity to identify patterns of action within the routine, the study also incorporates organisational mindfulness to provide further social analysis of these patterns of action.MethodIn light of the lack of theoretical maturity around the viewing of an ISD process as a ‘live routine’, this exploratory research sought to build theory using a single instrumental case study spanning over 3 years (several ISD projects) and a variety of methods (e.g. workshop, experiment).ResultsVisualising the ISD process as a live routine it was possible to identify a number of patterns of action. These patterns were triangulated against the organisational mindfulness analysis of the case data to highlight underlying mindless behaviours. A rule was then implemented (experimentally) during the first iteration of a new ISD project and notable improvements in the ISD process were observed. Furthermore, the knock on organisational impact of the experimental implementation of the rule is also examined.ConclusionFrom a practitioner perspective the study provides an alternative method for analysing the ISD process of an organisation and highlights the benefits of observing the ISD process as a live routine. From an academic perspective, contributions are made to both the ISD and Organisational body of knowledge. 
56|5||Systematic analyses and comparison of development performance and product quality of Incremental Process and Agile Process|ContextAlthough Agile software development models have been widely used as a base for the software project life-cycle since 1990s, the number of studies that follow a sound empirical method and quantitatively reveal the effect of using these models over Traditional models is scarce.ObjectiveThis article explains the empirical method of and the results from systematic analyses and comparison of development performance and product quality of Incremental Process and Agile Process adapted in two projects of a middle-size, telecommunication software development company. The Incremental Process is an adaption of the Waterfall Model whereas the newly introduced Agile Process is a combination of the Unified Software Development Process, Extreme Programming, and Scrum.MethodThe method followed to perform the analyses and comparison is benefited from the combined use of qualitative and quantitative methods. It utilizes; GQM Approach to set measurement objectives, CMMI as the reference model to map the activities of the software development processes, and a pre-defined assessment approach to verify consistency of process executions and evaluate measure characteristics prior to quantitative analysis.ResultsThe results of the comparison showed that the Agile Process had performed better than the Incremental Process in terms of productivity (79%), defect density (57%), defect resolution effort ratio (26%), Test Execution V&V Effectiveness (21%), and effort prediction capability (4%). These results indicate that development performance and product quality achieved by following the Agile Process was superior to those achieved by following the Incremental Process in the projects compared.ConclusionThe acts of measurement, analysis, and comparison enabled comprehensive review of the two development processes, and resulted in understanding their strengths and weaknesses. The comparison results constituted objective evidence for organization-wide deployment of the Agile Process in the company. 
56|5||Performance appraisal of software testers|ContextTo determine the effectiveness of software testers a suitable performance appraisal approach is necessary, both for research and practice purposes. However, review of relevant literature reveals little information of how software testers are appraised in practice.Objective(i) To enhance our knowledge of industry practice of performance appraisal of software testers and (ii) to collect feedback from project managers on a proposed performance appraisal form for software testers.MethodA web-based survey with questionnaire was used to collect responses. Participants were recruited using cluster and snowball sampling. 18 software development project managers participated.ResultsWe found two broad trends in performance appraisal of software testers – same employee appraisal process for all employees and a specialized performance appraisal method for software testers. Detailed opinions were collected and analyzed on how performance of software testers should be appraised. Our proposed appraisal approach was generally well-received.ConclusionFactors such as number of bugs found after delivery and efficiency of executing test cases were considered important in appraising software testers’ performance. Our proposed approach was refined based on the feedback received. 
56|5||Performance on agile teams: Relating iteration objectives and critical decisions to project management success factors|ContextWhile project management success factors have long been established via the golden triangle, little is known about how project iteration objectives and critical decisions relate to these success factors. It seems logical that teams’ iteration objectives would reflect project management success factors, but this may not always be the case. If not, how are teams’ objectives for iterations differing from the golden triangle of project management success factors?ObjectiveThis study identifies iteration objectives and the critical decisions that relate to the golden triangle of project management success factors in agile software development teams working in two-week iterations.MethodThe author conducted semi-structured interviews with members across three different agile software development teams using a hybrid of XP and Scrum agile methodologies. Iteration Planning and Retrospective meetings were also observed. Interview data was transcribed, coded and reviewed by the researcher and two independently trained research assistants. Data analysis involved organizing the data to identify iteration objectives and critical decisions to identify whether they relate to project management success factors.ResultsAgile teams discussed four categories of iteration objectives: Functionality, Schedule, Quality and Team Satisfaction. Two of these objectives map directly to two aspects of the golden triangle: schedule and quality. The agile teams’ critical decisions were also examined to understand the types of decisions the teams would have made differently to ensure success, which resulted in four categories of such decisions: Quality, Dividing Work, Iteration Amendments and Team Satisfaction.ConclusionThis research has contributed to the software development and project management literature by examining iteration objectives on agile teams and how they relate to the golden triangle of project management success factors to see whether these teams incorporate the golden triangle factors in their objectives and whether they include additional objectives in their iterations. What’s more, this research identified four critical decisions related to the golden triangle. These findings provide important insight to the continuing effort to better assess project management success, particularly for agile teams. 
56|5||Evaluating performance in the development of software-intensive products|ContextOrganizational performance measurements in software product development have received a lot of attention in the literature. Still, there is a general discontent regarding the way performance is evaluated in practice, with few studies really focusing on why this is the case. In this paper research focusing on the context of developing software-intensive products in large established multi-national organizations is reported on.ObjectiveThe purpose of this research is to investigate performance measurement practices related to software product development activities. More specifically, focus is on exploring how managers engaged in software product development activities perceive and evaluate performance in large organizations from a managerial perspective.MethodThe research approach pursued in this research consist of exploratory multiple case studies. Data is collected mainly through 54 interviews in five case studies in large international organizations developing software-intensive products in Sweden. Focused group interviews with senior managers from eight companies have also been used in the data collection.ResultsThe results of this research indicate that managers within software product development in general are dissatisfied with their current way of evaluating performance. Performance measurements and the perception of performance are today focused on cost, time, and quality, i.e. what is easily measurable and not necessarily what is important. The dimensions of value creation and learning are missing. Moreover, measurements tend to be result oriented, rather than process oriented, making it difficult to integrate these measurements in the management practices.ConclusionManagers that are dissatisfied with their performance measurement system and want to improve the current situation should not start by focusing on the current measurements directly; instead they should focus on how the organization perceives performance and how important performance criteria are being developed. By developing relevant performance criteria the first step in developing an effective performance measurement system is made. Moreover, it is concluded that manager’s perception of performance is affected by the currently used measurements, hence limiting the scope of the performance criteria. Thus, a change in the way managers perceive performance is necessary before there can be any changes in the way performance is evaluated. 
56|6|http://www.sciencedirect.com/science/journal/09505849/56/6|Potential and limitations of the ISBSG dataset in enhancing software engineering research: A mapping review|ContextThe International Software Benchmarking Standards Group (ISBSG) maintains a software development repository with over 6000 software projects. This dataset makes it possible to estimate a project’s size, effort, duration, and cost.ObjectiveThe aim of this study was to determine how and to what extent, ISBSG has been used by researchers from 2000, when the first papers were published, until June of 2012.MethodA systematic mapping review was used as the research method, which was applied to over 129 papers obtained after the filtering process.ResultsThe papers were published in 19 journals and 40 conferences. Thirty-five percent of the papers published between years 2000 and 2011 have received at least one citation in journals and only five papers have received six or more citations. Effort variable is the focus of 70.5% of the papers, 22.5% center their research in a variable different from effort and 7% do not consider any target variable. Additionally, in as many as 70.5% of papers, effort estimation is the research topic, followed by dataset properties (36.4%). The more frequent methods are Regression (61.2%), Machine Learning (35.7%), and Estimation by Analogy (22.5%). ISBSG is used as the only support in 55% of the papers while the remaining papers use complementary datasets. The ISBSG release 10 is used most frequently with 32 references. Finally, some benefits and drawbacks of the usage of ISBSG have been highlighted.ConclusionThis work presents a snapshot of the existing usage of ISBSG in software development research. ISBSG offers a wealth of information regarding practices from a wide range of organizations, applications, and development types, which constitutes its main potential. However, a data preparation process is required before any analysis. Lastly, the potential of ISBSG to develop new research is also outlined. 
56|6||Knowledge-based approaches in software documentation: A systematic literature review|ContextSoftware documents are core artifacts produced and consumed in documentation activity in the software lifecycle. Meanwhile, knowledge-based approaches have been extensively used in software development for decades, however, the software engineering community lacks a comprehensive understanding on how knowledge-based approaches are used in software documentation, especially documentation of software architecture design.ObjectiveThe objective of this work is to explore how knowledge-based approaches are employed in software documentation, their influences to the quality of software documentation, and the costs and benefits of using these approaches.MethodWe use a systematic literature review method to identify the primary studies on knowledge-based approaches in software documentation, following a pre-defined review protocol.ResultsSixty studies are finally selected, in which twelve quality attributes of software documents, four cost categories, and nine benefit categories of using knowledge-based approaches in software documentation are identified. Architecture understanding is the top benefit of using knowledge-based approaches in software documentation. The cost of retrieving information from documents is the major concern when using knowledge-based approaches in software documentation.ConclusionsThe findings of this review suggest several future research directions that are critical and promising but underexplored in current research and practice: (1) there is a need to use knowledge-based approaches to improve the quality attributes of software documents that receive less attention, especially credibility, conciseness, and unambiguity; (2) using knowledge-based approaches with the knowledge content in software documents which gets less attention in current applications of knowledge-based approaches in software documentation, to further improve the practice of software documentation activity; (3) putting more focus on the application of software documents using the knowledge-based approaches (knowledge reuse, retrieval, reasoning, and sharing) in order to make the most use of software documents; and (4) evaluating the costs and benefits of using knowledge-based approaches in software documentation qualitatively and quantitatively. 
56|6||A systematic literature review of software requirements prioritization research|ContextDuring requirements engineering, prioritization is performed to grade or rank requirements in their order of importance and subsequent implementation releases. It is a major step taken in making crucial decisions so as to increase the economic value of a system.ObjectiveThe purpose of this study is to identify and analyze existing prioritization techniques in the context of the formulated research questions.MethodSearch terms with relevant keywords were used to identify primary studies that relate requirements prioritization classified under journal articles, conference papers, workshops, symposiums, book chapters and IEEE bulletins.Results73 Primary studies were selected from the search processes. Out of these studies; 13 were journal articles, 35 were conference papers and 8 were workshop papers. Furthermore, contributions from symposiums as well as IEEE bulletins were 2 each while the total number of book chapters amounted to 13.ConclusionPrioritization has been significantly discussed in the requirements engineering domain. However, it was generally discovered that, existing prioritization techniques suffer from a number of limitations which includes: lack of scalability, methods of dealing with rank updates during requirements evolution, coordination among stakeholders and requirements dependency issues. Also, the applicability of existing techniques in complex and real setting has not been reported yet. 
56|6||Querying large models efficiently|ContextThe paradigm of Model-Driven Engineering (MDE) has emerged as a new area of software engineering that uses models to improve the productivity and reusability of software in order to achieve industrial standards. As models grow in size and complexity, the need of model persistence and model querying solutions arises to efficiently store large models and obtain information from them in an efficient, usable and safe way. Morsa is a model repository that uses a No-SQL database backend; it has been recently presented [1] and achieves scalable access to models and transparent integration with tools.ObjectiveOur goal was to develop a query language for Morsa, as the existing model querying approaches cannot take advantage of the design of the our repository.MethodThe method followed in this paper comprises the following steps: (i) analyze the problem of model querying and identify a set of dimensions that can be used to characterize querying approaches; (ii) study and evaluate a representative set of model querying approaches and (iii) use the experience gained to design, develop and evaluate a dedicated model querying approach for Morsa that performs better than the studied ones (plain EMF, EMF Query, MDT OCL, IncQuery and CDO OCL). A test case has been defined to evaluate and compare the different approaches.ResultsThe contributions of this work are: first, an efficient, usable querying approach called Morsa Query Language (MorsaQL) that extends Morsa with querying capabilities, as the existing querying approaches cannot take advantage of our repository, and second, a comparative study of the current model persistence and querying approaches.ConclusionThe experience of analyzing and evaluating the different querying approaches has been very useful, as it has helped us developing our own one, which has been proven to be the best choice for Morsa. Moreover, the results of this paper can guide the MDE developers on which querying approach to use, depending on their needs. 
56|6||Perceived causes of software project failures â An analysis of their relationships|ContextSoftware project failures are common. Even though the reasons for failures have been widely studied, the analysis of their causal relationships is lacking. This creates an illusion that the causes of project failures are unrelated.ObjectiveThe aim of this study is to conduct in-depth analysis of software project failures in four software product companies in order to understand the causes of failures and their relationships. For each failure, we want to understand which causes, so called bridge causes, interconnect different process areas, and which causes were perceived as the most promising targets for process improvement.MethodThe causes of failures were detected by conducting root cause analysis. For each cause, we classified its type, process area, and interconnectedness to other causes. We quantitatively analyzed which type, process area, and interconnectedness categories (bridge, local) were common among the causes selected as the most feasible targets for process improvement activities. Finally, we qualitatively analyzed the bridge causes in order to find common denominators for the causal relationships interconnecting the process areas.ResultsFor each failure, our method identified causal relationships diagrams including 130–185 causes each. All four cases were unique, albeit some similarities occurred. On average, 50% of the causes were bridge causes. Lack of cooperation, weak task backlog, and lack of software testing resources were common bridge causes. Bridge causes, and causes related to tasks, people, and methods were common among the causes perceived as the most feasible targets for process improvement. The causes related to the project environment were frequent, but seldom perceived as feasible targets for process improvement.ConclusionPrevention of a software project failure requires a case-specific analysis and controlling causes outside the process area where the failure surfaces. This calls for collaboration between the individuals and managers responsible for different process areas. 
56|6||Systematizing requirements elicitation technique selection|ContextThis research deals with requirements elicitation technique selection for software product requirements and the overselection of open interviews.ObjectivesThis paper proposes and validates a framework to help requirements engineers select the most adequate elicitation techniques at any time.MethodWe have explored both the existing underlying theory and the results of empirical research to build the framework. Based on this, we have deduced and put together justified proposals about the framework components. We have also had to add information not found in theoretical or empirical sources. In these cases, we drew on our own experience and expertise.ResultsA new validated approach for requirements technique selection. This new approach selects techniques other than open interview, offers a wider range of possible techniques and captures more requirements information.ConclusionsThe framework is easily extensible and changeable. Whenever any theoretical or empirical evidence for an attribute, technique or adequacy value is unearthed, the information can be easily added to the framework. 
56|6||Mockup-Driven Development: Providing agile support for Model-Driven Web Engineering|ContextAgile software development approaches are currently becoming the industry standard for Web Application development. On the other hand, Model-Driven Web Engineering (MDWE) methodologies are known to improve productivity when building this kind of applications. However, current MDWE methodologies tend to ignore important aspects of Web Applications development supported by agile processes, such as constant customer feedback or early design of user interfaces.ObjectiveIn this paper we analyze the difficulties of supporting agile features in MDWE methodologies. Then, we propose an approach that eases the incorporation of well-known agile practices to MDWE.MethodWe propose using User Interface prototypes (usually known as mockups) as a way to start the modeling process in the context of a mixed agile-MDWE process. To assist this process, we defined a lightweight metamodel that allows modeling features over mockups, interacting with end-users and generating MDWE models. Then, we conducted a statistical evaluation of both approaches (traditional vs. mockup-based modeling).ResultsFirst we comment on how agile features can be added to MDWE processes using mockups. Then, we show by means of a quantitative study that the proposed approach is faster, less error-prone and still as complete as traditional MDWE processes.ConclusionThe use of mockups to guide the MDWE process helps in the reduction of the development cycle as well as in the incorporation of agile practices in the model-driven workflow. Complete MDWE models can be built and generated by using lightweight modeling over User Interface mockups, and this process suggests being more efficient, in terms of errors and effort, than traditional modeling in MDWE. 
56|7|http://www.sciencedirect.com/science/journal/09505849/56/7|An extended systematic literature review on provision of evidence for safety certification|ContextCritical systems in domains such as aviation, railway, and automotive are often subject to a formal process of safety certification. The goal of this process is to ensure that these systems will operate safely without posing undue risks to the user, the public, or the environment. Safety is typically ensured via complying with safety standards. Demonstrating compliance to these standards involves providing evidence to show that the safety criteria of the standards are met.ObjectiveIn order to cope with the complexity of large critical systems and subsequently the plethora of evidence information required for achieving compliance, safety professionals need in-depth knowledge to assist them in classifying different types of evidence, and in structuring and assessing the evidence. This paper is a step towards developing such a body of knowledge that is derived from a large-scale empirically rigorous literature review.MethodWe use a Systematic Literature Review (SLR) as the basis for our work. The SLR builds on 218 peer-reviewed studies, selected through a multi-stage process, from 4963 studies published between 1990 and 2012.ResultsWe develop a taxonomy that classifies the information and artefacts considered as evidence for safety. We review the existing techniques for safety evidence structuring and assessment, and further study the relevant challenges that have been the target of investigation in the academic literature. We analyse commonalities in the results among different application domains and discuss implications of the results for both research and practice.ConclusionThe paper is, to our knowledge, the largest existing study on the topic of safety evidence. The results are particularly relevant to practitioners seeking a better grasp on evidence requirements as well as to researchers in the area of system safety. As a major finding of the review, the results strongly suggest the need for more practitioner-oriented and industry-driven empirical studies in the area of safety certification. 
56|7||Metamodeling generalization and other directed relationships in UML|ContextGeneralization is a fundamental relationship in object orientation and in the UML (Unified Modeling Language). The generalization relationship is represented in the UML metamodel as a “directed relationship”.ObjectiveBeing a directed relationship corresponds to the nature of generalization in the semantic domain of object orientation: a relationship that is directed from the subclass to the superclass. However, we claim that the particular form this relationship adopts in the metamodel is erroneous, which entails a series of inconveniencies for model manipulation tools that try to adhere to the UML specification. Moreover, we think that this error could be due to a misinterpretation of the relationships between metamodeling levels in the UML: represented reality (M0), model (M1) and metamodel (M2). This problem also affects other directed relationships: Dependency and its various subtypes, Include and Extend between use cases, and others.MethodWe analyze the features of the generalization relationship in various domains and how it has been metamodeled in UML. We examine the problems, both theoretical and technological, posed by the UML metamodel of generalization. We then compare it with the metamodel of other directed relationships.ResultsWe arrive at the conclusion that the metamodel of all directed relationships could be improved. Namely, we claim that, at level M2, the metamodel should not contain any one-way meta-associations: all meta-associations should be two-way, both for practical and theoretical reasons.ConclusionsThe rationale for our main claim can be summarized as follows: connected graphical symbols do know each other, and the goal of a metamodel is to specify the syntactic properties of a language, ergo meta-associations must be two-way. This, of course, does not preclude at all the use of one-way associations at the user model level (M1). 
56|7||Agile product-line architecting in practice: A case study in smart grids|ContextSoftware Product Line Engineering implies the upfront design of a Product-Line Architecture (PLA) from which individual product applications can be engineered. The big upfront design associated with PLAs is in conflict with the current need of “being open to change”. To make the development of product-lines more flexible and adaptable to changes, several companies are adopting Agile Product Line Engineering. However, to put Agile Product Line Engineering into practice it is still necessary to make mechanisms available to assist and guide the agile construction and evolution of PLAs.ObjectiveThis paper presents the validation of a process for “the agile construction and evolution of product-line architectures”, called Agile Product-Line Architecting (APLA). The contribution of the APLA process is the integration of a set of models for describing, documenting, and tracing PLAs, as well as an algorithm for guiding the change decision-making process of PLAs. The APLA process is assessed to prove that assists Agile Product Line Engineering practitioners in the construction and evolution of PLAs.MethodValidation is performed through a case study by using both quantitative and qualitative analysis. Quantitative analysis was performed using statistics, whereas qualitative analysis was performed through interviews using constant comparison, triangulation, and supporting tools. This case study was conducted according to the guidelines of Runeson and Höst in a software factory where three projects in the domain of Smart Grids were involved.ResultsAPLA is deployed through the Flexible-PLA modeling framework. This framework supported the successful development and evolution of the PLA of a family of power metering management applications for Smart Grids.ConclusionsAPLA is a well-supported solution for the agile construction and evolution of PLAs. This case study illustrates that the proposed solution for the agile construction of PLAs is viable in an industry project on Smart Grids. 
56|7||Model-based testing of global properties on large-scale distributed systems|ContextLarge-scale distributed systems are becoming commonplace with the large popularity of peer-to-peer and cloud computing. The increasing importance of these systems contrasts with the lack of integrated solutions to build trustworthy software. A key concern of any large-scale distributed system is the validation of global properties, which cannot be evaluated on a single node. Thus, it is necessary to gather data from distributed nodes and to aggregate these data into a global view. This turns out to be very challenging because of the system’s dynamism that imposes very frequent changes in local values that affect global properties. This implies that the global view has to be frequently updated to ensure an accurate validation of global properties.ObjectiveIn this paper, we present a model-based approach to define a dynamic oracle for checking global properties. Our objective is to abstract relevant aspects of such systems into models. These models are updated at runtime, by monitoring the corresponding distributed system.MethodWe conduce real-scale experimental validation to evaluate the ability of our approach to check global properties. In this validation, we apply our approach to test two open-source implementations of distributed hash tables. The experiments are deployed on two clusters of 32 nodes.ResultsThe experiments reveal an important defect on one implementation and show clear performance differences between the two implementations. The defect would not be detected without a global view of the system.ConclusionTesting global properties on distributed software consists of gathering data from different nodes and building a global view of the system, where properties are validated. This process requires a distributed test architecture and tools for representing and validating global properties. Model-based techniques are an expressive mean for building oracles that validate global properties on distributed systems. 
56|7||Building hybrid access control by configuring RBAC and MAC features|ContextRole-Based Access Control (RBAC) and Mandatory Access Control (MAC) are widely used access control models. They are often used together in domains where both data integrity and information flow are concerned. However, there is little work on techniques for building hybrid access control of RBAC and MAC.ObjectiveIn this work, we present a systematic approach for developing a hybrid access control model using feature modeling with the aim of reducing development complexity and error-proneness.MethodIn the approach, RBAC and MAC are defined in terms of features based on partial inheritance. Features are then configured for specific access control requirements of an application. Configured features are composed homogeneously and heterogeneously to produce a hybrid access model for the application. The resulting hybrid model is then instantiated in the context of the application to produce an initial design model supporting both RBAC and MAC. We evaluate the approach using a hospital system and present its tool support.ResultsRBAC and MAC features that are specifically configured for the application are systematically incorporated into a design model. The heterogeneous features of RBAC and MAC are not only present in the resulting model, but also semantically composed for seamless integration of RBAC and MAC. Discharging the proof obligations of composition rules to the resulting model proves its correctness. The successful development of the prototype demonstrates its practicality.ConclusionFeatures in the access control domain are relatively small in size and are suitable to be defined as design building blocks. The formal definition of partial inheritance and composition methods in the presented approach enables precisely specifying access control features and feature configuration, which paves the way for systematic development of a hybrid access control model in an early development phase. 
56|7||Analyzing the relationships between inspections and testing to provide a software testing focus|ContextQuality assurance effort, especially testing effort, is frequently a major cost factor during software development. Consequently, one major goal is often to reduce testing effort. One promising way to improve the effectiveness and efficiency of software quality assurance is the use of data from early defect detection activities to provide a software testing focus. Studies indicate that using a combination of early defect data and other product data to focus testing activities outperforms the use of other product data only. One of the key challenges is that the use of data from early defect detection activities (such as inspections) to focus testing requires a thorough understanding of the relationships between these early defect detection activities and testing. An aggravating factor is that these relationships are highly context-specific and need to be evaluated for concrete environments.ObjectiveThe underlying goal of this paper is to help companies get a better understanding of these relationships for their own environment, and to provide them with a methodology for finding relationships in their own environments.MethodThis article compares three different strategies for evaluating assumed relationships between inspections and testing. We compare a confidence counter, different quality classes, and the F-measure including precision and recall.ResultsOne result of this case-study-based comparison is that evaluations based on the aggregated F-measures are more suitable for industry environments than evaluations based on a confidence counter. Moreover, they provide more detailed insights about the validity of the relationships.ConclusionWe have confirmed that inspection results are suitable data for controlling testing activities. Evaluated knowledge about relationships between inspections and testing can be used in the integrated inspection and testing approach In2Test to focus testing activities. Product data can be used in addition. However, the assumptions have to be evaluated in each new context. 
56|7||Towards a theoretical framework of SPI success factors for small and medium web companies|ContextThe context of this research is software process improvement (SPI) success factors for small and medium Web companies.ObjectiveThe primary objective of this paper is to propose a theoretical framework of SPI success factors for small and medium Web companies.MethodThe theoretical framework presented in this study aggregated the results of three previous research phases by applying principles of theoretical integration and comparative analysis. Those three previous phases were all empirical in nature, and comprise: a systematic review of SPI in small and medium Web companies  and ; a replication study [3] and a grounded theory-based initial exploratory framework of factors in small and medium Web companies [4].ResultsThe theoretical framework includes 18 categories of SPI success factors, 148 properties of these categories and 25 corresponding relationships, which bind these categories together. With the help of these relationships, the categories and properties of SPI success factors can be directly translated into a set of guidelines, which can then be used by the practitioners of small and medium Web companies to improve the current state of SPI in their companies and achieve overall company success.ConclusionThe comprehensive theoretical framework of SPI success factors presented herein provides evidence regarding key factors for predicting SPI success for small and medium Web companies. The framework can be used as a baseline for a successful implementation of SPI initiatives in the mentioned domain. 
56|8|http://www.sciencedirect.com/science/journal/09505849/56/8|Formal verification of static software models in MDE: A systematic review|ContextModel-driven Engineering (MDE) promotes the utilization of models as primary artifacts in all software engineering activities. Therefore, mechanisms to ensure model correctness become crucial, specially when applying MDE to the development of software, where software is the result of a chain of (semi)automatic model transformations that refine initial abstract models to lower level ones from which the final code is eventually generated. Clearly, in this context, an error in the model/s is propagated to the code endangering the soundness of the resulting software. Formal verification of software models is a promising approach that advocates the employment of formal methods to achieve model correctness, and it has received a considerable amount of attention in the last few years.ObjectiveThe objective of this paper is to analyze the state of the art in the field of formal verification of models, restricting the analysis to those approaches applied over static software models complemented or not with constraints expressed in textual languages, typically the Object Constraint Language (OCL).MethodWe have conducted a Systematic Literature Review (SLR) of the published works in this field, describing their main characteristics.ResultsThe study is based on a set of 48 resources that have been grouped in 18 different approaches according to their affinity. For each of them we have analyzed, among other issues, the formalism used, the support given to OCL, the correctness properties addressed or the feedback yielded by the verification process.ConclusionsOne of the most important conclusions obtained is that current model verification approaches are strongly influenced by the support given to OCL. Another important finding is that in general, current verification tools present important flaws like the lack of integration into the model designer tool chain or the lack of efficiency when verifying large, real-life models. 
56|8||A Systematic Mapping Study of Software Reliability Modeling|ContextSoftware Reliability (SR) is a highly active and dynamic research area. Published papers have approached this topic from various and heterogeneous points of view, resulting in a rich body of literature on this topic. The counterpart to this is the considerable complexity of this body of knowledge.ObjectiveThe objective of this study is to obtain a panorama and a taxonomy of Software Reliability Modeling (SRM).MethodIn order to do this, a Systematic Mapping Study (SMS) which analyzes and structures the literature on Software Reliability Modeling has been carried out.ResultsA total of 972 works were obtained as a result of the Systematic Mapping Study. On the basis of the more than 500 selected primary studies found, the results obtained show an increasing diversity of work.ConclusionAlthough it was discovered that Software Reliability Growth Models (SRGM) are still the most common modeling technique, it was also found that both the modeling based on static and architectural characteristics and the models based on Artificial Intelligence and automatic learning techniques are increasingly more apparent in literature. We have also observed that most Software Reliability Modeling efforts take place in the Pacific Rim area and in academic environments. Industrial initiatives are as yet marginal, and would appear to be primarily located in the USA. 
56|8||Past and future of software architectural decisions â A systematic mapping study|ContextThe software architecture of a system is the result of a set of architectural decisions. The topic of architectural decisions in software engineering has received significant attention in recent years. However, no systematic overview exists on the state of research on architectural decisions.ObjectiveThe goal of this study is to provide a systematic overview of the state of research on architectural decisions. Such an overview helps researchers reflect on previous research and plan future research. Furthermore, such an overview helps practitioners understand the state of research, and how research results can help practitioners in their architectural decision-making.MethodWe conducted a systematic mapping study, covering studies published between January 2002 and January 2012. We defined six research questions. We queried six reference databases and obtained an initial result set of 28,895 papers. We followed a search and filtering process that resulted in 144 relevant papers.ResultsAfter classifying the 144 relevant papers for each research question, we found that current research focuses on documenting architectural decisions. We found that only several studies describe architectural decisions from the industry. We identified potential future research topics: domain-specific architectural decisions (such as mobile), achieving specific quality attributes (such as reliability or scalability), uncertainty in decision-making, and group architectural decisions. Regarding empirical evaluations of the papers, around half of the papers use systematic empirical evaluation approaches (such as surveys, or case studies). Still, few papers on architectural decisions use experiments.ConclusionOur study confirms the increasing interest in the topic of architectural decisions. This study helps the community reflect on the past ten years of research on architectural decisions. Researchers are offered a number of promising future research directions, while practitioners learn what existing papers offer. 
56|8||Individual empowerment of agile and non-agile software developers in small teams|ContextEmpowerment of employees at work has been known to have a positive impact on job motivation and satisfaction. Software development is a field of knowledge work wherein one should also expect to see these effects, and the idea of empowerment has become particularly visible in agile methodologies, in which proponents emphasise team empowerment and individual control of the work activities as a central concern.ObjectiveThis research aims to get a better understanding of how empowerment is enabled in software development teams, both agile and non-agile, to identify differences in empowering practices and levels of individual empowerment.MethodTwenty-five interviews with agile and non-agile developers from Norway and Canada on decision making and empowerment are analysed. The analysis is conducted using a conceptual model with categories for involvement, structural empowerment and psychological empowerment.ResultsBoth kinds of development organisations are highly empowered and they are similar in most aspects relating to empowerment. However, there is a distinction in the sense that agile developers have more possibilities to select work tasks and influence the priorities in a development project due to team empowerment. Agile developers seem to put a higher emphasis on the value of information in decision making, and have more prescribed activities to enable low-cost information flow. More power is obtained through the achievement of managing roles for the non-agile developers who show interest and are rich in initiatives.ConclusionAgile developers have a higher sense of being able to impact the organisation than non-agile developers and have information channels that is significantly differently from non-agile developers. For non-agile teams, higher empowerment can be obtained by systematically applying low-cost participative decision making practices in the manager–developer relation and among peer developers. For agile teams, it is essential to more rigorously follow the empowering practices already established. 
56|8||Empirical evaluations on the cost-effectiveness of state-based testing: An industrial case study|ContextTest models describe the expected behavior of the software under test and provide the basis for test case and oracle generation. When test models are expressed as UML state machines, this is typically referred to as state-based testing (SBT). Despite the importance of being systematic while testing, all testing activities are limited by resource constraints. Thus, reducing the cost of testing while ensuring sufficient fault detection is a common goal in software development. No rigorous industrial case studies of SBT have yet been published.ObjectiveIn this paper, we evaluate the cost-effectiveness of SBT on actual control software by studying the combined influence of four testing aspects: coverage criterion, test oracle, test model and unspecified behavior (sneak paths).MethodAn industrial case study was used to investigate the cost-effectiveness of SBT. To enable the evaluation of SBT techniques, a model-based testing tool was configured and used to automatically generate test suites. The test suites were evaluated using 26 real faults collected in a field study.ResultsResults show that the more detailed and rigorous the test model and oracle, the higher the fault-detection ability of SBT. A less precise oracle achieved 67% fault detection, but the overall cost reduction of 13% was not enough to make the loss an acceptable trade-off. Removing details from the test model significantly reduced the cost by 85%. Interestingly, only a 24–37% reduction in fault detection was observed. Testing for sneak paths killed the remaining eleven mutants that could not be killed by the conformance test strategies.ConclusionsEach of the studied testing aspects influences cost-effectiveness and must be carefully considered in context when selecting strategies. Regardless of these choices, sneak-path testing is a necessary step in SBT since sneak paths are common while also undetectable by conformance testing. 
56|8||Understanding agile software development practices using shared mental models theory|ContextAgile software development is an alternative software development methodology that originated from practice to encourage collaboration between developers and users, to leverage rapid development cycles, and to respond to changes in a dynamic environment. Although agile practices are widely used in organizations, academics call for more theoretical research to understand the value of agile software development methodologies.ObjectiveThis study uses shared mental models theory as a lens to examine practices from agile software methodologies to understand how agile practices enable software development teams to work together to complete tasks and work together effectively as a team.MethodA conceptual analysis of specific agile practices was conducted using the lens of shared mental models theory. Three agile practices from Xtreme Programming and Scrum are examined in detail, system metaphor, stand-up meeting, and on-site customer, using shared mental models theory.ResultsExamining agile practices using shared mental models theory elucidates how agile practices improve collaboration during the software development process. The results explain how agile practices contribute toward a shared understanding and enhanced collaboration within the software development team.ConclusionsThis conceptual analysis demonstrates the value of agile practices in developing shared mental models (i.e. shared understanding) among developers and customers in software development teams. Some agile practices are useful in developing a shared understanding about the tasks to be completed, while other agile practices create shared mental models about team processes and team interactions. To elicit the desired outcomes of agile software development methods, software development teams should consider whether or not agile practices are used in a manner that enhances the team’s shared understanding. Using three specific agile practices as examples, this research demonstrates how theory, such as shared mental models theory, can enhance our understanding regarding how agile practices are useful in enhancing collaboration in the workplace. 
56|8||A CSCW Requirements Engineering CASE Tool: Development and usability evaluation|ContextCSRML Tool 2012 is a Requirements Engineering CASE Tool for the Goal-Oriented Collaborative Systems Requirements Modeling Language (CSRML).ObjectiveThis work aims at evaluating the usability of CSRML Tool 2012, thus identifying any possible usability flaw to be solved in the next releases of the application, as well as giving a general overview on how to develop a DSL tool similar to the one evaluated in this work by means of Visual Studio Modelling SDK.MethodIn this evaluation, which was reported by following the ISO/IEC 25062:2006 Common Industry Format for usability tests, 28 fourth-course Computer Science students took part. They were asked to carry out a series of modifications to an incomplete CSRML requirements specification. Usability was assessed by measuring the task’s completion rate, the elapsed time, number of accesses to the help system of the tool and the instructor’s verbal assistance. The participants’ arousal and pleasantness were assessed by analyzing both facial expressions and a USE questionnaire.ResultsIn spite of obtaining high usability levels, the test outcome revealed some usability flaws that should be addressed.ConclusionThe important lessons learnt from this evaluation are also applicable to the success of other usability tests as well as to the development of new CASE tools. 
56|8||Change impact analysis for requirements: A metamodeling approach|ContextFollowing the evolution of the business needs, the requirements of software systems change continuously and new requirements emerge frequently. Requirements documents are often textual artifacts with structure not explicitly given. When a change in a requirements document is introduced, the requirements engineer may have to manually analyze all the requirements for a single change. This may result in neglecting the actual impact of a change. Consequently, the cost of implementing a change may become several times higher than expected.ObjectiveIn this paper, we aim at improving change impact analysis in requirements by using formal semantics of requirements relations and requirements change types.MethodIn our previous work we present a requirements metamodel with commonly used requirements relation types and their semantics formalized in first-order logic. In this paper the classification of requirements changes based on structure of a textual requirement is provided with formal semantics. The formalization of requirements relations and changes is used for propagating proposed changes and consistency checking of proposed changes in requirements models. The tool support for change impact analysis in requirements models is an extension of our Tool for Requirements Inferencing and Consistency Checking (TRIC).ResultsThe described approach for change impact analysis helps in the elimination of some false positive impacts in change propagation, and enables consistency checking of changes.ConclusionWe illustrate our approach in an example which shows that the formal semantics of requirements relations and change classification enables change alternatives to be proposed semi-automatically, the reduction of some false positive impacts and contradicting changes in requirements to be determined. 
56|8||Personality, emotional intelligence and work preferences in software engineering: An empirical study|ContextThere is an increasing awareness among Software Engineering (SE) researchers and practitioners that more focus is needed on understanding the engineers developing software. Previous studies show significant associations between the personalities of software engineers and their work preferences.ObjectiveVarious studies on personality in SE have found large, small or no effects and there is no consensus on the importance of psychometric measurements in SE. There is also a lack of studies employing other psychometric instruments or using larger datasets. We aim to evaluate our results in a larger sample, with software engineers in an earlier state of their career, using advanced statistics.MethodAn operational replication study where extensive psychometric data from 279 master level students have been collected in a SE program at a Swedish University. Personality data based on the Five-Factor Model, Trait Emotional Intelligence Questionnaire and Self-compassion have been collected. Statistical analysis investigated associations between psychometrics and work preferences and the results were compared to our previous findings from 47 SE professionals.ResultsAnalysis confirms existence of two main clusters of software engineers; one with more “intense” personalities than the other. This corroborates our earlier results on SE professionals. The student data also show similar associations between personalities and work preferences. However, for other associations there are differences due to the different population of subjects. We also found connections between the emotional intelligence and work preferences, while no associations were found for self-compassion.ConclusionThe associations can help managers to predict and adapt projects and tasks to available staff. The results also show that the Emotional Intelligence instrument can be predictive. The research methods and analytical tools we employ can detect subtle associations and reflect differences between different groups and populations and thus can be important tools for future research as well as industrial practice. 
56|8||A noun-based approach to feature location using time-aware term-weighting|ContextFeature location aims to identify the source code location corresponding to the implementation of a software feature. Many existing feature location methods apply text retrieval to determine the relevancy of the features to the text data extracted from the software repositories. One of the preprocessing activities in text retrieval is term-weighting, which is used to adjust the importance of a term within a document or corpus. Common term-weighting techniques may not be optimal to deal with text data from software repositories due to the origin of term-weighting techniques from a natural language context.ObjectiveThis paper describes how the consideration of when the terms were used in the repositories, under the condition of weighting only the noun terms, can improve a feature location approach.MethodWe propose a feature location approach using a new term-weighting technique that takes into account how recently a term has been used in the repositories. In this approach, only the noun terms are weighted to reduce the dataset volume and avoid dealing with dimensionality reduction.ResultsAn empirical evaluation of the approach on four open-source projects reveals improvements to the accuracy, effectiveness and performance up to 50%, 17%, and 13%, respectively, when compared to the commonly-used Vector Space Model approach. The comparison of the proposed term-weighting technique with the Term Frequency-Inverse Document Frequency technique shows accuracy, effectiveness, and performance improvements as much as 15%, 10%, and 40%, respectively. The investigation of using only noun terms, instead of using all terms, in the proposed approach also indicates improvements up to 28%, 21%, and 58% on accuracy, effectiveness, and performance, respectively.ConclusionIn general, the use of time in the weighting of terms, along with the use of only the noun terms, makes significant improvements to a feature location approach that relies on textual information. 
56|8||MoDisco: A model driven reverse engineering framework|ContextMost companies, independently of their size and activity type, are facing the problem of managing, maintaining and/or replacing (part of) their existing software systems. These legacy systems are often large applications playing a critical role in the company’s information system and with a non-negligible impact on its daily operations. Improving their comprehension (e.g., architecture, features, enforced rules, handled data) is a key point when dealing with their evolution/modernization.ObjectiveThe process of obtaining useful higher-level representations of (legacy) systems is called reverse engineering (RE), and remains a complex goal to achieve. So-called Model Driven Reverse Engineering (MDRE) has been proposed to enhance more traditional RE processes. However, generic and extensible MDRE solutions potentially addressing several kinds of scenarios relying on different legacy technologies are still missing or incomplete. This paper proposes to make a step in this direction.MethodMDRE is the application of Model Driven Engineering (MDE) principles and techniques to RE in order to generate relevant model-based views on legacy systems, thus facilitating their understanding and manipulation. In this context, MDRE is practically used in order to (1) discover initial models from the legacy artifacts composing a given system and (2) understand (process) these models to generate relevant views (i.e., derived models) on this system.ResultsCapitalizing on the different MDRE practices and our previous experience (e.g., in real modernization projects), this paper introduces and details the MoDisco open source MDRE framework. It also presents the underlying MDRE global methodology and architecture accompanying this proposed tooling.ConclusionMoDisco is intended to make easier the design and building of model-based solutions dedicated to legacy systems RE. As an empirical evidence of its relevance and usability, we report on its successful application in real industrial projects and on the concrete experience we gained from that. 
56|8||Understanding replication of experiments in software engineering: A classification|ContextReplication plays an important role in experimental disciplines. There are still many uncertainties about how to proceed with replications of SE experiments. Should replicators reuse the baseline experiment materials? How much liaison should there be among the original and replicating experimenters, if any? What elements of the experimental configuration can be changed for the experiment to be considered a replication rather than a new experiment?ObjectiveTo improve our understanding of SE experiment replication, in this work we propose a classification which is intend to provide experimenters with guidance about what types of replication they can perform.MethodThe research approach followed is structured according to the following activities: (1) a literature review of experiment replication in SE and in other disciplines, (2) identification of typical elements that compose an experimental configuration, (3) identification of different replications purposes and (4) development of a classification of experiment replications for SE.ResultsWe propose a classification of replications which provides experimenters in SE with guidance about what changes can they make in a replication and, based on these, what verification purposes such a replication can serve. The proposed classification helped to accommodate opposing views within a broader framework, it is capable of accounting for less similar replications to more similar ones regarding the baseline experiment.ConclusionThe aim of replication is to verify results, but different types of replication serve special verification purposes and afford different degrees of change. Each replication type helps to discover particular experimental conditions that might influence the results. The proposed classification can be used to identify changes in a replication and, based on these, understand the level of verification. 
56|8||Needsâ elaboration between users, designers and project leaders: Analysis of a design process of a virtual reality-based software|ContextThe participation of users in the design process is recognized as a positive and a necessary element as artifacts suit their needs. Two complementary approaches of users’ involvement co-exist: the user-centered design and the participatory design. These approaches involve learning process from users to designers and vice versa. However, there has no research in design of virtual reality (VR)-based software dealing with how the elaboration of needs is actually distributed in time and among users, designers and project leaders, as well as how it is actually supported by tools and methods.ObjectiveThis paper aims to observe, in a real design project of a virtual reality-based software, how the various stakeholders (users, designers, project leaders) actually participate by sharing and pulling pieces of information from the process of needs elaboration, and how these contributions evolve throughout the decisions made in the course of the project.MethodOur method, based on the observation of the practices in collective design, allows us to collect and analyze the relationship between each possible action (e.g., elicitation), each stakeholder who initiates these actions (e.g., users) and each phase of the design process (e.g., evaluation phase), and the dynamics of the construction of needs.ResultsOur results detail how the elicited needs are dealt with by designers, users and/or project leaders: (1) we show a strong contribution of users in the design, compared to others stakeholders, (2) among the needs elicited by users, most have been validated by the designers, (3) some elicited needs could have been firstly rejected and finally validated and implemented.ConclusionWe identify the reasons which justify and explain our results confronting them to the literature. We underline the conditions have been satisfied in our study in order to involve effectively users in the design of emerging technologies. 
56|9|http://www.sciencedirect.com/science/journal/09505849/56/9|Investigating the use of duration-based moving windows to improve software effort prediction: A replicated study|ContextMost research in software effort estimation has not considered chronology when selecting projects for training and testing sets. A chronological split represents the use of a projects starting and completion dates, such that any model that estimates effort for a new project p only uses as training data projects that were completed prior to p’s start. Four recent studies investigated the use of chronological splits, using moving windows wherein only the most recent projects completed prior to a projects starting date were used as training data. The first three studies (S1–S3) found some evidence in favor of using windows; they all defined window sizes as being fixed numbers of recent projects. In practice, we suggest that estimators think in terms of elapsed time rather than the size of the data set, when deciding which projects to include in a training set. In the fourth study (S4) we showed that the use of windows based on duration can also improve estimation accuracy.ObjectiveThis papers contribution is to extend S4 using an additional dataset, and to also investigate the effect on accuracy when using moving windows of various durations.MethodStepwise multivariate regression was used to build prediction models, using all available training data, and also using windows of various durations to select training data. Accuracy was compared based on absolute residuals and MREs; the Wilcoxon test was used to check statistical significances between results. Accuracy was also compared against estimates derived from windows containing fixed numbers of projects.ResultsNeither fixed size nor fixed duration windows provided superior estimation accuracy in the new data set.ConclusionsContrary to intuition, our results suggest that it is not always beneficial to exclude old data when estimating effort for new projects. When windows are helpful, windows based on duration are effective. 
56|9||A class loading sensitive approach to detection of runtime type errors in component-based Java programs|ContextThe employment of class loaders in component-based Java programs may introduce runtime type errors, which may happen at any statement related to class loading, and may be wrapped into various types of exceptions raised by JVM. Traditional static analysis approaches are inefficient to detect them.ObjectiveOur previous work proposed a semi-static detection work based on points-to analysis to detect such runtime type errors. In this paper, we extend previous work by referencing the information obtained from class loading to detect runtime type errors in component-based Java programs, without the need to running them.MethodOur approach extends the typical points-to analysis by gathering the behavior information of Java class loaders and figuring out the defining class loader of the allocation sites. By doing that, we obtain the runtime types of objects a reference variable may point to, and make use of such information to facilitate runtime type error detecting.ResultsResults on four case studies show that our approach is feasible, can effectively detect runtime errors missed by traditional static checking methods, and performs acceptably in both false negative test and scalability test. 
56|9||Low-disruptive dynamic updating of Java applications| ContextIn-use software systems are destined to change in order to fix bugs or add new features. Shutting down a running system before updating it is a normal practice, but the service unavailability can be annoying and sometimes unacceptable. Dynamic software updating (DSU) migrates a running software system to a new version without stopping it. State-of-the-art Java DSU systems are unsatisfactory as they may cause a non-negligible system pause during updating.ObjectiveIn this paper we present Javelus, a Java HotSpot VM-based Java DSU system with very short pausing time.MethodInstead of updating everything at once when the running application is suspended, Javelus only updates the changed code during the suspension, and migrates stale objects on-demand after the application is resumed. With a careful design this lazy approach neither sacrifices the update flexibility nor introduces unnecessary object validity checks or access indirections.ResultsEvaluation experiments show that Javelus can reduce the updating pausing time by one to two orders of magnitude without introducing observable overheads before and after the dynamic updating.ConclusionOur experience with Javelus indicates that low-disruptive and type-safe dynamic updating of Java applications can be practically achieved with a lazy updating approach. 
56|9||Editorial for the special section on Software Product Line Engineering: Selected papers from Software Product Line conference in 2012|
56|9||A framework for variable content document generation with multiple actors|ContextAdvances in customization have highlighted the need for tools supporting variable content document management and generation in many domains. Current tools allow the generation of highly customized documents that are variable in both content and layout. However, most frameworks are technology-oriented, and their use requires advanced skills in implementation-related tools, which means their use by end users (i.e. document designers) is severely limited.ObjectiveStarting from past and current trends for customized document authoring, our goal is to provide a document generation alternative in which variants are specified at a high level of abstraction and content reuse can be maximized in high variability scenarios.MethodBased on our experience in Document Engineering, we identified areas in the variable content document management and generation field open to further improvement. We first classified the primary sources of variability in document composition processes and then developed a methodology, which we called DPL – based on Software Product Lines principles – to support document generation in high variability scenarios.ResultsIn order to validate the applicability of our methodology we implemented a tool – DPLfw – to carry out DPL processes. After using this in different scenarios, we compared our proposal with other state-of-the-art tools for variable content document management and generation.ConclusionThe DPLfw showed a good capacity for the automatic generation of variable content documents equal to or in some cases surpassing other currently available approaches. To the best of our knowledge, DPLfw is the only framework that combines variable content and document workflow facilities, easing the generation of variable content documents in which multiple actors play different roles. 
56|9||Efficient synthesis of feature models|ContextVariability modeling, and in particular feature modeling, is a central element of model-driven software product line architectures. Such architectures often emerge from legacy code, but, creating feature models from large, legacy systems is a long and arduous task. We describe three synthesis scenarios that can benefit from the algorithms in this paper.ObjectiveThis paper addresses the problem of automatic synthesis of feature models from propositional constraints. We show that the decision version of the problem is NP-hard. We designed two efficient algorithms for synthesis of feature models from CNF and DNF formulas respectively.MethodWe performed an experimental evaluation of the algorithms against a binary decision diagram (BDD)-based approach and a formal concept analysis (FCA)-based approach using models derived from realistic models.ResultsOur evaluation shows a 10 to 1,000-fold performance improvement for our algorithms over the BDD-based approach. The performance of the DNF-based algorithm was similar to the FCA-based approach, with advantages for both techniques. We identified input properties that affect the runtimes of the CNF- and DNF-based algorithms.ConclusionsOur algorithms are the first known techniques that are efficient enough to be used on dependencies extracted from real systems, opening new possibilities of creating reverse engineering and model management tools for variability models. 
56|9||Toward automated feature model configuration with optimizing non-functional requirements|ContextA software product line is a family of software systems that share some common features but also have significant variabilities. A feature model is a variability modeling artifact, which represents differences among software products with respect to the variability relationships among their features. Having a feature model along with a reference model developed in the domain engineering lifecycle, a concrete product of the family is derived by binding the variation points in the feature model (called configuration process) and by instantiating the reference model.ObjectiveIn this work we address the feature model configuration problem and propose a framework to automatically select suitable features that satisfy both the functional and non-functional preferences and constraints of stakeholders. Additionally, interdependencies between various non-functional properties are taken into account in the framework.MethodThe proposed framework combines Analytical Hierarchy Process (AHP) and Fuzzy Cognitive Maps (FCM) to compute the non-functional properties weights based on stakeholders’ preferences and interdependencies between non-functional properties. Afterwards, Hierarchical Task Network (HTN) planning is applied to find the optimal feature model configuration.ResultOur approach improves state-of-art of feature model configuration by considering positive or negative impacts of the features on non-functional properties, the stakeholders’ preferences, and non-functional interdependencies. The approach presented in this paper extends earlier work presented in [1] from several distinct perspectives including mechanisms handling interdependencies between non-functional properties, proposing a novel tooling architecture, and offering visualization and interaction techniques for representing functional and non-functional aspects of feature models.Conclusionour experiments show the scalability of our configuration approach when considering both functional and non-functional requirements of stakeholders. 
57|-|http://www.sciencedirect.com/science/journal/09505849/57|Automatic generation algorithm of expected results for testing of component-based software system|ContextA component-based software (CBS) system is a typical example of a composite component, which is composed of two or more atomic components. In the test of a CBS system, it is necessary to automatically generate expected results because they are compared with the actual results.ObjectiveThis paper proposes an automatic generation algorithm of expected results for the testing of a CBS system.MethodThe algorithm analyzes Input/Output (I/O) relationships of a CBS system to identify inputs that influence its outputs. Then, the algorithm uses test cases of atomic components for each input and automatically generates expected results. To evaluate the proposed approach, we compare the proposed approach with the other I/O relationship based approach via two case studies which are the CBS systems for guide robot. The comparison shows the effectiveness of our approach.ResultsTo verify effectiveness of the proposed approach, we measure the cost of manual generation of expected results for each case study. The costs are from 0.001 to 0.015 in the first case study, and are from 0.590 to 0.998 in the second case study.ConclusionThis proposed approach reduces the time-consuming and error-prone task which manually generates expected results for the testing of a CBS system. 
57|-||On the journey to continuous deployment: Technical and social challenges along the way|ContextContinuous Deployment (CD) is an emerging software development process with organisations such as Facebook, Microsoft, and IBM successfully implementing and using the process. The CD process aims to immediately deploy software to customers as soon as new code is developed, and can result in a number of benefits for organisations, such as: new business opportunities, reduced risk for each release, and prevent development of wasted software. There is little academic literature on the challenges organisations face when adopting the CD process, however there are many anecdotal challenges that organisations have voiced on their online blogs.ObjectiveThe aim of this research is to examine the challenges faced by organisations when adopting CD as well as the strategies to mitigate these challenges.MethodAn explorative case study technique that involves in-depth interviews with software practitioners in an organisation that has adopted CD was conducted to identify these challenges.ResultsThis study found a total of 20 technical and social adoption challenges that organisations may face when adopting the CD process. The results are discussed to gain a deeper understanding of the strategies employed by organisations to mitigate the impacts of these challenges.ConclusionWhile a number of individual technical and social adoption challenges were uncovered by the case study in this research, most challenges were not faced in isolation. The severity of these challenges were reduced by a number of mitigation strategies employed by the case study organisation. It is concluded that organisations need to be well prepared to handle technical and social adoption challenges with their existing expertise, processes and tools before adopting the CD process. For practitioners, knowing how to address the challenges an organisation may face when adopting the CD process provides a level of awareness that they previously may not have had. 
57|-||A concern-oriented framework for dynamic measurements|Evolving software programs requires that software developers reason quantitatively about the modularity impact of several concerns, which are often scattered over the system. To this respect, concern-oriented software analysis is rising to a dominant position in software development. Hence, measurement techniques play a fundamental role in assessing the concern modularity of a software system. Unfortunately, existing measurements are still fundamentally module-oriented rather than concern-oriented. Moreover, the few available concern-oriented metrics are defined in a non-systematic and shared way and mainly focus on static properties of a concern, even if many properties can only be accurately quantified at run-time. Hence, novel concern-oriented measurements and, in particular, shared and systematic ways to define them are still welcome. This paper poses the basis for a unified framework for concern-driven measurement. The framework provides a basic terminology and criteria for defining novel concern metrics. To evaluate the framework feasibility and effectiveness, we have shown how it can be used to adapt some classic metrics to quantify concerns and in particular to instantiate new dynamic concern metrics from their static counterparts. 
57|-||Towards a governance framework for chains of Scrum teams|ContextLarge companies operating in the information intensive industries increasingly adopt Agile/Scrum to swiftly change IT functionality because of rapid changing business demands. IT functionality in large enterprises however is typically delivered by a portfolio of interdependent software applications involving a chain of Scrum teams. Usually, each application from the portfolio is allocated to a single Scrum team, which necessitates collaboration between the Scrum teams to jointly deliver functionality.ObjectiveIdentify the collaboration related issues in chains of Scrum teams.MethodWe used a qualitative approach with transcripted interviews from three case studies that were coded and analyzed to identify the issues.ResultsWe identified six issues in chains of codependent Scrum teams; coordination, prioritization, alignment, automation, predictability and visibility. The synthesis of these issues with existing theory resulted in nine propositions. These nine propositions have been combined into a conceptual model.ConclusionWe propose this conceptual model as a starting point for a governance framework to manage chains of Scrum teams that addresses the identified issues. 
57|-||Exception handling analysis and transformation using fault injection: Study of resilience against unanticipated exceptions|ContextIn software, there are the error cases that are anticipated at specification and design time, those encountered at development and testing time, and those that were never anticipated before happening in production. Is it possible to learn from the anticipated errors during design to analyze and improve the resilience against the unanticipated ones in production?ObjectiveIn this paper, we aim at analyzing and improving how software handles unanticipated exceptions. The first objective is to set up contracts about exception handling and a way to assess them automatically. The second one is to improve the resilience capabilities of software by transforming the source code.MethodWe devise an algorithm, called short-circuit testing, which injects exceptions during test suite execution so as to simulate unanticipated errors. It is a kind of fault-injection techniques dedicated to exception-handling. This algorithm collects data that is used for verifying two formal contracts that capture two resilience properties w.r.t. exceptions: the source-independence and pure-resilience contracts. Then we propose a code modification technique, called “catch-stretching” which allows error-recovery code (of the form of catch blocks) to be more resilient.ResultsOur evaluation is performed on 9 open-source software applications and consists in analyzing 241 catch blocks executed during test suite execution. Our results show that 101/214 of them (47%) expose resilience properties as defined by our exception contracts and that 84/214 of them (39%) can be transformed to be more resilient.ConclusionOur work shows that it is possible to reason on software resilience by injecting exceptions during test suite execution. The collected information allows us to apply one source code transformation that improves the resilience against unanticipated exceptions. This works best if the test suite exercises the exceptional programming language constructs in many different scenarios. 
57|-||An empirically-based characterization and quantification of information seeking through mailing lists during Open Source developersâ software evolution|ContextSeveral authors have proposed information seeking as an appropriate perspective for studying software evolution. Empirical evidence in this area suggests that substantial time delays can accrue, due to the unavailability of required information, particularly when this information must travel across geographically distributed sites.ObjectiveAs a first step in addressing the time delays that can occur in information seeking for distributed Open Source (OS) programmers during software evolution, this research characterizes the information seeking of OS developers through their mailing lists.MethodA longitudinal study that analyses 17 years of developer mailing list activity in total, over 6 different OS projects is performed, identifying the prevalent information types sought by developers, from a qualitative, grounded analysis of this data. Quantitative analysis of the number-of-responses and response time-lag is also performed.ResultsThe analysis shows that Open Source developers are particularly implementation centric and team focused in their use of mailing lists, mirroring similar findings that have been reported in the literature. However novel findings include the suggestion that OS developers often require support regarding the technology they use during development, that they refer to documentation fairly frequently and that they seek implementation-oriented specifics based on system design principles that they anticipate in advance. In addition, response analysis suggests a large variability in the response rates for different types of questions, and particularly that participants have difficulty ascertaining information on other developer’s activities.ConclusionThe findings provide insights for those interested in supporting the information needs of OS developer communities: They suggest that the tools and techniques developed in support of co-located developers should be largely mirrored for these communities: that they should be implementation centric, and directed at illustrating “how” the system achieves its functional goals and states. Likewise they should be directed at determining the reason for system bugs: a type of question frequently posed by OS developers but less frequently responded to. 
57|-||BPMNt: A BPMN extension for specifying software process tailoring|ContextAlthough SPEM 2.0 has great potential for software process modeling, it does not provide concepts or formalisms for precise modeling of process behavior. Indeed, SPEM fails to address process simulation, execution, monitoring and analysis, which are important activities in process management. On the other hand, BPMN 2.0 is a widely used notation to model business processes that has associated tools and techniques to facilitate the aforementioned process management activities. Using BPMN to model software development processes can leverage BPMN’s infrastructure to improve the quality of these processes. However, BPMN lacks an important feature to model software processes: a mechanism to represent process tailoring.ObjectiveThis paper proposes BPMNt, a conservative extension to BPMN that aims at creating a tailoring representation mechanism similar to the one found in SPEM 2.0.MethodWe have used the BPMN 2.0 extensibility mechanism to include the representation of specific tailoring relationships namely suppression, local contribution, and local replacement, which establish links between process elements (such as in the case of SPEM). Moreover, this paper also presents some rules to ensure the consistency of BPMN models when using tailoring relationships.ResultsIn order to evaluate our proposal we have implemented a tool to support the BPMNt approach and have applied it for representing real process adaptations in the context of an academic management system development project. Results of this study showed that the approach and its support tool can successfully be used to adapt BPMN-based software processes in real scenarios.ConclusionWe have proposed an approach to enable reuse and adaptation of BPMN-based software process models as well as derivation traceability between models through tailoring relationships. We believe that bringing such capabilities into BPMN will open new perspectives to software process management. 
57|-||Operational release planning in large-scale Scrum with multiple stakeholders â A longitudinal case study at F-Secure Corporation|ContextThe analysis and selection of requirements are important parts of any release planning process. Previous studies on release planning have focused on plan-driven optimization models. Unfortunately, solving the release planning problem mechanistically is difficult in an agile development context.ObjectiveWe describe how a release planning method was employed in two case projects in F-Secure, a large Finnish software company. We identify the benefits which the projects gained from the method, and analyze challenges in the cases and improvements made to the method during the case projects.MethodWe observed five release planning events and four retrospectives and we conducted surveys in the first two events. We conducted six post-project interviews. We conjoined the observation notes, survey results and interviews and analyzed them qualitatively and quantitatively.ResultsThe focal point of the method was release planning events where the whole project organization gathered to plan the next release. The planning was conducted by the development teams in close collaboration with each other and with the other stakeholders. We identified ten benefits which included improved communication, transparency, dependency management and decision making. We identified nine challenges which included the lacking preparation and prioritization of requirements, unrealistic schedules, insufficient architectural planning and lacking agile mindset. The biggest improvements to the method were the introduction of frequent status checks and a big visible planning status board.ConclusionThe release planning method ameliorated many difficult characteristics of the release planning problem but its efficiency was negatively affected by the performing organization that was in transition from a plan-driven to an agile development mindset. Even in this case the benefits clearly outweighed the challenges and the method enabled the early identification of the issues in the project. 
57|-||Are team personality and climate related to satisfaction and software quality? Aggregating results from a twice replicated experiment|ContextResearch into software engineering teams focuses on human and social team factors. Social psychology deals with the study of team formation and has found that personality factors and group processes such as team climate are related to team effectiveness. However, there are only a handful of empirical studies dealing with personality and team climate and their relationship to software development team effectiveness.ObjectiveWe present aggregate results of a twice replicated quasi-experiment that evaluates the relationships between personality, team climate, product quality and satisfaction in software development teams.MethodOur experimental study measures the personalities of team members based on the Big Five personality traits (openness, conscientiousness, extraversion, agreeableness, neuroticism) and team climate factors (participative safety, support for innovation, team vision and task orientation) preferences and perceptions. We aggregate the results of the three studies through a meta-analysis of correlations. The study was conducted with students.ResultsThe aggregation of results from the baseline experiment and two replications corroborates the following findings. There is a positive relationship between all four climate factors and satisfaction in software development teams. Teams whose members score highest for the agreeableness personality factor have the highest satisfaction levels. The results unveil a significant positive correlation between the extraversion personality factor and software product quality. High participative safety and task orientation climate perceptions are significantly related to quality.ConclusionsFirst, more efficient software development teams can be formed heeding personality factors like agreeableness and extraversion. Second, the team climate generated in software development teams should be monitored for team member satisfaction. Finally, aspects like people feeling safe giving their opinions or encouraging team members to work hard at their job can have an impact on software quality. Software project managers can take advantage of these factors to promote developer satisfaction and improve the resulting product. 
57|-||Gamification in software engineering â A systematic mapping|ContextGamification seeks for improvement of the user’s engagement, motivation, and performance when carrying out a certain task, by means of incorporating game mechanics and elements, thus making that task more attractive. Much research work has studied the application of gamification in software engineering for increasing the engagement and results of developers.ObjectiveThe objective of this paper is to carry out a systematic mapping of the field of gamification in software engineering in an attempt to characterize the state of the art of this field identifying gaps and opportunities for further research.MethodWe carried out a systematic mapping with a view to finding the primary studies in the existing literature, which were later classified and analyzed according to four criteria: the software process area addressed, the gamification elements used, the type of research method followed, and the type of forum in which they were published. A subjective evaluation of the studies was also carried out to evaluate them in terms of methodology, empirical evidence, integration with the organization, and replicability.ResultsAs a result of the systematic mapping we found 29 primary studies, published between January 2011 and June 2014. Most of them focus on software development, and to a lesser extent, requirements, project management, and other support areas. In the main, they consider very simple gamification mechanics such as points and badges, and few provide empirical evidence of the impact of gamification.ConclusionsExisting research in the field is quite preliminary, and more research effort analyzing the impact of gamification in SE would be needed. Future research work should look at other game mechanics in addition to the basic ones and should tackle software process areas that have not been fully studied, such as requirements, project management, maintenance, or testing. Most studies share a lack of methodological support that would make their proposals replicable in other settings. The integration of gamification with an organization’s existing tools is also an important challenge that needs to be taken up in this field. 
57|-||Testing robot controllers using constraint programming and continuous integration|ContextTesting complex industrial robots (CIRs) requires testing several interacting control systems. This is challenging, especially for robots performing process-intensive tasks such as painting or gluing, since their dedicated process control systems can be loosely coupled with the robot’s motion control.ObjectiveCurrent practices for validating CIRs involve manual test case design and execution. To reduce testing costs and improve quality assurance, a trend is to automate the generation of test cases. Our work aims to define a cost-effective automated testing technique to validate CIR control systems in an industrial context.MethodThis paper reports on a methodology, developed at ABB Robotics in collaboration with SIMULA, for the fully automated testing of CIRs control systems. Our approach draws on continuous integration principles and well-established constraint-based testing techniques. It is based on a novel constraint-based model for automatically generating test sequences where test sequences are both generated and executed as part of a continuous integration process.ResultsBy performing a detailed analysis of experimental results over a simplified version of our constraint model, we determine the most appropriate parameterization of the operational version of the constraint model. This version is now being deployed at ABB Robotics’s CIR testing facilities and used on a permanent basis. This paper presents the empirical results obtained when automatically generating test sequences for CIRs at ABB Robotics. In a real industrial setting, the results show that our methodology is not only able to detect reintroduced known faults, but also to spot completely new faults.ConclusionOur empirical evaluation shows that constraint-based testing is appropriate for automatically generating test sequences for CIRs and can be faithfully deployed in an industrial context. 
57|-||An empirical analysis of package-modularization metrics: Implications for software fault-proneness|ContextIn a large object-oriented software system, packages play the role of modules which group related classes together to provide well-identified services to the rest of the system. In this context, it is widely believed that modularization has a large influence on the quality of packages. Recently, Sarkar, Kak, and Rama proposed a set of new metrics to characterize the modularization quality of packages from important perspectives such as inter-module call traffic, state access violations, fragile base-class design, programming to interface, and plugin pollution. These package-modularization metrics are quite different from traditional package-level metrics, which measure software quality mainly from size, extensibility, responsibility, independence, abstractness, and instability perspectives. As such, it is expected that these package-modularization metrics should be useful predictors for fault-proneness. However, little is currently known on their actual usefulness for fault-proneness prediction, especially compared with traditional package-level metrics.ObjectiveIn this paper, we examine the role of these new package-modularization metrics for determining software fault-proneness in object-oriented systems.MethodWe first use principal component analysis to analyze whether these new package-modularization metrics capture additional information compared with traditional package-level metrics. Second, we employ univariate prediction models to investigate how these new package-modularization metrics are related to fault-proneness. Finally, we build multivariate prediction models to examine the ability of these new package-modularization metrics for predicting fault-prone packages.ResultsOur results, based on six open-source object-oriented software systems, show that: (1) these new package-modularization metrics provide new and complementary views of software complexity compared with traditional package-level metrics; (2) most of these new package-modularization metrics have a significant association with fault-proneness in an expected direction; and (3) these new package-modularization metrics can substantially improve the effectiveness of fault-proneness prediction when used with traditional package-level metrics together.ConclusionsThe package-modularization metrics proposed by Sarkar, Kak, and Rama are useful for practitioners to develop quality software systems. 
57|-||Generating semantically valid test inputs using constrained input grammars|ContextGenerating test cases based on software input interface is a black-box testing technique that can be made more effective by using structured input models such as input grammars. Automatically generating grammar-based test inputs may lead to structurally valid but semantically invalid inputs that may be rejected in early semantic error checking phases of a system under test.ObjectiveThis paper aims to introduce a method for specifying a grammar-based input model with the model’s semantic constraints to be used in the generation of positive test inputs. It is also important that the method can generate effective test suites based on appropriate grammar-based coverage criteria.MethodFormal specification of both input structure and input semantics provides the opportunity to use model instantiation techniques to create model instances that satisfy all specified constraints. The input interface of a subject system can be specified using a high-level specification scheme such as attribute grammars, and a transformation function from this scheme to an instantiable formal modeling language can generate the desired model instances.ResultsWe propose a declarative grammar-based input specification method that is based on a variation of attribute grammars and allows the user to specify input constraints in addition to input structure. The model can be instantiated automatically to generate structurally and semantically valid test inputs. The proposed method has the capability to specify test requirements and coverage criteria and use them to generate valid test suites that satisfy test coverage criteria requirements.ConclusionThe work presented in this paper provides a black-box test generation method for grammar-based software inputs that can automatically generate criteria-covering test suites. 
57|-||A comprehensive pattern-oriented approach to engineering security methodologies|ContextDeveloping secure software systems is an issue of ever-growing importance. Researchers have generally come to acknowledge that to develop such systems successfully, their security features must be incorporated in the context of a systematic approach: a security methodology. There are a number of such methodologies in the literature, but no single security methodology is adequate for every situation, requiring the construction of “fit-to-purpose” methodologies or the tailoring of existing methodologies to the project specifics at hand. While a large body of research exists addressing the same requirement for development methodologies – constituting the field of Method Engineering – there is nothing comparable for security methodologies as such; in fact, the topic has never been studied before in such a context.ObjectiveIn this paper we draw inspiration from a number of Method Engineering ideas and fill the latter gap by proposing a comprehensive approach to engineering security methodologies.MethodOur approach is embodied in three interconnected parts: a framework of interrelated security process patterns; a security-specific meta-model; and a meta-methodology to guide engineers in using the latter artefacts in a step-wise fashion. A UML-inspired notation is used for representing all pattern-based methodology models during design and construction. The approach is illustrated and evaluated by tailoring an existing, real-life security methodology to a distributed-system-specific project situation.ResultsThe paper proposes a novel pattern-oriented approach to modeling, constructing, tailoring and combining security methodologies, which is the very first and currently sole such approach in the literature. We illustrate and evaluate our approach in an academic setting, and perform a feature analysis to highlight benefits and deficiencies.ConclusionUsing our proposal, developers, architects and researchers can analyze and engineer security methodologies in a structured, systematic fashion, taking into account all security methodology aspects. 
57|-||VIVACE: A framework for the systematic evaluation of variability support in process-aware information systems| abstractContextThe increasing adoption of process-aware information systems (PAISs) such as workflow management systems, enterprise resource planning systems, or case management systems, together with the high variability in business processes (e.g., sales processes may vary depending on the respective products and countries), has resulted in large industrial process model repositories. To cope with this business process variability, the proper management of process variants along the entire process lifecycle becomes crucial.ObjectiveThe goal of this paper is to develop a fundamental understanding of business process variability. In particular, the paper will provide a framework for assessing and comparing process variability approaches and the support they provide for the different phases of the business process lifecycle (i.e., process analysis and design, configuration, enactment, diagnosis, and evolution).MethodWe conducted a systematic literature review (SLR) in order to discover how process variability is supported by existing approaches.ResultsThe SLR resulted in 63 primary studies which were deeply analyzed. Based on this analysis, we derived the VIVACE framework. VIVACE allows assessing the expressiveness of a process modeling language regarding the explicit specification of process variability. Furthermore, the support provided by a process-aware information system to properly deal with process model variants can be assessed with VIVACE as well.ConclusionsVIVACE provides an empirically-grounded framework for process engineers that enables them to evaluate existing process variability approaches as well as to select that variability approach meeting their requirements best. Finally, it helps process engineers in implementing PAISs supporting process variability along the entire process lifecycle. 
57|-||The impact of global dispersion on coordination, team performance and software quality â A systematic literature review|ContextGlobal software development (GSD) contains different context setting dimensions, which are essential for effective teamwork and success of projects. Although considerable research effort has been made in this area, as yet, no agreement has been reached about the impact of these dispersion dimensions on team coordination and project outcomes.ObjectiveThis paper summarizes empirical evidence on the impact of global dispersion dimensions on coordination, team performance and project outcomes.MethodWe performed a systematic literature review of 46 publications from 25 journals and 19 conference and workshop proceedings, which were published between 2001 and 2013. Thematic analysis was used to identify global dimensions and their measures. Vote counting was used to decide on the impact trends of dispersion dimensions on team performance and software quality.ResultsGlobal dispersion dimensions are consistently conceptualized, but quantified in many different ways. Different dispersion dimensions are associated with a distinct set of coordination challenges. Overall, geographical dispersion tends to have a negative impact on team performance and software quality. Temporal dispersion tends to have a negative impact on software quality, but its impact on team performance is inconsistent and can be explained by type of performance.ConclusionFor researchers, we reveal several opportunities for future research, such as coordination challenges in inter-organizational software projects, impact of processes and practices mismatches on project outcomes, evolution of coordination needs and mechanism over time and impact of dispersion dimensions on open source project outcomes. For practitioners, they should consider the tradeoff between cost and benefits while dispersing tasks, alignment impact of dispersion dimensions with individual and organizational objectives, coordination mechanisms as situational approaches and collocation of development activities of high quality demand components in GSD projects. 
57|-||The impact of inadequate and dysfunctional training on Agile transformation process: A Grounded Theory study|ContextTraining is an essential facilitator in moving from traditional to Agile software development.ObjectiveThis paper addresses the importance of adequate and functional training in Agile transformation process, the causes of inadequate and dysfunctional training, and the heuristic strategies that can be used in software companies for dealing with this phenomenon.MethodA Grounded Theory study was conducted with participation of 35 Agile experts from 13 different countries.ResultsThis research discovered that inadequate and dysfunctional training was one of the critical issues that affected Agile transformation process. This study shows that comprehensive and functional training is not often provided to support Agile transformation. This paper shows the primary causes of inadequate and dysfunctional training, its adverse consequences on the transformation process, and the heuristic and ad-hoc treatments as the strategies used by Agile teams to cope with this challenge.ConclusionComprehensive training is important in Agile transformation process. Inadequate and dysfunctional training causes several challenges and problems for software companies and development teams when moving to Agile. Several ad-hoc strategies identified by this study can be employed to help software teams and companies facing similar problems. 
57|-||A framework for comparing multiple cost estimation methods using an automated visualization toolkit|ContextThe importance of accurate predictions in Software Cost Estimation and the related challenging research problems, led to the introduction of a plethora of methodologies in literature. However, the wide variety of cost estimation methods, the techniques for improving them and the different measures of accuracy have caused new problems such as the inconsistent findings and the conclusion instability. Today, there is a confusion regarding the choice of the most appropriate method for a specific dataset and therefore a need for well-established statistical frameworks as well as for automated tools that will reinforce and lead a comprehensive experimentation and comparison process, based on the thorough study of the cost estimation errors.ObjectiveThe purpose of this paper is to present a framework for visualization and statistical comparison of the errors of several cost estimation methods. It is based on an automated tool which can facilitate strategies for an intelligent decision-making.MethodA systematic procedure comprised of a series of steps corresponding to research questions is proposed. For each of the steps, StatREC, a Graphical User Interface statistical toolkit is utilized. StatREC was designed and developed to take as input a simple data matrix of predictions by multiple models and to provide a variety of graphical tools and statistical hypothesis tests for aiding the users to answer the questions and choose the appropriate model themselves.ResultsThe study of prediction errors by the proposed framework provides insight of several aspects related to prediction performance of different models. The systematic examination of candidate models by a series of research questions supports the user to make the final decision.ConclusionStructured procedures based on automated tools like StatREC can efficiently be used for studying the error and comparing cost estimation models. 
57|-||CCIC: Clustering analysis classes to identify software components|ContextComponent identification during software design phase denotes a process of partitioning the functionalities of a system into distinct components. Several component identification methods have been proposed that cannot be customized to software architect’s preferences.ObjectivesIn this paper, we propose a clustering-based method by the name of CCIC (Clustering analysis Classes to Identify software Components) to identify logical components from analysis classes according to software architect’s preferences.MethodCCIC uses a customized HEA (Hierarchical Evolutionary Algorithm) to automatically classify analysis classes into appropriate logical components and avoid the problem of searching for the proper number of components. Furthermore, it allows software architects to determine the constraints in their deployment and implementation framework.ResultsA series of experiments were conducted for four real-world case studies according to various proposed weighting schemes.ConclusionAccording to experimental results, it is concluded that CCIC can identify more cohesive and independent components with respect to software architect’s preferences in comparison with the existing component identification methods such as FCA-based and CRUD-based methods. 
57|-||A comparative study of software tools for user story management|ContextUser stories have become widely accepted in agile software development. Consequently, a great number of software tools that provide, inter alia, support for practices based on user stories have emerged in recent years. These tools may have different features and focus in terms of support for agile requirements engineering (RE) concepts and practices.ObjectiveThe present study aims to provide a deep insight into the current capabilities and future trends of software support for agile RE practices based on user stories.MethodA comparative qualitative study of a set of agile software tools has been conducted according to the following criteria: coverage of the key functional requirements, support for basic agile RE concepts and practices, and user satisfaction with the tool. The criteria for tool selection were: diversity of software tools, high rating on the user-stories community Web site (http://www.userstories.com), and availability for review.ResultsThe results show a generally good coverage of key functional requirements related to management of user stories and epics, high-level release planning and low-level iteration planning. On the other hand, user-role modeling and persona support have not been addressed at all, and it has been found that requirements for acceptance testing support were completely covered by only one tool. More importantly, the study has revealed significant differences in the way different tools support agile RE concepts and practices (if at all). Finally, qualitative analysis of user reviews has demonstrated that practitioners prefer tools that are easy to set up, easy to learn, easy to use, and easy to customize, over more sophisticated but simultaneously more demanding tools.ConclusionAlthough the progress that has been made since the inception of these tools is quite clear, there is still room for improvements in terms of support for various agile RE practices within a specific agile process. 
57|-||Automated classification of software change messages by semi-supervised Latent Dirichlet Allocation|ContextTopic models such as probabilistic Latent Semantic Analysis (pLSA) and Latent Dirichlet Allocation (LDA) have demonstrated success in mining software repository tasks. Understanding software change messages described by the unstructured nature-language text is one of the fundamental challenges in mining these messages in repositories.ObjectiveWe seek to present a novel automatic change message classification method characterized by semi-supervised topic semantic analysis.MethodIn this work, we present a semi-supervised LDA based approach to automatically classify change messages. We use domain knowledge of software changes to make labeled samples which are added to build the semi-supervised LDA model. Next, we verify the cross-project analysis application of our method on three open-source projects. Our method has two advantages over existing software change classification methods: First of all, it mitigates the issue of how to set the appropriate number of latent topics. We do not have to choose the number of latent topics in our method, because it corresponds to the number of class labels. Second, this approach utilizes the information provided by the label samples in the training set.ResultsOur method automatically classified about 85% of the change messages in our experiment and our validation survey showed that 70.56% of the time our automatic classification results were in agreement with developer opinions.ConclusionOur approach automatically classifies most of the change messages which record the cause of the software change and the method is applicable to cross-project analysis of software change messages. 
57|-||Knowledge management initiatives in software testing: A mapping study|ContextSoftware testing is a knowledge intensive process, and, thus, Knowledge Management (KM) principles and techniques should be applied to manage software testing knowledge.ObjectiveThis study conducts a survey on existing research on KM initiatives in software testing, in order to identify the state of the art in the area as well as the future research. Aspects such as purposes, types of knowledge, technologies and research type are investigated.MethodThe mapping study was performed by searching seven electronic databases. We considered studies published until December 2013. The initial resulting set was comprised of 562 studies. From this set, a total of 13 studies were selected. For these 13, we performed snowballing and direct search to publications of researchers and research groups that accomplished these studies.ResultsFrom the mapping study, we identified 15 studies addressing KM initiatives in software testing that have been reviewed in order to extract relevant information on a set of research questions.ConclusionsAlthough only a few studies were found that addressed KM initiatives in software testing, the mapping shows an increasing interest in the topic in the recent years. Reuse of test cases is the perspective that has received more attention. From the KM point of view, most of the studies discuss aspects related to providing automated support for managing testing knowledge by means of a KM system. Moreover, as a main conclusion, the results show that KM is pointed out as an important strategy for increasing test effectiveness, as well as for improving the selection and application of suited techniques, methods and test cases. On the other hand, inadequacy of existing KM systems appears as the most cited problem related to applying KM in software testing. 
57|-||Shared service recommendations from requirement specifications: A hybrid syntactic and semantic toolkit|ContextSoftware Requirement Specifications (SRSs) are central to software lifecycles. An SRS defines the functionalities and constraints of a desired software system, hence it often serves as reference for further development. Software lifecycles concerned with the conversion of traditional systems into more service-oriented infrastructures can benefit from understanding potential shared capabilities through the analysis of SRSs.ObjectiveIn this paper, we propose an automated approach capable of recommending shared software services from multiple text-based SRSs created by different organizations. Our goal is to facilitate the identification of overlapping requirements in these specifications and subsequently recommend shared components, which promotes software reuse. The shared components can be implemented as services that are invoked across different systems.MethodOur approach leverages the syntactic similarity of the SRS text augmented with semantic information derived from the WordNet database. This work extends our earlier studies by introducing an algorithm that utilizes noun, verb, and predicate relations to enhance the discovery of equivalent requirements and the recommendation of reusable services. A prototype system is implemented to evaluate the approach and experimental results have shown effective recommendation of requirements and their realized shared services.ResultsOur automatic recommendation approach generates recommendations in few minutes compared to 9 h when services are manually inspected by developers. Our approach is also able to recommend services that are overlooked by the same developers, and to identify similarity between requirements even if these requirements are reworded.ConclusionWe show through experimentation that we can efficiently recommend services by leveraging both the syntactical structure and the semantic information of a requirements document and that our approach is more effective than the manual selection of services by experts. We also show that our approach is effective in detecting similar requirements for a single system and hence discovering opportunities for software reuse. 
57|-||Validating a model-driven software architecture evaluation and improvement method: A family of experiments|ContextSoftware architectures should be evaluated during the early stages of software development in order to verify whether the non-functional requirements (NFRs) of the product can be fulfilled. This activity is even more crucial in software product line (SPL) development, since it is also necessary to identify whether the NFRs of a particular product can be achieved by exercising the variation mechanisms provided by the product line architecture or whether additional transformations are required. These issues have motivated us to propose QuaDAI, a method for the derivation, evaluation and improvement of software architectures in model-driven SPL development.ObjectiveWe present in this paper the results of a family of four experiments carried out to empirically validate the evaluation and improvement strategy of QuaDAI.MethodThe family of experiments was carried out by 92 participants: Computer Science Master’s and undergraduate students from Spain and Italy. The goal was to compare the effectiveness, efficiency, perceived ease of use, perceived usefulness and intention to use with regard to participants using the evaluation and improvement strategy of QuaDAI as opposed to the Architecture Tradeoff Analysis Method (ATAM).ResultsThe main result was that the participants produced their best results when applying QuaDAI, signifying that the participants obtained architectures with better values for the NFRs faster, and that they found the method easier to use, more useful and more likely to be used. The results of the meta-analysis carried out to aggregate the results obtained in the individual experiments also confirmed these results.ConclusionsThe results support the hypothesis that QuaDAI would achieve better results than ATAM in the experiments and that QuaDAI can be considered as a promising approach with which to perform architectural evaluations that occur after the product architecture derivation in model-driven SPL development processes when carried out by novice software evaluators. 
57|-||State dependency probabilistic model for fault localization|ContextFault localization is an important and expensive activity in software debugging. Previous studies indicated that statistically-based fault-localization techniques are effective in prioritizing the possible faulty statements with relatively low computational complexity, but prior works on statistical analysis have not fully investigated the behavior state information of each program element.ObjectiveThe objective of this paper is to propose an effective fault-localization approach based on the analysis of state dependence information between program elements.MethodIn this paper, state dependency is proposed to describe the control flow dependence between statements with particular states. A state dependency probabilistic model uses path profiles to analyze the state dependency information. Then, a fault-localization approach is proposed to locate faults by differentiating the state dependencies in passed and failed test cases.ResultsWe evaluated the fault-localization effectiveness of our approach based on the experiments on Siemens programs and four UNIX programs. Furthermore, we compared our approach with current state-of-art fault-localization methods such as SOBER, Tarantula, and CP. The experimental results show that, our approach can locate more faults than the other methods in every range on Siemens programs, and the overall efficiency of our approach in the range of 10–30% of analyzed source code is higher than the other methods on UNIX programs.ConclusionOur studies show that our approach consistently outperforms the other evaluated techniques in terms of effectiveness in fault localization on Siemens programs. Moreover, our approach is highly effective in fault localization even when very few test cases are available. 
57|-||Automated end user-centred adaptation of web components through automated description logic-based reasoning|ContextThis paper addresses one of the major end-user development (EUD) challenges, namely, how to pack today’s EUD support tools with composable elements. This would give end users better access to more components which they can use to build a solution tailored to their own needs. The success of later end-user software engineering (EUSE) activities largely depends on how many components each tool has and how adaptable components are to multiple problem domains.ObjectiveA system for automatically adapting heterogeneous components to a common development environment would offer a sizeable saving of time and resources within the EUD support tool construction process. This paper presents an automated adaptation system for transforming EUD components to a standard format.MethodThis system is based on the use of description logic. Based on a generic UML2 data model, this description logic is able to check whether an end-user component can be transformed to this modelling language through subsumption or as an instance of the UML2 model. Besides it automatically finds a consistent, non-ambiguous and finite set of XSLT mappings to automatically prepare data in order to leverage the component as part of a tool that conforms to the target UML2 component model.ResultsThe proposed system has been successfully applied to components from four prominent EUD tools. These components were automatically converted to a standard format. In order to validate the proposed system, rich internet applications (RIA) used as an operational support system for operators at a large services company were developed using automatically adapted standard format components. These RIAs would be impossible to develop using each EUD tool separately.ConclusionThe positive results of applying our system for automatically adapting components from current tool catalogues are indicative of the system’s effectiveness. Use of this system could foster the growth of web EUD component catalogues, leveraging a vast ecosystem of user-centred SaaS to further current EUSE trends. 
57|-||Automated test generation technique for aspectual features in AspectJ|ContextAspect-oriented programming (AOP) has been promoted as a means for handling the modularization of software systems by raising the abstraction level and reducing the scattering and tangling of crosscutting concerns. Studies from literature have shown the usefulness and application of AOP across various fields of research and domains. Despite this, research shows that AOP is currently used in a cautious way due to its natural impact on testability and maintainability.ObjectiveTo realize the benefits of AOP and to increase its adoption, aspects developed using AOP should be subjected to automated testing. Automated testing, as one of the most pressing needs of the software industry to reduce both effort and costs in assuring correctness, is a delicate issue in testing aspect-oriented programs that still requires advancement and has a way to go before maturity.MethodPrevious attempts and studies in automated test generation process for aspect-oriented programs have been very limited. This paper proposes a rigorous automated test generation technique, called RAMBUTANS, with its tool support based on guided random testing for the AspectJ programs.ResultsThe paper reports the results of a thorough empirical study of 9 AspectJ benchmark programs, including non-trivial and larger software, by means of mutation analysis to compare RAMBUTANS and the four existing automated AOP testing approaches for testing aspects in terms of fault detection effectiveness and test effort efficiency. The results of the experiment and statistical tests supplemented by effect size measures presented evidence of the effectiveness and efficiency of the proposed technique at 99% confidence level (i.e. p < 0.01).ConclusionThe study showed that the resulting randomized tests were reasonably good for AOP testing, thus the proposed technique could be worth using as an effective and efficient AOP-specific automated test generation technique. 
57|-||Besouro: A framework for exploring compliance rules in automatic TDD behavior assessment|ContextThe improvements promoted by Test-Driven Design (TDD) have not been confirmed by quantitative assessment studies. To a great extent, the problem lies in the lack of a rigorous definition for TDD. An emerging approach has been to measure the conformance of TDD practices with the support of automated systems that embed an operational definition, which represent the specific TDD process assumed and the validation tests used to determine its presence and quantity. The empirical construction of TDD understanding and consensus building requires the ability of comparing different definitions, evaluating them with regard to practitioners’ perception, and exploring code information for improvement of automatic assessment.ObjectiveThis paper describes Besouro, a framework targeted at the development of systems for automatic TDD behavior assessment. The main rationale of Besouro’s design is the ability to compare distinct operational definitions, evaluate them with regard to users’ perception, and explore code information for further analysis and conformance assessment improvement.MethodWe developed an architecture with clear separation of concerns, which enables to vary: (a) the atomic events and respective metrics to be collected from developing and testing environments; (b) the organization of atomic events in streams of actions or processes; and (c) the classification and assessment components for each set of operational definitions adopted. The architecture also includes a mechanism for on-line user assessment awareness and feedback, and integrates event-related information with the respective code in a code version system.ResultsWe illustrate the usefulness of Besouro’s features for understanding the actions and processes underlying TDD through a prototype developed to support an experiment based on user feedback. We show how it was possible to compare variations of a same operational definition by exploring users’ feedback, and use source code to improve the automatic classification of TDD practices.ConclusionUnderstanding the actions and processes underlying successful TDD application is key for leveraging TDD benefits. In the absence of a rigorous definition for TDD, the proposed approach aims at building consensus from experimentation and empirical validation. 
57|-||Using simulation to aid decision making in managing the usability evaluation process|ContextThis paper is developed in the context of Usability Engineering. More specifically, it focuses on the use of modelling and simulation to help decision-making in the scope of usability evaluation.ObjectiveThe main goal of this paper is to present UESim: a System Dynamics simulation model to help decision-making in the make-up of the usability evaluation team during the process of usability evaluation.MethodTo develop this research we followed four main research phases: (a) study identification, (b) study development, (c) running and observation and finally, (d) reflexion. In relation with these phases the paper describes the literature revision, the model building and validation, the model simulation and its results and finally the reflexion on it.ResultsWe developed and validated a model to simulate the usability evaluation process. Through three different simulations we analysed the effects of different compositions of the evaluation team on the outcome of the evaluation. The simulation results show the utility of the model in the decision making of the usability evaluation process by changing the number and expertise of evaluators employed.ConclusionOne of the main advantages of using such a simulation model is that it allows developers to observe the evolution of the key indicators of the evaluation process over time. UESim represents a customisable tool to help decision-making in the management of the usability evaluation process, since it makes it possible to analyse how the key process indicators are affected by the main management options of the Usability Evaluation Process. 
57|-||Using a multi-method approach to understand Agile software product lines|ContextSoftware product lines (SPLs) and Agile are approaches that share similar objectives. The main difference is the way in which these objectives are met. Typically evidence on what activities of Agile and SPL can be combined and how they can be integrated stems from different research methods performed separately. The generalizability of this evidence is low, as the research topic is still relatively new and previous studies have been conducted using only one research method.ObjectiveThis study aims to increase understanding of Agile SPL and improve the generalizability of the identified evidence through the use of a multi-method approach.MethodOur multi-method research combines three complementary methods (Mapping Study, Case Study and Expert Opinion) to consolidate the evidence.ResultsThis combination results in 23 findings that provide evidence on how Agile and SPL could be combined.ConclusionAlthough multi-method research is time consuming and requires a high degree of effort to plan, design, and perform, it helps to increase the understanding on Agile SPL and leads to more generalizable evidence. The findings confirm a synergy between Agile and SPL and serve to improve the body of evidence in Agile SPL. When researchers and practitioners develop new Agile SPL approaches, it will be important to consider these synergies. 
57|-||A semi-automated approach to adapt activity diagrams for new use cases|ContextWeb engineering methodologies generally assign a crucial role to design models. Therefore, providing a model reuse approach is very interesting since it reduces development costs and improves quality. Current works on model reuse mainly focus on retrieval of the promising reusable assets, and much less is done regarding adaptation of the retrieved assets. This research proposes a semi-automatic approach for adaptation of UML activity diagrams to new use cases.ObjectiveUML use case diagrams and activity diagrams are traditionally used for the brief and the detailed specification of the functional requirements. Since many web applications have similar functionalities, and hence similar functional requirements, this research proposes an approach to take a use case diagram as input and semi-automatically create corresponding activity diagrams by adapting existing activity diagrams.MethodThe proposed approach includes five main components: (1) a model repository, (2) an ontology repository as a source of domain knowledge, (3) an algorithm for annotating activity diagrams, (4) a similarity metric for retrieval of similar use cases, and (5) an adaptation algorithm for creating activity diagram of a new use case from an existing activity diagram The proposed approach uses the semantic web data model as the underlying representation format.ResultsThe initial experiments show that the proposed approach is promising and it provides an average reuse percent of 76%. However, it has still some weaknesses like being much dependent on the quality of the model repository and having low tolerance in case of inconsistency in the model repository.ConclusionEnabling model reuse in the early stages of a model based development approach is very important in reducing development costs. This paper proposes a semi-automatic approach to reuse activity diagrams through their adaptation for new use cases. The approach is demonstrated to be promising although it has still some limitations. 
57|-||Generating optimized configurable business process models in scenarios subject to uncertainty|ContextThe quality of business process models (i.e., software artifacts that capture the relations between the organizational units of a business) is essential for enhancing the management of business processes. However, such modeling is typically carried out manually. This is already challenging and time consuming when (1) input uncertainty exists, (2) activities are related, and (3) resource allocation has to be considered. When including optimization requirements regarding flexibility and robustness it becomes even more complicated potentially resulting into non-optimized models, errors, and lack of flexibility.ObjectiveTo facilitate the human work and to improve the resulting models in scenarios subject to uncertainty, we propose a software-supported approach for automatically creating configurable business process models from declarative specifications considering all the aforementioned requirements.MethodFirst, the scenario is modeled through a declarative language which allows the analysts to specify its variability and uncertainty. Thereafter, a set of optimized enactment plans (each one representing a potential execution alternative) are generated from such a model considering the input uncertainty. Finally, to deal with this uncertainty during run-time, a flexible configurable business process model is created from these plans.ResultsTo validate the proposed approach, we conduct a case study based on a real business which is subject to uncertainty. Results indicate that our approach improves the actual performance of the business and that the generated models support most of the uncertainty inherent to the business.ConclusionsThe proposed approach automatically selects the best part of the variability of a declarative specification. Unlike existing approaches, our approach considers input uncertainty, the optimization of multiple objective functions, as well as the resource and the control-flow perspectives. However, our approach also presents a few limitations: (1) it is focused on the control-flow and the data perspective is only partially addressed and (2) model attributes need to be estimated. 
57|-||The contextual nature of innovation â An empirical investigation of three software intensive products|ContextNew products create significant opportunities for differentiation and competitive advantage. To increase the chances of new product success, a universal set of critical activities and determinants have been recommended. Some researchers believe, however, that these factors are not universal, but are contextual.ObjectiveThis paper reports innovation processes followed to develop three software intensive products for understanding how and why innovation practice is dependent on innovation context.MethodThis paper reports innovation processes and practices with an in-depth multi-case study of three software product innovations from Ericsson, IBM, and Rorotika. It describes the actual innovation processes followed in the three cases and discusses the observed innovation practice and relates it to state-of-the-art.ResultsThe cases point to a set of contextual factors that influence the choice of innovation activities and determinants for developing successful product innovations. The cases provide evidence that innovation practice cannot be standardized, but is contextual in nature.ConclusionThe rich description of the interaction between context and innovation practice enables future investigations into contextual elements that influence innovation practice, and calls for the creation of frameworks enabling activity and determinant selection for a given context – since one size does not fit all. 
57|-||Special section from the International Conference on Evaluation and Assessment in Software Engineering, 2013|
57|-||Naming the pain in requirements engineering: A design for a global family of surveys and first results from Germany|ContextFor many years, we have observed industry struggling in defining a high quality requirements engineering (RE) and researchers trying to understand industrial expectations and problems. Although we are investigating the discipline with a plethora of empirical studies, they still do not allow for empirical generalisations.ObjectiveTo lay an empirical and externally valid foundation about the state of the practice in RE, we aim at a series of open and reproducible surveys that allow us to steer future research in a problem-driven manner.MethodWe designed a globally distributed family of surveys in joint collaborations with different researchers and completed the first run in Germany. The instrument is based on a theory in the form of a set of hypotheses inferred from our experiences and available studies. We test each hypothesis in our theory and identify further candidates to extend the theory by correlation and Grounded Theory analysis.ResultsIn this article, we report on the design of the family of surveys, its underlying theory, and the full results obtained from Germany with participants from 58 companies. The results reveal, for example, a tendency to improve RE via internally defined qualitative methods rather than relying on normative approaches like CMMI. We also discovered various RE problems that are statistically significant in practice. For instance, we could corroborate communication flaws or moving targets as problems in practice. Our results are not yet fully representative but already give first insights into current practices and problems in RE, and they allow us to draw lessons learnt for future replications.ConclusionOur results obtained from this first run in Germany make us confident that the survey design and instrument are well-suited to be replicated and, thereby, to create a generalisable empirical basis of RE in practice. 
57|-||Are Forward Designed or Reverse-Engineered UML diagrams more helpful for code maintenance?: A family of experiments|ContextAlthough various success stories of model-based approaches are reported in literature, there is still a significant resistance to model-based development in many software organizations because the UML is perceived to be expensive and not necessarily cost-effective. It is also important to gather empirical evidence in which context and under which conditions the UML makes or does not make a practical difference.ObjectiveOur objective is to provide empirical evidence as to which UML diagrams are more helpful during software maintenance: Forward Designed (FD) UML diagrams or Reverse Engineered (RE) UML diagrams.MethodWe carried out a family of experiments which consisted of one experiment and two replications with a total of 169 Computer Science undergraduate students.ResultsThe individual data analysis and the meta-analysis conducted on the whole family, show a tendency in favor of FD diagrams and are significantly different as regards the effectiveness and efficiency of the subjects who participated and played the role of maintainers. The analysis of the qualitative data, collected using a post-experiment survey, reveals that the subjects did not consider RE diagrams helpful.ConclusionsOur findings show that there are some objective results (descriptive statistics or statistical tests) related to the maintenance effectiveness and efficiency in favor of the use of FD UML diagrams during software maintenance. Subjective opinions also lead us to recommend the use of UML diagrams (especially class diagrams) created during the design phase for software maintenance because they improve the understanding of the system in comparison with RE diagrams. Nevertheless, we can only assume that these results are valid in the context of Computer Science undergraduate students when working with small systems related to well-known domains, and other contexts should be explored in order to reaffirm the results in an industrial context by carrying out replications with professionals. 
57|-||Usage and usefulness of technical software documentation: An industrial case study|ContextSoftware documentation is an integral part of any software development process. However, software practitioners are often concerned about the value, degree of usage and usefulness of documentation during development and maintenance.ObjectiveMotivated by the needs of NovAtel Inc. (NovAtel), a world-leading company developing software systems in support of global navigation satellite systems, and based on the results of a former systematic mapping study, we aimed at better understanding of the usage and the usefulness of various technical documents during software development and maintenance.MethodWe utilized the results of a former systematic mapping study and performed an industrial case study at NovAtel. From the joint definition of the analysis goals, the research method incorporates qualitative and quantitative analysis of 55 documents (design, test and process related) and 1630 of their revisions. In addition, we conducted a survey on the usage and usefulness of documents. A total of 25 staff members from the industrial partner, all having a medium to high level of experience, participated in the survey.ResultsIn the context of the case study, a number of findings were derived. They include that (1) technical documentation was consulted least frequently for maintenance purpose and most frequently as an information source for development, (2) source code was considered most frequently as the preferred information source during software maintenance, (3) there is no significant difference between the usage of various documentation types during both development and maintenance, and (4) initial hypotheses stating that up-to-date information, accuracy and preciseness have the highest impact on usefulness of technical documentation.ConclusionsIt is concluded that the usage of documentation differs for various purposes and it depends on the type of the information needs as well as the tasks to be completed (e.g., development and maintenance). The results have been confirmed to be helpful for the company under study, and the firm is currently implementing some of the recommendations given. 
57|-||Introduction to special section on Search Based Software Engineering|
57|-||Learning from optimization: A case study with Apache Ant|ContextSoftware architecture degrades when changes violating the design-time architectural intents are imposed on the software throughout its life cycle. Such phenomenon is called architecture erosion. When changes are not controlled, erosion makes maintenance harder and negatively affects software evolution.ObjectiveTo study the effects of architecture erosion on a large software project and determine whether search-based module clustering might reduce the conceptual distance between the current architecture and the design-time one.MethodTo run an exploratory study with Apache Ant. First, we characterize Ant’s evolution in terms of size, change dispersion, cohesion, and coupling metrics, highlighting the potential introduction of architecture and code-level problems that might affect the cost of changing the system. Then, we reorganize the distribution of Ant’s classes using a heuristic search approach, intending to re-emerge its design-time architecture.ResultsIn characterizing the system, we observed that its original, simple design was lost due to maintenance and the addition of new features. In optimizing its architecture, we found that current models used to drive search-based software module clustering produce complex designs, which maximize the characteristics driving optimization while producing class distributions that would hardly be acceptable to developers maintaining Ant.ConclusionThe structural perspective promoted by the coupling and cohesion metrics precludes observing the adequate software module clustering from the perspective of software engineers when considering a large open source system. Our analysis adds evidence to the criticism of the dogma of driving design towards high cohesion and low coupling, at the same time observing the need for better models to drive design decisions. Apart from that, we see SBSE as a learning tool, allowing researchers to test Software Engineering models in extreme situations that would not be easily found in software projects. 
57|-||Search-based automated testing of continuous controllers: Framework, tool support, and case studies|ContextTesting and verification of automotive embedded software is a major challenge. Software production in automotive domain comprises three stages: Developing automotive functions as Simulink models, generating code from the models, and deploying the resulting code on hardware devices. Automotive software artifacts are subject to three rounds of testing corresponding to the three production stages: Model-in-the-Loop (MiL), Software-in-the-Loop (SiL) and Hardware-in-the-Loop (HiL) testing.ObjectiveWe study testing of continuous controllers at the Model-in-Loop (MiL) level where both the controller and the environment are represented by models and connected in a closed loop system. These controllers make up a large part of automotive functions, and monitor and control the operating conditions of physical devices.MethodWe identify a set of requirements characterizing the behavior of continuous controllers, and develop a search-based technique based on random search, adaptive random search, hill climbing and simulated annealing algorithms to automatically identify worst-case test scenarios which are utilized to generate test cases for these requirements.ResultsWe evaluated our approach by applying it to an industrial automotive controller (with 443 Simulink blocks) and to a publicly available controller (with 21 Simulink blocks). Our experience shows that automatically generated test cases lead to MiL level simulations indicating potential violations of the system requirements. Further, not only does our approach generate significantly better test cases faster than random test case generation, but it also achieves better results than test scenarios devised by domain experts. Finally, our generated test cases uncover discrepancies between environment models and the real world when they are applied at the Hardware-in-the-Loop (HiL) level.ConclusionWe propose an automated approach to MiL testing of continuous controllers using search. The approach is implemented in a tool and has been successfully applied to a real case study from the automotive domain. 
58|-|http://www.sciencedirect.com/science/journal/09505849/58|Using intentional fragments to bridge the gap between organizational and intentional levels|ContextBusiness process models provide a natural way to describe real-world processes to be supported by software-intensive systems. These models can be used to analyze processes in the system-as-is and describe potential improvements for the system-to-be. But, how well does a given business process model satisfy its business goals? How can different perspectives be integrated in order to describe an inter-organizational process?ObjectiveThe aim of the present paper is to link the local and the global perspectives of the inter-organizational business process defined in BPMN 2.0 (Business Process Model and Notation) to KAOS goal models (Keep All Objectives Satisfied). We maintain a separation of concerns between the intentional level captured by the goal model and the organizational level captured by the process model. The paper presents the concept of intentional fragment (a set of flow elements of the process with a common purpose) and assess its usefulness.MethodWe conducted empirical experiments where the proposed concepts – here the intentional fragments – are validated by users. Our method relies on an iterative improvement process led by users feedback.ResultsWe find that the concept of intentional fragment is useful for (1) analyzing the business process model (2) reasoning about the relations between the goal model and the business process model and (3) identifying new goals. In a previous work we focused on BPMN 2.0 collaboration models (local view). This paper extends the previous work by integrating the global view given by choreography models in the approach.ConclusionWe conclude that the notion of intentional fragment is a useful mean to relate business process models and goal models while dealing with their different nature (activity oriented vs goal oriented). Intentional fragments can also be used to analyze the process model and to infer new goals in an iterative manner. 
58|-||Using CMMI together with agile software development: A systematic review|BackgroundThe search for adherence to maturity levels by using lightweight processes that require low levels of effort is regarded as a challenge for software development organizations.ObjectiveThis study seeks to evaluate, synthesize, and present results on the use of the Capability Maturity Model Integration (CMMI) in combination with agile software development, and thereafter to give an overview of the topics researched, which includes a discussion of their benefits and limitations, the strength of the findings, and the implications for research and practice.MethodsThe method applied was a Systematic Literature Review on studies published up to (and including) 2011.ResultsThe search strategy identified 3193 results, of which 81 included studies on the use of CMMI together with agile methodologies. The benefits found were grouped into two main categories: those related to the organization in general and those related to the development process, and were organized into subcategories, according to the area to which they refer. The limitations were also grouped into these categories. Using the criteria defined, the strength of the evidence found was considered low. The implications of the results for research and practice are discussed.ConclusionAgile methodologies can be used by companies to reduce efforts in getting to levels 2 and 3 of CMMI, there even being reports of applying agile practices that led to achieving level 5. However, agile methodologies alone, according to the studies, were not sufficient to obtain a rating at a given level, it being necessary to resort to additional practices to do so. 
58|-||Empirical evaluation of a cloud computing information security governance framework|ContextCloud computing is a thriving paradigm that supports an efficient way to provide IT services by introducing on-demand services and flexible computing resources. However, significant adoption of cloud services is being hindered by security issues that are inherent to this new paradigm. In previous work, we have proposed ISGcloud, a security governance framework to tackle cloud security matters in a comprehensive manner whilst being aligned with an enterprise’s strategy.ObjectiveAlthough a significant body of literature has started to build up related to security aspects of cloud computing, the literature fails to report on evidence and real applications of security governance frameworks designed for cloud computing environments. This paper introduces a detailed application of ISGCloud into a real life case study of a Spanish public organisation, which utilises a cloud storage service in a critical security deployment.MethodThe empirical evaluation has followed a formal process, which includes the definition of research questions previously to the framework’s application. We describe ISGcloud process and attempt to answer these questions gathering results through direct observation and from interviews with related personnel.ResultsThe novelty of the paper is twofold: on the one hand, it presents one of the first applications, in the literature, of a cloud security governance framework to a real-life case study along with an empirical evaluation of the framework that proves its validity; on the other hand, it demonstrates the usefulness of the framework and its impact to the organisation.ConclusionAs discussed on the paper, the application of ISGCloud has resulted in the organisation in question achieving its security governance objectives, minimising the security risks of its storage service and increasing security awareness among its users. 
58|-||Using Bayesian regression and EM algorithm with missing handling for software effort prediction|ContextAlthough independent imputation techniques are comprehensively studied in software effort prediction, there are few studies on embedded methods in dealing with missing data in software effort prediction.ObjectiveWe propose BREM (Bayesian Regression and Expectation Maximization) algorithm for software effort prediction and two embedded strategies to handle missing data.MethodThe MDT (Missing Data Toleration) strategy ignores the missing data when using BREM for software effort prediction and the MDI (Missing Data Imputation) strategy uses observed data to impute missing data in an iterative manner while elaborating the predictive model.ResultsExperiments on the ISBSG and CSBSG datasets demonstrate that when there are no missing values in historical dataset, BREM outperforms LR (Linear Regression), BR (Bayesian Regression), SVR (Support Vector Regression) and M5′ regression tree in software effort prediction on the condition that the test set is not greater than 30% of the whole historical dataset for ISBSG dataset and 25% of the whole historical dataset for CSBSG dataset. When there are missing values in historical datasets, BREM with the MDT and MDI strategies significantly outperforms those independent imputation techniques, including MI, BMI, CMI, MINI and M5′. Moreover, the MDI strategy provides BREM with more accurate imputation for the missing values than those given by the independent missing imputation techniques on the condition that the level of missing data in training set is not larger than 10% for both ISBSG and CSBSG datasets.ConclusionThe experimental results suggest that BREM is promising in software effort prediction. When there are missing values, the MDI strategy is preferred to be embedded with BREM. 
58|-||Semantics for consistent activation in context-oriented systems|ContextContext-oriented programming languages provide dedicated programming abstractions to define behavioral adaptations and means to combine those adaptations dynamically according to sensed context changes. Some of these languages feature programming abstractions to explicitly define interaction dependencies among contexts. However, the semantics of context activation and the meaning of dependency relations have been described only informally, which in some cases has led to incorrect specifications, faulty implementations and inconsistent system behavior.ObjectiveWith the aim of avoiding faulty implementations and inconsistencies during system execution, this paper proposes both a formal and run-time model of contexts, context activation and context interaction.MethodAs a formal and computational basis, we introduce context Petri nets, a model based on Petri nets, which we found to match closely the structure of contexts in context-oriented systems. The operational semantics of Petri nets permits the modeling of run-time context activations. Existing Petri net analyses allow us to reason about system properties. As validation, we carried out small and medium-sized case studies.ResultsIn the cases explored, context Petri nets served effectively as underlying run-time model to ensure that declared context interaction constraints remain consistent during context manipulation. Moreover, context Petri nets enabled us to analyze certain properties regarding the activation state of particular contexts.ConclusionContext Petri nets thus proved to be appropriate to encode and manage the semantics of context activation, both formally and computationally, so as to preserve the consistency of context-oriented systems. 
58|-||Automatic transformation of iterative loops into recursive methods|ContextIn software engineering, taking a good election between recursion and iteration is essential because their efficiency and maintenance are different. In fact, developers often need to transform iteration into recursion (e.g., in debugging, to decompose the call graph into iterations); thus, it is quite surprising that there does not exist a public transformation from loops to recursion that can be used in industrial projects (i.e., it is automatic, it handles all kinds of loops, it considers exceptions, etc.).ObjectiveThis article describes an industrial algorithm implemented as a Java library able to automatically transform iterative loops into equivalent recursive methods. The transformation is described for the programming language Java, but it is general enough as to be adapted to many other languages that allow iteration and recursion.MethodWe describe the changes needed to transform loops of types while/do/for/foreach into recursion. We provide a transformation schema for each kind of loop.ResultsOur algorithm is the first public transformation that can be used in industrial projects and faces the whole Java language (i.e., it is fully automatic, it handles all kinds of loops, it considers exceptions, it treats the control statements break and continue, it handles loop labels, it is able to transform any number of nested loops, etc.). This is particularly interesting because some of these features are missing in all previous work, probably, due to the complexity that their mixture introduce in the transformation.ConclusionDevelopers should use a methodology when transforming code, specifically when transforming loops into recursion. This article provides guidelines and algorithms that allow them to face different problems such as exception handling. The implementation has been made publicly available as open source. 
58|-||Automated events identification in use cases|ContextUse cases are a popular method of expressing functional requirements. One contains a main scenario and a set of extensions, each consisting of an event and an alternative sequence of activities. Events omitted in requirements specification can lead to rework. Unfortunately, as it follows from the previous research, manual identification of events is rather ineffective (less than 1/3 of events are identified) and it is slow.ObjectiveThe goal of this paper is to propose an automatic method of identification of events in use cases and evaluate its quality.MethodEach step of a main scenario is analyzed by a sequence of NLP tools to identify its performer, activity type and information object. It has been observed that performer, activity type and some attributes of information objects determine types of events that can occur when that activity is performed. That empirical knowledge is represented as a set of axioms and two inference rules have been proposed which allow to identify types of possible events. For each event type an NLG pattern is proposed which allows to generate description of the event type in natural language. The proposed method was compared with two manual approaches to identification of events: ad hoc and HAZOP-based. Also a kind of Turing test was performed to evaluate linguistic quality of generated descriptions.ResultsAccuracy of the proposed method is about 80% (for manual approaches it is less than 1/3) and its speed is about 11 steps/minute (ad hoc approach is 4 times slower, and HAZOP-based approach is 20 times slower). Understandability of the generated event descriptions was not worse than understandability of the descriptions written by humans.ConclusionsThe proposed method could be used to enhance contemporary tools for managing use cases. 
58|-||Software test-code engineering: A systematic mapping|ContextAs a result of automated software testing, large amounts of software test code (script) are usually developed by software teams. Automated test scripts provide many benefits, such as repeatable, predictable, and efficient test executions. However, just like any software development activity, development of test scripts is tedious and error prone. We refer, in this study, to all activities that should be conducted during the entire lifecycle of test-code as Software Test-Code Engineering (STCE).ObjectiveAs the STCE research area has matured and the number of related studies has increased, it is important to systematically categorize the current state-of-the-art and to provide an overview of the trends in this field. Such summarized and categorized results provide many benefits to the broader community. For example, they are valuable resources for new researchers (e.g., PhD students) aiming to conduct additional secondary studies.MethodIn this work, we systematically classify the body of knowledge related to STCE through a systematic mapping (SM) study. As part of this study, we pose a set of research questions, define selection and exclusion criteria, and systematically develop and refine a systematic map.ResultsOur study pool includes a set of 60 studies published in the area of STCE between 1999 and 2012. Our mapping data is available through an online publicly-accessible repository. We derive the trends for various aspects of STCE. Among our results are the following: (1) There is an acceptable mix of papers with respect to different contribution facets in the field of STCE and the top two leading facets are tool (68%) and method (65%). The studies that presented new processes, however, had a low rate (3%), which denotes the need for more process-related studies in this area. (2) Results of investigation about research facet of studies and comparing our result to other SM studies shows that, similar to other fields in software engineering, STCE is moving towards more rigorous validation approaches. (3) A good mixture of STCE activities has been presented in the primary studies. Among them, the two leading activities are quality assessment and co-maintenance of test-code with production code. The highest growth rate for co-maintenance activities in recent years shows the importance and challenges involved in this activity. (4) There are two main categories of quality assessment activity: detection of test smells and oracle assertion adequacy. (5) JUnit is the leading test framework which has been used in about 50% of the studies. (6) There is a good mixture of SUT types used in the studies: academic experimental systems (or simple code examples), real open-source and commercial systems. (7) Among 41 tools that are proposed for STCE, less than half of the tools (45%) were available for download. It is good to have this percentile of tools to be available, although not perfect, since the availability of tools can lead to higher impact on research community and industry.ConclusionWe discuss the emerging trends in STCE, and discuss the implications for researchers and practitioners in this area. The results of our systematic mapping can help researchers to obtain an overview of existing STCE approaches and spot areas in the field that require more attention from the research community. 
58|-||A systematic review on the relationship between user involvement and system success|ContextFor more than four decades it has been intuitively accepted that user involvement (UI) during system development lifecycle leads to system success. However when the researchers have evaluated the user involvement and system success (UI-SS) relationship empirically, the results were not always positive.ObjectiveOur objective was to explore the UI-SS relationship by synthesizing the results of all the studies that have empirically investigated this complex phenomenon.MethodWe performed a Systematic Literature Review (SLR) following the steps provided in the guidelines of Evidence Based Software Engineering. From the resulting studies we extracted data to answer our 9 research questions related to the UI-SS relationship, identification of users, perspectives of UI, benefits, problems and challenges of UI, degree and level of UI, relevance of stages of software development lifecycle (SDLC) and the research method employed on the UI-SS relationship.ResultsOur systematic review resulted in selecting 87 empirical studies published during the period 1980–2012. Among 87 studies reviewed, 52 reported that UI positively contributes to system success, 12 suggested a negative contribution and 23 were uncertain. The UI-SS relationship is neither direct nor binary, and there are various confounding factors that play their role. The identification of users, their degree/level of involvement, stage of SDLC for UI, and choice of research method have been claimed to have impact on the UI-SS relationship. However, there is not sufficient empirical evidence available to support these claims.ConclusionOur results have revealed that UI does contribute positively to system success. But it is a double edged sword and if not managed carefully it may cause more problems than benefits. Based on the analysis of 87 studies, we were able to identify factors for effective management of UI alluding to the causes for inconsistency in the results of published literature. 
58|-||Current state of research on cross-site scripting (XSS) â A systematic literature review|ContextCross-site scripting (XSS) is a security vulnerability that affects web applications. It occurs due to improper or lack of sanitization of user inputs. The security vulnerability caused many problems for users and server applications.ObjectiveTo conduct a systematic literature review on the studies done on XSS vulnerabilities and attacks.MethodWe followed the standard guidelines for systematic literature review as documented by Barbara Kitchenham and reviewed a total of 115 studies related to cross-site scripting from various journals and conference proceedings.ResultsResearch on XSS is still very active with publications across many conference proceedings and journals. Attack prevention and vulnerability detection are the areas focused on by most of the studies. Dynamic analysis techniques form the majority among the solutions proposed by the various studies. The type of XSS addressed the most is reflected XSS.ConclusionXSS still remains a big problem for web applications, despite the bulk of solutions provided so far. There is no single solution that can effectively mitigate XSS attacks. More research is needed in the area of vulnerability removal from the source code of the applications before deployment. 
58|-||A systematic literature review of studies on business process modeling quality|ContextBusiness process modeling is an essential part of understanding and redesigning the activities that a typical enterprise uses to achieve its business goals. The quality of a business process model has a significant impact on the development of any enterprise and IT support for that process.ObjectiveSince the insights on what constitutes modeling quality are constantly evolving, it is unclear whether research on business process modeling quality already covers all major aspects of modeling quality. Therefore, the objective of this research is to determine the state of the art on business process modeling quality: What aspects of process modeling quality have been addressed until now and which gaps remain to be covered?MethodWe performed a systematic literature review of peer reviewed articles as published between 2000 and August 2013 on business process modeling quality. To analyze the contributions of the papers we use the Formal Concept Analysis technique.ResultsWe found 72 studies addressing quality aspects of business process models. These studies were classified into different dimensions: addressed model quality type, research goal, research method, and type of research result. Our findings suggest that there is no generally accepted framework of model quality types. Most research focuses on empirical and pragmatic quality aspects, specifically with respect to improving the understandability or readability of models. Among the various research methods, experimentation is the most popular one. The results from published research most often take the form of intangible knowledge.ConclusionWe believe there is a lack of an encompassing and generally accepted definition of business process modeling quality. This evidences the need for the development of a broader quality framework capable of dealing with the different aspects of business process modeling quality. Different dimensions of business process quality and of the process of modeling still require further research. 
58|-||Analogy-based software development effort estimation: A systematic mapping and review|ContextAnalogy-based Software development Effort Estimation (ASEE) techniques have gained considerable attention from the software engineering community. However, existing systematic map and review studies on software development effort prediction have not investigated in depth several issues of ASEE techniques, to the exception of comparisons with other types of estimation techniques.ObjectiveThe objective of this research is twofold: (1) to classify ASEE studies which primary goal is to propose new or modified ASEE techniques according to five criteria: research approach, contribution type, techniques used in combination with ASEE methods, and ASEE steps, as well as identifying publication channels and trends and (2) to analyze these studies from five perspectives: estimation accuracy, accuracy comparison, estimation context, impact of the techniques used in combination with ASEE methods, and ASEE tools.MethodWe performed a systematic mapping of studies for which the primary goal is to develop or to improve ASEE techniques published in the period 1990–2012, and reviewed them based on an automated search of four electronic databases.ResultsIn total, we identified 65 studies published between 1990 and 2012, and classified them based on our predefined classification criteria. The mapping study revealed that most researchers focus on addressing problems related to the first step of an ASEE process, that is, feature and case subset selection. The results of our detailed analysis show that ASEE methods outperform the eight techniques with which they were compared, and tend to yield acceptable results especially when combining ASEE techniques with Fuzzy Logic (FL) or Genetic Algorithms (GA).ConclusionBased on the findings of this study, the use of other techniques such FL and GA in combination with an ASEE method is promising to generate more accurate estimates. However, the use of ASEE techniques by practitioners is still limited: developing more ASEE tools may facilitate the application of these techniques and then lead to increasing the use of ASEE techniques in industry. 
58|-||Identifying refactoring opportunities in object-oriented code: A systematic literature review|ContextIdentifying refactoring opportunities in object-oriented code is an important stage that precedes the actual refactoring process. Several techniques have been proposed in the literature to identify opportunities for various refactoring activities.ObjectiveThis paper provides a systematic literature review of existing studies identifying opportunities for code refactoring activities.MethodWe performed an automatic search of the relevant digital libraries for potentially relevant studies published through the end of 2013, performed pilot and author-based searches, and selected 47 primary studies (PSs) based on inclusion and exclusion criteria. The PSs were analyzed based on a number of criteria, including the refactoring activities, the approaches to refactoring opportunity identification, the empirical evaluation approaches, and the data sets used.ResultsThe results indicate that research in the area of identifying refactoring opportunities is highly active. Most of the studies have been performed by academic researchers using nonindustrial data sets. Extract Class and Move Method were found to be the most frequently considered refactoring activities. The results show that researchers use six primary existing approaches to identify refactoring opportunities and six approaches to empirically evaluate the identification techniques. Most of the systems used in the evaluation process were open-source, which helps to make the studies repeatable. However, a relatively high percentage of the data sets used in the empirical evaluations were small, which limits the generality of the results.ConclusionsIt would be beneficial to perform further studies that consider more refactoring activities, involve researchers from industry, and use large-scale and industrial-based systems. 
58|-||On the probability distribution of faults in complex software systems|ContextThere are several empirical principles related to the distribution of faults in a software system (e.g. the Pareto principle) widely applied in practice and thoroughly studied in the software engineering research providing evidence in their favor. However, the knowledge of the underlying probability distribution of faults, that would enable a systematic approach and refinement of these principles, is still quite limited.ObjectiveIn this paper we study the probability distribution of faults detected during verification in four consecutive releases of a large-scale complex software system for the telecommunication exchanges. This is the first such study analyzing closed software system, replicating two previous studies for open source software.MethodWe take into consideration the Weibull, lognormal, double Pareto, Pareto, and Yule–Simon probability distributions, and investigate how well these distributions fit our empirical fault data using the non-linear regression.ResultsThe results indicate that the double Pareto distribution is the most likely choice for the underlying probability distribution. This is not consistent with the previous studies on open source software.ConclusionThe study shows that understanding the probability distribution of faults in complex software systems is more complicated than previously thought. Comparison with previous studies shows that the fault distribution strongly depends on the environment, and only further replications would make it possible to build up a general theory for a given context. 
58|-||Approach for estimating similarity between procedures in differently compiled binaries|ContextDetection of an unauthorized use of a software library is a clone detection problem that in case of commercial products has additional complexity due to the fact that only binary code is available.ObjectiveThe goal of this paper is to propose an approach for estimating the level of similarity between the procedures originating from different binary codes. The assumption is that the clones in the binary codes come from the use of a common software library that may be compiled with different toolsets.MethodThe approach uses a set of software metrics adapted from the high level languages and it also extends the set with new metrics that take into account syntactical changes that are introduced by the usage of different toolsets and optimizations. Moreover, the approach compares metric values and introduces transformers and formulas that can use training data for production of measure of similarities between the two procedures in binary codes. The approach has been evaluated on programs from STAMP benchmark and BusyBox tool, compiled with different toolsets in different modes.ResultsThe experiments with programs from STAMP benchmark show that detecting the same procedures recall can be up to 1.44 times higher using new metrics. Knowledge about the used compiling toolset can bring up to 2.28 times improvement in recall. The experiment with BusyBox tool shows 43% recall for 43% precision.ConclusionThe most useful newly proposed metrics are those that consider the frequency of arithmetic instructions, the number and frequency of occurrences for instructions, and the number of occurrences for target addresses in calls. The best way to combine the results of comparing metrics is to use a geometric mean or when previous knowledge is available, to use an arithmetic mean with appropriate transformer. 
58|-||Investigating software testing and maintenance reports: Case study|ContextAlthough many papers have been published on software development and defect prediction techniques, problem reports in real projects quite often differ from those described in the literature. Hence, there is still a need for deeper exploration of case studies from industry.ObjectiveThe aim of this study is to present the impact of fine-grained problem reports on improving evaluation of testing and maintenance processes. It is targeted at projects involving several releases and complex schemes of problem handling. This is based on our experience gained while monitoring several commercial projects.MethodExtracting certain features from detailed problem reports, we derive various measures and present analysis models which characterize and visualize the effectiveness of testing and problem resolution processes. The considered reports describe types of problems (e.g. defects), their locations in project versions and software modules, ways of their resolution, etc. The performed analysis is related to eleven projects developed in the same company. This study is an exploratory research with some explanatory features. Moreover, having identified some drawbacks, we present extensions of problem reports and their analysis which have been verified in another industrial case study project.ResultsFine-grained (accurate) problem handling reports provide a wider scope of possible measures to assess the relevant development processes. This is helpful in controlling single projects (local perspective) as well as in managing these processes in the whole company (global perspective).ConclusionDetailed problem handling reports extend the space and quality of statistical analysis, they provide significant enhancement in evaluation and refinement of software development processes as well as in reliability prediction. 
58|-||âOldâ theories, âNewâ technologies: Understanding knowledge sharing and learning in Brazilian software development companies|ContextNew technologies such as social networks, wikis, blogs and other social software enable collaborative work and are important facilitators of the learning process. They provide a simple mechanism for people to communicate and collaborate and thus support the creation of knowledge. In software-development companies they are used to creating an environment in which communication and collaboration between workers take place more effectively.ObjectiveThis paper identifies the main tools and technologies used by software-development companies in Brazil to manage knowledge and attempts to determine how these tools and technologies relate to important knowledge-sharing and learning theories and how they support the concepts described by these theories.MethodA survey was conducted in a group of Brazilian software development companies with high levels of process software maturity to see how they implement the Brazilian Software Processes Improvement model (MPS.Br) and use new tools and technologies. The survey used a qualitative analysis to identify which tools are used most and how frequently employees use them. The results of the analysis were compared with data from the literature on three knowledge-sharing and learning theories to understand how the use of these tools relates to the concepts proposed in these theories.ResultsThe results show that some of the tools used by the companies do not apply the concepts described in the theories as they do not help promote organizational learning. Furthermore, although the companies have adopted the tools, these are not often used, mainly because they are felt not to organize information efficiently.ConclusionThe use of certain tools can help promote several concepts described in the theories considered. Moreover, the use of these tools can help reduce the impact of, some common organizational problems. However, companies need to improve existing organizational policies that encourage employees to use these tools more regularly. 
58|-||A Bayesian network model for likelihood estimations of acquirement of critical software vulnerabilities and exploits|ContextSoftware vulnerabilities in general, and software vulnerabilities with publicly available exploits in particular, are important to manage for both developers and users. This is however a difficult matter to address as time is limited and vulnerabilities are frequent.ObjectiveThis paper presents a Bayesian network based model that can be used by enterprise decision makers to estimate the likelihood that a professional penetration tester is able to obtain knowledge of critical vulnerabilities and exploits for these vulnerabilities for software under different circumstances.MethodData on the activities in the model are gathered from previous empirical studies, vulnerability databases and a survey with 58 individuals who all have been credited for the discovery of critical software vulnerabilities.ResultsThe proposed model describes 13 states related by 17 activities, and a total of 33 different datasets.ConclusionEstimates by the model can be used to support decisions regarding what software to acquire, or what measures to invest in during software development projects. 
58|-||Product derivation in practice|ContextThe process of constructing a product from a product line of software assets is known product derivation. An effective product derivation process is important in order to ensure that the efforts required to develop these shared assets is lower than the benefits achieved through their use. Despite its importance, relatively little work has been dedicated to the product derivation process and the strategies applied in practice. Additionally, there is a lack of empirical reports describing product derivation in industrial settings, and, in general, where these reports are available, they have been conducted as informal studies.ObjectiveOur aim is to investigate how product derivation is performed in practice.MethodWe apply a multi-case study design to two different industrial software product line projects with the goal of investigating how they derive their products in practice. The findings from our studies were individually analyzed using the Constant Comparison technique. In order to identify patterns across these studies, the findings were compared using a Cross-case analysis approach.ResultsThe research approach allowed us to examine the case study outcomes from different perspectives, capturing similarities and differences. From the cases, we identified context specific strategies for product derivation which are easier for practitioners to contextualise and implement.ConclusionsThe case studies provide method-in-action insights into concepts explored in the literature, such as: iterative and incremental product derivation, instantiation and integration of platform components and derivation of product databases. Practitioners can use this work as a basis for defining, adapting or evaluating their own product derivation approaches. While researchers can use this work as a starting point for new industrial reports, presenting their experiences with product derivation. 
58|-||A framework to identify primitives that represent usability within Model-Driven Development methods|ContextNowadays, there are sound methods and tools which implement the Model-Driven Development approach (MDD) satisfactorily. However, MDD approaches focus on representing and generating code that represents functionality, behaviour and persistence, putting the interaction, and more specifically the usability, in a second place. If we aim to include usability features in a system developed with a MDD tool, we need to extend manually the generated code.ObjectiveThis paper tackles how to include functional usability features (usability recommendations strongly related to system functionality) in MDD through conceptual primitives.MethodThe approach consists of studying usability guidelines to identify usability properties that can be represented in a conceptual model. Next, these new primitives are the input for a model compiler that generates the code according to the characteristics expressed in them. An empirical study with 66 subjects was conducted to study the effect of including functional usability features regarding end users’ satisfaction and time to complete tasks. Moreover, we have compared the workload of two MDD analysts including usability features by hand in the generated code versus including them through conceptual primitives according to our approach.ResultsResults of the empirical study shows that after including usability features, end users’ satisfaction improves while spent time does not change significantly. This justifies the use of usability features in the software development process. Results of the comparison show that the workload required to adapt the MDD method to support usability features through conceptual primitives is heavy. However, once MDD supports these features, MDD analysts working with primitives are more efficient than MDD analysts implementing these features manually.ConclusionThis approach brings us a step closer to conceptual models where models represent not only functionality, behaviour or persistence, but also usability features. 
58|-||Similarity testing for access control|ContextAccess control is among the most important security mechanisms, and XACML is the de facto standard for specifying, storing and deploying access control policies. Since it is critical that enforced policies are correct, policy testing must be performed in an effective way to identify potential security flaws and bugs. In practice, exhaustive testing is impossible due to budget constraints. Therefore the tests need to be prioritized so that resources are focused on their most relevant subset.ObjectiveThis paper tackles the issue of access control test prioritization. It proposes a new approach for access control test prioritization that relies on similarity.MethodThe approach has been applied to several policies and the results have been compared to random prioritization (as a baseline). To assess the different prioritization criteria, we use mutation analysis and compute the mutation scores reached by each criterion. This helps assessing the rate of fault detection.ResultsThe empirical results indicate that our proposed approach is effective and its rate of fault detection is higher than that of random prioritization.ConclusionWe conclude that prioritization of access control test cases can be usefully based on similarity criteria. 
58|-||Categorization of risk factors for distributed agile projects|ContextOrganizations combine agile approach and Distributed Software Development (DSD) in order to develop better quality software solutions in lesser time and cost. It helps to reap the benefits of both agile and distributed development but pose significant challenges and risks. Relatively scanty evidence of research on the risks prevailing in distributed agile development (DAD) has motivated this study.ObjectiveThis paper aims at creating a comprehensive set of risk factors that affect the performance of distributed agile development projects and identifies the risk management methods which are frequently used in practice for controlling those risks.MethodThe study is an exploration of practitioners’ experience using constant comparison method for analyzing in-depth interviews of thirteen practitioners and work documents of twenty-eight projects from thirteen different information technology (IT) organizations. The field experience was supported by extensive research literature on risk management in traditional, agile and distributed development.ResultsAnalysis of qualitative data from interviews and project work documents resulted into categorization of forty-five DAD risk factors grouped under five core risk categories. The risk categories were mapped to Leavitt’s model of organizational change for facilitating the implementation of results in real world. The risk factors could be attributed to the conflicting properties of DSD and agile development. Besides that, some new risk factors have been experienced by practitioners and need further exploration as their understanding will help the practitioners to act on time.ConclusionOrganizations are adopting DAD for developing solutions that caters to the changing business needs, while utilizing the global talent. Conflicting properties of DSD and agile approach pose several risks for DAD. This study gives a comprehensive categorization of the risks faced by the practitioners in managing DAD projects and presents frequently used methods to reduce their impact. The work fills the yawning research void in this field. 
58|-||Software defect prediction using ensemble learning on selected features|ContextSeveral issues hinder software defect data including redundancy, correlation, feature irrelevance and missing samples. It is also hard to ensure balanced distribution between data pertaining to defective and non-defective software. In most experimental cases, data related to the latter software class is dominantly present in the dataset.ObjectiveThe objectives of this paper are to demonstrate the positive effects of combining feature selection and ensemble learning on the performance of defect classification. Along with efficient feature selection, a new two-variant (with and without feature selection) ensemble learning algorithm is proposed to provide robustness to both data imbalance and feature redundancy.MethodWe carefully combine selected ensemble learning models with efficient feature selection to address these issues and mitigate their effects on the defect classification performance.ResultsForward selection showed that only few features contribute to high area under the receiver-operating curve (AUC). On the tested datasets, greedy forward selection (GFS) method outperformed other feature selection techniques such as Pearson’s correlation. This suggests that features are highly unstable. However, ensemble learners like random forests and the proposed algorithm, average probability ensemble (APE), are not as affected by poor features as in the case of weighted support vector machines (W-SVMs). Moreover, the APE model combined with greedy forward selection (enhanced APE) achieved AUC values of approximately 1.0 for the NASA datasets: PC2, PC4, and MC1.ConclusionThis paper shows that features of a software dataset must be carefully selected for accurate classification of defective components. Furthermore, tackling the software data issues, mentioned above, with the proposed combined learning model resulted in remarkable classification performance paving the way for successful quality control. 
58|-||Infeasible path generalization in dynamic symbolic execution|ContextAutomatic code-based test input generation aims at generating a test suite ensuring good code coverage. Dynamic Symbolic Execution (DSE) recently emerged as a strong code-based testing technique to increase coverage by solving path conditions with a combination of symbolic constraint solving and concrete executions.ObjectiveWhen selecting paths in DSE for generating test inputs, some paths are actually detected as being infeasible, meaning that no input can be found to exercize them. But, showing path infeasibility instead of generating test inputs is costly and most effort could be saved in DSE by reusing path infeasibility information.MethodIn this paper, we propose a method that takes opportunity of the detection of a single infeasible path to generalize to a possibly infinite family of infeasible paths. The method first extracts an explanation of path condition, that is, the reason of the path infeasibility. Then, it determines conditions, using data dependency information, that paths must respect to exhibit the same infeasibility. Finally, it constructs an automaton matching the generalized infeasible paths.ResultsWe implemented our method in a prototype tool called IPEG (Infeasible Path Explanation and Generalization), for DSE of C programs. First experimental results obtained with IPEG show that our approach can save considerable effort in DSE, when generating test inputs for increasing code coverage.ConclusionInfeasible path generalization allows test generation to know of numerous infeasible paths ahead of time, and consequently to save the time needed to show their infeasibility. 
58|-||Search based algorithms for test sequence generation in functional testing|ContextThe generation of dynamic test sequences from a formal specification, complementing traditional testing methods in order to find errors in the source code.ObjectiveIn this paper we extend one specific combinatorial test approach, the Classification Tree Method (CTM), with transition information to generate test sequences. Although we use CTM, this extension is also possible for any combinatorial testing method.MethodThe generation of minimal test sequences that fulfill the demanded coverage criteria is an NP-hard problem. Therefore, search-based approaches are required to find such (near) optimal test sequences.ResultsThe experimental analysis compares the search-based technique with a greedy algorithm on a set of 12 hierarchical concurrent models of programs extracted from the literature. Our proposed search-based approaches (GTSG and ACOts) are able to generate test sequences by finding the shortest valid path to achieve full class (state) and transition coverage.ConclusionThe extended classification tree is useful for generating of test sequences. Moreover, the experimental analysis reveals that our search-based approaches are better than the greedy deterministic approach, especially in the most complex instances. All presented algorithms are actually integrated into a professional tool for functional testing. 
59|-|http://www.sciencedirect.com/science/journal/09505849/59|An architecture for automatically developing secure OLAP applications from models|ContextDecision makers query enterprise information stored in Data Warehouses (DW) by using tools (such as On-Line Analytical Processing (OLAP) tools) which use specific views or cubes from the corporate DW or Data Marts, based on the multidimensional modeling. Since the information managed is critical, security constraints have to be correctly established in order to avoid unauthorized accesses.ObjectiveIn previous work we have defined a Model-Driven based approach for developing a secure DWs repository by following a relational approach. Nevertheless, is also important to define security constraints in the metadata layer that connects the DWs repository with the OLAP tools, that is, over the same multidimensional structures that final users manage. This paper defines a proposal to develop secure OLAP applications and incorporates it into our previous approach.MethodOur proposal is composed of models and transformations. Our models have been defined using the extension capabilities from UML (conceptual model) and extending the OLAP package of CWM with security (logical model). Transformations have been defined by using a graphical notation and implemented into QVT and MOFScript. Finally, this proposal has been evaluated through case studies.ResultsA complete MDA architecture for developing secure OLAP applications. The main contributions of this paper are: improvement of a UML profile for conceptual modeling; definition of a logical metamodel for OLAP applications; and definition and implementation of transformations from conceptual to logical models, and from logical models to the secure implementation into a specific OLAP tool (SSAS).ConclusionOur proposal allows us to develop secure OLAP applications, providing a complete MDA architecture composed of several security models and automatic transformations towards the final secure implementation. Security aspects are early identified and fitted into a most robust solution that provides us a better information assurance and a saving of time in maintenance. 
59|-||New bounds for mixed covering arrays in t-way testing with uniform strength|ContextCombinatorial testing (CT) can increase the effectiveness of software testing by ensuring that all t  -way input combinations are covered in a test suite. When software components have different input cardinalities, CT uses a mixed covering array (MCA) to represent the test suite. This study proposes a new methodology for constructing MCAs of t∈{2-6}t∈{2-6} by using Mixed-Tabu Search (MiTS) as the construction strategy.ObjectiveThe objective of this study is to significantly improve the best bounds of MCAs of t∈{2-6}t∈{2-6} with uniform strength.MethodThe proposed solution incorporates a new procedure for efficient parameter tuning where statistical testing is used to identify the setting values that significantly affect the performance of MiTS. For validation purposes, we used a robust benchmark that comprised a set of 35 instances of real cases and a set of 95 academic instances, which represented the best bounds reported previously.ResultThe experimental results showed that our MiTS-based methodology improved 93 bounds and matched 36 of them. The Wilcoxon signed-rank test demonstrated that our MiTS-based methodology significantly enhanced the best bounds of MCAs compared with those reported previously with 95% confidence.ConclusionMCAs for t-way testing with a good solution quality (in terms of test size), which involves artificial intelligence-based strategies, may be obtained by following a well-established methodology during the construction process. 
59|-||Automated refactoring to the Null Object design pattern|ContextNull-checking conditionals are a straightforward solution against null dereferences. However, their frequent repetition is considered a sign of poor program design, since they introduce source code duplication and complexity that impacts code comprehension and maintenance. The Null Object design pattern enables the replacement of null-checking conditionals with polymorphic method invocations that are bound, at runtime, to either a real object or a Null Object.ObjectiveThis work proposes a novel method for automated refactoring to Null Object that eliminates null-checking conditionals associated with optional class fields, i.e., fields that are not initialized in all class instantiations and, thus, their usage needs to be guarded in order to avoid null dereferences.MethodWe introduce an algorithm for automated discovery of refactoring opportunities to Null Object. Moreover, we specify the source code transformation procedure and an extensive set of refactoring preconditions for safely refactoring an optional field and its associated null-checking conditionals to the Null Object design pattern. The method is implemented as an Eclipse plug-in and is evaluated on a set of open source Java projects.ResultsSeveral refactoring candidates are discovered in the projects used in the evaluation and their refactoring lead to improvement of the cyclomatic complexity of the affected classes. The successful execution of the projects’ test suites, on their refactored versions, provides empirical evidence on the soundness of the proposed source code transformation. Runtime performance results highlight the potential for applying our method to a wide range of project sizes.ConclusionOur method automates the elimination of null-checking conditionals through refactoring to the Null Object design pattern. It contributes to improvement of the cyclomatic complexity of classes with optional fields. The runtime processing overhead of applying our method is limited and allows its integration to the programmer’s routine code analysis activities. 
59|-||An efficient approach to identify multiple and independent Move Method refactoring candidates|ContextApplication of a refactoring operation creates a new set of dependency in the revised design as well as a new set of further refactoring candidates. In the studies of stepwise refactoring recommendation approaches, applying one refactoring at a time has been used, but is inefficient because the identification of the best candidate in each iteration of refactoring identification process is computation-intensive. Therefore, it is desirable to accurately identify multiple and independent candidates to enhance efficiency of refactoring process.ObjectiveWe propose an automated approach to identify multiple refactorings that can be applied simultaneously to maximize the maintainability improvement of software. Our approach can attain the same degree of maintainability enhancement as the method of the refactoring identification of the single best one, but in fewer iterations (lower computation cost).MethodThe concept of maximal independent set (MIS) enables us to identify multiple refactoring operations that can be applied simultaneously. Each MIS contains a group of refactoring candidates that neither affect (i.e., enable or disable) nor influence maintainability on each other. Refactoring effect delta table quantifies the degree of maintainability improvement each elementary candidate. For each iteration of the refactoring identification process, multiple refactorings that best improve maintainability are selected among sets of refactoring candidates (MISs).ResultsWe demonstrate the effectiveness and efficiency of the proposed approach by simulating the refactoring operations on several large-scale open source projects such as jEdit, Columba, and JGit. The results show that our proposed approach can improve maintainability by the same degree or to a better extent than the competing method, choosing one refactoring candidate at a time, in a significantly smaller number of iterations. Thus, applying multiple refactorings at a time is both effective and efficient.ConclusionOur proposed approach helps improve the maintainability as well as the productivity of refactoring identification. 
59|-||A systematic literature review on the barriers faced by newcomers to open source software projects|ContextNumerous open source software projects are based on volunteers collaboration and require a continuous influx of newcomers for their continuity. Newcomers face barriers that can lead them to give up. These barriers hinder both developers willing to make a single contribution and those willing to become a project member.ObjectiveThis study aims to identify and classify the barriers that newcomers face when contributing to open source software projects.MethodWe conducted a systematic literature review of papers reporting empirical evidence regarding the barriers that newcomers face when contributing to open source software (OSS) projects. We retrieved 291 studies by querying 4 digital libraries. Twenty studies were identified as primary. We performed a backward snowballing approach, and searched for other papers published by the authors of the selected papers to identify potential studies. Then, we used a coding approach inspired by open coding and axial coding procedures from Grounded Theory to categorize the barriers reported by the selected studies.ResultsWe identified 20 studies providing empirical evidence of barriers faced by newcomers to OSS projects while making a contribution. From the analysis, we identified 15 different barriers, which we grouped into five categories: social interaction, newcomers’ previous knowledge, finding a way to start, documentation, and technical hurdles. We also classified the problems with regard to their origin: newcomers, community, or product.ConclusionThe results are useful to researchers and OSS practitioners willing to investigate or to implement tools to support newcomers. We mapped technical and non-technical barriers that hinder newcomers’ first contributions. The most evidenced barriers are related to socialization, appearing in 75% (15 out of 20) of the studies analyzed, with a high focus on interactions in mailing lists (receiving answers and socialization with other members). There is a lack of in-depth studies on technical issues, such as code issues. We also noticed that the majority of the studies relied on historical data gathered from software repositories and that there was a lack of experiments and qualitative studies in this area. 
59|-||Defining the resource perspective in the development of processes-aware information systems|ContextThe resource perspective has impact on the performance of business processes. However, current Workflow Management Systems (WfMSs) provide disparate support to its implementation and business process modeling languages provide limited capabilities for its definition. Thus, it is difficult to specify requirements regarding this perspective and to select an appropriate WfMS to support them in order to obtain a technological solution aligned with the organizational needs.ObjectiveTo provide support to the definition, implementation, verification and validation of resource perspective requirements in the development of Process-Aware Information Systems (PAISs) based on WfMSs.MethodThe following activities were carried out: (i) identification of resource perspective aspects in executable workflow specifications, (ii) analysis of the elements provided by the BPMN modeling language to represent these aspects, (iii) development of a framework based on BPMN for defining and implementing these aspects by using the extension mechanism provided by this language, (iv) development of a model-driven development method that leverages the framework to develop PAISs, and (v) validation of the proposed framework and method through the development of a tool supporting them, a case study, and the evaluation against the Workflow Resource Patterns.ResultsA framework, a method and a tool that support the definition of the resource perspective in the development of PAISs.ConclusionBy using the proposed framework and method, practitioners are able to: define the resource perspective requirements in conceptual process models, select a WfMS as implementation platform, and define the implementation of these requirements maintaining the consistency between the conceptual process models and the workflow specifications. 
59|-||Synergy between Activity Theory and goal/scenario modeling for requirements elicitation, analysis, and evolution|ContextIt is challenging to develop comprehensive, consistent, analyzable requirements models for evolving requirements. This is particularly critical for certain highly interactive types of socio-technical systems that involve a wide range of stakeholders with disparate backgrounds; system success is often dependent on how well local social constraints are addressed in system design.ObjectiveThis paper describes feasibility research, combining a holistic social system perspective provided by Activity Theory (AT), a psychological paradigm, with existing system development methodologies and tools, specifically goal and scenario modeling.MethodAT is used to understand the relationships between a system, its stakeholders, and the system’s evolving context. The User Requirements Notation (URN) is used to produce rigorous, analyzable specifications combining goal and scenario models. First, an AT language was developed constraining the framework for automation, second consistency heuristics were developed for constructing and analyzing combined AT/URN models, third a combined AT/URN methodology was developed, and consequently applied to a proof-of-concept system.ResultsAn AT language with limited tool support was developed, as was a combined AT/URN methodology. This methodology was applied to an evolving disease management system to demonstrate the feasibility of adapting AT for use in system development with existing methodologies and tools. Bi-directional transformations between the languages allow proposed changes in system design to be propagated to AT models for use in stakeholder discussions regarding system evolution.ConclusionsThe AT framework can be constrained for use in requirements elicitation and combined with URN tools to provide system designs that include social system perspectives. The developed AT/URN methodology can help engineers to track the impact on system design due to requirement changes triggered by changes in the system’s social context. The methodology also allows engineers to assess the impact of proposed system design changes on the social elements of the system context. 
59|-||Analyzing impact of experience curve on ROI in the software product line adoption process|ContextExperience curve is a well-known concept in management and education science, which explains the phenomenon of increased worker efficiency with repetitive production of a good or service.ObjectiveWe aim to analyze the impact of the experience curve effect on the Return on Investment (ROI) in the software product line engineering (SPLE) process.MethodWe first present the results of a systematic literature review (SLR) to explicitly depict the studies that have considered the impact of experience curve effect on software development in general. Subsequently, based on the results of the SLR, the experience curve effect models in the literature, and the SPLE cost models, we define an approach for extending the cost models with the experience curve effect. Finally, we discuss the application of the refined cost models in a real industrial context.ResultsThe SLR resulted in 15 primary studies which confirm the impact of experience curve effect on software development in general but the experience curve effect in the adoption of SPLE got less attention. The analytical discussion of the cost models and the application of the refined SPLE cost models in the industrial context showed a clear impact of the experience curve effect on the time-to-market, cost of development and ROI in the SPLE adoption process.ConclusionsThe proposed analysis with the newly defined cost models for SPLE adoption provides a more precise analysis tool for the management, and as such helps to support a better decision making. 
59|-||A multivariate statistical framework for the analysis of software effort phase distribution|ContextIn software project management, the distribution of resources to various project activities is one of the most challenging problems since it affects team productivity, product quality and project constraints related to budget and scheduling.ObjectiveThe study aims to (a) reveal the high complexity of modelling the effort usage proportion in different phases as well as the divergence from various rules-of-thumb in related literature, and (b) present a systematic data analysis framework, able to offer better interpretations and visualisation of the effort distributed in specific phases.MethodThe basis for the proposed multivariate statistical framework is Compositional Data Analysis, a methodology appropriate for proportions, along with other methods like the deviation from rules-of-thumb, the cluster analysis and the analysis of variance. The effort allocations to phases, as reported in around 1500 software projects of the ISBSG R11 repository, were transformed to vectors of proportions of the total effort and were analysed with respect to prime project attributes.ResultsThe proposed statistical framework was able to detect high dispersion among data, distribution inequality and various interesting correlations and trends, groupings and outliers, especially with respect to other categorical and continuous project attributes. Only a very small number of projects were found close to the rules-of-thumb from the related literature. Significant differences in the proportion of effort spent in different phrases for different types of projects were found.ConclusionThere is no simple model for the effort allocated to phases of software projects. The data from previous projects can provide valuable information regarding the distribution of the effort for various types of projects, through analysis with multivariate statistical methodologies. The proposed statistical framework is generic and can be easily applied in a similar sense to any dataset containing effort allocation to phases. 
59|-||An empirical study on software defect prediction with a simplified metric set|ContextSoftware defect prediction plays a crucial role in estimating the most defect-prone components of software, and a large number of studies have pursued improving prediction accuracy within a project or across projects. However, the rules for making an appropriate decision between within- and cross-project defect prediction when available historical data are insufficient remain unclear.ObjectiveThe objective of this work is to validate the feasibility of the predictor built with a simplified metric set for software defect prediction in different scenarios, and to investigate practical guidelines for the choice of training data, classifier and metric subset of a given project.MethodFirst, based on six typical classifiers, three types of predictors using the size of software metric set were constructed in three scenarios. Then, we validated the acceptable performance of the predictor based on Top-k metrics in terms of statistical methods. Finally, we attempted to minimize the Top-k metric subset by removing redundant metrics, and we tested the stability of such a minimum metric subset with one-way ANOVA tests.ResultsThe study has been conducted on 34 releases of 10 open-source projects available at the PROMISE repository. The findings indicate that the predictors built with either Top-k metrics or the minimum metric subset can provide an acceptable result compared with benchmark predictors. The guideline for choosing a suitable simplified metric set in different scenarios is presented in  Table 12.ConclusionThe experimental results indicate that (1) the choice of training data for defect prediction should depend on the specific requirement of accuracy; (2) the predictor built with a simplified metric set works well and is very useful in case limited resources are supplied; (3) simple classifiers (e.g., Naïve Bayes) also tend to perform well when using a simplified metric set for defect prediction; and (4) in several cases, the minimum metric subset can be identified to facilitate the procedure of general defect prediction with acceptable loss of prediction precision in practice. 
59|-||Improving the management of product lines by performing domain knowledge extraction and cross product line analysis|ContextIncrease in market competition is one of the main reasons for developing and maintaining families of systems, termed Product Lines (PLs). Managing those PLs is challenging, let alone the management of several related PLs. Currently, those PLs are managed separately or their relations are analyzed assuming explicit specification of dependencies or use of an underlying terminology. Such assumptions may not hold when developing the PLs in different departments or companies applying various engineering processes.ObjectiveIn this work we call for utilizing the knowledge gained from developing and maintaining different PLs in the same domain in order to recommend on improvements to the management of PLs.MethodThe suggested approach conducts domain knowledge extraction and cross PL analysis on feature diagrams – the main aid for modeling PL variability. The domain knowledge is extracted by applying similarity metrics, clustering, and mining techniques. Based on the created domain models, the approach performs cross PL analysis that examines relations in the domain models and generates improvement recommendations to existing PLs and overall management recommendations (e.g., merging or splitting PLs).ResultsThe approach outcomes were evaluated by humans in a domain of mobile phones. The evaluation results may provide evidence that the outcomes of the approach in general and its recommendations in particular meet human perception of the given domain.ConclusionWe conclude that through domain knowledge extraction and cross PL analysis the suggested approach may generate recommendations useful to the management of individual PLs, as well as to the overall management of different PLs in the same domain. 
59|-||A framework for software process deployment and evaluation|ContextSoftware Process Engineering promotes the systematic production of software by following a set of well-defined technical and management processes. A comprehensive management of these processes involves the accomplishment of a number of activities such as model design, verification, validation, deployment and evaluation. However, the deployment and evaluation activities need more research efforts in order to achieve greater automation.ObjectiveWith the aim of minimizing the required time to adapt the tools at the beginning of each new project and reducing the complexity of the construction of mechanisms for automated evaluation, the Software Process Deployment & Evaluation Framework (SPDEF) has been elaborated and is described in this paper.MethodThe proposed framework is based on the application of well-known techniques in Software Engineering, such as Model Driven Engineering and Information Integration through Linked Open Data. It comprises a systematic method for the deployment and evaluation, a number of models and relationships between models, and some software tools.ResultsAutomated deployment of the OpenUP methodology is tested through the application of the SPDEF framework and support tools to enable the automated quality assessment of software development or maintenance projects.ConclusionsMaking use of the method and the software components developed in the context of the proposed framework, the alignment between the definition of the processes and the supporting tools is improved, while the existing complexity is reduced when it comes to automating the quality evaluation of software processes. 
59|-||A measurement method for sizing the structure of UML sequence diagrams|ContextThe COSMIC functional size measurement method on UML diagrams has been investigated as a means to estimate the software effort early in the software development life cycle. Like other functional size measurement methods, the COSMIC method takes into account the data movements in the UML sequence diagrams for example, but does not consider the data manipulations in the control structure. This paper explores software sizing at a finer level of granularity by taking into account the structural aspect of a sequence diagram in order to quantify its structural size. These functional and structural sizes can then be used as distinct independent variables to improve effort estimation models.ObjectiveThe objective is to design an improved measurement of the size of the UML sequence diagrams by taking into account the data manipulations represented by the structure of the sequence diagram, which will be referred to as their structural size.MethodWhile the design of COSMIC defines the functional size of a functional process at a high level of granularity (i.e. the data movements), the structural size of a sequence diagram is defined at a finer level of granularity: the size of the flow graph of their control structure described through the alt, opt and loop constructs. This new measurement method was designed by following the process recommended in Software Metrics and Software Metrology (Abran, 2010).ResultsThe size of sequence diagrams can now be measured from two perspectives, both functional and structural, and at different levels of granularity with distinct measurement units.ConclusionIt is now feasible to measure the size of functional requirements at two levels of granularity: at an abstract level, the software functional size can be measured in terms of COSMIC Function Point (CFP) units; and at a detailed level, the software structural size can be measured in terms of Control Structure Manipulation (CSM) units. These measures represent complementary aspects of software size and can be used as distinct independent variables to improve effort estimation models. 
59|-||An approach and tool for measurement of state variable based data-flow test coverage for aspect-oriented programs|ContextData-flow testing approaches have been used for procedural and object-oriented programs, and shown to be effective in detecting faults. However, few such approaches have been evaluated for aspect-oriented programs. In such programs, data-flow interactions can occur between base classes and aspects, which can affect the behavior of both. Faults resulting from such interactions are hard to detect unless the interactions are specifically targeted during testing.ObjectiveThis paper presents an approach and tool implementation for measuring data-flow coverage based on state variables defined in base classes or aspects in AspectJ programs. The paper also reports on an empirical study that compares the cost and effectiveness of data-flow test criteria that are based on state variables with two control-flow criteria.MethodEffectiveness of the criteria was evaluated for various fault types. Cost-effectiveness of test suites that cover all state variable definition-use associations (DUAs) was evaluated for three coverage levels: 100%, 90%, and 80%.ResultsThe effort needed to obtain a test case that achieves data-flow coverage is higher than the effort needed to obtain a test case that covers a block or a branch in an advised class. Covering certain data flow associations requires more effort than for other types of data flow associations. The data-flow test criteria based on state variables of a base-class are in general more effective than control-flow criteria.ConclusionsOverall, it is cost-effective to obtain test suites at the 90% coverage level of data-flow criteria. 
60|-|http://www.sciencedirect.com/science/journal/09505849/60|Evidence management for compliance of critical systems with safety standards: A survey on the state of practice|ContextDemonstrating compliance of critical systems with safety standards involves providing convincing evidence that the requirements of a standard are adequately met. For large systems, practitioners need to be able to effectively collect, structure, and assess substantial quantities of evidence.ObjectiveThis paper aims to provide insights into how practitioners deal with safety evidence management for critical computer-based systems. The information currently available about how this activity is performed in the industry is very limited.MethodWe conducted a survey to determine practitioners’ perspectives and practices on safety evidence management. A total of 52 practitioners from 15 countries and 11 application domains responded to the survey. The respondents indicated the types of information used as safety evidence, how evidence is structured and assessed, how evidence evolution is addressed, and what challenges are faced in relation to provision of safety evidence.ResultsOur results indicate that (1) V&V artefacts, requirements specifications, and design specifications are the most frequently used safety evidence types, (2) evidence completeness checking and impact analysis are mostly performed manually at the moment, (3) text-based techniques are used more frequently than graphical notations for evidence structuring, (4) checklists and expert judgement are frequently used for evidence assessment, and (5) significant research effort has been spent on techniques that have seen little adoption in the industry. The main contributions of the survey are to provide an overall and up-to-date understanding of how the industry addresses safety evidence management, and to identify gaps in the state of the art.ConclusionWe conclude that (1) V&V plays a major role in safety assurance, (2) the industry will clearly benefit from more tool support for collecting and manipulating safety evidence, and (3) future research on safety evidence management needs to place more emphasis on industrial applications. 
60|-||An industrial case study on variability handling in large enterprise software systems|ContextEnterprise software systems (e.g., enterprise resource planning software) are often deployed in different contexts (e.g., different organizations or different business units or branches of one organization). However, even though organizations, business units or branches have the same or similar business goals, they may differ in how they achieve these goals. Thus, many enterprise software systems are subject to variability and adapted depending on the context in which they are used.ObjectiveOur goal is to provide a snapshot of variability in large scale enterprise software systems. We aim at understanding the types of variability that occur in large industrial enterprise software systems. Furthermore, we aim at identifying how variability is handled in such systems.MethodWe performed an exploratory case study in two large software organizations, involving two large enterprise software systems. Data were collected through interviews and document analysis. Data were analyzed following a grounded theory approach.ResultsWe identified seven types of variability (e.g., functionality, infrastructure) and eight mechanisms to handle variability (e.g., add-ons, code switches).ConclusionsWe provide generic types for classifying variability in enterprise software systems, and reusable mechanisms for handling such variability. Some variability types and handling mechanisms for enterprise software systems found in the real world extend existing concepts and theories. Others confirm findings from previous research literature on variability in software in general and are therefore not specific to enterprise software systems. Our findings also offer a theoretical foundation for describing variability handling in practice. Future work needs to provide more evaluations of the theoretical foundations, and refine variability handling mechanisms into more detailed practices. 
60|-||The CARE platform for the analysis of behavior model inference techniques|ContextFinite State Machine (FSM) inference from execution traces has received a lot of attention over the past few years. Various approaches have been explored, each holding different properties for the resulting models, but the lack of standard benchmarks limits the ability of comparing the proposed techniques. Evaluation is usually performed on a few case studies, which is useful for assessing the feasibility of the algorithm on particular cases, but fails to demonstrate effectiveness in a broad context. Consequently, understanding the strengths and weaknesses of inference techniques remains a challenging task.ObjectiveThis paper proposes CARE, a general, approach-independent, platform for the intensive evaluation of FSM inference techniques.MethodGrounded in a program specification scheme that provides a good control on the expected program structures, it allows the production of large benchmarks with well identified properties.ResultsThe CARE platform demonstrates the following features: (1) providing a benchmarking mechanism for FSM inference techniques, (2) allowing analysis of existing techniques w.r.t. a class of programs and/or behaviors, and (3) helping users in choosing the best suited approach for their objective. Moreover, our extensive experiments on different FSM inference techniques highlight that they do not behave in the same manner on every class of program. Characterizing different classes of programs thus helps understanding the strengths and weaknesses of the studied techniques.ConclusionExperiments reported in this paper show examples of use cases that demonstrate the ability of the platform to generate large and diverse sets of programs, which allows to carry out meaningful inference techniques analysis. The analysis strategies the CARE platform offers open new opportunities for program behavior learning, particularly in conjunction with model checking techniques. The CARE platform is available at http://care.lip6.fr. 
60|-||Facilitating construction of safety cases from formal models in Event-B|ContextCertification of safety–critical software systems requires submission of safety assurance documents, e.g., in the form of safety cases. A safety case is a justification argument used to show that a system is safe for a particular application in a particular environment. Different argumentation strategies (informal and formal) are applied to determine the evidence for a safety case. For critical software systems, application of formal methods is often highly recommended for their safety assurance.ObjectiveThe objective of this paper is to propose a methodology that combines two activities: formalisation of system safety requirements of critical software systems for their further verification as well as derivation of structured safety cases from the associated formal specifications.MethodWe propose a classification of system safety requirements in order to facilitate the mapping of informally defined requirements into a formal model. Moreover, we propose a set of argument patterns that aim at enabling the construction of (a part of) a safety case from a formal model in Event-B.ResultsThe results reveal that the proposed classification-based mapping of safety requirements into formal models facilitates requirements traceability. Moreover, the provided detailed guidelines on construction of safety cases aim to simplify the task of the argument pattern instantiation for different classes of system safety requirements. The proposed methodology is illustrated by numerous case studies.ConclusionFirstly, the proposed methodology allows us to map the given system safety requirements into elements of the formal model to be constructed, which is then used for verification of these requirements. Secondly, it guides the construction of a safety case, aiming to demonstrate that the safety requirements are indeed met. Consequently, the argumentation used in such a constructed safety case allows us to support it with formal proofs and model checking results used as the safety evidence. 
60|-||Empirical evaluation of a decision support model for adopting software product line engineering|ContextThe software product line engineering (SPLE) community has provided several different approaches for assessing the feasibility of SPLE adoption and selecting transition strategies. These approaches usually include many rules and guidelines which are very often implicit or scattered over different publications. Hence, for the practitioners it is not always easy to select and use these rules to support the decision making process. Even in case the rules are known, the lack of automated support for storing and executing the rules seriously impedes the decision making process.ObjectiveWe aim to evaluate the impact of a decision support system (DSS) on decision-making in SPLE adoption. In alignment with this goal, we provide a decision support model (DSM) and the corresponding DSS.MethodFirst, we apply a systematic literature review (SLR) on the existing primary studies that discuss and present approaches for analyzing the feasibility of SPLE adoption and transition strategies. Second, based on the data extraction and synthesis activities of the SLR, the required questions and rules are derived and implemented in the DSS. Third, for validation of the approach we conduct multiple case studies.ResultsIn the course of the SLR, 31 primary studies were identified from which we could construct 25 aspects, 39 questions and 312 rules. We have developed the DSS tool Transit-PL that embodies these elements.ConclusionsThe multiple case study validation showed that the adoption of the developed DSS tool is justified to support the decision making process in SPLE adoption. 
60|-||An empirical research agenda for understanding formal methods productivity|ContextFormal methods, and particularly formal verification, is becoming more feasible to use in the engineering of large highly dependable software-based systems, but so far has had little rigorous empirical study. Its artefacts and activities are different to those of conventional software engineering, and the nature and drivers of productivity for formal methods are not yet understood.ObjectiveTo develop a research agenda for the empirical study of productivity in software projects using formal methods and in particular formal verification. To this end we aim to identify research questions about productivity in formal methods, and survey existing literature on these questions to establish face validity of these questions. And further we aim to identify metrics and data sources relevant to these questions.MethodWe define a space of GQM goals as an investigative framework, focusing on productivity from the perspective of managers of projects using formal methods. We then derive questions for these goals using Easterbrook et al.’s (2008) taxonomy of research questions. To establish face validity, we document the literature to date that reflects on these questions and then explore possible metrics related to these questions. Extensive use is made of literature concerning the L4.verified project completed within NICTA, as it is one of the few projects to achieve code-level formal verification for a large-scale industrially deployed software system.ResultsWe identify more than thirty research questions on the topic in need of investigation. These questions arise not just out of the new type of project context, but also because of the different artefacts and activities in formal methods projects. Prior literature supports the need for research on the questions in our catalogue, but as yet provides little evidence about them. Metrics are identified that would be needed to investigate the questions. Thus although it is obvious that at the highest level concepts such as size, effort, rework and so on are common to all software projects, in the case of formal methods, measurement at the micro level for these concepts will exhibit significant differences.ConclusionsEmpirical software engineering for formal methods is a large open research field. For the empirical software engineering community our paper provides a view into the entities and research questions in this domain. For the formal methods community we identify some of the benefits that empirical studies could bring to the effective management of large formal methods projects, and list some basic metrics and data sources that could support empirical studies. Understanding productivity is important in its own right for efficient software engineering practice, but can also support future research on cost-effectiveness of formal methods, and on the emerging field of Proof Engineering. 
volume|issue|url|title|abstract
61|-|http://www.sciencedirect.com/science/journal/09505849/61|Manual test case derivation from UML activity diagrams and state machines: A controlled experiment|ContextIt is a difficult and challenging task to fully automatize model-based testing because this demands complete and unambiguous system models as input. Therefore, in practice, test cases, especially on the system level, are still derived manually from behavioral models like UML activity diagrams or state machines. But this kind of manual test case derivation is error-prone and knowing these errors makes it possible to provide guidelines to reduce them.ObjectiveThe objective of the study presented in this paper therefore is to examine which errors are possible and actually made when manually deriving test cases from UML activity diagrams or state machines and whether there are differences between these diagram types.MethodWe investigate the errors made when deriving test cases manually in a controlled student experiment. The experiment was performed and internally replicated with overall 84 participants divided into three groups at two institutions.ResultsAs a result of our experiment, we provide a taxonomy of errors made and their frequencies. In addition, our experiment provides evidence that activity diagrams have a higher perceived comprehensibility but also a higher error-proneness than state machines with regard to manual test case derivation. This information helps to develop guidelines for manual test case derivation from UML activity diagrams and state machines.ConclusionMost errors observed were due to missing test steps, conditions or results, or content was written into the wrong field. As activity diagrams have a higher perceived comprehensibility, but also more error-prone than state machines, both diagram types are useful for manual test case derivation. Their application depends on the context and should be complemented with clear rules on how to derive test cases. 
61|-||Supporting the semi-automatic semantic annotation of web services: A systematic literature review|ContextSemantically annotating web services is gaining more attention as an important aspect to support the automatic matchmaking and composition of web services. Therefore, the support of well-known and agreed ontologies and tools for the semantical annotation of web services is becoming a key concern to help the diffusion of semantic web services.ObjectiveThe objective of this systematic literature review is to summarize the current state-of-the-art for supporting the semantical annotation of web services by providing answers to a set of research questions.MethodThe review follows a predefined procedure that involves automatically searching well-known digital libraries. As a result, a total of 35 primary studies were identified as relevant. A manual search led to the identification of 9 additional primary studies that were not reported during the automatic search of the digital libraries. Required information was extracted from these 44 studies against the selected research questions and finally reported.ResultsOur systematic literature review identified some approaches available for semantically annotating functional and non-functional aspects of web services. However, many of the approaches are either not validated or the validation done lacks credibility.ConclusionWe believe that a substantial amount of work remains to be done to improve the current state of research in the area of supporting semantic web services. 
61|-||A systematic mapping study of search-based software engineering for software product lines|ContextSearch-Based Software Engineering (SBSE) is an emerging discipline that focuses on the application of search-based optimization techniques to software engineering problems. Software Product Lines (SPLs) are families of related software systems whose members are distinguished by the set of features each one provides. SPL development practices have proven benefits such as improved software reuse, better customization, and faster time to market. A typical SPL usually involves a large number of systems and features, a fact that makes them attractive for the application of SBSE techniques which are able to tackle problems that involve large search spaces.ObjectiveThe main objective of our work is to identify the quantity and the type of research on the application of SBSE techniques to SPL problems. More concretely, the SBSE techniques that have been used and at what stage of the SPL life cycle, the type of case studies employed and their empirical analysis, and the fora where the research has been published.MethodA systematic mapping study was conducted with five research questions and assessed 77 publications from 2001, when the term SBSE was coined, until 2014.ResultsThe most common application of SBSE techniques found was testing followed by product configuration, with genetic algorithms and multi-objective evolutionary algorithms being the two most commonly used techniques. Our study identified the need to improve the robustness of the empirical evaluation of existing research, a lack of extensive and robust tool support, and multiple avenues worthy of further investigation.ConclusionsOur study attested the great synergy existing between both fields, corroborated the increasing and ongoing interest in research on the subject, and revealed challenging open research questions. 
61|-||Requirements simulation for early validation using Behavior Trees and Datalog|ContextThe role of formal specification in requirements validation and analysis is generally considered to be limited because considerable expertise is required in developing and understanding the mathematical proofs. However, formal semantics of a language can provide a basis for step-by-step execution of requirements specification by building an easy to use simulator to assist in requirements elicitation, validation and analysis.ObjectiveThe objective of this paper is to illustrate the usefulness of a simulator that executes requirements and captures system states as rules and facts in a database. The database can then be queried to carry out analysis after all the requirements have been executed a given number of timesMethodBehavior Trees (BTs)1 are automatically translated into Datalog facts and rules through a simulator called SimTree. The translation process involves model-to-model (M2M) transformation and model-to-text (M2T) transformation which automatically generates the code for a simulator called SimTree. SimTree on execution produces Datalog code. The effectiveness of the simulator is evaluated using the specifications of a published case study – Ambulatory Infusion Pump (AIP)2.ResultsThe BT specification of the AIP was transformed into SimTree code for execution. The simulator produced a complete state-space for a predetermined number of runs in the form of Datalog facts and rules, which were then analyzed for various properties of interest like safety and liveness.ConclusionQueries of the resultant Datalog code were found to be helpful in identifying defects in the specification. However, probability values had to be manually assigned to all the events to ensure reachability to all leaf nodes of the tree and timely completion of all the runs. We identify optimization of execution paths to reduce execution time as our future work. 
61|-||Comparing development approaches and reuse strategies: An empirical evaluation of developer views from the aerospace industry|ContextThere is a debate in the aerospace industry whether lessons from reuse successes and failures in nonembedded software can be applied to embedded software. Reuse supposedly reduces development time and errors. The aerospace industry was an early advocate of reuse, but in Aerospace, not all reuse experiences have been as successful as expected. Some major projects experienced large overruns in time, budget, as well as inferior performance, at least in part, due to the gap between reuse expectations and reuse outcomes. This seemed to be especially the case for embedded systems.ObjectiveOur goal is to discover software reuse practices in the aerospace industry. In particular, we wish to learn whether practitioners who develop embedded systems use the same development approaches and artifacts as software practitioners who develop nonembedded systems. We wish to learn whether reuse influences selection of development approaches and artifacts and whether outcomes are impacted.MethodWe developed a survey given to software practitioners in a major Aerospace Corporation developing either embedded or nonembedded systems. The survey probed to identify development methods used, artifacts reused and outcomes resulting from the reuse. We used qualitative and quantitative methods such as descriptive statistics, MANOVA, Principle Component Analysis and an analysis of freeform comments to compare reuse practices between embedded systems and nonembedded systems development.ResultsWe found that embedded systems were more likely to include component based development, product line development and model based development in their development approach, whereas nonembedded systems were more likely to include Ad Hoc and COTS/GOTS in their development approach. Embedded systems developers tended to reuse more and different reuse artifacts.ConclusionWe found that, while outcomes were nearly identical, the development approaches and artifacts used did, in fact, differ. In particular, the tight coupling between code and the platform in embedded systems often dictated the development approach and reuse artifacts and identified some of the reasons. 
61|-||ELBlocker: Predicting blocking bugs with ensemble imbalance learning|ContextBlocking bugs are bugs that prevent other bugs from being fixed. Previous studies show that blocking bugs take approximately two to three times longer to be fixed compared to non-blocking bugs.ObjectiveThus, automatically predicting blocking bugs early on so that developers are aware of them, can help reduce the impact of or avoid blocking bugs. However, a major challenge when predicting blocking bugs is that only a small proportion of bugs are blocking bugs, i.e., there is an unequal distribution between blocking and non-blocking bugs. For example, in Eclipse and OpenOffice, only 2.8% and 3.0% bugs are blocking bugs, respectively. We refer to this as the class imbalance phenomenon.MethodIn this paper, we propose ELBlocker to identify blocking bugs given a training data. ELBlocker first randomly divides the training data into multiple disjoint sets, and for each disjoint set, it builds a classifier. Next, it combines these multiple classifiers, and automatically determines an appropriate imbalance decision boundary to differentiate blocking bugs from non-blocking bugs. With the imbalance decision boundary, a bug report will be classified to be a blocking bug when its likelihood score is larger than the decision boundary, even if its likelihood score is low.ResultsTo examine the benefits of ELBlocker, we perform experiments on 6 large open source projects – namely Freedesktop, Chromium, Mozilla, Netbeans, OpenOffice, and Eclipse containing a total of 402,962 bugs. We find that ELBlocker achieves F1 and EffectivenessRatio@20% scores of up to 0.482 and 0.831, respectively. On average across the 6 projects, ELBlocker improves the F1 and EffectivenessRatio@20% scores over the state-of-the-art method proposed by Garcia and Shihab by 14.69% and 8.99%, respectively. Statistical tests show that the improvements are significant and the effect sizes are large.ConclusionELBlocker can help deal with the class imbalance phenomenon and improve the prediction of blocking bugs. ELBlocker achieves a substantial and statistically significant improvement over the state-of-the-art methods, i.e., Garcia and Shihab’s method, SMOTE, OSS, and Bagging. 
61|-||Real-Time Reflexion Modelling in architecture reconciliation: A multi case study|ContextReflexion Modelling is considered one of the more successful approaches to architecture reconciliation. Empirical studies strongly suggest that professional developers involved in real-life industrial projects find the information provided by variants of this approach useful and insightful, but the degree to which it resolves architecture conformance issues is still unclear.ObjectiveThis paper aims to assess the level of architecture conformance achieved by professional architects using Reflexion Modelling, and to determine how the approach could be extended to improve its suitability for this task.MethodAn in vivo, multi-case-study protocol was adopted across five software systems, from four different financial services organizations. Think-aloud, video-tape and interview data from professional architects involved in Reflexion Modelling sessions were analysed qualitatively.ResultsThis study showed that (at least) four months after the Reflexion Modelling sessions less than 50% of the architectural violations identified were removed. The majority of participants who did remove violations favoured changes to the architectural model rather than to the code. Participants seemed to work off two specific architectural templates, and interactively explored their architectural model to focus in on the causes of violations, and to assess the ramifications of potential code changes. They expressed a desire for dependency analysis beyond static-source-code analysis and scalable visualizations.ConclusionThe findings support several interesting usage-in-practice traits, previously hinted at in the literature. These include (1) the iterative analysis of systems through Reflexion models, as a precursor to possible code change or as a focusing mechanism to identify the location of architecture conformance issues, (2) the extension of the approach with respect to dependency analysis of software systems and architectural modelling templates, (3) improved visualization support and (4) the insight that identification of architectural violations in itself does not lead to their removal in the majority of instances. 
61|-||Estimating, planning and managing Agile Web development projects under a value-based perspective|ContextThe processes of estimating, planning and managing are crucial for software development projects, since the results must be related to several business strategies. The broad expansion of the Internet and the global and interconnected economy make Web development projects be often characterized by expressions like delivering as soon as possible, reducing time to market and adapting to undefined requirements. In this kind of environment, traditional methodologies based on predictive techniques sometimes do not offer very satisfactory results. The rise of Agile methodologies and practices has provided some useful tools that, combined with Web Engineering techniques, can help to establish a framework to estimate, manage and plan Web development projects.ObjectiveThis paper presents a proposal for estimating, planning and managing Web projects, by combining some existing Agile techniques with Web Engineering principles, presenting them as an unified framework which uses the business value to guide the delivery of features.MethodThe proposal is analyzed by means of a case study, including a real-life project, in order to obtain relevant conclusions.ResultsThe results achieved after using the framework in a development project are presented, including interesting results on project planning and estimation, as well as on team productivity throughout the project.ConclusionIt is concluded that the framework can be useful in order to better manage Web-based projects, through a continuous value-based estimation and management process. 
61|-||Automated measurement of API usability: The API Concepts Framework|ContextUsability is an important software quality attribute for APIs. Unfortunately, measuring it is not an easy task since many things like experienced evaluators, suitable test users, and a functional product are needed. This makes existing usability measurement methods difficult to use, especially for non-professionals.ObjectiveTo make API usability measurement easier, an automated and objective measurement method would be needed. This article proposes such a method. Since it would be impossible to find and integrate all possible factors that influence API usability in one step, the main goal is to prove the feasibility of the introduced approach, and to define an extensible framework so that additional factors can easily be defined and added later.MethodA literature review is conducted to find potential factors influencing API usability. From these factors, a selected few are investigated more closely with usability studies. The statistically evaluated results from these studies are used to define specific elements of the introduced framework. Further, the influence of the user as a critical factor for the framework’s feasibility is evaluated.ResultsThe API Concepts Framework is defined, with an extensible structure based on concepts that represent the user’s actions, measurable properties that define what influences the usability of these concepts, and learning effects that represent the influence of the user’s experience. A comparison of values calculated by the framework with user studies shows promising results.ConclusionIt is concluded that the introduced approach is feasible and provides useful results for evaluating API usability. The extensible framework easily allows to add new concepts and measurable properties in the future. 
61|-||Exploring principles of user-centered agile software development: A literature review|ContextIn the last decade, software development has been characterized by two major approaches: agile software development, which aims to achieve increased velocity and flexibility during the development process, and user-centered design, which places the goals and needs of the system’s end-users at the center of software development in order to deliver software with appropriate usability. Hybrid development models, referred to as user-centered agile software development (UCASD) in this article, propose to combine the merits of both approaches in order to design software that is both useful and usable.ObjectiveThis paper aims to capture the current state of the art in UCASD approaches and to derive generic principles from these approaches. More specifically, we investigate the following research question: Which principles constitute a user-centered agile software development approach?MethodWe conduct a systematic review of the literature on UCASD. Identified works are analyzed using a coding scheme that differentiates four levels of UCASD: the process, practices, people/social and technology dimensions. Through subsequent synthesis, we derive generic principles of UCASD.ResultsWe identified and analyzed 83 relevant publications. The analysis resulted in a comprehensive coding system and five principles for UCASD: (1) separate product discovery and product creation, (2) iterative and incremental design and development, (3) parallel interwoven creation tracks, (4) continuous stakeholder involvement, and (5) artifact-mediated communication.ConclusionOur paper contributes to the software development body of knowledge by (1) providing a broad overview of existing works in the area of UCASD, (2) deriving an analysis framework (in form a coding system) for works in this area, going beyond former classifications, and (3) identifying generic principles of UCASD and associating them with specific practices and processes. 
62|-|http://www.sciencedirect.com/science/journal/09505849/62|A systematic literature review on Enterprise Architecture Implementation Methodologies|ContextEnterprise Architecture (EA) is a strategy to align business and Information Technology (IT) within an enterprise. EA is managed, developed, and maintained throughout the EA Implementation Methodology (EAIM).ObjectiveThe aims of this study are to identify the existing effective practices that are used by existing EAIMs, identify the factors that affect the effectiveness of EAIM, identify the current tools that are used by existing EAIMs, and identify the open problems and areas related to EAIM for improvement.MethodA Systematic Literature Review (SLR) was carried out. 669 papers were retrieved by a manual search in 6 databases and 46 primary studies were finally included.ResultFrom these studies 33% were journal articles, 41% were conference papers while 26% were contributions from the studies consisted of book chapters. Consequently, 28 practices, 19 factors, and 15 tools were identified and analysed.ConclusionSeveral rigorous researches have been done in order to provide effective EAIM, however there are still problems in components of EAIM, including: there is lack of tool support for whole part of EA implementation, there are deficiency in addressing the EAIM’s practices especially in modeling, management, and maintenance, there is lack of consideration on non-functional requirement in existing EAIM, there is no appropriate consideration on requirement analysis in most existing EAIM. This review provides researchers with some guidelines for future research on this topic. It also provides broad information on EAIM, which could be useful for practitioners. 
62|-||An adaptive middleware design to support the dynamic interpretation of domain-specific models|ContextAs the use of Domain-Specific Modeling Languages (DSMLs) continues to gain popularity, we have developed new ways to execute DSML models. The most popular approach is to execute code resulting from a model-to-code transformation. An alternative approach is to directly execute these models using a semantic-rich execution engine – Domain-Specific Virtual Machine (DSVM). The DSVM includes a middleware layer responsible for the delivery of services in a given domain.ObjectiveWe will investigate an approach that performs the dynamic combination of constructs in the middleware layer of DSVMs to support the delivery of domain-specific services. This middleware should provide: (a) a model of execution (MoE) that dynamically integrates decoupled domain-specific knowledge (DSK) for service delivery, (b) runtime adaptability based on context and available resources, and (c) the same level of operational assurance as any DSVM middleware.MethodOur approach will involve (1) defining a framework that supports the dynamic combination of MoE and DSK and (2) demonstrating the applicability of our framework in the DSVM middleware for user-centric communication. We will measure the overhead of our approach and provide a cost-benefit analysis factoring in its runtime adaptability using appropriate experimentation.ResultsOur experiments show that combining the DSK and MoE for a DSVM middleware allow us to realize efficient specialization while maintaining the required operability. We also show that the overhead introduced by adaptation is not necessarily deleterious to overall performance in a domain as it may result in more efficient operation selection.ConclusionThe approach defined for the DSVM middleware allows for greater flexibility in service delivery while reducing the complexity of application development for the user. These benefits are achieved at the expense of increased execution times, however this increase may be negligible depending on the domain. 
62|-||Development of service-oriented architectures using model-driven development: A mapping study|ContextModel-Driven Development (MDD) and Service-Oriented Architecture (SOA) are two challenging research areas in software engineering. MDD is about improving software development whilst SOA is a service-based conceptual development style, therefore investigating the available proposals in the literature to use MDD when developing SOA may be insightful. However, no studies have been found with this purpose.ObjectiveThis work aims at assessing the state of the art in MDD for SOA systems. It mainly focuses on: what are the characteristics of MDD approaches that support SOA; what types of SOA are supported; how do they handle non-functional requirements.MethodWe conducted a mapping study following a rigorous protocol. We identified the representative set of venues that should be included in the study. We applied a search string over the set of selected venues. As result, 129 papers were selected and analysed (both frequency analysis and correlation analysis) with respect to the defined classification criteria derived from the research questions. Threats to validity were identified and mitigated whenever possible.ResultsThe analysis allows us to answer the research questions. We highlight: (1) predominance of papers from Europe and written by researchers only; (2) predominance of top-down transformation in software development activities; (3) inexistence of consolidated methods; (4) significant percentage of works without tool support; (5) SOA systems and service compositions more targeted than single services and SOA enterprise systems; (6) limited use of metamodels; (7) very limited use of NFRs; and (8) limited application in real cases.ConclusionThis mapping study does not just provide the state of the art in the topic, but also identifies several issues that deserve investigation in the future, for instance the need of methods for activities other than software development (e.g., migration) or the need of conducting more real case studies. 
62|-||Negative samples reduction in cross-company software defects prediction|ContextSoftware defect prediction has been widely studied based on various machine-learning algorithms. Previous studies usually focus on within-company defects prediction (WCDP), but lack of training data in the early stages of software testing limits the efficiency of WCDP in practice. Thus, recent research has largely examined the cross-company defects prediction (CCDP) as an alternative solution.ObjectiveHowever, the gap of different distributions between cross-company (CC) data and within-company (WC) data usually makes it difficult to build a high-quality CCDP model. In this paper, a novel algorithm named Double Transfer Boosting (DTB) is introduced to narrow this gap and improve the performance of CCDP by reducing negative samples in CC data.MethodThe proposed DTB model integrates two levels of data transfer: first, the data gravitation method reshapes the whole distribution of CC data to fit WC data. Second, the transfer boosting method employs a small ratio of labeled WC data to eliminate negative instances in CC data.ResultsThe empirical evaluation was conducted based on 15 publicly available datasets. CCDP experiment results indicated that the proposed model achieved better overall performance than compared CCDP models. DTB was also compared to WCDP in two different situations. Statistical analysis suggested that DTB performed significantly better than WCDP models trained by limited samples and produced comparable results to WCDP with sufficient training data.ConclusionsDTB reforms the distribution of CC data from different levels to improve the performance of CCDP, and experimental results and analysis demonstrate that it could be an effective model for early software defects detection. 
62|-||Supporting distributed product configuration by integrating heterogeneous variability modeling approaches|ContextIn industrial settings products are developed by more than one organization. Software vendors and suppliers commonly typically maintain their own product lines, which contribute to a larger (multi) product line or software ecosystem. It is unrealistic to assume that the participating organizations will agree on using a specific variability modeling technique—they will rather use different approaches and tools to manage the variability of their systems.ObjectiveWe aim to support product configuration in software ecosystems based on several variability models with different semantics that have been created using different notations.MethodWe present an integrative approach that provides a unified perspective to users configuring products in multi product line environments, regardless of the different modeling methods and tools used internally. We also present a technical infrastructure and a prototype implementation based on web services.ResultsWe show the feasibility of the approach and its implementation by using it with the three most widespread types of variability modeling approaches in the product line community, i.e., feature-based, OVM-style, and decision-oriented modeling. To demonstrate the feasibility and flexibility of our approach, we present an example derived from industrial experience in enterprise resource planning. We further applied the approach to support the configuration of privacy settings in the Android ecosystem based on multiple variability models. We also evaluated the performance of different model enactment strategies used in our approach.ConclusionsTools and techniques allowing stakeholders to handle variability in a uniform manner can considerably foster the initiation and growth of software ecosystems from the perspective of software reuse and configuration. 
62|-||Operationalised product quality models and assessment: The Quamoco approach|ContextSoftware quality models provide either abstract quality characteristics or concrete quality measurements; there is no seamless integration of these two aspects. Quality assessment approaches are, hence, also very specific or remain abstract. Reasons for this include the complexity of quality and the various quality profiles in different domains which make it difficult to build operationalised quality models.ObjectiveIn the project Quamoco, we developed a comprehensive approach aimed at closing this gap.MethodThe project combined constructive research, which involved a broad range of quality experts from academia and industry in workshops, sprint work and reviews, with empirical studies. All deliverables within the project were peer-reviewed by two project members from a different area. Most deliverables were developed in two or three iterations and underwent an evaluation.ResultsWe contribute a comprehensive quality modelling and assessment approach: (1) A meta quality model defines the structure of operationalised quality models. It includes the concept of a product factor, which bridges the gap between concrete measurements and abstract quality aspects, and allows modularisation to create modules for specific domains. (2) A largely technology-independent base quality model reduces the effort and complexity of building quality models for specific domains. For Java and C# systems, we refined it with about 300 concrete product factors and 500 measures. (3) A concrete and comprehensive quality assessment approach makes use of the concepts in the meta-model. (4) An empirical evaluation of the above results using real-world software systems showed: (a) The assessment results using the base model largely match the expectations of experts for the corresponding systems. (b) The approach and models are well understood by practitioners and considered to be both consistent and well suited for getting an overall view on the quality of a software product. The validity of the base quality model could not be shown conclusively, however. (5) The extensive, open-source tool support is in a mature state. (6) The model for embedded software systems is a proof-of-concept for domain-specific quality models.ConclusionWe provide a broad basis for the development and application of quality models in industrial practice as well as a basis for further extension, validation and comparison with other approaches in research. 
62|-||Assessing the use of slicing-based visualizing techniques on the understanding of large metamodels|ContextMetamodels are cornerstones of various metamodeling activities. Such activities consist of, for instance, transforming models into code or comparing metamodels. These activities thus require a good understanding of a metamodel and/or its parts. Current metamodel editing tools are based on standard interactive visualization features, such as physical zooms.ObjectiveHowever, as soon as metamodels become large, navigating through large metamodels becomes a tedious task that hinders their understanding. So, a real need to support metamodel comprehension appears.MethodIn this work we promote the use of model slicing techniques to build interactive visualization tools for metamodels. Model slicing is a model comprehension technique inspired by program slicing. We show how the use of Kompren, a domain-specific language for defining model slicers, can ease the development of such interactive visualization features.ResultsWe specifically make four main contributions. First, the proposed interactive visualization techniques permit users to focus on metamodel elements of interest, which aims at improving the understandability. Second, these proposed techniques are developed based on model slicing, a model comprehension technique that involves extracting a subset of model elements of interest. Third, we develop a metamodel visualizer, called Explen, embedding the proposed interactive visualization techniques. Fourth, we conducted experiments. showing that Explen significantly outperforms EcoreTools, in terms of time, correctness, and navigation effort, on metamodeling tasks.ConclusionThe results of the experiments, in favor of Explen, show that improving metamodel understanding can be done using slicing-based interactive navigation features. 
62|-||Using metrics in Agile and Lean Software Development â A systematic literature review of industrial studies|ContextSoftware industry has widely adopted Agile software development methods. Agile literature proposes a few key metrics but little is known of the actual metrics use in Agile teams.ObjectiveThe objective of this paper is to increase knowledge of the reasons for and effects of using metrics in industrial Agile development. We focus on the metrics that Agile teams use, rather than the ones used from outside by software engineering researchers. In addition, we analyse the influence of the used metrics.MethodThis paper presents a systematic literature review (SLR) on using metrics in industrial Agile software development. We identified 774 papers, which we reduced to 30 primary studies through our paper selection process.ResultsThe results indicate that the reasons for and the effects of using metrics are focused on the following areas: sprint planning, progress tracking, software quality measurement, fixing software process problems, and motivating people. Additionally, we show that although Agile teams use many metrics suggested in the Agile literature, they also use many custom metrics. Finally, the most influential metrics in the primary studies are Velocity and Effort estimate.ConclusionThe use of metrics in Agile software development is similar to Traditional software development. Projects and sprints need to be planned and tracked. Quality needs to be measured. Problems in the process need to be identified and fixed. Future work should focus on metrics that had high importance but low prevalence in our study, as they can offer the largest impact to the software industry. 
62|-||In search of evidence for model-driven development claims: An experiment on quality, effort, productivity and satisfaction|ContextModel-Driven Development (MDD) is a paradigm that prescribes building conceptual models that abstractly represent the system and generating code from these models through transformation rules. The literature is rife with claims about the benefits of MDD, but they are hardly supported by evidences.ObjectiveThis experimental investigation aims to verify some of the most cited benefits of MDD.MethodWe run an experiment on a small set of classes using student subjects to compare the quality, effort, productivity and satisfaction of traditional development and MDD. The experiment participants built two web applications from scratch, one where the developers implement the code by hand and another using an industrial MDD tool that automatically generates the code from a conceptual model.ResultsOutcomes show that there are no significant differences between both methods with regard to effort, productivity and satisfaction, although quality in MDD is more robust to small variations in problem complexity. We discuss possible explanations for these results.ConclusionsFor small systems and less programming-experienced subjects, MDD does not always yield better results than a traditional method, even regarding effort and productivity. This contradicts some previous statements about MDD advantages. The benefits of developing a system with MDD appear to depend on certain characteristics of the development context. 
62|-||Automating correctness verification of artifact-centric business process models|ContextThe artifact-centric methodology has emerged as a new paradigm to support business process management over the last few years. This way, business processes are described from the point of view of the artifacts that are manipulated during the process.ObjectiveOne of the research challenges in this area is the verification of the correctness of this kind of business process models where the model is formed of various artifacts that interact among them.MethodIn this paper, we propose a fully automated approach for verifying correctness of artifact-centric business process models, taking into account that the state (lifecycle) and the values of each artifact (numerical data described by pre and postconditions) influence in the values and the state of the others. The lifecycles of the artifacts and the numerical data managed are modeled by using the Constraint Programming paradigm, an Artificial Intelligence technique.ResultsTwo correctness notions for artifact-centric business process models are distinguished (reachability and weak termination), and novel verification algorithms are developed to check them. The algorithms are complete: neither false positives nor false negatives are generated. Moreover, the algorithms offer precise diagnosis of the detected errors, indicating the execution causing the error where the lifecycle gets stuck.ConclusionTo the best of our knowledge, this paper presents the first verification approach for artifact-centric business process models that integrates pre and postconditions, which define the behavior of the services, and numerical data verification when the model is formed of more than one artifact. The approach can detect errors not detectable with other approaches. 
62|-||Combinatorial testing, random testing, and adaptive random testing for detecting interaction triggered failures|ContextSoftware behavior depends on many factors, and some failures occur only when certain factors interact. This is known as an interaction triggered failure, and the corresponding selection of factor values can be modeled as a Minimal Failure-causing Schema (MFS). (An MFS involving m factors is an m-MFS.) Combinatorial Testing (CT) has been developed to exercise (“hit”) all MFS with few tests. Adaptive Random Resting (ART) endeavors to make tests as different as possible, ensuring that testing of MFS is not unnecessarily repeated. Random Testing (RT) chooses tests at random without regard to the MFS already treated. CT might be expected to improve on RT for finding interaction triggered faults, and yet some studies report no significant difference. CT can also be expected to be better than ART, and yet other studies report that ART can be much better than RT. In light of these, the relative merits of CT, ART, and RT for finding interaction triggered faults are unclear.ObjectiveTo investigate the relationships among CT, ART, and RT, we conduct the first complete and systematic comparison for the purpose of hitting MFS.MethodA systematic review of six aspects of CT, RT and ART is conducted first. Then two kinds of experiments are used to compare them under four metrics.ResultsART improves upon RT, but t  -way CT is better than both. In hitting t′t′-MFS the advantage is typically in the range from 10% to 30% when t=t′t=t′, but becomes much smaller when t′<tt′<t, and there may be no advantage when t′>tt′>t. The latter case may explain the studies reporting no significant difference between RT and CT.ConclusionRT is easily implemented. However, depending on its implementation, ART can improve upon RT. CT does as well as ART whether or nott′=tt′=t, but provides a valuable improvement in the cases when t′=tt′=t. 
63|-|http://www.sciencedirect.com/science/journal/09505849/63|Distributed Pair Programming: A Systematic Literature Review|ContextGeographically distributed teams have adopted agile practices as a work strategy. One of these practices is Distributed Pair Programming (DPP). DPP consists in two developers working remotely on the same design, algorithm or code.ObjectiveIn this paper we sought to identify and synthesize papers that describe and analyze DPP both from teaching and practice perspectives.MethodWe conducted a Systematic Literature Review to search for empirical evidence in eight digital libraries.ResultsMost of the 34 DPP primary studies identified explore DPP from a teaching perspective. We found that DPP requires a specific infrastructure, but the existing studies do not explore the impact of the distribution in the details. There are many tools proposed that support DPP practice, but few of them are evaluated within a software development team.ConclusionWe need more studies that explore the effects of Pair Programming in the context of Distributed Software Development, such as coordination and communication. Most of the studies do not empirically evaluate DPP in industry. There is also a need to propose guidelines to use DPP in industry and as a teaching strategy. 
63|-||A conceptual framework to study the role of communication through social software for coordination in globally-distributed software teams|BackgroundIn Global Software Development (GSD) the lack of face-to-face communication is a major challenge and effective computer-mediated practices are necessary to mitigate the effect of physical distance. Communication through Social Software (SoSo) supports team coordination, helping to deal with geographical distance; however, in Software Engineering literature, there is a lack of suitable theoretical concepts to analyze and describe everyday practices of globally-distributed software development teams and to study the role of communication through SoSo.ObjectiveThe paper proposes a theoretical framework for analyzing how communicative and coordinative practices are constituted and maintained in globally-distributed teams.MethodThe framework is based on the concepts of communicative genres and coordination mechanisms; it is motivated and explicated through examples from two qualitative empirical cases.ResultsCoordination mechanisms and communicative genres mutually support each other. In particular, communication through SoSo supports team members in establishing, developing and maintaining social protocols within the distributed team. Software Engineering tools and methods provide templates for coordination mechanism that need to be adapted and adopted in order to support the project at hand. SoSo serves as a medium for the necessary metawork. The theoretical framework proposed is used to describe both the practices in an established industrial project and the establishing of practices in three student teams. The framework allows explaining the heterogeneity of practices observed.ConclusionsThis paper presents a conceptual framework to study the role of communication through SoSo for coordination in GSD. The usefulness of the framework is supported by empirical findings on the role of SoSo. The theoretical framework can be beneficial for future research that aims to analyze and describe not only the role of SoSo, but also how communicative and coordinative practices can be established and maintained in GSD teams. 
63|-||Measures of process harmonization|ContextMany large organizations juggle an application portfolio that contains different applications that fulfill similar tasks in the organization. In an effort to reduce operating costs, they are attempting to consolidate such applications. Before consolidating applications, the work that is done with these applications must be harmonized. This is also known as process harmonization.ObjectiveThe increased interest in process harmonization calls for measures to quantify the extent to which processes have been harmonized. These measures should also uncover the factors that are of interest when harmonizing processes. Currently, such measures do not exist. Therefore, this study develops and validates a measurement model to quantify the level of process harmonization in an organization.MethodThe measurement model was developed by means of a literature study and structured interviews. Subsequently, it was validated through a survey, using factor analysis and correlations with known related constructs.ResultsAs a result, a valid and reliable measurement model was developed. The factors that are found to constitute process harmonization are: the technical design of the business process and its data, the resources that execute the process, and the information systems that are used in the process. In addition, strong correlations were found between process harmonization and process standardization and between process complexity and process harmonization.ConclusionThe measurement model can be used by practitioners, because it shows them the factors that must be taken into account when harmonizing processes, and because it provides them with a means to quantify the extent to which they succeeded in harmonizing their processes. At the same time, it can be used by researchers to conduct further empirical research in the area of process harmonization. 
63|-||A fuzzy logic based approach for phase-wise software defects prediction using software metrics|ContextThe software defect prediction during software development has recently attracted the attention of many researchers. The software defect density indicator prediction in each phase of software development life cycle (SDLC) is desirable for developing a reliable software product. Software defect prediction at the end of testing phase may not be more beneficial because the changes need to be performed in the previous phases of SDLC may require huge amount of money and effort to be spent in order to achieve target software quality. Therefore, phase-wise software defect density indicator prediction model is of great importance.ObjectiveIn this paper, a fuzzy logic based phase-wise software defect prediction model is proposed using the top most reliability relevant metrics of the each phase of the SDLC.MethodIn the proposed model, defect density indicator in requirement analysis, design, coding and testing phase is predicted using nine software metrics of these four phases. The defect density indicator metric predicted at the end of the each phase is also taken as an input to the next phase. Software metrics are assessed in linguistic terms and fuzzy inference system has been employed to develop the model.ResultsThe predictive accuracy of the proposed model is validated using twenty real software project data. Validation results are satisfactory. Measures based on the mean magnitude of relative error and balanced mean magnitude of relative error decrease significantly as the software project size increases.ConclusionIn this paper, a fuzzy logic based model is proposed for predicting software defect density indicator at each phase of the SDLC. The predicted defects of twenty different software projects are found very near to the actual defects detected during testing. The predicted defect density indicators are very helpful to analyze the defect severity in different artifacts of SDLC of a software project. 
63|-||Two controlled experiments on model-based architectural decision making|ContextIn recent years, architectural design decisions are becoming more and more common for documenting software architectures. Rather than describing the structure of software systems, architectural decisions capture the design rationale and – often reusable – architectural knowledge. Many approaches and tools have been proposed in the literature to support architectural decision making and documentation (for instance, based on models, ontologies, or templates). In this context, the capturing, organization, and effective reuse of architectural knowledge has gained a lot of attention.ObjectiveHowever, there is little empirical evidence about the supportive effect of reusable architectural knowledge on the effectiveness and efficiency of architectural decision making.MethodTo investigate these aspects, we conducted two separate controlled experiments with software architecture students in which we tested the supportive effect of reusable decision models in decision making and documentation.ResultsOur results show that the use of reusable decision models can significantly increase both the efficiency and the effectiveness of novice architects.ConclusionWe can report, that our findings are in line with similar studies and support the claims regarding reusable architectural design decisions in principle. 
64|-|http://www.sciencedirect.com/science/journal/09505849/64|Guidelines for conducting systematic mapping studies in software engineering: An update|ContextSystematic mapping studies are used to structure a research area, while systematic reviews are focused on gathering and synthesizing evidence. The most recent guidelines for systematic mapping are from 2008. Since that time, many suggestions have been made of how to improve systematic literature reviews (SLRs). There is a need to evaluate how researchers conduct the process of systematic mapping and identify how the guidelines should be updated based on the lessons learned from the existing systematic maps and SLR guidelines.ObjectiveTo identify how the systematic mapping process is conducted (including search, study selection, analysis and presentation of data, etc.); to identify improvement potentials in conducting the systematic mapping process and updating the guidelines accordingly.MethodWe conducted a systematic mapping study of systematic maps, considering some practices of systematic review guidelines as well (in particular in relation to defining the search and to conduct a quality assessment).ResultsIn a large number of studies multiple guidelines are used and combined, which leads to different ways in conducting mapping studies. The reason for combining guidelines was that they differed in the recommendations given.ConclusionThe most frequently followed guidelines are not sufficient alone. Hence, there was a need to provide an update of how to conduct systematic mapping studies. New guidelines have been proposed consolidating existing findings. 
64|-||Passive testing of communicating systems with timeouts|ContextThe design of complex systems demands methodologies to analyze its correct behaviour. It is usual that a correct behaviour is determined by the compliance with temporal requirements. Currently, testing is the most used technology to validate the correctness of systems. Although several techniques that take into account time aspects have been proposed, most of them require the tester interacts with the system. However, if this is not possible, it is necessary to apply a passive testing approach where the tester monitors the behaviour of the system.ObjectiveThe aim of this paper is to propose a methodology to perform passive testing on communicating systems in which the behaviour of their components must fulfill temporal restrictions associated with both performance and delays/timeouts.MethodOur framework uses algorithms for checking traces collected from the systems against invariants which formally represent the most relevant properties that must be fulfilled by the system. In order to support the feasibility of the methodology, we have performed an empirical study on a complex system for automatic recognition of images based on a pipeline architecture. We have analyzed the correctness of the system’s behaviour with respect to a set of invariants. Finally, an experiment, based on mutations of the system, was conducted to study the level of detection of a set of invariants.ResultsDifferent errors were detected and fixed along the development of the system by means of the proposed methodology. The results of the experiments with the mutated versions of the system indicated that the designed set of invariants was more effective in finding errors associated to temporal aspects than those related to communication among components.ConclusionThe proposed technique has been shown to be very useful for analyzing complex timed systems, and find errors when the tester has no control over their behaviour. 
64|-||In defence of deep modelling|ContextSince multi-level modelling emerged as a strategy for leveraging classification levels in conceptual models, there have been discussions about what it entails and how best to support it. Recently, some authors have claimed that the deep modelling approach to multi-level modelling entails paradoxes and significant weaknesses. By drawing upon concepts from speech act theory and foundational ontologies these authors argue that hitherto accepted principles for deep modelling should be abandoned and an alternative approach be adopted instead (Eriksson et al., 2013).ObjectiveWe investigate the validity of these claims and motivate the need to shift the focus of the debate from philosophical arguments to modelling pragmatics.MethodWe present each of the main objections raised against deep modelling in turn, classify them according to the kinds of arguments put forward, and analyse the cogency of the supporting justification. We furthermore analyse the counter proposal regarding its pragmatic value for modellers.ResultsMost of the criticisms against deep modelling are based on mismatches between the premisses used in published definitions of deep modelling and those used by the authors as the basis of their challenges. Hence, most of the criticisms levelled at deep modelling do not actually apply to deep modelling as defined in the literature. We also explain how the proposed alternative introduces new problems of its own, and evaluate its merits from a pragmatic modelling perspective. Finally, we show how deep modelling is indeed compatible with, and can be founded on, classic work in linguistics and logic.ConclusionsThe inappropriate interpretations of the core principles of deep modelling identified in this article indicate that previous descriptions of them have not had sufficient clarity. We therefore provide further clarification and foundational background material to reduce the chance for future misunderstandings and help establish deep modelling as a solid foundation for multi-level modelling. 
64|-||The financial aspect of managing technical debt: A systematic literature review|ContextTechnical debt is a software engineering metaphor, referring to the eventual financial consequences of trade-offs between shrinking product time to market and poorly specifying, or implementing a software product, throughout all development phases. Based on its inter-disciplinary nature, i.e. software engineering and economics, research on managing technical debt should be balanced between software engineering and economic theories.ObjectiveThe aim of this study is to analyze research efforts on technical debt, by focusing on their financial aspect. Specifically, the analysis is carried out with respect to: (a) how financial aspects are defined in the context of technical debt and (b) how they relate to the underlying software engineering concepts.MethodIn order to achieve the abovementioned goals, we employed a standard method for SLRs and applied it on studies retrieved from seven general-scope digital libraries. In total we selected 69 studies relevant to the financial aspect of technical debt.ResultsThe most common financial terms that are used in technical debt research are principal and interest, whereas the financial approaches that have been more frequently applied for managing technical debt are real options, portfolio management, cost/benefit analysis and value-based analysis. However, the application of such approaches lacks consistency, i.e., the same approach is differently applied in different studies, and in some cases lacks a clear mapping between financial and software engineering concepts.ConclusionThe results are expected to prove beneficial for the communication between technical managers and project managers, in the sense that they will provide a common vocabulary, and will help in setting up quality-related goals, during software development. To achieve this we introduce: (a) a glossary of terms and (b) a classification scheme for financial approaches used for managing technical debt. Based on these, we have been able to underline interesting implications for researchers and practitioners. 
64|-||Editorial for the special section on Empirical Studies in Software Engineering Selected, and extended papers from the Eighteenth International Conference on Evaluation and Assessment in Software Engineering, May 13thâ14th 2014, London, UK|
64|-||Investigations about replication of empirical studies in software engineering: A systematic mapping study|ContextTwo recent mapping studies which were intended to verify the current state of replication of empirical studies in Software Engineering (SE) identified two sets of studies: empirical studies actually reporting replications (published between 1994 and 2012) and a second group of studies that are concerned with definitions, classifications, processes, guidelines, and other research topics or themes about replication work in empirical software engineering research (published between 1996 and 2012).ObjectiveIn this current article, our goal is to analyze and discuss the contents of the second set of studies about replications to increase our understanding of the current state of the work on replication in empirical software engineering research.MethodWe applied the systematic literature review method to build a systematic mapping study, in which the primary studies were collected by two previous mapping studies covering the period 1996–2012 complemented by manual and automatic search procedures that collected articles published in 2013.ResultsWe analyzed 37 papers reporting studies about replication published in the last 17 years. These papers explore different topics related to concepts and classifications, presented guidelines, and discuss theoretical issues that are relevant for our understanding of replication in our field. We also investigated how these 37 papers have been cited in the 135 replication papers published between 1994 and 2012.ConclusionsReplication in SE still lacks a set of standardized concepts and terminology, which has a negative impact on the replication work in our field. To improve this situation, it is important that the SE research community engage on an effort to create and evaluate taxonomy, frameworks, guidelines, and methodologies to fully support the development of replications. 
64|-||On the usefulness of ownership metrics in open-source software projects|ContextCode ownership metrics were recently defined in order to distinguish major and minor contributors of a software module, and to assess whether the ownership of such a module is strong or shared between developers.ObjectiveThe relationship between these metrics and software quality was initially validated on proprietary software projects. Our objective in this paper is to evaluate such relationship in open-source software projects, and to compare these metrics to other code and process metrics.MethodOn a newly crafted dataset of seven open-source software projects, we perform, using inferential statistics, an analysis of code ownership metrics and their relationship with software quality.ResultsWe confirm the existence of a relationship between code ownership and software quality, but the relative importance of ownership metrics in multiple linear regression models is low compared to metrics such as the number of lines of code, the number of modifications performed over the last release, or the number of developers of a module.ConclusionAlthough we do find a relationship between code ownership and software quality, the added value of ownership metrics compared to other metrics is still to be proven. 
64|-||Communication and personality profiles of global software developers|ContextPrior research has established that a small proportion of individuals dominate team communication during global software development. It is not known, however, how these members’ contributions affect their teams’ knowledge diffusion process, or whether their personality profiles are responsible for their dominant presence.ObjectiveWe set out to address this gap through the study of repository artifacts.MethodArtifacts from ten teams were mined from the IBM Rational Jazz repository. We employed social network analysis (SNA) to group practitioners into two clusters, Top Members and Others, based on the numbers of messages they communicated and their engagement in task changes. SNA metrics (density, in-degree and closeness) were then used to study practitioners’ importance in knowledge diffusion. Thereafter, we performed psycholinguistic analysis on practitioners’ messages using linguistic dimensions that had been previously correlated with the Big Five personality profiles.ResultsFor our sample of 146 practitioners we found that Top Members occupied critical roles in knowledge diffusion, and demonstrated more openness to experience than the Others. Additionally, all personality profiles were represented during teamwork, although openness to experience, agreeableness and extroversion were particularly evident. However, no specific personality predicted members’ involvement in knowledge diffusion.ConclusionTask assignment that promotes highly connected team communication networks may mitigate tacit knowledge loss in global software teams. Additionally, while members expressing openness to experience are likely to be particularly driven to perform, this is not entirely responsible for a global team’s success. 
64|-||Performance Alignment Work: How software developers experience the continuous adaptation of team performance in Lean and Agile environments|ContextCompanies increasingly strive to adapt to market and ecosystem changes in real time. Gauging and understanding team performance in such changing environments present a major challenge.ObjectiveThis paper aims to understand how software developers experience the continuous adaptation of performance in a modern, highly volatile environment using Lean and Agile software development methodology. This understanding can be used as a basis for guiding formation and maintenance of high-performing teams, to inform performance improvement initiatives, and to improve working conditions for software developers.MethodA qualitative multiple-case study using thematic interviews was conducted with 16 experienced practitioners in five organisations.ResultsWe generated a grounded theory, Performance Alignment Work, showing how software developers experience performance. We found 33 major categories of performance factors and relationships between the factors. A cross-case comparison revealed similarities and differences between different kinds and different sizes of organisations.ConclusionsBased on our study, software teams are engaged in a constant cycle of interpreting their own performance and negotiating its alignment with other stakeholders. While differences across organisational sizes exist, a common set of performance experiences is present despite differences in context variables. Enhancing performance experiences requires integration of soft factors, such as communication, team spirit, team identity, and values, into the overall development process. Our findings suggest a view of software development and software team performance that centres around behavioural and social sciences. 
65|-|http://www.sciencedirect.com/science/journal/09505849/65|An Integer Linear Programming approach to the single and bi-objective Next Release Problem|ContextThe Next Release Problem involves determining the set of requirements to implement in the next release of a software project. When the problem was first formulated in 2001, Integer Linear Programming, an exact method, was found to be impractical because of large execution times. Since then, the problem has mainly been addressed by employing metaheuristic techniques.ObjectiveIn this paper, we investigate if the single-objective and bi-objective Next Release Problem can be solved exactly and how to better approximate the results when exact resolution is costly.MethodsWe revisit Integer Linear Programming for the single-objective version of the problem. In addition, we integrate it within the Epsilon-constraint method to address the bi-objective problem. We also investigate how the Pareto front of the bi-objective problem can be approximated through an anytime deterministic Integer Linear Programming-based algorithm when results are required within strict runtime constraints. Comparisons are carried out against NSGA-II. Experiments are performed on a combination of synthetic and real-world datasets.FindingsWe show that a modern Integer Linear Programming solver is now a viable method for this problem. Large single objective instances and small bi-objective instances can be solved exactly very quickly. On large bi-objective instances, execution times can be significant when calculating the complete Pareto front. However, good approximations can be found effectively.ConclusionThis study suggests that (1) approximation algorithms can be discarded in favor of the exact method for the single-objective instances and small bi-objective instances, (2) the Integer Linear Programming-based approximate algorithm outperforms the NSGA-II genetic approach on large bi-objective instances, and (3) the run times for both methods are low enough to be used in real-world situations. 
65|-||How have we evaluated software pattern application? A systematic mapping study of research design practices|ContextSoftware patterns encapsulate expert knowledge for constructing successful solutions to recurring problems. Although a large collection of software patterns is available in literature, empirical evidence on how well various patterns help in problem solving is limited and inconclusive. The context of these empirical findings is also not well understood, limiting applicability and generalizability of the findings.ObjectiveTo characterize the research design of empirical studies exploring software pattern application involving human participants.MethodWe conducted a systematic mapping study to identify and analyze 30 primary empirical studies on software pattern application, including 24 original studies and 6 replications. We characterize the research design in terms of the questions researchers have explored and the context of empirical research efforts. We also classify the studies in terms of measures used for evaluation, and threats to validity considered during study design and execution.ResultsUse of software patterns in maintenance is the most commonly investigated theme, explored in 16 studies. Object-oriented design patterns are evaluated in 14 studies while 4 studies evaluate architectural patterns. We identified 10 different constructs with 31 associated measures used to evaluate software patterns. Measures for ‘efficiency’ and ‘usability’ are commonly used to evaluate the problem solving process. While measures for ‘completeness’, ‘correctness’ and ‘quality’ are commonly used to evaluate the final artifact. Overall, ‘time to complete a task’ is the most frequently used measure, employed in 15 studies to measure ‘efficiency’. For qualitative measures, studies do not report approaches for minimizing biases 27% of the time. Nine studies do not discuss any threats to validity.ConclusionSubtle differences in study design and execution can limit comparison of findings. Establishing baselines for participants’ experience level, providing appropriate training, standardizing problem sets, and employing commonly used measures to evaluate performance can support replication and comparison of results across studies. 
65|-||A complete approach for CIM modelling and model formalising|ContextComputation Independent Model (CIM) as a business model describes the requirements and environment of a business system and instructs the designing and development; it is a key to influencing software success. Although many studies currently focus on model driven development (MDD); those researches, to a large extent, study the PIM-level and PSM-level model, and few have dealt with CIM-level modelling for case in which the requirements are unclear or incomplete.ObjectiveThis paper proposes a CIM-level modelling approach, which applies a stepwise refinement approach to modelling the CIM-level model starting from a high-level goal model to a lower-level business process model. A key advantage of our approach is the combination of the requirement model with the business model, which helps software engineers to define business models exactly for cases in which the requirements are unclear or incomplete.MethodThis paper, based on the model driven approach, proposes a set of models at the CIM-level and model transformations to connect these models. Accordingly, the formalisation approach of this paper involves formalising the goal model using the category theory and the scenario model and business process model using Petri nets.ResultsWe have defined a set of metamodels and transformation rules making it possible to obtain automatically a scenario model from the goal model and a business process model from the scenario model. At the same time, we have defined a mapping rule to formalise these models. Our proposed CIM modelling approach and formalisation approach are implemented with an MDA tool, and it has been empirically validated by a travel agency case study.ConclusionThis study shows how a CIM modelling approach helps to build a complete and consistent model at the CIM level for cases in which the requirements are unclear or incomplete in advance. 
65|-||Investigating the penalty reward calculus of software users and its impact on requirements prioritization|ContextThe current requirements engineering techniques for prioritization of software requirements implicitly assume that each user requirement will have an independent and symmetric impact on user satisfaction. For example, it is assumed that implementing a high priority user requirement will positively impact his satisfaction and not implementing a high priority user requirement will negatively impact his satisfaction. Further, the impacts of implementing multiple user requirements on his satisfaction are expected to be additive. But is this always the case?ObjectiveThis paper empirically examines whether the assumptions of symmetric and multiplicative impacts of user requirements on his satisfaction are valid. Further, the study assesses the relative efficacy of 5 methods of requirements prioritization in managing these effects as reflected by the user satisfaction with the prioritized requirement sets.MethodTo test for existence and mitigation of asymmetric effects an adaptation of the widely accepted PRCA (Penalty Reward Contrast Analysis) method was used for 5 requirements prioritization techniques. To test for existence and mitigation of multiplicative effects MHMR (Moderated Hierarchical Multiple Regression) a well-accepted technique for testing interaction effects was used.ResultsBoth asymmetric and multiplicative effects of software requirements on user satisfaction were observed for requirements prioritized using all 5 requirements prioritization methods raising questions about the efficacy of present day requirements prioritization techniques. Further, the results of the experiment led to proposing a new method for requirements prioritization for managing these effects.ConclusionThe study empirically demonstrates the complexities of prioritizing software requirements and calls for a new generation of methods to address them. Understanding and resolving these complexities will enable software providers to conserve resources by enabling them to parsimoniously selecting only those requirements for implementation in the software product that have maximum incremental impact on user satisfaction. 
65|-||Test Governance Framework for contracted IS development: Ethnographically informed action research|ContextOver the past two decades, interest has increased in software development and testing outsourcing. Although the decision to outsource development or test processes is founded on various background motives, e.g., costs, capacity, time-to-market, etc., additional factors that influence the outsourcing relationships are frequently overlooked by client organizations.ObjectiveThe main objectives of this paper are to investigate the role of testing in contracted software development projects and examine the interactions between both parties during the process. A Test Governance Framework (TeGoF) was developed to propose an organization-wide but project-centred mechanism for control of the test process in contracted software development projects.MethodThe principles of design science were applied to develop the TeGoF. The principles of ethnographically informed action research were used to evaluate this management artefact in an industrial context and examine the impact of a particular organizational setting on the framework.ResultsOf a total of three projects, the TeGoF was applied smoothly in only one case. However, this observation should not be interpreted as a TeGoF deficiency because its primary goal is to define a powerful detection mechanism to cope with quality-related issues in contractual relationships. In this sense, the TeGoF proved itself as a feasible tool. Additionally, an analysis is presented that describes the factors that contributed to the implementation difficulties of the TeGoF principles in the two remaining projects. Finally, the relationships among trust, control and power are indicated as well as the potential influence of national, organizational and occupational culture.ConclusionThe main contribution of this paper consists in the development of the TeGoF as a tool that pinpoints significant limitations in the current research related to control issues in the domain of contracted software systems testing. Additionally, the authors analysed key factors that influence the success of the TeGoF in client organizations. 
65|-||Editorial of special section from Software Evolution Week 2014|
65|-||SQA-Mashup: A mashup framework for continuous integration|ContextContinuous Integration (CI) has become an established best practice of modern software development. Its philosophy of regularly integrating the changes of individual developers with the master code base saves the entire development team from descending into Integration Hell, a term coined in the field of extreme programming. In practice, CI is supported by automated tools to cope with this repeated integration of source code through automated builds and testing. One of the main problems, however, is that relevant information about the quality and health of a software system is both scattered across those tools and across multiple views.ObjectiveThis paper introduces a quality awareness framework for CI-data and its conceptional model used for the data integration and visualization. The framework called SQA-Mashup makes use of the service-based mashup paradigm and integrates information from the entire CI-toolchain into a single service.MethodThe research approach followed in our work consists out of (i) a conceptional model for data integration and visualization, (ii) a prototypical framework implementation based on tool requirements derived from literature, and (iii) a controlled user study to evaluate its usefulness.ResultsThe results of the controlled user study showed that SQA-Mashup’s single point of access allows users to answer questions regarding the state of a system more quickly (57%) and accurately (21.6%) than with standalone CI-tools.ConclusionsThe SQA-Mashup framework can serve as one-stop shop for software quality data monitoring in a software development project. It enables easy access to CI-data which otherwise is not integrated but scattered across multiple CI-tools. Our dynamic visualization approach allows for a tailoring of integrated CI-data according to information needs of different stakeholders such as developers or testers. 
65|-||Understanding the triaging and fixing processes of long lived bugs|ContextBug fixing is an integral part of software development and maintenance. A large number of bugs often indicate poor software quality, since buggy behavior not only causes failures that may be costly but also has a detrimental effect on the user’s overall experience with the software product. The impact of long lived bugs can be even more critical since experiencing the same bug version after version can be particularly frustrating for user. While there are many studies that investigate factors affecting bug fixing time for entire bug repositories, to the best of our knowledge, none of these studies investigates the extent and reasons of long lived bugs.ObjectiveIn this paper, we investigate the triaging and fixing processes of long lived bugs so that we can identify the reasons for delay and improve the overall bug fixing process.MethodologyWe mine the bug repositories of popular open source projects, and analyze long lived bugs from five different perspectives: their proportion, severity, assignment, reasons, as well as the nature of fixes.ResultsOur study on seven open-source projects shows that there are a considerable number of long lived bugs in each system and over 90% of them adversely affect the user’s experience. The reasons for these long lived bugs are diverse including long assignment time, not understanding their importance in advance, etc. However, many bug-fixes were delayed without any specific reasons. Furthermore, 40% of long lived bugs need only small fixes.ConclusionOur overall results suggest that a significant number of long lived bugs may be minimized through careful triaging and prioritization if developers could predict their severity, change effort, and change impact in advance. We believe our results will help both developers and researchers better to understand factors behind delays, improve the overall bug fixing process, and investigate analytical approaches for prioritizing bugs based on bug severity as well as expected bug fixing effort. 
65|-||How Java APIs break â An empirical study|ContextIt has become common practice to build programs by using libraries. While the benefits of reuse are well known, an often overlooked risk are system runtime failures due to API changes in libraries that evolve independently. Traditionally, the consistency between a program and the libraries it uses is checked at build time when the entire system is compiled and tested. However, the trend towards partially upgrading systems by redeploying only evolved library versions results in situations where these crucial verification steps are skipped. For Java programs, partial upgrades create additional interesting problems as the compiler and the virtual machine use different rule sets to enforce contracts between the providers and the consumers of APIs.ObjectiveWe have studied the extent of the problem in real world programs. We were interested in two aspects: the compatibility of API changes as libraries evolve, and the impact this has on programs using these libraries.MethodThis study is based on the qualitas corpus version 20120401. A data set consisting of 109 Java open-source programs and 564 program versions was used from this corpus. We have investigated two types of library dependencies: explicit dependencies to embedded libraries, and dependencies defined by symbolic references in Maven build files that are resolved at build time. We have used JaCC for API analysis, this tool is based on the popular ASM byte code analysis library.ResultsWe found that for most of the programs we investigated, APIs are unstable as incompatible changes are common. Surprisingly, there are more compatibility problems in projects that use automated dependency resolution. However, we found only a few cases where this has an actual impact on other programs using such an API.ConclusionIt is concluded that API instability is common and causes problems for programs using these APIs. Therefore, better tools and methods are needed to safeguard library evolution. 
65|-||Performance comparison of query-based techniques for anti-pattern detection|ContextProgram queries play an important role in several software evolution tasks like program comprehension, impact analysis, or the automated identification of anti-patterns for complex refactoring operations. A central artifact of these tasks is the reverse engineered program model built up from the source code (usually an Abstract Semantic Graph, ASG), which is traditionally post-processed by dedicated, hand-coded queries.ObjectiveOur paper investigates the costs and benefits of using the popular industrial Eclipse Modeling Framework (EMF) as an underlying representation of program models processed by four different general-purpose model query techniques based on native Java code, OCL evaluation and (incremental) graph pattern matching.MethodWe provide in-depth comparison of these techniques on the source code of 28 Java projects using anti-pattern queries taken from refactoring operations in different usage profiles.ResultsOur results show that general purpose model queries can outperform hand-coded queries by 2–3 orders of magnitude, with the trade-off of an increased in memory consumption and model load time of up to an order of magnitude.ConclusionThe measurement results of usage profiles can be used as guidelines for selecting the appropriate query technologies in concrete scenarios. 
66|-|http://www.sciencedirect.com/science/journal/09505849/66|MSR4SM: Using topic models to effectively mining software repositories for software maintenance tasks|ContextMining software repositories has emerged as a research direction over the past decade, achieving substantial success in both research and practice to support various software maintenance tasks. Software repositories include bug repository, communication archives, source control repository, etc. When using these repositories to support software maintenance, inclusion of irrelevant information in each repository can lead to decreased effectiveness or even wrong results.ObjectiveThis article aims at selecting the relevant information from each of the repositories to improve effectiveness of software maintenance tasks.MethodFor a maintenance task at hand, maintainers need to implement the maintenance request on the current system. In this article, we propose an approach, MSR4SM, to extract the relevant information from each software repository based on the maintenance request and the current system. That is, if the information in a software repository is relevant to either the maintenance request or the current system, this information should be included to perform the current maintenance task. MSR4SM uses the topic model to extract the topics from these software repositories. Then, relevant information in each software repository is extracted based on the topics.ResultsMSR4SM is evaluated for two software maintenance tasks, feature location and change impact analysis, which are based on four subject systems, namely jEdit, ArgoUML, Rhino and KOffice. The empirical results show that the effectiveness of traditional software repositories based maintenance tasks can be greatly improved by MSR4SM.ConclusionsThere is a lot of irrelevant information in software repositories. Before we use them to implement a maintenance task at hand, we need to preprocess them. Then, the effectiveness of the software maintenance tasks can be improved. 
66|-||Achievement of minimized combinatorial test suite for configuration-aware software functional testing using the Cuckoo Search algorithm|ContextSoftware has become an innovative solution nowadays for many applications and methods in science and engineering. Ensuring the quality and correctness of software is challenging because each program has different configurations and input domains. To ensure the quality of software, all possible configurations and input combinations need to be evaluated against their expected outputs. However, this exhaustive test is impractical because of time and resource constraints due to the large domain of input and configurations. Thus, different sampling techniques have been used to sample these input domains and configurations.ObjectiveCombinatorial testing can be used to effectively detect faults in software-under-test. This technique uses combinatorial optimization concepts to systematically minimize the number of test cases by considering the combinations of inputs. This paper proposes a new strategy to generate combinatorial test suite by using Cuckoo Search concepts.MethodCuckoo Search is used in the design and implementation of a strategy to construct optimized combinatorial sets. The strategy consists of different algorithms for construction. These algorithms are combined to serve the Cuckoo Search.ResultsThe efficiency and performance of the new technique were proven through different experiment sets. The effectiveness of the strategy is assessed by applying the generated test suites on a real-world case study for the purpose of functional testing.ConclusionResults show that the generated test suites can detect faults effectively. In addition, the strategy also opens a new direction for the application of Cuckoo Search in the context of software engineering. 
66|-||Conflict resolution effectiveness on the implementation efficiency and achievement of business objectives in IT programs: A study of IT vendors|ContextThe information technology (IT) field presents a unique context for the management of multiple projects because of the variety of stakeholders involved, the complexity of interdependencies among projects, and the frequent use of external vendors. In practice, IT vendors typically employ advanced project governance techniques such as program management to work effectively with the numbers and variety of clients while still pursuing the benefits of a single oversight. These structural features lend themselves to conflict across teams with individual requirements. However, little research exists on program management, much less in the IT context, that represents conflict across IT project teams.ObjectiveIn this study, the effectiveness of conflict resolution on the implementation efficiency and fulfillment of business objectives is studied through the lens of constructive controversy theories. A number of hypotheses are derived by tailoring the constructive conflict resolution concepts to IT context and making a comprehensive literature review to identify the mediator and dependent variables. A model is developed to consider the management of conflict across multiple projects combined into a single program.MethodA quantitative questionnaire related to the program environment was developed for five variables to include conflict resolution, cognition-based trust, interpersonal cooperation, business objectives and implementation efficiency. The hypotheses were tested by performing a survey study, where a number of well-established measures in the literature were used. 92 paired responses from program teams in 38 organizations located in India were obtained and represent a variety of individual characteristics, and program sizes.ResultsThis study identified the composite role of constructive conflict resolution and cognition-based trust in improving interpersonal cooperation. The impacts of constructive conflict resolution on business objectives were not fully mediated by cognition-based trust and interpersonal cooperation, although implementation efficiency is fully mediated.ConclusionThe management of conflict promotes trust and interpersonal cooperation necessary to improve the efficient completion of the program and benefits to the organization. 
66|-||Goal-oriented dynamic test generation|ContextMemory safety errors such as buffer overflow vulnerabilities are one of the most serious classes of security threats. Detecting and removing such security errors are important tasks of software testing for improving the quality and reliability of software in practice.ObjectiveThis paper presents a goal-oriented testing approach for effectively and efficiently exploring security vulnerability errors. A goal is a potential safety violation and the testing approach is to automatically generate test inputs to uncover the violation.MethodWe use type inference analysis to diagnose potential safety violations and dynamic symbolic execution to perform test input generation. A major challenge facing dynamic symbolic execution in such application is the combinatorial explosion of the path space. To address this fundamental scalability issue, we employ data dependence analysis to identify a root cause leading to the execution of the goal and propose a path exploration algorithm to guide dynamic symbolic execution for effectively discovering the goal.ResultsTo evaluate the effectiveness of our proposed approach, we conducted experiments against 23 buffer overflow vulnerabilities. We observed a significant improvement of our proposed algorithm over two widely adopted search algorithms. Specifically, our algorithm discovered security vulnerability errors within a matter of a few seconds, whereas the two baseline algorithms failed even after 30 min of testing on a number of test subjects.ConclusionThe experimental results highlight the potential of utilizing data dependence analysis to address the combinatorial path space explosion issue faced by dynamic symbolic execution for effective security testing. 
66|-||Quantitative analysis of fault density in design patterns: An empirical study|ContextThere are many claimed advantages for the use of design patterns and their impact on software quality. However, there is no enough empirical evidence that supports these claimed benefits and some studies have found contrary results.ObjectiveThis empirical study aims to quantitatively measure and compare the fault density of motifs of design patterns in object-oriented systems at different levels: design level, category level, motif level, and role level.MethodAn empirical study was conducted that involved five open-source software systems. Data were analyzed using appropriate statistical test of significance differences.ResultsThere is no consistent difference in fault density between classes that participate in design motifs and non-participant classes. However, classes that participate in structural design motifs tend to be less fault-dense. For creational design motifs, it was found that there is no clear tendency for the difference in fault density. For behavioral design motifs, it was found that there is no significant difference between participant classes and non-participant classes. We observed associations between five design motifs (Builder, Factory Method, Adapter, Composite and Decorator) and fault density. At the role level, we found that only one pair of roles (Adapter vs. Client) shows a significant difference in fault density.ConclusionThere is no clear tendency for the difference in fault density between participant and non-participant classes in design motifs. However, structural design motifs have a negative association with fault density. The Builder design motif has a positive association with fault density whilst the Factory Method, Adapter, Composite, and Decorator design motifs have negative associations with fault density. Classes that participate in the Adapter role are less dense in faults than classes that participate in the Client role. 
66|-||A scientific evaluation of the misuse case diagrams visual syntax|ContextMisuse case modeling is a well-known technique in the domain of capturing and specifying functional security requirements. Misuse case modeling provides a mechanism for security analysts to consider and account for security requirements in the early stages of a development process instead of relying on generic defensive mechanisms that are augmented to software systems towards the latter stages of development.ObjectiveMany research contributions in the area of misuse case modeling have been devoted to extending the notation to increase its coverage of additional security related semantics. However, there lacks research that evaluates the perception of misuse case models by its readers. A misread or misinterpreted misuse case model can have dire consequences downstream leading to the development of an insecure system.MethodThis paper presents an assessment of the design of the original misuse case modeling notation based on the Physics of Notations framework. A number of improvements to the notation were suggested. A survey and a controlled experiment were carried out to compare the cognitive effectiveness of the new notation in comparison to the original notation.ResultsThe survey had 55 participants for have mostly indicated that the new notation is more semantically transparent than the original notation. The results of the experiment show that subjects reading diagrams developed using the new notation performed their tasks an average 6 min quicker, while in general the subjects performed their tasks in approximately 14.5 min. The experimental tasks only required subjects reading diagrams and not creating them.ConclusionThe main finding of this paper is that the use of colors and icons has improved the readability of misuse case diagrams. Software engineering notations are usually black and white. It is expected that the readability of other software notations will improve if they utilize colors and icons. 
67|-|http://www.sciencedirect.com/science/journal/09505849/67|Comparative case studies of open source software peer review practices|ContextThe power of open source software peer review lies in the involvement of virtual communities, especially users who typically do not have a formal role in the development process. As communities grow to a certain extent, how to organize and support the peer review process becomes increasingly challenging. A universal solution is likely to fail for communities with varying characteristics.ObjectiveThis paper investigates differences of peer review practices across different open source software communities, especially the ones engage distinct types of users, in order to offer contextualized guidance for developing open source software projects.MethodComparative case studies were conducted in two well-established large open source communities, Mozilla and Python, which engage extremely different types of users. Bug reports from their bug tracking systems were examined primarily, complemented by secondary sources such as meeting notes, blog posts, messages from mailing lists, and online documentations.ResultsThe two communities differ in the key activities of peer review processes, including different characteristics with respect to bug reporting, design decision making, to patch development and review. Their variances also involve the designs of supporting technology. The results highlight the emerging role of triagers, who bridge the core and peripheral contributors and facilitate the peer review process. The two communities demonstrate alternative designs of open source software peer review and their tradeoffs were discussed.ConclusionIt is concluded that contextualized designs of social and technological solutions to open source software peer review practices are important. The two cases can serve as learning resources for open source software projects, or other types of large software projects in general, to cope with challenges of leveraging enormous contributions and coordinating core developers. It is also important to improve support for triagers, who have not received much research effort yet. 
67|-||Enhancing mirror adaptive random testing through dynamic partitioning|ContextAdaptive random testing (ART), originally proposed as an enhancement of random testing, is often criticized for the high computation overhead of many ART algorithms. Mirror ART (MART) is a novel approach that can be generally applied to improve the efficiency of various ART algorithms based on the combination of “divide-and-conquer” and “heuristic” strategies.ObjectiveThe computation overhead of the existing MART methods is actually on the same order of magnitude as that of the original ART algorithms. In this paper, we aim to further decrease the order of computation overhead for MART.MethodWe conjecture that the mirroring scheme in MART should be dynamic instead of static to deliver a higher efficiency. We thus propose a new approach, namely dynamic mirror ART (DMART), which incrementally partitions the input domain and adopts new mirror functions.ResultsOur simulations demonstrate that the new DMART approach delivers comparable failure-detection effectiveness as the original MART and ART algorithms while having much lower computation overhead. The experimental studies further show that the new approach also delivers a better and more reliable performance on programs with failure-unrelated parameters.ConclusionIn general, DMART is much more cost-effective than MART. Since its mirroring scheme is independent of concrete ART algorithms, DMART can be generally applied to improve the cost-effectiveness of various ART algorithms. 
67|-||On organisational influences in software standards and their open source implementations|ContextIt is widely acknowledged that standards implemented in open source software can reduce risks for lock-in, improve interoperability, and promote competition on the market. However, there is limited knowledge concerning the relationship between standards and their implementations in open source software. This paper reports from an investigation of organisational influences in software standards and open source software implementations of software standards. The study focuses on the RDFa standard and its implementation in the Drupal project.ObjectiveThe overarching goal of the study is to establish organisational influences in software standards and their implementations in open source software. More specifically, our objective is to establish organisational influences in the RDFa standard and its implementation in the Drupal project.MethodBy conduct of a case study of the RDFa standard and its implementation in the Drupal project we investigate organisational influences in software standards and their implementations in open source software. Specifically, the case study involved quantitative analyses of issue tracker data for different issue trackers for W3C RDFa and the Drupal implementation of RDFa.ResultsThe case study provides details on how and to what extent organisational influences occur in W3C RDFa and the Drupal implementation of RDFa, by specifically providing a characterisation of issues and results concerning contribution to issue raising and commenting, organisational involvement over time, and individual and organisational collaboration on issues.ConclusionWe find that widely deployed standards can benefit from contributions provided by a range of different individuals, organisations, and types of organisations either directly to a standardisation project or indirectly via an open source project implementing the standard. Further, we also find that open processes for standardisation adopted by W3C may also contribute to open source projects implementing specific standards. 
67|-||Requirements communication and balancing in large-scale software-intensive product development|ContextSeveral industries developing products on a large-scale are facing major challenges as their products are becoming more and more software-intensive. Whereas software was once considered a detail to be bundled, it has since become an intricate and interdependent part of most products. The advancement of software increases the uncertainty and the interdependencies between development tasks and artifacts. A key success factor is good requirements engineering (RE), and in particular, the challenges of effectively and efficiently coordinating and communicating requirements.ObjectiveIn this work we present a lightweight RE framework and demonstrate and evaluate its industrial applicability in response to the needs of a Swedish automotive company for improving specific problems in inter-departmental requirements coordination and communication in large-scale development of software-intensive systems.MethodA case study approach and a dynamic validation were used to develop and evaluate the framework in close collaboration with our industrial partner, involving three real-life cases in an ongoing car project. Experience and feedback were collected through observations when applying the framework and from 10 senior industry professionals in a questionnaire and in-depth follow-up interviews.ResultsThe experience and feedback about using the framework revealed that it is relevant and applicable for the industry as well as a useful and efficient way to resolve real problems in coordinating and communicating requirements identified at the case company. However, other concerns, such as accessibility to necessary resources and competences in the early development phases, were identified when using the method, which allowed for earlier pre-emptive action to be taken.ConclusionOverall, the experience from using the framework and the positive feedback from industry professionals indicated a feasible framework that is applicable in the industry for improving problems related to coordination and communication of requirements. Based on the promising results, our industrial partner has decided upon further validations of the framework in a large-scale pilot program. 
67|-||Automatically propagating changes from reference implementations to code generation templates|ContextCode generators can automatically perform some tedious and error-prone implementation tasks, increasing productivity and quality in the software development process. Most code generators are based on templates, which are fundamentally composed of text expansion statements. To build templates, the code of an existing, tested and validated implementation may serve as reference, in a process known as templatization. With the dynamics of software evolution/maintenance and the need for performing changes in the code generation templates, there is a loss of synchronism between the templates and this reference code. Additional effort is required to keep them synchronized.ObjectiveThis paper proposes automation as a way to reduce the extra effort needed to keep templates and reference code synchronized.MethodA mechanism was developed to semi-automatically detect and propagate changes from reference code to templates, keeping them synchronized with less effort. The mechanism was also submitted to an empirical evaluation to analyze its effects in terms of effort reduction during maintenance/evolution templatization.ResultsIt was observed that the developed mechanism can lead to a 50% reduction in the effort needed to perform maintenance/evolution templatization, when compared to a manual approach. It was also observed that this effect depends on the nature of the evolution/maintenance task, since for one of the tasks there was no observable advantage in using the mechanism. However, further studies are needed to better characterize these tasks.ConclusionAlthough there is still room for improvement, the results indicate that automation can be used to reduce effort and cost in the maintenance and evolution of a template-based code generation infrastructure. 
67|-||A systematic literature review on the usage of eye-tracking in software engineering|ContextEye-tracking is a mean to collect evidence regarding some participants’ cognitive processes. Eye-trackers monitor participants’ visual attention by collecting eye-movement data. These data are useful to get insights into participants’ cognitive processes during reasoning tasks.ObjectiveThe Evidence-based Software Engineering (EBSE) paradigm has been proposed in 2004 and, since then, has been used to provide detailed insights regarding different topics in software engineering research and practice. Systematic Literature Reviews (SLR) are also useful in the context of EBSE by bringing together all existing evidence of research and results about a particular topic. This SLR evaluates the current state of the art of using eye-trackers in software engineering and provides evidence on the uses and contributions of eye-trackers to empirical studies in software engineering.MethodWe perform a SLR covering eye-tracking studies in software engineering published from 1990 up to the end of 2014. To search all recognised resources, instead of applying manual search, we perform an extensive automated search using Engineering Village. We identify 36 relevant publications, including nine journal papers, two workshop papers, and 25 conference papers.ResultsThe software engineering community started using eye-trackers in the 1990s and they have become increasingly recognised as useful tools to conduct empirical studies from 2006. We observe that researchers use eye-trackers to study model comprehension, code comprehension, debugging, collaborative interaction, and traceability. Moreover, we find that studies use different metrics based on eye-movement data to obtain quantitative measures. We also report the limitations of current eye-tracking technology, which threaten the validity of previous studies, along with suggestions to mitigate these limitations.ConclusionHowever, not withstanding these limitations and threats, we conclude that the advent of new eye-trackers makes the use of these tools easier and less obtrusive and that the software engineering community could benefit more from this technology. 
