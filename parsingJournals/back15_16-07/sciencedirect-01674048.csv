volume|issue|url|title|abstract
19|-|http://www.sciencedirect.com/science/journal/01674048/19|From The Editor|
19|-||From Stones in the Street to Cyberspace â Israeli and Palestinians Continue Their Virtual Battles 24Ã7|
19|-||Feeling Sorry for Microsoft?|
19|-||However, Microsoft Could Use Some Sympathy â Dutch Hacker Busts Microsoft Web Site - Again|
19|-||Wireless Big Brother|
19|-||NASA Hacker Pleads Guilty|
19|-||Canadian Teen Mafiaboy Pleads Guilty|
19|-||Getting the Laws to Help Combat Cybercrime (Thereâs a grand idea)|
19|-||Internet Biz Group Calls for Cybercrime Treaty Delay|
19|-||Laptop Theft Now Targeted Towards Data and Not Necessarily Hardware|
19|-||Hacktivism Increasing|
19|-||US Department of Defense Prepares Cybercrime Database|
19|-||CIA Chat Room Causes Grief|
19|-||Japan Creates Cyberconditions for IT National Revolution|
19|-||US Privacy Law Proposals Cause Concerns|
19|-||New Names For Old â A Personal Surf Through Compsec 2000|
19|-||Americaâs Internet Commerce and The Threat of Fraud|
19|-||The Software Colander â Holes in Messaging|
19|-||Information Security Management: An Approach to Combine Process Certification And Product Evaluation|Information Security (IS) is the key to the effective management of any organisation in today’s commercial and industrial sectors. Line managers’ performance, for instance, is rated according to the extent to which their operations conform to the IS policies of their respective organizations. In the same way, senior management’s performance is judged by how well the organization performs in terms of internationally accepted codes of IS practice. IS management, however, is not always a quantifiable entity and its evaluation is complicated by the fact that it can be viewed either from an electronic perspective, in which case the focus will fall solely on product and/or systems evaluation, or from a procedural and management perspective, in which case the focus will, instead, fall on the certification of the IS management process. This article will, therefore, be devoted to providing a consolidated approach to the evaluation of IS management, in terms of which full cognisance will be taken of both these perspectives. 
19|-||Opening Up The Enterprise|The future of business is E-business. To enable this strategy to succeed we shall need an entirely new paradigm for securing our enterprise applications and infrastructure. We can no longer sustain the approach of a hardened perimeter and we shall need to open up the enterprise. This paper addresses the security architecture and technical strategies that we shall need. 
19|-||Abstracts of Recent Articles and Literature|
19|-||Securing The Electronic Market: The KEYSTONE Public Key Infrastructure Architecture|In this paper, the unified, abstract KEYSTONE Public Key Infrastructure is presented. This architecture consists of a reference model, a functional architecture specification, and a set of technologies that can be used for implementing the functional units, along with all relevant standards. It was derived within the course of the KEYSTONE project, which was funded by the European Commission under the Electronic Trust Services II Programme. The proposed PKI architecture guarantees openness, scalability, flexibility, extensibility, integration with existing TTP and information infrastructure, transparency and, above all, security. Thus, it enjoys all the desirable characteristics and fulfils all those criteria that are essential for a PKI to constitute a successful framework for the development of inter-domain and international Trusted Services. 
19|1|http://www.sciencedirect.com/science/journal/01674048/19/1|From The Editor|
19|1||Security Views|
19|1||Key Recovery Alliance (KRA) Technology Papers, Special Issue â Introduction|
19|1||Cryptographic Information Recovery Using Key Recovery|A note to readers: the authors consider that all techniques for key recovery may be viewed as having a position on a broad continuum. The only way to avoid misunderstanding is to identify particular techniques by listing their specific characteristics, rather than using multiply-defined terms. Different characteristics have advantages in different environments, so there is no ‘best’ key recovery technique. The paper notes some typical advantages and disadvantages of several techniques but should not be construed as an endorsement of any particular technique relative to another. Similarly, the authors recognize that terminology may vary from country to country.Cryptographic information recovery techniques provide for the recovery of plaintext from encrypted data. This (exceptional) need arises when the cryptographic keys involved are not available. For example, data files may have been encrypted using a key derived from a now forgotten or misplaced password. Overlapping and confusing terminology has been applied to the techniques of information recovery, including key escrow, key backup, key recovery, and trusted third party (ttp), all of which refer to methods for retrieving, recovering, or re-constructing keys. Even the underlying concept of ‘trust’ has broad meaning. Instead of attempting to ‘define’ these terms precisely, a continuum of functionality is defined. Several generic technologies, together with desirable characteristics of cryptographic information/key recovery techniques, are described. 
19|1||Features, Attributes, Characteristics, and Traits (FACTs) of Key Recovery Schemes/Products|
19|1||Key Recovery Functional Model|This document describes a model for key recovery. The means by which plaintext may be recovered or reconstructed from recovered key(s) is not addressed in the key recovery model. The key recovery model is a generalized model that encompasses a wide variety of key recovery systems, including both key backup and encapsulated key recovery information techniques. The key recovery model is a functional model. That is, the components of the model are functional components not system components. 
19|1||Additional Key Recovery Functions|This document describes additional functions that may be required by specific implementations of the key recovery model. This document should not be read in isolation, but must be read in conjunction with the Key Recovery Functional Model document. 
19|1||A Common Key Recovery Block Format: Promoting Interoperability Between Dissimilar Key Recovery Mechanisms|When cryptographic products that employ different key recovery mechanisms need to interoperate with one another, one of the major obstacles is the inability of the decryptor product to recognize, and optionally validate, the key recovery information generated by the encryptor product. In this paper, a common Key Recovery Block (KRB) format is being proposed to facilitate interoperability between heterogeneous key recovery systems. The KRB serves as a container for mechanism-specific key recovery information, and supports techniques to identify and optionally validate the contained key recovery information. This specification provides an extensible set of KRB validation techniques - however, it makes no attempt to set a preference for one technique over the others. The choice of validation technique(s) used is determined by the policies (with respect to the use of cryptography and key recovery) that apply to the encryptor and decryptor products. It may be noted that the KRB format specification is independent of the encryption algorithm used to protect the confidentiality of the data, and independent of the communication or storage protocol used to carry the encrypted data.It should also be recognized that the KRB format proposed in this paper is of limited scope. It assumes that the key recovery information can be made available to interested parties along with the encrypted data. There are a number of open issues regarding the techniques that allow key recovery information to be associated with encrypted data (whether the key recovery information and the encrypted data are transmitted over the same channel or separate channels) — these issues are beyond the scope of this proposal. 
19|1||Global Interoperability for Key Recovery|This document describes techniques for implementing key recovery. The document then addresses interoperability between encryptors and decryptors using the key encapsulation method of key recovery. The Common Key Recovery Block is presented as a cornerstone for interoperability. The interoperability is summarized in an interoperability matrix for key recovery systems. 
19|1||Public Key Infrastructure: Analysis of Existing and Needed Protocols and Object Formats for Key Recovery|This document identifies the impacts to a public key infrastructure (PKI) created by the presence of key recovery, as well as elements in the public key infrastructure needed to support or accommodate key recovery. The key recovery infrastructure includes key recovery techniques based both on (1) encapsulation of key recovery information and (2) key backup. The data applications covered by the study include interactive communication sessions, store-and-forward communications, and data storage. The document identifies and lists elements of existing PKI that support or accommodate key recovery, and it identifies and lists needed changes to PKI to support or accommodate key recovery. 
19|1||Organization Implementation Guidelines for Recovery of Encrypted Information|
19|1||Organization Considerations for Retrieval of Stored Data via Key Recovery Methods|The purpose of this document is to identify technical and procedural issues that need to be considered when selecting and deploying key recovery systems to enable emergency recovery of data that is stored or archived in encrypted form. Emergency recovery of encrypted data is necessary in the event that the decryption keys are unavailable through normal key management mechanisms. In many cases, the need to support emergency recovery of encrypted data is vital to ongoing operations and jurisdictional policy compliance. A disaster recovery plan may also give consideration to an ability to recover many data keys by using a single key recovery request. 
19|1||Key Recovery Header for IPSEC|This document describes a mechanism for transmitting a key recovery header (KRH) within an IPSEC datagram. The KRH is normally inserted following the AH header and before the ESP header. The Key Recovery Header was designed to be compatible with IETF’s IPSEC Architecture. 
19|1||ISAKMP Key Recovery Extensions|This document describes the proposed approach for negotiating and exchanging key recovery information within the Internet Security Association Key Management Protocol (ISAKMP). 
19|1||SSL/TLS Protocol Enablement for Key Recovery|This document describes an approach for modifying the SSL/TLS (TLS) protocol to allow transmission of Key Recovery Block (KRBs) before communication takes place between clients and servers using strong encryption techniques. From a TLS perspective “strong encryption” implies use of a session key with the secret portion being larger than 40 bits. 
19|2|http://www.sciencedirect.com/science/journal/01674048/19/2|From The Editor|
19|2||Security Views|
19|2||New Millennium, Old Failures|
19|2||Crimes And Misdemeanours: How to Protect Corporate Information in the Internet Age|
19|2||Internet Targets|This article discusses the assets typically targeted by Internet robbers. It also provides a brief overview of the increased importance of ‘intangible’ information assets to the knowledge-based businesses and government agencies of the Information Age and some of the ways that the global adoption of Internet technology brings increased risk to these assets. 
19|2||Telecommunications Crime â Part 3|The previous part of this series which appeared in Vol. 18 No. 8, dealt nusiance and malicious attacks, phreaking and dial through fraud. The series now concludes with a look at mobile phone fraud and some of the sharp practice associated with telecommunications crime. 
19|2||Security For Remote Access And Mobile Applications|
19|2||Abstracts of Recent Articles and Literature|
19|2||Author Index for Volume 18|
19|2||Subject Index for Volume 18|
19|2||A Formalized Approach to the Effective Selection and Evaluation of Information Security Controls|Electronic commerce holds many advantages for the commercial world, but before it can really take off, the associated information security problems need to be addressed satisfactorily. The identification, implementation and management of the most effective set of controls to provide an adequate level of security is the first step towards this goal. The second step is the possible evaluation and certification of the installed controls in an IT-environment.The selection of the security controls should be driven by the business needs and the associated security requirement. This security requirement should be clearly defined in the information security policy and the security policy should dictate the set of controls that will provide the required protection. If this set of controls can be evaluated and certified as meeting the business needs of the organization, the trust that is required for electronic commerce can be provided. This paper will provide a formalized approach towards identifying a set of controls meeting the business needs and also suggest a model whereby this can be evaluated and certified. 
19|3|http://www.sciencedirect.com/science/journal/01674048/19/3|From The Editor|
19|3||Security Views|
19|3||Life Was Simple Then|
19|3||Implementing Public Key Infrastructures in a Dynamic Business Environment|
19|3||Information Systems Risk Management: Key Concepts and Business Processes|Information systems risk management is as a problem area extremely wide, complex and of an interdisciplinary nature, which highlights the importance of having an adequate understanding of the many concepts that are included in the area. Dealing with definitions of those concepts is a somewhat ‘boring’ task, however probably it can be considered to be an important one. In the second part of this article my attempt is to move to perhaps more ‘exciting’ things, namely to highlight the significant importance that business processes and internal controls have in IS risk management. 
19|3||Information Security Management: A Hierarchical Framework for Various Approaches|The present article is aimed at clarifying the oft-times confusing terminology and at elucidating the various approaches obtaining to the realm of Information Security (IS) management. The IS management approaches selected for discussion in this article will specifically address those rudiments and concepts that play a key role in the assessment of the IS status of an organization. Following, a hierarchical framework will be developed in terms of which to elucidate ill-defined terms and concepts. By so doing, issues such as certification, benchmarking, guidelines and codes of practice will come under consideration. IS management approaches widely accepted in the international arena will also be mapped onto the said hierarchical framework. 
19|3||Abstracts of Recent Articles and Literature|
19|3||Achieving Interoperability in a Multiple-Security- Policies Environment|The interoperability problems that emerge when information systems cooperate, are often attributed to incompatible security policies. In this paper, we introduce a systemic framework for achieving interoperability when multiple security policies are employed. First, we present a Metapolicy Development System (MDS) for the resolution of interoperability problems caused by incompatible security policies. Then we provide a policy framework and a metapolicy framework to serve as conceptual devices in the application of the MDS. Finally, we examine the possibility of developing software tools to support the MDS. We argue that a policy repository may serve as the basic component of a software tool for the management of multiple security policies and the application of the MDS. The policy repository is implemented in Telos, an object-oriented knowledge representation language. 
19|3||Generation of RSA Keys That Are Guaranteed to be Unique for Each User|This paper describes a method for assuring that the RSA primes generated by two different parties will always be different. While this condition is not required for the RSA signatures and encryption, this user-dependence of RSA primes is highly desirable in view of possible disputes around the authenticity of a digital signature. A cheating party may claim that a pair of prime numbers was generated by someone else. Our method will eliminate such possibility and will make the entire class of attacks obsolete. In addition to being different when different users derive them, the RSA primes generated according to the algorithm described in this paper will satisfy all of the requirements imposed by the national and international standards. 
19|4|http://www.sciencedirect.com/science/journal/01674048/19/4|From The Editor|
19|4||Suspected Hackers Arrested in Russian Credit-Card Fraud|
19|4||Intel Eliminates ID from New Chips|
19|4||European Parliament Doesnât Like Anonymity Online|
19|4||DOD Finds Plans Online â Reserve Unit Assesses Risk|
19|4||Agencies Are âOwn Worst Enemyâ|
19|4||US Department of Energy Security Criticized â Again|
19|4||This is What Happens When You Lose Your Computer With Classified Data On Itâ¦|
19|4||Not Everyone Wants PKI â NSF Opts for Digital Signature Alternative|
19|4||Justice Department Conducting Criminal Probe in Former CIA Director Activities|
19|4||Apache Site Defaced|
19|4||US and Europe Cybercrime Agreement Problems|
19|4||Cyberstalking on the Rise|
19|4||Large Child Pornography Ring Busted in Texas|
19|4||New Denial of Service Attack on Internet|
19|4||US Supreme Court Confirms ISPs Not Liable in E-mail Messages|
19|4||Software Scam â 17 Indicted|
19|4||Smurfing, Swamping, Spamming, Spoofing, Squatting, Slandering, Surfing, Scamming and Other Mischiefs of the World Wide Web|
19|4||Security in a Mobile World â Is Bluetooth the Answer?|Richard Barber, group technical advisor of Articon-Integralis looks, at the evolution of tele- and data-communications systems and assesses the potential advantages for mobile security offered by technologies like Bluetooth 
19|4||Netspionage â The Global Threat to Information, Part I: What is it and Why I Should Care?10|Those responsible for the protection of information systems and the information that they store, process and transmit have enough challenges in trying to provide massive amounts of information to internal and external customers in a secure manner. Now, there is a new, ever-increasing threat and it is global in nature. That threat is Netspionage (Network-enabled espionage). 
19|4||Implementing Information Security In The 21st Century â Do You Have the Balancing Factors?|
19|4||B2C Security â Be Just Secure Enough|
19|4||Letter To The Editor|
19|4||Abstracts of Recent Articles and Literature|
19|4||Multiparty Biometric-Based Authentication|A multiparty biometric-based access control might be desirable, or even required, in some applications. That is, a group of individuals might have to present valid readings of biometrics parameters in order to gain access to a system or application in order to receive authorization to use a particular resource. Such multiparty schemes can often provide greater protection. This paper presents several techniques for multiparty biometric-based authentication. It also shows a technique for biometric substitution which allows different biometrics to be used concurrently or substituted over time. 
19|5|http://www.sciencedirect.com/science/journal/01674048/19/5|From The Editor|
19|5||Security Views|
19|5||âI Agreeâ Legal Agreements Get Legislated|
19|5||The US Government Tells Its Users to Clean Up Their E-mail|
19|5||Home PCâs Targeted by Hackers (Duh!)|
19|5||CIOâs Get Serious About Best Security Practices|
19|5||Yahoo Faces Identity Crisis|
19|5||Microsoft a Popular Virus Target Due to Ubiquity|
19|5||UKâs RIP Bill Creates Official Snooping|
19|5||Extortion Going Online|
19|5||Office 2000 Patch Recommended by CERT|
19|5||Self-Regulation for Privacy Doesnât Work Says FTC|
19|5||â¦But the Industry Wants Self-Regulation|
19|5||US Senate Wakes Up to Need for Cybertools for Law Enforcers|
19|5||PGP V5.0 Keys Potentially Insecure|
19|5||Canadian Credit Card Conundrum Caused by a Cracker|
19|5||Suspect Charged in âLove Bugâ Worm has Charges Dropped|
19|5||Network Solutions Re-evaluates Security|
19|5||CMU Sets Up a Sister to CERT|
19|5||Speedy Response Yields Success at DOE|
19|5||AOL Gets Busted â Again|
19|5||Whoâs to Blame?|
19|5||Wireless Ubiquity for E-Biz|
19|5||And Now, the Bug-in-Your-Site Award Goes Toâ¦|
19|5||Isnât It Interesting What You Can Buy At An Auction Site? TVâs, Computers, Drugsâ¦|
19|5||G8 Thinks About Cybercrime (Itâs About Time, Too)|
19|5||Love Conquers All?|
19|5||Netspionage â The Global Threat to Information, Part II: Information Collection in the Gray Zone|
19|5||A Biometric Standard for Information Management and Security|Today, biometric systems are being widely developed and deployed to provide greater security to users and there is an increased awareness of the value of biometric systems. Biometric systems are being developed and deployed, users are gaining experience and confidence in biometric systems and are beginning to reap the benefits of this technology. Users and developers of this technology have also recognized the need for a biometric standard and work on a defining standard is currently underway. The standard establishes an appropriate biometric model and the associated security requirements that will allow different biometric solutions to co-exist in the marketplace. The standard views biometric systems within a global user community and it assures that the security of any one biometric system will be unaffected by the security of any other biometric system. This paper argues that integrity of authentication data is the primary security requirement and that confidentiality is secondary, even though the majority of authentication schemes today encrypt PINs and passwords. 
19|5||Abstracts of Recent Articles and Literature|
19|5||A Generic Electronic Payment Model Supporting Multiple Merchant Transactions|This paper presents a generic electronic payment model which is intended for a multi-merchant transaction, in which more than one merchant (the retailer, multiple resellers and content providers) are involved in the distribution of electronic contents. With our payment model, every involved merchant will be notified to verify the license information of a content on the settlement day before the customer actually pays for his purchase. Thus, piracy can be discouraged, intellectual property rights of electronic contents can be protected. Each of the involved merchants can obtain his own deserved share of the customer’s payment in accordance with their union agreement on this content. As examples to demonstrate the applications of our generic payment model, two well-known payment systems are enhanced herein, based on our payment model. 
19|5||Methods for Protecting Password Transmission|In this paper, we present a secure method for protecting passwords while being transmitted over untrusted networks. We also present a secure method for changing an old password to a new password. The proposed solutions do not require the use of any additional keys (such as symmetric keys or public/private keys) to protect password exchanges. Unlike existing solutions, the proposed schemes do not use any symmetric-key or public-key cryptosystems (such as DES, RC5, RSA, etc.). Our schemes only employ a collision-resistant hash function such as SHA-1. 
19|6|http://www.sciencedirect.com/science/journal/01674048/19/6|From The Editor|
19|6||A Need for a Network Security Czar|
19|6||US Government Fighting for Expanded Wiretap Laws|
19|6||E-Commerce Paranoia: Flaws in Code|
19|6||Truly Certified: Security Certifications Update|
19|6||Digital Certificates Get Creative|
19|6||Health Care Security: A Hard Look at a Growing Problem|
19|6||Safeway UKâs Website Shut Down|
19|6||Philippine Government Creates Incident Response Team|
19|6||DDoS Defence Gets Regulatory|
19|6||Visaâs Ten Commandments for E-Security Online|
19|6||âPorngateâ?|
19|6||What You Sell Online in France Could Be Restrictedâ¦|
19|6||Hackers Still Love to Attack the Pentagon|
19|6||Spying At Home: A New Pastime to Detect Online Romance|
19|6||Is a Convicted Hacker Really Reformed and Should You Hire Them?|
19|6||DefCon Recruiting JamFest|
19|6||Hacker Target: Mobile Phones|
19|6||E-Mail Privacy Issues Escalate|
19|6||GeoCities Ordered to Report on Information Poster|
19|6||âMafiaboyâ Hit With 64 New Charges|
19|6||Hackers Breach Firewall-1|
19|6||Lotus Domino Holes Up|
19|6||Fireworks, Beer and Old Halfpennies â The Risks of Assumption|
19|6||Netspionage â Part III: The Black Zone, Who Uses Netspionage, How and Why25|
19|6||A Mathematical Structure of Simple Defensive Network Deceptions|
19|6||Authentication and Supervision: A Survey of User Attitudes|User authentication is a vital element in ensuring the secure operation of IT systems. In the vast majority of cases, this role is fulfilled by the password, but evidence suggests that this approach is easily compromised. Whilst many alternatives exist, particularly in the form of biometric methods, questions remain over the likely user acceptance. This paper presents the results of a survey that examines user attitudes towards a range of authentication and supervision techniques. It is concluded that whilst there is still an element of reluctance amongst users to depart from the familiar password based mechanisms, many are convinced of the need for improved authentication controls. The acceptability to users of various new techniques is variable, but many seem willing to consider a range of alternative methods. 
19|6||Abstracts of Recent Articles and Literature|
19|6||Ticket and Challenge-Based Protocols for Timestamping|In this paper we introduce two methods that allow you to certify the time when a particular document was presented to a certifying authority. While some of the algorithms that served this purpose already existed in the literature, our methodology has significant practical advantages. The two methods we show are more straightforward, by giving a user a chance, in some cases, to operate on one value rather than two. They give the user the flexibility to select the most appropriate algorithm. They provide for a reasonable sharing of the workload between the user and the timestamping authority. 
19|6||Information Security in Multiprocessor Systems Based on the X86 Architecture|This paper examines security in systems based on Intel x86 architecture and covers aspects from timing attacks to the prevention of service bugs. It reviews some software protection techniques and introduces some new techniques focused on multiprocessor systems. 
19|7|http://www.sciencedirect.com/science/journal/01674048/19/7|From The Editor|
19|7||New Classes of Unix/Linux Attacks|
19|7||American Express Creates Disposable Credit Card Numbers|
19|7||Microsoft Releases New IIS Security Tool|
19|7||Baltimore Releases Security Developer Freeware|
19|7||âFiles Streamsâ Virus Infects in an Old and New Way|
19|7||Trinity v3, a DDoS Tool, Hits the Streets|
19|7||Hacker Insurance Now Part of the Business Risk Management Kit|
19|7||Wireless Crazed? No Security For You!|
19|7||White House Supporting Open Source Code|
19|7||White House Security Summit Builds Towards a B2B Security Standard|
19|7||Cellular Security Hazards|
19|7||UK Fuel Taxes Protestor Accused of Mass Hacking|
19|7||US Government Board Setting Up Security Metrics|
19|7||Privacy Group Wants âWeb Bugsâ Disclosures|
19|7||Site Spoofing Becomes More Popular|
19|7||First PDA Virus Hits the Airwaves|
19|7||Do You Know Your Organizationâs Achilles Heel?|
19|7||Romancing The Internet And Managementâs Quagmire|
19|7||A Practical Risk Analysis Approach: Managing BCM Risk|
19|7||Information Security â The Third Wave?|
19|7||Abstracts of Recent Articles and Literature|
19|7||Word Association Computer Passwords: The Effect of Formulation Techniques on Recall and Guessing Rates|Several recent studies have attempted to lay the groundwork for a simple, low level computer security system involving cognitive and word association passwords. The present research investigated the effect of three different word association formulation techniques on recall rates when participants were presented with their original cue words after a 2 week retention interval. Differences in the susceptibility of word associations to being guessed by someone the participant knew well were also examined. In the ‘response only’ group (n = 24), respondents were provided with a list of 20 cue words and were required to generate an associated response for each cue. In the ‘cues and responses’ group (n = 25), respondents generated both cues and associated responses, while in the ‘theme’ group (n = 24), respondents generated both cue and response words having first decided upon a theme for their word associations. The average recall and guessing rates were 66% and 8%, respectively. No statistically significant differences in recall or guessing rates were found between the three groups. The effect sizes obtained strongly suggest that insufficient statistical power was available to produce significant effects. Nevertheless, it is tentatively concluded that the formulation techniques employed have only a small effect on ease of recall. Future research on the utility of word associations as computer passwords needs to be based in a real computer security setting. 
20|1|http://www.sciencedirect.com/science/journal/01674048/20/1|From The Editor|
20|1||Y2K & Security â A Low Priority Year|
20|1||Voodoo is Alive and Well on Your Computer|
20|1||Hand-held Virii On The Way|
20|1||China Passes Law to Promote Internet Security â Supposedly|
20|1||Smarter Marines|
20|1||E-tailers Batten Down the Hatches|
20|1||Information Warfare Highlighted as a Concern by US Government|
20|1||E-mail Spammers Jailed|
20|1||The Chief Security Officerâs Top Ten List for 2001|
20|1||âSafe Harborâ Provision Adopters Few and Far Between|
20|1||The Wireless Great Divide â But Getting Smaller|
20|1||Itâs a Privacy Thing in 2001|
20|1||Fighting SPAM in Europe|
20|1||Privacy Advocates Grow in Europe|
20|1||2001: A Privacy Odyssey|
20|1||Software Concerns And The Internet|
20|1||Protecting The Web Server And Applications|
20|1||NIDS â Pattern Search vs. Protocol Decode|
20|1||Protecting Critical Information Infrastructures|
20|1||Trends In Cybercrime â An Overview Of Current Financial Crimes On The Internet|
20|1||Abstracts of Recent Articles and Literature|Law on cracking security codes toughened,Amy Harmon. The US copyright office has passed a new law making it illegal to break the security methods in place to prevent the copying of digital music, books and movies. The law will update the existing 1998 Digital Millennium Copyright Act and will come into effect immediately. The decision was opposed by groups such as Universities, programmers and libraries who argue that copyrighted work should be archived and leant out. Programmers also claimed that reverse-engineering was a valid technique in the cause of learning how technology works. The Association of American Universities has stated that in order to maintain civil liberties, there must be broad exemptions to allow for “fair use” of purchased copyrighted digital goods. The Toronto Star, 30 October, E3.Game could hold key to Net security,Gareth Cook. According to top mathematicians, the key to cracking encryption algorithms could be linked to the popular Minesweeper game which comes free with the Windows operating system. The problem, the ‘P versus NP conjecture’, asks why some problems are so difficult to solve using a computer, while others can be solved very quickly. Problems like putting a list into alphabetical order, which are easily solved by a computer are classified as P problems, and those which cannot are classed as NP. A prominent specialist challenged the conjecture in the spring edition of the Mathematical Intelligencer journal, saying that the conjecture would be proved false if the Minesweeper logic game could be cracked, for any sized board, using a computer program. If the conjecture is true, then certain types of problem will always remain insoluble by computers and the current system of encryption algorithms will continue to be considered to be safe. The Clay Mathematical Institute considers the matter to be so important that they have offered a US $1 million prize for a solution. The impact of such a discovery would be to mathematically map the boundary beyond which computers, no matter how powerful, cannot succeed. The Boston Globe, 1 November, B1, B4.MOJ proposes new laws to crack down on hackers. The Japanese Ministry of Justice has announced that it intends to toughen up Taiwan’s Criminal Code as it relates to hackers. Those who break into government, military or financial institutions would face up to 15 year jail sentences. Officials also say that any form of hacking over the Internet should be made illegal. The code will address holes in legislation relating to computer crimes and will also criminalize the distribution of pornography and selling of prohibited substances while regulating the responsibilities of ISPs, pornographic websites and those which facilitate gambling. The China Post, 23 October.Canada’s computer security loopholes called threat to US.David Akin. James Adams, senior security advisor to the US, told the PopTech2000 conference that Canada is incapable of defending itself against attacks against its telecommunications and computer networks. This endangers the US because “Canada is essentially the back door into America”, due to the amount of major US corporations which have Canadian offices. The private sector in the US and Canada do not have the right to share information gleaned by the security agencies, which leaves it more vulnerable that those from countries like France, Israel, China, Russia and India. The criticism comes as 30 countries including the US, but not Canada, are gearing up to simulate information warfare attacks. Adams also warned against purchasing hardware and software from France, China, India or Russia, “We know without any question whatsoever that a lot of things that are sold in the United States, be it a firewall or some software, are infected with implants from these countries so that anything that goes across their networks, or is stored in their programs is sent back to the country of manufacture.” Canadian National Post, 30 October.Viruses the next generation, Kim Zetter. Viruses are getting faster and more prolific. They have been around since the early 60s when they infected through floppy disks, the ‘sneaker net’. They can go around the world in minutes, sent by E-mail. With the trend within industry to use the same off the shelf products — Windows, Word, Outlook — viruses can hit millions of machines in one fell swoop. Ron Moritz at Symantec, an anti-virus company, said, “If the virus writers ever thought through their programs, we would see much more virulent viruses that would really do damage.” The threat is growing worse, and although anti-virus software is important, it can only target a virus once it has hit a few computers. In the future, viruses are likely to spread without you having to open an E-mail — a stage beyond Bubbleboy and Kak. Fred Cohen, who coined the phrase ‘computer virus’, describes mini-viruses which will dodge detection and spawn sub-variants which will cluster and attack various parts of your computer. They have already hit PDAs and mobile phones and Linux will be the next victims. The automation of inoculation has been considered as a possible protection, however it is feared that this would pass too much control onto the software suppliers — and the consequences of a hacker intercepting such a transfer would be disastrous. Others want to ban the writing of malicious code, but this is deemed to be against the principals of freedom of speech. Moritz predicts that the next generation of viruses will combine swift propagation like the Melissa and LoveLetter viruses with a destructive payload — something that neither of these two debilitating viruses actually had. NewLove followed on from LoveLetter the next month, June 1999, and closely resembles a super-virus. It had a damaging payload, overwriting system files and making computers inoperable, and could spread quickly. However, fortunately, it failed because it was so acerbic that it destroyed Outlook before it could be mailed on, and so failed to multiply. But the greatest fear is that the technology will be used for cyber-warfare. Protection in the form of anti-virus software affords some comfort, but the users must also learn caution when on the Internet or using E-mail. PC World, December, p.p. 191–203.IPSec VPNs: they’re not all virtually the same,Isaac Hillson. What you need from a VPN is speed of throughput, simple management and security that you can trust. All of the products out there have been assembled to the same IPSec standards, however there are massive differences in functionality. An independent test of 12 systems was commissioned from Network Test (www.networktest.com). The results were worrying, as some were “alarmingly vulnerable to attack”. Some of the products were 10 times faster than others, there was varying interoperability with Certificate Authorities and PKI standards were a bit of a let down. However, the best systems are definitely worth a look. The top three were Intel’s Netstructure 3130, Lucent’s VPN firewall and Alcatel’s Secure VPN Solution. The systems tested ranged from $2000 to $21â000 and full results can be found at www.commweb.com. Computer Telephony, November, p. 138.Shockwave virus creates little damage,Jaikumar Vijayan. The virus Shockwave, first reported at the end of November appears to be spreading more slowly than was expected thanks, at least in part, to an increased awareness of the spread of E-mail viruses. The virus disguises itself as an attachment to an E-mail called creative.exe and appears to come from a familiar E-mail address. If a recipient double clicks on this attachment the virus replicates onto the victim’s system and sends new copies of itself via E-mail to all the addresses in the E-mail address book. It doesn’t destroy any files, but it does rename some graphics and .zip files. Computerworld, 11 December 2000, p. 24.Is Carnivore dangerous? Controversy continues,Michael Meehan. Researchers studying the FBI’s Carnivore E-mail surveillance system have “serious concerns” about the controversial technology, despite a generally favourable draft report. The five researchers said that the review carried out by the IIT Research Institute “appears to represent a good-faith effort at [an] independent review” of the surveillance system. But, they added, “the limited nature of the analysis described in the draft report simply cannot support a conclusion that Carnivore is correct, safe or always consistent with legal limitations”. Carnivore can be legally deployed only to monitor alleged criminal activity under a court order, but privacy advocates have said that they are worried that the software could lead to widespread and random surveillance of E-mail messages. The researchers took particular issue with what they said are vague and alterable audit trails within Carnivore. Without a definitive audit trail it would be impossible to determine who had monitored what. One of the researchers said that Carnivore could be misconfigured to monitor almost anything. Computerworld, 11 December 2000, p. 24.Virus vigilance,Deborah Radcliff. In the year since ‘I LoveYou’ struck variants of this self-replicating virus are still reaching the mailbox. It seems as though a new variant strikes every day. In a two-week period a plethora of new viruses were reported, including variants of Romeo & Juliet, Navidad, Afeto and Shockwave. Like Melissa and I LoveYou they all take advantage of mail programs to spread. The problem with today’s viruses is two-fold: not only can they be easily altered to change their signatures, but they also have tempting attachment types to persuade users to open them. No matter how often you tell users not to open executables some will succumb to temptation and training is not enough. It is necessary to set up filters to block executable attachments before they enter the desktop. Blocking file types known to carry viruses and Trojan horses sounds extreme, but if these attachments aren’t used for business purposes the decision is easier. Microsoft Outlook 2000 E-mail security patch was designed to filter a number of executable file types. In addition, a number of vendors offer filtering products that can block custom-specified file types and even subject lines. So keep your anti-virus software up to date, educate your users and block unnecessary executables at the gate. Computerworld, 11 December 2000, p. 68.Bush eyes overhaul of E-security,Dan Verton. In the United States, national security experts are preparing for what could be a major change in the way the Government and the private sector prevent cyberattacks. President Elect Bush plans to appoint an IT ‘czar’ to better manage the Government’s IT investments. That move is likely to involve reorganizing the federal critical infrastructure protection effort and possibly changing the role of the FBI’s National Infrastructure Protection Center (NIPC). Changes to NIPC could involve asking Congress for new legislation to make it easier for the national security community to get access to investigative information, making NIPC subordinate to a federal IT czar or security officer, or starting from scratch with a different type of organization. Computerworld, 18 December 2000, p. 1, 85.Hospital confirms hacker stole 5000 patient files,Marc Songini. A major hospital in Seattle, Washington, USA confirmed that during last summer a hacker penetrated its computer network and stole files containing information on 5000 patients. Officials at the University of Washington Medical Center said that the hacker, calling himself ‘Kane’, stole users’ passwords and copied thousands of files. The hacker gained access to the hospital’s network through an exposed Linux server in the hospital’s pathology department. The medical centre suspected a breach but was unaware that the files had been stolen until Kane provided information about the intrusion to SecurityFocus.com, a website that focuses on security issues. Kane, who is believed to live in the Netherlands, sent the files to the website to verify that he had accessed sensitive data. Kane it is believed, views himself as an ethical hacker. He used sniffer software to steal the electronic identifications of a number of hospital employees from the exposed server and then used the credentials to access files related to patients in the medical centre’s cardiology and rehabilitation departments. Computerworld, 18 December 2000, p. 7.Hacker cracks Egghead’s security. Online technology retailer Egghead.com had revealed that a hacker penetrated its computer systems, possibly including the customer databases where the company stores credit-card numbers and other personal information about the users of its website. Egghead reported the hacking incident to credit-card companies “as a precautionary measure”, in case card numbers had been stolen by the intruder. Computerworld, 1 January 2001, p. 6.Big brother plan slammed,Steve Ranger. A proposal by the security services in the UK to collect and keep records on phone and Web usage for seven years by everyone in the country has been attacked by MPs and industry experts. The document, written by the deputy director general of the National Criminal Intelligence Service, said that telephone operators and Internet service providers should hold onto phone records, mobile phone location data, E-mail and Web usage information for a year. The data would then by moved to a government data warehouse for another six years. A budget of £3 million has been proposed to set up the warehouse, which would be combined with the project to develop the controversial ‘black boxes’ which ISPs would plug into systems to snoop on E-mail and Web activity. The communications industry is concerned that it may by caught between demands from law enforcement to retain information and the Data Protection Act, under which it must delete information as soon as it can. Computing, 7 December 2000, p. 3.Network Associates says code is safe,Ian Lynch. The anti-virus company, Network Associates, denies that its anti-virus source code was compromised by hackers following a breach of security at two of the company’s Brazilian sites. “No files or data were compromised by the hack”, said Jack Clark. The company claims that the hackers only accessed sites hosted by a local contractor, Brazilian Internet service provider Matrix. Clark admitted that the company was embarrassed by the incident. “It’s embarrassing. Using a local ISP was deemed to be a better solution at the time, as it was thought to give our Brazilian customers better performance. It appears that Matrix was slower responding to a published security vulnerability than we’d have liked it to have been.” Computing, 7 December 2000, p. 8.IBM provides key to faster secure communications,John Geralds. IBM has developed a way of simultaneously performing software encryption and authentication that it claims is twice as fast as other methods. The security algorithm, which has yet to be given a name, uses the same secret key to encrypt and decrypt a message. Existing approaches require encryption and authentication to be performed separately to ensure secure communications. Using parallel processors gives even faster performance as the encryption is spread over multiple processors. IBM also expects the algorithm to be used in fibre optic networks and several E-business applications. The company has submitted the algorithm to the National Institute of Standards as a standard for securing communications. Computing, 7 December 2000, p. 10.National security threatened by Internet, studies say,Dan Verton. During the next 15 years, the US will face a new breed of Internet-enabled terrorists, criminals and national adversaries that could launch attacks with computer viruses and logic bombs. A report by the CIA’s National Intelligence Council (NIC) mentioned critical electronic infrastructure protection and information warfare only briefly, it warned Americans that adversaries worldwide are hard at work developing tools to bring down the USA’s private sector infrastructure. Another report, by the Center for Strategic and International Studies, went even further, warning of a future cyber arms race and the rise of terrorist groups supported by “computer-literate youngsters” bent on disrupting the Internet. China is of particular concern because it’s devising strategies for unrestricted electronic warfare. Online extortion and falsification of shipping manifests by criminals, and attempts by countries to use hacking techniques to evade trade sanctions are a growing concern. Officials are also becoming increasingly concerned with the proliferation of ‘always on’ Internet applications, such as modems and network printers. Hackers are finding ways to penetrate these devices and possibly use them as launching pads for more devastating distributed denial-of-service attacks. Computerworld, 1 January 2001, p. 7.CSSA targets security issues for E-commerce,Andy McCue. E-commerce companies will be told by their trade associations to take a more targeted approach to security. The Computing Services and Software Association (CSSA) is working with the Alliance for Electronic Business in the UK to spearhead a programme that will allow companies to report security incidents anonymously for analysis by security experts. Users will have access to tailored guidelines for their industry sector based on the principles of BS7799. The proposals were discussed by representatives from the UK’s DTI, the Confederation of British Industry, CMG Admiral, Microsoft, Unisys, Baltimore Technologies and the Post Office. Computing, 14 December 2000, p. 6.Security spend must rise,Liesbeth Evers. According to a research report released by Forrester Research, security managers plan to triple their budgets over the next four years. However, even with this increase, half of the 50 security managers questioned said that the lack of money would prevent them from implementing all the security measures that were required. Frank Prince, the author of the report, The Security Market, predicts that by 2005, many companies would have implemented automated E-business systems such as hands-free procurement. “Automated processes need automated security”, he said. “Firms will struggle with cross-company security, as this requires agreeing with standards at many levels.” Companies should make business managers responsible for all aspects of security, and should formalize this by creating an integrated corporate security policy. Over time, criminals will discover new ways to use the Internet to aid crime. Computing, 7 December 2000, p. 67.Crossing the wireless security gap,Alan Radding. Wireless security is difficult to implement, requiring organizations to piece together myriad technologies. Most organizations would prefer to support only a single security model for E-commerce, preferably the Internet model in use today. E-commerce in the wired world relies primarily on Secure Sockets Layer (SSL), but when you try to move this approach to the wireless world, you immediately encounter problems starting with cellular phones with Wireless Application Protocol (WAP) capabilities. WAP phones are pretty limited when it comes to security and lack the CPU power and memory necessary for RSA encryption, a key element of SSL. WAP devices include their own security protocol, Wireless Transport Layer Security (WTLS). This is equivalent to SSL but it uses less resource-intensive encryption algorithms, such as elliptic-curve cryptography. Wireless messages travel through the air to the carrier’s transmitter, where they are received and passed to a gateway that funnels them into the conventional wired network for transmission to the destination. At the gateway, the WTLS message is converted into SSL. For a brief moment, the message sits unencrypted inside the gateway, creating a security vulnerability. Many of the obstacles confronting wireless security will disappear with the widespread adoption of third-generation wireless technology. The third-generation phones will be IP-based and sport more processing power, memory and bandwidth, which should allow end-to-end SSL security. By combining third-generation wireless with smart cards and biometrics, organizations should finally have a unified security system that works for both the wireless and wired worlds. Computerworld, 1 January 2001, pp. 52–53. 
20|1||Calendar|
20|1||International Board of Referees|
20|1||How to Construct Cryptographic Primitives from Stream Ciphers|A general stream cipher with memory (SCM) mode in which each ciphertext symbol depends both on the current and on the previous plaintext symbols is pointed out. It is shown how to convert any keystream generator into the SCM mode and the security of both the modes is discussed. It is proposed how to construct secure self-synchronizing stream ciphers, keyed hash functions, hash functions, and block ciphers from any secure stream cipher in the SCM mode. Rather new and unusual designs can thus be obtained, such as the designs of block ciphers and (keyed) hash functions based on clock-controlled shift registers only. 
20|1||IFIP Technical Committee 11|
20|1||Guide For Authors|
20|2|http://www.sciencedirect.com/science/journal/01674048/20/2|From The Editor|
20|2||A Practical Risk|
20|2||The Search For Privacy|
20|2||The Evolution of Intrusion Detection Systems â The Next Step|The current situation in the intrusion detection arena has spurred global information security group Articon-Integralis to make the next step in Intrusion Detection Systems. Accordingly, the group decided to conduct a detailed commercial, corporate and technical assessment of all key players in the market during the second half of 2000. This article will look at the specification that drove the selection and test process and will cover the key technical parameters which were used to assess each offering. It will go on to detail the topology, machines and attacks that were used to make the assessment. It will also attempt to give some insight into the process that the group has adopted to keep abreast of all key ‘best of breed' security products and the companies that provide them. 
20|2||Controlling Risks of E-commerce Content|Traditional approaches to security are failing as we move towards ever more open networks and business models. The combined trends of outsourcing and joint ventures have rendered passé the concept of a secure perimeter shell and a firewall to protect it, as we extend the boundaries of trust outside of our former closed spheres. But perimeter firewalls are also becoming deficient for other reasons, as E-commerce content and applications contain increasingly active functionality that can neither be inspected nor controlled using perimeter firewalls alone. This means that organizations must examine their own ‘risk appetites’ to arrive at their own ideal balance of E-risks against E-benefits. This session explores the status and future trends for E-commerce content security, derived from practical experience in securing one of the largest corporate networks. 
20|2||Abstracts of Recent Articles and Literature|
20|2||Calendar|
20|2||Refereed Papers|
20|2||Violation of Safeguards by Trusted Personnel and Understanding Related Information Security Concerns|A majority of computer security breaches occur because internal employees of an organization subvert existing controls. While exploring the issue of violation of safeguards by trusted personnel, with specific reference to Barings Bank and the activities of Nicholas Leeson, this paper provides an understanding of related information security concerns. In a final synthesis, guidelines are provided which organizations could use to prevent computer security breaches. 
20|2||Alternative Method for Unique RSA Primes Generation|This paper describes a new method for generating RSA primes. While the primes generated according to this method satisfy all of the existing requirements for making RSA encryption and signature generation secure and efficient, they possess two additional properties. First, the RSA primes derived by different parties will always be different. This is a highly desirable property in view of possible disputes around the authenticity of a digital signature. Second, it will not be necessary to store any seed values to verify that these primes were generated according to the prescribed procedure. This is achieved by incorporating the required information about the seeds into the primes themselves without weakening the security properties of the RSA primes. 
20|2||IFIP Technical Committee 11|
20|2||Guide For Authors|
20|3|http://www.sciencedirect.com/science/journal/01674048/20/3|From The Editor|
20|3||Security Views|
20|3||The Times They Are A-Changinâ|
20|3||Protecting 21st Century Information â Itâs Time for a Change|
20|3||Corporate Governance and Information Security|
20|3||Security Training: Education For an Emerging Profession?|
20|3||Securing Your Brand In Cyberspace|
20|3||Abstracts of Recent Articles and Literature|
20|3||Calendar|
20|3||Author Index for Volume 19|
20|3||Subject Index for Volume 19|
20|3||International Board of Referees|
20|3||Methods for Timestamping Electronic Documents Using Certificates and User-Specified Times|
20|3||A Framework for Understanding Vulnerabilities in Firewalls Using a Dataflow Model of Firewall Internals1|Vulnerabilities in vendor as well as freeware implementations of firewalls continue to emerge at a rapid pace. Each vulnerability superficially appears to be the result of something such as a coding flaw in one case, or a configuration weakness in another. Given the large number of firewall vulnerabilities that have surfaced in recent years, it is important to develop a comprehensive framework for understanding both what firewalls actually do when they receive incoming traffic and what can go wrong when they process this traffic. An intuitive starting point is to create a firewall dataflow model composed of discrete processing stages that reflect the processing characteristics of a given firewall. These stages do not necessarily all occur in all firewalls, nor do they always conform to the sequential order indicated in this paper. This paper also provides a more complete view of what happens inside a firewall, other than handling the filtering and possibly other rules that the administrator may have established. Complex interactions that influence the security that a firewall delivers frequently occur. Firewall administrators too often blindly believe that filtering rules solely decide the fate of any given packet. Distinguishing between the surface functionality (i.e., functionality related to packet filtering) and the deeper, dataflow-related functionality of firewalls provides a framework for understanding vulnerabilities that have surfaced in firewalls. 
20|3||IFIP Technical Committee 11|
20|3||GUIDE FOR AUTHORS|
20|4|http://www.sciencedirect.com/science/journal/01674048/20/4|From The Editor|
20|4||Security Views|
20|4||The Weakest Link|
20|4||The Corporate Information Assurance Officer (CIAO)1|
20|4||Incremental Information Security Certification|
20|4||Cyber crime: the backdrop to the Council of Europe Convention|
20|4||Fighting Against the Invisible Enemy: Methods for detecting an unknown virus|
20|4||Abstracts of Recent Articles and Literature|
20|4||Calendar|
20|4||International Board of Referees|
20|4||Extending the Risk Analysis Model to Include Market-Insurance|
20|4||IFIP Technical Committee 11|
20|5|http://www.sciencedirect.com/science/journal/01674048/20/5|From The Editor|
20|5||Security Views|
20|5||Cyberthreats: Perceptions, Reality and Protection|
20|5||Proprietary Data: Eldorado in Cyber-Space|
20|5||Security in a Mobile World â is Bluetooth the Answer?|Richard Barber, group technical advisor of Articon-Integralis looks, at the evolution of tele- and data-communications systems and assesses the potential advantages for mobile security offered by technologies like Bluetooth 
20|5||The Council of Europe Cyber-Crime Convention|
20|5||A Framework for the Implementation of Socio-ethical Controls in Information Security|
20|5||Computer Licence Plates|
20|5||Conference Report â the Global Forum for Law Enforcement and National Security, Edinburgh, 2001|
20|5||Abstracts of Recent Articles and Literature|
20|5||Calendar|
20|5||International Board of referees|
20|5||Information Systems Audit Trails in Legal Proceedings as Evidence|Australian State and Commonwealth Governments are interested in the collection, storage and presentation of audit trail information, particularly within a legal framework. Law enforcement agencies have a legal obligation to keep audit records of all activity on information systems used within their operations. Little to no research has been identified in relation to the use of internal audit systems for evidentiary purpose.A brief history of audit trails is given with requirements for such audit trails beyond the year 2000.The Queensland Police Service (QPS), Australia, is used as a major case study . Information on principles, techniques and processes used, and the reason for the recording, storing and releasing of audit information for evidentiary purposes have been studied.To assist in determining current practice in the Australian Commonwealth and State Governments the results of an Australia wide survey of all government departments are given and contrasted to the major study for QPS.Reference is also made to the legal obligations for authorization of audit analysis, expert witnessing and legal precedence in relation to court acceptance or rejection of audit information used in evidence.It is shown that most organizations studied generate and retain audit trails but the approach is not consistent nor is it comprehensive. It is suggested that these materials would not withstand a serious legal challenge. 
20|5||Multi-party Fair Exchange Protocol Using Ring Architecture Model|We present a new protocol which allows multiple parties to exchange electronic items over the Internet in a secure and fair way. This allows either each party to get what it expects to receive, or neither party to receive anything. This protocol is applicable to various electronic businesses, for example, electronic shopping, group purchases, electronic voting, electronic auctions, and electronic bidding. The multi-party fair exchange protocol using the ring model is more efficient and robust than the protocol using the full-mesh model proposed by SEMPER. Our protocol reduces its communication complexity and simplifies its recovery procedures compared with SEMPER’s. 
20|5||IFIP Technical Committee 11|
20|5||GUIDE FOR AUTHORS|
20|6|http://www.sciencedirect.com/science/journal/01674048/20/6|From The Editor|
20|6||U.S. DoD Puts Up Blocks to Code Red|
20|6||Omnia te adversum spectantia, nulla retorsum*|
20|6||Organized Crime Goes Cyber|
20|6||A Simple Graphical Tool For Modelling Trust|Founder Member of ‘Club de Sécurité des Systèmes Informatiques au Luxembourg (CLUSSIL)’ 
20|6||Money Laundering on the Internet|
20|6||Managed Security Monitoring:Network Security for the 21st Century|
20|6||Information Security â A Multidimensional Discipline|Immediate Past Chairman : TC 11 of IFIP; Information Security Consultant to Deloitte & Touche (South Africa) 
20|6||Abstracts of Recent Articles and Literature|
20|6||Calendar|
20|6||COMPSEC 2001 Exhibitors 2001|
20|6||International Board of Referees|
20|6||Access Control in Document-centric Workflow Systems â An Agent-based Approach|Workflow Systems are increasingly being used to streamline organizations’ business processes. During the execution of business processes, information often traverses organizations’ networks as documents. With the proliferation of the Internet, documents travel across open networks. These documents can, however, contain potentially sensitive information. The documents used in Workflow Systems must therefore be protected from unauthorized access.This paper enumerates three access control requirements of workflow environments, including the well-known principle of separation of duty. Thereafter the CSAC (Context-sensitive Access Control) model is presented to address the requirements. In conclusion it is demonstrated how this model can be implemented in an agent-based architecture. 
20|6||Hiding Digital Information Using a Novel System Scheme|This work builds on Lin and Lee’s document protection scheme, termed as Confused Document Encrypting Scheme (CDES), to present a novel strategy that could hide digital information and provide secret communication between two communication parties. Lin and Lee invented a document protection scheme to transmit large quantities of secret information. However, their design strategy is only suitable for single-byte character-based text document. The proposed scheme presents a novel strategy that modifies the restrictions in Lin and Lee’s work. Any kind of digital data, not only the plain text, can be employed as a carrier or as a secret in the proposed system. The proposed method utilizes the hexadecimal representation of digital data in a manner that ensures it will have many practical applications. In addition, the procedure is easy to implement and capable of transmitting large quantities of secret information. 
20|6||IFIP Technical Committee 11|
20|6||GUIDE FOR AUTHORS|
20|7|http://www.sciencedirect.com/science/journal/01674048/20/7|From The Editor|
20|7||Security Views|
20|7||Lessons Learned|
20|7||Incalculable potential for damage by cyber-terrorism|
20|7||The European Union Proposal for a Policy Towards Network and Information Security|
20|7||From Risk Analysis to Security Requirements|
20|7||Abstracts of Recent Articles and Literature|
20|7||Calendar|
20|7||International Board of Referees|
20|7||Evaluating Trust in a Public Key Certification Authority|With the growth of many different public key infrastructures on the Internet, relying parties have the difficult task of deciding whether the sender of digitally signed message is really who the public key certificate says they are. We have built an expert system that calculates the amount of trust, or trust quotient, that one can place in the name to public key binding in a certificate. The structure of the expert system is based on the CPS framework of Chokhani and Ford (RFC 2527), whilst the relative importance of the various factors that comprise the trust quotient, were determined by interviewing PKI experts from around the globe. This paper discusses the knowledge analysis strategy employed to collect this expert information and how we used it to develop the KBS. The analysis of the results of the interviews are also presented, and they can be summarised succinctly as “there are some factors concerning trust in a PKI which nearly all experts agree upon, and there are other factors in which there is very little agreement at all”. The importance of identifying contextual factors when building a knowledge base is very important. In many cases, a disagreement between experts, as shown by a bimodal split in importance, was traced to differences in context and we show how this can be a source of new knowledge. 
20|7||Hierarchical key assignment without public-key cryptography|VLSI chips make possible the hardware devices employed in today’s computing environment for security functions. Controlling access in a hierarchy is an interesting research topic in computer security. Many investigations have been published in the literature with solutions involving assigning cryptographic keys to users at different access clearance levels. However, the existing schemes require a large number of costly arithmetic operations with large integers. This type of system is difficult to implement in a chip with lower computation ability. In this paper, we present a solution, suitable for a low cost chip, to the hierarchical control problem. The proposed scheme has promising characteristics such as high computational efficiency, little required memory in the chip and low cost implementation. This method possesses all of the dynamic properties that appear in existing methods. 
20|7||Usability and Security An Appraisal of Usability Issues in Information Security Methods|
20|7||IFIP Technical Committee 11|
20|7||Guide for Authors|
20|8|http://www.sciencedirect.com/science/journal/01674048/20/8|From The Editor|
20|8||Security Views|
20|8||If you can meet with triumph and disaster and treat those two impostors just the sameâ¦|
20|8||What Do The Recent Terrorist Attacks Mean For The American Information Security Profession?|The 11 September 2001 terrorist attacks shocked Americans and caused them to question the safety of life in America today. While many of us are still internally processing the recent events, it is important for information security professionals to capitalize on this new shift in security consciousness. This article explores six specific ways in which the recent attacks have changed the way we practice information security in America. For each, there are specific actions that proactive information security specialists can and should take without delay. 
20|8||Guidelines for the Protecting the Corporate against Viruses|
20|8||A comparison of Intrusion Detection systems|A computer system intrusion is seen as any set of actions that attempt to compromise the integrity, confidentiality or availability of a resource.1 The introduction of networks and the Internet caused great concern about the protection of sensitive information and have resulted in many computer security research efforts during the past few years. Although preventative techniques such as access control and authentication attempt to prevent intruders, these can fail, and as a second line of defence, intrusion detection has been introduced. Intrusion detection systems (IDS) are implemented to detect an intrusion as it occurs, and to execute countermeasures when detected.Usually, a security administrator has difficulty in selecting an IDS approach for his unique set-up. In this Report, different approaches to intrusion detection systems are compared, to supply a norm for the best-fit system. The results would assist in the selection of a single appropriate intrusion detection system or combine approaches that best fit any unique computer system. 
20|8||Abstracts of Recent Articles and Literature|
20|8||Book Reviews|
20|8||Book Reviews|
20|8||Calendar|
20|8||International Board of Referees|
20|8||The Benefits of a Notification Process in Addressing the Worsening Computer Virus Problem: Results of a Survey and a Simulation Model|Computer viruses present an increasing risk to the integrity of information systems and the functions of a modern business enterprise. Systematic study of this problem can yield better indicators of the impact of computer viruses as well as a better understanding of strategies to reduce that impact.We conducted a Computer Virus Epidemiology Survey (CVES) on the World Wide Web to examine indicators of the impact of computer viruses. A major finding from the CVES is that multiple indicators of the impact of computer viruses reveal a problem growing more severe that affects large, as well as small, organizations. Another important finding is that apparently undetectable viruses caused only about 15% to 21% of problems reported in workgroups using antiviral software, leaving a substantial amount of damage due to viruses that were probably detectable. Another important finding is that viruses not detected despite regular updating of antiviral software caused only about 15% to 21% of virus problems reported in workgroups using antiviral software. The possible reasons for failure to detect include improper configuration of software and the inability of all known anti-virus detectors to detect. A related implication is that a substantial amount of damage due to viruses could probably have been prevented by regular updating of antiviral software.We also used the CVES in the development of a simulation model for the spread of computer viruses in workgroups in order to analyze the effect of a notification process on control. Our major finding is that the process of notification, whether by human behaviour or by technology, substantially reduces the impact of computer viruses in workgroups. For example, if a workgroup has a period of vulnerability when only 80% of its workstations are effectively using antiviral software, then even a 50% probability of notification of a detected virus substantially reduces the burden. An added benefit of maintaining an environment with high effective antiviral software usage and high levels of notification is that greater rates of communication rates events that can potentially transmit computer viruses within the workgroup actually become protective reduce the impact of computer viruses in the workgroup. Anecdotal observations also indicate that the process of notification is significant in controlling the spread of “new” viruses not yet detectable by software, although the process of notification from law enforcement authorities to workgroups was not in the simulation model.More formally, the reduced impact of computer viruses in a workgroup due to a protective effect of greater rate of communication events that can potentially transmit computer viruses corresponds to a situation when a computer virus introduced into the workgroup produces, on average, less than one copy in the workgroup. This threshold corresponds to the basic reproduction ratio in epidemiology that describes the spread of infectious disease. 
20|8||Computer crimes: theorizing about the enemy within|A majority of computer crimes occur because a current employee of an organization has subverted existing controls. By considering two case studies, this paper analyzes computer crimes resulting because of violations of safeguards by employees. The paper suggests that various technical, procedural and normative controls should be put in place to prevent illegal and malicious acts from taking place. Ultimately a good balance between various kinds of controls would help in instituting a cost-effective means to make both accidental and intentional misconduct difficult. This would also ensure, wherever possible, individual accountability for all potentially sensitive negative actions. 
20|8||Cryptography Regulations for E-commerce and Digital Rights Management.|Strong encryption is an urgent need for e-commerce development, as it allows the privacy and secure transactions of the financial data. International regulations must allow the spreading of e-commerce and the associated encryption products, in order to establish a secure e-commerce environment that customers can trust and allowing an international deployment of e-commerce solutions without restrictions. 
20|8||IFIP Technical Committee 11|
20|8||Guide for Authors|
volume|issue|url|title|abstract
21|-|http://www.sciencedirect.com/science/journal/01674048/21|The gap between cryptography and information security|Few controls available to information security professionals today are more potentially powerful than is encryption. Among the many important benefits of encryption are data confidentiality, integrity of data and system files, protection against repudiation in business and other transactions, assurance of individuals’ identity, protection against cheating in voting and contract signing, and others. Security professionals are required to know at least the basics of cryptography to pass professional certification tests such as the CISSP exam; they must often know much more to be able to be proficient on the job. A number of vendors have produced impressive encryption products for desktop encryption, authentication methods, virtual private networks (VPNs), and digital signatures. Significant advances in cryptography and cryptanalysis research have also occurred over the years. 
21|-||Security Views|
21|-||Compsec 2002: the complete security circle|
21|-||KLEZ H|
21|-||Giga Security|The flow of information, within organizations, between networks, and from single users to other individuals and networks, is commonly at rates that only a few years ago were dreams. Earlier methods for the detection and prevention of malicious activities are anywhere from inefficient to unworkable with transmissions at the giga speeds that are prevalent today. This writing sets forth the problems and threats associated with these new high speed transmissions, and presents methodologies and systems for treating them. 
21|-||Global Trust, Certification and (ISC)2|With the onset of the global network economy, the conventional wisdom about information security and privacy protection is undergoing serious re-evaluation. 
21|-||Policy challenges in building dependability in global infrastructures|[This paper was given at Compsec 2002, in London, on 30 October 2002].Global or continental critical infrastructures — including electric power, telecommunications, and the Internet — are now the control plane for advanced economies. The occasional failures of these key infrastructures illustrate not only our dependence, but also the unanticipated interdependencies between systems. For example, the 1998 failure of a single telecommunications satellite, Galaxy 4, led to an outage of nearly 90% of all pagers in the United States, while also causing a number of unanticipated failures: many banking and financial services (credit card purchases, automated teller machines) were interrupted, as was communications with doctors and emergency workers [1].With awareness of economic and social dependence on these distributed infrastructures has come a growing concern about their reliability and security. Defense against deliberate attack — critical infrastructure protection — emerged as part of the US national security posture in the mid-1990s with the work of the President’s Commission on Critical Infrastructure, and was codified by Presidential Decision Directive 63 in 1998. Other nations are also beginning to develop national strategies for infrastructure protection.Reliability is more than protection against deliberate attack. An accidental cut of a fiber optic trunk shut down air traffic control along the east coast of the US for a day. A cascading series of events, starting with a tree limb falling, caused much of the western US to lose electricity.The challenge of improving the reliability of global networked infrastructures presents us with significant analytical and decision-making complexities, with both technical and policy relevant dimensions [2]. This paper — using principally examples from the Internet and other distributed IT systems — presents two perspectives on these complexities. First is to present critical global infrastructures as complex adaptive systems, which share certain characteristics that policy makers and managers need to account for. Secondly, the balance of the paper outlines five major dimensions of the analytical and decision-making complexity, and presents the research and policy-making agendas that need to be addressed if we are to significantly improve the reliability of global infrastructures.Neither of these perspectives is purely technical or engineering based. Success in increasing the reliability of global infrastructures will require much more analytically sophisticated research in, among other topics, the issue areas identified here — in addition to ongoing technology-based research. 
21|-||Digest of recent IT security press coverage: Compiled by Bill McKenna|
21|-||In brief: Compiled by Bill McKenna|
21|-||Events|
21|-||Guide for Authors|
21|-||International Board of Referees|
21|-||Individual Authentication in Multiparty Communications|In this paper we introduce a new authentication scheme to achieve individual authentication in group communications. The scheme is particularly efficient and suitable for applications where users require to transmit stream of data of undefined length through noisy channels. Our scheme is in fact, robust against loss of packets during the transmission. We present the scheme called chained stream authentication (CSA) and then we prove that the scheme is conditionally secure. We then describe two variations of CSA, one interactive to use when multicast is available and a timed version suitable for broadcast communications. We conclude by describing our implementation of the timed version that is integrated and fully compatible with RAT. 
21|-||Differentially secure multicasting and its implementation methods|Though the areas of secure multicast group architecture, key distribution and sender authentication are under scrutiny, one topic that has not been explored is how to integrate these with multi-level security. Multi-level security is the ability to distinguish subjects according to classification levels, which determines to what degree they can access confidential objects. In the case of groups, this means that some members can exchange messages at a higher sensitivity level than others. The Bell-La Padula model [BL76] outlines the rules of these multi-level accesses. In multicast groups that employ multi-level security, some of these rules are not desirable so a modified set of rules is developed in this paper and is termed differential security.Also, this paper proposes three methods to set up a differentially secure multicast group: (i) NaÄ±Ìve approach, (ii) multiple tree differential security (DiffSec) approach, and (iii) single DiffSec tree approach. In order to evaluate the performances (in terms of the number of links used per packet transmitted) of these approaches, extensive simulation experiments were conducted by varying the network connectivity and group size for both uniform and non-uniform membership distribution across security levels. Our studies show that the multiple tree and single DiffSec tree approaches perform much better than the NaÄ±Ìve approach. While the multiple tree approach could be implemented using current technology, this scheme consumes many times more addresses and network resources than the single DiffSec tree approach. From our studies, we conclude that the single DiffSec tree is a viable option for supporting multi-level security as it maximizes the resource utilization and is also scalable. 
21|-||A practical key management scheme for access control in a user hierarchy|In a user hierarchy we say that a security class is subordinate to another security class if the former has a lower security privilege than the latter. To implement such a hierarchical structure, it is often desirable to allow the user of each security class to derive the keys of its subordinating classes. This problem has been extensively studied but the existing solutions have various drawbacks. In this paper, we present a practical solution to this problem, which is an efficient key management scheme that needs only a reasonable amount of extra storage. It is secure because illegal key derivations are prevented, and key replacements do not reveal information about the relationship between the old key and the new key. It is also very flexible in that it supports convenient topological changes and membership updates. Furthermore, it provides a solution to the ex-member problem, that has been ignored in many existing research works. 
21|-||IFIP technical committee 11|
21|1|http://www.sciencedirect.com/science/journal/01674048/21/1|From the editor-in-chief|Trying to secure computers, data stored in them and transmitted between them, devices that are used with computers and networks, media, and similar elements is the task that information security professionals face. Few would dispute that this is a very arduous undertaking. Although we typically are caught up in one proverbial whirlwind after another, every once in a while it behoves us to “come up for breath,” so to speak — to revisit the foundations of information security. 
21|1||Security Views|
21|1||2001: A Privacy Odyssey Revisited|
21|1||What InfoSec professionals should know about information warfare tactics by terrorists|
21|1||In brief|
21|1||Abstracts of recent articles and literature|
21|1||Calendar of forthcoming conferences and events|
21|1||ACSAC 2001 review|
21|1||International board of referees|
21|1||Guide for Authors|
21|1||Insider Threat Prediction Tool: Evaluating the probability of IT misuse|Despite the well documented and emerging insider threat to information systems, there is currently no substantial effort devoted to addressing the problem of internal IT misuse. In fact, the great majority of misuse counter measures address forms of abuse originating from external factors (i.e. the perceived threat from unauthorized users). This paper suggests a new and innovative approach of dealing with insiders that abuse IT systems. The proposed solution estimates the level of threat that is likely to originate from a particular insider by introducing a threat evaluation system based on certain profiles of user behaviour. However, a substantial amount of work is required, in order to materialize and validate the proposed solutions. 
21|1||Cryptanalysis of a Timestamp-Based Password Authentication Scheme|In this paper, we present a cryptanalysis of a timestamp-based password authentication scheme which is based on the concepts of ID-based schemes and smart cards. We show that the scheme is breakable. An intruder is able to construct forged login request from intercepted login requests and then he can impersonate other legal users and pass the system authentication. 
21|1||Hybrid Key Escrow: A New Paradigm*,â |A new paradigm for the design of key recovery systems called hybrid key escrow will be presented. It will be shown that such a design can guard the privacy of system users and at the same time enable authorized key recovery. The system will be analyzed against the three fundamental properties of any robust key recovery system: compliance, enforceability and traceability. 
21|1||IFIP technical committee 11|
21|2|http://www.sciencedirect.com/science/journal/01674048/21/2|From the Editor-in-chief|In my last editorial I discussed the foundations of information security, focusing on some of the things that information security professionals need to do to be successful in securing systems and networks. Although we can choose our approach and focus, the truth is that we have less control over other factors that affect our success. The information security function may, for example, be placed in a lower tier of an organization chart (giving it little leverage and influence within the organization to which it belongs) or it may be allocated too few resources to enable it to accomplish what needs to get done. 
21|2||Security Views|
21|2||What InfoSec Professionals Should Know About Information Warfare Tactics by Terrorists|
21|2||Security Insights|Predicting the future is never easy, but in this first of a series of interviews with infosecurity professionals we ask them to do just that. In this issue, Bob Blakley, Chief Scientist for Tivoli, an IBM company, speaks about his priorities for the year ahead. 
21|2||In brief|
21|2||Abstracts of Recent Articles & Literature|
21|2||Events|
21|2||Author Index for Volume 20|
21|2||Subject Index for Volume 20|
21|2||Refereed Papers|
21|2||Guide for Authors|
21|2||The Benefits of a Notification Process in Addressing the Worsening Computer Virus Problem: Results of a Survey and a Simulation Model|Computer viruses present an increasing risk to the integrity of information systems and the functions of a modern business enterprise. Systematic study of this problem can yield better indicators of the impact of computer viruses as well as a better understanding of strategies to reduce that impact.We conducted a Computer Virus Epidemiology Survey (CVES) on the World Wide Web to examine indicators of the impact of computer viruses. A major finding from the CVES is that multiple indicators of the impact of computer viruses reveal a problem growing more severe that affects large, as well as small, organizations. Another important finding is that viruses not detected despite regular updating of antiviral software caused only about 15% to 21% of virus problems reported in workgroups using antiviral software. The possible reasons for failure to detect include improper configuration of software and the inability of all known anti-virus detectors to detect. A related implication is that a substantial amount of damage due to viruses could probably have been prevented by regular updating of antiviral software.We also used the CVES in the development of a simulation model for the spread of computer viruses in workgroups in order to analyze the effect of a notification process on control. Our major finding is that the process of notification, whether by human behaviour or by technology, substantially reduces the impact of computer viruses in workgroups. For example, if a workgroup has a period of vulnerability when only 80% of its workstations are effectively using antiviral software, then even a 50% probability of notification of a detected virus substantially reduces the burden. An added benefit of maintaining an environment with high effective antiviral software usage and high levels of notification is that greater rates of communication events that can potentially transmit computer viruses within the workgroup actually reduce the impact of computer viruses in the workgroup. Anecdotal observations also indicate that the process of notification is significant in controlling the spread of ‘new’ viruses not yet detectable by software, although the process of notification from law enforcement authorities to workgroups was not in the simulation model.More formally, the reduced impact of computer viruses in a workgroup due to a greater rate of communication events that can potentially transmit computer viruses corresponds to a situation when a computer virus introduced into the workgroup produces, on average, less than one copy in the workgroup. This threshold corresponds to the basic reproduction ratio in epidemiology that describes the spread of infectious disease. 
21|2||Erratum|
21|2||A Novel Key Management Scheme Based on Discrete Logarithms and Polynomial Interpolations|The authors present a novel cryptographic key assignment scheme in order to solve the dynamic access control problems in a partially ordered user hierarchy. By using the scheme based on the discrete logarithms and Newton’s polynomial interpolations, each security class is assigned a secret key that can be used to derive his successors’ secret keys efficiently. The dynamic key management problems, such as adding/deleting classes, adding/deleting relationships, and changing secret keys, are discussed in detail. Moreover, it is unnecessary to consider the status of the security class at lower levels because any user can freely change his/her own secret key for some security reasons. 
21|2||Development of Information Security Baselines for Healthcare Information Systems in New Zealand|In 1996 New Zealand had introduced security standard AS/NZCS 4444 based on the British Standard BS 7799, which has recently been accepted as an international standard ISO 17799. This standard is very often referred to as the ‘baseline lane approach’ to the issue of managing information security. On the other hand the health information systems (HIS) are undergoing rapid development both in the number of installed systems as in the law and regulations governing HIS developments and deployment. The project was aimed at reviewing the AS/NZCS 4444 standard from the HIS requirements point of view. In this paper, we began with an overview of healthcare information systems (HIS) infrastructure in New Zealand and associated security issues around privacy and confidentiality, followed by a general review of the security baseline approach. We analyzed each clause of the AS/NZS 4444 with the information collected about technical and non-technical approaches to protecting HIS, consisting of a series of multi-case studies of healthcare organizations that collect, process, store and transmit electronic medical records. Finally, we proposed a new set of information security baselines based on the research to build an information security model for healthcare organizations. 
21|2||IFIP Technical Committee|
21|3|http://www.sciencedirect.com/science/journal/01674048/21/3|From the editor-in-chief|
21|3||Security Views|
21|3||It Was DÃ©jÃ  vu all Over Again|
21|3||Acceptance of Subscriber Authentication Methods For Mobile Telephony Devices|Mobile phones are now an accepted part of everyday life, with users becoming more reliant on the services that they can provide. In the vast majority of systems, the only security to prevent unauthorized use of the handset is a four digit Personal Identification Number (PIN). This paper presents the findings of a survey into the opinions of subscribers regarding the need for security in mobile devices, their use of current methods, and their attitudes towards alternative approaches that could be employed in the future. It is concluded that, although the need for security is understood and appreciated, the current PIN-based approach is under-utilized and can, therefore, be considered to provide inadequate protection in many cases. Surveyed users responded positively towards alternative methods of authentication, such as fingerprint scanning and voice verification. Based upon these findings, the paper concludes that a non-intrusive, and possibly hybrid, method of authentication (using a combination of techniques) would best satisfy the needs of future subscribers. 
21|3||On Bricks and Walls: Why Building Secure Software is Hard|
21|3||Anti-Terrorism Legislation: The Impact on The Processing of Data|Just before Christmas last year, and in response to last year’s terrorist outrage of 11 September, the Government drove its controversial Anti-terrorism, Crime and Security Act (ATCSA) 2001 through Parliament. This article reviews the impact of this legislation on the processing of data and explores the provisions which permit the authorities to access telecommunications data, to use and disclose information about financial matters, and to exchange information internationally. 
21|3||In brief|
21|3||Abstracts of Recent Articles & Literature|
21|3||Events|
21|3||International Board of Referees|
21|3||Guide for authors|
21|3||On the Security of Todayâs Online Electronic Banking Systems|Current technology is evolving fast and is constantly bringing new dimensions to our daily life. Electronic banking systems provide us with easy access to banking services. The interaction between user and bank has been substantially improved by deploying ATMs, phone banking, Internet banking, and more recently, mobile banking. This paper discusses the security of today’s electronic banking systems. We focus on Internet and mobile banking and present an overview and evaluation of the techniques that are used in the current systems. The best practice is indicated, together with improvements for the future. The issues discussed in this paper are generally applicable in other electronic services such as E-commerce and E-government. 
21|3||A Prototype for Assessing Information Technology Risks in Health Care|Although a vast number of risk-management methodologies have been proposed thus far and even though these methodologies are being applied to all types of organizations quite effectively, a few concerns are raised when the self-same risk-management methodologies are applied to the health-care environment. The authors, therefore, developed a risk-management methodology, entitled “Risk Management in Health Care — using cognitive fuzzy techniques” (RiMaHCoF), that is specifically tailored for the health-care environment. The methodology comprises five successive stages in all, namely initiation, domain analysis, risk assessment, risk analysis and domain monitoring. In the present paper, however, the authors will focus only on the third stage, viz. the risk assessment stage.This paper is principally aimed at expounding a prototype for the risk assessment stage, which prototype will incorporate cognitive fuzzy-logic techniques — as opposed to conventional techniques, such as annual-loss exposure (ALE) calculation — by means of which to assess the information-technology risks potentially to be incurred in the health-care domain. In this way, it will be ensured that human common sense and intuition (which form the basis of any risk assessment exercise) will not be omitted from the risk management process. 
21|3||IFIP technical committee 11|
21|4|http://www.sciencedirect.com/science/journal/01674048/21/4|The Sorry State of Law Enforcement|
21|4||Security views|
21|4||Why access control is difficult|
21|4||Security surveys spring crop|What does the latest round of IT security surveys have to tell us? Stephen Hinde samples, savours and pronounces. 
21|4||The Klez.H worm dissected|
21|4||The emergence of a comprehensive obligation towards computer security|Chris Pounder explains why organizations will, if they are not already so obliged, be required to maintain the security of their processing systems to independently established standards, and why organizations which fail to maintain such standards can expect to find themselves subject to legal sanctions. 
21|4||Syndicated crime and international terrorism: the lessons of â9â11â|
21|4||In Brief|
21|4||Digest of recent IT security press coverage|
21|4||Calendar of forthcoming conferences and events|
21|4||Guide for authors|
21|4||International Board of Referees|
21|4||Integrating Software Lifecycle Process Standards with Security Engineering|Since the advent and astronomical rise of the Internet and E-business, organizations must secure their computer systems or risk malicious attacks. While there have been several software lifecycle process standards (SLPS) for both military and industrial software development, their activities and deliverables are not yet integrated with security engineering (SE) activities. This lack of integration has created conflicts among the system development stakeholders (e.g. system acquirers and developers) during secure information systems development projects. This paper proposes an integration model that interweaves all the process activities and deliverables of SLPS with SE activities, taking IEEE/EIA 12207 as an example of SLPS. This model provides practical guidelines for the development of secure information systems while informing stakeholders how SLPS is related to the SE activities. 
21|4||The Development of Access Control Policies for Information Technology Systems|The identification of the major information technology (IT) access control policies is required to direct “best practice” approaches within the IT security program of an organisation. In demonstrating the need for security access control policies in the IT security program, it highlights the significant shift away from centralised mainframes towards distributed networked computing environments. The study showed that the traditional and proven security control mechanisms used in the mainframe environments were not applicable to distributed systems, and as a result, a number of inherent risks were identified with the new technologies.Because of the critical nature of the information assets of organisations, then appropriate risk management strategies should be afforded through access control policies to the IT systems. The changing technology has rendered mainframe centralised security solutions as ineffective in providing controls on distributed network systemsThis investigation revealed that the need for policies for access control of an information system from corporate governance guidelines and risk management strategies were required to protect information assets of an organisation. The paper proposes a high level approach to implementing security policies through information security responsibilities, management accountability policy, and other baseline access control security policies individual and distributed systems. 
21|4||An Efficient and Practical Solution to Remote Authentication: Smart Card|The smart card-based scheme is a very promising and practical solution to remote authentication. Compared with other smart card-based schemes, our solution achieves more functionality and requires much less computational cost. These important merits include: (1) there is no verification table; (2) users can freely choose their passwords; (3) the communication cost and the computational cost is very low; and (4) it provides mutual authentication between the user and the server. 
21|4||IFIP technical committee 11|
21|5|http://www.sciencedirect.com/science/journal/01674048/21/5|Taking a stand on hackers|What should our relationship with the hacking community be? You may remember a news item in last month’s ‘Security Views’ that concerned ‘ethical hacking’ and ‘benevolent hacking’. Recall, please, that a young man from India who once defaced a website and then told the owner how to close the site to attacks was later hired as a security consultant by a US Government agency. Then the ‘Dynamic Duo’ hacked the FAA to expose its vulnerability to external attack. Somehow, I cannot get these stories and the issues behind them out of my mind, so I’ll wrestle with them in this month’s editorial. The main issue to be addressed is the type and level of relationship with the hacking community we as information security professionals should maintain. 
21|5||Security Views|
21|5||Security crisis management â the basics|Of the more pervasive problems in any kind of security event is how the security event is managed from the inception to the end. There’s a lot written about how to manage a specific incident or how to deal with a point problem such as a firewall log, but little tends to be written about how to deal with the management of a security event as part of corporate crisis management. This article will focus on the basics of security crisis management and of the logical steps required to ensure that a security crisis does not get out of hand. 
21|5||Information security policy â what do international information security standards say?|One of the most important information security controls, is the information security policy. This vital direction-giving document is, however, not always easy to develop and the authors thereof battle with questions such as what constitutes a policy. This results in the policy authors turning to existing sources for guidance. One of these sources is the various international information security standards. These standards are a good starting point for determining what the information security policy should consist of, but should not be relied upon exclusively for guidance. Firstly, they are not comprehensive in their coverage and furthermore, tending to rather address the processes needed for successfully implementing the information security policy. It is far more important the information security policy must fit in with the organisation’s culture and must therefore be developed with this in mind. 
21|5||Much Ado About Nothing: Win32.Perrun|
21|5||Trusted â¦orâ¦ trustworthy: the search for a new paradigm for computer and network security1|On the occasion of the presentation of the Kristian Beckman Award for 2002 it is appropriate to pause and reflect on the state of computer and associated data network security at the start of the new millennium; appropriately in a country that itself pioneered the use of encryption some thousands of years ago. This paper sets out a number of major questions and challenges which include:•Just what is meant by ‘trusted’ or ‘trustworthy’ systems after 20 years of experience, or more likely, lack of business level experience, with the ‘trusted computer system’ criteria anyway?•Does anyone really care about the adoption of international standards for computer system security evaluation by IT product and system manufacturers and suppliers (IS 15408) and, if so, how does it all relate to business risk management anyway (IS 17799)?•With the explosion of adoption of the microcomputer and personal computer some 20 years ago, has the industry abandoned all that it learnt about security during the ‘mainframe era’; or — “whatever happened to MULTICS” and its lessons?•Has education kept up with security requirements by industry and government alike in the need for safe and secure operation of large scale and networked information systems on national and international bases, particularly where Web or Internet-based information services are being proposed as THE major “next best thing” in the IT industry?•Has the ‘fourth generation’ of computer professionals inherited the spirit of information systems management and control that resided by necessity with the last ‘generation’, the professionals who developed and created the applications for shared mainframe and minicomputer systems?Overall, this paper proposes that, like other industries before it, from the car industry to food to pharmaceuticals and drugs, the role of government in relation to its community care responsibilities cannot be ignored in societies that have now become dependent upon the safe, secure and reliable operation of computer systems and data networks; so-called “National Information Infrastructure Protection” or NIIP.Is it time for government to enact legislation that places firm and legally binding obligations on the information technology industry to create and distribute safe and secure products and systems; to be liable for deliberate, or even negligent, release of faulty software products and the like?For the computer professionals creating the necessary applications that enable such systems to be used, is it time for legal responsibility based around concepts of professional conduct and education?Finally, in relation to specialized information security professionals acting to manage such critical information systems, is it time for government registration of those IT security professionals and, if so, just what has to be the base levels of education and training needed for a person to act as such? Unlike other industries, some professionals given charge of IT security may have had little more INFOSEC education than “buying a copy of Bruce Schneier’s book” (an actual statement made to the author in relation to IT security training in a business situation and the nomination of an IT manager as the ‘security’ manager). 
21|5||Enterprise in focus at NetSec 2002|NetSec 2002 took place in San Francisco, amid industry reflection on the balance to be struck between combatting cyber-terrorism and safeguarding civil liberties post-9-11. Brian McKenna reports on the punditry and the pedagogy at the CSI’s event, focusing on security in the enterprise. 
21|5||The perils of privacy|
21|5||Digest of recent IT security press coverage|
21|5||In brief|
21|5||Events|
21|5||Guide for Authors|
21|5||International Board of Referees|
21|5||Use of K-Nearest Neighbor classifier for intrusion detection1|A new approach, based on the k-Nearest Neighbor (kNN) classifier, is used to classify program behavior as normal or intrusive. Program behavior, in turn, is represented by frequencies of system calls. Each system call is treated as a word and the collection of system calls over each program execution as a document. These documents are then classified using kNN classifier, a popular method in text categorization. This method seems to offer some computational advantages over those that seek to characterize program behavior with short sequences of system calls and generate individual program profiles. Preliminary experiments with 1998 DARPA BSM audit data show that the kNN classifier can effectively detect intrusive attacks and achieve a low false positive rate. 
21|5||Steganographic Method for Secure Communications|Cryptographic methods secure an important message by encrypting it to an unrecognized form of data which may arouse the interest of cryptanalysis for part of the recipients. Steganographic methods hide the encrypted message in cover carriers so that it cannot be seen while it is transmitted on public communication channels such as computer network. Many steganogrphic methods embed a large amount of the secret information in the first k LSBs of the pixels of the cover images. Because of the imperfect sensibility of the human visual system, the existence of the embedded secret information can be imperceptible. Unfortunately, the hidden secret information may be discovered by the common-cover-carrier attack if it has not been appropriately disposed. In this paper, an LSB-based steganographic method is proposed to resolve this problem. By using variable-size insertion and redundant Gaussion noise adding, the stego-images created with the proposed method can survive both the human visual system and the common-cover-carrier attack. Moreover, many cryptographic protocols are involved in the proposed method to resolve the problems of security and key management that may be encountered in other steganogrpahic methods. The proposed method is hence suitable for secure communications. 
21|5||The Open Source approach â opportunities and limitations with respect to security and privacy*|Today’s software often does not fulfil basic security or privacy requirements. Some people regard the Open Source paradigm as the solution to this problem. First, we carefully explain the security and privacy aspects of Open Source, which in particular offer the possibility for a dramatic increase in trustworthiness for and autonomy of the user. We show which expectations for an improvement of the software trustworthiness dilemma are realistic. Finally, we describe measures necessary for developing secure and trustworthy Open Source systems. 
21|5||IFIP Technical committee 11|
21|6|http://www.sciencedirect.com/science/journal/01674048/21/6|From the editor-in-chief: The Revenger's Tragedy|
21|6||Security Views|
21|6||The Blue Screen of Death and other deadly bugs|
21|6||I-Worm.Lentin (aka Yaha)|
21|6||The USâs National Strategy for Homeland Security|This article briefly reviews the National Strategy which defines the US’s approach to counter future terrorist activity. In the context of computer security and privacy, the chapters on ‘Protecting Critical Infrastructure’, ‘Information Sharing and Systems’ and ‘Law’ are particularly relevant. The Strategy is also important, because all western democracies will be considering the same counter-terrorism imperatives, and are likely to consider similar security enhancing mechanisms as proposed in the Strategy. 
21|6||Policy enforcement in the workplace|It is well known, at least among true security professionals, that formal policy is a prerequisite of security. While many organizations have security policy of varying types, having policy and being able to enforce it are totally different things. This writing looks at the importance of formal security policy, and then presents and discusses a new set of tools that provide a ready method for assuring critical policy enforcement in the workplace. 
21|6||White collar crime: a handmaiden of international tech terrorism|
21|6||Palladium, fraud, and surviving terrorism â Compsec 2002: Preview of Compsec 2002, 30 Octâ1 Nov, Queen Elizabeth II Conference Centre, Westminster, London, UK|This year’s Compsec addresses security issues brought to the fore by the terrorist attacks and frauds that have rocked the US in the last year. Sarah Hilley, Editor of Computer Fraud & Security, and Network Security previews the event. 
21|6||A framework for understanding and predicting insider attacks|In this paper an insider attack is considered to be deliberate misuse by those who are authorized to use computers and networks. Applying this definition in real-life settings to determine whether or not an attack was caused by an insider is often, however, anything but straightforward. We know very little about insider attacks, and misconceptions concerning insider attacks abound. The belief that “most attacks come from inside” is held by many information security professionals, for example, even though empirical statistics and firewall logs indicate otherwise. This paper presents a framework based on previous studies and models of insider behavior as well as first-hand experience in dealing with insider attacks. This framework defines relevant types of insider attack-related behaviors and symptoms—“indicators” that include deliberate markers, meaningful errors, preparatory behaviors, correlated usage patterns, verbal behavior and personality traits. From these sets of indicators, clues can be pieced together to predict and detect an attack. The presence of numerous small clues necessitates the use of quantitative methods; multiple regression equations appear to be a particularly promising approach for quantifying prediction. 
21|6||Digest of recent IT security press coverage|
21|6||in brief|
21|6||Events|
21|6||Refereed papers|
21|6||Principles and requirements for a secure e-voting system|Electronic voting (e-voting) is considered a means to further enhance and strengthen the democratic processes in modern information societies. E-voting should first comply with the existing legal and regulatory framework. Moreover, e-voting should be technically implemented in such a way that ensures adequate user requirements. As a result, the aim of this paper is twofold. Firstly, to identify the set of generic constitutional requirements, which should be met when designing an e-voting system for general elections. This set will lead to the specific (design) principles of a legally acceptable e-voting system. Second, to identify, using the Rational Unified Process, the requirements of an adequately secure e-voting system. These requirements stem from the design principles identified previously. The paper concludes that an e-voting capability should, for the time being, be considered only as a complementary means to the traditional election processes. This is mainly due to the digital divide, to the inherent distrust in the e-voting procedure, as well as to the inadequacy of the existing technological means to meet certain requirements. 
21|6||University systems security logging: who is doing it and how far can they go?|The importance of providing a secure environment for individual and corporate data, research, and communications has grown to critical proportions as more of the mission and business of colleges and universities is carried out over networked information infrastructures. System administrators must implement new, more extensive processes to protect data, to identify and eliminate vulnerabilities, and to find and manage abuses of the systems they manage. They have responded by increasing the network and major systems logging and monitoring efforts and want to do more. But how far can they go before their logging for the sake of security becomes surveillance and a violation of student record privacy under the Family Educational Rights and Privacy Act (FERPA)? What systems are they logging? How are they managing logs? What training have they had to support their work in the areas of security and data management? What processes are in place to manage log data from unauthorized access?In 2001, researchers at the University of Michigan, funded by the National Science Foundation, explored these questions. The results of this study raise important questions for security professionals and systems developers. This paper provides information about the sample’s system administrators, designated as logging experts on campuses, about their training in security and information protection. It discusses the types of logging that administrators are doing and the data it yields. The paper raises issues about how far administrators can go before they violate student record privacy law and makes recommendations for needed actions. 
21|6||Hierarchical access control based on Chinese Remainder Theorem and symmetric algorithm|This paper presents an improvement in a cryptographic key assignment scheme that is proposed by Kuo et al. Considering the problem of dynamic access control in a user hierarchy, such an improvement focuses on the ability to implement easily. Moreover, this improvement is an attempt to greatly reduce the computation time and the storage size required by the key generation and the key derivation phases in Kuo’s scheme. 
21|6||IFIP technical committee 11|
21|7|http://www.sciencedirect.com/science/journal/01674048/21/7|The US Governmentâ bigger and better information security?|
21|7||Security Views|
21|7||Note from the Publishers|
21|7||Spam, scams, chains, hoaxes and other junk mail|
21|7||Bugbear|
21|7||Trends in academic research: vulnerabilities analysis and intrusion detection|
21|7||Managed Security Services â new economy relic or wave of the future?|Is IT security ready to go the way of physical security? Should it be done in-house, or should corporates start eating out? Brian McKenna takes some soundings. 
21|7||Vulnerabilities categories for intrusion detection systems|
21|7||Security policy update|This article reviews the recent spate of public policy initiatives which will impact on an organization’s approach towards the security of its data. 
21|7||Technology and Electronic Communications Act 2000|
21|7||Digest of recent IT security press coverage|
21|7||In brief|
21|7||Events|
21|7||International Board of Referees|
21|7||Guide for authors|
21|7||Cyberterrorism?|The term cyberterrorism is becoming increasingly common in the popular culture, yet a solid definition of the word seems to be hard to come by. While the phrase is loosely defined, there is a large amount of subjectivity in what exactly constitutes cyberterrorism. In the aftermath of the September 11th attacks, this is somewhat disconcerting. In an attempt to define cyberterrorism more logically, a study is made of definitions and attributes of terrorism and terrorist events. From these attributes a list of attributes for traditional terrorism is developed. This attribute list is then examined in detail with the addition of the computer and the Internet considered for each attribute. Using this methodology, the online world and terrorism is synthesized to produce a broader but more useful assessment of the potential impact of computer-savvy terrorists. Most importantly, the concept of ‘traditional’ cyberterrorism, which features the computer as the target or the tool is determined to be only a limited part of the true risk faced. Finally, the authors discuss the impact this new view of cyberterrorism has on the way in which one should build one’s defenses. In particular, the breadth of the issue poses significant questions for those who argue for vertical solutions to what is certainly a horizontal problem. Thus, the validity of special cyberterrorism task forces that are disconnected or loosely connected with other agencies responsible for fighting the general problem of terrorism is questioned, and a broader, more inclusive method suggested. 
21|7||Applying digital rights management systems to privacy rights management|DisclaimerThe views expressed by the authors of this article are their own and may not necessarily be taken to be those of either the Dutch Data Protection Authority or the National Research Council of Canada.AbstractWhile there are growing concerns about how to manage citizen privacy, currently there are no established technology solutions that meet the privacy needs required in some cases by legislation. In this paper we examine the prospect of adapting systems developed for Digital Rights Management to meet the challenges of Privacy Rights Management. In particular, the goal of this work is the adaptation of DRM technology to produce a privacy management architecture that reflects the requirements of Directive 95/46/EC for the protection of personal data. This paper first outlines the requirements for management of the personal data within the European Community it then describes the changes that would be required to transform a digital rights management system into a system to manage the handling of personal data. The paper concludes with a thorough discussion of the issues and potential of this approach. 
21|7||An enhancement of timestamp-based password authentication scheme|Yang and Shieh proposed a timestamp-based password authentication scheme. Chan and Cheng proved that it is insecure. In this paper, we will give a further cryptanalysis of the scheme, and give an easier attack on it. Finally, we will propose an improved scheme that can withstand both of the attacks. Compared to other authentication schemes, this improved scheme allows the host to authenticate a user only with his login request. The host need not keep any secret or information of the user. 
21|7||IFIP technical committee 11|
22|1|http://www.sciencedirect.com/science/journal/01674048/22/1|From the editor-in-chief|
22|1||Security views|
22|1||Time cost$ money|
22|1||Reseacrh in cryptography and security mechanisms|
22|1||Computer forensics|
22|1||Security in a Flash|A new class of devices has started appearing on the market in large numbers. They are about the size of a typical house key, weigh next to nothing, plug in to USB ports, and provide from 8 megabytes to several gigabytes of removable and easily transportable storage. This article looks at the possible security benefits of such devices. 
22|1||A tangled Web of libel lies?|
22|1||UK police promise charter to guard good names|
22|1||LIRVA virus|
22|1||Events|
22|1||Guide for Authors|
22|1||International Board of Referees|
22|1||Efficient anomaly detection by modeling privilege flows using hidden Markov model|Anomaly detection techniques have been devised to address the limitations of misuse detection approaches for intrusion detection with the model of normal behaviors. A hidden Markov model (HMM) is a useful tool to model sequence information, an optimal modeling technique to minimize false-positive error while maximizing detection rate. In spite of high performance, however, it requires large amounts of time to model normal behaviors and determine intrusions, making it difficult to detect intrusions in real-time. This paper proposes an effective HMM-based intrusion detection system that improves the modeling time and performance by only considering the privilege transition flows based on the domain knowledge of attacks. Experimental results show that training with the proposed method is significantly faster than the conventional method trained with all data, without loss of detection performance. 
22|1||A quantitative study of Public Key Infrastructures|Public Key Infrastructures have not reached the widespread diffusion expected of them, although they are well understood from a security point of view, because, like many say, the killer application has not been found yet. The lack of a clear understanding of the performance of these systems also contributes significantly to their limited diffusion. Studies have appeared of specific aspects of the operations of PKIs, but no complete studies of the overall system are known.In this paper we present an evaluation study of X.509-compliant Public Key Infrastructures using queuing network models. We focus our analysis on the performance of the subsystem in charge of generating and managing digital certificates, under a variety of load conditions, both in terms of the type of requests and their number. We also investigate the impact on the performance of the system of some implementation choices such as revocation mechanisms and auditing activities. The main result of our analysis is that the system we consider, given the current state of technology, can guarantee acceptable response time in steady state even in the presence of PKI with a consistent number of users. However, in order to guarantee such a performance level, throughput must not exceed 3.5 requests per second, where a request can be a certificate generation or revocation request. Such a limitation hinders the deployment of PKIs with large numbers of users, since recovering after a system compromise may require an unacceptable amount of time. 
22|1||A password authentication scheme with secure password updating|Recently, Hwang and Yeh proposed an improvement on the Peyravian-Zunic password scheme. The Hwang-Yeh scheme comprises a password authentication protocol, a password change protocol, and can also provide key distribution. Though the Hwang-Yeh scheme repaired several security problems of the Peyravian-Zunic scheme, it has several security problems: the password change protocol in the Hwang-Yeh scheme is vulnerable to a denial of service attack; and it does not provide the forward secrecy property in session key distribution. Furthermore, we shall fix the Hwang-Yeh scheme to avoid these problems. 
22|1||IFIP technical committee 11|
22|2|http://www.sciencedirect.com/science/journal/01674048/22/2|Internet security: whatâs in the future?|Only seven or eight years ago the Internet was still more or less a novelty to many commercial organizations (particularly those outside North America). Many used it sparingly (mostly for email communications); a few others used it for E-commerce and other functions that the Internet was making possible. E-commerce was at the time being sold as a panacea for business; emerging technology such as application firewalls, password filters, and intrusion detection tools seemed to hold great promise for Internet security. 
22|2||Security Views|
22|2||The law, cybercrime, risk assessment and cyber protection|
22|2||The circumstances of seizure|IntroductionIn our last column we discussed the basics of computer forensics, trying to describe a basis for validating evidence captured using well-documented and accepted methods and tools. This is still a very new discipline and precedents are few and perhaps there will be many yet to come before findings based on it are accepted as readily as fingerprint evidence is today. Nevertheless, I believe that electronic forensic evidence gathering will become a commonly used investigative technique of ever increasing importance.This column is devoted to the process of seizure. On the surface it may seem not all that complex, however, every circumstance is different and therefore so too may be the complexities of any given seizure. 
22|2||Implementing enterprise security: a case study3|IntroductionInformation is an essential asset for organizations, because it supports the day-today operations, and facilitates decision-making by the organization’s key stakeholders. The challenge facing organizations is how to provide access to this asset without compromising its integrity. This asset is received and distributed by the organization through various distribution channels, which is connected together by the telecommunications network. These channels include:•Email•Internet•Applications (e.g. Financial, Logistics, Retail, Property and Construction, Energy etc.)•DBMS (MS SQL Server, Oracle, DB2, Sybase, etc.)•Operating systems (e.g. Unix, NT/Windows 2000, etc.) 
22|2||Security with unfortunate side effects|
22|2||The economic impact of war with Iraq â asymmetric risks|Concern about war between some Western countries and Iraq has mounted in recent weeks. War with Iraq may have important economic consequences as well as political and security-related consequences. First-order consequences might include increased oil prices and also higher defence expenditure at a time when it appears that tax revenues will be rising much more slowly than government spending. There will also be second order consequences. These will depend on the outcome of war and on whether war achieves its objective of bringing peace and stability to the region or, in fact, makes the region less stable. 
22|2||Safeguards for IT managers and staff under the Sarbanes-Oxley Act|
22|2||Events|
22|2||Computers & Security Volume 21 â Author Index|
22|2||Computers & Security Volume 21 â Subject Index|
22|2||International Board of Referees|
22|2||Guide for Authors|
22|2||IP Traceback using header compression|Denial-of-service and other malicious attacks have become increasingly prevalent in recent years. A major issue hindering the ability to trace attacks to their sources is the ease of IP address spoofing which conceals the attackers’ identity. Several techniques, generically named IP traceback, have been proposed to enable tracing of IP packets from destination to source despite IP spoofing. In this paper, we propose a Simple, Novel IP Traceback using Compressed Headers (SNITCH) that is based upon Probabilistic Packet Marking (PPM). This technique employs header compression to increase the number of bits available for insertion of traceback information. Simulations performed on empirical data have shown that 100% of the attack paths can be determined with a maximum of 0.43% false positive paths. 
22|2||Dealing with contextual vulnerabilities in code: distinguishing between solutions and pseudosolutions|Vulnerabilities in objects in various operating systems or add-ons continue to surface at a rapid rate, posing a unique security problem, one with which vendors appear to be struggling. Patching a vulnerability discovered in a default system binary, such as the highly publicized sendmail debug vulnerability (this vulnerability has been discussed extensively in the literature and was even exploited in the infamous Internet Worm [1]), is relatively easy. The vendor often simply issues a new version of the binary to replace the vulnerable one. The interface for all applications that invoke this binary remains the same. However, with componentized code, such as in modern object-oriented systems, things do not work quite as smoothly. For example, how should vulnerabilities be patched if an object is vulnerable to attack only if it is used in a certain context, or if only one function out of many is vulnerable? Patching the vulnerability is simple if a function can be replaced. If the vulnerability is contextual and the function has legitimate uses in other areas, however, replacing the function altogether may be inappropriate. What kinds of alternative remedies are appropriate? This paper presents several different approaches to dealing with this difficult problem, and analyzes the strengths and weaknesses of each. Of all the solutions considered, removing code altogether and adding warnings at run time are the least viable. Allowing code to run only if the execution context is correct, permitting only certain callers to execute code, barring certain callers from executing code, and using access control lists to govern access to objects and methods are more reasonable approaches, although each also has limitations. 
22|2||A flexible date-attachment scheme on e-cash|The electronic cash payment system is an important technique that has been utilized for electronic commerce. In this paper, we propose a flexible date attaching method to Chaum’s electronic cash scheme, which can also be applied to many other electronic cash schemes. The date attaching method provides an unforgeable and variable scheme to deal with the effective date of e-cash that will allow for many applications. The customer can modify the date slip but it does not become effective until the merchant passes it on to the bank. Also, the usage of this proposed scheme is very flexible, unlike most e-cash schemes that embed an expiration date when e-cash is withdrawn. This scheme still possesses the untraceability property and the computation complexity for customers and merchants is simple when compared to bank operations. 
22|2||IFIP technical committee 11|
22|3|http://www.sciencedirect.com/science/journal/01674048/22/3|From the editor-in-chief: Virus and worm trends|I’ll remember the evening of Friday, 24 January for a long time. I had just gotten to sleep when I was awakened by my emergency pager. I jumped out of bed, headed for the nearest phone, and called the number that was displayed on my pager. I reached the person who had initiated the page soon afterwards and learned that a new worm, the ‘MS-SQL Slammer worm’ (which also has other names such as the ‘Sapphire worm’) had struck. Advised that the worm was generating a huge amount of network traffic and that it was targeting port 1434 of Windows systems, I had to make several decisions concerning what to do next. After searching for information about the worm and notifying others with whom I work (as well as CERT/CC, something that unfortunately proved futile) shortly thereafter, I once again went to bed, only to have to deal with the proverbial fallout this worm had created over the next few days. 
22|3||Security views|
22|3||Cyber-terrorism in context|Cyber-terrorism has been much discussed in the media, especially at times of heightened international tension. The Institute for Security Technology Studies has tracked previous attacks and, using conflicts such as the Israel-Palestine and India-Pakistan as examples, shows there is a strong correlation between political and military conflicts and the incidence of cyber-terrorism. Research group IDC has recently predicted that in the next year we can expect to experience a cyber-terrorist attack as a result of a war on Iraq, resulting in short term economic disruptions. 
22|3||Evidence acquisition|In our last column we discussed the seizure process, stressing the important elements of planning, methodology, interviewing, documentation and thoroughness. The acceptability of evidence gathered will be influenced and possibly accepted or refuted as a result of carrying out the tasks described by these elements in a professional and methodical way. Each step by itself cannot make a successful case, however, any step that is flawed or missed may cause a case to fail.This column is devoted to the acquisition process. This activity, when done properly, forms the basis for further analysis of data and provides the potential for capturing relevant evidence. The methodologies followed during the acquisition process are critical to the validity of any evidence found. In addition, following a formal plan will ensure that all relevant data is captured for later analysis. 
22|3||International legal aspects of cryptography: Understanding cryptography|Historically, the legal systems in different countries have had to adapt to the advances in technology. For instance, the appearance of automobiles as a popular means of transportation led in the last century to the creation of driving laws and traffic police; in the telecommunications field, telephone networks have made their way into local legislation, with implementations such as government security agencies able to perform surveillance activities through wiretapping of telephone communications. The phenomenal advance of computer technology and network communications, most clearly exemplified by the reach and popularity of the Internet today, is gradually finding its way within legal frameworks throughout the world. However, the constantly evolving environment and complex technical issues involved in this area make it a particularly hard subject to approach legally, requiring from lawmakers both expertise and up-to-date knowledge in order to legislate logically and usefully. In this context, cryptography is only one of numerous computer-based technologies in widespread use [1], but it is a mathematically intricate and often misunderstood area, and thus one of the most difficult to include in legal frameworks. Furthermore, attempts to regulate cryptography, most predominantly when some kind of limit or control is involved, have originated heated debates in recent times, and the delicate balance between national security and individual freedoms has proven to be extremely hard to achieve in this particular field. 
22|3||A contest to evaluate IT security services management|This article discusses a project that used a multi-team competition to define, test and validate the added value and costs of a premium level of ‘managed security services’. The services were intended for a limited number of servers used to store and process extremely sensitive information on a large IT infrastructure. They were defined by a specialist third party managed security services (MSS) provider. They included recommended server configuration and intrusion detection software, as well as monitoring services.The project contest was structured to benchmark the risks and controls related to the existing level of service, and to then determine the added value, effectiveness, and cost alternatives for an increased level of service. The company’s infrastructure group and a MSS provider were to be defenders of specific servers for a sensitive application. Prior to the contest, the protected application servers were hardened by each defender. The servers and the application were then attacked by an independent third party professional hacker team.The overall conclusion was that the study approach provided a good way to evaluate information risks, control requirements, and the cost(s) of alternative solutions to meet those requirements by using a combination of company resources and an external supplier(s). It also provided a very effective means to stimulate staff interest and obtain senior management attention and support. 
22|3||Governments act to improve security|This article reviews the final versions of “The National Strategy to Secure CyberSpace” [1] and “The National Strategy for the Physical Protection of Critical Infrastructures and Key Assets” [2] which together define the US’s approach to maintain continuity of IT infrastructure and systems which could be disrupted by terrorist activity. In this context, the article also reviews the UK’s Treasury Department Green Paper “The Financial System and Major Operational Disruption” [3] as this indicates that the UK Government has been influenced by the strategies and methods adopted by the Bush Administration. The US’s approach is likely to be followed by other advanced economies when deciding how to secure their essential electronic infrastructure. 
22|3||Events|
22|3||International Board of Referees|
22|3||Guide for authors|
22|3||Analysis of vulnerabilities in Internet firewalls|Firewalls protect a trusted network from an untrusted network by filtering traffic according to a specified security policy. A diverse set of firewalls is being used today. As it is infeasible to examine and test each firewall for all possible potential problems, a taxonomy is needed to understand firewall vulnerabilities in the context of firewall operations. This paper describes a novel methodology for analyzing vulnerabilities in Internet firewalls. A firewall vulnerability is defined as an error made during firewall design, implementation, or configuration, that can be exploited to attack the trusted network that the firewall is supposed to protect. We examine firewall internals, and cross-reference each firewall operation with causes and effects of weaknesses in that operation, analyzing twenty reported problems with available firewalls. The result of our analysis is a set of matrices that illustrate the distribution of firewall vulnerability causes and effects over firewall operations. These matrices are useful in avoiding and detecting unforeseen problems during both firewall implementation and firewall testing. Two case studies of Firewall-1 and Raptor illustrate our methodology. 
22|3||A model for deriving information security control attribute profiles|How does an organization ensure that all information security loopholes are covered? This paper describes a possible solution in terms of an Information Security Control Attribute Profile for an organization. This profile will dictate attributes that should accompany each and every information security control in an organization, thus minimizing the likelihood of malfunctioning controls.It is up to the organization to investigate the best way of implementing information security for itself. This is usually done by the implementation of information security controls in the organization. The paper does not suggest which controls to implement, as the literature provides standardized methods for choosing from lists of controls. Rather, the paper suggests which attributes should support every control in an organization.The organization will be able to derive a set of attributes that should accompany every information security control. The process that should be followed, in order to derive the optimal set of control attributes, is described in a model and presented in this paper. The derived set of control attributes will be called the Information Security Control Attribute Profile for the organization. 
22|3||Generalization of proxy signature-based on discrete logarithms1|In the past few years, many excellent studies on proxy signature schemes have been published. Yet, traditional proxy signature schemes are mainly aimed at dealing with one or two separate proxy conditions each. In this article, the authors shall present a generalized version of the (t1/n1–t2/n2) proxy signature scheme based on the discrete logarithms that can be applied to every proxy situation. The (t1/n1–t2/n2) proxy signature scheme allows the original group of original signers to delegate their signing capability to a designated proxy group. The proxy group of proxy signers can cooperatively generate the proxy signature on behalf of the original group. Any verifier can verify the proxy signature on the message with the knowledge of the identities of the actual original signers and the actual proxy signers. Furthermore, some possible attacks have been considered, and our security analysis shows that none of them can successfully break the proposed scheme. 
22|3||Fault trees for security system design and analysis|The academic literature concerning fault tree analysis relates almost entirely to the design and development of safety-critical systems. This paper illustrates how similar techniques can be applied to the design and analysis of security-critical systems. The application of this technique is illustrated in an example inspired by a current public-key cryptosystem. 
22|3||IFIP technical committee 11|
22|4|http://www.sciencedirect.com/science/journal/01674048/22/4|Why canât Microsoft stay out of the InfoSec headlines?|Despite all the recent information security-related events — significant privacy infringements, the many new, serious vulnerabilities discovered in various operating systems and applications, recent legal rulings, convictions, and/or acquittals, controversial legislation, and many other significant events, Microsoft seems to always be in the information security headlines. Other vendors such as Oracle also make the headlines, but usually only briefly (such as when Oracle’s CEO pronounced his company’s software “hack proof”). Why does Microsoft seem to constantly be in the spotlight? Consider recent Microsoft-related events: 
22|4||Attackers hit Web hosting servers|An unknown attacker broke into a Web hosting server belonging to Bargainhost.co.uk and obtained passwords for 1500 hosted websites. Several of Bargainhost’s customers’ websites were defaced afterwards. Worse yet, the attacker also damaged backups needed to rebuild the sites. Bargainhost reported that it was attempting to manually restore the sites, but could not estimate when it would be able complete this task. Meanwhile, Bargainhost urged its customers to change their passwords. Several of Bargainhost’s customers have complained that the company has been remiss in fixing the problem; many have discontinued using Bargainhost’s Web hosting service as a result of the break-in. After losing nearly seven months of emails, contacts, and forums, snowboarding site Powderroom.net has decided to move its Web hosting business elsewhere despite still having a two-year maintenance contract with Bargainhost. Powderroom’s owner also stated that his company will from now on make its own Web server backups rather than rely on a Web hosting service provider. 
22|4||Careless about privacy|At the end of last year the FBI broke up a New York fraud ring which cost US consumers upwards of $2.7 million in fraudulent transactions. It was the largest ever case of identity theft, affecting over 30â000 victims. The perpetrators were able to obtain confidential passwords and codes enabling them to download victims’ credit reports via the Internet and sell the information on at $30 each to criminals, who were able to impersonate the victims and run up fraudulent debts. Passwords are often the weakest link in terms of protection. 
22|4||Evidence analysis|In our last column we discussed the evidence acquisition process stressing the chain of evidence and the importance of following a documented method. These two issues are important in order to ensure that the evidence captured will be acceptable in a court of law — provided that it is relevant to the case at hand. 
22|4||The role of criminal profiling in the computer forensics process|In today’s increasingly complex world, we find ourselves at a rather unique societal and cultural cross roads. At no other time in history has society been so dependent on technology and its various offshoots and incarnations1. Almost every facet of our day-to-day lives is impacted to some extent by technology (e.g., email, Internet, online banking, digital music, etc.). This reliance and to some extent dependence on technology, has had a ripple effect on other less obvious areas of society (,  and ). One such area is law enforcement and, more specifically, criminal investigations (Kruse and Heiser, 2002). Historically, criminal investigations relied on such concepts as physical evidence, eyewitnesses, and confessions. Today, the criminal investigator must recognize that a vast amount of evidence will be in the electronic or digital form. The crime scene may consist of a computer system or network as opposed to the traditional ‘physical’ scene (Kruse and Heiser, 2002). The eyewitness of today and tomorrow may be a computer generated ‘log file’. 
22|4||A taxonomy for information security technologies|The Internet is a public network, which is open and used by all — also for communicating private information. “But private information should be secured!”, I hear you say. Yes. But where should one start looking for help when attempting to secure private information? This paper discusses a taxonomy for information security technologies, which provides information on current state-of-the-art technologies used to secure information at application, host and network level. 
22|4||Why we need a new definition of information security|There is an old Peanuts strip where Charlie Brown says, “Working here is like wetting your pants in the pool, wearing a dark bathing suit. You get that warm feeling but nobody notices.” Increasingly, I think computer security professionals in large enterprises are in that metaphorical swimming pool. In fact, many are swimming in the deep end without their water wings. When computer security professionals do an excellent job protecting systems and information, the number of bad outcomes decreases. After a generation of peace, pretty soon people start asking why we need the army. I believe this problem stems in part from a fuzzy fundamental: the definition of information security. 
22|4||Events|
22|4||International Board of Referees|
22|4||Guide for authors|
22|4||Methods for preventing unauthorized software distribution|In this paper we present algorithms for protecting software from unauthorized installation. We assume that the user buys software on a disk or downloads it from the Internet — although our methods are not limited to protecting software under these circumstances. We consider two kinds of adversaries. One kind of attacker is a sophisticated hacker who can monitor a line and can read and intercept any information flowing unprotected over the Internet. These attackers are also skillful programmers who can analyze the software, locate any data of interest to them and also write and execute any programs, even the most complicated ones. Another kind of attacker is an average attacker who can copy and use personal or business software. 
22|4||Is the mouse click mighty enough to bring society to its knees?|Over the years, we have created an information infrastructure, with most of us connected but nobody totally responsible, which is easily targeted for attacks by adversaries. A series of deadly viruses and denial-of-service attacks are warnings of the fragile state of information security. Information warfare (IW) is a serious concern as it has no border and operates in a different realm. This paper articulates how IW can devastatingly attack the critical information infrastructure. Two aspects of IW are considered, viz., Defensive IW and Offensive IW. Currently, security solutions lag far behind the potential threats. Loosely coupled defenders cannot avert the damage caused by orchestrated coordinated attacks. Society is locked up in a vicious circle of wits and resources. This situation is likely to continue and we need to be proactive and make progress on a rocky path. It is time we got battle space ready. Without the coordinated efforts of government, disparate groups and organisations, society is one click away from grave danger of exploitation and dominance. 
22|4||An integral framework for information systems security management|Business use of Internet has exposed security as one of the key-factors for successful online competition. Contemporary management of E-business security involves various approaches in different areas, ranging from technology to organizational issues and legislation. These approaches are often isolated, while management of security requires an integrated approach. This article presents an attempt at management of E-business systems security that is based on integrating existing approaches in a balanced way. To foster practical use of the conceptual model in this paper, brief background knowledge in related areas is given. 
22|4||IFIP technical committee 11|
22|5|http://www.sciencedirect.com/science/journal/01674048/22/5|Pandoraâs Box: spyware, adware, autoexecution, and NGSCB|The increased sophistication of today’s computing environment has produced many benefits — better performance, considerably more sophisticated graphics and functionality, greater reliability, and others. At the same time, however, something insidious is occurring. Spyware and adware programs are being deliberately included with other legitimate software packages to deliver functionality that users do not want or need. 
22|5||Security Views|A Windows worm known as W32.Sobig.B (but also as the Palyh and the Mankx worm) is arriving in the form of an attachment in messages that appear to be from Microsoft support. Sobig.B creates and then sends messages to addresses it finds in address books of systems it infects. The indicated address of the sender is support@microsoft.com. Subject lines vary, but ‘Screensaver’, ‘Cool Movie’, ‘Re: My application’, ‘Approved (Ref: 38446-263)’, and ‘Your password’ are frequently used. The name of the attachment that contains this worm has a .pif file extension, but the actual name varies. ‘movie28.pif’, ‘screen_temp.pif’, ‘doc_details.pif’, ‘ref-394755.pif’, and ‘password.pif’ are common attachment names. If the recipient of an infected message that Sobig.B sends opens the attachment, the recipient’s system becomes infected. Once the system is infected, Sobig.B creates a Registry entry that causes this worm to be started up whenever the infected system boots. 
22|5||Privacy legislation: a comparison of the US and European approaches|We are living in an increasingly connected world, not only in the office, but at home; a world in which more and more personal data is being stored in huge databases. With the increasing demands for personal data, often far in excess of the data required for the transaction in hand, so too does the reluctance to disclose such personal information. There is a perception that our personal information gets translated into unwanted direct marketing; that the most obvious manifestation of the e-connected world is of ever increasing amounts of spam, often from organisations that we have never heard of. This in turn has fuelled an awareness of the need to protect personal information. 
22|5||Encountering encryption|In our last column we took a look at analysing evidence. There are lots of other issues to consider in the analysis process that, by the nature and brevity of this column were not addressed. However, no one expects that after reading a few narrowly focused columns on electronic forensics to have become an electronic forensics expert or practitioner. There’s a bit more to it than that.This column will raise some of the issues and problems faced by investigators who encounter data encryption as part of a specific investigation. We’ll discuss some administrative remedies. Then we’ll focus on some other more proactive techniques that can be attempted in the event that the administrative procedures fail to produce results. 
22|5||The 419 scam: information warfare on the spam front and a proposal for local filtering|The infamous ‘419’ advance fee fraud scam is a major source of spam which takes its victims for hundreds of millions of dollars yearly. Email headers confirm that much of this traffic currently comes out of Lagos and Amsterdam. 419ers also create phony bank websites. Ad hoc information warfare has begun in which anti-scammers hijack these websites as well as scammers’ email accounts. Spam legislation will not stop these particular emails. Filtering outgoing mail, as close as practical to the source, could suppress that portion of the fraud carried out by email, at least in the short term. 
22|5||Leading attackers through attack graphs with deceptions|This paper describes a series of experiments in which specific deceptions were created in order to induce red teams attacking computer networks to attack network elements in sequence. It demonstrates the ability to control the path of an attacker through the use of deceptions and allows us to associate metrics with paths and their traversal. 
22|5||Next generation security for wireless: elliptic curve cryptography|Scott Vanstone, from Certicom, polemicizes for elliptic curve cryptography. He advances his company's view that ECC is the next generation of public-key cryptography for wireless. 
22|5||SSL Virtual Private Networks|Andrew Harding, technical director of clientless VPN provider Neoteris, argues the case for SSL VPNs. 
22|5||Events|
22|5||International Board of Referees|
22|5||Utilising fuzzy logic and trend analysis for effective intrusion detection|Computer security, and intrusion detection in particular, has become increasingly important in today’s business environment, to ensure safe and trusted commerce between business partners as well as effective organizational functioning. Various approaches to intrusion detection are currently being utilized, but unfortunately in practice these approaches are relatively ineffective and inefficient. New means and ways that will minimize these shortcomings must, therefore, continuously be researched and defined. This paper will propose a proactive and dynamic approach, based on trend analysis and fuzzy logic that could be utilized to minimize and control intrusion in an organization’s computer system. 
22|5||A new taxonomy of Web attacks suitable for efficient encoding|Web attacks, i.e. attacks exclusively using the HTTP/HTTPS protocol, are rapidly becoming one of the fundamental threats for information systems connected to the Internet. When the attacks suffered by Web servers through the years are analyzed, it is observed that most of them are very similar, using a reduced number of attacking techniques. It is generally agreed that classification can help designers and programmers to better understand attacks and build more secure applications. As an effort in this direction, a new taxonomy of Web attacks is proposed in this paper, with the objective of obtaining a useful reference framework for security applications. The use of the taxonomy is illustrated by means of multiplatform real world Web attack examples. Along with this taxonomy, important features of each attack category are discussed. A semantic-dependent Web attack encoding scheme is also defined that, together with the taxonomy, can be used to process the attack information with low time and memory consumption. Applications of the taxonomy and the encoding scheme are described, such as intrusion detection systems and application firewalls. 
22|5||A comment on the Chen-Chung scheme for hierarchical access control|Chen and Chung proposed an improved scheme of hierarchical access control based on the Chinese Remainder theorem and symmetric encryption [1]. In this paper, we point out that the part of their scheme for adding a new class is not correct. Furthermore, we discuss how to fix this problem. 
22|5||Cryptanalyses and improvements of two cryptographic key assignment schemes for dynamic access control in a user hierarchy|Recently, Wu and Chang and Shen and Chen separately proposed a cryptographic key assignment scheme for solving access control problem in a partially ordered user hierarchy. However, this paper will show the security leaks inherent in both schemes based on polynomial interpolations. That is, the users can have access to the information items held by others without following the predefined partially ordered relation. Finally, we proposed two improvements to eliminate such security flaws. 
22|5||IFIP technical committee 11|
22|5||Refereed papers â Guide for Authors|
22|6|http://www.sciencedirect.com/science/journal/01674048/22/6|From the editor-in-chief: Gartner âs prediction concerning intrusion detection systems: sense or nonsense?|Over the years I’ve listened to various Gartner Group predictions with less than a sense of awe. Although Gartner has sometimes been correct (such as with its prediction that the firewall market would grow substantially), anyone who listens to these predictions must certainly also remember all the wrong ones. 
22|6||Security views: Online piracy battle heats up in US|US Senator Orrin Hatch, chairman of the Senate Judiciary Committee, stated at a recent hearing on copyright abuse that he wants to teach cyberpirates a lesson by destroying their computers if they have illegally downloaded copyright-protected material such as music or movies. During the hearing witnesses explained the dangers of using peer-to-peer file-sharing services. They gave examples of peer-to-peer users inadvertently allowing access to their entire hard drives, exposing financial documents and medical data. After a volley of strenuous objections to his proposal, Mr. Hatch backed off from his initial hard-line stance, saying he merely wants to push private industry to come up with solutions to copyright abuse. Curiously, while the furor over his statements raged, Mr. Hatch’s Web site had an unexplained link to a pornographic page. 
22|6||Spam: the evolution of a nuisance|Junk e-mail (spam) is twenty five years old — the first junk e-mail was sent on the Arpanet, the fore runner to the Internet, in 1978. Not that there has been any cracking open of the bottles of bubbly to celebrate. It did not receive the spam designation until ten years ago based on the Monty Python sketch in which customers of a restaurant are offered spam with everything. Ironically, companies that offer anti-spam software are being threatened with breach of trademark litigation if they include the word spam in the product title by the company that owns the spam trademark. (for those of you unacquainted with spam, it is a tinned pork luncheon meat that really took off during the Second World War which declined in popularity in the Sixties). 
22|6||Presenting the Evidence Report: Introduction|In our last column we took a look at encountering encrypted evidence. There is some additional information that has come to light since the last column. Hard wired keystroke loggers are available that can be installed within the target’s own keyboard. These take about fifteen minutes to install but cannot be detected easily by the person under surveillance. They typically have a capacity to store, using 128 bit encryption, up to two megabytes of keystrokes. This is about 300,000 words or a year’s worth of typing. Viewing the log is, after opening a word processor or WordPad, as simple as typing on the modified keyboard a password that you control. This will execute a menu program stored on the device and keystrokes can then be downloaded for analysis. There are other menu options that allow you to manage the storage associated with the surveillance device as well as changing its controlling password. 
22|6||Security engineering and security RoI|IT Security has been practised as a dark art for too long. We should treat it as an engineering discipline and reset our expectations about how security systems should be designed and evaluated. All it would take is a fresh approach, the right metrics and a little competent analysis. This is how it might work. 
22|6||Operationalizing IT Risk Management|In a study of four major global organisations conducted during 2002, it was found that all conducted some form of risk assessment to assist in the management of security risks. However, when we analysed the risks that they addressed, three of the four organisations had major gaps in their risk assessment coverage that could result in significant risks being missed. We wondered: why did the gaps exist; are there inhibitors to effective risk assessment; are there blind spots; are approaches to risk assessment deficient in some way; how could we make the process of risk assessment more robust but easier to do? This paper seeks to address some of these questions. 
22|6||Security analysis of XML usage and XML parsing|Web-based applications greatly increase the availability of information and the ability of people to access and share information in a collaborative environment. Organisations can only truly make use of this technology to create a competitive advantage if they can trust the technology to distribute and mediate information in a safe and secure manner. The Web was not designed with security in mind and the use of XML as a vehicle for marking up information and mediating information flows does not directly support the imposition of a security architecture to manage the security of collaborative information sharing and dissemination. The adoption of XML as the vehicle for electronic commerce has created an environment where XML is now a core technology to most organisations, yet most organisations are relying on off-the-shelf solutions to parsing and manipulating it. In this paper we will examine how XML and XML parsers can be attacked and used to modify, and enter false or misleading, information relating to an electronic transaction. The attack scenarios will be divided into five categories: DTD, Document Corruption, single-node, multi-node and back-end systems. For each attack type we will explore how the attack is perpetrated and what, if any, countermeasures exist to mitigate the attacks. 
22|6||Roadmap to checking data migration|
22|6||RBAC models â concepts and trends|A key function in any information security infrastructure is represented by access control which concerns the ways according to which users can access resources in a computer systems. Access control is one of the most pervasive security mechanisms in use today and is present in almost all systems, from operating systems to database management systems. Access control is usually based on access permits, also called authorizations, specifying which subjects can access which objects for performing which actions. Access control, however, imposes great administrative and architectural challenges and also requires careful design. In particular, a relevant problem, especially when dealing with large systems, is represented by the complexity of access control administration. Access control administration deals with assigning and revoking authorizations. 
22|6||Web services set to provoke new sthreats: Preview of Compsec 2003, 30 Oct-1 Nov, Queen Elizabeth II Conference Centre, Westminster, London, UK|This year’s Compsec aims to map out the near future of IT security, offering a practical guide to action on current and upcoming threats. It addresses some of the frameworks of information security — privacy and regulation — and looks at the latest in technologies from intrusion detection through authentication to wireless. Conference programme director Brian McKenna previews the event. 
22|6||Events|
22|6||Refereed papers|
22|6||New hierarchical assignment without Public Key cryptography|The access privileges in many distributed systems can be effectively organized as a hierarchical tree. Distributing distinct cryptographic keys to distinct entities, according to their privileges, provides a good solution to the hierarchical access control problem. Many existing key assignment schemes use public key cryptography, which requires lots of costly public key operations and thus leads to a limited degree of deployment. In this paper, we shall propose a new key assignment protocol that employs only a low cost smart card with little memory to perform simple arithmetic operations. Our approach greatly reduces the computational load and the implementation cost. Compared with Lin’s scheme, which is a very efficient scheme without using any public key cryptography, our new scheme further reduces the computational cost by as much as 66% and the quantity of public data by 50%. 
22|6||Efficient proxy multisignature schemes based on the elliptic curve cryptosystem|For improving proxy-signature research, Sun [5] attempted to resolve problems related to defective security in the scheme of Yi [3]. However, both Yi and Sun’s schemes involve a significant number of exponential operations to verify the proxy signature. Accordingly, an improvement is proposed here to change the exponential operations into elliptic curve multiplicative ones. As proposed by both Koblitz  and  and Miller [8] in 1985, the elliptic curve is used in developing the cryptosystems. The elliptic curve cryptosystem can achieve a level of security equal to that of RSA or DSA but has a lower computational overhead and a smaller key size than both of these. Therefore, it is used in Sun’s schemes to improve their efficiency. 
22|6||Security middleware for enhancing interoperability of Public Key Infrastructure|This paper describes a security middleware for enhancing the interoperability of public key infrastructure (PKI). Security is a key concern in e-commerce and is especially critical in cross-enterprise transactions. Public key cryptography is widely accepted as an important mechanism for addressing the security needs of e-commerce transactions because of its ability to implement non-repudiation. The deployment of public key cryptography is facilitated by the provision of PKI which assures the integrity of cryptographic keys. Nevertheless, industry experiences have shown that the task of implementing PKI-based e-commerce applications is challenging. Prior studies have identified interoperability as a major issue that hinders the adoption of PKI in spite of its effectiveness in implementing strong security mechanisms and protocols. In this paper, we discuss the interoperability issue of PKI applications. This research is part of our effort in designing security infrastructure for e-commerce systems. A middleware architecture was designed to enhance interoperability of PKI applications. The security middleware aims to promote cross-enterprise cross-border e-commerce transactions. The proposed mechanism is proven to be practical in real deployment environment. 
22|6||A user friendly remote authentication scheme with smart cards|Based on a one-way function, Sun [6] has proposed an efficient remote authentication scheme using smart cards. The scheme is very elaborate since no password table is required to keep as well as low communication and low computation costs. However, the password of a user has to be computed by the system. This, in general, cannot satisfy user’s requirements. To achieve the aim of user friendliness, we propose a modified version that inherits the advantages of Sun’s scheme while still allowing the users to choose and change their passwords freely. 
22|6||IFIP technical committee 11|
22|7|http://www.sciencedirect.com/science/journal/01674048/22/7|Patching Pandemonium|I am not in the best of moods right now because of a problem I am having after installing a patch. While using my mostly reliable Windows 2000 machine, I read an email message that announced that vulnerability scans in the part of the network to which my machine is connected were going to be conducted soon. I quickly brought up the Microsoft Baseline Security Analyzer (MBSA) and checked whether any patches were missing. 
22|7||Security Views|Worms continue to plague WindowsFormer Symantec VP becomes US Cybersecurity ChiefWorries over voting systems in Maryland persistUS Department of Interior internet connections shut downThieves filch mainframes at Sydney InternationalThreat Crew Two chargedBlackmail stunt using steganography is thwartedBlackout prompts rethink of backup and disaster recoverySecurity certifications for IBM, SuSEFTC report: identity theft takes a huge tollNet anonymity service appealing monitoring rulingRuling says that subpoena for email violated anti-hacking lawsUS legislation would restrict spywareThe copyright infringement saga: more legal developmentsUniversities adopt strict policies to protect their networksSecurity spending predicted to rise 
22|7||Nimbyism, dominoes and creaking infrastructure|It was another hot sweltering afternoon in down town New York, when the whirl of air conditioning systems straining on their highest settings suddenly ceased. The power to run them had failed right across New York City, and indeed much of the Midwest and North East America and Eastern Canada. Businesses were brought to a halt, as were subways and surface trains and most airports across the region. Thousands of people were trapped in subways and elevators as officials battled to bring power back online. The New York Stock Exchange (NYSE) and the United Nations were left without power and evacuated. Times Square fell dark along with much of the city. 
22|7||Forensic evidence testimony â some thoughts|
22|7||Applying information security governance|BackgroundWinds of change have been blowing through the corporate world since the Enron collapse. Corporate Governance and accountability are now at the top of government and investor agendas — not just in the US, but throughout Europe and Asia. Chief Executives’ and Corporate Boards’ responsibilities for control are increasingly demanding. The Sarbanes Oxley Act in the USA will require all companies that are US listed (this includes many of the world’s largest corporations that have US listings) to include reporting on internal controls in their annual reports. The programmes to achieve compliance with these and probable European driven legislation are only just beginning. They will have a significant knock-on effect on technology, and particularly security governance. Additionally, financial institutions face the prospect of complying with the requirements of Basle 2. 
22|7||Data dependent rotations, a trustworthy approach for future encryption systems/ciphers: low cost and high performance|This work focuses an alternative direction of cryptography, based on Data Dependent Rotations (DDR). This methodology gives many promises for secure communication networks of the next years. DDR transformations have attracted the interest of researchers, and a great number of new ciphers have been developed. These encryption algorithms are intended to be used in all the applications of present and future. The article summarizes the key issues for this new approach and presents both performance and implementation cost of DDR implementations. The purpose of this work is to provide a state-of-the-art overview on DDR ciphers, which are proved a trustworthy applied methodology for modern cryptography. 
22|7||Events|
22|7||International Board of Referees|
22|7||Security enhancement for the timestamp-based password authentication scheme using smart cards|In 1999, Yang and Shieh proposed a timestamp-based password authentication scheme with smart cards. However, Chan and Cheng showed that it was insecure because the scheme was vulnerable to the forged login attack. In this paper, we propose a modified Yang-Shieh scheme to enhance security. Our modification can help withstand the forged login attack and also provide a mutual authentication method to prevent the forged server attack. 
22|7||An anomaly intrusion detection method by clustering normal user behavior|For detecting an intrusion based on the anomaly of a user's activities, previous works are concentrated on statistical techniques or frequent episode mining in order to analyze an audit data set. However, since they mainly analyze the average behavior of a user’s activities, some anomalies can be detected inaccurately. This paper proposes an anomaly detection method which utilizes a clustering algorithm for modeling the normal behavior of a user’s activities in a host. Since clustering can identify an arbitrary number of dense ranges in an analysis domain, it can eliminate the inaccuracy caused by statistical analysis. Consequently, it can model the frequent activities of a user more accurately than the statistical analysis does. The common knowledge of activities in the transactions of a user is represented by the occurrence frequency of similar activities by the unit of a transaction as well as the repetitive ratio of similar activities in each transaction. The proposed method also addresses how to maintain identified common knowledge as a concise profile. Furthermore, this paper addresses the selection of good features that can improve the detection rate of anomalous behavior in an on-line transaction. 
22|7||Detecting intrusion with rule-based integration of multiple models|As the information technology grows interests in the intrusion detection system (IDS), which detects unauthorized usage, misuse by a local user and modification of important data, has been raised. In the field of anomaly-based IDS several data mining techniques such as hidden Markov model (HMM), artificial neural network, statistical techniques and expert systems are used to model network packets, system call audit data, etc. However, there are undetectable intrusion types for each measure and modeling method because each intrusion type makes anomalies at individual measure. To overcome this drawback of single-measure anomaly detector, this paper proposes a multiple-measure intrusion detection method. We measure normal behavior by systems calls, resource usage and file access events and build up profiles for normal behavior with hidden Markov model, statistical method and rule-base method, which are integrated with a rule-based approach. Experimental results with real data clearly demonstrate the effectiveness of the proposed method that has significantly low false-positive error rate against various types of intrusion. 
22|7||The reduced Enigma|This article describes a simplified cryptographic machine, based closely on the World War II Enigma. This ‘reduced Enigma’ exposes some of the design flaws of the original Enigma in a new way. Had the Axis powers built a reduced Enigma, the outcome of the war might have been different.A fully working reduced Enigma has been used very successfully in numerous public lectures, in school talks, and in university seminars. Hands-on demonstrations of the reduced Enigma dramatically brings alive ideas about design, codes, permutations and groups. As a working trapdoor function, the reduced Enigma also provides an unusually clear introduction to public key cryptography. This article provides background information, lecture suggestions, and details for building it. 
22|7||Cryptanalysis of an enhanced timestamp-based password authentication scheme|Recently, Fan proposed an enhanced scheme to improve the security of Yang-Shieh’s timestamp-based password authentication scheme. The enhanced scheme can withstand the attacks presented by Chan, Cheng and Fan. In this paper, we show that the enhanced scheme is still insecure. An intruder is able to construct a forged login request by intercepting the legitimate login requests and pass the system authentication with a non-negligible probability. 
22|7||IFIP Technical Committee 11|
22|8|http://www.sciencedirect.com/science/journal/01674048/22/8|Information security and the media|The relationship between the media and information security is intriguing. The media is quick to cover security-related incidents such as worm outbreaks and intrusions into systems and networks, serious vulnerabilities and so on, in many respects helping in the job of raising public awareness of security issues. The media's fascination with information security also has negative consequences, however. Information security professionals, especially those who are consultants, often compete for media exposure. Several weeks ago the president, founder, and business administrator of Forensic Tec, a California-based security consultancy, were indicted for breaking into numerous US government and Department of Defense systems. After allegedly breaking into these systems, members of this consultancy openly bragged about how easy it was to breach their security. The press ran stories to the effect that some of the most critical computers within the US were wide open to attack. Interestingly, the indictment accused the individuals of creating a publicity stunt to drum up business for this new, small consultancy. 
22|8||Security views|
22|8||Computer security: Mapping the future|
22|8||Setting up an electronic evidence forensics laboratory|In our last column we took a look at being an expert witness and giving forensic evidence testimony. An added word of cautionary advice for private practitioners: CHARGE A LOT for expert testimony! If you are involved in a high-profile case that drags on and on and you must testify as an expert witness repeatedly at the whim and caprice of the various attorneys, it could disrupt your private practice and cause potential loss of current and future earnings. Your current case load could be seriously delayed and your credibility for future work may also be damaged. The previous discussion, by nature, was very general, however each jurisdiction has a formal set of directives and guidelines specifically to assist expert witnesses so you must also refer to these for more details.The following discussion will be focused on setting up an electronic evidence forensics laboratory and the various parts that make up a professional facility. The many parts also include the portable forensics kit(s), which includes documentation forms, evidence bags, tags, labels, etc, as well as portable hardware and associated software for undertaking an evidentiary acquisition on site. Not all such activities may be performed in the lab, but the mobile forensics toolkit must be fully compatible and in sync with the laboratory acquisition equipment and software at all times.There may be accreditation for such laboratories, depending on the jurisdiction. For example, in the US one such accreditation may be sought from the American Society of Crime Laboratory Directors. If accreditation is possible in your jurisdiction, it may be advisable to explore the criteria for achieving it. While there may be differing views as to the value of accreditation, it is my opinion that having it is one more stone in the foundation of credibility, and therefore it should be viewed in a positive light. 
22|8||Beyond cryptography: Bruce Schneier's Beyond Fear: thinking sensibly about security in an uncertain world|Bruce Schneier is best known for his classic work, Applied Cryptography, a lengthy, scholarly tome that includes almost 70 pages just for the references. At first glance, it's not at all apparent that Schneier's latest book, Beyond Fear, is even from the same author. Written not for specialised technologists, Beyond Fear is meant to be universally accessible. 
22|8||Security and human computer interfaces|Computer users are exposed to technology mainly through user interfaces. Most users' perceptions are based on their experience with these interfaces. HCI (human computer interaction) is concerned with these interfaces and how they can be improved. Considerable research has been conducted and major advances have been made in the area of HCI. Information security is becoming increasingly important and more complex as business is conducted electronically. However, state-of-the-art security-related product development has ignored general aspects of HCI. The objective of this paper is to promote and enable security awareness of end-users in their interaction with computer systems. It thus aims to consolidate and integrate the two fields of information security and HCI. HCI as a research discipline is a well developed field of study, and the authors are of the opinion that the use of security technologies can be significantly enhanced by employing proven HCI concepts in the design of these technologies. In order to achieve this, various criteria for a successful HCI in a security-specific environment will be examined. Part of the Windows XP Internet Connection Firewall will be used as a case study and analysed according to these criteria, and recommendations will be made. 
22|8||Improving user security behaviour|Many organisations suspect that their internal security threat is more pressing than their external security threat. The internal threat is predominantly the result of poor user security behaviour. Yet, despite that, security awareness programmes often seem more likely to put users to sleep than to improve their behaviour. This article discusses the influences that affect a user's security behaviour and outlines how a well structured approach focused on improving behaviour could be an excellent way to take security slack out of an organisation and to achieve a high return for a modest, low-risk investment. 
22|8||Calendar of forthcoming conferences and events|
22|8||International Board of Referees|
22|8||Understanding users' keystroke patterns for computer access security|User authentication is a major problem in gaining access rights for computer resources. A recent approach to enhance the computer access rights is the use of biometric properties as the keystroke rhythms of users. Therefore user authentication for computers can be more secure using keystroke rhythms as biometric authentication. Methods like minimum distance, statistical, vector based, neural network type and data mining techniques have been applied in analyzing the keystroke patterns. In this paper, a vector based algorithm for a recent approach has been applied in the identification of keystroke patterns. Keystroke Identification system that is a neuro physical characteristic is studied to realize biometric authentication. 
22|8||The availability of source code in relation to timely response to security vulnerabilities|Once a vulnerability has been found in an application or service that runs on a computer connected to the Internet, fixing that exploit in a timely fashion is of the utmost importance. There are two parts to fixing vulnerability: a party acting on behalf of the application's vendor gives instructions to fix it or makes a patch available that can be downloaded; then someone using that information fixes the computer or application in question. This paper considers the effects of proprietary software versus non-proprietary software in determining the speed with which a security fix is made available, since this can minimize the amount of time that the computer system remains vulnerable. 
22|8||Attacks on the (enhanced) Yang-Shieh authentication|The Yang-Shieh authentication is a time-stamp based password authentication scheme that uses smart cards [1]. In [ and ], various attacks on this scheme are described. However, an enhancement of the scheme is proposed in [3] and enables the scheme to resist these existing attacks. In this paper, we show two new attack that can break the enhanced scheme. We further point out that the fundamental computational assumption of the Yang-Shieh authentication scheme is incorrect. 
22|8||Efficient anonymous auction protocols with freewheeling bids|The need for electronic auction services has been increasing in recent years. Taking security into account, anonymity of online bidders becomes more important than it used to be. However, bidders cannot bid of his/her free will in existing anonymous auction protocols. For real-time applications, time delays are the significant factor taken into account. As a result, the proposed paper presents a simple and efficient method to ensure that the bidders can bid arbitrarily and anonymously. 
22|8||IFIP technical committee 11|
23|1|http://www.sciencedirect.com/science/journal/01674048/23/1|Security training and awarenessâfitting a square peg in a round hole|
23|1||Security views|
23|1||The future of computer forensics: a needs analysis survey|The current study was a pilot study and attempted to add to the growing body of knowledge regarding inherent issues in computer forensics. The study consisted of an Internet-based survey that asked respondents to identify the top five issues in computer forensics. Sixty respondents answered the survey using a free form text field. The results indicated that education/training and certification were the most reported issue (18%) and lack of funding was the least reported (4%). These findings are consistent with a similar law enforcement community study (Stambaugh, Beaupre, Icove, Cassaday, Williams. State and local law enforcement needs to combat electronic crime. National Institute of Justice Research in Brief (2001)). The findings emphasize the fragmented nature of the computer forensics discipline. Currently there is a lack of a national framework for curricula and training development, and no gold standard for professional certification. The findings further support the criticism that there is a disproportional focus on the applied aspects of computer forensics, at the expense of the development of fundamental theories. Further implications of the findings are discussed and suggestions for future research in the area are presented. 
23|1||Applying application security standardsâa case study|Initiating a program to raise awareness of application development security issues within a globally dispersed organisation is a big challenge. This article describes how a combination of both technical and people skills can work towards achieving the goals of technical education, and awareness of corporate goals and policies. 
23|1||TBSEâan engineering approach to the design of accurate and reliable security systems|For many years, the IT Security industry has been trying to devise a way to quantify risk and the benefits provided by security countermeasures in a form meaningful to senior business management. Threat-Based Security Engineering (TBSE) is a fresh approach to modelling and forecasting information security risk. TBSE takes a non-deterministic approach to modelling how security threats interact with countermeasures enabling quantitative forecasts of the likelihood and characteristics of security incidents as a direct function of the security measures employed. Preliminary results are encouraging and there appears to be no reason why the TBSE techniques could not be applied to a wide range of threats and countermeasures. Assuming they can, these techniques could become the foundation for a greatly needed disciplined engineering approach to the design of accurate and reliable security systems. Amongst the many other benefits, this would give senior business management the much sought after tools with which to oversee and direct corporate security expenditures. This article describes the TBSE approach and what it can do. 
23|1||Events calender|
23|1||Mitigation of network tampering using dynamic dispatch of mobile agents|Detection of malicious activity by insiders, people with legitimate access to resources and services, is particularly difficult in a network environment. In this paper, a novel classification of tampering modes is identified that can be undertaken by insiders against network Intrusion Detection Systems (IDSs). Five categories of tampering modes are defined as spoofing, termination, sidetracking, alteration of internal data, and selective deception. These are further distinguished specifically toward IDS sensor, control, and alarm categories such as spoonfeeding, sugarcoating, and scapegoating.The Collaborative Object Notification Framework for Insider Defense using Autonomous Network Transactions, or CONFIDANT, uses distributed mobile agents to mitigate these tampering exposures. CONFIDANT employs techniques such as encapsulation, redundancy, scrambling, and mandatory obsolescence. This paper describes how these mitigation techniques are applied within the CONFIDANT framework. The approach focuses on evaluating file integrity through the use of dynamically dispatched mobile agents. 
23|1||An empirical investigation of network attacks on computer systems|In this paper we have analyzed data on reported network incidents at a large number of computer sites over time. We have examined various patterns in the incidents process such as the distribution of the inter-incident times, trends in the incident rates and variations with type of incident. Some suggestions for future data collection are included. 
23|1||Restraining and repairing file system damage through file integrity control|Today, many security researches focus on survivable or intrusion-tolerant systems, which are not conceived to be invulnerable, but rather to be able to withstand the impact of an attack and continue providing critical services despite ongoing attacks. We join this fast-growing research community and use this article to present a solution of file integrity control which is able to detect unauthorized file system modifications as fast as possible and repair them in order to keep the system's integrity, availability and confidentiality. 
23|1||Holistic security requirement engineering for electronic commerce|With the introduction of electronic commerce, business is becoming dependent on information systems in a new way. Information security is thus becoming more and more important to companies' self-protection. In contrast to previous systems, this is also directly visible to the customer. The changing situation means, however, that the requirements for security cannot be solely filled by new policies and risk analysis. This article proposes an approach called “holistic security requirement engineering” meant to elicit security requirements according to system-theoretic considerations. It will show that security requirements can be defined with the help of investigations in the business environment, workshops with stakeholders and risk analysis. This multidimensional approach will lead to a holistic understanding of the requirements that fit into the system development life cycles. 
23|1||Biometric random number generators|Up to now biometric methods have been used in cryptography for authentication purposes. In this paper we propose to use biological data for generating sequences of random bits. We point out that this new approach could be particularly useful to generate seeds for pseudo-random number generators and so-called “key sessions”. Our method is very simple and is based on the observation that, for typical biometric readings, the last binary digits fluctuate “randomly”. We apply our method to two data sets, the first based on animal neurophysiological brain responses and the second on human galvanic skin response. For comparison we also test our approach on numerical samplings of the Ornstein–Uhlenbeck stochastic process. To verify the randomness of the sequences generated, we apply the standard suite of statistical tests (FIPS 140-2) recommended by the National Institute of Standard and Technology for studying the quality of the physical random number generators, especially those implemented in cryptographic modules. Additionally, to confirm the high cryptographic quality of the biometric generators, we also use the often recommended Maurer's universal test and the Lempel–Ziv complexity test, which estimate the entropy of the source. The results of all these verifications show that, after appropriate choice of encoding and experimental parameters, the sequences obtained exhibit excellent statistical properties, which opens the possibility of a new design technology for true random number generators. It remains a challenge to find appropriate biological phenomena characterized by easy accessibility, fast sampling rate, high accuracy of measurement and variability of sampling rate. 
23|1||Erratum to âAttacks on the (enhanced) Yang-Shieh authenticationâ [Comput Secur 22(8) (2003) 725â727]|
23|2|http://www.sciencedirect.com/science/journal/01674048/23/2|Incident response teams need to change|
23|2||Security views|
23|2||Non-PKI methods for public key distribution|The X.509 certification authority-based (CA) public key infrastructure (PKI) is a widely accepted PKI standard which defines data formats and procedures related to the distribution of public keys via public key certificates that are digitally signed by CAs. However, X.509 requires a huge and expensive infrastructure with complex operations. This overhead may be tolerable in some cases, but it is highly desirable to find other solutions. The objective of this paper is to present alternative simpler solutions to the X.509 PKI to save storage, bandwidth and to reduce the complexity of the operations. We offer three such solutions. They rely on the existence of passwords that are known to both users and service providers. 
23|2||Events calender|
23|2||A secure electronic voting protocol for general elections|Elections and voting behavior are always in our lives. As the activities increase on Internet, some elections and voting behavior would be brought on Internet. It is called “Electronic voting”. Electronic voting can solve the cost problem that occurs in traditional election. But electronic voting still has some problems like completeness, uncoercibility, non-cheating problems. In this paper, most properties of the electronic voting would be discussed and a new proposed electronic voting scheme would try to satisfy these properties and to solve problems mentioned. Furthermore, it also investigates what happens in the real world and how to solve these problems in practice. 
23|2||Efficient user identification scheme with key distribution preserving anonymity for distributed computer networks|In 2000, Lee and Chang (Comput Syst Sci Eng 15 (2000) 211) presented a user identification scheme that also can simultaneously achieve key exchange requirement while preserving the user anonymity. Their idea is valuable especially when it is applied to some applications in which the identity of the user should be protected from the public in the distributed computer networks. Unfortunately, our paper shows that their scheme is insecure under two attacks and we further proposed a more efficient identification scheme preserving the same merits. The proposed scheme not only effectively eliminates the security leaks of the Lee–Chang scheme, but also reduces computational complexities and communication costs as compared with their scheme. 
23|2||Symmetric RBAC model that takes the separation of duty and role hierarchies into consideration|RBAC is a family of reference models in which permissions are assigned to roles, and users are also assigned to appropriate roles. Studies on the permission–role part of RBAC model are relatively insufficient compared with those on the user–role part, and researches on symmetric RBAC models to overcome this is also in an incipient stage. Therefore there is difficulty in assigning permissions suitable for roles.This paper proposes a symmetric RBAC model that supplements the constraints on permission assignment set forth by previous studies. The proposed symmetric RBAC model reflects the conflicts of interests between roles and the sharing and integration of permissions on the assignment of permissions by presenting the constraints on permission assignment that take the separation of duties and role hierarchies into consideration. In addition, by expressing constraints prescribing prerequisite relations between permissions through AND/OR graphs, it is possible to effectively limit the complicated prerequisite relations of permissions. The constraints on permission assignment for the proposed symmetric RBAC model reduce errors in permission assignment by properly detailing rules to observe at the time of permission assignment. 
23|2||An alternative architectural framework to the OSI security model|In this paper an alternative framework to the OSI security model is presented. An identification of the principles governing security function assignment inside the OSI communication layers is given, followed by an analysis of the advantages of the security reference model. Also the IPsec and Stream Control Transmission Protocol (SCTP) architectures are briefly presented, illustrating their features and usages. The disadvantages and implementation pitfalls of the presented models are then brought forward, in relation to performance and security issues. The Future Core Networks System (FCNS) is presented, which constitutes the proposed reference architecture. The features of the FCNS are given, together with an analysis of the advantages our proposal exhibits with respect to the protocols presented, followed by the software implementation of our model. Results from simulations show that FCNS offers an improvement in throughput of at least 10% in comparison with currently used communication protocol stack architectures. These throughput benefits are achieved even when the full security measures of FCNS are in operation. Finally, we present the FCNS applicability in current network systems and reveal future work. 
23|2||An analysis of the tools used for the generation and prevention of spam|This paper examines the problems caused by the spamming of e-mail and newsgroup users. Spamming is now considered to be a serious threat to the Internet and is posing a serious threat to both ISP and users' resources. In particular, this paper examines the motivation of, and the tools used to generate, spam. Methods of protection and prevention are then discussed. The paper includes case studies of some spam generation and prevention tools as well as examines evolving spam-related laws. 
23|2||Efficient password authenticated key agreement using smart cards|The smart-card based remote user authentication and key agreement scheme is a very practical solution to create a secure distributed computer environment. In this paper, we propose a novel user authentication and key agreement scheme with much less computational cost and more functionality. The main merits include: (1) the scheme needs no verification table; (2) users can freely choose their own passwords; (3) the communication and computation cost is very low; (4) users and servers can authenticate each other; and (5) it generates a session key agreed by the user and the server. Also, our proposed scheme is a nonce-based scheme which does not have a serious time-synchronization problem. 
23|2||An improvement of nonrepudiable threshold proxy signature scheme with known signers|In a (t, n) threshold proxy signature scheme, which is a variant of the proxy signature scheme, the proxy signature key is shared among a group of n proxy signers delegated by the original signer. Any t or more proxy signers can cooperatively sign messages on behalf of the original signer. In 2000, Hwang et al. (Int J Inf 11 (2000) 1) proposed a secure nonrepudiable threshold proxy signature scheme with known signers. In this paper, we point out a cryptanalysis of their scheme. Furthermore, we improve the security of the threshold proxy signature scheme which remedies the weakness of Hwang et al.'s scheme. 
23|3|http://www.sciencedirect.com/science/journal/01674048/23/3|Worms and viruses: are we losing control?|
23|3||Security views|
23|3||Towards information security behavioural compliance|Auditing has always played an important role in the business environment. With the introduction of information technology and the resulting security challenges that organizations face daily, it has become essential to ensure the security of the organization's information and other valuable assets. However, one aspect that auditing does not cover effectively is that of the behaviour of the employee, which is so crucial to any organization's security.The objective of this paper is to explore the potential problems concerning the attempt to audit the behaviour of the employee. It will be demonstrated that it is extremely difficult to audit human behaviour and so an alternative method to behavioural auditing needs to be found, where policing the employee is not necessary, but instead a softer, more informal approach is used to change the culture to a more information security conscious one. 
23|3||A fair and secure mobile agent environment based on blind signature and proxy host|The advancement in information technology has made it possible to maintain fairness in on-line transactions such as on-line shopping and auctioning. However, traditional techniques have only been developed for authentication instead of for maintaining the fairness principle, which is defined in this paper as the equal treatment of authenticated mobile agents by service hosts. In other words, if the fairness principle were followed, service hosts, such as merchant hosts, auction hosts, and so on, would process the requests from authenticated mobile agents according to their time of arrival rather than according to the service hosts' own benefit.Mobile agent technology suffers from two drawbacks. The first is the possibility of being attacked by malicious unknown mobile agents. The second is that, in transactions, service hosts may breach the fairness principle. We, therefore, propose an improvement model that would not only protect service hosts but also guarantee the fairness of on-line transactions. The environment offers the promise of tracking malicious mobile agents once a service host is attacked. It also protects the information collected from a mobile agent from being manipulated. 
23|3||The effect of intrusion detection management methods on the return on investment|This paper examines how implementation methods, management methods, and Intrusion Detection System (IDS) policy affect Return on Investment (ROI). The paper will seek to demonstrate the value associated with a well thought out implementation and effective lifecycle management of IDS technology and will culminate in a case study with a number crunching exercise to calculate the ROI for an IDS deployment by a hypothetical financial company named UTVE, Inc. on risk.The paper also discusses general IDS types and expands on the impact that the logical location of a company's critical networked assets could have on the risk equations. To this end, the Cascading Threat Multiplier (CTM) is introduced to expand on the Single Loss Expectancy (SLE) equation. Also, implementation and management costs based on various support profiles and commonly accepted risk equations are reviewed. Finally, a formula for calculating ROI for security, otherwise commonly known as Return on Security Investment (ROSI) is devised. 
23|3||Digital signature of multicast streams secure against adaptive chosen message attack|We design a secure multicast stream signature scheme which can resist adaptive chosen message attack through splitting a multicast stream into a sequence of blocks. Firstly, we propose the definition of one-time block signature scheme and its construction based on one-time message signature scheme and hash tree. Secondly, we propose the definition of multicast stream signature scheme secure against adaptive chosen message attack and its construction based on traditional message signature scheme secure against adaptive chosen message attack and our one-time block signature scheme. Thirdly, we prove the securities of our one-time block signature scheme and multicast stream signature scheme. Finally, we analyze the performance of our multicast stream signature scheme.Our multicast stream signature scheme proposed in this paper is secure against adaptive chosen message attack and has the properties of low send delay, instant verification, efficient signing and verification, robustness to packet loss at the cost of slight per-packet authentication information. 
23|3||Predicting the intrusion intentions by observing system call sequences|Identifying the intentions or attempts of the monitored agents through observations is very vital in computer network security. In this paper, a plan recognition method for predicting the anomaly events and the intentions of possible intruders to a computer system is developed based on the observation of system call sequences. The probability of the goal state for a system call sequence is defined as the prediction index to determine if the intention is normal. An efficient algorithm based on the dynamic Bayesian network theory with parameter compensation is derived and then applied to update the index recursively. Extensive empirical testing is performed on the data sets published in the literature and those collected in an actual computer system at our lab. The testing results showed that this method can identify the intrusion behaviors from the observed system call sequences with good accuracy. 
23|3||Computer security impaired by legitimate users|Computer security has traditionally been assessed from a technical point of view. Another way to assess it is by investigating the role played by legitimate users of systems in impairing the level of protection. In order to address this issue, we wish to adopt a multidisciplinary standpoint and investigate some of the human aspects involved in computer security. From research in psychology, it is known that people make biased decisions. They sometimes overlook rules in order to gain maximum benefits for the cost of a given action. This situation leads to insidious security lapses whereby the level of protection is traded-off against usability. In this paper, we highlight the cognitive processes underlying such security impairments. At the end of the paper, we propose a short usability-centred set of recommendations. 
23|3||Events calender|
23|4|http://www.sciencedirect.com/science/journal/01674048/23/4|Intrusion prevention|
23|4||Security views|
23|4||From policies to culture|Management normally sets company vision, rules and regulations through policies. These policies should provide guidance to employees and partners as to how they should act and behave to be in line with management's wishes. These policies need to be structured and organized effectively to cater for business and technological dynamics and advances. Having defined a series of company policies does not ensure that all employees will necessarily obey these policies. Ideally these policies must manifest in some company culture to ensure appropriate behaviour. This can only be achieved through a proper education process. This paper addresses exactly the process of integrating policies, education and culture. 
23|4||Events calendar|
23|4||Tele-Lab âIT-Securityâ on CD: portable, reliable and safe IT security training|Besides gaining theoretical knowledge, IT students and professionals need to be prepared to apply security technologies and tools in their daily work. Therefore, today's security training should intend to provide hands-on experience by integrating practical exercises into the learning process. Tele-Lab “IT-Security” is a novel training system that makes interactive security exercises in a real laboratory environment possible that is equipped with rich security tools. Since for security tasks privileged operations have to be allowed, this laboratory environment needs to be carefully prevented from corruption by misuse or failures. To this end, we integrated Tele-Lab “IT-Security” including its operating system into a live system which is completely run on a small-sized CD without hard-disk installation. In this way, its portability, reliability and safety are also improved. Students can very easily access security training on any decent PC by booting Tele-Lab “IT-Security” from CD. Any activity in the training does not affect hardware and software systems; any system failure can be recovered by reboot.In this paper, the architecture, implementation and applications of the Tele-Lab are described. 
23|4||Logical analysis of AUTHMAC_DH: a new protocol for authentication and key distribution|In the present paper, a new protocol for authentication and key distribution is proposed. The new protocol aims to achieve a comparable performance with the Kerberos protocol and to overcome its drawbacks. For authentication of messages exchanged during authentication and key distribution, the new protocol uses the message authentication codes (MAC) to exchange the Diffie–Hellman components. It has to be noted that the use of MAC will fasten the proposed protocol. On the other hand, the new protocol uses nonces to ensure the freshness of the exchanged messages. Subsequently, there is no need for clock synchronization which will simplify the system requirements. The new protocol is analyzed, using a logical tool, to ensure that it achieves the goals of authentication and key distribution as defined in the BAN logic. The analysis shows that the new protocol achieves the goals of authentication and key distribution without bugs. 
23|4||Formal support for certificate management policies|Traditionally, creation and revocation of certificates are governed by policies that are carried manually, off-line, by trusted agents. This approach to certificate management is appropriate for many current applications, where these policies cannot be verified automatically (e.g. require verification of non-digital credentials). But it is expensive, time consuming and error-prone for the growing class of applications where certificate management policies can be formalized and carried out automatically. We argue that, in these cases, creation and revocation of certificates could be viewed as any other on-line service available in a system. Access to these particular service instances could be regulated much in the same manner as file access or resource allocation.This paper proposes a formulation for certification and revocation policies, and a framework for their support. In this framework, certificate management policies are enforced by generic policy engines, wrapped around certification authorities and revocation servers. The proposed framework is easy to deploy, requiring no modifications of current public-key infrastructure (PKI). Moreover, we show that this framework is quite affordable, even in its present, experimental stage. 
23|4||SAD: web session anomaly detection based on parameter estimation|Web attacks are too numerous in numbers and serious in potential consequences for modern society to tolerate. Unfortunately, current generation signature-based intrusion detection systems (IDS) are inadequate, and security techniques such as firewalls or access control mechanisms do not work well when trying to secure web services. In this paper, we empirically demonstrate that the Bayesian parameter estimation method is effective in analyzing web logs and detecting anomalous sessions. When web attacks were simulated with Whisker software, Snort, a well-known IDS based on misuse detection, caught only slightly more than one third of web attacks. Our technique, session anomaly detection (SAD), on the other hand, detected nearly all such attacks without having to rely on attack signatures at all. SAD works by first developing normal usage profile and comparing the web logs, as they are generated, against the expected frequencies. Our research indicates that SAD has the potential of detecting previously unknown web attacks and that the proposed approach would play a key role in developing an integrated environment to provide secure and reliable web services. 
23|4||A scalable and distributed multicast security protocol using a subgroup-key hierarchy|In the present paper, a scalable protocol for securing multicast communication is proposed. The proposed protocol is based on the idea of dividing the whole group into smaller subgroups as in the Iolus protocol. For a member join or leave, the decomposition of the group into smaller subgroups will reduce the computation complexity from O(M), where M is the number of the whole group members, to O(N), where N is the number of the subgroup members. Moreover, each subgroup is organized in a logical key hierarchy as in the LKH protocol. The use of logical key hierarchy will reduce the computation complexity cost from O(N) to O(log(N)) in case of member leave/join. Furthermore, the number of communicating messages containing the changed keys will be reduced. The proposed protocol is compared with the two well-known protocols: Iolus and LKH. The comparison is undertaken according to two criteria: the cost of encryption required for the re-key operation in case of member join or leave and the length of the re-key message. The results show that the proposed protocol outperforms both the Iolus and the LKH protocols. Therefore, the proposed protocol will enhance the group performance in terms of computation and communication. 
23|4||The design of a secure anonymous Internet voting system|In this paper, we propose a very practical and secure anonymous Internet voting protocol. Our scheme does not require a special voting channel and communications can occur entirely over the current Internet. This method integrates Internet convenience and cryptology. Issues such as the kinds of “certificate authority” and “public proxy server” are integrated in our scheme to solve the Internet identification and anonymity problems. This protocol combines the RSA blind signature scheme and secret sharing cryptosystem, to provide a fair and practical election. 
23|4||Rico: a security proxy for mobile code|Security technology suitable for the burgeoning embedded system market has not been widespread. Untrusted code downloaded from the Internet poses numerous security risks due to the possible presence of viruses or other malicious entities. System administrators typically administer one or more administrative domains making policy management for mobile code a challenge because of the diverse security rules that must be adhered to. In this paper, we introduce Rico, a binary rewriting, security policy and code management system that sits between clients and servers. The system interposes itself between a client that downloads mobile code and the target server to provide the system administrator a means to secure untrusted code by rewriting it. The system supports the following features: (1) A security policy editor that simplifies policy writing with frame wizards, syntax reminders and error checking. (2) Third-party policy incorporation enabling reuse of security policies created by trusted third parties. (3) Policy composition, the capability of combining multiple security policies into one logical policy that can be applied to a mobile program. (4) Efficient security management supported by a graphical user interface and a self-training database. 
23|5|http://www.sciencedirect.com/science/journal/01674048/23/5|SarbanesâOxleyâa huge boon to information security in the US|
23|5||Security views|
23|5||On risk: perception and direction|The idea of risk permeates the information security field. We use terms like “risk management”, “risk assessment”, “risk model” and “risk analysis” every day, and those topics are themselves the subject of countless papers and articles in security journals and magazines.But has the concept of risk become so ingrained within our profession that we have become over confident about how much we really understand it? In this paper I discuss how difficult it is to truly understand risk. I describe why we need to fundamentally reassess many of our current activities that involve trying to calculate and manipulate risk. I also make several proposals for how we can collectively treat risk in a more pragmatic and realistic way. 
23|5||The 10 deadly sins of information security management|This paper identifies 10 essential aspects, which, if not taken into account in an information security governance plan, will surely cause the plan to fail, or at least, cause serious flaws in the plan. These 10 aspects can be used as a checklist by management to ensure that a comprehensive plan has been defined and introduced. 
23|5||Events calender|
23|5||Search engines and privacy|Search engines have become a fundamental tool to access the vast amounts of information available in the World Wide Web in an optimized fashion. As they become ever more powerful, there has been concern on what this could mean for privacy issues, considering the accessibility to personal information in electronic format. This article addresses the nature of these concerns, attempting to clarify the issues at stake in a balanced view considering the position of all parties involved in the problem. 
23|5||An English auction scheme in the online transaction environment|Internet technology in the recent years has progressed with great strides, and has transcended physical boundaries to achieve a global community. It has been an efficient tool in the development of modern communication, electronic commerce, and various living applications. Under an Internet environment, a reliable and high-performing English auction scheme is presented in the research involving three parties, namely the Registration Manager, Auction Manager, and Bidder. The Registration Manager identifies and authenticates the bidder. The Auction Manager issues the bidding rights and maintains order during the auction. The proposed scheme has the following features: anonymity, traceability, no framing, unforgeability, non-repudiation, fairness, public verifiability, unlinkability among different auction rounds, linkability in a round of auction, efficiency of bidding, one-time registration, and easy revocation. Given the Internet environment, significant importance is attached to time costs in transmitting bidding data. Hence, the bulletin board method is used to enable both the registration and auction managers to declare the necessary parameters. Furthermore, the elliptic curve cryptosystem, owing to its low computational amount and small key size, is applied to the scheme. Consequently, the auction-manager server load can be effectively reduced, while simultaneously significantly increasing bidding efficiency. 
23|5||Internet privacy law: a comparison between the United States and the European Union|The increasing use of personal information in Internet-based applications has created privacy concerns worldwide. This has led to awareness among policy makers in several countries of the desirability of harmonizing privacy laws. The greatest challenge to privacy legislation from an international perspective arises because, while the Internet is virtually borderless, legislative approaches differ from country to country.This paper presents a functional comparison between current privacy law in the European Union (EU) and in the United States (U.S.), as such laws relate to regulation of websites and online service providers. In addition, similarities and differences between the 2002 EU Directive 2002/58/EC, Directive on Privacy and Electronic Communications, which has been adopted by the EU but not yet implemented, and the proposed U.S. Online Privacy Protection Act, are illuminated. Employing a qualitative approach, we use the Fair Information Practices to organize discussion of comparisons and contrasts between U.S. and EU privacy laws. Our investigation of this topic leads us to conclude that the right to privacy is more strictly protected in the EU than in the U.S. The Online Privacy Protection Act, recently introduced as a bill in Congress, has the potential to significantly affect commercial practices in the U.S. and move the U.S. towards current EU privacy protection laws. This analysis benefits managers as well as security professionals since the results can be used as guidelines in ensuring that an organization's website practices are consistent with requirements imposed by countries with which they exchange information. It also provides information that can guide organizations as they prepare for potential privacy legislation. 
23|5||Cumulative notarization for long-term preservation of digital signatures|The long-term preservation of digitally signed documents may be approached and analyzed from various perspectives, i.e. future data readability, signature validity, storage media longevity, etc. The paper focuses on technology and trust issues related to the long-term validation of a digital signature. We exploit the notarization paradigm and propose a mechanism for cumulative data notarization that results in a successive trust transition towards new entities, modern technologies, and refreshed data. A future relying party will have to trust only the information provided by the last notary, in order to verify the validity of the initial signature, thus eliminating any dependency on ceased entities, obsolete data, and weak old technologies. The proposed framework uses recursive XML elements so that a notarization token structure encapsulates an identical data structure containing a previous notarization token. 
23|5||Cryptanalysis of a user friendly remote authentication scheme with smart cards|Recently, Sun proposed an efficient remote user authentication scheme by employing one-way hash function. In his scheme, the system does not need to maintain a password table and the communication cost of the scheme is low, but the user cannot freely choose a password. Therefore, Wu and Chieu improved Sun's scheme for enhancing the user's demand that the user can choose and change his password freely. However, in this paper, we point out that Wu and Chieu's improvement is vulnerable to the password guessing and forgery attacks. 
23|5||Keystroke dynamics identity verificationâits problems and practical solutions|Password is the most widely used identity verification method in computer security domain. However, because of its simplicity, it is vulnerable to imposter attacks. Use of keystroke dynamics can result in a more secure verification system. Recently, Cho et al. (J Organ Comput Electron Commerce 10 (2000) 295) proposed autoassociative neural network approach, which used only the user's typing patterns, yet reporting a low error rate: 1.0% false rejection rate (FRR) and 0% false acceptance rate (FAR). However, the previous research had some limitations: (1) it took too long to train the model; (2) data were preprocessed subjectively by a human; and (3) a large data set was required. In this article, we propose the corresponding solutions for these limitations with an SVM novelty detector, GA–SVM wrapper feature subset selection, and an ensemble creation based on feature selection, respectively. Experimental results show that the proposed methods are promising, and that the keystroke dynamics is a viable and practical way to add more security to identity verification. 
23|6|http://www.sciencedirect.com/science/journal/01674048/23/6|The case for one-time credentials|
23|6||Security views|
23|6||The implications of immunology for secure systems design|The immune system can be a powerful model for understanding and improving computer security. In this article we explore the analogy, starting with a description of early work at discovering “peptides” for computer systems in the form of sequences of system calls, and moving on to the implications of immunology for secure systems design. In particular, we discuss how the immune system tolerates errors, and how we can borrow these ideas to improve the robustness of our computer systems. We observe that a key aspect of the biology is that the immune system and the body have co-evolved so that the body is easier to protect; what we need is a similar co-evolution of computer systems and the methods we use to secure them. 
23|6||Events Calendar|
23|6||The use and usability of direction-based filtering in firewalls|The common match fields in firewall rules refer to a packet's source and destination IP addresses, protocol, and source and destination port numbers. However, most firewalls are also capable of filtering based on a packet's direction: which network interface card the packet is crossing, and whether the packet is crossing the interface from the network into the firewall (“inbound”) or vice versa (“outbound”). Taking a packet's direction into account in the firewall's rules is extremely useful: it lets the firewall administrator protect against source address spoofing, write effective egress-filtering rules, and avoid unpleasant side-effects when referring to subnets that span the firewall.Unfortunately, the firewall's definition of a packet's direction is different from what users normally assume. If interface eth0 connects the firewall to the internal network, then, from a user's perspective, “inbound on eth0” is actually “Outbound” traffic. This discrepancy makes it very confusing for firewall administrators to use the packet direction correctly, and creates a significant usability problem.In this paper we review the usefulness of direction-based filtering, identify the usability problem, and critically review the approaches taken by several major firewall vendors. Most vendors expose the raw and confusing functionality to the firewall administrators, while one vendor (Check Point) hides the functionality entirely. Both approaches leave much to be desired. However, recent advances in firewall research show that better alternatives exist: the Firmato prototype demonstrates that the firewall management software can compute the directions algorithmically for a perimeter firewall. 
23|6||Further analysis of password authentication schemes based on authentication tests|In this paper, we present further analysis of Yang–Shieh's password authentication schemes. At first, we formally analyze Yang–Shieh's two password authentication schemes on the basis of authentication tests to disclose the insecurity of the two schemes, and then give two kind of examples, one is our attack to the nonce-based scheme and the other is Chan–Cheng's attack (Comput Secur 21 (2002) 74) and Fan–Li–Zhu's attack (Comput Secur 21 (2002) 665) to the timestamp-based scheme. Secondly, we propose an amendment of the timestamp-based scheme to withstand the attacks of Chan–Cheng and Fan–Li–Zhu, and propose our improved nonce-based scheme. Finally, we formally analyze our two improved schemes with the authentication tests, and prove they are secure in password authentication. Our improved schemes preserve the merits of Yang–Shieh's schemes, and the improved timestamp-based scheme can withstand the attacks of Chan–Cheng and Fan–Li–Zhu, and the improved nonce-based scheme is able to prevent malicious replay attacks in the network without synchronized clock or with long transmission delay. 
23|6||Peer-assisted carrying authentication (PACA)|In this paper we present a method for password recovery through the employment of multiple Web servers, and which we name Peer-Assisted Carrying Authentication (PACA). The paper starts by highlighting the vulnerabilities of the commonly used techniques for password recovery, namely the question–answer approach. It then proceeds to providing a general coverage of the proposed approach and discusses the details and offered solutions to issues that relate to implementation and security. We present a software application that we developed for proof-of-concept and as a tool for class-based experiments. These were conducted to show the ability of users to hack accounts of other users with whom they have or had some kind of relationship and test the effectiveness of piecewise password recovery. The results indicate that people who are close to others can often guess some of their passwords correctly and therefore, are able to hack their computer accounts. It is shown that PACA makes the hacker's job very difficult through the multiple peer authentication mechanism. In this regard, the findings could be used to set a lower bound on the number of peer sites for authenticating users. 
23|6||Vulnerability forecastingâa conceptual model|Vulnerability scanners (VSs) are information security tools able to detect security weaknesses on hosts in a network. VSs secure hosts in a proactive manner. A proactive approach is considered to be better than reactive approaches followed by, for example, intrusion detection systems, because prevention is better than cure. There are many problems and disadvantages of currently available VSs, such as hampering system resources while conducting scans. This paper introduces a conceptual model for vulnerability forecasting. The model uses intelligent techniques to improve on the efficiency of currently available VSs. The model aims to do vulnerability forecasting specifically by predicting the number of known vulnerabilities that will occur in the near future by using intelligent techniques and vulnerability history data. The model is tested by means of a prototype and an evaluation of the model's results is also provided in the paper. 
23|6||An operational model and language support for securing XML documents|In this paper we present an operational model for XML document security. Given an XML document X, the operational model defines the process of encrypting data and embedding digital signatures which sign the data in X. The secured XML document Xs includes encrypted and unencrypted data of X, and embedded digital signatures. The operational model also defines the processes of decrypting Xs and verifying the digital signatures embedded in Xs. It offers a security mechanism which integrates element-wise encryption and temporal-based element-wise digital signatures. Our operational model provides element-wise encryption that is more general than previous forms of XML security, by including element encryption, content encryption, and two types of attribute encryption. Moreover, the model of temporal-based element-wise digital signature is novel. Based on the generalized operational model, we define a new language—called document security language (DSL)—to support it. The syntax of the encrypted document and the corresponding transformation language are presented. For automation reasons, the DSL includes a definition for the “standard DSL algorithm downloading and linking protocol” which fulfills automatic algorithm download and linking requirements in the operational model. This makes the DSL based securing tool configurable. Two different implementations further demonstrate its practicability: one uses the Java programming language to implement the securing tool, whilst the other employs the extension mechanism of XSLT 1.0 to implement the encryption and decryption transforms. The two implementations are available free on the Internet. Experimental results obtained when using our securing tool demonstrate the automation, efficiency, and practicability of the proposal operational model. In addition, we have developed a DSL editor with a friendly graphic user interface to make it easier for users to generate DSL documents. 
23|7|http://www.sciencedirect.com/science/journal/01674048/23/7|The gap between cryptography and information security: has it narrowed?|
23|7||Security views|
23|7||Improving the ROI of the security management process|This article provides a number of guidelines for improving the ROI of the information security process. Where security initiatives are concerned, an important component of this ROI is realised in the form of risk mitigation and it is important to include this as an explicit factor in the ROI calculation. This calculation need only be sufficiently accurate to support the decision making process. Distinguishing between tactical and strategic initiatives will enable organisations to respond to short-term business drivers without interrupting strategic projects. Organisations will then be able to concentrate on re-engineering current processes in order to better align them with business needs. 
23|7||Events Calendar|
23|7||An approach to reliably identifying signs of DDOS flood attacks based on LRD traffic pattern recognition|In the aspect of intrusion detection, reliable detection remains a challenge issue as stated in Kemmrer and Vigna (Suppl IEEE Comput (IEEE Secur Priv) 35(4) (2002) 28). “The challenge is to develop a system that detects close to 100% of attacks with minimal false positives. We are still far from achieving this goal.” Hence, reliable detection of distributed denial-of-service (DDOS) attacks is worth studying. By reliable detection, we mean that signs of attacks can be identified with predetermined detection probability and false alarm probability. This paper focuses on reliable detection of DDOS flood attacks by identifying pattern of traffic with long-range dependence (LRD). In this aspect, there are three fundamental issues in theory and practice:•What is a statistical feature of traffic to be used for pattern recognition?•How to represent distributions of identification probability, false alarm probability and miss probability?•How to assure a decision-making that has high identification probability, low false alarm probability and low miss probability?This paper gives a statistical detection scheme based on identifying abnormal variations of LRD traffic time series. The representations of three probability distributions mentioned above are given and a decision-making region is explained. With this region, one can know what an identification (or false alarm or miss) probability is for capturing signs of DDOS flood attacks. The significance of a decision-making region is that it provides a guideline to set appropriate threshold value so as to assure high identification probability, low false alarm probability and low miss probability. A case study is demonstrated. 
23|7||Towards Web Service access control|The Internet has revolutionised the capacity to share information and services across organisations. Web Service technology enables organisations to exploit software as a service. Services are accessed by method invocations. Method interfaces are described and published, and may be freely available. Method requests and responses are conveyed in SOAP, which has the ability to pass unhindered through firewalls. Applications that process SOAP requests may be endangered by messages with malicious intent. Protection of methods and resources exposed by SOAP is thus a critical requirement for Web Services to be acceptable to organisations. In Web Service environments, access control is required to cross the borders of security domains, to be implemented between heterogeneous systems. New approaches are required that would address the movement of unknown users across borders so that access to resources can be granted. Specifications have been released to address access control, but are not well established. In this paper, an analysis of current approaches to Web Service access control is made, which leads to five requirements to be addressed by future access control solutions. To address such requirements, a logic-based access control approach is defined for a Web Service endpoint. The paper does not address the access control logic that is required when more than one Web Service is used in an integrated business solution. 
23|7||Enhanced three-party encrypted key exchange without server public keys|This investigation proposes a secure and efficient three-party encrypted key exchange (3PEKE) protocol based on the LSSH-3PEKE protocol proposed by Lin et al. [Lin, C.-L., Sun, H.-M., Steiner, M., Hwang, T., 2001. IEEE Commun. Lett. 5 (12), 497–499]. The computational cost of the proposed protocol is equal to that of the LSSH-3PEKE protocol. However, the number of steps in communication is one fewer. A round efficient version of the same protocol is also described. 
23|7||Authentication and authorization infrastructures (AAIs): a comparative survey|In this article, we argue that traditional approaches for authorization and access control in computer systems (i.e., discretionary, mandatory, and role-based access controls) are not appropriate to address the requirements of networked or distributed systems, and that proper authorization and access control requires infrastructural support in one way or another. This support can be provided, for example, by an authentication and authorization infrastructure (AAI). Against this background, we overview, analyze, discuss, and put into perspective some technologies that can be used to build and operate AAIs. More specifically, we address Microsoft .NET Passport and some related activities (e.g. the Liberty Alliance Project), Kerberos-based solutions, and AAIs that are based on digital certificates and public key infrastructures (PKIs). We conclude with the observation that there is no single best approach for providing an AAI, that every approach has specific advantages and disadvantages, and that a comprehensive AAI must combine various technologies and approaches. 
23|7||Filtering XPath expressions for XML access control|XPath is a standard for specifying parts of XML documents and a suitable language for both query processing and access control of XML. In this paper, we use the XPath expression for representing user queries and access control for XML. And we propose an access-control method for XML, where we control accesses to XML documents by filtering query XPath expressions through access-control XPath expressions. For filtering the access-denied parts out of query XPath expressions, set operations (such as, intersection and difference) between the XPath expressions are essential. However, it is known that the containment problem of two XPath expressions is coNP-hard when the XPath expressions contain predicates (or branch), wildcards and descendant axes. To solve the problem, we directly search XACT (XML Access Control Tree) for a query XPath expression and extract the access-granted parts. The XACT is our proposed structure, where the edges are structural summary of XML elements and the nodes contain access-control information. We show that the query XPath expressions are successfully filtered through the XACT by our proposed method, and also show the performance improvement by comparing the proposed method with the previous work. 
23|7||Personalised cryptographic key generation based on FaceHashing|Among the various computer security techniques practice today, cryptography has been identified as one of the most important solutions in the integrated digital security system. Cryptographic techniques such as encryption can provide very long passwords that are not required to be remembered but are in turn protected by simple password, hence defecting their purpose. In this paper, we proposed a novel two-stage technique to generate personalized cryptographic keys from the face biometric, which offers the inextricably link to its owner. At the first stage, integral transform of biometric input is to discretise to produce a set of bit representation with a set of tokenised pseudo random number, coined as FaceHash. In the second stage, FaceHash is then securely reduced to a single cryptographic key via Shamir secret-sharing. Tokenised FaceHashing is rigorously protective of the face data, with security comparable to cryptographic hashing of token and knowledge key-factor. The key is constructed to resist cryptanalysis even against an adversary who captures the user device or the feature descriptor. 
23|7||Improvement on Li et al.'s generalization of proxy signature schemes|Recently, Li et al. proposed their generalization of proxy signature schemes. However, all of Li et al.'s schemes have a common security weakness. In Li et al.'s schemes, an adversary first intercepts a valid proxy signature generated by a proxy group on behalf of the proxy group GP. From the intercepted proxy signature, the adversary can forge illegal proxy signatures being likely generated by the proxy group on behalf of an adversary. To overcome this weakness, our improvement is also proposed. 
23|8|http://www.sciencedirect.com/science/journal/01674048/23/8|Is the U.S. government really getting serious about information security?|
23|8||Security views|
23|8||From secure wired networks to secure wireless networks â what are the extra risks?|This paper investigates the information security requirements for wireless networks, and then compares that with the information security requirements for wired networks. The extra requirements needed for wireless networks security are then identified and discussed. 
23|8||A framework for the governance of information security|This paper highlights the importance of protecting an organization's vital business information assets by investigating several fundamental considerations that should be taken into account in this regard. Based on this, it is illustrated that information security should be a priority of executive management, including the Board and CEO and should therefore commence as a corporate governance responsibility. This paper, therefore, motivates that there is a need to integrate information security into corporate governance through the development of an information security governance (ISG) framework. This paper further proposes such a framework to aid an organization in its ISG efforts. 
23|8||Events Calendar|
23|8||XML distributed security policy for clusters|With the increasing use of clusters, efficient and flexible security has now become an essential requirement, though it has not yet been addressed in a coherent fashion for distributed systems.This paper presents a new security policy language for clusters: Distributed Security Policy (DSP). Based on XML, this language offers a precise and easy way to customize security of clusters. Contrary to other existing security policy languages, it is not limited to access control, and may be used for other security services such as cluster's inner communication.Finally, the paper also explains how this security policy is used in practice, and how it is transparently enforced onto all nodes of the cluster. 
23|8||Access control in a hierarchy using one-way hash functions|This paper presents a cryptographic key management solution to solve the access control problem in a hierarchy. Based on one-way hash functions, an efficient key assignment and derivation method is proposed. This solution uses limited number of keys and hash functions. Also, the dynamic access control problems, such as adding/deleting nodes, or modifying relationships between nodes in the hierarchy are considered and can be resolved. 
23|8||Characterization of defense mechanisms against distributed denial of service attacks|We propose a characterization of distributed denial of service (DDOS) defenses where reaction points are network-based and attack responses are active. The purpose is to provide a framework for comparing the performance and deployment of DDOS defenses. We identify the characteristics in attack detection algorithms and attack responses by reviewing defenses that have appeared in the literature. We expect that this characterization will provide practitioners and academia insights into deploying DDOS defense as network services. 
23|8||Embedding biometric identifiers in 2D barcodes for improved security|Two-dimensional (2D) barcode symbology is an emerging technology used for compactly storing and retrieving information. These barcodes can be found on the back of drivers' licenses and are encoded with secure text data. Standard 2D barcode such as PDF417 uses upper and lowercase alphabets, numeric digits and special characters for encoding. Some barcodes also include a compressed photo of the individual. The visual quality of the compressed image is usually poor and occupies a large amount of space which greatly reduces the capacity needed for encoding text. This paper presents a novel approach for embedding uncompressed images in a standard PDF417 2D barcode using a blind digital watermarking technique. The text is encoded in the standard PDF417 format with error correction, while the face and fingerprint images are watermarked in the encoded 2D barcode. Experimental results show that the proposed technique effectively increased the standard capacity of the PDF417 2D barcode without altering the contents of the encoded data. The results also show that the visual quality of the extracted photo image is high. The extracted fingerprint image when compared with the original fingerprint using an AFIS system yielded a high matching score. 
23|8||Modelling and solving the intrusion detection problem in computer networks|We introduce a novel anomaly intrusion detection method based on a Within-Class Dissimilarity, WCD. This approach functions by using an appropriate metric WCD to measure the distance between an unknown user and a known user defined respectively by their profile vectors. First of all, each user performs a set of commands (events) on a given system (Unix for example). The events vector of a given user profile is a binary vector, such that an element of this vector is equal to “1” if an event happens, and to “0” otherwise. In addition to this, each user's class k has a typical profile defined by the vector Pk, in order to test if a new user i defined by its profile vector Pi belongs to the same class k or not. The Pk vector is a weighted events vector Ek, such that each weight represents the number of occurrences of an event ek. If the “distance” dki (measured by a dissimilarity parameter) between an unknown profile Pi and a known profile Pk is reasonable according to a given threshold and to some constraints, then there is no intrusion. Else, the user i is suspicious. A simple example illustrates the WCD procedure. A survey of intrusion detection methods is presented.Our proposed method based on clustering users and using simple statistical formulas is very easy for implementation. 
23|8||New efficient user identification and key distribution scheme providing enhanced security|Apart from user identification and key distribution, it is very useful for the login process to achieve user anonymity. Recently, Wu and Hsu proposed an efficient user identification scheme with key distribution while preserving user anonymity by extending an earlier work of Lee and Chang. We however find out that the Wu and Hsu scheme has a serious weakness, which can be exploited by the service provider to learn the secret token of the user who requests services from the service provider. We further propose a scheme to overcome this limitation while attaining the same set of objectives as the previous works. Performance analyses have shown that efficiency in terms of both computation and communication is not sacrificed in our scheme. 
23|8||A hybrid scheme for multicast authentication over lossy networks|For multicast communication, authentication is a challenging problem, since it requires that a large number of recipients must verify the data originator. Many of multicast applications are running over IP networks, in which several packet losses could occur. Therefore, multicast authentication protocols must resist packet loss. Other requirements of multicast authentication protocols are: to perform authentication in real-time and to have low communication and computation overheads. In the present paper, a hybrid scheme for authenticating real-time data applications, in which low delay at the sender is acceptable, is proposed. In order to provide authentication, the proposed scheme uses both public key signature and hash functions. It is based on the idea of dividing the stream into blocks of m packets. Then a chain of hashes is used to link each packet to the one preceding it. In order to resist packet loss, the hash of each packet is appended to another place in the stream. Finally, the first packet is signed. The proposed scheme resists packet loss and is joinable at any point. The proposed scheme is compared to other multicast authentication protocols. The comparison shows that the proposed scheme has the following advantages: first, it has low computation and communication overheads. Second, it has reasonable buffer requirements. Third, the proposed scheme has a low delay at the sender side and no delay at the receiver side, assuming no loss occurs. Finally, its latency equals to zero, assuming no loss occurs. 
24|1|http://www.sciencedirect.com/science/journal/01674048/24/1|Security views: Malware update|
24|1||The concept of security and trust in electronic payments|The use of electronic communication channels to conduct businesses without the need for physical conduct or presence has already been established and accepted warmly. But the issue of paying electronically still remains risky and muddy. This article implicates the security and trust issues that are essential for every electronic payment mechanism in order to be accepted and established as a common medium of financial transactions. 
24|1||Management of risk in the information age|Linked together, organisations can exchange information and engage in transactions in ways unanticipated before, the emphasis being on information, which became core to most business activities and without which business will fail to operate [Owens S. Information security management :an introduction. London: British Standards Institution; 1998. pp. 1–2]. Consequently, to contribute to ensuring business continuity, the protection of information resources had to be pursued. Risk analysis was traditionally used to analyse risks posing a threat to mostly IT assets [Jung C, Han I, Suh B. Risk analysis for electronic commerce using case-based reasoning. International Journal of Intelligent Systems in Accounting, Finance & Management 1999;8:61–73. John Wiley & Sons, Ltd., p. 62]. Resulting in recommendations for the implementation of appropriate security measures, to reduce those identified high priority risks to an acceptable level. However, Bandyopadhyay et al. [Bandyopadhyay K, Mykytyn PP, Mykytyn K. A framework for integrated risk management in information technology. Management Decision 1999;37(5):437–44. MCB Press, p. 440] state that the evaluation of risk related to IT alone is unrealistic. A holistic view of assessing risks should instead be adopted, moving away from the isolated and partial view of today's “closed world assumption” of searching only within a specific domain to evaluate the risks associated to IT, to consider the entire spectrum related to the IT environment. Thus an alternative approach to risk analysis might have to be developed, to assist in analysing risks to information-specific resources. 
24|1||Events Calendar|
24|1||A taxonomy of network and computer attacks|Attacks over the years have become both increasingly numerous and sophisticated. This paper focuses on the provisioning of a method for the analysis and categorisation of both computer and network attacks, thus providing assistance in combating new attacks, improving computer and network security as well as providing consistency in language when describing attacks. Such a taxonomy is designed to be useful to information bodies such as CERTs (Computer Emergency Response Teams) who have to handle and categorise an every increasing number of attacks on a daily basis. Information bodies could use the taxonomy to communicate more effectively as the taxonomy would provide a common classification scheme. The proposed taxonomy consists of four dimensions which provide a holistic taxonomy in order to deal with inherent problems in the computer and network attack field. The first dimension covers the attack vector and the main behaviour of the attack. The second dimension allows for classification of the attack targets. Vulnerabilities are classified in the third dimension and payloads in the fourth. Finally, to demonstrate the usefulness of this taxonomy, a case study applies the taxonomy to a number of well known attacks. 
24|1||A randomized RSA-based partially blind signature scheme for electronic cash|Blind signature schemes can yield a signature and message pair whose information does not leak to the signer. However, when blind signatures are used to design e-cash schemes, there are two problems. One is the unlimited growth of the bank's database which keeps all spent e-cashes for preventing double spending. Another problem is that the signer must assure himself that the message contains accurate information such as the face value of the e-cash without seeing it. Partially blind signatures can cope with these problems. In partially blind signatures, the signer can explicitly include some agreed common information such as the expiration date and the face value in the blind signature. Randomized signature schemes can withstand one-more-forgery under the chosen plaintext attack. Based on RSA cryptosystem Fan–Chen–Yeh proposed a randomized blind signature scheme and Chien–Jan–Tseng also proposed a randomized partially blind signature scheme. But, the attacker can remove the randomizing factor from the messages to be signed in these two schemes. The attacker can also change the common information of Chien–Jan–Tseng's partially blind signature. In this paper, we propose a secure randomized RSA-based partially blind signature scheme, and show that the proposed scheme satisfies the blindness and unforgeability properties. We also analyse the computation cost of the proposed scheme. 
24|1||An improvement of HwangâLeeâTang's simple remote user authentication scheme|Recently, Hwang–Lee–Tang proposed a simple remote user authentication scheme using smart card, whereby it does not require any password or verification tables in the remote system and any legal users could choose and change their passwords freely. However, their schemes previously generated user's secret hash values are insecure if the secret key of the server is leaked or is stolen, also when the smart card is stolen, unauthorized users can easily change new password of the smart card. Furthermore, their scheme cannot resist the denial of service attack using stolen smart card and does not provide mutual authentication. Accordingly, the current paper demonstrates the vulnerability of Hwang–Lee–Tang's scheme and presents an enhancement to resolve such problems. As a result, the proposed scheme previously generated secret hash values are secure even if the secret key of the system is leaked or is stolen and enables users to update their passwords freely and securely, while also providing mutual authentication and fast detect it when user inputs wrong password. In addition, the computational costs of this scheme are less than those of any previously proposed schemes. 
24|1||H2A: Hybrid Hash-chaining scheme for Adaptive multicast source authentication of media-streaming|Many applications, such as broadcasting stock quotes and video-conferencing require data source authentication of the received multicast traffic. Multicast data source authentication must take into consideration the scalability and the efficiency of the underlying cryptographic schemes and mechanisms, because multicast groups can be very large and the exchanged data are likely to be important in volume (streaming). Besides, multicast data source authentication must be robust enough against packet loss because most of multicast multimedia applications do not use reliable packet delivery.In this paper, we propose a hybrid hash-chaining scheme in conjunction with an adaptive and efficient data source authentication protocol which tolerates packet loss and guarantees non-repudiation of media-streaming origin. We have simulated our protocol using NS-2, and the simulation results show that the protocol has remarkable features and efficiency compared to other recent data source authentication protocols. 
24|1||Information security obedience: a definition|Information is a fundamental asset within any organisation and the protection of this asset, through a process of information security, is of equal importance. This paper examines the relationships that exist between the fields of corporate governance, information security and corporate culture. It highlights the role that senior management should play in cultivating an information security conscious culture in their organisation, for the benefit of the organisation, senior management and the users of information. 
24|1||Improvements on the WTLS protocol to avoid denial of service attacks|The current WTLS protocol is closely modeled after the well-studied SSL protocol. However, since some differences exist between these two protocols, even if the SSL protocol is secure, the WTLS protocol may not.We propose three kinds of possible Denial of Service (DoS) attacks on the existing WTLS protocol, which can be categorized into two types: memory exhaustion attacks and CPU exhaustion attacks. The first and the second kinds of attacks belong to memory exhaustion attacks, and the third kind of attack is a CPU exhaustion attack.Not only wireless network clients but also Internet clients can launch these three kinds of attacks, which are very simple and effective. Since Internet clients are more powerful in network bandwidth and CPU resources, damages made by these attackers are more serious, which can even make the WTLS server stop providing services for legitimate clients.Client cookies, client puzzles and an application timer is used to improve the current protocol, and our improvements are secure against such attacks. 
24|1||Multiplexer-based double-exponentiation for normal basis of GF(2m)|In many cryptographic protocols, double-exponentiation is a key arithmetic operation. In this study, we will present a multiplexer-based algorithm for double-exponentiation in GF(2m). The proposed algorithm utilizes the concept of the modified Booth's algorithm. Multiplexers are employed for implementation of the proposed algorithm. The proposed double-exponentiation algorithm only requires m multiplications and saves about 66% time complexity while comparing with the ordinary binary method. 
24|2|http://www.sciencedirect.com/science/journal/01674048/24/2|Search engines: a growing contributor to security risk|
24|2||Lycos crosses the line|
24|2||Security views: Malware Update|
24|2||Information Security governance: COBIT or ISO 17799 or both?|This paper investigates the co-existence of and complementary use of COBIT and ISO 17799 as reference frameworks for Information Security governance. The investigation is based on a mapping between COBIT and ISO 17799 which became available in 2004, and provides a level of ‘synchronization’ between these two frameworks. 
24|2||The economic approach of information security|This article introduces to the reader the sceptic of the economic evaluation of a security framework. We identify that there must be an economic evaluation of security investment, in order to avoid cost and risks of a security breach. We vindicate why the security economic plan must encompass our choices to provide security solutions. Furthermore, what are the measurements that are employed to provide the confidence of security to an acceptable level. 
24|2||Keyjacking: the surprising insecurity of client-side SSL|In theory, PKI can provide a flexible and strong way to authenticate users in distributed information systems. In practice, much is being invested in realizing this vision via client-side SSL and various client keystores. However, whether this works depends on whether what the machines do with the private keys matches what the humans think they do: whether a server operator can conclude from an SSL request authenticated with a user's private key that the user was aware of and approved that request. Exploring this vision, we demonstrate via a series of experiments that this assumption does not hold with standard desktop tools, even if the browser user does all the right things. A fundamental rethinking of the trust, usage, and storage model might result in more effective tools for achieving the PKI vision. 
24|2||Analysis of end user security behaviors|Many information security specialists believe that promoting good end user behaviors and constraining bad end user behaviors provide one important method for making information security effective within organizations. Because of the important of end user security-related behaviors, having a systematic viewpoint on the different kinds of behavior that end users enact could provide helpful benefits for managers, auditors, information technologists, and others with an interest in assessing and/or influencing end user behavior. In the present article, we describe our efforts to work with subject matter experts to develop a taxonomy of end user security-related behaviors, test the consistency of that taxonomy, and use behaviors from that taxonomy to conduct a U.S. survey of an important set of end user behaviors. We interviewed 110 individuals who possessed knowledge of end user security-related behaviors, conducted a behavior rating exercise with 49 information technology subject matter experts, and ran a U.S. survey of 1167 end users to obtain self-reports of their password-related behaviors. Results suggested that six categories of end user security-related behaviors appeared to fit well on a two-dimensional map where one dimension captured the level of technical knowledge needed to enact the behavior and another dimension captured the intentionality of the behavior (including malicious, neutral, and benevolent intentions). Our U.S. survey of non-malicious, low technical knowledge behaviors related to password creation and sharing showed that password “hygiene” was generally poor but varied substantially across different organization types (e.g., military organizations versus telecommunications companies). Further, we documented evidence that good password hygiene was related to training, awareness, monitoring, and motivation. 
24|2||Cryptanalyses of two key assignment schemes based on polynomial interpolations|Wu and Chang [2001. Cryptographic key assignment scheme for hierarchical access control. International Journal of Computer Systems Science and Engineering 16(1), 25–28] and Shen and Chen [2002. A novel key management scheme based on discrete logarithms and polynomial interpolations. Computers & Security 21(2), 164–171] separately proposed a cryptographic key assignment scheme based on polynomial interpolations to solve the access control problem in a partially ordered user hierarchy. Recently, Hsu and Wu [2003. Cryptanalyses and improvements of two cryptographic key assignment schemes for dynamic access control in a user hierarchy. Computers & Security 22(5), 453–356] cryptanalyzed both schemes by finding the roots of polynomials. Then they proposed some modifications to fix the security flaw. In this paper, we show that both Wu–Chang and Shen–Chen schemes are still insecure even with these modifications. From the public information, the attacker could derive the secret keys of some security classes. Furthermore, the attack could be mounted even without using any polynomial root finding algorithm. 
24|2||Dealing with packet loss in the Interactive Chained Stream Authentication protocol|This paper presents an improvement to the Interactive Chained Stream Authentication (I-CSA) protocol which is an efficient protocol useful to authenticate multicast transmissions over a network. The lockstep behavior intrinsic in its structure for data exchange may produce deadlocks, but not security breaches, in case the authentication information of the protocol is destroyed or modified by an intruder. The solution proposed (Enhanced I-CSA) introduces some modifications to the I-CSA protocol. This solution allows the parties involved in the transmission to continue the exchange of data in a secure manner even in case of packet losses, by accepting a reduced efficiency with respect to the I-CSA protocol. 
24|2||ISRAM: information security risk analysis method|Continuously changing nature of technological environment has been enforcing to revise the process of information security risk analysis accordingly. A number of quantitative and qualitative risk analysis methods have been proposed by researchers and vendors. The purpose of these methods is to analyze today's information security risks properly. Some of these methods are supported by a software package. In this study, a survey based quantitative approach is proposed to analyze security risks of information technologies by taking current necessities into consideration. The new method is named as Information Security Risk Analysis Method (ISRAM). Case study has shown that ISRAM yields consistent results in a reasonable time period by allowing the participation of the manager and staff of the organization. 
24|2||Empirical evaluation of SVM-based masquerade detection using UNIX commands|Masqueraders who impersonate other users pose serious threat to computer security. Unfortunately, firewalls or misuse-based intrusion detection systems are generally ineffective in detecting masqueraders. Although anomaly detection techniques have long been considered as an effective approach to complement misuse detection techniques, they are not widely used in practice due to poor accuracy and relatively high degree of false alarms. In this paper, we performed an empirical study investigating the effectiveness of SVM (support vector machine) in detecting masquerade activities using two different UNIX command sets used in previous studies [R. Maxion, N. Townsend, Proceedings of international conference on dependable systems and networks (DSN-02), p. 219–28, June 2002; R. Maxion, Proceedings of international conference on dependable systems and networks (DSN-03), p. 5–14, June 2003]. Concept of “common commands” was introduced as a feature to more effectively reflect diverse command patterns exhibited by various users. Though still imperfect, we detected masqueraders 80.1% and 94.8% of the time, while the previous studies reported the accuracy of 69.3% and 62.8%, respectively, using the same data set containing only the command names. When command names and arguments were included in the experiment, SVM-based approach detected masqueraders 87.3% of the time while the previous study, using the same data set, reported 82.1% of accuracy. These combined experiments convincingly demonstrate that SVM is an effective approach to masquerade detection. 
24|2||An improvement on efficient anonymous auction protocols|In this paper, we propose an improvement on Chang et al.'s efficient anonymous auction protocols to overcome the security weakness in initiation phase of Chang et al.'s scheme. At first, we formally analyze the initiation phase of Chang et al.'s scheme on the basis of authentication tests to disclose the insecurity of the initiation phase. Then we give our attack to the initiation phase, which can make the following auction phase fail to run. Later, we propose our improvement on the initiation phase. Finally, we formally analyze our improved scheme with authentication tests, and prove the security of our improved scheme. Our improved scheme can preserve the merits of Chang et al.'s scheme and overcome the security weakness in initiation phase of Chang et al.'s scheme. 
24|3|http://www.sciencedirect.com/science/journal/01674048/24/3|Security dilemmas with Microsoft's Internet Explorer|
24|3||Security views - Malware update|
24|3||Smart card based authentication â any future?|
24|3||Performance of the Java security manager|The Java Security Manager is one major security feature of the Java programming language. However, in many Java applications the Security Manager is not enabled because it slows execution time. This paper explores the performance of the Java Security Manager in depth, identifies the permissions with the worst performance and gives advice on how to use the Security Manager in a more efficient way.Our performance test shows that the CPU execution time penalty varies between 5% and 100% per resource access statement. This extreme range is due to the fact that some resource accesses are costly (such as file and socket access) and therefore hide the performance penalty for the access control check almost completely. The time penalty is much more noticeable with access to main memory resources (such as Java objects).In order to achieve reasonable response times, it is of utmost importance to tune garbage collection because the Java Security Manager creates short-lived objects during its permission check. Also, the order of permissions in the policy file can be important. 
24|3||Secure business application logic for e-commerce systems|The major reason why most people are still sceptical about e-commerce is the perceived security and privacy risks associated with e-transactions, e.g., data, smart cards, credit cards and exchange of business information by means of online transactions. Today, vendors of e-commerce systems have relied solely on secure transaction protocols such as SSL, while ignoring the security of server and client software. This article, Secure Business Application Logic for e-commerce Systems, discusses a key weak link in e-commerce systems: the business application logic. Although the security issues of the front-end and back-end software systems in e-commerce application warrant equal attention, but this research focuses on the Security of Middle Tier of e-commerce server that implements the business application logic and traditionally, e-commerce sites implemented the middle tier of software on the web server using CGI. We also present strategies for secure business application logic: good design and engineering, secure configuration, defensive programming and secure wrappers for server-side software. 
24|3||Multiple behavior information fusion based quantitative threat evaluation|How to evaluate network security threat quantitatively is one of key issues in the field of network security, which is vital for administrators to make decision on the security of computer networks. A novel model of security threat evaluation with a series of quantitative indices is proposed on the analysis of prevalent network intrusions. This model is based on multiple behavior information fusion and two indices of privilege validity and service availability that are proposed to evaluate the impact of prevalent network intrusions on system security, so as to provide security evolution over time, i.e., monitor security changes with respect to modification of security factors. The Markov model and the algorithm of D-S evidence reasoning are proposed to measure these two indices, respectively. Compared with other methods, this method mitigates the impact of unsuccessful intrusions on threat evaluation. It evaluates the impact of important intrusions on system security comprehensively and helps administrators to insight into intrusion steps, determine security state and identify dangerous intrusion traces. Testing in a real network environment shows that this method is reasonable and feasible in alleviating the tremendous task of data analysis and facilitating the understanding of the security evolution of the system for its administrators. 
24|3||Matching key recovery mechanisms to business requirements|This paper addresses the business needs for key recovery as a countermeasure to the threat of losing potentially valuable information. Several requirements essential for a sound key recovery mechanism are described, and the applicability of two main classes of existing key recovery schemes to a corporate environment is examined. Different requirements are identified for key recovery mechanisms for communicated and archived data, and a further study is made of the applicability of existing mechanisms to these two cases. 
24|3||Information systems security policies: a contextual perspective|The protection of information systems is a major problem faced by organisations. The application of a security policy is considered essential for managing the security of information systems. Implementing a successful security policy in an organisation, however, is not a straightforward task and depends on many factors. This paper explores the processes of formulating, implementing and adopting a security policy in two different organisations. A theoretical framework based on the theory of contextualism is proposed and applied in the analysis of these cases. The contextual perspective employed in this paper illuminates the dynamic nature of the application of security policies and brings forth contextual factors that affect their successful adoption. 
24|4|http://www.sciencedirect.com/science/journal/01674048/24/4|Personal information compromises: It is time for the U.S. Government to wake up|
24|4||Security views|
24|4||From information security toâ¦business security?|This short opinion paper argues that information security, the discipline responsible for protecting a company's information assets against business risks, has now become such a crucial component of good Corporate Governance, that it should rather be called Business Security instead of Information Security. 
24|4||Why users cannot use security|With an increasing range of potential threats, the use of security within end-user systems and applications is becoming ever more important. However, a significant obstacle to achieving this can be the usability of the security features that are offered, and although related functionality is now provided in a wide range of end-user applications, the users themselves will fail to benefit if they cannot make it work for them. This paper highlights the importance of enabling users to protect themselves, and identifies that they may currently encounter problems in terms of finding, understanding, and ultimately using the security features that are meant to be at their disposal. The security options within Microsoft Word are used to provide illustrative examples of typical problems, with consequent suggestions to improve both the presentation and guidance available to users within such applications. 
24|4||Infection dynamics on the Internet|In previous works, the connectivity of nodes in social networks such as the Internet has been shown to follow a scale-free distribution in which there is a larger probability of nodes with lower connectivity and a smaller probability of nodes with higher connectivity. This network structure facilitates communication but also aids in the propagation of viruses. In this work, solutions have been obtained for a dynamical mean-field equation that characterizes virus infections and growth in scale-free networks. In contrast to previous findings, a threshold condition has been found for the persistence of computer infections. The effect of connectivity-dependent growth and recovery rates is also reported. It has been found that it is possible to reduce the deleterious effects of viruses by preferentially discouraging growth and enhancing recovery in high-connectivity nodes. Significantly, a security “figure-of-merit” has been derived that will allow network administrators to sample their environment in real time and measure the risk relative to E-mail-borne threats. 
24|4||Real-time intrusion detection for high-speed networks|Network-based intrusion detection systems (NIDSs) frequently have problems with handling heavy traffic loads in real-time, which result in packet loss and false negatives. This paper presents a high-performance network intrusion detection system, called HPMonitor, which combines a high-efficiency detection engine and a load-balancing device to address these problems. The paper describes HPMonitor's system architecture, discusses a flow-based dynamic load-balancing algorithm called dynamic least load first (DLLF) algorithm, and introduces a new multi-pattern string matching algorithm called shift max algorithm (SMA). The test results reveal that the DLLF algorithm is an effective balancing algorithm for NIDS. Meanwhile, the experimental results show that the SMA algorithm is faster in searching large sets of patterns when compared with other algorithms, and its performance is affected little when the patterns set number increases. 
24|4||Feature deduction and ensemble design of intrusion detection systems|Current intrusion detection systems (IDS) examine all data features to detect intrusion or misuse patterns. Some of the features may be redundant or contribute little (if anything) to the detection process. The purpose of this study is to identify important input features in building an IDS that is computationally efficient and effective. We investigated the performance of two feature selection algorithms involving Bayesian networks (BN) and Classification and Regression Trees (CART) and an ensemble of BN and CART. Empirical results indicate that significant input feature selection is important to design an IDS that is lightweight, efficient and effective for real world detection systems. Finally, we propose an hybrid architecture for combining different feature selection algorithms for real world intrusion detection. 
24|4||Secure information systems development â a survey and comparison|Nowadays, security solutions are mainly focused on providing security defences (such as firewalls, routers, configuration server, password and encryption) instead of solving one of the main reasons of security problems that refers to an appropriate information systems design. Fortunately, there have been developed new methodologies incorporating security into their development processes. This paper makes a comparison of eleven secure systems design methodologies. The analysed methodologies fulfil criteria partially and in this paper, we make it clear that security aspects cannot be completely specified by these methodologies since they have a series of limitations that we have to take into account. At the same time, each one of these methodologies comprises very important aspects concerning security that can be used as a basis for new methodologies or extensions that may be developed. 
24|4||Information Assurance for security protocols|Security protocols are used pervasively to protect distributed communications in the third Millennium. This motivates the need for a definition of Information Assurance for security protocols, which, to the best of our knowledge, is still missing. Such a definition is advanced in terms of the requirements that security protocols be analysed at the same time realistically, accurately and formally, notions that the existing literature only favours in separate contexts. The precise meanings of these terms are described by means of general considerations and concrete examples. The main goal of this paper is to draw attention to and raise concern on this novel but significant niche of computer security. 
24|4||A survey and trends on Internet worms|With the explosive growth and increasing complexity of network applications, the threats of Internet worms against network security are more and more serious. This paper presents the concepts and research situations of Internet worms, their function component, and their execution mechanism. It also addresses the scanning strategies, propagation models, and the critical techniques of Internet worm prevention. Finally, the remaining problems and emerging trends in this area are also outlined. 
24|5|http://www.sciencedirect.com/science/journal/01674048/24/5|Non-infosec professionals in infosec?|
24|5||Security views|
24|5||Technology evolution drives need for greater information technology security|Securing the enterprise perimeter is not enough; we must secure the data itself. This simple idea has taken decades to become a practical technology, and is built upon a foundation of policies and processes. 
24|5||The five Ps of patch management: Is there a simple way for businesses to develop and deploy an advanced security patch management strategy?|
24|5||Recent attacks on alleged SecurID and their practical implications|SecurID tokens are developed by SDTI/RSA Security to authenticate users to a corporate computer infrastructure. In this paper we show the results of our analysis of the function contained in these tokens. The block cipher at the heart of the function can be broken in milliseconds. We present two attack scenarios on the full function: if one can observe the output of the device during some time period, one can predict with high probability future output values and one can recover the secret key significantly faster than by exhaustive search. 
24|5||A preliminary model of end user sophistication for insider threat prediction in IT systems|The dangers that originate from acts of IT system misuse by legitimate users constitute a separate category of threats with well documented consequences for the integrity, privacy and availability of computer systems and networks. Amongst the various properties of malicious legitimate users one of the most notable ones is the level of his/her sophistication. Various studies indicate that user sophistication and the potential to misuse IT systems are properties that are strongly related. This paper presents a methodology that automates the process of gauging end user sophistication. The establishment of suitable metrics to characterize end user sophistication is discussed followed by an experimental verification of the metrics on a sample of 60 legitimate users, using the UNIX Operating System. The results indicate that a combination of application execution audits and computational resource utilization metrics could be used to characterize the level of IT sophistication of an end user. Although additional testing in a greater variety of computational environments is required in order to validate the derived preliminary scheme, it is considered that the derived methodology could serve as a component of experimental insider threat prediction processes, or any other model that requires a procedure to measure the level of IT knowledge of a legitimate user base. 
24|5||Secure authentication scheme for session initiation protocol|The Session Initiation Protocol provides an expandable and easy solution to the IP-based telephony environment. When users ask to use an SIP service, they need to be authenticated in order to get service from the server. Therefore, some SIP authentication procedure schemes were proposed to meet the above demand. However, there are security problems that need to be solved, such as off-line password guessing attacks and server spoofing. In this article, we shall propose a new scheme for a secure authentication procedure for the Session Initiation Protocol to enhance the security of the original scheme. 
24|5||CIDS: An agent-based intrusion detection system|The paper describes security agent architecture, called CIDS, which is useful as an administrative tool for intrusion detection. Specifically, it is an agent-based monitoring and detection system, which is developed to detect malfunctions, faults, abnormalities, misuse, deviations, intrusions, and provide recommendations (in the form of common intrusion detection language). The CIDS can simultaneously monitor networked-computer activities at multiple levels (user to packet level) in order to find correlation among the deviated values (from the normal or defined policy) to determine specific security violations. The current version of CIDS (CIDS 1.4) is tested with different simulated attacks in an isolated network, and some of those results are reported here. 
24|5||Two-level controllers hierarchy for a scalable and distributed multicast security protocol|LKH protocols are considered one of the best solutions proposed for solving the scalability of multicast security protocols. It has been proved that binary trees have the best computation and communication overheads. For a binary tree, the computation and communication overheads – in case of a member join or leave – of LKH protocols is about 2h encryption operations, where h represents the tree height. Many enhancements to LKH protocols are proposed in order to lower the computation and communication overheads. One of these solutions, a protocol that we named CEKPS protocol achieves a lower computation overhead, which is about h encryption operations and h one-way functions. In addition, its communication overhead is h messages. The abovementioned protocols rely on one manager, which could represent a bottleneck in case of a group with large number of members and where many join/leave operations occur. In the present paper, we propose a protocol for a scalable and distributed multicast security protocol. The aim of the proposed protocol is to achieve a lower computation overhead compared to CEKPS protocol. In order to achieve its goal, the proposed protocol relies on two levels of managers in order to distribute the computation required in case of a member join or leave. The proposed protocol is based on the idea of LKH protocols. To fasten the operations required in case of a member leave or join, the proposed protocol uses one-way functions as in CEKPS protocol. The proposed protocol is compared with LKH and CEKPS protocols. The comparison is undertaken according to two criteria: the cost of encryption required for the re-key operation in case of member join or leave and the length of the re-key message. The results show that the proposed protocol outperforms both LKH and CEKPS protocols. 
24|5||Capital market reaction to defective IT products: The case of computer viruses|Studies in various industries indicate that market reaction to recall announcements is used as a catalyst to control the creation of substandard products. In the IT industry, flawed software is being blamed for the increasing numbers of computer viruses that plague information systems and the escalating costs to repair these viruses. This paper examines whether the market penalizes firms that produce substandard IT products. We use the event study methodology to assess the impact of public virus announcements on the stock prices of responsible IT vendors between 1988 and 2002. The results show that the market reacts negatively to the production of flawed Information Technology in approximately 50% of the cases. However, this negative market reaction is not statistically significant over extended periods and is limited to announcements involving certain types of defects (i.e., IT products that contain computer viruses). There was no statistically significant negative market reaction for announcements involving IT products that are susceptible to computer viruses. Our analysis implies that unlike in other industries, market forces alone cannot be used as an effective control mechanism for the production of substandard IT products. The study concludes that under these present conditions, IT vendors have little economic incentives to invest in defect-free computing. 
24|6|http://www.sciencedirect.com/science/journal/01674048/24/6|The human factor in security|
24|6||Security views|
24|6||Telecom fraud: The cost of doing nothing just went up|Craig Pollard, head of project management at Siemens Communications examines the increasing threat of telephony fraud and highlights the vulnerabilities to IT managers. 
24|6||Host intrusion prevention: Part of the operating system or on top of the operating system?|Instrusion prevention systems (IPS) are becoming essential for securing information technology (IT). However, IPS will never become a fully integral part of the operational system, because of the complexity of the problem, the changing nature of threats, and the dependence of IPS on the particulars of the applications being protected. But certain IPS functionality is likely to move into the operating system, namely innate defenses that offer simple protection against vulnerability classes, and IPS plumbing, i.e. the IPS components that are currently in kernel modules. The integration of these aspects of IPS with the operating system will result in an OS that is more adaptable and easier to develop new innovative IPS technologies upon. 
24|6||Information Security Governance â Compliance management vs operational management|This paper discusses the difference that should exist between Information Security Operational Management and Information Security Compliance Management.The paper argues that for good Information Security Governance, good IT Governance and good Corporate Governance, these two dimensions of Information Security Management should be totally separate, and housed in separate departments. 
24|6||Information security policy's impact on reporting security incidents|The New Health Privacy Rule, effective from April 14, 2003, has made it illegal for healthcare providers and insurers to release a patient's medical records without the individual's consent [Cropper, Carol Marie. How to keep prying eyes off your medical records. Business Week November 19, 2001;130–2]. Rule provisions dictate that healthcare providers and insurers must have a written information security policy and present it to patients [Cropper, Carol Marie. How to keep prying eyes off your medical records. Business Week November 19, 2001;130–2].This paper evaluated the utility of having such a policy by examining the reporting of computer abuse incidents and the reporting of the seriousness of computer abuse incidents in those hospitals that either have or do not have a written information security policy. The premise of this study is that for an information security policy to be effective, computer abuse incidents and the seriousness of those computer abuse incidents must be reported.For this study, there were two factors that were examined for all respondent hospitals. The first factor was the reporting of computer abuse incidents that occurred the year prior to the study. The second factor was the reporting of the seriousness of computer abuse incidents that occurred the year prior to the study.Survey instruments were distributed to hospitals of various sizes, specialties, ownership, and types. The questionnaire collected information about the reporting of computer abuse incidents and their seriousness level to determine if an information security policy is effective in influencing the reporting of each. In addition, background information was collected from each hospital to aid in the analysis of the survey results. 
24|6||A novel digital image watermarking scheme based on the vector quantization technique|In this paper, a novel VQ-based digital image watermarking scheme is proposed. During the encoding process of the VQ compression technique, the proposed scheme embeds a representative digital watermark in the protected image so that the watermark can be retrieved from the image to effectively prove which party is in legal possession of the copyright in case an ownership dispute arises. In our method, the codewords in the VQ codebook are classified into different groups according to different characteristics and then each binary watermark bit is embedded into the selected VQ encoded block. The main feature of the proposed scheme is that the watermark exists both in the VQ compressed image and in the reconstructed image after VQ decoding. Because the watermark is hidden inside the compressed image, which is much smaller in size, much transmission time and storage space can be saved when the compressed data, instead of the original form, are transmitted over the Internet. Furthermore, the reconstructed image has robustness against aggressive image processing. The embedded watermark can even survive JPEG lossy compression. 
24|6||The insider threat to information systems and the effectiveness of ISO17799|Insider threat is widely recognised as an issue of utmost importance for IS security management. In this paper, we investigate the approach followed by ISO17799, the dominant standard in IS security management, in addressing this type of threat. We unfold the criminology theory that has designated the measures against insider misuse suggested by the standard, i.e. the General Deterrence Theory, and explore the possible enhancements to the standard that could result from the study of more recent criminology theories. The paper concludes with supporting the argument for a multiparadigm and multidisciplinary approach towards IS security management and insider threat mitigation. 
24|6||Can critical infrastructures rely on the Internet?|The functionality of critical infrastructures (CI) depends more and more on an operative Internet, which has to secure data packet transport even during Internet Exchange Point (IX) failures and “worm” attacks. The paper focuses on the questions:•Do priority services for data packets which could be implemented in an Internet Protocol, version 6 (IPv6) world reduce CI packet loss during IX failures or a worm attack?•Do priority services, used by two CIs, lead to competition between their packets during failures and attacks?•Under what circumstances do priority services offer a safety margin for CI packet transport?Simulation shows that priority services produce dialectics of alleviation and risks during an attack. They ensure routing update information thus making routing more stable and thereby reducing packet losses, in particular for CI data packets. Whereas priority service always matters during CI failures, that service itself could be a risk for packet transport if a worm takes up the highest priority class, reserved for routing information, to ensure its unhampered propagation. Therefore, the remedy defenders introduce a crisis, i.e. priority service, which could be abused by those attacking the network. 
24|6||A distributed systems approach to secure Internet mail|One of the obstacles to improve security of the Internet is ad hoc development of technologies with different design goals and different security goals. This paper proposes reconceptualizing the Internet as a secure distributed system, focusing specifically on the application layer. The notion is to redesign specific functionality, based on principles discovered in research on distributed systems in the decades since the initial development of the Internet. Because of the problems in retrofitting new technology across millions of clients and servers, any options with prospects of success must support backward compatibility. This paper outlines a possible new architecture for internet-based mail which would replace existing protocols by a more secure framework. To maintain backward compatibility, initial implementation could offer a web browser-based front end, but the longer-term approach would be to implement the system using appropriate models of replication. 
24|6||Improvement on the flexible tree-based key management framework|Matsuzaki et al. had proposed a flexible tree-based key management framework for a terminal to connect with multiple content distribution systems (CDSs) using a public bulletin board. In their scheme, the key management center constructs the public bulletin board by utilizing symmetric cryptosystem to protect terminal node keys. On the other hand, the terminals can obtain its node keys by decrypting the cipher which is posted on the public bulletin board of CDS. However, this method is not efficient for a large group which has a number of terminals. When a terminal changes its membership, the key managerial center needs a large amount of computation to structure the public bulletin, and the terminal cannot efficiently compute its node keys from the large pubic bulletin board. In this paper, we propose an improved scheme to structure a public bulletin board efficiently by the key management center of CDS. The improved scheme is capable of a large group of terminals and ensures that low powered equipment can efficiently obtain their node key. 
24|7|http://www.sciencedirect.com/science/journal/01674048/24/7|Aligning disaster recovery and security incident response|
24|7||Security views - Malware update|
24|7||Beyond SarbanesâOxley compliance|The information security issues of the Sarbanes–Oxley Act require companies to be able to document which users accessed and attempted to modify data. Biometric systems are the only way to uniquely identify the user and to prove in a log file, which person did access and change data or was denied trying to access restricted data. Passwords alone clearly offer no protection and no identity management at all. 
24|7||Authentication of users on mobile telephones â A survey of attitudes and practices|With the ever-increasing functionality and services accessible via mobile telephones, there is a strong argument that the level of user authentication implemented on the devices should be extended beyond the Personal Identification Number (PIN) that has traditionally been used. This paper presents the results of a survey of 297 mobile subscribers, which attempted to assess their use of mobile devices, their use of current authentication methods, and their attitudes towards future security options. The findings revealed that the majority of the respondents make significant use of their devices, with clear demands for protection against unauthorised use. However, the use of current PIN-based authentication is problematic, with a third of the respondents indicating that they do not use it at all, and other problems being reported amongst those that do. In view of this, the respondents' opinions in relation to future security options are interesting, with 83% being willing to accept some form of biometric authentication on their device. The discussion considers these findings, and the potential applicability of the preferred techniques to mobile devices. 
24|7||Events calendar, please update, see marked copy|
24|7||PAID: A Probabilistic Agent-Based Intrusion Detection system|In this paper we describe architecture and implementation of a Probabilistic Agent-Based Intrusion Detection (PAID) system. The PAID system has a cooperative agent architecture. Autonomous agents can perform specific intrusion detection tasks (e.g., identify IP-spoofing attacks) and also collaborate with other agents. The main contributions of our work are the following: our model allows agents to share their beliefs, i.e., the probability distribution of an event occurrence. Agents are capable to perform soft-evidential update, thus providing a continuous scale for intrusion detection. We propose methods for modelling errors and resolving conflicts among beliefs. Finally, we have implemented a proof-of-concept prototype of PAID. 
24|7||Query-directed passwords|A classical tradeoff in the field of user authentication is between user convenience and system security. Should users authenticate themselves with their mother's maiden name, which is easily recalled but not very secure; or should they memorize a long, random password that is secure but unmemorable? In recent years, tokens and biometrics have been offered as the answer to this convenience-versus-security conflict; however, these require infrastructure modifications.We introduce query-directed passwords (QDP), an authentication procedure based on questions and answers – where the answers are known, not memorized. QDP is particularly convenient for infrequent use, such as monthly or yearly authentication to seldom-accessed accounts. Applications are described that capitalize on advantages of QDP. One of these is an automated password recovery system where testing showed a reduced use of Help Desk personnel for repeated, forgotten passwords from 20% to 2.7%. We discuss other applications, experimental results, and future research directions. 
24|7||Sinkhole intrusion in mobile ad hoc networks: The problem and some detection indicators|We analyze the “sinkhole” problem in the context of the Dynamic Source Routing (DSR) protocol for wireless mobile ad hoc networks (MANETs). The sinkhole effect is caused by attempts to draw all network traffic to malicious nodes that broadcast fake shortest path routing information. Two reliable indicators of sinkhole intrusion are proposed and analyzed. We will study how these sinkholes may be detected and term such as sinkhole intrusion detection. One indicator is based on the sequence number in the routing message and the other one is related to the proportion of the routes that travel to a suspected node. Threshold values that imply possible sinkhole intrusion are derived for these two indicators. The simulation results show that the indicators are consistent and reliable for detecting sinkhole intrusion. 
24|7||Defending against spoofed DDoS attacks with path fingerprint|In this paper, we propose a new scheme, called ANTID, for detecting and filtering DDoS attacks which use spoofed packets to circumvent the conventional intrusion detection schemes. The proposed anti-DDoS scheme intends to complement, rather than replace conventional schemes. By embedding in each IP packet a unique path fingerprint that represents the route an IP packet has traversed, ANTID is able to distinguish IP packets that traverse different Internet paths. In ANTID, a server maintains for each of its communicating clients the mapping from the client's IP address to the corresponding path fingerprint. The construction and renewal of these mappings is performed in an on-demand fashion that helps to reduce the cost of maintenance. With presence of the mapping table, the onset of a spoofed DDoS attack can be detected by observing a surge of spoofed packets. Consequently, spoofed attack packets are filtered so as to sustain the quality of protected Internet services. ANTID is lightweight, robust, and incrementally deployable. Our experiment results showed that the proposed scheme can detect 99.95% spoofed IP packets and can discard them with little collateral damage to legitimate clients. It also showed that the higher the aggregated attack rate is, the sooner the attack can be detected. 
24|8|http://www.sciencedirect.com/science/journal/01674048/24/8|Infosec certification: Which way do we turn from here?|
24|8||Security views|
24|8||Timing is everything|Social engineering attacks are well-known to prey on human weaknesses. Besides these weaknesses, humans insist on eating, sleeping, and partaking in non-work activities. On a global scale, work schedules combined with IT policies leave large windows of vulnerability – but how large? We examine calendar data through the year 2010 and locate the longest vulnerability windows which could be exploited by well-timed attacks by malicious software. The same data can be analyzed to solve a related problem: determining the best times to release software patches. 
24|8||Real-time information integrity = system integrity + data integrity + continuous assurances|A majority of companies today are totally dependent on their information assets, in most cases stored, processed and communicated within information systems in digital format. These information systems are enabled by modern information and communication technologies. These technologies are exposed to a continuously increasing set of risks. Yet, management and stakeholders continuously make important business decisions on information produced in real-time from these information systems. This information is unaccompanied by objective assurances as the current auditing procedures provide assurances months later. Therefore, risk management, including a system of internal controls, has become paramount to ensure the information's integrity. A system of internal controls, including IT controls at its core, help limit uncertainty and mitigate the risks to an acceptable level. Auditors play an increasingly important role in providing independent assurances that the information system's infrastructure and data maintain their integrities. These assurances include proposed new methods such as continuous auditing for assurance on demand. 
24|8||Safety and Security in Multiagent Systems: Report on the 2nd SASEMAS workshop (SASEMAS'05)|As intelligent autonomous agents and multiagent systems' applications become more pervasive, it becomes increasingly more important to understand the risks associated with using these systems. Incorrect or inappropriate agent behaviour can have harmful effects including financial cost, loss of data, and injury to humans or systems. Thus, security and safety are two central issues when developing and deploying such systems.However, the process of developing safe and secure multiagent systems, and verifying and validating them, is much more difficult than for conventional software systems. This is due to many agent-related aspects, such as the complex and rich multiagent environments, the risks involved in such environments, and the characteristics that can be found in agent systems such as learning, dynamic reacting and adapting. Hence, new and different techniques and perspectives are required to assist with the development and deployment of such systems.The Safety and Security in Multiagent Systems (SASEMAS) workshop presents new developments, and lessons learned from real world cases, and it provides a forum for the exchange of ideas and discussion on areas related to security and safety in multiagent systems. 
24|8||Calendar of Events|
24|8||Robust remote authentication scheme with smart cards|Due to low-computation cost and convenient portability, smart cards are usually adopted to store the personal secret information of users for remote authentication. Although many remote authentication schemes using smart cards have been introduced in the literatures, they still suffer from some possible attacks or cannot guarantee the quality of performance for smart cards. In this paper, we classify the security criteria of remote authentication and propose a new remote login scheme using smart cards to satisfy all of these criteria. Not only does the proposed scheme achieve the low-computation requirement for smart cards, but also it can withstand the replay and the offline dictionary attacks as well. Moreover, our scheme requires neither any password table for verification nor clock synchronization between each user and the server while providing both mutual authentication and the uniqueness of valid cards. 
24|8||A novel mix-based location privacy mechanism in Mobile IPv6|Mobile IP (MIP), a link-layer-independent protocol, is suitable for Internet Protocol (IP) based mobility across homogeneous media as well as heterogeneous networks. Mobile IPv6 (MIPv6) not only possesses the major characteristics of Mobile IPv4 (MIPv4), but also has more advantages such as the expansion of address space and elimination of the “triangle routing”, which make MIPv6 the most suitable candidate for future heterogeneous environment. Location privacy is very important for mobile node (MN) in mobile communications because exposure of the relationship between MN's real physical location and its identity will lead to serious violation of the MN's privacy. And the attackers can easily launch the traffic analysis attack according to such revealed relationship. However, the location privacy of MN to avoid attackers tracing in MIP is not paid more attention up to the present. As the most widely used anonymous communication technology, mix-network can be used to provide the location privacy in MIP. In this paper, we employ the practical mix-network to provide location privacy on signaling control information in MIPv6. By utilizing the practical mix-network, a novel MIPv6 network model is proposed. Based on the network model, a new location privacy extension to MN's home binding and correspondent registration in MIPv6 is proposed and it can be integrated into MIPv6 easily. As a result, our location privacy proposal possesses the benefits succeeded from the adopted practical mix-network, e.g. reducing the trust requirements among the mix servers and increasing the robustness compared with other mix-based MIP location privacy schemes. In addition, the computation load in MN does not increase significantly during the binding procedures according to the analysis, thus it is more suitable for the asymmetric wireless environment. 
24|8||SEAS, a secure e-voting protocol: Design and implementation|This paper presents SEAS, the Secure E-voting Applet System, a protocol for implementing a secure system for polling over computer networks, usable in distributed organizations whose members may range up to dozens of thousands. We consider an architecture requiring the minimum number of servers involved in the validation and voting phases. Sensus, [Cranor L, Cytron RK. Sensor: a security-conscious electronic polling system for the internet. In: Proceedings of HICSS'97. IEEE; 1997. p. 561–70], a well-known e-voting protocol, requires only two servers, namely a validator and a tallier. Even if satisfying most of the security requirements of an e-voting system, Sensus suffers from a vulnerability that allows one of the entities involved in the election process to cast its own votes in place of those that abstain from the vote. SEAS is a portable and flexible system that preserves the limited number of servers of Sensus, but it avoids the mentioned vulnerability. We propose a prototype implementation of SEAS based on Java applet and XML technology. 
24|8||Hash channels|It is believed that in authentication protocols where no random numbers are involved, it is hard to introduce subliminal channels. In this paper we show that subliminal channels exist in digital signature schemes where no random numbers are used. The source where the subliminal channels exist is widely acceptable hash functions with their outputs being of some randomness. Because the subliminal channels are established by using hash functions, the subliminal channels are called hash channels in this paper. This indicates that there exist subliminal channels in any authentication protocols that use hash functions. 
24|8||A multinomial logistic regression modeling approach for anomaly intrusion detection|Although researchers have long studied using statistical modeling techniques to detect anomaly intrusion and profile user behavior, the feasibility of applying multinomial logistic regression modeling to predict multi-attack types has not been addressed, and the risk factors associated with individual major attacks remain unclear. To address the gaps, this study used the KDD-cup 1999 data and bootstrap simulation method to fit 3000 multinomial logistic regression models with the most frequent attack types (probe, DoS, U2R, and R2L) as an unordered independent variable, and identified 13 risk factors that are statistically significantly associated with these attacks. These risk factors were then used to construct a final multinomial model that had an ROC area of 0.99 for detecting abnormal events. Compared with the top KDD-cup 1999 winning results that were based on a rule-based decision tree algorithm, the multinomial logistic model-based classification results had similar sensitivity values in detecting normal (98.3% vs. 99.5%), probe (85.6% vs. 83.3%), and DoS (97.2% vs. 97.1%); remarkably high sensitivity in U2R (25.9% vs. 13.2%) and R2L (11.2% vs. 8.4%); and a significantly lower overall misclassification rate (18.9% vs. 35.7%). The study emphasizes that the multinomial logistic regression modeling technique with the 13 risk factors provides a robust approach to detect anomaly intrusion. 
25|1|http://www.sciencedirect.com/science/journal/01674048/25/1|Dilemmas and boundaries of digital rights management|
25|1||Security views - Malware update|
25|1||Re-engineering enterprise security|Security concerns plague businesses of all sizes, but for large, international organisations, security management can be very complex. Many companies are therefore taking a fresh look at security to see how it can be re-engineered on an enterprise level to deliver seamless 24×7 management support globally. This article looks at the steps they must take and the issues they may face, as well as showing how the success of a programme can be measured using KPIs. 
25|1||Radio frequency identification (RFID)|First conceived in 1948, Radio Frequency Identification (RFID) has taken many years for the technology to mature to the point where it is sufficiently affordable and reliable for widespread use. From Electronic Article Surveillance (EAS) for article (mainly clothing) security to more sophisticated uses, RFID is seen by some as the inevitable replacement for bar codes. With increasing use comes increasing concern on privacy and security. Clearly there is considerable work to be undertaken before RFID becomes as pervasive as bar codes although the tempo of change is increasing rapidly. 
25|1||The challenges of understanding and using security: A survey of end-users|Many applications contain security features that are available for end-users to select and configure, as well as the potential to place users in situations where they must take security-related decisions. However, the manner in which these aspects are implemented and presented can often serve to complicate the process, such that users cannot actually use the security that they desire, or which may be expected of them. This paper presents the results of a survey of over 340 end-users in order to determine their understanding of the security features within Windows XP and three popular applications (Internet Explorer, Outlook Express, and Word). The study reveals some significant areas of difficulty, with many standard security features presenting apparent usability challenges for large proportions of the respondents. The results highlight the need for a more considered approach towards the presentation of security functionality if users are to have a realistic chance of protecting themselves. 
25|1||Events|
25|1||Towards a location-based mandatory access control model|With the growing use of wireless networks and mobile devices, we are moving towards an era where location information will be necessary for access control. The use of location information can be used for enhancing the security of an application, and it can also be exploited to launch attacks. For critical applications, such as the military, a formal model for location-based access control is needed that increases the security of the application and ensures that the location information cannot be exploited to cause harm. In this paper, we show how the mandatory access control (MAC) model can be extended to incorporate the notion of location. We also show how the different components in the MAC model are related with location and how this location information can be used to determine whether a subject has access to a given object. This model is suitable for military applications consisting of static and dynamic objects, where location of a subject and object must be considered before granting access. 
25|1||An identity management protocol for Internet applications over 3G mobile networks|This paper, proposes a protocol (IDM3G) for implementing identity management for Internet applications over 3G mobile networks. IDM3G combines the identity management principles of the Liberty Alliance specifications, elements of the OASIS's SAML and the 3GPP UMTS security specifications, targeting to a more effective and lightweight identity management solution than the existing ones. IDM3G instead of establishing new authentication and authorization mechanisms, utilizes the latest security features of 3G mobile networks in order to implement trust relationships, focusing on mutual authentication and authorization, avoiding at the same time the submission of the user identity itself. 
25|1||Cryptanalysis of two password-based authentication schemes using smart cards|Recently, Juang [2004. Comput Secur (23)] and Yoon et al. [2005. Comput Secur (24)] proposed password-based authentication schemes using smart cards. Juang's scheme further allows for key agreement. In this paper, we present attacks on both schemes. 
25|1||Aligning the information security policy with the strategic information systems plan|Two of the most important documents for ensuring the effective deployment of information systems and technologies within the modern business enterprise are the strategic information systems plan (SISP) and the information security policy. The strategic information systems plan ensures that new systems and technologies are deployed in a way that will support an organisation's strategic goals whilst the information security policy provides a framework to ensure that systems are developed and operated in a secure manner. To date, the literature with regard to the formulation of the information security policy has tended to ignore its important relationship with the strategic information systems plan, and vice versa. In this paper we argue that these two important policy documents should be explicitly and carefully aligned to ensure that the outcomes of strategically important information system initiatives are not compromised by problems with their security. 
25|1||Steganography in games: A general methodology and its application to the game of Go|Techniques to hide valuable information within seemingly harmless messages have been widely used for centuries. Typically, their use is appropriate when encryption is not available or not adequate (e.g. when available cryptography is too weak), or simply when it is convenient that no external observer can infer that some information is being exchanged. In the digital era, new cover mediums for hiding data in communication are constantly being proposed, from the classical image files (such as bmp, gif, and jpg formats) to audio files (i.e. wav and mp3), text and html documents, emails disguised as spam, TCP/IP packets, executables programs, DNA strands, etc. In this work, we present and analyze a novel methodology that illustrates how games (such as Chess, Backgammon, Go, etc.) can be used to hide digital contents. We also look at some of its possible advantages and limitations when compared with other techniques, discussing some improvements and extensions. Finally, we present the results of a first implementation of an open-source prototype, called StegoGo, for hiding digital contents in Go games. 
25|1||Efficient remote mutual authentication and key agreement|A smart card based scheme is very practical to authenticate remote users. In 2004, Juang [Juang WS. Efficient password authenticated key agreement using smart cards. Computers and Security 2004;23:167–73] proposed a mutual authentication scheme using smart cards. The advantages in the scheme include freely chosen passwords, no verification tables, low communication and computation cost, and session key agreement. In addition, synchronized clocks are not required in the scheme due to its nonce based approach. In this paper, however, we shall discuss the weakness of Juang's [Juang WS. Efficient password authenticated key agreement using smart cards. Computers and Security 2004;23:167–73] scheme and propose another similar scheme to improve the weakness. Our scheme not only preserves all the advantages of Juang's scheme but also improves its efficiency. 
25|2|http://www.sciencedirect.com/science/journal/01674048/25/2|About âUnofficial patchesâ|
25|2||Security Views - Malware Update|
25|2||The metamorphosis of malware writers|The reasons for writing malware are changing – and so is the malware itself. Danny Bradbury reports on the development of a seedy commercial market. 
25|2||Computer forensics and electronic discovery: The new management challenge|Recent American court decisions and legislation have shown that the failure of an organization to retain electronic documents and to be able to locate the information when needed can cost the organization millions of dollars as well as its reputation. In spite of understanding the need for compliance, very few organizations actually have a good understanding of how to implement a system that will satisfy the requirements for electronic document retention and retrieval for litigation purposes.This paper suggests some positive steps that an organization can take to minimize the likelihood of court-imposed sanctions for noncompliance with discovery requests for electronic documents. The first step is the creation of an Information Management Team that includes experts in computer forensics, law, information management, information technology, and auditing. The next step is to develop and implement an electronic document retention and deletion policy. Any such policy must retain the flexibility to implement litigation holds by suspending routine document deletion when litigation is imminent. 
25|2||Calendar of Events|
25|2||Uncovering identities: A study into VPN tunnel fingerprinting|Operating System fingerprinting is a reconnaissance method which can be used by attackers or forensic investigators. It identifies a system's identity by observing its responses to targeted probes, or by listening on a network and passively observing its network ‘etiquette’. The increased deployment of encrypted tunnels and Virtual Private Networks (VPNs) calls for the formulation of new fingerprinting techniques, and poses the question: “How much information can be gleaned from encrypted tunnels?” This paper investigates IPSec VPN tunnel-establishment and tear-down on three IPSec implementations: Microsoft Windows 2003, Sun Solaris 9 x86, and racoon on Linux 2.6 kernel. By analysing each platform's Internet Key Exchange (IKE) messages, which negotiate the IPSec tunnel, we identify a number of discriminants, and show that each of these platforms can be uniquely identified by them. We also show that the nature of some encrypted traffic can be determined, thus giving the observer an idea of the type of communication that is taking place between the IPSec endpoints. 
25|2||Provably secure authenticated key exchange protocols for low power computing clients|Low power computing devices such as cellular phones, PDAs, and smart cards are very popular and widely used by people nowadays. To secure communications between a client and a server through a low power computing device, several AKE–LPC (Authenticated Key Exchange – for Low Power Computing clients) protocols have been proposed recently. This paper proposes a new efficient AKE–LPC protocol, which requires the client to perform only one hash operation during the execution phase. An augmented protocol which can additionally provide explicit mutual authentication is also presented. Furthermore, the security of the proposed protocols is formally proven in the random oracle model. 
25|2||A new protocol to counter online dictionary attacks|The most popular method of authenticating users is through passwords. Though passwords are the most convenient means of authentication, they bring along themselves the threat of dictionary attacks. While offline dictionary attacks are possible only if the adversary is able to collect data for a successful protocol execution by eavesdropping on the communication channel and can be successfully countered by using public key cryptography, online dictionary attacks can be performed by anyone and there is no satisfactory solution to counter them. In this paper, we propose an authentication protocol which is easy to implement without any infrastructural changes and yet prevents online dictionary attacks. Our protocol uses only one way hash functions and eliminates online dictionary attacks by implementing a challenge–response system. This challenge–response system is designed in a fashion that it hardly poses any difficulty to a genuine user but is extremely burdensome, time consuming and computationally intensive for an adversary trying to launch as many as hundreds of thousands of authentication requests as in case of an online dictionary attack. The protocol is perfectly stateless and thus less vulnerable to denial of service (DoS) attacks. 
25|2||Layered security design for mobile ad hoc networks|When security of a given network architecture is not properly designed from the beginning, it is difficult to preserve confidentiality, authenticity, integrity and non-repudiation in practical networks. Unlike traditional mobile wireless networks, ad hoc networks rely on individual nodes to keep all the necessary interconnections alive. In this article we investigate the principal security issues for protecting mobile ad hoc networks at the data link and network layers. The security requirements for these two layers are identified and the design criteria for creating secure ad hoc networks using multiple lines of defence against malicious attacks are discussed. 
25|2||Security considerations for incremental hash functions based on pair block chaining|Incremental hash functions have gained much attention due to their incremental property, i.e. hashes of updated messages can be speedily computed from previous hashes without having to re-hash the message as was the case in conventional hash functions. In this paper, we first show how collisions can be obtained in such incremental hash functions that are based on pair block chaining, highlighting that more caution should be taken into its design process. We then identify some design and implementation criteria for such incremental hash functions. 
25|2||A framework and taxonomy for comparison of electronic voting schemes|Electronic voting is an emerging social application of cryptographic protocols. A vast amount of literature on electronic voting has been developed over the last two decades. In this paper, we provide a framework that classifies these approaches and defines a set of metrics under which their properties can be compared. Such a methodology reveals important differences in security properties between the classes and allows for selection and future design of voting schemes, based on application requirements. We illustrate the use of our framework by analyzing some of the existing electronic voting schemes. 
25|3|http://www.sciencedirect.com/science/journal/01674048/25/3|Special systems: Overlooked sources of security risk?|
25|3||Security Views - Malware Update|
25|3||Modeling network security|
25|3||Information Security â The Fourth Wave|In a previous article [von Solms, 2000], the development of Information Security up to the year 2000 was characterized as consisting of three waves:•the technical wave,•the management wave, and•the institutional wave.This paper continues this development of Information Security by characterizing the Fourth Wave – that of Information Security Governance. 
25|3||Calendar of events - update|
25|3||Real-time analysis of intrusion detection alerts via correlation|With the growing deployment of networks and the Internet, the importance of network security has increased. Recently, however, systems that detect intrusions, which are important in security countermeasures, have been unable to provide proper analysis or an effective defense mechanism. Instead, they have overwhelmed human operators with a large volume of intrusion detection alerts. This paper presents a fast and efficient system for analyzing alerts. Our system basically depends on the probabilistic correlation. However, we enhance the probabilistic correlation by applying more systematically defined similarity functions and also present a new correlation component that is absent in other correlation models. The system can produce meaningful information by aggregating and correlating the large volume of alerts and can detect large-scale attacks such as distributed denial of service (DDoS) in early stage. We measured the processing rate of each elementary component and carried out a scenario-based test in order to analyze the efficiency of our system. Although the system is still imperfect, we were able to reduce the numerous redundant alerts 5.5% of the original volume without distorting the meaning through two-phase reduction. This ability reduces the management overhead drastically and makes the analysis and correlation easy. Moreover, we were able to construct attack scenarios for multistep attacks and detect large-scale attacks in real time. 
25|3||A novel remote user authentication scheme using bilinear pairings|The paper presents a remote user authentication scheme using the properties of bilinear pairings. In the scheme, the remote system receives user login request and allows login to the remote system if the login request is valid. The scheme prohibits the scenario of many logged in users with the same login-ID, and provides a flexible password change option to the registered users without any assistance from the remote system. 
25|3||A novel approach for computer security education using Minix instructional operating system|To address national needs for computer security education, many universities have incorporated computer and security courses into their undergraduate and graduate curricula. In these courses, students learn how to design, implement, analyze, test, and operate a system or a network to achieve security. Pedagogical research has shown that effective laboratory exercises are critically important to the success of these types of courses. However, such effective laboratories do not exist in computer security education.Intrigued by the successful practice in operating system and network courses education, we adopted a similar practice, i.e., building our laboratories based on an instructional operating system. We use Minix operating system as the lab basis, and in each lab we require students to add a different security mechanism to the system. Benefited from the instructional operating system, we design our lab exercises in a way such that students can focus on one or a few specific security concepts while doing each exercise. The similar approach has proved to be effective in teaching operating system and network courses, but it has not yet been used in teaching computer security courses. 
25|3||A traceable threshold signature scheme with multiple signing policies|In recent years, a great deal of work has been done on threshold signature schemes and many excellent schemes have been proposed. In Eurocrypt'94, Li et al. [Threshold-multisignature schemes where suspected forgery implies traceability of adversarial shareholders. In: Advances in Cryptology—Proceedings of EUROCRYPT 94; 1994. p. 413–9] proposed a threshold signature scheme with traceability, which allows us to trace back to find the signer without revealing the secret keys. And in 2001, Lee [Threshold signature scheme with multiple signing policies. IEE Proc Comput Digit Tech 2001;148(2):95–9] proposed a threshold signature scheme with multiple signing policies, which allows multiple secret keys to be shared among a group of users, and each secret key has its specific threshold value. In this paper, based on these schemes, we present a traceable threshold signature scheme with multiple signing policies, which not only inherits their properties, but also fixes their weaknesses. 
25|3||Security implications in RFID and authentication processing framework|The objective of this paper is to propose an idea called APF (Authentication Processing Framework) as one of the ways to deter the growing concerns of unauthorized readers from accessing the tag (transponder) which could result into the violations of information stored in the tag. On one hand, we will discuss the importance of RFID systems and on the other hand, we will discuss about the security implications that the RFID systems have over consumers' privacy and security. In this paper, we are trying to weigh the two issues, importance of RFID system and the RFID security implications. Having done that, we are recommending our idea called APF (Authentication Processing Framework) as a good method to overcome the above mentioned problem. 
25|3||Change trend of averaged Hurst parameter of traffic under DDOS flood attacks|Distributed denial-of-service (DDOS) flood attacks remain great threats to the Internet though various approaches and systems have been proposed. Because arrival traffic pattern under DDOS flood attacks varies significantly away from the pattern of normal traffic (i.e., attack free traffic) at the protected site, anomaly detection plays a role in the detection of DDOS flood attacks. Hence, quantitatively studying statistics of traffic under DDOS flood attacks (abnormal traffic for short) are essential to anomaly detections of DDOS flood attacks.References regarding qualitative descriptions of abnormal traffic are quite rich, but quantitative descriptions of its statistics are seldom seen. Though statistics of normal traffic are affluent, where the Hurst parameter HH of traffic plays a key role, how HH of traffic varies under DDOS flood attacks is rarely reported. As a supplementary to our early work, this paper shows that averaged HH of abnormal traffic usually tends to be significantly smaller than that of normal one at the protected site. This abnormality of abnormal traffic is demonstrated with test data provided by MIT Lincoln Laboratory and explained from a view of Fourier analysis. 
25|3||An empirical examination of the reverse engineering process for binary files|Reverse engineering of binary code file has become increasingly easier to perform. The binary reverse engineering and subsequent software exploitation activities represent a significant threat to the intellectual property content of commercially supplied software products. Protection technologies integrated within the software products offer a viable solution towards deterring the software exploitation threat. However, the absence of metrics, measures, and models to characterize the software exploitation process prevents execution of quantitative assessments to define the extent of protection technology suitable for application to a particular software product. This paper examines a framework for collecting reverse engineering measurements, the execution of a reverse engineering experiment, and the analysis of the findings to determine the primary factors that affect the software exploitation process. The results of this research form a foundation for the specification of metrics, gathering of additional measurements, and development of predictive models to characterize the software exploitation process. 
25|3||A simple, configurable SMTP anti-spam filter: Greylists|This paper addresses methods for combating spam, focusing especially on those based on the economic motivations of unsolicited commercial e-mail. Considering the fact that to date no machine has passed the Turing test, well-known blacklist and whitelist solutions can be generalized by greylists. An outline of a simple SMTP anti-spam application following these ideas and running on a UNIX machine is offered. Some problems regarding the application are discussed, together with some of the results obtained after a two-month test period. 
25|4|http://www.sciencedirect.com/science/journal/01674048/25/4|Representing information security fairly and accurately|
25|4||Malware Update|
25|4||Reflecting on 20 SEC conferences|The ever-increasing use of information technology in business and everyday life led to a raised awareness of security issues. The last three decades spawned a large amount of research literature on security.The belief that the future can only be realized if the past is well understood has motivated an investigation into the history of information security. This report therefore focuses on analyzing the work reported in the past 20 SEC conferences, the flagship conference series of the IFIP Technical Committee 11 (TC-11).The study indicates that the focus of papers increased over time and that the output of papers became more technical. Certain topics such as auditing and business continuity have largely disappeared as a topic. The study confirmed an expected increase in the prominence of papers dealing with network related research. Surprisingly, information security management showed no upward trend; instead a slight decrease could be seen. In contrast, crypto-like topics showed a strong growth.Finally the paper reflects on the significance of the observations made, specifically with respect to future research considerations by the TC-11 community. 
25|4||Calendar of events|
25|4||Predation and the cost of replication: New approaches to malware prevention?|Computer viruses and worms are often compared to their biological counterparts. Researchers in the field speak of “infection”, “innate immunity” and “epidemics” – all expressions with distinctly biological connotations. However, despite the similarity of language, there are marked and important differences between computer viruses and their biological namesake. In this paper, some of the most critical differences are examined, and an illustration of how they may limit the application of biologically inspired defenses to computer virus spread is given. Furthermore, due to our historical lack of success in containing computer virus outbreaks, we apply a different biological metaphor to the problem: that of predator and prey. In particular, we focus on the issue of the cost of predation, and note that the essentially “free” predation computer viruses enjoy limits the applicability of biological analogies of protection in the global computing infrastructure. 
25|4||Scalable balanced batch rekeying for secure group communication|Secure group communication is important for applications such as pay-per-view. Other authors have proposed the key tree approach to distribute a shared group key in a way such that the rekeying cost scales linearly with the logarithm of the group size for a join or depart request. The efficiency of the key tree approach depends critically on whether the key tree remains balanced. Periodic rebalancing can be used to balance the key tree whenever it becomes unbalanced but this adds extra costs to the network. In this paper, we present two Merging Algorithms suitable for batch join events. As the multicast session consists of other events as well, we then show how we can extend our algorithms into existing work to minimise the maximum difference in height without adding extra network costs. Simulation results show our Merging Algorithms not only balance the key tree but their rekeying costs are lower compared to existing algorithms. 
25|4||A hybrid honeypot framework for improving intrusion detection systems in protecting organizational networks|This paper proposes a hybrid and adaptable honeypot-based approach that improves the currently deployed IDSs for protecting networks from intruders. The main idea is to deploy low-interaction honeypots that act as emulators of services and operating systems and have them direct malicious traffic to high-interaction honeypots, where hackers engage with real services. The setup permits for recording and analyzing the intruder's activities and using the results to take administrative actions toward protecting the network. The paper describes the basic components, design, operation, implementation and deployment of the proposed approach, and presents several performance and load testing scenarios. Implementation and performance plus load testing show the adaptability of the proposed approach and its effectiveness in reducing the probability of attacks on production computers. 
25|4||A prototype for assessing information security awareness|Due to the intensified need for improved information security, many organisations have established information security awareness programs to ensure that their employees are informed and aware of security risks, thereby protecting themselves and their profitability. In order for a security awareness program to add value to an organisation and at the same time make a contribution to the field of information security, it is necessary to have a set of methods to study and measure its effect. The objective of this paper is to report on the development of a prototype model for measuring information security awareness in an international mining company. Following a description of the model, a brief discussion of the application results is presented. 
25|4||Design of an enhancement for SSL/TLS protocols|When studying the Transport Layer Security (TLS) Protocol, it is noticed that the most time-consuming phase is the handshaking process between the client and the server, since many messages should be sent until successful negotiation is done and a secure session is created. The goal of this work is to design a security management system (SMS) to improve the handshaking process by making use of TLS client-side session caching, and allowing trusted users to share sessions with others, as well as giving the client an option to create his own private session with the server even when there is no trusted digital certificate from a certificate authority (CA) to link them. According to our experimental setup, the use of the proposed design has improved the performance by 3.5 times relative to the handshaking of traditional TLS. 
25|4||An anonymous voting mechanism based on the key exchange protocol|In democratic society, elections and voting are always the most important hallmarks. However, there are plenty of problems in the traditional election such as inconvenience, unfairness, non-mobility, non-anonymity, and so forth. What is more, the cost of the traditional election often places a heavy burden on the nation. To solve the problems of the traditional election, the concept of “electronic voting” is introduced, where people are allowed to vote over the Internet and the government can save lots of money. The properties of mobility and convenience are the most significant reasons why people may adopt the electronic voting mechanism in the future. Although technologies for electronic voting have been developed for about 20 years, some problems, such as uncoercibility and completeness, still cannot be overcome. In this article, we are going to present an efficient and secure voting mechanism by employing Chaum's blind signature scheme and Diffie–Hellman key exchange protocol. Our proposed electronic voting mechanism not only achieves lots of essential requirements of general electronic voting schemes but also possesses better efficiency such that it can be the practical one. 
25|5|http://www.sciencedirect.com/science/journal/01674048/25/5|The changing winds of information security|
25|5||Security Views - Malware Update|
25|5||Continuous auditing technologies and models: A discussion|In the age of real-time accounting and real-time communication current audit practices, while effective, often provide audit results long after fraud and/or errors have occurred. Real-time assurances can assist in preventing intentional or unintentional errors. This can best be achieved through continuous auditing which relies heavily on technology. These technologies are embedded within and are crucial to continuous auditing models. 
25|5||Calender of Events|
25|5||PING attack â How bad is it?|PING-based Distributed Denial of Service (DDoS) attacks are infamous as they are known to have brought down high profile web sites such as Ebay, ETrade and Yahoo. They have also been used in an attempt to bring down the entire Internet by attacking its DNS root servers. In this paper, we investigate the impact of PING-flooding on computer systems. We create real PING-attack traffic in a controlled lab environment at UTPA to understand the intensity of the attack and its impact on processing power of a Windows-XP computer deploying Pentium-4, 2.66 GHz processor. In this experiment, we set out to measure the rate of resource exhaustion of the computer as its bandwidth is increasingly consumed by the PING-attack traffic. It is observed that PING attack causes resource starvation for the computer when the PING-attack traffic increasingly consumes the bandwidth of a Fast Ethernet Link. 
25|5||Comparing Java and .NET security: Lessons learned and missed|Many systems execute untrusted programs in virtual machines (VMs) to mediate their access to system resources. Sun introduced the Java VM in 1995, primarily intended as a lightweight platform for executing untrusted code inside web pages. More recently, Microsoft developed the .NET platform with similar goals. Both platforms share many design and implementation properties, but there are key differences between Java and .NET that have an impact on their security. This paper examines how .NET's design avoids vulnerabilities and limitations discovered in Java and discusses lessons learned (and missed) from experience with Java security. 
25|5||On Incident Handling and Response: A state-of-the-art approach|Incident Response has always been an important aspect of Information Security but it is often overlooked by security administrators. Responding to an incident is not solely a technical issue but has many management, legal, technical and social aspects that are presented in this paper. We propose a detailed management framework along with a complete structured methodology that contains best practices and recommendations for appropriately handling a security incident. We also present the state-of-the art technology in computer, network and software forensics as well as automated trace-back artifacts, schemas and protocols. Finally, we propose a generic Incident Response process within a corporate environment. 
25|5||Authentication delegation for subscription-based remote network services|There is growing interest in collaboration and resource sharing among institutions and organizations. In this paper, we investigate the problems of identity management inherent in distributed subscription-based resource sharing. The paper describes the design, implementation and performance of a system that provides controlled access to subscription-based remote network services through a browser. A third-party authentication protocol is designed and employed to exchange security assertions among involved parties. The web servers use plug-ins to provide an authentication-delegation service and a policy-based authorization service. Users can use a single userID and password to access multiple subscribed resource sites. 
25|5||A qualitative analysis of software security patterns|Software security, which has attracted the interest of the industrial and research community during the last years, aims at preventing security problems by building software without the so-called security holes. One way to achieve this goal is to apply specific patterns in software architecture. In the same way that the well-known design patterns for building well-structured software have been defined, a new kind of patterns called security patterns have emerged. These patterns enable us to incorporate a level of security already at the design phase of a software system. There exists no strict set of rules that can be followed in order to develop secure software. However, a number of guidelines have already appeared in the literature. Furthermore, the key problems in building secure software and major threat categories for a software system have been identified. An attempt to evaluate known security patterns based on how well they follow each principle, how well they encounter with possible problems in building secure software and for which of the threat categories they do take care of, is performed in this paper. Thirteen security patterns were evaluated based on these three sets of criteria. The ability of some of these patterns to enhance the security of the design of a software system is also examined by an illustrative example of fortifying a published design. 
25|6|http://www.sciencedirect.com/science/journal/01674048/25/6|(iii) Contents|
25|6||Microsoft is back in the hot seat|
25|6||Security Views - Malware Update|
25|6||Microsoft's new window on security|Microsoft may be trying to escape from its past by building new security features into its next operating system, but it has been forced into some compromises. 
25|6||Information Security Governance: A model based on the DirectâControl Cycle|It is generally accepted that Information Security Governance is an integral part of Corporate Governance. It is therefore essential for any company to have a proper Information Security Governance program which reflects this integration with Corporate Governance. One of the core principles of Governance, and specifically Corporate Governance, is the Direct–Control Cycle which, in its simplest form, ‘prescribes’ and ‘checks’. This paper presents an Information Security Governance model based on this cycle. 
25|6||A quantitative method for ISO 17799 gap analysis|ISO/IEC 17799:2005 is one of the leading standards of information security. It is the code of practice including 133 controls in 11 different domains. There are a number of tools and software that are used by organizations to check whether they comply with this standard. The task of checking compliance helps organizations to determine their conformity to the controls listed in the standard and deliver useful outputs to the certification process. In this paper, a quantitative survey method is proposed for evaluating ISO 17799 compliance. Our case study has shown that the survey method gives accurate compliance results in a short time with minimized cost. 
25|6||A Secure Identification and Key agreement protocol with user Anonymity (SIKA)|Anonymity is a desirable security feature in addition to providing user identification and key agreement during a user's login process. Recently, Yang et al., proposed an efficient user identification and key distribution protocol while preserving user anonymity. Their protocol addresses a weakness in the protocol proposed by Wu and Hsu. Unfortunately, Yang's protocol poses a vulnerability that can be exploited to launch a Denial-of-Service (DoS) attack. In this paper, we cryptanalyze Yang's protocol and present the DoS attack. We further secure their protocol by proposing a Secure Identification and Key agreement protocol with user Anonymity (SIKA) that overcomes the above limitation while achieving security features like identification, authentication, key agreement and user anonymity. 
25|6||FTKM: A fault-tolerant key management protocol for multicast communications|Several protocols have been proposed to deal with the group key management problem. Unfortunately, most of these protocols lack fault-tolerance. This is a serious drawback in real network conditions, especially, when considering asynchronous wide-area networks, such as the Internet, where messages may be delayed indefinitely and nodes may fail. To cope with this limitation, failure detection/correction mechanisms are necessary. This kind of solutions introduces a new challenge, which is the requirement of periodically exchanging failure information between group members. This is a heavy communication overhead even with small groups. To avoid the periodic broadcast of failure information, we propose in this paper a solution that structures group members into a logical ring where only pair-wise communications are necessary to detect failures. Compared to other schemes, our solution is efficient and fault-tolerant. 
25|6||A secure extension of the KwakâMoon group signcryption scheme|This paper presents the secure extension of the Kwak–Moon group signcryption scheme [Kwak D, Moon S. Efficient distributed signcryption scheme as group signcryption. In: First applied cryptography and network security – ACNS'03. Lecturer notes in computer science, vol. 2846. Springer-Verlag; 2003. p. 403–17] as a countermeasure against the cryptanalysis in [Wang G, Deng RH, Kwak D, Moon S. Security analysis of two signcryption scheme. In: Information security conference – ISC 2004. Lecturer notes in computer science, vol. 3225. Springer-Verlag; 2004. p. 123–33]. The cryptanalysis revealed that the Kwak–Moon scheme cannot satisfy the properties of unforgeability, coalition-resistance, and traceability. Therefore, to avoid these weaknesses, while providing the same functions, we add confidentiality to the original group signature by distributing a shared secret among group members through an efficient group key agreement. However, in case of just combining a group signature and a group key agreement, if an attacker who does not belong to the group acquires a valid group signature, it is still possible for him to impersonate a valid group member and delegate the group. Thus, to avoid this possibility, the proposed scheme confirms whether or not the sender is equal to the signer by including a session key encryption in the signed message. In addition, we analyze the security of the proposed scheme and apply it to an anonymous statistical survey of attributes. 
25|6||NetHost-Sensor: Investigating the capture of end-to-end encrypted intrusive data|Intrusion Detection Systems (IDSs) are systems that protect against violation of data integrity, confidentiality and availability of resources. In the past 20 years, these systems have evolved with the technology and have become more sophisticated. Despite these advances, IDS is still an immature field, and the benefits obtained from detecting end-to-end encrypted attacks justify the need for more research.This paper presents possible advantages of an IDS that uses a target host's kernel as its audit source for intrusion analysis against specific attacks. In addition, we describe our research experience in determining what layer, within a protocol stack of a target host, where decrypted data can be captured for intrusion detection. Then, it examines how to capture decrypted data, while communicating via an End-to-End (ETE) encryption channel. The paper proceeds further to discuss our methodology using network communication driver interfaces, investigative experimental procedures and present our experimental results. Finally, discussions on the methodology of our future research, modelling HTTP network data via procedure analysis technique to reduce false positive rate of attacks are presented. 
25|6||Application of temporal and spatial role based access control in 802.11 wireless networks|In this study, we have investigated the security aspects of wireless local area networks and discussed the weaknesses associated with various conventional 802.11 security protocols such as WEP and 802.1X. We propose an architecture to control access to 802.11 wireless networks, based on roles, location and time information, using the tested wired network components such as VPNs and Firewalls. The presented architecture, in which temporal and spatial RBAC is implemented, reduces the security risks in enterprise level deployment of wireless LANs. 
25|6||RT-UNNID: A practical solution to real-time network-based intrusion detection using unsupervised neural networks|With the growing rate of network attacks, intelligent methods for detecting new attacks have attracted increasing interest. The RT-UNNID system, introduced in this paper, is one such system, capable of intelligent real-time intrusion detection using unsupervised neural networks. Unsupervised neural nets can improve their analysis of new data over time without retraining. In previous work, we evaluated Adaptive Resonance Theory (ART) and Self-Organizing Map (SOM) neural networks using offline data. In this paper, we present a real-time solution using unsupervised neural nets to detect known and new attacks in network traffic. We evaluated our approach using 27 types of attack, and observed 97% precision using ART nets, and 95% precision using SOM nets. 
25|6||Infection, imitation and a hierarchy of computer viruses|Infection is an essential character of computer viruses. In addition, computer viruses can also imitate the behavior of infected programs in some ways in order to hide themselves. In this paper we define infection and imitation mathematically, and classify computer viruses into 3 types according to their different imitation behaviors. Furthermore, we give some results about the degree of unsolvability of each type of computer viruses. We show that the set of type 0 and type 1 computer viruses is Î 2-complete, while the set of type 2 computer viruses is Î 3-complete. 
25|6||Guide for Authors.|
25|6||IBC - Calendar of Events|
25|7|http://www.sciencedirect.com/science/journal/01674048/25/7|(iii) Contents|
25|7||Issues concerning the distribution of vulnerability information|
25|7||Security Views - Malware Update|
25|7||Risk and restitution: Assessing how users establish online trust|The belief that users must be assured of security prior to engaging with an online service is challenged through the examination of attitudes from participants of a number of focus groups within the UK. What is apparent from our evidence is that rather than accepting simple assurances of protection, the average user is far more informed than service providers often credit, and will carry out a personal risk assessment prior to engaging with a service. Rather than guarantees of security, clearly defined indications of mitigation and restitution in the event of failure or problems are what users consider important. These findings have far reaching implications for service providers and a number of consequent recommendations are defined. 
25|7||Information security governance: Due care|Most modern corporate governance guidelines, and also some country laws, make the Board and specifically the CEO responsible for the well-being of the organization. These parties must ensure that critical company assets are identified and that these assets are protected against possible risks that may negatively influence the organization. Information can certainly be regarded as a critical business asset in most organizations today. Therefore, due care needs to be applied in the protection of information resources. Failure to do so can lead to a legal charge of negligence. As best practices can be argued as a very effective approach to apply due care, this paper proposes a self-evaluation exercise (based on best practices) for boards of companies to be used to determine whether due care has indeed been applied. 
25|7||Security issues in SCADA networks|The increasing interconnectivity of SCADA (Supervisory Control and Data Acquisition) networks has exposed them to a wide range of network security problems. This paper provides an overview of all the crucial research issues that are involved in strengthening the cyber security of SCADA networks. The paper describes the general architecture of SCADA networks and the properties of some of the commonly used SCADA communication protocols. The general security threats and vulnerabilities in these networks are discussed followed by a survey of the research challenges facing SCADA networks. The paper discusses the ongoing work in several SCADA security areas such as improving access control, firewalls and intrusion detection systems, SCADA protocol analyses, cryptography and key management, device and operating system security. Many trade and research organizations are involved in trying to standardize SCADA security technologies. The paper concludes with an overview of these standardization efforts. 
25|7||A dynamic context-aware access control architecture for e-services|The universal adoption of the Internet and the emerging web services technologies constitutes the infrastructure that enables the provision of a new generation of e-services and applications. However, the provision of e-services through the Internet imposes increased risks, since it exposes data and sensitive information outside the client premises. Thus, an advanced security mechanism has to be incorporated, in order to protect this information against unauthorized access. In this paper, we present a context-aware access control architecture, in order to support fine-grained authorizations for the provision of e-services, based on an end-to-end web services infrastructure. Access permissions to distributed web services are controlled through an intermediary server, in a completely transparent way to both clients and protected resources. The access control mechanism is based on a Role-Based Access Control (RBAC) model, which incorporates dynamic context information, in the form of context constraints. Context is dynamically updated and provides a high level of abstraction of the physical environment by using the concepts of simple and composite context conditions. Also, the paper deals with implementation issues and presents a system that incorporates the proposed access control mechanism in a web services infrastructure that conform to the OPC XML-DA specification. 
25|7||A taxonomy and comparison of computer security incidents from the commercial and government sectors|Cyber incidents are growing in intensity and severity. Several industry groups are therefore taking steps to better coordinate and improve information security across sectors. Also, various different types of public–private partnerships are developing, where cyber incident information is shared across institutions. This cooperation may improve the understanding of various types of cyber incidents, their severity, and impact on various types of targets. Research has shown that different types of attackers may be distinguished in terms of sophistication, skill level, attacking style, and objective of attack. It may further be proposed that different sectors experience different types of attacks. Attack characteristics and information about the modus operandi of criminal offenders have been used to learn more about the attacker and the motive of an attack. This information may also be used to distinguish between cyber attacks towards different types of targets. The current study focuses on reported cyber intrusions by the commercial and government sectors. The reported data come from CERT®Coordination Center (CERT/CC), which has categorized the aspects of cyber intrusions in the current study. The aspects analyzed are: ‘Method of Operation (MO)’ which refers to the methods used by perpetrator to carry out an attack; ‘Impact’ which refers to the effect of the attack; ‘Source’ which refers to the source of the attack, and ‘Target’ which refers to the victim of the attack. The current study uses 839 cases of cyber attacks towards the commercial sector and 558 cases towards the government sector. The 23 variables from the four different cyber intrusion aspects; MO, impact, source sector and target sector, were analyzed using multidimensional scaling (MDS), which is a technique that has often been used when profiling traditional types of crimes. The analysis gave a Guttman–Lingoes' coefficient of alienation of 0.19 with 42 iterations in a 3-dimensional solution. It was shown that the commercial and government sectors experience different types of attacks, with different types of impact, stemming from different sources. The findings and implications are discussed in relation to the benefits of standardization, reporting, and sharing of cyber incident information. 
25|7||Profiling program behavior for anomaly intrusion detection based on the transition and frequency property of computer audit data|Intrusion detection is an important technique in the defense-in-depth network security framework. In recent years, it has been a widely studied topic in computer network security. In this paper, we present two methods, namely, the Hidden Markov Models (HMM) method and the Self Organizing Maps (SOM) method, to profile normal program behavior for anomaly intrusion detection based on computer audit data. The HMM method utilizes the transition property of events while SOM method relies on the frequency property of events. Two data sets, CERT synthetic Sendmail system call data collected in the University of New Mexico (UNM) and Live FTP system call data collected in the CNSIS lab of Xi'an Jiaotong University, were used to assess the two methods. Testing results show that the HMM method using the transition property of events produces good detection performance while high computational expense is required both for training and detection. The HMM method is better than other two methods reported previously in terms of detection accuracy for the same data set. The SOM method considering the frequency property of events, on the other hand, is suitable for real-time intrusion detection because of its capability of processing a large amount of data with low computational overhead. 
25|7||Call for papersâIFIPSEC 2007|
25|7||Guide for Authors.|
25|7||IBC - Calendar of Events|
25|8|http://www.sciencedirect.com/science/journal/01674048/25/8|(iii) Contents|
25|8||Predicting the future of InfoSec|
25|8||Security Views - Malware update|
25|8||Tightening the net: A review of current and next generation spam filtering tools|This paper provides an overview of current and potential future spam filtering approaches. We examine the problems spam introduces, what spam is and how we can measure it. The paper primarily focuses on automated, non-interactive filters, with a broad review ranging from commercial implementations to ideas confined to current research papers. Both machine learning- and non-machine learning-based filters are reviewed as potential solutions and a taxonomy of known approaches is presented. While a range of different techniques have and continue to be evaluated in academic research, heuristic and Bayesian filtering dominate commercial filtering systems; therefore, a case study of these techniques is presented to demonstrate and evaluate the effectiveness of these popular techniques. 
25|8||Expected benefits of information security investments|Ideally, decisions concerning investments of scarce resources in new or additional procedures and technologies that are expected to enhance information security will be informed by quantitative analyses. But security is notoriously hard to quantify, since absence of activity challenges us to establish whether lack of successful attacks is the result of good security or merely due to good luck. However, viewing security as the inverse of risk enables us to use computations of expected loss to develop a quantitative approach to measuring gains in security by measuring decreases in risk. In using such an approach, making decisions concerning investments in information security requires calculation of net benefits expected to result from the investment. Unfortunately, little data are available upon which to base an estimate of the probabilities required for developing the expected losses. This paper develops a mathematical approach to risk management based on Kaplan–Meier and Nelson–Aalen non-parametric estimators of the probability distributions needed for using the resulting quantitative risk management tools. Differences between the integrals of these estimators evaluated for enhanced and control groups of systems in an information infrastructure provide a metric for measuring increased security. When combined with an appropriate value function, the expected losses can be calculated and investments evaluated quantitatively in terms of actual enhancements to security. 
25|8||A virtual disk environment for providing file system recovery|File system recovery (FSR) is a kind of recovery facility that allows users to roll back the file system state to a previous state. In this paper, we present a virtual disk environment (VDE) which allows previous write operations to a hard disk to be undone, and previous version of files to be recovered. It can be used to recover the file system quickly even when computer system suffers the serious disaster such as system crash or boot failure. The VDE is same as virtual disk in the virtual machine (VM) environment in some way, but it can be applied to the environment without VM supports. Algorithms for implementing the VDE are presented and its implementation on Windows platform is discussed. Based on the implementation, the experimental results of the VDE performance are analyzed. Comparing with other FSRs, the main advantage of the VDE is low overhead and high recovery speed. 
25|8||Wavelet based Denial-of-Service detection|Network Denial-of-Service (DoS) attacks that disable network services by flooding them with spurious packets are on the rise. Criminals with large networks (botnets) of compromised nodes (zombies) use the threat of DoS attacks to extort legitimate companies. To fight these threats and ensure network reliability, early detection of these attacks is critical. Many methods have been developed with limited success to date. This paper presents an approach that identifies change points in the time series of network packet arrival rates. The proposed process has two stages: (i) statistical analysis that finds the rate of increase of network traffic, and (ii) wavelet analysis of the network statistics that quickly detects the sudden increases in packet arrival rates characteristic of botnet attacks.Most intrusion detections are tested using data sets from special security testing configurations, which leads to unacceptable false positive rates being found when they are used in the real world. We test our approach using data from both network simulations and a large operational network. The true and false positive detection rates are determined for both data sets, and receiver operating curves use these rates to find optimal parameters for our approach. Evaluation using operational data proves the effectiveness of our approach. 
25|8||Guide for Authors.|
25|8||IBC - Calendar of Events|
26|1|http://www.sciencedirect.com/science/journal/01674048/26/1|(iii) Contents|
26|1||What infosec changes are likely to result from the recent US election?|
26|1||Security Views - Malware Update|
26|1||Biometric attack vectors and defences|Much has been reported on attempts to fool biometric sensors with false fingerprints, facial overlays and a myriad of other spoofing approaches. Other attack vectors on biometric systems have, however, had less prominence. This paper seeks to present a broader and more practical view of biometric system attack vectors, placing them in the context of a risk-based systems approach to security and outlining defences. 
26|1||Information Lifecycle Security Risk Assessment: A tool for closing security gaps|News media continue to report stories of critical information loss through physical means. Most information security programs include physical protection for information system infrastructure, but not for the physical (non-electronic) forms of the information itself. Thus organizations have persistent critical information vulnerabilities that are not addressed by even the most extensive of information systems security programs.An Information Lifecycle Security Risk Assessment, as described in this paper, can be used to extend the reach of information security programs to encircle all forms of critical data from creation to destruction—even data in human memory form. Such an assessment can leverage existing data management and information systems security efforts. By incorporating both electronic and physical information elements, previously unaddressed information security gaps can be identified and mitigated. The end result should be a risk treatment plan which senior management can understand and approve, and which managers and security personnel can execute. 
26|1||Decoding digital rights management|Digital rights management technology is designed to prevent piracy and facilitate the creation of innovative business models around digital content. Its technological limitations may be surpassed only by its economic ones. 
26|1||IFIP workshop â Information security culture|
26|1||Value-focused assessment of ICT security awareness in an academic environment|Security awareness is important to reduce human error, theft, fraud, and misuse of computer assets. A strong ICT security culture cannot develop and grow in a company without awareness programmes. This paper focuses on ICT security awareness and how to identify key areas of concern to address in ICT security awareness programmes by making use of the value-focused approach. The result of this approach is a network of objectives where the fundamental objectives are the key areas of concern that can be used in decision making in security planning. The fundamental objectives were found to be in line with the acknowledged goals of ICT security, e.g. confidentiality, integrity and availability. Other objectives that emerged were more on the social and management side, e.g. responsibility for actions and effective use of resources. 
26|1||Bridging the gap between general management and technicians â A case study on ICT security in a developing country|The lack of planning, business re-engineering, and coordination in the whole process of computerisation is the most pronounced problem facing organisations. These problems often lead to a discontinuous link between technology and the business processes. As a result, the introduced technology poses some critical risks for the organisations due, in part, to different perceptions of the management and technical staffs in viewing the ICT security problem. This paper discusses a practical experience on bridging the gap between the general management and ICT technicians. 
26|1||Organisational security culture: Extending the end-user perspective|The concept of security culture is relatively new. It is often investigated in a simplistic manner focusing on end-users and on the technical aspects of security. Security, however, is a management problem and as a result, the investigation of security culture should also have a management focus. This paper describes a framework of eight dimensions of culture. Each dimension is discussed in terms of how they relate specifically to security culture based on a number of previously published case studies. We believe that use of this framework in security culture research will reduce the inherent biases of researchers who tend to focus on only technical aspects of culture from an end-users perspective. 
26|1||A video game for cyber security training and awareness|Although many of the concepts included in cyber security awareness training are universal, such training often must be tailored to address the policies and requirements of a particular organization. In addition, many forms of training fail because they are rote and do not require users to think about and apply security concepts. A flexible, highly interactive video game, CyberCIEGE, is described as a security awareness tool that can support organizational security training objectives while engaging typical users in an engaging security adventure. The game is now being successfully utilized for information assurance education and training by a variety of organizations. Preliminary results indicate the game can also be an effective addition to basic information awareness training programs for general computer users (e.g., annual awareness training.) 
26|1||Phishing for user security awareness|User security education and training is one of the most important aspects of an organizations security posture. Using security exercises to reinforce this aspect is frequently done by education and industry alike; however these exercises usually enlist willing participants. We have taken the concept of using an exercise and modified it in application to evaluate a users propensity to respond to email phishing attacks in an unannounced test. This paper describes the considerations in establishing and the process used to create and implement an evaluation of one aspect of our user information assurance education program. The evaluation takes the form of a exercise, where we send out a phishing styled email record the responses. 
26|1||A privacy-preserving clustering approach toward secure and effective data analysis for business collaboration|The sharing of data has been proven beneficial in data mining applications. However, privacy regulations and other privacy concerns may prevent data owners from sharing information for data analysis. To resolve this challenging problem, data owners must design a solution that meets privacy requirements and guarantees valid data clustering results. To achieve this dual goal, we introduce a new method for privacy-preserving clustering called Dimensionality Reduction-Based Transformation (DRBT). This method relies on the intuition behind random projection to protect the underlying attribute values subjected to cluster analysis. The major features of this method are: (a) it is independent of distance-based clustering algorithms; (b) it has a sound mathematical foundation; and (c) it does not require CPU-intensive operations. We show analytically and empirically that transforming a data set using DRBT, a data owner can achieve privacy preservation and get accurate clustering with a little overhead of communication cost. 
26|1||Simple three-party key exchange protocol|Three-party authenticated key exchange protocol is an important cryptographic technique in the secure communication areas, by which two clients, each shares a human-memorable password with a trusted server, can agree a secure session key. Over the past years, many three-party authenticated key exchange protocols have been proposed. However, to our best knowledge, not all of them can meet the requirements of security and efficiency simultaneously. Therefore, in this paper, we would like to propose a new simple three-party password based authenticated key exchange protocol. Compared with other existing protocols, our proposed protocol does not require any server's public key, but can resist against various known attacks. Therefore, we believe it is suitable for some practical scenarios. 
26|1||Guide for Authors.|
26|1||IBC - Calendar of Events|
26|2|http://www.sciencedirect.com/science/journal/01674048/26/2|(iii) Contents|
26|2||Windows Vista: Microsoft's brave new world|
26|2||Malware Update|
26|2||Advanced user authentication for mobile devices|As mobile devices continue to evolve in terms of the capabilities and services offered, so they introduce additional demands in terms of security. An issue that has traditionally been poorly served is user authentication, with the majority of devices relying upon problematic secret knowledge approaches. This paper proposes the use of more advanced biometric methods as an alternative. After considering the general range of available techniques and their applicability to mobile devices, the discussion focuses upon the concept of keystroke analysis. Results of a practical evaluation are presented based upon the entry of both telephone numbers and text messages on a mobile phone. The findings reveal the technique to have promise for certain users with average error rates below 5%. The paper then proceeds to explain how the accuracy could be further improved by incorporating keystroke analysis within a composite authentication mechanism that utilises a portfolio of authentication techniques to provide robust, accurate and transparent authentication of the user. 
26|2||Clustering subjects in a credential-based access control framework|Currently, access control of distributed Internet resources (such as files, documents and web services) has become extremely demanding. Several new access control models have been introduced. Most of the proposed approaches increase the complexity of the access control procedure and at the same time expressing these models is becoming complicated. Improving the execution time of the access control procedures is a challenging task due to the increased number of resources (available over the Internet) and the size of the audience involved. In this paper, we introduce an approach for speeding up the access control procedure under an environment accessed by known subjects (i.e. subjects whose identity and attributes are known apriori through a subscription phase). This approach is based on some update functions (employed at the background during idle times) over files which are associated with subjects. The core task of the proposed update is its dynamic nature and its clustering of subjects according to their interests and credentials. Moreover, this work associates subjects with security policies that are most likely to be triggered according to (the subjects) interests. Credential-based access control is considered to properly protect frameworks distributing resources to known subjects and here emphasis is given to the complexity involved in order to decrease the access request evaluation time under a credential-based access control framework. 
26|2||Privacy-preserving programming using sython|Programmers often have access to confidential data that are not strictly needed for program development. Broad privileges from accounts given to programmers allow them to view files, database table entries or even variables in team members' code that are not critical to their own code. The risk inherent in such unchecked access to possibly private and sensitive data is exacerbated in cases where software development is part of a larger functioning system with data already in place, and is especially severe in cases where development is contracted out to third parties. This paper focuses on the problem of providing developers with a programming language that incorporates privacy-preserving features. We present Sython, a preliminary prototype based on the Python programming language that incorporates such features, examining both implementation and the appearance of the system as viewed by a programmer. The main purpose of this paper is to explore the use of language syntax and underlying support for secure variables so that data owners can contract out programming tasks without worrying about information leakage. 
26|2||Probabilistic analysis of an algorithm to compute TCP packet round-trip time for intrusion detection|Estimating the length of a connection chain is challenging and critical in detecting stepping-stone intrusion. In this paper, we propose a novel method, called standard deviation-based clustering approach (SDBA), to estimate the length of an interactive connection chain by computing round-trip time (RTT). SDBA takes advantage of RTTs distribution and inter-arrival distribution of “send” packets. We prove that the probability of making a correct selection of RTT through SDBA is bounded by 1 − (1/q2), where q is a number related to standard deviation of RTTs distribution and send packets inter-arrival distribution. Experimental results showed that SDBA can compete against the best known algorithm in packet-matching rate and accuracy. This paper also presents the restrictions of SDBA. 
26|2||A study on decision consolidation methods using analytic models for security systems|The successful management of information security within an organization is vital to its survival and success. The necessary security controls need to be implemented and managed effectively. In this paper, using the characteristics of the AHP, a study on information security management systems is selected from the perspective of Process Model and Criteria. A case study has proven potential value of this methodology in helping decision-makers in supporting their selection of security controls. 
26|2||A framework for behavior-based detection of user substitution in a mobile context|Personal mobile devices, such as mobile phones, smartphones, and communicators can be easily lost or stolen. Due to the functional abilities of these devices, their use by unintended persons may result in severe security breaches concerning private or corporate data and services. Organizations develop their security policy and employ preventive techniques to combat unauthorized use. Current solutions, however, are still breakable and there is a strong need for means to detect user substitution when it happens. A crucial issue in designing such means is to define the measures to be monitored.In this paper, a structured conceptual framework for mobile-user substitution detection is proposed. The framework is based on the idea that some aspects of user behavior and environment reflect the user's personality in a recognizable way. These hypothesized aspects are further studied in order to identify the characteristics describing the individuality of these aspects, and to identify the measures whereby the characteristics can be represented.The main constructs of the framework are defined and explained in the paper; these include the components describing individuality of user behavior and environment, and the technical components needed to implement user substitution detection based on this individuality. The paper also provides a tentative list of individual behavioral and environmental aspects, along with characteristics and measures to represent them. The contemporary solutions, aimed at user substitution detection, are analyzed from the perspective of the framework, and the needs for further research are discussed. 
26|2||Information security in networkable Windows-based operating system devices: Challenges and solutions|This paper explores information security risks in networkable Windows-based operating system (NWOS) devices. While these devices face the same information security risks as any other Windows platform, NWOS devices present additional challenges to vendors and buyers throughout the product lifecycle. It appears that NWOS devices are particularly vulnerable to information security threats because of the vendors' and buyers' lack of awareness of the security risks associated with such devices. Based on evidence collected from a manufacturer of Digital Storage Oscilloscopes, the paper offers a set of challenges faced and solution applied by this vendor in its interactions with buyers. In order to reduce the vulnerability of NWOS devices, the paper considers several information security measures for the production, sales and after-sales phases. Lastly, the paper outlines the business reasoning for both vendors and buyers to pursue this information security strategy. 
26|2||Investigative response: After the breach|
26|2||Guide for Authors.|
26|3|http://www.sciencedirect.com/science/journal/01674048/26/3|(iii) Contents|
26|3||Mobile computing: The next Pandora's Box|
26|3||Malware update|
26|3||SVision: A novel visual network-anomaly identification technique|We propose a novel graphical technique (SVision) for intrusion detection, which pictures the network as a community of hosts independently roaming in a 3D space defined by the set of services that they use. The aim of SVision is to graphically cluster the hosts into normal and abnormal ones, highlighting only the ones that are considered as a threat to the network. Our experimental results conducted on DARPA 1999 and 2000 intrusion detection and evaluation datasets as well as real network data captured between 2003 and 2005 from the University of New Brunswick main link, and also a private network, show the proposed technique as a good candidate for the detection of various network threats such as vertical and horizontal scanning attacks, Denial of Service (DoS) attacks, Distributed DoS (DDoS) attacks, as well as worm propagation attack. Finally, the visualization technique proves to cope with high number of hosts in the network, the experimental results using network data of up to 1,000,000 distinct IPs per time interval. 
26|3||Modeling and analyzing the spread of active worms based on P2P systems|Active worms spread in an automated fashion and can flood the Internet in a very short time. Hit-list scanning is a technique for accelerating the initial spread of a worm. Due to the recent surge of Peer-to-Peer (P2P) systems with large numbers of users, P2P systems can be a potential vehicle for the active worms to achieve fast worm propagation in the Internet too. When the technique of hit-list scanning is used on top of P2P system, some new characters emerge. In this paper, we have defined an L system and an O system. Based on modeling the spread of active worms, we focused all our attention on analyzing the characteristics of the spread of active worms between the L system and the O system which can help us design and control the P2P systems effectively as well as defend against the propagation of worms. 
26|3||Measuring, analyzing and predicting security vulnerabilities in software systems|In this work we examine the feasibility of quantitatively characterizing some aspects of security. In particular, we investigate if it is possible to predict the number of vulnerabilities that can potentially be present in a software system but may not have been found yet. We use several major operating systems as representatives of complex software systems. The data on vulnerabilities discovered in these systems are analyzed. We examine the results to determine if the density of vulnerabilities in a program is a useful measure. We also address the question about what fraction of software defects are security related, i.e., are vulnerabilities. We examine the dynamics of vulnerability discovery hypothesizing that it may lead us to an estimate of the magnitude of the undiscovered vulnerabilities still present in the system. We consider the vulnerability discovery rate to see if models can be developed to project future trends. Finally, we use the data for both commercial and open-source systems to determine whether the key observations are generally applicable. Our results indicate that the values of vulnerability densities fall within a range of values, just like the commonly used measure of defect density for general defects. Our examination also reveals that it is possible to model the vulnerability discovery using a logistic model that can sometimes be approximated by a linear model. 
26|3||Evaluating information security tradeoffs: Restricting access can interfere with user tasks|Computer security is a balance between protecting information and enabling authorized access. Tightening security by making systems more inaccessible can hinder employees and make them less productive. It can also result in lower security as workers struggle to find ways around the security conditions to enable them to do their jobs. This study analyzes an information systems user survey to evaluate the tradeoffs between protection and accessibility. Over one-third of the respondents report problems with interference from security provisions. A structural equation model explores the impact of these effects on eventual security levels. 
26|3||M-CLIQUES: Modified CLIQUES key agreement for secure multicast|In secure multicast applications, members may join or leave frequently and key management is one of the most challenging problems. In this research, we proposed a modified CLIQUES key management protocol. It was the modification of CLIQUES that consisted of two stages: Static CLIQUES and Hierarchical CLIQUES. In Static CLIQUES, a static group controller was used to distribute the partial keys to group members. Compared with traditional CLIQUES, the Static CLIQUES was more secure for key storage, less complex, less processing requirement in the user machine, and easier to provide member privacy protection. In Hierarchical CLIQUES, a hierarchical structure was employed to support larger size of group members. Our experiments showed that the modified CLIQUES protocol was more scalable than CLIQUES. Also, it required less processing power than the Key Tree-Based approaches. 
26|3||Adaptable security mechanism for dynamic environments|Electronic services in dynamic environment (e.g. e-government, e-banking, e-commerce, etc.), meet many different barriers reducing their efficient applicability. One of them is the requirement of information security when it is transmitted, transformed, and stored in an electronic service. It is possible to provide the appropriate level of security by applying the present-day information technology. However, the level of protection of information is often much higher than it is necessary to meet potential threats. Since the level of security strongly affects the performance of the whole system, the excessive protection decreases its reliability and availability and, as a result, its global security. In this paper we present a mechanism of adaptable security for, digital information transmission systems (being usually the crucial part of e-service). It makes it possible to guarantee the adequate level of protection for actual level of threats dynamically changing in the environment. In our model the basic element of the security is the Public Key Infrastructure (PKI) is enriched with specific cryptographic modules. 
26|3||Holistic security management framework applied in electronic commerce|With the advance of electronic commerce more and more companies have become dependent on their information systems for their daily business operations. This dependency requires the security of these systems to be managed. This paper presents a holistic security management framework that should allow for easy and affordable security management. This process framework is described by hierarchically organized processes which allow for a business, technology and social driven security management. It presents the activities involved in the five core and two support processes which are conducted iteratively. To support this framework three cases of successful applications and an informal evaluation against SSE-CMM are presented. 
26|3||Guide for Authors.|
26|4|http://www.sciencedirect.com/science/journal/01674048/26/4|(iii) Contents|
26|4||Struggles in the academic side of infosec|
26|4||Security Views - Malware|
26|4||A qualitative study of users' view on information security|Users play an important role in the information security performance of organisations by their security awareness and cautious behaviour. Interviews of users at an IT-company and a bank were qualitatively analyzed in order to explore users' experience of information security and their personal role in the information security work. The main patterns of the study were: (1) users state to be motivated for information security work, but do not perform many individual security actions; (2) high information security workload creates a conflict of interest between functionality and information security; and (3) documented requirements of expected information security behaviour and general awareness campaigns have little effect alone on user behaviour and awareness. The users consider a user-involving approach to be much more effective for influencing user awareness and behaviour. 
26|4||Teaching information systems security courses: A hands-onÂ approach|It has become imperative for companies, governments, and organizations to understand how to guard against hackers, outsiders, and even disgruntled employees who threaten their information security, integrity and daily business operations. To address national needs for computer security education, many universities have incorporated computer and security courses into their undergraduate and graduate curricula. At the Miller College of Business, Department of Information Systems and Operations Management, Ball State University, we have introduced an information systems security option for students majoring in information systems. This paper describes our approach, our experiences and lessons learned for teaching security courses using a hands-on approach. 
26|4||Retraining a keystroke dynamics-based authenticator with impostor patterns|In keystroke dynamics-based authentication, novelty detection methods are used since only the valid user's patterns are available when a classifier is first constructed. After a while, however, impostors' keystroke patterns become available from failed login attempts. We propose to employ the retraining framework where a novelty detector is retrained with the impostor patterns to enhance authentication accuracy. In this paper, learning vector quantization for novelty detection and support vector data description are retrained with the impostor patterns. Experimental results show that retraining improves the authentication performance and that learning vector quantization for novelty detection outperforms other widely used novelty detectors. 
26|4||Masquerade detection by boosting decision stumps using UNIX commands|Masqueraders who impersonate other users pose a serious threat to computer security. They are generally difficult to detect using firewalls or misuse-based intrusion detection systems. Although anomaly detection techniques provide a promising approach for masquerade detection, these techniques are not widely used due to their poor accuracy and relatively high false alarm rate. Previous studies of anomaly detection have mainly focused on model-based approaches, such as the support vector machine (SVM) and the hidden Markov model (HMM). Characteristics of user behavior were entered, and an evaluation value was calculated by the model. To judge whether or not the user was a masquerader, this value was compared with a predefined threshold within the model. However, the judgment processes in these models were invisible and uninterpretable by the security administrator. This study examines a different method for masquerader detection, a rule-based approach, which compares n-grams of command sequence using a technique known as boosting decision stumps. The main advantage of a rule-based method is that the generated rules are easier to interpret. The decision stump is the simplest form of a decision tree. Its “decision” is made by checking the presence or absence of a specified n-gram of command sequence. The boosting decision stumps method uses the weighted combination of the decision stumps in an application of the AdaBoost algorithm. Experiments were carried out on the common data set of UNIX commands that has been used in previous studies. The boosting decision stumps method results in an accuracy rate of 89.2% with a false alarm rate of 10.1%, while the best previously reported results had an accuracy rate of 80.1% with a false alarm rate of 9.7%. Experimental results show that the boosting decision stumps method is more effective and a more interpretable method for masquerade detection. 
26|4||Dual-wrapped digital watermarking scheme for image copyright protection|Digital watermarking is an effective way to protect the rightful ownership of multimedia contents. In this paper, a two-phase watermarking scheme is proposed, which extracts both the grayscale watermark and the binary one from the protected images to achieve the copyright protection goal. In the first phase, the proposed method utilizes the pixel values of the original image to construct a grayscale watermark image. In the second phase, a binary watermark image can be further retrieved via the just-procured-permuted grayscale watermark from the first phase. Under these circumstances, the proposed technique results in lossless embedding; in other words, the protected images are the same as the original ones. The overall verification procedure does not need the original image. Only those who have the original grayscale watermark and the corresponding secret keys can extract the grayscale and binary watermarks sequentially, which enhances security and robustness of the proposed watermarking system. Experimental results show that the proposed approach satisfies the general requirements of image watermarking and is superior to related methods in terms of transparency and robustness. Moreover, it is easier to be implemented than transform-domain techniques. These flexible features make the proposed method more feasible and practical for copyright protection. 
26|4||A resource-constrained group key agreement protocol for imbalanced wireless networks|Secure group communication is an important research issue for network security because of the popularity of group-oriented applications such as electronic conferences and collaborative works. The secure group key agreement protocol design is crucial for achieving secure group communications. As we all know, most security technologies are currently deployed in wired networks and are not fully applicable to wireless networks involving mobile devices with limited computing capability. In 2005, Nam et al. proposed a group key agreement protocol for a wireless environment. Unfortunately, in this paper we present that their protocol has a security weakness in which participants cannot confirm that their contributions were actually involved in the group key establishment. This is an important property of group key agreement. Therefore, we propose a new group key agreement protocol for an imbalanced wireless network consisting of many mobile nodes with limited computing capability and a powerful node with less restriction. We show that the proposed protocol produces contributory group key agreement. We demonstrate that the proposed protocol is provably secure against passive attacks under the decisional Diffie–Hellman problem assumption. A simulation result on a personal digital assistant (PDA) shows that the proposed protocol is well suited for mobile devices with limited computing capability. 
26|4||Functional similarities between computer worms and biological pathogens|Computer worms pose a serious threat to computer and network security. Interestingly, they share many common tactics with biological pathogens with respect to infecting and propagating. In this paper, we study the six most common fatal infectious diseases—measles, malaria, HIV/AIDS, tuberculosis, influenza and the diarrhoeal diseases—to (1) determine the individual mechanisms and environmental conditions that have contributed to their success, and (2) show the parallels between the mechanisms and behavior of successful biological infections and successful digital infections. Moreover, by identifying the specific areas of similarity and looking at effective preventive and creative measures used against biological pathogens, we draw insights about what steps individual computers and networks can take to protect themselves. 
26|4||Guide for Authors.|
26|5|http://www.sciencedirect.com/science/journal/01674048/26/5|(iii) Contents|
26|5||Vulnerability Take Grant (VTG): An efficient approach to analyze network vulnerabilities|Modeling and analyzing information system vulnerabilities help predict possible attacks to computer networks using vulnerabilities information and the network configuration. In this paper, we propose a comprehensive approach to analyze network vulnerabilities in order to answer the safety problem focusing on vulnerabilities. The approach which is called Vulnerability Take Grant (VTG) is a graph-based model consists of subject/objects as nodes and rights/relations as edges to represent the system protection state. Each node may have properties including single vulnerabilities such as buffer overflow. We use the new concept of vulnerability rewriting rule to specify the requirements and consequences of exploiting vulnerabilities. Analysis of the model is achieved using our bounded polynomial algorithm, which generates the most permissive graph in order to verify whether a subject can obtain an access right over an object. The algorithm also finds the likely attack scenarios. Applicability of the approach is investigated by modeling widespread vulnerabilities in their general patterns. A real network is modeled as a case study in order to examine how an attacker can gain unauthorized access via exploiting the chain of vulnerabilities. Our experience shows the efficiency, applicability, and expressiveness in modeling a broader range of vulnerabilities in our approach in comparison to the previous approaches. 
26|5||The impact that placing email addresses on the Internet has on the receipt of spam: An empirical analysis|Email communication is encumbered with a mass of email messages which their recipients have neither requested nor require. Even worse, the impacts of these messages are far from being simply an annoyance, as they also involve economic damage. This manuscript examines the resource “email addresses”, which is vital for any potential bulk mailer and spammer. Both a methodology and a honeypot conceptualization for implementing an empirical analysis of the usage of email addresses placed on the Internet are proposed here. Their objective is to assess, on a quantitative basis, the extent of the current harassment and its development over time. This “framework” is intended to be extensible to measuring the effectiveness of address obscuring techniques. The implementation of a pilot honeypot is described, which led to key findings, some of them being: (1) Web placements attract more than two-thirds (70%) of all honeypot spam emails, followed by newsgroup placements (28.6%) and newsletter subscriptions (1.4%). (2) The proportions of spam relating to the email addresses' top-level domain can be statistically assumed to be uniformly distributed. (3) More than 43% of addresses on the web have been abused, whereas about 27% was the case for addresses on newsgroups and only about 4% was the case for addresses used for a newsletter subscription. (4) Regarding the development of email addresses' attractiveness for spammers over time, the service “web sites” features a negative linear relationship, whereas the service “Usenet” shows a negative exponential relationship. (5) Only 1.54% of the spam emails showed an interrelation between the topic of the spam email and that of the location where the recipient's address was placed, so that spammers are assumed to send their emails in a “context insensitive” manner. The results of the empirical analysis motivate the need for the protection of email addresses through obscuration. We analyze this need by formulating requirements for address obscuring techniques and we reveal to which extent today's most relevant approaches fulfill these requirements. 
26|5||Authentication in a layered security approach for mobile ad hoc networks|An ad hoc network is a collection of nodes that do not need to rely on a predefined infrastructure to keep the network connected. Nodes communicate amongst each other using wireless radios and operate by following a peer-to-peer network model. In this article we investigate authentication in a layered approach, which results to multiple lines of defense for mobile ad hoc networks. The layered security approach is described and design criteria for creating secure ad hoc network using multiple authentication protocols are analyzed. The performance of several such known protocols, which are based on challenge–response techniques, is presented through simulation results. 
26|5||Using header session messages to anti-spamming|The Internet is popular, with email use functioning as the major Internet activity. However, spam has recently become a major problem impeding the use of email. Many spam filtering techniques have been implemented so far. Most current anti-spamming techniques filter out junk emails based on email subjects and body messages. Nevertheless, subjects and email contents are not the only cues for judging spam. This investigation presents a statistical analysis of the header session messages of junk and normal emails, and explores the possibility of utilizing these messages to perform spam filtering. The message head session, including the sender's mail address, receiver's mail address and time, which is of little interest to most users, also provides further information for anti-spamming purpose. A statistical analysis is undertaken on the content of 10,024 junk emails collected from a Spam Archive database, and 599 regular emails in company with 635 solicited listserv or commercial emails contributed by volunteers. Content analysis results demonstrate that up to 92.5% of junk emails are filtered out when utilizing the message-ID, mail user agent, and sender and receiver addresses in the header session as cues. Additionally, the proposed approach may induce a low block error rate for normal emails for the sample utilized in this investigation. This low rate of over-block errors is a significant merit of the proposed anti-spamming approach. The proposed approach of utilizing header session messages to filter out junk emails may coexist with other anti-spamming approaches. Therefore, no conflict arises between the proposed approach and existing spam prevention approaches. 
26|5||Security for a Multi-Agent System based on JADE|The present paper explores the challenges, issues and solutions to satisfy the security requirements of a Multi-Agent System (MAS) based on the JADE framework. By means of a prototype system used for Learning Management, an adequate security concept for MAS in general is presented. Hereby several security features are considered, ranging, among others, from the authentication of users over encryption of the exchanged data up to the authorization of the access to services designated only to a determined group of users. 
26|5||Comparative studies on authentication and key exchange methods for 802.11 wireless LAN|IEEE 802.11 wireless LAN has become one of the hot topics on the design and development of network access technologies. In particular, its authentication and key exchange (AKE) aspects, which form a vital building block for modern security mechanisms, deserve further investigation. In this paper we first identify the general requirements used for WLAN authentication and key exchange (AKE) methods, and then classify them into three levels (mandatory, recommended, and additional operational requirements). We present a review of issues and proposed solutions for AKE in 802.11 WLANs. Three types of existing methods for addressing AKE issues are identified, namely, the legacy, layered and access control-based AKE methods. Then, we compare these methods against the identified requirements. Based on the analysis, a multi-layer AKE framework is proposed, together with a set of design guidelines, which aims at a flexible, extensible and efficient security as well as easy deployment. 
26|5||Assessing the security perceptions of personal Internet users|Personal Internet users are increasingly finding themselves exposed to security threats during their use of home PC systems. However, concern can be raised about users' awareness of these problems, and the extent to which they are consequently protected and equipped to deal with incidents they may encounter. This paper presents results from a survey of 415 home users to assess their perceptions of security issues, and their attitudes towards the use of related safeguards. The findings reveal that although there is a high degree of confidence at a surface level, with respondents claiming to be aware of the threats and utilising many of the relevant safeguards, a deeper inspection suggests that there are several areas in which desirable knowledge and understanding are lacking. Although many of the problems were predictably acute amongst novice users, there were also notable shortcomings amongst users who considered themselves to have advanced levels of computing experience. 
26|5||Guide for Authors.|
26|6|http://www.sciencedirect.com/science/journal/01674048/26/6|(iii) Contents|
26|6||From the Publisher|
26|6||Anti-keylogging measures for secure Internet login: An example of the law of unintended consequences|Traditional authentication systems used to protect access to online services (such as passwords) are vulnerable to compromise via the introduction of a keystroke logger to the service user's computer. This has become a particular problem now that many malicious programs have keystroke logging capabilities. When banks first introduced Online Banking services they realised this, and added features to protect users against keystroke logging. In this paper we show, using a real Online Banking system as an example, that if these features are incorrectly implemented they can allow an attacker to bypass them completely and gain access to a user's bank account within a small number of attempts. The vulnerability was initially noticed in a particular Online Banking service, but any system implemented in the way we describe is equally vulnerable. 
26|6||An adaptive method for anomaly detection in symmetric network traffic|Symmetry is an obvious phenomenon in two-way communications. In this paper, we present an adaptive nonparametric method that can be used for anomaly detection in symmetric network traffic. Two important features are emphasized in this method: (i) automatic adjustment of the detection threshold according to the traffic conditions; and (ii) timely detection of the end of an anomalous event. Source-end defense against SYN flooding attacks is used to illustrate the efficacy of this method. Experiments on real traffic traces show that this method has high detection accuracy and low detection delays, and excels at detecting low intensity attacks. 
26|6||Making security usable: Are things improving?|Given the increased focus on the need for usable security, it is now to be hoped that the issue will receive greater attention in new software releases. Unfortunately, however, there is still evidence to suggest that usable security receives insufficient consideration when the related features are presented in the context of larger applications. As an illustration of this claim, the paper examines how the security-related features have evolved within new releases of Internet Explorer and Word, and identifies that although there have been some improvements when compared to earlier versions, there are also aspects that will represent new or ongoing problems for users. Examples of such problems are highlighted in a number of security-related interfaces from both applications, with the use of technical terminology and/or a lack of accompanying help being amongst the frequent concerns. Nielsen's usability heuristics are then used as the basis for a summary-level evaluation, to illustrate how the identified issues also contravene good practice in user interface design. 
26|6||Guide for Authors.|
26|7-8|http://www.sciencedirect.com/science/journal/01674048/26/7-8|(iii) Contents|
26|7-8||An assessment of website password practices|Password-based authentication is frequently criticised on the basis of the ways in which the approach can be compromised by end-users. However, a fundamental point in the defence of many users is that they may not know any better, and lack appropriate guidance and support when choosing their passwords and subsequently attempting to manage them. Given that such support could reasonably be expected to come from the systems upon which the passwords are used, this paper presents an assessment of password practices on 10 popular websites, examining the extent to which they provide guidance for password selection, enforce restrictions on password choices, and support easy and effective recovery or reset if passwords are forgotten. The findings reveal that the situation is extremely variable, with none of the assessed sites performing ideally across all of the assessed criteria. Better efforts are consequently required if password practices amongst the general populous are expected to improve. 
26|7-8||Key agreement for key hypergraph|In this paper, we propose a key agreement protocol for a key hypergraph. In a key hypergraph, a party is represented as a vertex and a group of parties is represented as a hyperedge. A key agreement protocol for a key hypergraph establishes all the keys for hyperedges in a key hypergraph at the same time. A naive approach would be to run a group key protocol concurrently for each hyperedge. By using the randomness re-use technique, we propose an efficient key agreement protocol for a key hypergraph, which is of two rounds.We formalize the key exchange model for a key hypergraph which is an extension of a group key exchange model and especially incorporates insider attacks. The proposed key agreement protocol for a key hypergraph provides key independence and forward secrecy in the random oracle model under the computational Diffie–Hellman assumption. 
26|7-8||An active learning based TCM-KNN algorithm for supervised network intrusion detection|As network attacks have increased in number and severity over the past few years, intrusion detection is increasingly becoming a critical component of secure information systems and supervised network intrusion detection has been an active and difficult research topic in the field of intrusion detection for many years. However, it hasn't been widely applied in practice due to some inherent issues. The most important reason is the difficulties in obtaining adequate attack data for the supervised classifiers to model the attack patterns, and the data acquisition task is always time-consuming and greatly relies on the domain experts. In this paper, we propose a novel supervised network intrusion detection method based on TCM-KNN (Transductive Confidence Machines for K-Nearest Neighbors) machine learning algorithm and active learning based training data selection method. It can effectively detect anomalies with high detection rate, low false positives under the circumstance of using much fewer selected data as well as selected features for training in comparison with the traditional supervised intrusion detection methods. A series of experimental results on the well-known KDD Cup 1999 data set demonstrate that the proposed method is more robust and effective than the state-of-the-art intrusion detection methods, as well as can be further optimized as discussed in this paper for real applications. 
26|7-8||A non-intrusive biometric authentication mechanism utilising physiological characteristics of the human head|This paper proposes and evaluates a non-intrusive biometric authentication technique drawn from the discrete areas of biometrics and Auditory Evoked Responses. The technique forms a hybrid multi-modal biometric in which variations in the human voice due to the propagation effects of acoustic waves within the human head are used to verify the identity of a user. The resulting approach is known as the Head Authentication Technique (HAT). Evaluation of the HAT authentication process is realised in two stages. First, the generic authentication procedures of registration and verification are automated within a prototype implementation. Second, a HAT demonstrator is used to evaluate the authentication process through a series of experimental trials involving a representative user community. The results from the trials confirm that multiple HAT samples from the same user exhibit a high degree of correlation, yet samples between users exhibit a high degree of discrepancy. Statistical analysis of the prototype performance realised system error rates of 6% False Non-Match Rate (FNMR) and 0.025% False Match Rate (FMR). 
26|7-8||Mining TCP/IP packets to detect stepping-stone intrusion|An effective approach of detecting stepping-stone intrusion is to estimate the number of hosts compromised through estimating the length of a connection chain. This can be done by studying the changes in TCP packet round-trip time. In this paper, we propose a new algorithm by using data mining method to find the round-trip time from the timestamps of TCP send and echo packets. Previous algorithms produce either good packet matches on very few packets, or poor matches on many packets. This method gives us better round-trip time and more matched packets than other algorithms proposed in the past. It can estimate the length of a connection more accurate than other methods and has largely decreased false positive error and false negative error in detecting stepping-stone intrusion comparing with existing methods. 
26|7-8||The security challenges inherent in VoIP|VoIP offers unparalleled flexibility for users, but with that flexibility also come security concerns. Moving both the control and data channels to IP opens the doors to new types of attack not seen before on phone systems. 
26|7-8||Intrusion detection using text processing techniques with a kernel based similarity measure|This paper focuses on intrusion detection based on system call sequences using text processing techniques. It introduces kernel based similarity measure for the detection of host-based intrusions. The k-nearest neighbour (kNN) classifier is used to classify a process as either normal or abnormal. The proposed technique is evaluated on the DARPA-1998 database and its performance is compared with other existing techniques available in the literature. It is shown that this technique is significantly better than the other techniques in achieving lower false positive rates at 100% detection rate. 
26|7-8||Run-time label propagation for forensic audit data|It is desirable to be able to gather more forensically valuable audit data from computing systems than is currently done or possible. This is useful for the analysis of events that took place on the system for the purpose of digital forensic investigations. In this paper we propose a mechanism that allows arbitrary meta-information bound to principals on a system to be propagated based on causality and influenced by information flow. We further discuss how to implement such a mechanism for the FreeBSD operating system and present a proof-of-concept implementation that has little overhead compared to the system without label propagation. 
26|7-8||Guide for Authors.|
27|1-2|http://www.sciencedirect.com/science/journal/01674048/27/1-2|(iii) Contents|
27|1-2||Editorial|
27|1-2||Improvement of keystroke data quality through artificial rhythms and cues|Keystroke dynamics based user authentication (KDA) can achieve a relatively high performance if a fairly large number of typing patterns are available. It is almost always the case that KDA is combined with password based authentication. Users are often required to change their passwords. When a user changes one's password, however, only a handful of new patterns become available. In a mobile situation, moreover, very short passwords are used. Under such a circumstance, the quality of data becomes important. Recently, artificial rhythms and cues were proposed to improve the quality of data. In this paper, we verify the effectiveness of artificial rhythms and cues through hypotheses tests using the data from 25 users under various situations. The experimental results show that artificial rhythms increase the uniqueness while cues increase the consistency. 
27|1-2||Efficient identity-based RSA multisignatures|A digital multisignature is a digital signature of a message generated by multiple signers with knowledge of multiple private keys. In this paper, an efficient RSA multisignature scheme based on Shamir's identity-based signature (IBS) scheme is proposed. To the best of our knowledge, this is the first efficient RSA-based multisignature scheme with both fixed length and the verification time. The proposed identity-based multisignature scheme is secure against forgerability under chosen-message attack. It is also secure against multi-signer collusion attack and adaptive chosen-ID attack. 
27|1-2||Cryptanalysis of simple three-party key exchange protocol|Recently, Lu and Cao published a novel protocol for password-based authenticated key exchanges (PAKE) in a three-party setting in Journal of Computers and Security, where two clients, each shares a human-memorable password with a trusted server, can construct a secure session key. They argued that their simple three-party PAKE (3-PAKE) protocol can resist against various known attacks. In this paper, we show that this protocol is vulnerable to a kind of man-in-the-middle attack that exploits an authentication flaw in their protocol and is subject to the undetectable on-line dictionary attack. We also conduct a detailed analysis on the flaws in the protocol and provide an improved protocol. 
27|1-2||Enterprise information security strategies|Security decisions are made at every level of an organization and from diverse perspectives. At the tactical and operational levels of an organization, decision making focuses on the optimization of security resources, that is, an integrated combination of plans, personnel, procedures, guidelines and technology that minimize damages and losses. While these actions and tactics reduce the frequency and/or consequences of security breaches, they are bounded by the organization's global security budget. At the strategic, enterprise level management must answer the question, “What is the security budget (cost expenditures), where each dollar spent on security must be weighed against alternative non-security expenditures, that is justified by the foregone (prevented) losses and damages?” The answer to that question depends on the tolerances of decision makers for risk and the information employed to reach it. 
27|1-2||A global security architecture for intrusion detection on computer networks|Detecting all kinds of intrusions efficiently requires a global view of the monitored network. Built to increase the security of computer networks, traditional IDS's are unfortunately unable to give a global view of the security of a network. To overcome this situation, we are developing a distributed SOC (Security Operation Center) which is able to detect attacks occurring simultaneously on several sites in a network and to give a global view of the security of that network. In this article, we present the global architecture of our system, called DSOC as well as several methods used to test its accuracy and performance. 
27|1-2||Containing large-scale worm spreading in the Internet by cooperative distribution of traffic filtering policies|The Internet is crucial to business, government, education and many other facets of society, but the easy access and wide usage of the most common network services make it a primary target for the propagation of viral infections or worms. It has been widely experienced that the massive worldwide spreading of very fast and aggressive worms may easily disrupt or damage the connectivity of large sections of the Internet, affecting millions of users. Classical containment strategies, based on manual application of traffic filters will be almost totally ineffective in the wide area. Consequently, developing an automated self-distributing containment strategy is the most viable way to defeat the worm propagation in an acceptable time The objective of our work is to develop a distributed and cooperative containment strategy based on having traffic filtering information dynamically disseminate throughout the network at a speed that is faster than (or at least comparable with) the propagation of worms. Our framework based on BGP extensions to distribute traffic filtering information has the advantage of using the existing infrastructure and inter-as communication channels. We envision that the above solution will be one of the most effective and challenging lines of defense against next-generation more aggressive worms. 
27|3-4|http://www.sciencedirect.com/science/journal/01674048/27/3-4|(iii) Contents|
27|3-4||Editorial|
27|3-4||SSL/TLS session-aware user authentication revisited|Man-in-the-middle (MITM) attacks pose a serious threat to SSL/TLS-based e-commerce applications. In Oppliger R, Hauser R, Basin D [SSL/TLS session-aware user authentication – or how to effectively thwart the man-in-the-middle. Computer Communications August 2006;29(12):2238–46] and Oppliger R, Hauser R, Basin D [SSL/TLS session-aware user authentication. IEEE Computer March 2008;41(3) 59-65], we introduced the notion of SSL/TLS session-aware user authentication to protect SSL/TLS-based e-commerce applications against MITM attacks and we proposed an implementation based on impersonal authentication tokens. In this paper, we present a number of extensions of the basic idea. These include multi-institution tokens, possibilities for changing the PIN, and different ways of making several popular and widely deployed user authentication systems SSL/TLS session-aware. 
27|3-4||Standardising vulnerability categories|Each vulnerability scanner (VS) represents, identifies and classifies vulnerabilities in its own way, thus making the different scanners difficult to study and compare. Despite numerous efforts by researchers and organisations to solve the disparity in vulnerability names used in the different VSs, vulnerability categories have still not been standardised. This paper highlights the importance of having a standard vulnerability category set. It also outlines an approach towards achieving this goal by generating a standard set of vulnerability categories. A data-clustering algorithm that employs artificial intelligence is used for this purpose. The significance of this research results from having an intelligent technique that aids in the generation of standardised vulnerability categories in a relatively fast way. In addition, the technique is generic in the sense that it allows one to accommodate any VS currently known on the market to create such vulnerability categories. Another benefit is that the approach followed in this paper allows one to also compare various VSs currently available on the market. A prototype is presented to verify the concept. 
27|3-4||A feasible intrusion detector for recognizing IIS attacks based on neural networks|Most activities on the Internet can be recorded as log files of websites and website administrators can inspect log files to locate problems after any network intrusion occurs. However, since log files usually contain a huge quantity of data, without effective methods, it is generally not feasible for administrators to determine the concealed meanings within log files. One method for dealing with this issue is to use neural networks; this is an effective means to distinguish and classify abnormal data in log files, thus alleviating the administrator's burden. This paper presents the results of a study on intrusion detection on IIS (Internet information services) utilizing a hybrid intrusion detection system (IDS). The feasibility of the hybrid IDS is validated based on the Internet scanner system (ISS). In the intrusion detection system proposed, we used four different training data sets: 200, 800, 1400, and 2000. The system is trained either by Taguchi's experimental design or full factorial experimental design under different training data sets; the former can save much more time than the latter. Under Taguchi's experimental design, the best results are obtained when the training data set is of size 1400; overall accuracy in this case is 97.5%. On the contrary, for the full factorial experimental design, the best results are reached when the training data set is of size 2000; overall accuracy is 97.6%. Our study indicates that when to retrain the detector and how much time to allow for this training fully depend on the downgrade percentage of the detection rate, which determines the size of the retraining data set. To reduce the void time for updating the detector, the downgrade percentage should be restricted. 
27|3-4||An aspect-oriented approach for the systematic security hardening of code|In this paper, we present an aspect-oriented approach for the systematic security hardening of source code. It aims at allowing developers to perform software security hardening by providing an abstraction over the actions required to improve the security of the program. This is done by giving them the capabilities to specify high-level security hardening plans that leverage a priori defined security hardening patterns. These patterns describe the required steps and actions to harden security code, including detailed information on how and where to inject the security code. We show the viability and relevance of our approach by: (1) elaborating security hardening patterns and plans to common security hardening practices, (2) realizing these patterns by implementing them into aspect-oriented languages, (3) applying them to secure applications, (4) testing the hardened applications. Furthermore, we discuss, in this paper, our insights on the appropriateness, strengths and limitations of the aspect-oriented paradigm for security hardening. 
27|3-4||Efficient multi-server authentication scheme based on one-way hash function without verification table|Following advances in network technologies, an increasing number of systems have been provided to help network users via the Internet. In order to authenticate the remote users, password-based security mechanisms have been widely used. They are easily implemented, but these mechanisms must store a verification table in the server. If an attacker steals the verification table from the server, the attacker may masquerade as a legal user. To solve the verification table stolen problem, numerous single server authentication schemes without verification tables have been proposed. These single authentication schemes suffer from a shortcoming. If a remote user wishes to use numerous network services, they must register their identity and password in these servers. In response to this problem, numerous related studies recently have been proposed. These authentication schemes enable remote users to obtain service from multiple servers without separately registering with each server. This study proposes an alternative multi-server authentication scheme using smart cards. The proposed scheme is based on the nonce, uses one-way hash function, and does not need to store any verification table in the server and registration center. The proposed scheme can withstand seven well known network security attacks. 
27|5-6|http://www.sciencedirect.com/science/journal/01674048/27/5-6|(iii) Contents|
27|5-6||From the Editor-in-Chief|
27|5-6||Information security requirements â Interpreting the legal aspects|With information security being the focal point of business in the media and in legislatures around the world, organisations face complex requirements to comply with security and privacy standards and regulations. The escalating magnitude of national and international laws and regulations, such as Sarbanes–Oxley, Gramm–Leach–Bliley and Basel II, caused organisations to become increasingly aware of the importance of legal compliance and the obligations that arise from it. The challenge of meeting these obligations has become a complex web of requirements that grows exponentially as organisations cross international boundaries. This paper attempts to provide an interpretation of the legal aspects, as a starting point for clarifying compliance issues, as referred to by ISO/IEC 27002 (ISO/IEC 27002, 2005; previously known as ISO/IEC 17799, 2005). ISO/IEC 27002 further mentions three sources from which information security requirements can be derived, of which one will be focused on within this paper, namely the legal requirements. The interpretation of the legal aspects thus forms the foundation for motivating a proposed model for determining legal requirements, which in turn, indicates relevant information security controls from the list provided in ISO/IEC 27002, to satisfy the identified legal requirements. 
27|5-6||A SIP-oriented SPIT Management Framework|Voice over IP (VoIP) telephony is increasingly gaining popularity among home and business users alike, as a viable alternative to traditional telephony, and is expected to achieve a significant market share in the near future. When this happens, it is also expected that several new threats exploiting the Internet vulnerabilities will appear. One of these is the Spam over Internet Telephony (SPIT). This paper examines in detail the SPIT attack, provides a review and assessment of previously proposed SPIT management techniques and proposes the use of an attack-oriented methodology for thwarting the SPIT threat. This results in a generic SPIT management framework, which combines the strengths of existing solutions, while alleviating their insufficiencies. 
27|5-6||SMSSec: An end-to-end protocol for secure SMS|Short Message Service is usually used to transport unclassified information, but with the rise of mobile commerce it has become an integral tool for conducting business. However, SMS does not guarantee confidentiality and integrity of the message content. This paper proposes a protocol called SMSSec that can be used to secure an SMS communication sent by Java's Wireless Messaging API. The physical limitations of the intended devices such as mobile phones, made it necessary to develop a protocol which would make minimal use of computing resources. SMSSec has a two-phase protocol with the first handshake using asymmetric cryptography which occurs only once, and a more efficient symmetric nth handshake which is used more dominantly. What distinguishes this work from conventional protocols is the ability to perform the secure transmission with limited size messages. Performance analysis showed that the encryption speed on the mobile device is faster than the duration of the transmission. To achieve security in the mobile enterprise environment, this is deemed a very acceptable overhead. Furthermore, a simple mechanism handles fault tolerance without additional overhead is proposed. 
27|5-6||Critical study of neural networks in detecting intrusions|This paper presents a critical study about the use of some neural networks (NNs) to detect and classify intrusions. The aim of our research is to determine which NN classifies well the attacks and leads to the higher detection rate of each attack. This study focused on two classification types of records: a single class (normal, or attack), and a multiclass, where the category of attack is also detected by the NN. Five different types of NNs were tested: multilayer perceptron (MLP), generalized feed forward (GFF), radial basis function (RBF), self-organizing feature map (SOFM), and principal component analysis (PCA) NN. A KDD data subset containing 18,285 records manually chosen was trained in order to be tested on the KDD testing set. Our simulations show that the GFF NN leads to the best confusion matrix in the multiclass case. For the same case, the RBF performs the higher detection rate of the DoS attack category. In the single class case, the PCA NN performs the higher detection rate. 
27|5-6||Application-based anomaly intrusion detection with dynamic information flow analysis|This paper presents a new approach to detecting software security failures, whose primary goal is facilitating identification and repair of security vulnerabilities rather than permitting online response to attacks. The approach is based on online capture of executions and offline execution replay, profiling, and analysis. It employs fine-grained dynamic information flow analysis in conjunction with anomaly detection. This approach, which we call information flow anomaly detection, is capable of detecting a variety of security failures, including both ones that involve violations of confidentiality or integrity requirements and ones that do not. A prototype tool called DynFlow implementing the approach has been developed for use with Java byte code programs. To illustrate the potential of the approach, it is applied to detect security failures of four open source systems. Also, its effectiveness is compared to the effectiveness of an approach to anomaly detection that is based on analyzing method call stacks. 
27|5-6||Building network attack graph for alert causal correlation|Most network administrators have got unpleasant experience of being overwhelmed by tremendous unstructured network security alerts produced by heterogeneous devices. To date, various approaches have been proposed to correlate security alerts, including the adoption of attack graphs to clarify their causal relationship. However, there still lacks an efficient and operational method to generate attack graphs tailored to alert causal correlation.In this paper, we propose a kind of “one-step worst” attack graph which can be built in polynomial time using an intuitive object-oriented method. Based on the graph, a principle is given out to correlate security alerts into scenarios. To prove its feasibility, we implemented a prototype system which can efficiently divide real-time alert streams into plausible attack scenarios. 
27|5-6||Enforcing memory policy specifications in reconfigurable hardware|While general-purpose processor based systems are built to enforce memory protection to prevent the unintended sharing of data between processes, current systems built around reconfigurable hardware typically offer no such protection. Several reconfigurable cores are often integrated onto a single chip where they share external resources such as memory. While this enables small form factor and low cost designs, it opens up the opportunity for modules to intercept or even interfere with the operation of one another. We investigate the design and synthesis of an FPGA memory protection mechanism capable of enforcing access control policies and a methodology for translating formal policy descriptions into FPGA enforcement mechanisms. The efficiency of our access language design flow is evaluated in terms of area and cycle time across a variety of security scenarios. We also describe a technique for ensuring that the internal state of the reference monitor cannot be used as a covert storage channel. 
27|5-6||Practical anonymous user authentication scheme with security proof|An authenticated key distribution scheme preserving user anonymity is important to those e-commerce applications where user anonymity is required or desirable. However, the previous works on this issue have security flaws. This paper shows the security weaknesses of a recently published work, and then proposes our new scheme, which not only overcomes the weaknesses but also improves the computational performance. The security of the proposed scheme is rigorously examined in a modified Bellare–Rogaway model. 
27|5-6||Information security management: An information security retrieval and awareness model for industry|The purpose of this paper is to present a conceptual view of an Information Security Retrieval and Awareness (ISRA) model that can be used by industry to enhance information security awareness among employees. A common body of knowledge for information security that is suited to industry and that forms the basis of this model is accordingly proposed. This common body of knowledge will ensure that the technical information security issues do not overshadow the non-technical human-related information security issues. The proposed common body of knowledge also focuses on both professionals and low-level users of information. The ISRA model proposed in this paper consists of three parts, namely the ISRA dimensions (non-technical information security issues, IT authority levels and information security documents), information security retrieval and awareness, and measuring and monitoring. The model specifically focuses on the non-technical information security that forms part of the proposed common body of knowledge because these issues have, in comparison with the technical information security issues, always been neglected. 
27|7-8|http://www.sciencedirect.com/science/journal/01674048/27/7-8|(iii) Contents|
27|7-8||Editorial|
27|7-8||Security beliefs and barriers for novice Internet users|End-users are now recognized as being at increased risk in online scenarios, with a range of threats that seek to specifically target them and exploit their systems. Novice users are particularly likely to face difficulties in this context, as their unfamiliarity with the technology can limit their ability to recognize the threats and understand the required protection. This paper presents the results from a qualitative study, arising from detailed interviews conducted with 20 novice users in order to assess their views and experiences with Internet security. The findings reveal a general awareness of the existence of threats, but less familiarity with the appropriate safeguards beyond a very basic level. Although users generally recognize that they have a responsibility for their own protection, they often appear unconcerned about the potential impacts of the problems. In other cases, they felt unable to address their concerns as a result of their lack of technical knowledge or obstacles posed by security tools. 
27|7-8||Information security awareness in higher education: An exploratory study|The research explores factors that affect information security awareness of staff, including information systems decision makers, in higher education within the context of a developing country, namely the UAE. An interpretive case-study approach is employed using multiple data gathering methods. The research reveals that factors such as conscientiousness, cultural assumptions and beliefs, and social conditions affect university staff behaviour and attitude towards work, in general, and information security awareness, in particular. A number of recommendations are provided to initiate and promote IS security awareness in the studied environment. 
27|7-8||Consensus ranking â An ICT security awareness case study|There are many disciplines where the problem of consensus ranking plays a vital role. Decision-makers are frequently asked to express their preferences for a group of objects, e.g. new projects, new products, candidates in an election, etc. The basic problem then becomes one of combining the individual rankings into a group choice or consensus ranking. The objective of this paper is to report on the application of two management science methodologies to the problem of identifying the most important areas to be included in an Information Communications Technology (ICT) security awareness program. The first methodology is based on the concept of minimizing the distance (disagreement) between individual rankings, while the second one employs a heuristic approach. A real-world case study from the mining industry is presented to illustrate the methods. 
27|7-8||Criteria to evaluate Automated Personal Identification Mechanisms|The consequences of digital identity compromises suggest that selected Automated Personal Identification Mechanisms, which enable computer systems to identify individuals, may be unsuitable in some contexts. Currently, there is no commonly agreed set of factors upon which to base an evaluation, regardless of purpose or requirements.We establish over 200 evaluation criteria to aid decision on the selection of the most appropriate mechanism for a given context. We consider that the suitability of these mechanisms should be ascertained from a broad approach. Our criteria are designed to expose strategic issues and risk management aspects that influence organisations' objectives and policies for introducing these mechanisms. Additionally, criteria are developed to acquire functional and performance requirements for the intended user community. Our criteria are also formulated to help describe the characteristics of contesting solutions. These qualities range from technological efficiencies to usability effectiveness. Each mechanism may then be assessed for its suitability against the context's risks, issues and operational requirements within an evaluation framework capable of accommodating diverse perspectives and multiple objectives. 
27|7-8||An ontology-based policy for deploying secure SIP-based VoIP services|Voice services over Internet Protocol (VoIP) are nowadays much promoted by telecommunication and Internet service providers. However, the utilization of open networks, like the Internet, raises several security issues that must be accounted for. On top of that, there are new sophisticated attacks against VoIP infrastructures that capitalize on vulnerabilities of the protocols employed for the establishment of a VoIP session (for example the Session Initiation Protocol – SIP).This paper provides a categorization of potential attacks against VoIP services, followed by specific security recommendations and guidelines for protecting the underlying infrastructure from these attacks and thus ensuring the provision of robust and secure services. In order to utilize (share) the aforementioned security guidelines and recommendations into different domains, it is necessary to have them represented in some formal way. To this end, ontologies have been used for representing the proposed guidelines and recommendations in the form of a unified security policy for VoIP infrastructures. This ontology-based policy has been then transformed to a First Order Logic (FOL) formal representation.The proposed ontology-based security policy can be applied in a real VoIP environment for detecting attacks against an SIP-based service, but it can be also utilized for security testing purposes and vulnerabilities identification.The work presented in this paper has been focused to the SIP protocol. However, generalization to other signaling protocols is possible. 
27|7-8||Secure log management for privacy assurance in electronic communications|In this paper we examine logging security in the environment of electronic communication providers. We review existing security threat models for system logging and we extend these to a new security model especially suited for communication network providers, which also considers internal modification attacks. We also propose a framework for secure log management in public communication networks as well as an implementation design, in order to provide traceability under the extended security model. A key role to the proposed framework is given to an independent Regulatory Authority, which is responsible to maintain log integrity proofs in a remote environment and verify the integrity of the provider's log files during security audits. 
27|7-8||An optimistic fair exchange protocol based on signature policies|The growth of the e-commerce has allowed companies and individuals to sell and purchase almost any kind of product and service through the Internet. However, during the purchase transaction there is a moment during which the seller has sensitive information from the buyer, typically his/her credit card information, while the buyer has nothing from the seller. This situation clearly places the buyer at disadvantage and is, together with fear of fraud, one of the reasons of the lack of confidence in e-commerce. For resolving this situation a new fair exchange protocol based on signature policies is presented. A signature policy is a set of rules to create and validate electronic signatures, under which an electronic signature can be determined to be valid in a particular transaction context. Due to the signature policy-based design, the proposed protocol allows the buyer to decide if trust or not in the rules that will manage the transaction, increasing the user's confidence in e-commerce. Security, fairness and timeliness characteristics of the protocol are evaluated. Implementation guidelines are also provided taking into consideration latest security standards. 
27|7-8||Automated containment of rootkits attacks|Rootkit attacks are a serious threat to computer systems. Packaged with other malwares such as worms, viruses and spyware, rootkits pose a more potent threat than ever before by allowing malware to evade detection. In the absence of appropriate tools to counter such attacks, compromised machines stay undetected for extended periods of time. Leveraging virtual machine technology, we propose a solution for real-time automated detection and containment of rootkit attacks. We have developed a prototype using VMware Workstation to illustrate the solution. Our analysis and experimental results indicate that this approach can very successfully detect and contain the effects of a large percentage of rootkits found for Linux today. We also demonstrate with an example, how this approach is particularly effective against malwares that use rootkits to hide. 
27|7-8||Evaluation of a low-rate DoS attack against application servers|In the network security field there is a need to identify new movements and trends that attackers might adopt, in order to anticipate their attempts with defense and mitigation techniques. The present study explores new approaches that attackers could use in order to make denial of service attacks against application servers. We show that it is possible to launch such attacks by using low-rate traffic directed against servers, and apply the proposed techniques to defeat a persistent HTTP server. The low-rate feature is highly beneficial to the attacker for two main reasons: firstly, because the resources needed to carry out the attack are considerably reduced, easing its execution. Secondly, the attack is more easily hidden to security mechanisms that rely on the detection of high-rate traffic. In this paper, a mechanism that allows the attacker to control the attack load in order to bypass an IDS is contributed. We present the fundamentals of the attack, describing its strategy and design issues. The performance is also evaluated in both simulated and real environments. Finally, a study of possible improvement techniques to be used by the attackers is contributed. 
27|7-8||Dynamic models for computer viruses|Computer viruses are an important risk to computational systems endangering either corporations of all sizes or personal computers used for domestic applications. Here, classical epidemiological models for disease propagation are adapted to computer networks and, by using simple systems identification techniques a model called SAIC (Susceptible, Antidotal, Infectious, Contaminated) is developed. Real data about computer viruses are used to validate the model. 
28|1-2|http://www.sciencedirect.com/science/journal/01674048/28/1-2|Contents|
28|1-2||Editorial|
28|1-2||Building access control models with attribute exploration|The use of lattice-based access control models has been somewhat restricted by their complexity. We argue that attribute exploration from formal concept analysis can help create lattice models of manageable size, while making it possible for the system designer to better understand dependencies between different security categories in the domain and, thus, providing certain guarantees for the relevance of the constructed model to a particular application. In this paper, we introduce the method through an example. 
28|1-2||A distributed requirements management framework for legal compliance and accountability|Increasingly, new regulations are governing organizations and their information systems. Individuals responsible for ensuring legal compliance and accountability currently lack sufficient guidance and support to manage their legal obligations within relevant information systems. While software controls provide assurances that business processes adhere to specific requirements, such as those derived from government regulations, there is little support to manage these requirements and their relationships to various policies and regulations. We propose a requirements management framework that enables executives, business managers, software developers and auditors to distribute legal obligations across business units and/or personnel with different roles and technical capabilities. This framework improves accountability by integrating traceability throughout the policy and requirements lifecycle. We illustrate the framework within the context of a concrete healthcare scenario in which obligations incurred from the Health Insurance Portability and Accountability Act (HIPAA) are delegated and refined into software requirements. Additionally, we show how auditing mechanisms can be integrated into the framework and how auditors can certify that specific chains of delegation and refinement decisions comply with government regulations. 
28|1-2||Anomaly-based network intrusion detection: Techniques, systems and challenges|The Internet and computer networks are exposed to an increasing number of security threats. With new types of attacks appearing continually, developing flexible and adaptive security oriented approaches is a severe challenge. In this context, anomaly-based network intrusion detection techniques are a valuable technology to protect target systems and networks against malicious activities. However, despite the variety of such methods described in the literature in recent years, security tools incorporating anomaly detection functionalities are just starting to appear, and several important problems remain to be solved. This paper begins with a review of the most well-known anomaly-based intrusion detection techniques. Then, available platforms, systems under development and research projects in the area are presented. Finally, we outline the main challenges to be dealt with for the wide scale deployment of anomaly-based intrusion detectors, with special emphasis on assessment issues. 
28|1-2||Huffman-based join-exit-tree scheme for contributory key management|Time efficiency in key establishment and rekeying is one of the major problems contributory key management schemes strive to address. Some schemes have been put forward to improve time efficiencies of key establishment and key update, yet they did not consider the scenario where users have varying costs and capabilities. Although conference key tree based on Huffman coding has been proposed to obtain minimum average cost on key establishment considering users' differences, it did not give the efficient key updating algorithm. We propose a Huffman-based join-exit-tree (HJET) scheme to minimizing the average key establishment time and reducing the key rekeying time for join/departure events. HJET scheme separates users into subgroups according to users' locations, designs the key tree of each subgroup using Huffman coding, and lets the combined weights locate in a higher place of the Huffman tree to minimize the key establishment time. To reduce the key rekeying cost, join tree and exit tree are adopted and served as the temporary buffers for joining and leaving users. Performance analysis and simulation results demonstrate that HJET is efficient in key establishment and update, and achieves the asymptotic time cost of O(1) for join event and nearly O(1)for leave events when group dynamics are known a priori. 
28|1-2||RAID-RMS: A fault tolerant stripped mirroring RAID architecture for distributed systems|Disk arrays, or RAIDs, have become the solution to increase the capacity, bandwidth and reliability of most storage systems. In spite of its high redundancy level, disk mirroring is a popular RAID paradigm, because replicating data also doubles the bandwidth available for processing read requests, improves the reliability and achieves fault tolerance. In this paper, we present a new RAID architecture called RAID-RMS in which a special hybrid mechanism is used to map the data blocks to the cluster. The main idea behind the proposed algorithm is to combine the data block striping and disk mirroring technique with a data block rotation. The resulting architecture improves the parallelism reliability and efficiency of the RAID array. We show that the proposed architecture is able to serve many more disk requests compared to the other mirroring-based architectures. We also argue that a more balanced disk load is attained by the given architecture, especially when there are some disk failures. 
28|1-2||User perceptions of security, convenience and usability for ebanking authentication tokens|This research compared three different two-factor methods of eBanking authentication. Three devices employing incremental security layers in the generation of one time passcodes (OTPs) were compared in a repeated-measures, controlled experiment with 50 eBanking customers. Attitudes towards usability and usage logs were taken for each experience. Comparisons of the devices in terms of overall quality, security and convenience as perceived by participants were also recorded. There were significant differences between all three methods in terms of usability measures, perceived quality, convenience and security ratings – with the perceived security ratings following a reverse order to the other measures. Almost two thirds of the participant sample chose the device they perceived the least secure as their preference. Participants were asked to use their preferred method again and tended to find their chosen device more usable. This research illustrates the usability-security trade off, where convenience, quality and usability are sacrificed when increasing layers of security are required. In their preferences, customers were driven by their attitudes towards usability and convenience rather than their perceptions of security. 
28|1-2||Why Johnny can't surf (safely)? Attacks and defenses for web users|In their seminal article “Why Johnny Can't Encrypt” [Whitten A, Tygar JD. Why Johnny can't encrypt: a usability case study of PGP 5.0. In: Proceedings of the eighth USENIX security symposium; August 1999.], Whitten and Tygar showed that usability weaknesses of encryption software may result in failure to protect users, in spite of good cryptography. A similar situation happens, on a huge scale, on the Web: the widely deployed SSL/TLS protocols provide good cryptography, yet there is a growing amount of successful attacks on web users, causing massive damages. In this article, we focus on password theft via fake websites, to which we refer as phishing. We believe that phishing is currently the most severe threat facing web users.We begin with a brief review of SSL/TLS. Many sensitive sites do not use SSL/TLS, or use it incorrectly (e.g. to encrypt password, filled into an unprotected login form); we explain why.Even if sites use SSL/TLS (correctly), this may not be enough to prevent phishing – at least, using the basic security and identification indicators of most browsers (URL, padlock and HTTPS). We discuss basic and advanced indicators, and their usability problems. We review recent usability studies, whose results are rather alarming, and put in question the ability of users to avoid phishing sites based on security and identification indicators. 
28|1-2||Recognition of electro-magnetic leakage information from computer radiation with SVM|This paper focuses on the far-field reception of electromagnetic (EM) radiation and the recognition of letters recovered from the EM leakage. EM radiation captured by a wideband antenna is strengthened by a pre-manipulation system with amplifiers and filters, and the useful information is extracted. After being recovered from the EM radiation with signal processing method, the text image is further recognized by support vector machine (SVM) algorithm. Here, a two-layered SVM network with 60 SVMs is constructed to recognize the letters. In the process, the text image is cut apart into single letters to meet the input requirement of letter-based SVMs. To handle those exceptions of stroke connections, an interactive method and an automatic method are proposed. The received text image is usually obtained in low resolution with characteristics of large stroke distortion, font variation and variable size. In our two applications, however, the recognition accuracy reached 99.2% for the larger font size texts and 96.4% for the smaller size texts. From this, we may draw a conclusion that the proposed SVMs network works well in recognizing textual information and emphasize the potential risk of information leakage for computer system. 
28|1-2||Securing communication using function extraction technology for malicious code behavior analysis|Since computer hardware and Internet is growing so fast today, security threats of malicious executable code are getting more serious. Basically, malicious executable codes are categorized into three kinds – virus, Trojan Horse, and worm. Current anti-virus products cannot detect all the malicious codes, especially for those unseen, polymorphism malicious executable codes. The newly developed virus will create the damages before it has been found and updated in database. The basic idea of the proposed system is, it will analyze the behavior of the malicious codes and based on the behavior signature of the malicious code content filtering mechanism will be used to filter out contents, so that, the system will be secured from the future communication processes. The behavior of the code is analyzed using the function extraction technology. The function extraction technology will replace the function codes into algebraic expressions. Based on the behavior of the malicious codes, it will be categorized into different kinds of malicious codes. The detected malicious code will be prevented from execution. Based on the type of malicious code, appropriate security mechanism will be used for further communication. 
28|1-2||Keystroke dynamics-based authentication for mobile devices|Recently, mobile devices are used in financial applications such as banking and stock trading. However, unlike desktops and notebook computers, a 4-digit personal identification number (PIN) is often adopted as the only security mechanism for mobile devices. Because of their limited length, PINs are vulnerable to shoulder surfing and systematic trial-and-error attacks. This paper reports the effectiveness of user authentication using keystroke dynamics-based authentication (KDA) on mobile devices. We found that a KDA system can be effective for mobile devices in terms of authentication accuracy. Use of artificial rhythms leads to even better authentication performance. 
28|1-2||An oblique-matrix technique for data integrity assurance|Data integrity is insuring that the data retrieved is the same as the data stored or transmitted. It is an important aspect of storage security and reliability which are prerequisite for most computer system applications. This paper proposes a new technique for improving the detection of data integrity violations. The method is set up the check determinant approach. Each block of data is arranged in a matrix form, and then into its corresponding oblique matrix. Because of the new arrangement in the block elements through the oblique matrix, a powerful error detection mechanism is obtained. Simulation results show that the new method outperforms the traditional techniques. 
28|1-2||Digital multisignature on the generalized conic curve over Zn|This paper proposes a digital multi-signature scheme on the generalized conic curve over Zn. The generalized conic curve Rn (a, b, c) over residue class ring Zn and its group structure are presented. The paper gives the representation of the order and base point on Rn (a, b, c), and introduces the representation of operation by parameters to simplify its calculation. The scheme security is based on the factoring and discrete logarithms simultaneously. This scheme is easy to accomplish for convenient embedding plaintext, computing element order and points in the conic curve. In addition, it can speed up the inverse operation and resist Pohlig–Hellman's attack and Wiener's attack. 
28|1-2||Guide for Authors.|
28|3-4|http://www.sciencedirect.com/science/journal/01674048/28/3-4|Contents|
28|3-4||Understanding the limitations of S/MIME digital signatures for e-mails: A GUI based approach|S/MIME (Secure/Multipurpose Internet Mail Extensions) is a well-known standard for secure e-mail exchange. S/MIME builds its identity management on e-mail addresses, rather than real names. This fact may sometimes cause sending a signed e-mail with a bogus name on it. Moreover, header information of a signed e-mail message, such as subject and name, can be altered without affecting the verifiability of the signature. This paper spots the details of such problems of S/MIME and discusses some solutions from both developer and user points of view. Moreover, GUI considerations about these problems are also analyzed in this paper. An ideal GUI is modeled and developed. 
28|3-4||SDriver: Location-specific signatures prevent SQL injection attacks|SQL injection attacks involve the construction of application input data that will result in the execution of malicious SQL statements. Many web applications are prone to SQL injection attacks. This paper proposes a novel methodology of preventing this kind of attacks by placing a secure database driver between the application and its underlying relational database management system. To detect an attack, the driver uses stripped-down SQL queries and stack traces to create SQL statement signatures that are then used to distinguish between injected and legitimate queries. The driver depends neither on the application nor on the RDBMS and can be easily retrofitted to any system. We have developed a tool, SDriver, that implements our technique and used it on several web applications with positive results. 
28|3-4||From desktop to mobile: Examining the security experience|The use of mobile devices is becoming more commonplace, with data regularly able to make the transition from desktop systems to pocket and handheld devices such as smartphones and PDAs. However, although these devices may consequently contain or manipulate the same data, their security capabilities are not as mature as those offered in fully-fledged desktop operating systems. This paper explores the availability of security mechanisms from the perspective of a user who is security-aware in the desktop environment and wishes to consider utilising similar protection in a mobile context. Key issues of concern are whether analogous functionality can be found, and if so, whether it is offered in a manner that parallels the desktop experience (i.e. to ensure understanding and usability). The discussion is supported by an examination of the Windows XP and Windows Mobile environments, with specific consideration given to the facilities available for user authentication, secure connectivity, and content protection on the devices. It is concluded that although security aspects receive some attention, the provided means generally suffer from usability issues or limitations that would prevent a user from achieving the same level of protection that they might enjoy in the desktop environment. 
28|3-4||An ID-based remote mutual authentication with key agreement scheme for mobile devices on elliptic curve cryptosystem|Recently, remote user authentication schemes are implemented on elliptic curve cryptosystem (ECC) to reduce the computation loads for mobile devices. However, most remote user authentication schemes on ECC are based on public-key cryptosystem, in which the public key in the system requires the associated certificate to prove its validity. Thus, the user needs to perform additional computations to verify the certificate in these schemes. In addition, we find these schemes do not provide mutual authentication or a session key agreement between the user and the remote server. Therefore, we propose an ID-based remote mutual authentication with key agreement scheme on ECC in this paper. Based upon the ID-based concept, the proposed scheme does not require public keys for users such that the additional computations for certificates can be reduced. Moreover, the proposed scheme not only provides mutual authentication but also supports a session key agreement between the user and the server. Compared with the related works, the proposed scheme is more efficient and practical for mobile devices. 
28|3-4||Detecting rogue access points using client-side bottleneck bandwidth analysis|A rogue access point (AP) is an unauthorized AP plugged into a network. This poses a serious security threat. To detect an AP, a network manager traditionally takes the electric wave sensor across an entire protected place. This task is very labor-intensive and inefficient. This study presents a new AP detection method without extra hardware or hard work. This new method determines whether the network packets of an IP address are routed from APs, according to client-side bottleneck bandwidth. The network manager can perform his job from his office by monitoring the packets passing through the core switch. The accuracies remain above 99% when the parameter, sliding window size, of the proposed algorithm is larger than 20, according to experimental results. The proposed method effectively reduces the network manager's workload, and increases network security. 
28|3-4||An incremental frequent structure mining framework for real-time alert correlation|With the large volume of alerts produced by low-level detectors, management of intrusion alerts is becoming more challenging. Manual analysis of a large number of raw alerts is both time consuming and labor intensive. Alert Correlation addresses this issue by finding similarity and causality relationships between raw alerts to provide a condensed, yet more meaningful view of the network from the intrusion standpoint. While some efforts have been made in the literature by researchers to find the relationships between alerts automatically, not much attention has been given to the issue of real-time correlation of alerts. Previous learning-based approaches either fail to cope with a large number of generated alerts in a large-scale network or do not address the problem of concept drift directly.In this paper, we propose a framework for real-time alert correlation which incorporates novel techniques for aggregating alerts into structured patterns and incremental mining of frequent structured patterns. Our approach to aggregation provides a reduced view of developed patterns of alerts. At the core of the proposed framework is a new algorithm (FSP_Growth) for mining frequent patterns of alerts considering their structures. In the proposed framework, time-sensitive statistical relationships between alerts are maintained in an efficient data structure and are updated incrementally to reflect the latest trends of patterns.The results of experiments conducted with the DARPA 2000 dataset as well as artificial data clearly demonstrate the efficiency of proposed techniques. A promising reduction ratio of 96% is achieved on the DARPA 2000 dataset. The running time of the FSP_Growth algorithm scales linearly with the size of artificial datasets. Moreover, testing the proposed framework with alert logs of a real-world network shows its ability to extract interesting patterns among the alerts. The ability to answer useful time-sensitive queries regarding pattern co-occurrences is another advantage of the proposed method compared to other approaches. 
28|3-4||Preventing massive automated access to web resources|Automated web tools are used to achieve a wide range of different tasks, some of which are legal activities, whilst others are considered attacks to the security and data integrity of online services. Effective solutions to counter the threat represented by such programs are therefore required. In this work, we present MosaHIP, a Mosaic-based Human Interactive Proof (HIP), which is able to prevent massive automated access to web resources. Properties of the proposed solution grant an improved security over usual text-based and image-based HIPs, whereas the user-friendliness of the system alleviates the user from the discomfort of typing any text before accessing to a web content. Experimental evidence of the effectiveness of the proposed technique is given by submitting our system to a series of tests simulating possible bot attacks. 
28|3-4||Information security: The moving target|Information security has evolved from addressing minor and harmless security breaches to managing those with a huge impact on organisations' economic growth. This paper investigates the evolution of information security; where it came from, where it is today and the direction in which it is moving. It is argued that information security is not about looking at the past in anger of an attack once faced; neither is it about looking at the present in fear of being attacked; nor about looking at the future with uncertainty about what might befall us. The message is that organisations and individuals must be alert at all times. Research conducted for this paper explored literature on past security issues to set the scene. This is followed by the assessment and analysis of information security publications in conjunction with surveys conducted in industry. Results obtained are compared and analysed, enabling the development of a comprehensive view regarding the current status of the information security landscape. Furthermore, this paper also highlights critical information security issues that are being overlooked or not being addressed by research efforts currently undertaken. New research efforts are required that minimise the gap between regulatory issues and technical implementations. 
28|3-4||Reliable and fully distributed trust model for mobile ad hoc networks|A mobile ad hoc network (MANET) is a wireless communication network which does not rely on a pre-existing infrastructure or any centralized management. Securing the exchanges in MANETs is compulsory to guarantee a widespread development of services for this kind of networks. The deployment of any security policy requires the definition of a trust model that defines who trusts who and how. Our work aims to provide a fully distributed trust model for mobile ad hoc networks. In this paper, we propose a fully distributed public key certificate management system based on trust graphs and threshold cryptography. It permits users to issue public key certificates, and to perform authentication via certificates' chains without any centralized management or trusted authorities. Moreover, thanks to the use of threshold cryptography; our system resists against false public keys certification. We perform an overall evaluation of our proposed approach through simulations. The results indicate out performance of our approach while providing effective security. 
28|3-4||How significant is human error as a cause of privacy breaches? An empirical study and a framework for error management|Privacy breaches and their regulatory implications have attracted corporate attention in recent times. An often overlooked cause of privacy breaches is human error. In this study, we first apply a model based on the widely accepted GEMS error typology to analyze publicly reported privacy breach incidents within the U.S. Then, based on an examination of the causes of the reported privacy breach incidents, we propose a defense-in-depth solution strategy founded on error avoidance, error interception, and error correction. Finally, we illustrate the application of the proposed strategy to managing human error in the case of the two leading causes of privacy breach incidents. This study finds that mistakes in the information processing stage constitute the most cases of human error-related privacy breach incidents, clearly highlighting the need for effective policies and their enforcement in organizations. 
28|3-4||Design, implementation and analysis of hardware efficient stream ciphers using LFSR based hash functions|Design and implementation of hardware efficient stream ciphers using hash functions and analysis of their periodicity and security are presented in this paper. The hash generation circuits used for the design and development of stream ciphers are low power, low hardware complexity Linear Feedback Shift Register (LFSR) based circuits. One stream cipher design uses LFSR based Toeplitz hash generation circuit together with LFSR keystream generator circuit, while the other design combines LFSR based filter generator circuit with LFSR based polynomial modular division circuit. Both designs possess good security and periodicity properties for the keystreams generated. The developed circuits can compete with the most popular classic LFSR based stream ciphers in hardware complexity at the same time providing additional advantage that the same circuit can be used for hash generation. 
28|3-4||IFIP SEC 2009 Advert|
28|5|http://www.sciencedirect.com/science/journal/01674048/28/5|Contents|
28|5||Editorial|
28|5||Multi-dimensional credentialing using veiled certificates: Protecting privacy in the face of regulatory reporting requirements|Traditional certificates are designed to establish and document characteristics belonging to a specific individual, be it an identification number (i.e., social security number, driver's license number), a level of achievement (i.e., college degree, license to practice a profession), or membership status (i.e., company ID, trade union card). The digital certificate extends this concept into the electronic world, identifying and linking the certificate holder to a public encryption key that is subsequently used as a means of identification. Current identity certificates provide unique identification and tracking, however it is exactly these characteristics that have led to concerns over identity theft and privacy of personal information. The veiled certificate introduced in this paper addresses these issues by providing means of linking certificates from multiple certifying authorities while masking the user's identity from non-authorized individuals and satisfying the regulatory need of unique, explicit identification. With the ability to be implemented within existing X.509 standards, veiled certification extends traditional digital certificates with features useful in combating identity theft and invasion of privacy. 
28|5||Towards secure dynamic collaborations with group-based RBAC model|Role-Based Access Control (RBAC) has become a popular technique for security purposes with increasing accessibility of information and data, especially in large-scale enterprise environments. However, authorization management in dynamic and ad-hoc collaborations between different groups or domains in these environments is still an unresolved problem. Traditional RBAC models cannot solve this problem because they cannot support security policy composition from different groups, and lack efficient administrative models for dynamic collaborations. In this paper, we propose a group-based RBAC model (GB-RBAC) for secure collaborations which is based on RBAC96 and extended with group concept to capture dynamic users and permissions. We propose a decentralized security administrative model for GB-RBAC to address the management issues of RBAC in collaborations. As a unique property, our model supports two levels of authorization management: global or system level management by system administrators and local or group level management by group administrators. In this way, our model implements the principles of management autonomy and separation of duty (SoD) in security administrations. We apply our model for authorization management in collaborations by introducing the concept of virtual group. A virtual group is built for a collaboration between multi-groups, where all members build trust relation within the group and are authorized to join and perform operations for the collaborative work. Compared with existing work, our model supports dynamic and ad-hoc collaborations in large-scale systems with the properties of controllable, decentralized, and fine-grained security management. 
28|5||Fast detection and visualization of network attacks on parallel coordinates|This article presents what we call the parallel coordinate attack visualization (PCAV) for detecting unknown large-scale Internet attacks including Internet worms, DDoS attacks and network scanning activities. PCAV displays network traffic on the plane of parallel coordinates using the flow information such as the source IP address, destination IP address, destination port and the average packet length in a flow. The parameters are used to draw each flow as a connected line on the plane, where a group of polygonal lines form a particular shape in case of attack. From the observation that each attack type of significance forms a unique pattern, we develop nine signatures and their detection mechanism based on an efficient hashing algorithm. Using the graphical signatures, PCAV can quickly detect new attacks and enable network administrators to intuitively recognize and respond to the attacks. Compared with existing visualization works, PCAV can handle hyper-dimensions, i.e., can visualize more than 3 parameters if necessary, which significantly reduces false positives. As a consequence, Internet worms are more precisely detectable by machine and more easily recognizable by human. Another strength of PCAV is handling flows instead of packets. Per-flow visualization greatly reduces the processing time and further provides compatibility with legacy routers which export flow information, e.g., as NetFlow does in Cisco routers. We demonstrate the effectiveness of PCAV using real-life Internet traffic traces. The PCAV program is publicly available. 
28|5||Secure multiparty payment with an intermediary entity|During the last years, many secure electronic payment solutions have been proposed but most of them are focused on the traditional two-party business models with a customer and just one provider. In this paper we propose a new secure multiparty payment model with an intermediary, who helps the customer to make purchases and payments with many providers simultaneously. In our secure infrastructure it is assumed that the intermediary does not need to be a trusted entity (it does not need to be a TTP). One of the most important issues of this contribution is the Intermediary-3D: we propose a simple adaptation of the 3D Secure™ payment protocol in order to maintain the 3D Secure™ working modes but offering the possibility of making multiple secure payments through an intermediary. By means of this slight adaptation, our model avoids the provider's enrolment process in a centralized system (e.g. Visa Domain) and it makes more robust and secure the multipayment scenarios, as well as, it favors its deployment in global networks like Internet. 
28|5||A real-time network intrusion detection system for large-scale attacks based on an incremental mining approach|None of the previously proposed Network Intrusion Detection Systems (NIDSs), which are subject to fuzzy association rules, can meet real-time requirements because they all apply static mining approaches. This study proposed a real-time NIDS with incremental mining for fuzzy association rules. By consistently comparing the two rule sets, one mined from online packets and the other mined from training attack-free packets, the proposed system can render a decision every 2 seconds. Thus, compared with traditional static mining approaches, the proposed system can greatly improve efficiency from offline detection to real-time online detection. Since the proposed system derives features from packet headers only, like the previous works based on fuzzy association rules, large-scale attack types are focused. Many DoS attacks were experimented in this study. Experiments were performed to demonstrate the excellent effectiveness and efficiency of the proposed system. The system may not cause false alarms because normal programs supposedly would not generate enough mal-formatted packets, or packets that violate normal network protocols. 
28|5||Specifying authentication using signal events in CSP|The formal analysis of cryptographic protocols has developed into a comprehensive body of knowledge, building on a wide variety of formalisms and treating a diverse range of security properties, foremost of which is authentication. The formal specification of authentication has long been a subject of examination. In this paper, we discuss the use of correspondence to formally specify authentication and focus on Schneider's use of signal events in the process algebra Communicating Sequential Processes (CSP) to specify authentication. The purpose of this effort is to strengthen this formalism further. We develop a formal structure for these events and use them to specify a general authentication property. We then develop specifications for recentness and injectivity as sub-properties, and use them to refine authentication further. Finally, we use signal events to specify a range of authentication definitions and protocol examples to clarify their use and make explicit related theoretical issues. Our work is motivated by the desire to effectively analyse and express security properties in formal terms, so as to make them precise and clear. 
28|6|http://www.sciencedirect.com/science/journal/01674048/28/6|Contents|
28|6||Editorial|
28|6||A personal mobile DRM manager for smartphones|In this paper we report on our experience in building the experimental Personal Digital Rights Manager for Motorola smartphones, an industry first.Digital Rights Management allows producers or owners of digital content to control the manner in which the content is consumed. This may range from simply preventing duplication to finer access policies such as restricting who can use the content, on what devices, and for how long. In most commercial DRM systems, the average end user plays the role of content consumer, using DRM protected content made available by a service. Here we present a personal digital rights system for mobile devices where the end user has the ability to place DRM protection and controls on his or her own personal content.We designed the personal DRM system to allow users of a mobile device to transparently define controls and generate licenses on custom content and securely transfer them to other mobile devices. A user is able to define and restrict the intended audience and ensure expiration of the content as desired. Compatible devices automatically detect each other and exchange credentials. The personal DRM system on each device safely enforces the content usage rules and also handles moving licenses between devices while preventing leakage of content. We implemented a prototype of our system on Motorola E680i smartphones. 
28|6||New aspect-oriented constructs for security hardening concerns|In this paper, we present new pointcuts and primitives to Aspect-Oriented Programming (AOP) languages that are needed for systematic hardening of security concerns. The two proposed pointcuts allow to identify particular join points in a program's control-flow graph (CFG). The first one is the GAFlow, Closest Guaranteed Ancestor, which returns the closest ancestor join point to the pointcuts of interest that is on all their runtime paths. The second one is the GDFlow, Closest Guaranteed Descendant, which returns the closest child join point that can be reached by all paths starting from the pointcut of interest. The two proposed primitives are called ExportParameter and ImportParameter and are used to pass parameters between two pointcuts. They allow to analyze a program's call graph in order to determine how to change function signatures for passing the parameters associated with a given security hardening. We find these pointcuts and primitives to be necessary because they are needed to perform many security hardening practices and, to the best of our knowledge, none of the existing ones can provide their functionalities. Moreover, we show the viability and correctness of the proposed pointcuts and primitives by elaborating and implementing their algorithms and presenting the result of explanatory case studies. 
28|6||RIP â A robust IP access architecture|This research is of the view that only tightly coordinated work among security components as we know them today including firewalls, traffic analysis modules, intrusion detection systems, antivirus remediation systems, etc., is likely to take us closer to a more effective solution against some security threats. A structured Robust IP (RIP) access architecture is described and its components are analyzed through the use of a proof of concept testbed. Examples of the use of RIP and the heuristics it implements are evaluated. We also compare RIP performance to existing work. We show that there are currently some tradeoffs that need to be made between accuracy and responsiveness. We believe that this collaborative communication style between the components represents a significant step in the direction of self-defending networks and innovation in the area. 
28|6||A survey of signature based methods for financial fraud detection|Fraud detection mechanisms support the successful identification of fraudulent system transactions performed through security flaws within deployed technology frameworks while maintaining optimal levels of service delivery and a minimal numbers of false alarms. Knowledge discovery techniques have been widely applied in fraud detection for data analysis and training of supervised learning algorithms to support the extraction of fraudulent account behaviour within static data sets. Escalating costs associated with fraud however have continued to drive the migration towards increasingly proactive methods of fraud detection, to support the real-time screening of transactional data and detection of ambiguous user behaviour prior to transaction completion. This shift in data processing from post to pre data storage significantly reduces the available time within which to evaluate newly arriving system requests and produce an accurate fraud decision, demanding increasingly robust and intelligent user profiling technologies to support advanced fraud detection. This paper provides a comprehensive survey of existing research into account signatures, an innovative account profiling technology which maintains a statistical representation of normal account usage for rapid recalculation in real-time. Fraud detection architectures, processing models and applications to date are critically examined and evaluated with respect to their proactive capabilities for detection of fraud within streaming financial data. Discussion is also presented on challenges which remain within the proactive profiling of account behaviour and future research directions within the signature domain. 
28|6||A robust software watermarking for copyright protection|This paper advocates protecting software copyright through hiding watermarks in various data structures used by the code, e.g., B+-trees, R-trees, linked lists, etc. Prior proposals hide the watermarks in dummy data structures, e.g., linked lists and graphs that are created, solely for this reason, during the execution of the hosting software. This makes them vulnerable to subtractive attacks, because the attacker can remove the dummy data structures without altering the functionality or the semantic of the software program. We argue that hiding watermarks in one or more data structures that are used by the program would make the watermark more robust because disturbing the watermark would affect the semantic and the functionality of the underlying software. The challenge is that the insertion of the watermark should have a minimal effect on the operations and performance of the data structure.This paper proposes a novel method for watermarking R-tree data structure and its variants. The proposed watermarking technique does not change the values of the stored data objects. It takes advantage of the redundancy in the order of entries inside the R-tree nodes. Entries are arranged relative to a “secret” initial order, known only to the software owner, using a technique based on a numbering system that uses variable radix with factorial base. The addition of the watermark in the R-tree data structure does not affect the performance nor does it increase the size of the R-tree. The paper provides a detailed security analysis and performance evaluation to show that the embedded watermarks are robust and can withstand various types of attacks. 
28|6||Reducing threats from flawed security APIs: The banking PIN case|Despite best efforts from security API designers, flaws are often found in widely deployed security APIs. Even APIs with a formal proof of security may not guarantee absolute security when used in a real-world device or application. In parallel to spending research efforts to improve security of these APIs, we argue that it may be worthwhile to explore design criteria that would reduce the impact of an API exploit, assuming flaws cannot completely be removed from security APIs. We use such a design philosophy in dealing with PIN cracking attacks on financial PIN processing APIs; several of these attacks have been reported in the last few years, e.g., Berkman and Ostrovsky (FC 2007), Bond (CHES 2001). Our solution is called salted-PIN: a randomly generated salt value of adequate length (e.g., 128 bits) is stored on a bank card in plaintext, and in an encrypted form at a verification facility under a bank-chosen salt key. Instead of sending the regular user PIN, salted-PIN requires an ATM to generate a Transport Final PIN from a user PIN, account number, and the salt value (stored on the bank card) through, e.g., a pseudo-random function. We explore different attacks on this solution, and propose variants of salted-PIN that can protect against known attacks. Depending on the solution variation, attacks at a malicious intermediate switch now may only reveal the Transport Final PIN; both the user PIN and salt value remain beyond the reach of an attacker's switch. Salted-PIN requires modifications to service points (e.g., ATM, point-of-sale), issuer/verification facilities, and bank cards; however, changes to intermediate switches are not required. 
28|6||A formal framework for real-time information flow analysis|We view Multi-Level Secure (MLS) real-time systems as systems in which MLS real-time tasks are scheduled and execute, according to a scheduling algorithm employed by the system. From this perspective, we develop a general trace-based framework that can carry out a covert-timing channel analysis of a real-time system. In addition, we propose a set of covert-timing channel free policies: If a system satisfies one of our proposed security policies, we demonstrated that the system can achieve a certain level of real-time information flow security. Finally, we compare the relative strength of the proposed covert-timing channel free security policies and analyze whether each security policy can be regarded as a property (a set of execution sequences). 
28|6||Providing true end-to-end security in converged voice over IP infrastructures|Voice over Internet Protocol (VoIP) is the future for voice communication and, by using a unique IP infrastructure as the common transport platform, it brings invaluable benefits such as deployment cost reduction, ease of management, ubiquitous coverage and convergence of data and voice together. On the other side, VoIP introduces new security vulnerabilities, since it comes with completely different operational and security settings than the old telephone network: the physical location of clients is not fixed and great flexibility is required to provide enhanced mobile services. Furthermore, the integration with wireless LANs, with their inherent security weaknesses, introduces the need of new security features: the payloads of voice packets should be protected during conversations and no-replay as well as user authentication must be ensured on and end-to-end basis. The above concerns are actually the major barrier that may prevent the wide deployment of VoIP technologies, and coping with them is a truly challenging task. Consequently, we developed a novel hybrid framework for enhanced end-to-end security in the new generation SIP-empowered VoIP environments, based on the introduction of proven technologies such as digital signatures and efficient streamline encryption to enforce calling party identification, privacy, no-replay and non-repudiation throughout the whole IP Telephony system. All the security mechanisms used have been carefully chosen so that no systematic method is known to break the framework in realistic times and the overall voice quality will not be affected. 
28|6||Probabilistic model checking for the quantification of DoS security threats|Secure authentication features of communication and electronic commerce protocols involve computationally expensive and memory intensive cryptographic operations that have the potential to be turned into denial-of-service (DoS) exploits. Recent proposals attempt to improve DoS resistance by implementing a trade-off between the resources required for the potential victim(s) with the resources used by a prospective attacker. Such improvements have been proposed for the Internet Key Exchange (IKE), the Just Fast Keying (JFK) key agreement protocol and the Secure Sockets Layer (SSL/TLS) protocol. In present article, we introduce probabilistic model checking as an efficient tool-assisted approach for systematically quantifying DoS security threats. We model a security protocol with a fixed network topology using probabilistic specifications for the protocol participants. We attach into the protocol model, a probabilistic attacker model which performs DoS related actions with assigned cost values. The costs for the protocol participants and the attacker reflect the level of some resource expenditure (memory, processing capacity or communication bandwidth) for the associated actions. From the developed model we obtain a Discrete Time Markov Chain (DTMC) via property preserving discrete-time semantics. The DTMC model is verified using the PRISM model checker that produces probabilistic estimates for the analyzed DoS threat. In this way, it is possible to evaluate the level of resource expenditure for the attacker, beyond which the likelihood of widespread attack is reduced and subsequently to compare alternative design considerations for optimal resistance to the analyzed DoS threat. Our approach is validated through the analysis of the Host Identity Protocol (HIP). The HIP base-exchange is seen as a cryptographic key-exchange protocol with special features related to DoS protection. We analyze a serious DoS threat, for which we provide probabilistic estimates, as well as results for the associated attacker and participants' costs. 
28|6||Building lightweight intrusion detection system using wrapper-based feature selection mechanisms|Intrusion Detection System (IDS) is an important and necessary component in ensuring network security and protecting network resources and network infrastructures. How to build a lightweight IDS is a hot topic in network security. Moreover, feature selection is a classic research topic in data mining and it has attracted much interest from researchers in many fields such as network security, pattern recognition and data mining. In this paper, we effectively introduced feature selection methods to intrusion detection domain. We propose a wrapper-based feature selection algorithm aiming at building lightweight intrusion detection system by using modified random mutation hill climbing (RMHC) as search strategy to specify a candidate subset for evaluation, as well as using modified linear Support Vector Machines (SVMs) iterative procedure as wrapper approach to obtain the optimum feature subset. We verify the effectiveness and the feasibility of our feature selection algorithm by several experiments on KDD Cup 1999 intrusion detection dataset. The experimental results strongly show that our approach is not only able to speed up the process of selecting important features but also to yield high detection rates. Furthermore, our experimental results indicate that intrusion detection system with feature selection algorithm has better performance than that without feature selection algorithm both in detection performance and computational cost. 
28|6||The information security digital divide between information security managers and users|Empirical findings from surveys and in-depth interviews with information security managers and users indicate that a digital divide exists between these groups in terms of their views on and experience of information security practices. Information security professionals mainly regard users as an information security threat, whereas users believe themselves that they are an untapped resource for security work. The limited interaction between users and information security managers results in a lack of understanding for the other's point of view. These divergent views on and interpretations of information security mean that managers tend to base their practical method on unrealistic assumptions, resulting in management approaches that are poorly aligned with the dynamics of the users' working day. 
28|6||IFIP TC11 - Aims and Scope|
28|6||IFIP Technical Committee|
28|7|http://www.sciencedirect.com/science/journal/01674048/28/7|Contents|
28|7||Editorial|
28|7||Information security policy: An organizational-level process model|To protect information systems from increasing levels of cyber threats, organizations are compelled to institute security programs. Because information security policies are a necessary foundation of organizational security programs, there exists a need for scholarly contributions in this important area. Using a methodology involving qualitative techniques, we develop an information security policy process model based on responses from a sample of certified information security professionals. As the primary contribution of this research study, the proposed model illustrates a general yet comprehensive policy process in a distinctive form not found in existing professional standards or academic publications. This study's model goes beyond the models illustrated in the literature by depicting a larger organizational context that includes key external and internal influences that can materially impact organizational processes. The model that evolved from the data in this research reflects the recommended practices of our sample of certified professionals, thus providing a practical representation of an information security policy process for modern organizations. Before offering our concluding comments, we compare the results of the study with the literature in both theory and practice and also discuss limitations of the study. To the benefit of the practitioner and research communities alike, the model in this study offers a step forward, as well as an opportunity for making further advancements in the increasingly critical area of information security policy. 
28|7||Human and organizational factors in computer and information security: Pathways to vulnerabilities|The purpose of this study was to identify and describe how human and organizational factors may be related to technical computer and information security (CIS) vulnerabilities. A qualitative study of CIS experts was performed, which consisted of 2, 5-member focus groups sessions. The participants in the focus groups each produced a causal network analysis of human and organizational factors pathways to types of CIS vulnerabilities. Findings suggested that human and organizational factors play a significant role in the development of CIS vulnerabilities and emphasized the relationship complexities among human and organizational factors. The factors were categorized into 9 areas: external influences, human error, management, organization, performance and resource management, policy issues, technology, and training. Security practitioners and management should be aware of the multifarious roles of human and organizational factors and CIS vulnerabilities and that CIS vulnerabilities are not the sole result of a technological problem or programming mistake. The design and management of CIS systems need an integrative, multi-layered approach to improve CIS performance (suggestions for analysis provided). 
28|7||Risk profiles and distributed risk assessment|Risk assessment is concerned with discovering threat paths between potential attackers and critical assets, and is generally carried out during a system's design and then at fixed intervals during its operational life. However, the currency of such analysis is rapidly eroded by system changes; in dynamic systems these include the need to support ad-hoc collaboration, and dynamic connectivity between the system's components. This paper resolves these problems by showing how risks can be assessed incrementally as a system changes, using risk profiles, which characterize the risk to a system from subverted components. We formally define risk profiles, and show that their calculation can be fully distributed; each component is able to compute its own profile from neighbouring information. We further show that profiles converge to the same risks as systematic threat path enumeration, that changes in risk are efficiently propagated throughout a distributed system, and that the distributed computation provides a criterion for when the security consequences of a policy change are local to a component, or will propagate into the wider system. Risk profiles have the potential to supplement conventional risk assessments with useful new metrics, maintain accurate continuous assessment of risks in dynamic distributed systems, link a risk assessment to the wider environment of the system, and evaluate defence-in-depth strategies. 
28|7||Adapting usage control as a deterrent to address the inadequacies of access controls|Access controls are difficult to implement and evidently deficient under certain conditions. Traditional controls offer no protection for unclassified information, such as a telephone list of employees that is unrestricted, yet available only to members of the company. On the opposing side of the continuum, organizations such as hospitals that manage highly sensitive information require stricter access control measures. Yet, traditional access control may well have inadvertent consequences in such a context. Often, in unpredictable circumstances, users that are denied access could have prevented a calamity had they been allowed access. It has been proposed that controls such as auditing and accountability policies be enforced to deter rather than prevent unauthorized usage. In dynamic environments preconfigured access control policies may change dramatically depending on the context. Moreover, the cost of implementing and maintaining complex preconfigured access control policies sometimes far outweighs the benefits. This paper considers an adaptation of usage control as a proactive means of deterrence control to protect information that cannot be adequately or reasonably protected by access control. 
28|7||Security threats scenarios in trust and reputation models for distributed systems|Trust and reputation management over distributed systems has been proposed in the last few years as a novel and accurate way of dealing with some security deficiencies which are inherent to those environments. Thus, many models and theories have been developed in order to effective and accurately manage trust and reputation in those communities. Nevertheless, very few of them take into consideration all the possible security threats that can compromise the system. In this paper, we present some of the most important and critical security threats that could be applied in a trust and reputation scheme. We will describe and analyze each of those threats and propose some recommendations to face them when developing a new trust and reputation mechanism. We will also study how some trust and reputation models solve them. This work expects to be a reference guide when designing secure trust and reputation models. 
28|7||DFANS: A highly efficient strategy for automated trust negotiation|Automated trust negotiation (ATN) is an approach establishing mutual trust between strangers wishing to share resources or conduct business by gradually requesting and disclosing digitally signed credentials. The digital credentials themselves are usually sensitive, so they have corresponding access control policies to control their disclosure. Therefore, an ATN strategy must be adopted to determine the search for a successful negotiation based on the access control policies. Previously proposed negotiation strategies are either not complete, disclosing irrelevant credentials, or not efficient enough. In this paper, we propose a novel ATN strategy, that is, Deterministic Finite Automaton Negotiation Strategy (DFANS). DFANS is complete and ensures that no irrelevant credentials are disclosed during the negotiation. Furthermore, DFANS is highly efficient. In the worst case, its communication complexity is O(n), where n is the total number of credentials requested, and its computational complexity is O(m) when not involving the cyclic dependencies, where m is the total size of the both sides' policies looked up during the negotiation. When cyclic dependencies exist, a reasonable additional cost of running OSBE protocol that is a provably secure and quite efficient scheme will be added to the computational cost of DFANS to guarantee the negotiation success whenever possible. 
28|7||What the heck is this application doing? â A security-by-contract architecture for pervasive services|Future pervasive environments are characterized by non-fixed architectures made of users and ubiquitous computers. They will be shaped by pervasive client downloads, i.e. new (untrusted) applications will be dynamically downloaded to make a better use of the computational power available in the ubiquitous computing environment.To address the challenges of this paradigm we propose the notion of security-by-contract (S × C), as in programming-by-contract, based on the notion of a mobile contract that a pervasive download carries with itself. It describes the relevant security features of the application and the relevant security interactions with its computing environment. The contract can be used to check it against the device policy for compliance.In this paper we describe the S × C concepts, the S × C architecture and implementation and sketch some interaction modalities of the S × C paradigm. 
28|7||Utilizing bloom filters for detecting flooding attacks against SIP based services|Any application or service utilizing the Internet is exposed to both general Internet attacks and other specific ones. Most of the times the latter are exploiting a vulnerability or misconfiguration in the provided service and/or in the utilized protocol itself. Consequently, the employment of critical services, like Voice over IP (VoIP) services, over the Internet is vulnerable to such attacks and, on top of that, they offer a field for new attacks or variations of existing ones. Among the various threats–attacks that a service provider should consider are the flooding attacks, at the signaling level, which are very similar to those against TCP servers but have emerged at the application level of the Internet architecture. This paper examines flooding attacks against VoIP architectures that employ the Session Initiation Protocol (SIP) as their signaling protocol. The focus is on the design and implementation of the appropriate detection method. Specifically, a bloom filter based monitor is presented and a new metric, named session distance, is introduced in order to provide an effective protection scheme against flooding attacks. The proposed scheme is evaluated through experimental test bed architecture under different scenarios. The results of the evaluation demonstrate that the required time to detect such an attack is negligible and also that the number of false alarms is close to zero. 
28|7||Client-side cross-site scripting protection|Web applications are becoming the dominant way to provide access to online services. At the same time, web application vulnerabilities are being discovered and disclosed at an alarming rate. Web applications often make use of JavaScript code that is embedded into web pages to support dynamic client-side behavior. This script code is executed in the context of the user's web browser. To protect the user's environment from malicious JavaScript code, browsers use a sand-boxing mechanism that limits a script to access only resources associated with its origin site. Unfortunately, these security mechanisms fail if a user can be lured into downloading malicious JavaScript code from an intermediate, trusted site. In this case, the malicious script is granted full access to all resources (e.g., authentication tokens and cookies) that belong to the trusted site. Such attacks are called cross-site scripting (XSS) attacks.In general, XSS attacks are easy to execute, but difficult to detect and prevent. One reason is the high flexibility of HTML encoding schemes, offering the attacker many possibilities for circumventing server-side input filters that should prevent malicious scripts from being injected into trusted sites. Also, devising a client-side solution is not easy because of the difficulty of identifying JavaScript code as being malicious. This paper presents Noxes, which is, to the best of our knowledge, the first client-side solution to mitigate cross-site scripting attacks. Noxes acts as a web proxy and uses both manual and automatically generated rules to mitigate possible cross-site scripting attempts. Noxes effectively protects against information leakage from the user's environment while requiring minimal user interaction and customization effort. 
28|7||Measuring IDS-estimated attack impacts for rational incident response: A decision theoretic approach|Intrusion detection system (IDS) plays a vital role in defending our cyberspace against attacks. Either misuse-based IDS or anomaly-based IDS, or their combinations, however, can only partially reflect the true system state due to excessive false alerts, low detection rate, and inaccurate incident diagnosis. An automated response component built upon IDS therefore must consider the stale and imperfect picture inferred from them and takes action accordingly.This article presents an approach for measuring attack impact with the evidence of IDS alerts, with the objective to suggest rational response by cost-benefit analysis. More specifically, based on a very realistic assumption that a system evolves as a Markov decision process conditioned upon the current system state, imperfect observation, and action, we use partially observable Markov decision process to model the efficacy of IDS as providing a probabilistic assessment of the state of system assets, and to maximize a reward signal (defined as a function of both cost and benefit) by taking appropriate actions in response to the estimated system states in terms of desirable security properties. The ultimate goal is to move the system to more secure states with respect to pre-specified security metrics, and assist system administrators to identify the best tradeoff between the cost and benefit of security policies. We finally use a benchmark data set to practically illustrate the application of our methodology and conduct a proof-of-concept validation on its feasibility and efficiency. 
28|7||Confidence in smart token proximity: Relay attacks revisited|Contactless and contact smart card systems use the physical constraints of the communication channel to implicitly prove the proximity of a token. These systems, however, are potentially vulnerable to an attack where the attacker relays communication between the reader and a token. Relay attacks are not new but are often not considered a major threat, like eavesdropping or skimming attacks, even though they arguably pose an equivalent security risk. In this paper we discuss the feasibility of implementing passive and active relay attacks against smart tokens and the possible security implications if an attacker succeeds. Finally, we evaluate the effectiveness of time-out constraints, distance bounding and the use of a additional verification techniques for making systems relay-resistant and explain the challenges still facing these mechanisms. 
28|7||Defending passive worms in unstructured P2P networks based on healthy file dissemination|Propagation of passive worms in unstructured peer-to-peer (P2P) networks can result in significant damages and the loss of network security. This paper obtains the average delay for all peers in the entire transmitting process, and proposes a mathematical model for simulating unstructured P2P networks-based passive worms' propagation taking into account network throughput. According to the file popularity which follows the Zipf distribution, we propose a new healthy file dissemination-based defense strategy. Some parameters related to the propagation of passive worms are studied based on the proposed model. Finally, the simulation results verify the effectiveness of our model, which can provide an important guideline in the control of passive worms in unstructured P2P networks. 
28|7||On the development of an internetwork-centric defense for scanning worms|Studies of worm outbreaks have found that the speed of worm propagation makes manual intervention ineffective. Consequently, many automated containment mechanisms have been proposed to contain worm outbreaks before they grow out of control. These containment systems, however, only provide protection for hosts within networks that implement them. Such a containment strategy requires complete participation to protect all vulnerable hosts. Moreover, collaborative containment systems, where participants share alert data, face a tension between resilience to false alerts and quick reaction to worm outbreaks.This paper suggests an alternative approach where an autonomous system in an internetwork, such as the Internet, protects not only its local hosts, but also all hosts that route traffic through it, which we call internetwork-centric containment. Additionally, we propose a novel reputation-based alerting mechanism to provide fast dissemination of infection information while maintaining the fairness of the system. Through simulation studies, we show that the combination of internetwork-centric containment and reputation-based alerting is able to contain an extremely virulent worm with relatively little participation in the containment system. In comparison to other collaborative containment systems, ours provides better protection against worm outbreaks and resilience to false alerts. 
28|7||A concise cost analysis of Internet malware|In this paper we present a cost model to analyze impacts of Internet malware in order to estimate the cost of incidents and risk caused by them. The model is useful in determining parameters needed to estimate recovery efficiency, probabilistic risk distributions, and cost of malware incidents. Many users tend to underestimate the cost of curiosity coming with stealth malware such as email-attachments, freeware/shareware, spyware (including keyloggers, password thieves, phishing-ware, network sniffers, stealth backdoors, and rootkits), popups, and peer-to-peer fileshares. We define two sets of functions to describe evolution of attacks and potential loss caused by malware, where the evolution functions analyze infection patterns, while the loss functions provide risk-impact analysis of failed systems. Due to a wide range of applications, such analyses have drawn the attention of many engineers and researchers. Analysis of malware propagation itself has little to contribute unless tied to analysis of system performance, economic loss, and risks. 
28|7||Providing secure execution environments with a last line of defense against Trojan circuit attacks|Integrated circuits (ICs) are often produced in foundries that lack effective security controls. In these foundries, sophisticated attackers are able to insert malicious Trojan circuits that are easily hidden in the large, complex circuitry that comprises modern ICs. These so-called Trojan circuits are capable of launching attacks directly in hardware, or, more deviously, can facilitate software attacks. Current defense against Trojan circuits consists of statistical detection techniques to find such circuits before product deployment. The fact that statistical detection can result in false negatives raises the obvious questions: can attacks be detected post-deployment, and is secure execution nonetheless possible using chips with undetected Trojan circuits? In this paper we present the Secure Heartbeat And Dual-Encryption (SHADE) architecture, a compiler–hardware solution for detecting and preventing a subset of Trojan circuit attacks in deployed systems. Two layers of hardware encryption are combined with a heartbeat of off-chip accesses to provide a secure execution environment using untrusted hardware. The SHADE system is designed to complement pre-deployment detection techniques and to add a final, last-chance layer of security. 
28|7||A new steganography algorithm based on color histograms for data embedding into raw video streams|Steganography, embedding secret data into unsuspected objects, has emerged as a significant sub-discipline of data-embedding methods. While mostly applied to still images in the past, it has become very popular for video streams recently. When steganographic methods are applied to digital video streams, the selection of target pixels, which are used to store the secret data, is especially crucial for an effective and successful-embedding process; if pixels are not selected carefully, undesired spatial and temporal perception problems occur in the stego-video. In this paper, two new steganographic algorithms are proposed utilizing similar histograms and dissimilar histograms. Both algorithms are based on selecting appropriate pixel approaches by focusing on perceptibility and capacity parameters of the cover video. When compared to traditional steganographic techniques, they not only result in improved temporal and spatial perception levels in the stego-video but also offer a relatively high data-embedding capacity. 
28|7||Blind image steganalysis based on content independent statistical measures maximizing the specificity and sensitivity of the system|This paper reports the design principles and evaluation results of a new experimental universal, blind image steganalysing system. This system investigates the use of content independent statistical evidences left by the steganograms, as features for an image steganalyzer. The work is aimed at maximizing the sensitivity and specificity of the steganalyzer and to accomplish both security and system performance. A genetic-X-means classifier is constructed to realize the proposed model. For performance evaluation, a database composed of 5600 plain and stego images (generated by using seven different embedding schemes) was established. The results of our empirical experiment prove the vitality of the proposed scheme in detecting stego anomalies in images. In addition, the simulation results show that the effectiveness of steganalytic system can be enhanced by considering the content independent distortion measures and maximizing the sensitivity and specificity of the system. 
28|7||A schema for protecting the integrity of databases|Unauthorized changes to databases can result in significant losses for organizations as well as individuals. Watermarking can be used to protect the integrity of databases against unauthorized alterations. Prior work focused on watermarking database tables or relations. Malicious alteration cannot be detected in all cases. In this paper we argue that watermarking database indexes in addition to the database tables would improve the detection of unauthorized alterations. Usually, each database table in commercial applications has more than one index attached to it. Thus, watermarking the database table and all its indexes improve the likelihood of detecting malicious attacks. In general, watermarking different indexes like R-trees, B-trees, Hashes, require different watermarking techniques and exploit different redundancies in the underlying data structure. This diversity in watermarking techniques contributes to the overall integrity of the databases.Traditional relational watermarks introduce some error to the watermarked values and thus cannot be applied to all attributes. This paper proposes a novel watermarking scheme for R-tree data structures that does not change the values of the attributes. Moreover, the watermark does not change the size of the R-tree. The proposed technique takes advantage of the fact that R-trees do not put conditions on the order of entries inside the node. In the proposed scheme, entries inside R-tree nodes are rearranged, relative to a “secret” initial order (a secret key), in a way that corresponds to the value of the watermark.To achieve that, we propose a one-to-one mapping between all possible permutations of entries in the R-tree node and all possible values of the watermark. Without loss of generality, watermarks are assumed to be numeric values. The proposed mapping employs a numbering system that uses variable base with factorial value.The detection rate of the malicious attacks depends on the nature of the attack, distribution of the data, and the size of the R-tree node. Our extensive analysis and experimental results showed that the proposed technique detects data alteration with high probability (that reaches up to 99%) on real datasets using reasonable node sizes and attack model. The watermark insertion and extraction are mainly main memory operations, and thus, have minimal effect on the cost of R-tree operations. 
28|7||Design and implementation of highly reliable dual-computer systems|Two of the main parameters of real-time computer systems are reliability and performance. Researchers are always looking for solutions to increase the values of these parameters, which is the goal of this study. To this end, we propose an architecture for a dual-computer system that operates in real-time with fault tolerance implemented purely by hardware. The hardware, as designed and implemented, performs the following key services: 1) determination of the fault type (temporary or permanent) and 2) localization of the faulty computer without using self-testing techniques or diagnostic routines. Our design has several benefits: 1) the designed hardware shortens the recovery point time period; 2) the proposed nontrivial sequence of fault-tolerant services reduces (to two) the number of logical segments that must be re-run to recover computational processes; and 3) the determination of the fault type allows for the elimination of only computers with permanent faults. These contributions yield improvements in both the performance and reliability of the system. 
28|7||IFIP TC11 - Aims and Scope|
28|7||IFIP Technical Committee|
28|7||Call For Papers|
28|8|http://www.sciencedirect.com/science/journal/01674048/28/8|Contents|
28|8||Editorial|
28|8||DNS-based email sender authentication mechanisms: A critical review|We describe and compare three predominant email sender authentication mechanisms based on DNS: SPF, DKIM and Sender-ID Framework (SIDF). These mechanisms are designed mainly to assist in filtering of undesirable email messages, in particular spam and phishing emails. We clarify the limitations of these mechanisms, identify risks, and make recommendations. In particular, we argue that, properly used, SPF and DKIM can both help improve the efficiency and accuracy of email filtering. 
28|8||Issues and challenges in securing VoIP|Voice over the Internet protocol (VoIP) is being rapidly deployed, and the convergence of the voice and data worlds is introducing exciting opportunities. Lower cost and greater flexibility are the key factors luring enterprises to transition to VoIP. Some security problems may surface with the widespread deployment of VoIP. In this article, we discuss these security problems and propose a high-level security architecture that captures required features at each boundary-network-element in the VoIP infrastructure. We describe mechanisms to efficiently integrate information between distributed security components in the architecture. 
28|8||PENET: A practical method and tool for integrated modeling of security attacks and countermeasures|With the rise of cyber attack activities in the recent years, research in this area has gained immense emphasis. One of such research efforts is modeling of cyber attacks and countermeasures. In this context, several modeling approaches have been developed, such as approaches based on attack trees and on various stochastic tools. Attack tree model is one of the most intuitive and widely used tool. Although its simple design possesses various strengths, some unaddressed weaknesses such as imprecise analysis, limited modeling capabilities, and static nature plague its full potential. We propose a new modeling approach, called PENET, by extending the attack trees with new modeling constructs and analysis approaches. We add dynamic constructs for modeling dynamic behavior of system, arrival constructs that model periodic nature of attacks based on their cost, and defense constructs that model reparability of an insecure system. Petri Net Attack Modeling (PENET) approach has ability to convert and enhance existing attack trees with finer parameters, dynamic constructs, Petri net representation power, and intuitive time-domain analysis. We show how attack trees can be converted and analyzed in Petri net domain. We provide algorithm for time-domain analysis of PENET model, and performance metrics that are used to quantitatively describe survivability of a vulnerable system and effectiveness of attacker and victim's efforts. Next, we introduce PENET Tool as a practical software implementation of our approach. Finally, we provide a case study that illustrates the PENET approach. Security, dependability evaluation, security evaluation, performability evaluation, stochastic modeling. 
28|8||Proposal, design and evaluation of a mechanism to limit the length of anonymous overlay network paths|An alternative to guarantee anonymity in overlay networks may be achieved by building a multi-hop path between the initiator and the destination. Random walks (also known by means of the Crowds algorithm) have been widely used for this purpose in IP networks. Therefore, we explore the use of a Crowds-based mechanism to provide anonymity in overlay networks. However, the original algorithm does not limit the length of the paths, and in an overlay network the associated costs may grow excessively. Thus, controlling the length of the Crowds-based paths is a crucial issue in this scenario. A straightforward implementation makes use of a time-to-live (TTL) field. However, this implementation will immediately reveal whether the predecessor node is the initiator or not. This paper presents a novel mechanism to control the path length without using the TTL field. We propose an analytical model to evaluate the degree of anonymity when the path length is limited using our scheme. We conclude that limiting the multi-hop path length does not have any relevant impact over the degree of anonymity. We also prove that the new mechanism does not increase the vulnerability of Crowds over the traffic analysis and predecessor attacks. 
28|8||A study of on/off timing channel based on packet delay distribution|An on/off timing channel is a typical network covert timing channel, which can be used by attackers to steal information from compromised systems without triggering network firewalls and intrusion detection systems. In this paper, we discuss the principle of the information transmission in an on/off timing channel and categorize such channels into two types: deterministic channels and non-deterministic channels. We then analyze the components of packet delay and their characteristics, and provide a method of calculating the maximum transmission rate of a non-deterministic channel based on the packet delay distribution. After that, we conduct experiments to obtain the packet delay distribution in real network, and calculate the maximum transmission rate via our method. Then we construct an actual channel, and attain the actual transmission rate based on the observed symbol transmission probabilities. Our experiments show that the transmission rate calculated through our method is close to the real one, and can reveal the risk of the information leakage via on/off time channels in a network. In addition, the results indicate that non-deterministic channels may bring more threat than deterministic ones in the same network, and the information leakage via on/off timing channels should gain more intention. 
28|8||Classification of web robots: An empirical study based on over one billion requests|Many studies on detection and classification of web robots have focused their attention mostly on text crawlers, and empirical experiments used relatively small data collected at universities. In this paper, we analyzed more than one billion requests to www.microsoft.com in 24 h. Web logs were made anonymous to eliminate potential privacy concerns while preserving essential characteristics (e.g., frequency, queries, etc). We have developed an effective characterization metrics, based on workload characteristics and resource types, in detecting and classifying various web robots including text crawlers, link checkers, and icon crawlers. As expected, web robot behavior was clearly different from that of typical interactive users, and different types of web robots also exhibited different characteristics. However, comparison of the similar type of web robots, text crawlers in particular, revealed different characteristics, thereby enabling characterization with reasonably high confidence level. We divided various feature metrics into five groups, and effectiveness of each group in classification is shown in polar diagram in the decreasing order of effectiveness in the clockwise direction. One can use the findings to classify likely identify of unknown web robots, and organizations can develop appropriate measures to deal with them. Our analysis is based on recent web log data collected at one of the best known site which offers truly global service. 
28|8||OSNP: Secure wireless authentication protocol using one-time key|Handover security and efficiency have become more and more important in modern wireless network designs. In this paper, we propose a new protocol using the one-time key for user authentication. The proposed protocol can support both intra-domain and inter-domain authentications efficiently. Our protocol requires five messages for intra-domain initial authentication; three for subsequent authentication; and five for handover authentication. No authentication server is needed during handover, and our design reduces the computing load on the authentication server. We show an integration and implementation of EAP from 802.1X and our protocol, giving an easy way to apply our protocol on existing 802.11 wireless networks. The proposed protocol is realized and verified on the SWOON secure wireless testbed. 
28|8||Self-efficacy in information security: Its influence on end users' information security practice behavior|The ultimate success of information security depends on appropriate information security practice behaviors by the end users. Based on social cognitive theory, this study models and tests relationships among self-efficacy in information security, security practice behavior and motivation to strengthen security efforts. This study also explores antecedents to individuals' self-efficacy beliefs in information security. Results provide support for the many hypothesized relationships. This study provides an initial step toward understanding of the applicability of social cognitive theory in a new domain of information security. The results suggest that simply listing what not to do and penalties associated with a wrong doing in the users' information security policy alone will have a limited impact on effective implementation of security measures. The findings may help information security professionals design security awareness programs that more effectively increase the self-efficacy in information security. 
28|8||Using a bioinformatics approach to generate accurate exploit-based signatures for polymorphic worms|In this paper, we propose Simplified Regular Expression (SRE) signature, which uses multiple sequence alignment techniques, drawn from bioinformatics, in a novel approach to generating more accurate exploit-based signatures. We also provide formal definitions of what is “a more specific” and what is “the most specific” signature for a polymorphic worm and show that the most specific exploit-based signature generation is NP-hard. The approach involves three steps: multiple sequence alignment to reward consecutive substring extractions, noise elimination to remove noise effects, and signature transformation to make the SRE signature compatible with current IDSs. Experiments on a range of polymorphic worms and real-world polymorphic shellcodes show that our bioinformatics approach is noise-tolerant and as that because it extracts more polymorphic worm characters, like one-byte invariants and distance restrictions between invariant bytes, the signatures it generates are more accurate and precise than those generated by some other exploit-based signature generation schemes. 
28|8||Reverse OAuth: A solution to achieve delegated authorizations in single sign-on e-learning systems|Current scientific and technological progress has led to the proliferation of e-learning systems known as Learning Management Systems. These systems consist of a central application for managing the sequencing of students' tasks, and also on several other educational applications that allow its users (teachers and learners) to communicate, carry out experiments, etc. However, despite the widespread use of these systems they show a usability problem when both kinds of applications require spare authentication processes. Indeed, users have to introduce several kinds of credentials, preventing them from focusing their efforts on their studies and increasing the so-called “password stress”. Several initiatives such as OAuth or Delegation Permits have dealt with the problem of delegated authorizations, but their requirements are different from those that arise from an e-learning environment. In this paper we introduce Reverse OAuth – a protocol to enable the granting of authorizations to access protected resources in educational environments. 
28|8||IFIP TCII - Aims and Scope|
28|8||IFIP Technical Committee|
28|8||Call for Papers|
29|1|http://www.sciencedirect.com/science/journal/01674048/29/1|Contents|
29|1||Editorial|
29|1||A survey of video encryption algorithms|The popularity of multimedia applications is rapidly growing nowadays. The confidentiality of video communication is of primary concern for commercial usage, e.g. in video on demand services or business meetings. A variety of video encryption algorithms have been proposed in order to fulfill the specific requirements raised by the peculiarities of video communication. Video encryption algorithms can be classified according to their association with video compression into joint compression and encryption algorithms, and compression-independent encryption algorithms. From this classification perspective, we give a complete survey of the representative video encryption algorithms proposed so far and present their properties and limitations. We show by comparing and assessing the surveyed schemes that each scheme has its own strengths and weaknesses and no scheme can meet all specific requirements. Hence, video applications have to select an appropriate video encryption algorithm that meets their confidentiality requirements. 
29|1||An intruder model with message inspection for model checking security protocols|Model checking security protocols is based on an intruder model that represents the eavesdropping or interception of the exchanged messages, while at the same time performs attack actions against the ongoing protocol session(s). Any attempt to enumerate all messages that can be deduced by the intruder and the possible actions in all protocol steps results in an enormous branching of the model's state-space. In current work, we introduce a new intruder model that can be exploited for state-space reduction, optionally in combination with known techniques, such as partial order and symmetry reduction. The proposed intruder modeling approach called Message Inspection (MI) is based on enhancing the intruder's knowledge with metadata for the exchanged messages. In a preliminary simulation run, the intruder tags the analyzed messages with protocol-specific values for a set of predefined parameters. This metadata is used to identify possible attack actions, for which it is a priori known that they cannot cause a security violation. The MI algorithm selects attack actions that can be discarded, from an open-ended base of primitive attack actions. Thus, model checking focuses only on attack actions that may disclose a security violation. The most interesting consequence is a non-negligible state-space pruning, but at the same time our approach also allows customizing the behavior of the intruder model, in order e.g. to make it appropriate for model checking problems that involve liveness. We provide experimental results obtained with the SPIN model checker, for the Needham–Schroeder security protocol. 
29|1||Reducing false positives in intrusion detection systems|A post-processing filter is proposed to reduce false positives in network-based intrusion detection systems. The filter comprises three components, each one of which is based upon statistical properties of the input alert set. Special characteristics of alerts corresponding to true attacks are exploited. These alerts may be observed in batches, which contain similarities in the source or destination IPs, or they may produce abnormalities in the distribution of alerts of the same signature. False alerts can be recognized by the frequency with which their signature triggers false positives. The filter architecture and design are discussed. Evaluation results performed using the DARPA 1999 dataset indicate that the proposed approach can significantly reduce the number and percentage of false positives produced by Snort© (Roesch, 1999). Our filter limited false positives by a percentage up to 75%. 
29|1||On the detection and identification of botnets|We develop and discuss automated and self-adaptive systems for detecting and classifying botnets based on machine learning techniques and integration of human expertise. The proposed concept is purely passive and is based on analyzing information collected at three levels: (i) the payload of single packets received, (ii) observed access patterns to a darknet at the level of network traffic, and (iii) observed contents of TCP/IP traffic at the protocol level.We illustrate experiments based on real-life data collected with a darknet set up for this purpose to show the potential of the proposed concept for Levels (i) and (ii). As darknets cannot capture TCP/IP traffic data, we use a small spamtrap in our experiments at Level (iii). Strictly speaking, this approach for Level (iii) is not purely passive. However, traffic moving through a network could potentially be analyzed in a similar way to also obtain a purely passive system at this level. 
29|1||Anonymization models for directional location based service environments|Location based services (LBS) aim to deliver information based on a mobile user's location. However, knowledge of the location can be used by an adversary to physically locate the person, leading to the risk of physical harm, as well as possible leakage of certain personal information. This has serious consequences on privacy. The concept of location k-anonymity has been proposed to address this. Under this notion of anonymity, the adversary only has the knowledge that the LBS request is originating from a region containing at least k people, and therefore cannot individually distinguish the user. However, the existing anonymity models ignore the movement information of mobile users, assuming that it has no impact on privacy. Thus, existing work cannot ensure complete privacy while serving advanced type of LBS requests that require information about direction as well as speed of motion. We denote such LBS services as directional LBS. The key observation we make in this paper is that, in addition to the user's location, the user's movement direction should also be considered to ensure true anonymization. In this paper, we extend the notion of location k-anonymity by incorporating user's moving direction into the anonymization process while serving directional LBS. Specifically, our anonymization methods generalize both location and direction to the extent specified by the user. Our experimental results demonstrate that such anonymization can be achieved with marginal increase in computational cost when compared to the traditional location k-anonymity, while providing increased anonymity. 
29|1||Runtime monitoring for next generation Java ME platform|Many modern mobile devices, such as mobile phones or Personal digital assistants (PDAs), are able to run Java applications, such as games, Internet browsers, chat tools and so on. These applications perform some operations on the mobile device, that are critical from the security point of view, such as connecting to the Internet, sending and receiving SMS messages, connecting to other devices through the Bluetooth interface, browsing the user's contact list, and so on. Hence, an adequate security support is required to protect the device from malicious applications.This paper proposes an enhanced security support for next generation Java Micro Edition platform. This support performs a runtime monitoring of the operations performed by the Java applications, and enforces a security policy that defines which operations applications are allowed to perform. Two possible design approaches for the security support are presented and compared. 
29|1||The inference problem: Maintaining maximal availability in the presence of database updates|In this paper, we present the Dynamic Disclosure Monitor (D2Mon) architecture to prevent illegal inferences via database constraints. D2Mon extends the functionality of Disclosure Monitor (DiMon) to address database updates while preserving the soundness and completeness properties of the inference algorithms. We study updates from the perspective of increasing data availability. That is, updates on tuples that were previously released may affect the correctness of the user inferences over these tuples. We develop a mechanism, called Update Consolidator (UpCon), that propagates updates to a history file to ensure that no query is rejected based on inferences derived from outdated data. The history file is used by the Disclosure Inference Engine (DiIE) to compute inferences. We show that UpCon and DiIE working together guarantee confidentiality (completeness property of the data-dependent disclosure inference algorithm) and maximal availability (soundness property of the data-dependent disclosure inference algorithm) even in the presence of updates. We also present our implementation of D2Mon and our empirical results. 
29|1||Worm virulence estimation for the containment of local worm outbreak|A worm-infected host scanning globally may not cause any new infection in its underlying local network before it is detected and quarantined by a worm detector. To defend this type of scanning hosts, a number of worm scanner detection methods such as failed scan detection, honeypot, and dark port detection are proposed. However, for a stealthier worm limiting its scan inside an enterprise network, the chance of a successful local outbreak increases substantively due to the more limited scan space.To protect a local or enterprise network against a local outbreak, we need a coordinated and cost-conscious defense that entails an accurate estimate of worm virulence level. Unfortunately, many existing defense methods suffer from estimating the worm virulence level in a local or enterprise network. In this regard, we propose a maximum likelihood estimator to progressively estimate the size of susceptible host population in the local or enterprise network. From analysis and experimental evaluation, it is shown that the proposed estimator can report a reliable estimate of the size of susceptible population only after a few infections, sometimes only four, much faster than a similar method based on a Kalman filter. Also, based on maximum likelihood estimate, an appropriate containment threshold can be set to effectively stop the worm propagation while causing minimum service disruption to normal network users. 
29|1||A survey of coordinated attacks and collaborative intrusion detection|Coordinated attacks, such as large-scale stealthy scans, worm outbreaks and distributed denial-of-service (DDoS) attacks, occur in multiple networks simultaneously. Such attacks are extremely difficult to detect using isolated intrusion detection systems (IDSs) that monitor only a limited portion of the Internet. In this paper, we summarize the current research directions in detecting such attacks using collaborative intrusion detection systems (CIDSs). In particular, we highlight two main challenges in CIDS research: CIDS architectures and alert correlation algorithms. We review the current CIDS approaches in terms of these two challenges. We conclude by highlighting opportunities for an integrated solution to large-scale collaborative intrusion detection. 
29|1||Pitfalls in CAPTCHA design and implementation: The Math CAPTCHA, a case study|We present a black-box attack against an already deployed CAPTCHA that aims to protect a free service delivered using the Internet. This CAPTCHA, referred to as “Math CAPTCHA” or “QRBGS CAPTCHA”, requests the user to solve a mathematical problem in order to prove human. We study significant problems both in its design and its implementation, and how those flaws can be used to completely solve this CAPTCHA using a low-cost attack. This attack requires no development in Artificial Intelligence or automatic character recognition, the intended path, thus becoming a side-channel attack, based on the previously mentioned CAPTCHAs flaws. We relate these flaws to common flaws found in other CAPTCHA proposals. We conclude with some tips for enhancing this CAPTCHA that can be considered as general guidelines. 
29|1||IFIP TCII - Aims and Scope|
29|1||IFIP Technical Committee|
29|1||Call for papers|
29|2|http://www.sciencedirect.com/science/journal/01674048/29/2|Contents|
29|2||Editorial|
29|2||Certified electronic mail: Properties revisited|Certified electronic mail is an added value to traditional electronic mail. In the definition of this service some differences arise: a message in exchange for a reception proof, a message and a non repudiation of origin token in exchange for a reception proof, etc. It greatly depends on whether we want to emulate the courier service or improve the service in the electronic world. If the definition of the service seems conflictive, the definition of the properties and requirements of a good certified electronic mail protocol is even more difficult. The more consensuated features are the need of a fair exchange and the existence of a trusted third party (TTP). Each author chooses the properties that considers the most important, and many times the list is conditioned by the proposal. Which kind of TTP must be used? Must it be verifiable, transparent and/or stateless? Which features must the communication channel fulfil? Which temporal requirements must be established? What kind of fairness is desired? What efficiency level is required? Are confidentiality or transferability of the proofs compulsory properties? In this paper we collect the definitions, properties and requirements related with certified electronic mail. The aim of the paper is to create a clearer situation and analyze how some properties cannot be achieved simultaneously. Each protocol designer will have to decide which properties are the most important in the environment in where the service is to be deployed. 
29|2||A secure peer-to-peer backup service keeping great autonomy while under the supervision of a provider|Making backup is so cumbersome and expensive that individuals hardly ever backup their data and companies usually duplicate their data into a secondary server. This paper proposes a novel Peer-to-Peer (P2P) backup system known as the SecureBackup service, which was defined in the DisPairse research project. It utilizes the unused personal hard disk spaces attached to the Internet to implement a distributed backup service that is reliable, performant, and secure. Additionally to the existing approaches like pStore (Batten et al., 2001), Pastiche (Landon et al., 2002), and PeerStore (Landers et al., 2004), addressing the integrity, confidentiality and availability of data in a P2P backup system, the SecureBackup service implements the access control of the peers to the service, the detection of malicious peers, the evaluation of the reliability level of each peer, the rewarding or charging of the peers for the consumed resources or the resources they made available, and the incentives for peers to actively participate to the service.SecureBackup is mainly characterized by a newly meta-data structure for backup file, and a centralized node, under the control of the service provider, which supervises the Authentication, Authorization and Accounting (AAA) services and the operations of the system. While designing a centralized architecture, we paid attention to preserve the autonomy of the P2P backup service. According to some experiments made on few peer nodes, we are confident that the introduced security mechanisms do not much penalize the performances of the system. 
29|2||A framework and assessment instrument for information security culture|An organisation's approach to information security should focus on employee behaviour, as the organisation's success or failure effectively depends on the things that its employees do or fail to do. An information security-aware culture will minimise risks to information assets and specifically reduce the risk of employee misbehaviour and harmful interaction with information assets. Organisations require guidance in establishing an information security-aware or implementing an acceptable information security culture. They need to measure and report on the state of information security culture in the organisation. Various approaches exist to address the threats that employee behaviour could pose. However, these approaches do not focus specifically on the interaction between the behaviour of an employee and the culture in an organisation. Organisations therefore have need of a comprehensive framework to cultivate a security-aware culture. The objective of this paper is to propose a framework to cultivate an information security culture within an organisation and to illustrate how to use it. An empirical study is performed to aid in validating the proposed Information Security Culture Framework. 
29|2||WARP: A wormhole-avoidance routing protocol by anomaly detection in mobile ad hoc networks|The infrastructure of a Mobile Ad hoc Network (MANET) has no routers for routing, and all nodes must share the same routing protocol to assist each other when transmitting messages. However, almost all common routing protocols at present consider performance as first priority, and have little defense capability against the malicious nodes. Many researches have proposed various protocols of higher safety to defend against attacks; however, each has specific defense objects, and is unable to defend against particular attacks. Of all the types of attacks, the wormhole attack poses the greatest threat and is very difficult to prevent; therefore, this paper focuses on the wormhole attack, and proposes a secure routing protocol based on the AODV (Ad hoc On-demand Distance Vector) routing protocol, which is named WARP (Wormhole-Avoidance Routing Protocol). WARP considers link-disjoint multipaths during path discovery, and provides greater path selections to avoid malicious nodes, but eventually uses only one path to transmit data. Based on the characteristic that wormhole nodes can easily grab the route from the source node to the destination node, WARP enables the neighbors of the wormhole nodes to discover that the wormhole nodes have abnormal path attractions. Then, the wormhole nodes would be gradually isolated by their normal neighboring nodes, and finally be quarantined by the whole network. 
29|2||Survey of network security systems to counter SIP-based denial-of-service attacks|Session Initiation Protocol is a core protocol for coming real time communication networks, including VoIP, IMS and IPTV networks. Based on the open IP stack, it is similarly susceptible to Denial-of-Service Attacks launched against SIP servers. More than 20 different research works have been published to address SIP-related DoS problems. In this survey we explain three different types of DoS attacks on SIP networks, called SIP message payload tampering, SIP message flow tampering and SIP message flooding. We survey different approaches to counter these three types of attacks. We show that there are possible solutions for both payload and flow tampering attacks, and partial solutions for message flooding attacks. We conclude by giving hints how open flooding attacks issues could be addressed. 
29|2||Two proposed identity-based three-party authenticated key agreement protocols from pairings|The use of pairings has been shown promising for many two-party and three-party identity-based authenticated key agreement protocols. In recent years, several identity-based authenticated key agreement protocols have been proposed and most of them broken. In this paper, we propose two three-party identity-based authenticated key agreement protocols applying bilinear pairings. We show that the proposed protocols are secure (i.e. conform to defined security attributes) while being efficient. 
29|2||On the symbiosis of specification-based and anomaly-based detection|As the number of attacks on computer systems increases and become more sophisticated, there is an obvious need for intrusion detection systems to be able to effectively recognize the known attacks and adapt to novel threats. The specification-based intrusion detection has been long considered as a promising solution that integrates the characteristics of ideal intrusion detection system: the accuracy of detection and ability to recognize novel attacks. However, one of the main challenges of applying this technique in practice is its dependence on the user guidance in developing the specification of normal system behavior. In this work, we present an approach for automatic generation of specifications for any software systems executing on a single host based on the combination of two techniques: specification-based and anomaly-based approaches. The proposed technique allows automatic development of the normal and abnormal behavioral specifications in a form of variable-length patterns classified via anomaly-based approach. Specifically, we use machine-learning algorithm to classify fixed-length patterns generated via sliding window technique to infer the classification of variable-length patterns from the aggregation of the machine learning based classification results. We describe the design and implementation of our technique and show its practical applicability in the domain of security monitoring through simulation and experiments. 
29|2||An efficient and fair buyerâseller fingerprinting scheme for large scale networks|In digital watermarking, most existing schemes focus on the owners' copyright protection rather than protection of the customers' rights. Therefore, these schemes are unfair to legitimate customers who have no certificate to prove their right to use the watermarked digital content that they have purchased. In addition, these schemes are also unable to identify those who leak pirated copies of the watermarked digital content. To protect customers' rights and to identify the users of unauthorized copies, the fingerprinting technique is a feasible method for embedding a watermark so that content owners can identify users who have purchased the right to use the content and users who have not purchased this right. Although some fingerprinting schemes have been proposed in recent years, most of them are inefficient due to their homomorphic architecture that is based on public key cryptography. Therefore, in this paper, we propose a fair, traceable, and efficient watermarking scheme with a novel architecture. Due to the high computational complexity of the asymmetric cryptography, such as modular multiplications and exponentiations which lead much heavier burden than operations in symmetric cryptography, the proposed protocol transfers the demanding computational requirements from the buyer to a powerful server in protocol design. The proposed method can achieve these benefits: 1) the rights of legitimate buyers can be protected; 2) the proposed scheme is traceable; 3) the proposed scheme is more efficient than the previous schemes because public key cryptography is not frequently used; and 4) the buyer's anonymity can be well-protected until there is an infringement accusation. 
29|2||PKI-based trust management in inter-domain scenarios|Hierarchical cross-certification fits well within large organizations that want their root CA to have direct control over all subordinate CAs. However, both Peer-to-Peer and Bridge CA cross-certification models suits better than the hierarchical one with organizations where a certain level of flexibility is needed to form and revoke trust relationships with other organizations as changing policy or business needs dictate. It seems that this second approach better fits the current and next-generation inter-domain networking models existing in both the wired and wireless Internet. In this context, this paper analyses some relevant inter-domain scenarios and derives the main requirements in terms of cross-certification from them. It then describes the design and lab implementation of a pan-European scenario which is based on a research network composed by a set of organizations that may have their own PKIs running, and that are interested to link with others in terms of certification services. It provides a complete design, implementation and performance analysis for this complex scenario, including a procedure and practical recommendations for building and validating certification paths. 
29|2||IFIP TCII - Aims and Scope|
29|2||IFIP Technical Committee|
29|2||Call for papers|
29|2||Call for papers|
29|3|http://www.sciencedirect.com/science/journal/01674048/29/3|Contents|
29|3||Special issue on software engineering for secure systems|
29|3||Provably correct Java implementations of Spi Calculus security protocols specifications|Spi Calculus is an untyped high level modeling language for security protocols, used for formal protocols specification and verification. In this paper, a type system for the Spi Calculus and a translation function are formally defined, in order to formalize the refinement of a Spi Calculus specification into a Java implementation. The Java implementation generated by the translation function uses a custom Java library. Formal conditions on such library are stated, so that, if the library implementation code satisfies such conditions, then the generated Java implementation correctly simulates the Spi Calculus specification. A verified implementation of part of the custom library is further presented. 
29|3||Runtime verification of cryptographic protocols|There has been a significant amount of work devoted to the static verification of security protocol designs. Virtually all of these results, when applied to an actual implementation of a security protocol, rely on certain implicit assumptions on the implementation (for example, that the cryptographic checks that according to the design have to be performed by the protocol participants are carried out correctly). So far there seems to be no approach that would enforce these implicit assumptions for a given implementation of a security protocol (in particular regarding legacy implementations which have not been developed with formal verification in mind).In this paper, we use a code assurance technique called “runtime verification” to solve this open problem. Runtime verification determines whether or not the behaviour observed during the execution of a system matches a given formal specification of a “reference behaviour”. By applying runtime verification to an implementation of any of the participants of a security protocol, we can make sure during the execution of that implementation that the implicit assumptions that had to be made to ensure the security of the overall protocol will be fulfilled. The overall assurance process then proceeds in two steps: First, a design model of the security protocol in UML is verified against security properties such as secrecy of data. Second, the implicit assumptions on the protocol participants are derived from the design model, formalised in linear-time temporal logic, and the validity of these formulae at runtime is monitored using runtime verification. The aim is to increase one's confidence that statically verified properties are satisfied not only by a model of the system, but also by the actual running system itself. We demonstrate the approach at the hand of the open source implementation Jessie of the de-facto Internet security protocol standard SSL. We also briefly explain how to transfer the results to the SSL-implementation within the Java Secure Sockets Extension (JSSE) recently made open source by Sun Microsystems. 
29|3||A knowledgeable security model for distributed health information systems|Realising the vision of pervasive healthcare will generate new challenges to system security. Such challenges are fundamentally different from issues and problems that we face in centralised approaches as well as non-clinical scenarios. In this paper, we reflect upon our experiences in the HealthAgents project wherein a prototype system was developed and a novel approach employed that supports data transfer and decision making in human brain tumour diagnosis and treatment. While the decision making needs to rely on different clinical expertise, the HealthAgents system leveraged a domain ontology to align different sub-domain vocabularies and we have experimented with a process calculus to glue together distributed services. We examine the capability of the Lightweight Coordination Calculus (LCC), a process calculus based language, in meeting security challenges in pervasive settings, especially in the healthcare domain. The key difference in approach lies in making the representational abstraction reflect the relative autonomy of the various clinical specialisms involved in contributing to patient management. The scope within LCC of accommodating Boolean-valued constraints allows for flexible integration of heterogeneous sources in multiple formats, which are characteristic features of a pervasive healthcare environment. 
29|3||A framework of composable access control features: Preserving separation of access control concerns from models to code|Modeling of security policies, along with their realization in code, must be an integral part of the software development process, to achieve an acceptable level of security for a software application. Among all of the security concerns (e.g. authentication, auditing, access control, confidentiality, etc.), this paper addresses the incorporation of access control into software. The approach is to separate access control concerns from the rest of the design. To assist designers to visualize access control policies separated from non-security concerns, this paper proposes a set of access control diagrams, i.e., extensions to the UML to represent three main access control models: role-based access control (RBAC), mandatory access control (MAC), and discretionary access control (DAC). To better adapt to changing requirements, and assist designers to customize access control policies, this paper proposes a set of access control features, i.e., small components that realize specific capabilities of access control models. Designers can select the features they require, and compose them to yield different access control policies. When transitioning into code, the main focus is to preserve separation of access control concerns. This paper describes an approach to realize access control diagrams and features in code through structure-preserving mappings, describes three different approaches to enforce access control in code, and evaluates the way each of them separate access control from other concerns. 
29|3||IFIP TCII - Aims and Scope|
29|3||IFIP Technical Committee|
29|4|http://www.sciencedirect.com/science/journal/01674048/29/4|Contents|
29|4||Editorial|
29|4||Extensions to the source path isolation engine for precise and efficient log-based IP traceback|IP traceback is used to determine the source and path traversed by a packet received from the Internet. In this work we first show that the Source Path Isolation Engine (SPIE), a classical log-based IP traceback system, can return misleading attack graphs in some particular situations, which may even make it impossible to determine the real attacker. We show that by unmasking the TTL field SPIE returns a correct attack graph that precisely identifies the route traversed by a given packet allowing the correct identification of the attacker. Nevertheless, an unmasked TTL poses new challenges in order to preserve the confidentiality of the communication among the system's components. We solve this problem presenting two distributed algorithms for searching across the network overlay formed by the packet log bases. Two other extensions to SPIE are proposed that improve the efficiency of source discovery: separate logs are kept for each router interface improving the distributed search procedure; an efficient dynamic log paging strategy is employed, which is based on the actual capacity factor instead of the fixed time interval originally employed by SPIE. The system was implemented and experimental results are presented. 
29|4||A security privacy aware architecture and protocol for a single smart card used for multiple services|In the face of the expanding Internet and an ever-growing number of threats, today's society is becoming more geared towards greater security and protection of privacy and personal information. Smart cards provide protection for information at the hardware level, however, smart cards are designed for use with a single specific application. In this paper we introduce the concept of utilising a single smart card with multiple applications. Such a scheme would, however, increase the reward of an attack on the smart card due to the amount of information stored on a smart card. This paper proposes an architecture to allow a single smart card to be used in a dynamic multiple application environment. In conjunction with the architecture, a protocol messaging scheme is provided to protect all information communicated between the smart card and an application through the use of one-time passwords, whilst maintaining the privacy of one's personal information. 
29|4||Stability analysis of a SEIQV epidemic model for rapid spreading worms|Internet worms have drawn significant attention owing to their enormous threats to the Internet. Due to the rapid spreading nature of Internet worms, it is necessary to implement automatic mitigation on the Internet. Inspired by worm vaccinations, we propose a novel epidemic model which combines both vaccinations and dynamic quarantine methods, referred to as SEIQV model. Using SEIQV model, we obtain the basic reproduction number that governs whether or not a worm is extinct. The impact of different parameters on this model is studied. Simulation results show that the performance of our model is significantly better than other models, in terms of decreasing the number of infected hosts and reducing the worm propagation speed. 
29|4||Significantly improved performances of the cryptographically generated addresses thanks to ECC and GPGPU|Cryptographically Generated Addresses (CGA) are today mainly used with the Secure Neighbor Discovery Protocol (SEND). Despite CGA generalization, current standards only show how to construct CGA with the RSA algorithm and SHA-1 hash function. This limitation may prevent new usages of CGA and SEND in mobile environments where nodes are energy and storage limited.In this paper, we present the results of a performance and security study of the CGA and SEND. To significantly improve the performances of the CGA, we investigate first replacing RSA with ECC (Elliptic Curve Cryptography) and ECDSA (Elliptic Curve DSA), and second using the General-Purpose computing on Graphical Processing Units (GPGPU). Finally, a performance comparison between different hash algorithms (SHA-256, WHIRLPOOL,…) allows to prepare a better transition for the CGA when SHA-1 will be deprecated. 
29|4||Improving information security awareness and behaviour through dialogue, participation and collective reflection. An intervention study|The paper discusses and evaluates the effects of an information security awareness programme. The programme emphasised employee participation, dialogue and collective reflection in groups. The intervention consisted of small-sized workshops aimed at improving information security awareness and behaviour. An experimental research design consisting of one survey before and two after the intervention was used to evaluate whether the intended changes occurred. Statistical analyses revealed that the intervention was powerful enough to significantly change a broad range of awareness and behaviour indicators among the intervention participants. In the control group, awareness and behaviour remained by and large unchanged during the period of the study. Unlike the approach taken by the intervention studied in this paper, mainstream information security awareness measures are typically top-down, and seek to bring about changes at the individual level by means of an expert-based approach directed at a large population, e.g. through formal presentations, e-mail messages, leaflets and posters. This study demonstrates that local employee participation, collective reflection and group processes produce changes in short-term information security awareness and behaviour. 
29|4||Hybrid spam filtering for mobile communication|Spam messages are an increasing threat to mobile communication. Several mitigation techniques have been proposed, including white and black listing, challenge-response and content-based filtering. However, none are perfect and it makes sense to use a combination rather than just one. We propose an anti-spam framework based on the hybrid of content-based filtering and challenge-response. A message, that has been classified as uncertain through content-based filtering, is checked further by sending a challenge to the message sender. An automated spam generator is unlikely to send back a correct response, in which case, the message is classified as spam.Our simulation results show the trade-off between the accuracy of anti-spam classifiers and the incurring traffic overhead, and demonstrate that our hybrid framework is capable of achieving high accuracy regardless of the content-based filtering algorithm being used. 
29|4||A generic mechanism for efficient authentication in B3G networks|A user in Beyond 3rd Generation (B3G) networks in order to get access to the network services must perform a multi-pass authentication procedure, which includes two or three sequential authentications steps. These multiple authentication steps include a redundant repetition of the same or similar authentication functions, which impose an unnecessary authentication overhead. This paper proposes a security binding mechanism, which reduces the execution of the redundant authentication functions of multi-pass authentications in a simple yet effective and secure manner. To achieve this, the proposed mechanism authenticates a user in the second and third step of a multi-pass authentication, by using the user's authentication credentials of the initial step. The focal point of the security binding mechanism is its generic application in multi-pass authentications, regardless of the underlying network architecture or protocols. To prove this, we have selected to present and analyze the application of the proposed mechanism in two different B3G scenarios (i.e., 3G-WLAN and WiMAX), resulting in the improved authentication procedures. A security analysis of the improved procedures has been carried out to identify possible attacks and propose security measures to eliminate them. Moreover, a simulation model has been developed to estimate and compare the performance of the improved 3G-WLAN authentication procedure to that of the legacy 3G-WLAN authentication. Simulation results show that the improved procedure presents better performance than its legacy counterpart. 
29|4||Information security culture: A management perspective|Information technology has become an integral part of modern life. Today, the use of information permeates every aspect of both business and private lives. Most organizations need information systems to survive and prosper and thus need to be serious about protecting their information assets. Many of the processes needed to protect these information assets are, to a large extent, dependent on human cooperated behavior. Employees, whether intentionally or through negligence, often due to a lack of knowledge, are the greatest threat to information security. It has become widely accepted that the establishment of an organizational sub-culture of information security is key to managing the human factors involved in information security. This paper briefly examines the generic concept of corporate culture and then borrows from the management and economical sciences to present a conceptual model of information security culture. The presented model incorporates the concept of elasticity from the economical sciences in order to show how various variables in an information security culture influence each other. The purpose of the presented model is to facilitate conceptual thinking and argumentation about information security culture. 
29|4||Power system DNP3 data object security using data sets|Power system cyber security demand is escalating with the increased number of security incidents and the increased stakeholder participation in power system operations, specifically consumers. Rule-based cyber security is proposed for Distributed Network Protocol (DNP3) outstation devices, with a focus on smart distribution system devices. The security utilizes the DNP3 application layer function codes and data objects to determine data access authorization for outstations, augmenting other security solutions that include firewalls, encryption, and authentication. The cyber security proposed in this article protects outstation devices when masters are compromised or attempt unauthorized access that bypass the other security solutions. In this article, non-utility stakeholder data access is limited through DNP3 data sets rather than granting direct access to the data points within an outstation. The data set utilization greatly constrains possible attack methods against a device by reducing the interaction capabilities with an outstation. The data sets also decrease the security complexity through rule reduction, thereby increasing the security applicability for retrofitted or process constrained devices. Temporal security constraints are supported for the data sets, increasing security against denial of service attacks. 
29|4||Pervasive authentication and authorization infrastructures for mobile users|Network and device heterogeneity, nomadic mobility, intermittent connectivity and, more generally, extremely dynamic operating conditions, are major challenges in the design of security infrastructures for pervasive computing. Yet, in a ubiquitous computing environment, limitations of traditional solutions for authentication and authorization can be overcome with a pervasive public key infrastructure (pervasive-PKI). This choice allows the validation of credentials of users roaming between heterogeneous networks, even when global connectivity is lost and some services are temporarily unreachable. Proof-of-concept implementations and testbed validation results demonstrate that strong security can be achieved for users and applications through the combination of traditional PKI services with a number of enhancements like: (i) dynamic and collaborative trust model, (ii) use of attribute certificates for privilege management, and (iii) modular architecture enabling nomadic mobility and enhanced with reconfiguration capabilities. 
29|4||IFIP TCII - Aims and Scope|
29|4||IFIP Technical Committee|
29|5|http://www.sciencedirect.com/science/journal/01674048/29/5|Contents|
29|5||Editorial|
29|5||Reconstruction of electronic signatures from eDocument printouts|Governments and public administrations produce documents: laws, orders, permits, notifications, etc. With the transition from traditional paper-based administration to eGovernment that we have seen in the last decade, authentic electronic documents gain importance. Electronic signatures promise to be a tool of choice. However, given the choice of access channels public administrations offer, i.e. electronic or conventional access to services, eDocuments will have to co-exist with traditional paper documents for several years, if not for decades. In this paper we present a solution that visually adds electronic signatures to documents. This is done in a way that allows for verifying the electronic signature from printouts. The purpose is to make the electronic signature resistant against media-breaks and to allow the authority issuing electronic documents, even if the receiver prefers conventional paper documents. We discuss the Austrian practical experience gained with such eSignatures and eDocuments in eGovernment. 
29|5||Managing key hierarchies for access control enforcement: Heuristic approaches|Data outsourcing is emerging today as a successful paradigm allowing individuals and organizations to resort to external servers for storing their data, and sharing them with others. The main problem of this trend is that sensitive data are stored on a site that is not under the data owner's direct control. This scenario poses a major security problem since often the external server is relied upon for ensuring high availability of the data, but it is not authorized to read them. Data need therefore to be encrypted. In such a context, the application of an access control policy requires different data to be encrypted with different keys so to allow the external server to directly enforce access control and support selective dissemination and access. The problem therefore emerges of designing solutions for the efficient management of an encryption policy enforcing access control, with the goal of minimizing the number of keys to be maintained by the system and distributed to users.In this paper, we prove that the problem of minimizing the number of keys is NP-hard and present alternative approaches for its solution. We first formulate the minimization problem as an instance of an integer linear programming problem and then propose three different families of heuristics, which are based on a key derivation tree exploiting the relationships among user groups. Finally, we experimentally evaluate the performance of our heuristics, comparing them with previous approaches. 
29|5||Taming role mining complexity in RBAC|In this paper we address the problem of reducing the role mining complexity in RBAC systems. To this aim, we propose a three steps methodology: first, we associate a weight to roles; second, we identify user-permission assignments that cannot belong to roles with a weight exceeding a given threshold; and third, we restrict the role-finding problem to user-permission assignments identified in the second step. We formally show—the proofs of our results are rooted in graph theory—that this methodology allows role engineers for the elicitation of stable candidate roles, by contextually simplifying the role selection task. Efficient algorithms to implement our strategy are also described. Further, we discuss practical applications of our approach. Finally, we tested our methodology on real dataset. Results achieved confirm both the viability of our proposal and the analytical findings. 
29|5||On a taxonomy of delegation|Delegation, from a technical point of view, is widely considered as a potential approach in addressing the problem of providing dynamic access control decisions in activities with a high level of collaboration, either within a single security domain or across multiple security domains. Although delegation continues to attract significant attention from the research community, presently, there is no published work that presents a taxonomy of delegation concepts and models. This article intends to address this gap by presenting a set of taxonomic criteria relevant to the concept of delegation. This article also applies the taxonomy to a selection of significant delegation models published in the literature. 
29|5||Collaborative privacy management|The landscape of the World Wide Web with all its versatile services heavily relies on the disclosure of private user information. Unfortunately, the growing amount of personal data collected by service providers poses a significant privacy threat for Internet users. Targeting growing privacy concerns of users, privacy-enhancing technologies emerged. One goal of these technologies is the provision of tools that facilitate a more informative decision about personal data disclosures. A famous PET representative is the PRIME project that aims for a holistic privacy-enhancing identity management system. However, approaches like the PRIME privacy architecture require service providers to change their server infrastructure and add specific privacy-enhancing components. In the near future, service providers are not expected to alter internal processes. Addressing the dependency on service providers, this paper introduces a user-centric privacy architecture that enables the provider-independent protection of personal data. A central component of the proposed privacy infrastructure is an online privacy community, which facilitates the open exchange of privacy-related information about service providers. We characterize the benefits and the potentials of our proposed solution and evaluate a prototypical implementation. 
29|5||Roving bugnet: Distributed surveillance threat and mitigation|Advanced mobile devices such as laptops and smartphones make convenient hiding places for surveillance spyware. They commonly have a microphone and camera built-in, are increasingly network accessible, frequently within close proximity of their users, and almost always lack mechanisms designed to prevent unauthorized microphone or camera access.In order to explore surveillance intrusion and detection methods, we present a modernized version of a microphone hijacker for Windows and Mac OS X. The Windows attack can be executed as soon as the target connects to the Internet from anywhere in the world without requiring interaction from victimized users and the Mac OS X attack involves a trojaned installation routine. As the attacker compromises additional machines they are organized into a botnet so the attacker can maintain stealthy control of the systems and launch later surveillance attacks.We then use the attack to show how common elements of microphone hijacker programs can be used against them. From there we present a mechanism to detect the threat on Windows, as well as a novel method to deceive an attacker in order to permit traceback. As a result of the detection mechanism we address a missing segment of resource control, decreasing the complexity of privacy concerns as exploitable devices become more pervasive. 
29|5||Audio CAPTCHA: Existing solutions assessment and a new implementation for VoIP telephony|SPam over Internet Telephony (SPIT) is a potential source of future annoyance in Voice over IP (VoIP) systems. A typical way to launch a SPIT attack is the use of an automated procedure (i.e., bot), which generates calls and produces unsolicited audio messages. A known way to protect against SPAM is a Reverse Turing Test, called CAPTCHA (Completely Automated Public Turing Test to Tell Computer and Humans Apart). In this paper, we evaluate existing audio CAPTCHA, as this type of format is more suitable for VoIP systems, to help them fight bots. To do so, we first suggest specific attributes-requirements that an audio CAPTCHA should meet in order to be effective. Then, we evaluate this set of popular audio CAPTCHA, and demonstrate that there is no existing implementation suitable enough for VoIP environments. Next, we develop and implement a new audio CAPTCHA, which is suitable for SIP-based VoIP telephony. Finally, the new CAPTCHA is tested against users and bots and demonstrated to be efficient. 
29|5||A provably secure secret handshake with dynamic controlled matching|A Secret Handshake is a protocol that allows two users to mutually verify one another's properties, with the assurance that only authorized parties are able to engage in a successful protocol run. In case of simultaneous matching, the two parties share a key that can be used to secure subsequent communications. In this paper, we present the first Secret Handshake scheme that allows dynamic matching of properties under stringent security requirements: in particular, the right to prove and to verify are strictly under the control of an authority. This work merges characteristics of Secret Handshake with features peculiar to Secure Matchmaking. 
29|5||A note about the identifier parent property in Reed-Solomon codes|Codes with traceability properties are used in schemes where the identification of users that illegally redistribute content is required. For any code with traceability properties, the Identifiable Parent Property (c-IPP) seems to be less restrictive than the Traceability (c-TA) property. In this paper, we show that for Reed-Solomon codes both properties are in many cases equivalent. More precisely, we show that for an [n, k, d] Reed-Solomon code, defined over a field that contains the n – d roots of unity, both properties are equivalent. Also, we show how the strategy we propose can be applied to other cases by proving the equivalence of both properties for a particular code of characteristic 2. This answers a question posted by  12 and 13, for a large family of Reed-Solomon codes. 
29|5||IFIP TCII - Aims, Scope and Technical Committee|
29|6|http://www.sciencedirect.com/science/journal/01674048/29/6|Contents|
29|6||Editorial|
29|6||A multi-layer Criticality Assessment methodology based on interdependencies|In this paper we propose a holistic Criticality Assessment methodology, suitable for the development of an infrastructure protection plan in a multi-sector or national level. The proposed methodology aims to integrate existing security plans and risk assessments performed in isolated infrastructures, in order to assess sector-wide or intra-sector security risks. In order to achieve this, we define three different layers of security assessments with different requirements and goals; the operator layer, the sector layer and the intra-sector or national layer. We determine the characteristics of each layer, as well as their interdependencies. In this way, existing security plans can be fully exploited in order to provide a “shortcut” for the development of security plans for complex inter-dependent infrastructures. A key element in the proposed methodology is the formal definition of interdependencies between different infrastructures and their respective sectors. Interdependencies between infrastructures belonging to the same or to a different sector, as well as interdependencies between different sectors, act as interfaces through which threats and their impacts occurring on different layers or different sectors, are conveyed to others. Current risk assessment methodologies fail to address effectively this issue, thus, the formalization of these interfaces and their interference is an important element for the definition of a holistic Criticality Assessment methodology. 
29|6||A probabilistic relational model for security risk analysis|Information system security risk, defined as the product of the monetary losses associated with security incidents and the probability that they occur, is a suitable decision criterion when considering different information system architectures. This paper describes how probabilistic relational models can be used to specify architecture metamodels so that security risk can be inferred from metamodel instantiations.A probabilistic relational model contains classes, attributes, and class-relationships. It can be used to specify architectural metamodels similar to class diagrams in the Unified Modeling Language. In addition, a probabilistic relational model makes it possible to associate a probabilistic dependency model to the attributes of classes in the architectural metamodel. This paper proposes a set of abstract classes that can be used to create probabilistic relational models so that they enable inference of security risk from instantiated architecture models. If an architecture metamodel is created by specializing the abstract classes proposed in this paper, the instantiations of the metamodel will generate a probabilistic dependency model that can be used to calculate the security risk associated with these instantiations. The abstract classes make it possible to derive the dependency model and calculate security risk from an instance model that only specifies assets and their relationships to each other. Hence, the person instantiating the architecture metamodel is not required to assess complex security attributes to quantify security risk using the instance model. 
29|6||On the detection of pod slurping attacks|Time is recognised to be a dimension of paramount importance in computer forensics. In this paper, we report on the potential of identifying past pod slurping type of attacks by constructing a synthetic metric based on information contained in filesystem timestamps. More specifically, by inferring the transfer rate of a file from last access timestamps and correlating that to the characteristic transfer rate capabilities of a suspicious USB found in the Windows registry, one could assess the probability of having suffered an unauthorised copy of files. Preliminary findings indicate that file transfer rates can be associated with the make and model of the USB storage device and give supporting information to the forensic analyst to identify file leakages. 
29|6||Implementing a passive network covert timing channel|The paper concerns passive network covert timing channels, in which the channel senders reside in intermediate nodes (e.g. router, gateway) and forward the passing-by packets in a carefully planned manner to covertly transmit the information. In this study, we focus on constructing and testing a kind of passive network covert timing channel, in which the information is hidden in the transmission interval between two adjacent packets. We first introduce three channel states to cope with the fluctuation in the traffic used as carrier, and explore how to select suitable values for the channel parameters to obtain high communication performance. We then implement an actual channel using Video On Demand (VOD) traffic as carrier, and obtain the communication characteristics of the channel. Finally, we investigate an information transmission scheme over the channel, including frame design, frame synchronization and error correction. 
29|6||PREON: An efficient cascade revocation mechanism for delegation paths|In decentralized network-based environments, resource sharing occurs more frequently as computing becomes more pervasive. Access to shared resources must be protected allowing access only to authorized entities. Delegation is a powerful mechanism to provide flexible and distributed access control when a user acts on another user’s behalf. User’s rights/attributes are contained in digital certificates and successive delegations generate chains of certificates. When an access control decision related to a delegation path has to be taken, its corresponding certificate chain has to be validated. Validation of long delegation paths is a costly process that might be critical when constrained devices are involved. In this article, we propose a mechanism called PREON (Prefix Revocation) which is based on prefix codes. PREON allows a privilege verifier to efficiently check a delegation chain when cascade revocation is enabled. We show by statistical analysis that our proposal outperforms delegation systems without prefix coding especially for long delegation paths and high revocation probabilities. 
29|6||A preliminary two-stage alarm correlation and filtering system using SOM neural network and K-means algorithm|Intrusion Detection Systems (IDSs) play a vital role in the overall security infrastructure. Although the IDS has become an essential part of corporate network infrastructure, the art of detecting intrusion is still far from perfect. A significant problem is that of false alarms, as generating a huge volume of such alarms could render the system inefficient. In this paper, we propose a new method to reduce the number of false alarms. We develop a two-stage classification system using a SOM neural network and K-means algorithm to correlate the related alerts and to further classify the alerts into classes of true and false alarms. Preliminary experiments show that our approach effectively reduces all superfluous and noisy alerts, which often contribute to more than 50% of false alarms generated by a common IDS. 
29|6||IFIP TCII - Aims, Scope and Technical Committee|
29|7|http://www.sciencedirect.com/science/journal/01674048/29/7|Contents|
29|7||Editorial|
29|7||Ethical decision making: Improving the quality of acceptable use policies|While there is extensive literature on the positive effects of institutionalising ethics in organisational culture, our extensive research in information security culture has found no evidence of organisations encouraging ethical decision making in situations where information security might be at risk. Security policies, in particular acceptable use policies, have traditionally been written with a strategy of deterrence in mind, but in practice they rely mostly on deontological ethics, i.e. employees doing the right thing, to work. As far back as 1990, evidence has been reported of a widening socio-technical gap, where employees no longer always act according to expected social norms in an organisation. This change in moral behaviour is reducing the effectiveness of acceptable use policies in an organisation. In this paper, an alternative approach to the development of security policies is proposed to encourage ethical decision making based on consequential ethics. Acceptable use policies will need to distinguish between guidelines, standards and procedures, and guidelines will need to be written in such a way that the policy continuously acknowledges that employees are no longer expected to blindly follow these guidelines. And, as acceptable use policies can no longer cover all the possible risks related to an employee’s behaviour, the policy will need to emphasise both explicitly an implicitly that employees are expected to make an ethical judgement on all their actions that may possibly endanger the organisation’s security. This will in turn have positive effects on the usability and suitability of the acceptable use policy to the organisation. 
29|7||Network anomaly detection through nonlinear analysis|Nowadays every network is susceptible on a daily basis to a significant number of different threats and attacks both from the inside and outside world. Some attacks only exploit system vulnerabilities and their traffic pattern is undistinguishable from normal behavior, but in many cases the attack mechanisms combine protocol or OS tampering activity with a specific traffic pattern having its own particular characteristics. Since these traffic anomalies are now conceived as a structural part of the overall network traffic, it is more and more important to automatically detect, classify and identify them in order to react promptly and adequately. In this work we present a novel approach to network-based anomaly detection based on the analysis of non-stationary properties and “hidden” recurrence patterns occurring in the aggregated IP traffic flows. In the observation of the above transition patterns for detecting anomalous behaviors, we adopted recurrence quantification analysis, a nonlinear technique widely used in many science fields to explore the hidden dynamics and time correlations of statistical time series. Our model demonstrated to be effective for providing a deterministic interpretation of recurrence patterns originated by the complex traffic dynamics observable during the occurrence of “noisy” network anomaly phenomena (characterized by measurable variations in the statistical properties of the traffic time series), and hence for developing qualitative and quantitative observations that can be reliably used in detecting such events. 
29|7||Efficient hardware support for pattern matching in network intrusion detection|Deep packet inspection forms the backbone of any Network Intrusion Detection (NID) system. It involves matching known malicious patterns against the incoming traffic payload. Pattern matching in software is prohibitively slow in comparison to current network speeds. Due to the high complexity of matching, only FPGA (Field-Programmable Gate Array) or ASIC (Application-Specific Integrated Circuit) platforms can provide efficient solutions. FPGAs facilitate target architecture specialization due to their field programmability. Costly ASIC designs, on the other hand, are normally resilient to pattern updates. Our FPGA-based solution performs high-speed pattern matching while permitting pattern updates without resource reconfiguration. To its advantage, our solution can be adopted by software and ASIC realizations, however at the expense of much lower performance and higher price, respectively. Our solution permits the NID system to function while pattern updates occur. An off-line optimization method first finds common sub-patterns across all the patterns in the SNORT database of signatures. A novel technique then compresses each pattern into a bit vector, where each bit represents such a sub-pattern. This approach reduces drastically the required on-chip storage as well as the complexity of pattern matching. The bit vectors for newly discovered patterns can be generated easily using a simple high-level language program before storing them into the on-chip RAM. Compared to earlier approaches, not only is our strategy very efficient while supporting runtime updates but it also results in impressive area savings; it utilizes just 0.052 logic cells for processing and 17.77 bits for storage per character in the current SNORT database of 6455 patterns. Also, the total number of logic cells for processing the traffic payload does not change with pattern updates. 
29|7||A framework for security assurance of access control enforcement code|Modeling of access control policies, along with their implementation in code, must be an integral part of the software development process, to ensure that the proper level of security in an application is attained. Previous work of the authors in this area yielded a framework that incorporates access control at the design and code levels, through a set of new extensions to UML and a set of approaches to enfoce access control in an application (Pavlich-Mariscal et al., 2010). An essential property of the code that has not been addressed by that framework is security assurance, which, in the context of this research, is to insure that the application code behaves consistently with the access control policy. This paper proposes a security assurance mechanism that formalizes the application behavior using labeled transition systems and structural operational semantics ( Plotkin, 1981). Simulation relations ( Milner, 1971) are used to demonstrate the correctness of the access control code with respect to the design. To validate the approach, this paper proves correctness of two access control enforcement mechanisms that are part of our case study: a basic approach to implement access control in code and an aspect-oriented approach. 
29|7||Approach for selecting the most suitable Automated Personal Identification Mechanism (ASMSA)|Automated Personal Identification Mechanisms (APIMs), used in the identification of individuals, operate in diverse environments ranging from border control policing to on-line banking. APIM solutions, however, have vulnerabilities and some attract societal issues. In this paper we describe ASMSA, a qualitative approach for selecting the most suitable mechanism to automatically identify individuals for a given context. ASMSA determines the optimum APIM by prioritising stakeholders’ objectives and mapping these to comprehensive requirements in order to articulate suitability as measurable attributes. ASMSA includes an evaluation framework and a selection method that builds on our earlier work, which established over 200 criteria to evaluate APIMs. ASMSA’s selection method includes Multiple-Stakeholder Processes (MSPs) and the use of Multi-Objective Multi-Criteria (MOMC) decision-making techniques. ASMSA is designed to ensure that relevant stakeholders’ needs and preferences are considered effectively prior to the assessment of candidate solutions. 
29|7||IFIP TCII - Aims, Scope and Technical Committee|
29|8|http://www.sciencedirect.com/science/journal/01674048/29/8|Contents|
29|8||Security, technology, publishing, and ethics (Part I)|
29|8||Modeling and preventing TOCTTOU vulnerabilities in Unix-style file systems|TOCTTOU (Time-of-Check-To-Time-Of-Use) is a file-based race condition in Unix-style systems and characterized by a pair of file object access by a vulnerable program: a check operation establishes certain conditions about the file object (e.g., the file exists), followed by a use operation that assumes that the established condition still holds. Due to the lack of support for transactions in Unix-style file systems, an attacker can modify the established file condition in-between the check and use steps, thus causing significant harm. In this paper, we present a model of the TOCTTOU problem (called STEM), which enumerates all the potential file system call pairs (called exploitable TOCTTOU pairs) that form the check/use steps. The model shows that a successful TOCTTOU attack requires a change in the mapping of pathname to logical disk blocks between the check and use steps. We apply STEM to POSIX and Linux to demonstrate its practical value for Unix-style file systems. Then we propose a defense mechanism (called EDGI) that prevents an attacker from tampering with the file condition between exploitable TOCTTOU pairs during a vulnerable program’s execution. EDGI works at the file system level and does not require existing applications to change. We have implemented EDGI on Linux kernel 2.4.28 and our evaluation shows that EDGI is effective and incurs little overhead to application benchmarks such as Andrew and Postmark. 
29|8||A behaviorist perspective on corporate harassment online: Validation of a theoretical model of psychological motives|Cyber harassment has been seen in the literature as a problem among school-aged children, or at the adult-level, as a legal problem involving attacks that have typically been associated with retaliation for some psychosocial or monetary gain. However, with the rise of the social networking phenomenon, cyber harassment has spread from school-aged children to a wider developmental and behavioral phenomenon. Companies and professionals are increasingly the targets of personal attacks on social network sites and in blog postings by “trolls” and “cyber bullies”. Thus rather than gaining information, these kinds of attacks often disseminate misleading or false information to damage their targets, to interfere with them, or for the purposes of extortion. We conducted a randomized field study of a bounded population into motivations that may lead people to conduct these kinds of attacks, we validated the model empirically, and we drew some potential behavioral responses along with implications for further research. 
29|8||Cyber security for home users: A new way of protection through awareness enforcement|We are currently living in an age, where the use of the Internet has become second nature to millions of people. Not only businesses depend on the Internet for all types of electronic transactions, but more and more home users are also experiencing the immense benefit of the Internet.However, this dependence and use of the Internet bring new and dangerous risks. This is due to increasing attempts from unauthorised third parties to compromise private information for their own benefit – the whole wide area of cyber crime.It is therefore essential that all users understand the risks of using Internet, the importance of securing their personal information and the consequences if this is not done properly.It is well known that home users are specifically vulnerable, and that cyber criminals have such users squarely in their target. This vulnerability of home users is due to many factors, but one of the most important one is the fact that such home users in many cases are not aware of the risks of using the Internet, and often venture into cyber space without any awareness preparation for this journey.This paper specifically investigates the position of the home user, and proposes a new model, the E-Awareness Model (E-AM), in which home users can be forced to acquaint themselves with the risks involved in venturing into cyber space. The E-AM consists of two components: the awareness component housed in the E-Awareness Portal, and the enforcement component.This model proposes a way to improve information security awareness among home users by presenting some information security content and enforcing the absorption of this content.The main difference between the presented model and other existing information security awareness models is that in the presented model the acquiring/absorption of the awareness content is compulsory – the user is forced to proceed via the E-Awareness Portal without the option of bypassing it. 
29|8||Access control for smarter healthcare using policy spaces|A fundamental requirement for the healthcare industry is that the delivery of care comes first and nothing should interfere with it. As a consequence, the access control mechanisms used in healthcare to regulate and restrict the disclosure of data are often bypassed in case of emergencies. This phenomenon, called “break the glass”, is a common pattern in healthcare organizations and, though quite useful and mandatory in emergency situations, from a security perspective, it represents a serious system weakness. Malicious users, in fact, can abuse the system by exploiting the break the glass principle to gain unauthorized privileges and accesses.In this paper, we propose an access control solution aimed at better regulating break the glass exceptions that occur in healthcare systems. Our solution is based on the definition of different policy spaces, a language, and a composition algebra to regulate access to patient data and to balance the rigorous nature of traditional access control systems with the “delivery of care comes first” principle. 
29|8||A game-based intrusion detection mechanism to confront internal attackers|Insiders might threaten organizations’ systems any time. By interacting with a system, an insider plays games with the security mechanisms employed to protect it. We apply game theory to model these interactions in an extensive form game that is being played repeatedly with an Intrusion Detection System (IDS). The outcomes of the game are quantified by first specifying players’ preferences, and then, by using the von Neumann–Morgenstern utility function, to assign numbers that reflect these preferences. Examining players’ best responses, the solution of the game follows by locating all the Nash Equilibria (NE). We extend the NE notion to the logit Quantal Response Equilibrium (QRE), to capture players’ bounded rationality and model insider’s behavior. The QRE results are more realistic, and show that the solution of the game might be significantly different than the corresponding NE solution. Thus, we determine how an insider will interact in the future, and how an IDS will react to protect the system. To easily exploit QRE results in ID, we propose the use of a detection mechanism. To present a possible implementation scheme of the detection mechanism, we give the application model and a detailed game-based detection algorithm.Categories and Subject Descriptors: C.2.0 [Computer-Communication Networks]: General -- Security and protection. D.4.6 [Operating Systems]: Security and Protection. 
29|8||Classifying data from protected statistical datasets|Statistical Disclosure Control (SDC) is an active research area in the recent years. The goal is to transform an original dataset X into a protected one X′, such that X′ does not reveal any relation between confidential and (quasi-)identifier attributes and such that X′ can be used to compute reliable statistical information about X.Many specific protection methods have been proposed and analyzed, with respect to the levels of privacy and utility that they offer. However, when measuring utility, only differences between the statistical values of X and X′ are considered. This would indicate that datasets protected by SDC methods can be used only for statistical purposes.We show in this paper that this is not the case, because a protected dataset X′ can be used to construct good classifiers for future data. To do so, we describe an extensive set of experiments that we have run with different SDC protection methods and different (real) datasets. In general, the resulting classifiers are very good, which is good news for both the SDC and the Privacy-preserving Data Mining communities. In particular, our results question the necessity of some specific protection methods that have appeared in the privacy-preserving data mining (PPDM) literature with the clear goal of providing good classification. 
29|8||A method for forensic analysis of control|This paper examines technical underpinnings for the notion of control as identified in laws and regulations in order to provide a technical basis for performing forensic analysis of digital forensic evidence in cases where taking control over systems or mechanisms is the issue. 
29|8||IFIP TCII - Aims, Scope and Technical Committee|
29|8||Invitation to IFIP sec 2011|
30|1|http://www.sciencedirect.com/science/journal/01674048/30/1|Contents|
30|1||Editorial|
30|1||Security, technology, publishing, and ethics (part II)|
30|1||WhiteScript: Using social network analysis parameters to balance between browser usability and malware exposure|Drive-by-download malware exposes internet users to infection of their personal computers, which can occur simply by visiting a website containing malicious content. This can lead to a major threat to the user’s most sensitive information. Popular browsers such as Firefox, Internet Explorer and Maxthon have extensions that block JavaScript, Flash and other executable content. Some extensions globally block all dynamic content, and in others the user needs to specifically enable the content for each site (s)he trusts. Since most of the web-pages today contain dynamic content, disabling them damages user experience and page usability, and that prevents many users from installing security extensions. We propose a novel approach, based on Social Network Analysis parameters, that predicts the user trust perspective for the HTML page currently being viewed. Our system examines the URL that appears in the address bar of the browser and each of the inner HTML URL reputations, and only if all of them have a reputation greater than our predetermined threshold, it marks the webpage as trusted. Each URL reputation is calculated based on the number and quality of the links on the whole web pointing back to the URL. The method was examined on a corpus of 44,429 malware domains and on the top 2000 most popular Alexa sites. Our system managed to enable dynamic content of 70% of the most popular websites and block 100% of malware web-pages, all without any user intervention. Our approach can augment most browser security applications and enhance their effectiveness, thus encouraging more users to install these important applications. 
30|1||A study of self-propagating mal-packets in sensor networks: Attacks and defenses|Since sensor applications are implemented in embedded computer systems, cyber attacks that compromise regular computer systems via exploiting memory-related vulnerabilities present similar threats to sensor networks. However, the paper shows that memory fault attacks in sensors are not the same as in regular computers due to sensor’s hardware and software architecture. In contrast to worm attacks, mal-code carried by exploiting packets cannot be executed in sensors built upon Harvard architecture. Therefore, the paper proposes a range of attack approaches to illustrate that a mal-packet, which only carries specially crafted data, can exploit memory-related vulnerabilities and utilize existing application code in a sensor to propagate itself without disrupting the sensor’s functionality. The paper shows that such a mal-packet can have as few as 17 bytes. A prototype of a 27-byte mal-packet has been implemented and tested in Mica2 sensors. Simulation shows that the propagation pattern of such a mal-packet in a sensor network is very different from worm propagation. Mal-packets can either quickly take over the whole network or hardly propagate under different traffic situations. The paper also develops two defense schemes (S2Guard and S2Shuffle) based on existing defense techniques to protect sensor applications. The analysis shows that they only incur a little overhead and can stop the propagation of mal-packets. 
30|1||An efficient and non-interactive hierarchical key agreement protocol|The non-interactive identity-based key agreement schemes are believed to be applicable to mobile ad-hoc networks (MANETs) that have a hierarchical structure such as hierarchical military MANETs. It was observed by Gennaro et al. (2008) that there is still an open problem on the security of the existing schemes, i.e., how to achieve the desirable security against corrupted nodes in the higher levels of a hierarchy? In this paper, we propose a novel and very efficient non-interactive hierarchical identity-based key agreement scheme that solves the open problem and outperforms all existing schemes in terms of computational efficiency and data storage. 
30|1||Designing a cluster-based covert channel to evade disk investigation and forensics|Data confidentiality on a computer can be achieved using encryption. However, encryption is ineffective under a forensic investigation mainly because the presence of encrypted data on a disk can be easily detected and disk owners can subsequently be forced (by law or other means) to release decryption keys. To evade forensic investigation, intelligent information hiding techniques that support plausible deniability have been proposed as an alternative to encryption; plausible deniability allows an evader to hide data in a manner such that he/she can deny the very existence of the data.In this paper, we present a new, plausible deniability approach to store sensitive information on a cluster-based filesystem. Under the proposed approach, a covert channel is used to encode the sensitive information by modifying the fragmentation patterns in the cluster distribution of an existing file. As opposed to existing schemes, the proposed covert channel does not require storage of any additional information on the filesystem. Moreover, the channel provides two-fold plausible deniability so that an investigator without the key cannot prove the presence of hidden information.We derive the theoretical capacity of the covert channel and show that a capacity of up to 24 bits/cluster can be achieved on a half-empty disk. The proposed data hiding and recovery algorithms are implemented on FAT32 based disk drives and we show that the disk (read/write) access time of the algorithms is quite low as compared to the contemporary approaches. We also present statistics about the incidence of file fragmentation on actual file systems from 52 disk drives belonging to a diverse set of users. Based on these statistics, we present guidelines for selecting good cover files. Finally, we show that even if an investigator gets suspicious, he/she will incur an unreasonably high O(m2) complexity to reveal an m bit hidden message. 
30|1||Modeling vulnerability discovery process in Apache and IIS HTTP servers|Vulnerability discovery models allow prediction of the number of vulnerabilities that are likely to be discovered in the future. Hence, they allow the vendors and the end users to manage risk by optimizing resource allocation. Most vulnerability discovery models proposed use the time as an independent variable. Effort-based modeling has also been proposed, which requires the use of market share data. Here, the feasibility of characterizing the vulnerability discovery process in the two major HTTP servers, Apache and IIS, is quantitatively examined using both time and effort-based vulnerability discovery models, using data spanning more than a decade. The data used incorporates the effect of software evolution for both servers. In addition to aggregate vulnerabilities, different groups of vulnerabilities classified using both the error types and severity levels are also examined. Results show that the selected vulnerability discovery models of both types can fit the data of the two HTTP servers very well. Results also suggest that separate modeling for an individual class of vulnerabilities can be done. In addition to the model fitting, predictive capabilities of the two models are also examined. The results demonstrate the applicability of quantitative methods to widely-used products, which have undergone evolution. 
30|1||A comparative evaluation of intrusion detection architectures for mobile ad hoc networks|Mobile Ad Hoc Networks (MANETs) are susceptible to a variety of attacks that threaten their operation and the provided services. Intrusion Detection Systems (IDSs) may act as defensive mechanisms, since they monitor network activities in order to detect malicious actions performed by intruders, and then initiate the appropriate countermeasures. IDS for MANETs have attracted much attention recently and thus, there are many publications that propose new IDS solutions or improvements to the existing. This paper evaluates and compares the most prominent IDS architectures for MANETs. IDS architectures are defined as the operational structures of IDSs. For each IDS, the architecture and the related functionality are briefly presented and analyzed focusing on both the operational strengths and weaknesses. Moreover, methods/techniques that have been proposed to improve the performance and the provided security services of those are evaluated and their shortcomings or weaknesses are presented. A comparison of the studied IDS architectures is carried out using a set of critical evaluation metrics, which derive from: (i) the deployment, architectural, and operational characteristics of MANETs; (ii) the special requirements of intrusion detection in MANETs; and (iii) the carried analysis that reveals the most important strengths and weaknesses of the existing IDS architectures. The evaluation metrics of IDSs are divided into two groups: the first one is related to performance and the second to security. Finally, based on the carried evaluation and comparison a set of design features and principles are presented, which have to be addressed and satisfied in future research of designing and implementing IDSs for MANETs. 
30|1||IFIP TCII - Aims, Scope and Technical Committee|
30|1||IFIPTM 2011 â Call for Contributions|
30|1||Call for membership|
30|1||WISE 7 - Call for Papers|
30|2-3|http://www.sciencedirect.com/science/journal/01674048/30/2-3|Contents|
30|2-3||Editorial|
30|2-3||Fine-grained integration of access control policies|Collaborative and distributed applications, such as dynamic coalitions and virtualized grid computing, often require integrating access control policies of collaborating parties. Such an integration must be able to support complex authorization specifications and the fine-grained integration requirements that the various parties may have. In this paper, we introduce an algebra for fine-grained integration of sophisticated policies. The algebra, which consists of three binary and two unary operations, is able to support the specification of a large variety of integration constraints. For ease of use, we also introduce a set of derived operators and provide guidelines for users to edit a policy with desired properties. To assess the expressive power of our algebra, we define notion of completeness and prove that our algebra is complete and minimal with respect to the notion. We then propose a framework that uses the algebra for the fine-grained integration of policies expressed in XACML. We also present a methodology for generating the actual integrated XACML policy, based on the notion of Multi-Terminal Binary Decision Diagrams. Experimental results have demonstrated both effectiveness and efficiency of our approach. In addition, we also discuss issues regarding obligations. 
30|2-3||Semantic web-based social network access control|The existence of online social networks that include person specific information creates interesting opportunities for various applications ranging from marketing to community organization. On the other hand, security and privacy concerns need to be addressed for creating such applications. Improving social network access control systems appears as the first step toward addressing the existing security and privacy concerns related to online social networks. To address some of the current limitations, we have created an experimental social network using synthetic data which we then use to test the efficacy of the semantic reasoning based approaches we have previously suggested. 
30|2-3||Patient-centric authorization framework for electronic healthcare services|In modern healthcare environments, a fundamental requirement for achieving continuity of care is the seamless access to distributed patient health records in an integrated and unified manner, directly at the point of care. However, Electronic Health Records (EHRs) contain a significant amount of sensitive information, and allowing data to be accessible at many different sources increases concerns related to patient privacy and data theft. Access control solutions must guarantee that only authorized users have access to such critical records for legitimate purposes, and access control policies from distributed EHR sources must be accurately reflected and enforced accordingly in the integrated EHRs. In this paper, we propose a unified access control scheme that supports patient-centric selective sharing of virtual composite EHRs using different levels of granularity, accommodating data aggregation and privacy protection requirements. We also articulate and address issues and mechanisms on policy anomalies that occur in the composition of discrete access control policies from different data sources. 
30|2-3||Security analysis of GTRBAC and its variants using model checking|Security analysis is a formal verification technique to ascertain certain desirable guarantees on the access control policy specification. Given a set of access control policies, a general safety requirement in such a system is to determine whether a desirable property is satisfied in all the reachable states. Such an analysis calls for the use of formal verification techniques. While formal analysis on traditional Role Based Access Control (RBAC) has been done to some extent, recent extensions to RBAC lack such an analysis. In this paper, we consider the temporal RBAC extensions and propose a formal technique using timed automata to perform security analysis by analyzing both safety and liveness properties. Using safety properties one ensures that something bad never happens while liveness properties show that some good state is also achieved. GTRBAC is a well accepted generalized temporal RBAC model which can handle a wide range of temporal constraints while specifying different access control policies. Analysis of such a model involves a process of mapping a GTRBAC based system into a state transition system. Different reduction rules are proposed to simplify the modeling process depending upon the constraints supported by the system. The effect of different constraints on the modeling process is also studied. 
30|2-3||Symbolic reachability analysis for parameterized administrative role-based access control|Role-based access control (RBAC) is a widely used access control paradigm. In large organizations, the RBAC policy is managed by multiple administrators. An administrative role-based access control (ARBAC) policy specifies how each administrator may change the RBAC policy. It is often difficult to fully understand the effect of an ARBAC policy by simple inspection, because sequences of changes by different administrators may interact in unexpected ways. ARBAC policy analysis algorithms can help by answering questions, such as user-role reachability, which asks whether a given user can be assigned to given roles by given administrators.Allowing roles and permissions to have parameters significantly enhances the scalability, flexibility, and expressiveness of ARBAC policies. This paper defines PARBAC, which extends the classic ARBAC97 model to support parameters, proves that user-role reachability analysis for PARBAC is undecidable when parameters may range over infinite types, and presents a semi-decision procedure for reachability analysis of PARBAC. To the best of our knowledge, this is the first analysis algorithm specifically for parameterized ARBAC policies. We evaluate its efficiency by analyzing its parameterized complexity and benchmarking it on case studies and synthetic policies. We also experimentally evaluate the effectiveness of several optimizations. 
30|2-3||IFIP TCII - Aims, Scope and Technical Committee|
30|2-3||HAISA - Call for papers|
30|4|http://www.sciencedirect.com/science/journal/01674048/30/4|Contents|
30|4||Editorial|
30|4||Universally composable and customizable post-processing for practical quantum key distribution|In quantum key distribution (QKD), a secret key is generated between two distant parties by transmitting quantum states. Experimental measurements on the quantum states are then transformed to a secret key by classical post-processing. Here, we propose a construction framework in which QKD classical post-processing can be custom made. Though seemingly obvious, the concept of concatenating classical blocks to form a whole procedure does not automatically apply to the formation of a quantum cryptographic procedure since the security of the entire QKD procedure rests on the laws of quantum mechanics and classical blocks are originally designed and characterized without regard to any properties of these laws. Nevertheless, we justify such concept of concatenating classical blocks in constructing QKD classical post-processing procedures, along with a relation to the universal-composability-security parameter. Consequently, effects arising from an actual QKD experiment, such as those due to the finiteness of the number of signals used, can be dealt with by employing suitable post-processing blocks. Lastly, we use our proposed customizable framework to build a comprehensive generic recipe for classical post-processing that one can follow to derive a secret key from the measurement outcomes in an actual experiment. 
30|4||Legally âreasonableâ security requirements: A 10-year FTC retrospective|Growth in electronic commerce has enabled businesses to reduce costs and expand markets by deploying information technology through new and existing business practices. However, government laws and regulations require businesses to employ reasonable security measures to thwart risks associated with this technology. Because many security vulnerabilities are only discovered after attacker exploitation, regulators update their interpretation of reasonable security to stay current with emerging threats. With a focus on determining what businesses must do to comply with these changing interpretations of the law, we conducted an empirical, multi-case study to discover and measure the meaning and evolution of “reasonable” security by examining 19 regulatory enforcement actions by the U.S. Federal Trade Commission (FTC) over a 10 year period. The results reveal trends in FTC enforcement actions that are institutionalizing security knowledge as evidenced by 39 security requirements that mitigate 110 legal security vulnerabilities. 
30|4||Extending the enforcement power of truncation monitors using static analysis|Runtime monitors are a widely used approach to enforcing security policies. Truncation monitors are based on the idea of truncating an execution before a violation occurs. Thus, the range of security policies they can enforce is limited to safety properties. The use of an a priori static analysis of the target program is a possible way of extending the range of monitorable properties. This paper presents an approach to producing an in-lined truncation monitor, which draws upon the above intuition. Based on an a priori knowledge of the program behavior, this approach allows, in some cases, to enforce more than safety properties and is more powerful than a classical truncation mechanism. We provide and prove a theorem stating that a truncation enforcement mechanism considering only the set of possible executions of a specific program is strictly more powerful than a mechanism considering all the executions over an alphabet of actions. 
30|4||User perceptions of security and usability of single-factor and two-factor authentication in automated telephone banking|This paper describes an experiment to investigate user perceptions of the usability and security of single-factor and two-factor authentication methods in automated telephone banking. In a controlled experiment with 62 banking customers a knowledge-based, single-factor authentication procedure, based on those commonly used in the financial services industry, was compared with a two-factor approach where in addition to the knowledge-based step, a one-time passcode was generated using a hardware security token. Results were gathered on the usability and perceived security of the two methods described, together with call completion rates and call durations for the two methods. Significant differences were found between the two methods, with the two-factor version being perceived as offering higher levels of security than the single-factor authentication version; however, this gain was offset by significantly lower perceptions of usability, and lower ratings for convenience and ease of use for the two-factor version. In addition, the two-factor authentication version took longer for participants to complete. This research provides valuable empirical evidence of the trade-off between security and usability in automated systems. 
30|4||HMMPayl: An intrusion detection system based on Hidden Markov Models|Nowadays the security of Web applications is one of the key topics in Computer Security. Among all the solutions that have been proposed so far, the analysis of the HTTP payload at the byte level has proven to be effective as it does not require the detailed knowledge of the applications running on the Web server. The solutions proposed in the literature actually achieved good results for the detection rate, while there is still room for reducing the false positive rate.To this end, in this paper we propose HMMPayl, an IDS where the payload is represented as a sequence of bytes, and the analysis is performed using Hidden Markov Models (HMM). The algorithm we propose for feature extraction and the joint use of HMM guarantee the same expressive power of n – gram analysis, while allowing to overcome its computational complexity. In addition, we designed HMMPayl following the Multiple Classifiers System paradigm to provide for a better classification accuracy, to increase the difficulty of evading the IDS, and to mitigate the weaknesses due to a non optimal choice of HMM parameters. Experimental results, obtained both on public and private datasets, show that the analysis performed by HMMPayl is particularly effective against the most frequent attacks toward Web applications (such as XSS and SQL-Injection). In particular, for a fixed false positive rate, HMMPayl achieves a higher detection rate respect to previously proposed approaches it has been compared with. 
30|4||Modeling the behavior of users who are confronted with security mechanisms|In this paper, we describe a new approach to analyze the trade-off between usability and security frequently found in security-related user interfaces. The approach involves the simulation of potential user interaction behavior by a mixed probabilistic and rule-driven state machine. On the basis of the simulations, user behavior in security-relevant situations can be predicted and user interfaces optimizing intended behavior can be designed. The approach is evaluated in an artificial microworld setting which provides good control over the experimental factors guiding the behavior. A comparison of empirical and simulated behavior in this microworld shows that the approach is already able to accurately predict important aspects of user behavior toward security interfaces, but also identifies future work necessary to better cover all relevant aspects guiding this behavior in a real-world setting. 
30|4||Quantitative analysis of a certified e-mail protocol in mobile environments: A probabilistic model checking approach|Formal analysis techniques, such as probabilistic model checking, offer an effective mechanism for model-based performance and verification studies of communication systems’ behavior that can be abstractly described by a set of rules i.e., a protocol. This article presents an integrated approach for the quantitative analysis of the Certified E-mail Message Delivery (CEMD) protocol that provides security properties to electronic mail services. The proposed scheme employs a probabilistic model checking analysis and provides for the first time insights on the impact of CEMD’s error tolerance on computational and transmission cost. It exploits an efficient combination of quantitative analysis and specific computational and communication parameters, i.e., the widely used Texas Instruments TMS320C55x Family operating in an High Speed Downlink Packet Access (HSDPA) mobile environment, where multiple CEMD participants execute parallel sessions with high bit error rates (BERs). Furthermore, it offers a tool-assistant approach for the protocol designers and analysts towards the verification of their products under varying parameters. Finally, this analysis can be also utilized towards reliably addressing cost-related issues of certain communication protocols and deciding on their cost-dependent viability, taking into account limitations that are introduced by hardware specifications of mobile devices and noisy mobile environments. 
30|4||A secure multi-item e-auction mechanism with bid privacy|The recent focus within the auction field has been multi-item auctions where bidders are not restricted to buying only one item of the merchandise. It has been of practical importance in Internet auction sites and has been widely executed by them. In this paper, we concentrate on the use of the multi-item auction for task assignment scenarios and propose a novel PUPA auction protocol to solve the problem of bid privacy in multi-item auctions. A verifiable technique of shared key chain is proposed to find the winners without revealing the losing bid and bidder’s privacy. It can be shown that our new scheme is robust against cheating bidders. 
30|4||IFIP TCII - Aims, Scope and Technical Committee|
30|4||Call for membership|
30|5|http://www.sciencedirect.com/science/journal/01674048/30/5|Contents|
30|5||Editorial: Advances in network and system security|
30|5||Masquerade mimicry attack detection: A randomised approach|A masquerader is an (often external) attacker who, after succeeding in obtaining a legitimate user’s credentials, attempts to use the stolen identity to carry out malicious actions. Automatic detection of masquerading attacks is generally undertaken by approaching the problem from an anomaly detection perspective: a model of normal behaviour for each user is constructed and significant departures from it are identified as potential masquerading attempts. One potential vulnerability of these schemes lies in the fact that anomaly detection algorithms are generally susceptible to deception. In this work, we first investigate how a resourceful masquerader can successfully evade detection while still accomplishing his goals. For this, we introduce the concept of masquerade mimicry attacks, consisting of carefully constructed attacks that are not identified as anomalous. We then explore two different detection schemes to thwart such attacks. We first study the introduction of a blind randomisation strategy into a baseline anomaly detector. We then propose a more accurate algorithm, called Probabilistic Padding Identification (PPI) and based on the Kullback–Leibler divergence, which attempts to identify if a sufficiently anomalous attack is present within an apparently normal behavioural pattern. Our experimental results indicate that the PPI algorithm achieves considerably better detection quality than both blind randomised strategies and adversarial-unaware approaches. 
30|5||A pitfall in fingerprint bio-cryptographic key generation|The core of bio-cryptography lies in the stability of cryptographic keys generated from uncertain biometrics. It is essential to minimize every possible uncertainty during the biometric feature extraction process. In fingerprint feature extraction, it is perceived that pixel-level image rotation transformation is a lossless transformation process. In this paper, an investigation has been conducted on analyzing the underlying mechanisms of fingerprint image rotation processing and potential effect on the major features, mainly minutiae and singular point, of the rotation transformed fingerprint. Qualitative and quantitative analyses have been provided based on intensive experiments. It is observed that the information integrity of the original fingerprint image can be significantly compromised by image rotation transformation process, which can cause noticeable singular point change and produce a non-negligible number of fake minutiae. It is found that the quantization and interpolation process can change the fingerprint features significantly without affecting the visual image. Experiments show that up to 7% bio-cryptographic key bits can be affected due to this rotation transformation. 
30|5||Hierarchical attribute-based encryption and scalable user revocation for sharing data in cloud servers|With rapid development of cloud computing, more and more enterprises will outsource their sensitive data for sharing in a cloud. To keep the shared data confidential against untrusted cloud service providers (CSPs), a natural way is to store only the encrypted data in a cloud. The key problems of this approach include establishing access control for the encrypted data, and revoking the access rights from users when they are no longer authorized to access the encrypted data. This paper aims to solve both problems. First, we propose a hierarchical attribute-based encryption scheme (HABE) by combining a hierarchical identity-based encryption (HIBE) system and a ciphertext-policy attribute-based encryption (CP-ABE) system, so as to provide not only fine-grained access control, but also full delegation and high performance. Then, we propose a scalable revocation scheme by applying proxy re-encryption (PRE) and lazy re-encryption (LRE) to the HABE scheme, so as to efficiently revoke access rights from users. 
30|5||Injecting purpose and trust into data anonymisation|Data anonymisation is of increasing importance for allowing sharing individual data among various data requesters for a variety of social network data analysis and mining applications. Most existing works of data anonymisation target at the optimization of the anonymisation metrics to balance the data utility and privacy, whereas they ignore the effects of a requester’s trust level and application purposes during the data anonymisation. Our aim of this paper is to propose a much finer level anonymisation scheme with regard to the data requester’s trust and specific application purpose. We firstly prioritize the attributes for anonymisation based on their importance to application purposes. Secondly, we build the projection between the trust value and the degree of data anonymiztion, which intends to determine to what extent the data should be anonymized. The decomposition algorithm is developed to find the desired anonymous solution, which ensures the uniqueness and correctness. Finally, we conduct extensive experiments on two real-world data sets and the results show the benefits of our approach for both data requesters and providers. 
30|5||IFIP TCII - Aims, Scope and Technical Committee|
30|6-7|http://www.sciencedirect.com/science/journal/01674048/30/6-7|Contents|
30|6-7||Editorial|
30|6-7||Data preprocessing for anomaly based network intrusion detection: A review|Data preprocessing is widely recognized as an important stage in anomaly detection. This paper reviews the data preprocessing techniques used by anomaly-based network intrusion detection systems (NIDS), concentrating on which aspects of the network traffic are analyzed, and what feature construction and selection methods have been used. Motivation for the paper comes from the large impact data preprocessing has on the accuracy and capability of anomaly-based NIDS. The review finds that many NIDS limit their view of network traffic to the TCP/IP packet headers. Time-based statistics can be derived from these headers to detect network scans, network worm behavior, and denial of service attacks. A number of other NIDS perform deeper inspection of request packets to detect attacks against network services and network applications. More recent approaches analyze full service responses to detect attacks targeting clients. The review covers a wide range of NIDS, highlighting which classes of attack are detectable by each of these approaches.Data preprocessing is found to predominantly rely on expert domain knowledge for identifying the most relevant parts of network traffic and for constructing the initial candidate set of traffic features. On the other hand, automated methods have been widely used for feature extraction to reduce data dimensionality, and feature selection to find the most relevant subset of features from this candidate set. The review shows a trend toward deeper packet inspection to construct more relevant features through targeted content parsing. These context sensitive features are required to detect current attacks. 
30|6-7||Logic-based approach for digital forensic investigation in communication Networks|In this paper, we provide a logic for digital investigation of security incidents and its high-level-specification language. The logic is used to prove the existence or non-existence of potential attack scenarios which, if executed on the investigated system, would produce the different forms of specified evidence. To generate executable attack scenarios showing with details how the attack scenario was conducted and how the system behaved accordingly, we develop in this paper a Model Checker tool which provides tolerance to unknown attacks and integrates a technique for hypothetical actions generation 
30|6-7||Modeling behavioral considerations related to information security|The authors present experimental and simulation results of an outcome-based learning model for the identification of threats to security systems. This model integrates judgment, decision-making, and learning theories to provide a unified framework for the behavioral study of upcoming threats. 
30|6-7||Compliance by design â Bridging the chasm between auditors and IT architects|System and process auditors assure – from an information processing perspective – the correctness and integrity of the data that is aggregated in a company’s financial statements. To do so, they assess whether a company’s business processes and information systems process financial data correctly. The audit process is a complex endeavor that in practice has to rely on simplifying assumptions. These simplifying assumptions mainly result from the need to restrict the audit scope and to focus it on the major risks. This article describes a generalized audit process. According to our experience with this process, there is a risk that material deficiencies remain undiscovered when said simplifying assumptions are not satisfied. To address this risk of deficiencies, the article compiles thirteen control patterns, which – according to our experience – are particularly suited to help information systems satisfy the simplifying assumptions. As such, use of these proven control patterns makes information systems easier to audit and IT architects can use them to build systems that meet audit requirements by design. Additionally, the practices and advice offered in this interdisciplinary article help bridge the gap between the architects and auditors of information systems and show either role how to benefit from an understanding of the other role’s terminology, techniques, and general work approach. 
30|6-7||Unconstrained keystroke dynamics authentication with shared secret|Among all the existing biometric modalities, authentication systems based on keystroke dynamics present interesting advantages. These solutions are well accepted by users and cheap as no additional sensor is required for authenticating the user before accessing to an application. In the last thirty years, many researchers have proposed, different algorithms aimed at increasing the performance of this approach. Their main drawback lies on the large number of data required for the enrollment step. As a consequence, the verification system is barely usable, because the enrollment is too restrictive. In this work, we propose a new method based on the Support Vector Machine (SVM) learning satisfying industrial conditions (i.e., few samples per user are needed during the enrollment phase to create its template). In this method, users are authenticated through the keystroke dynamics of a shared secret (chosen by the system administrator). We use the GREYC keystroke database that is composed of a large number of users (100) for validation purposes. We compared the proposed method with six methods from the literature (selected based on their ability to work with few enrollment samples). Experimental results show that, even though the computation time to build the template can be longer with our method (54 s against 3 s for most of the others), its performance outperforms the other methods in an industrial context (Equal Error Rate of 15.28% against 16.79% and 17.02% for the two best methods of the state-of-the-art, on our dataset and five samples to create the template, with a better computation time than the second best method). 
30|6-7||PrivaKERB: A user privacy framework for Kerberos|Kerberos is one of the most well-respected and widely used authentication protocols in open and insecure networks. It is envisaged that its impact will increase as it comprises a reliable and scalable solution to support authentication and secure service acquisition in the Next Generation Networks (NGN) era. This means however that security and privacy issues related to the protocol itself must be carefully considered. This paper proposes a novel two-level privacy framework, namely PrivaKERB, to address user privacy in Kerberos. Our solution offers two privacy levels to cope with user anonymity and service access untraceability. We detail how these modes operate in preserving user privacy in both single-realm and cross-realm scenarios. By using the extensibility mechanisms already available in Kerberos, PrivaKERB does not change the semantics of messages and enables future implementations to maintain interoperability. We also evaluate our solution in terms of service time and resource utilization. The results show that PrivaKERB is a lightweight solution imposing negligible overhead in both the participating entities and network. 
30|6-7||A survey of certified mail systems provided on the Internet|Over the last several years, an increasing number of certified mail systems have been put into place on the Internet. Governments, postal operators and private businesses now provide value-added electronic services that match the quality of postal certified mail. So far, there is no common view on the security properties that an electronic certified mail system has to provide. This applies to implementers and, surprisingly, also applies to the research community. All certified mail systems provided on the Internet are autonomous, and most are closed systems. However, recent developments call for cross-border certified mail communications that are similar to what we have become accustomed to in e-mail. This demand is emphasized by the ongoing implementation of the European Union (EU) Services Directive. The interoperability of certified mail systems is a new and challenging research field. The aim of this paper is to assess and discuss various standards and certified mail systems deployed on a large scale by drawing on the literature. This will facilitate interoperability efforts by offering a clearer view on the security properties that are actually applied in practice, as opposed to what is in research. We do this by classifying systems according to the security properties defined to date in the literature. Our findings show that standards and systems provided on the Internet have adopted many aspects of postal certified mail with respect to fairness, non-repudiation services and applied trust models. Nevertheless, there are still differences and incompatibilities, and the community must work toward common and interoperable systems. We encourage research into additional properties that could be applied in practice. 
30|6-7||Understanding the mindset of the abusive insider: An examination of insidersâ causal reasoning following internal security changes|Employees can have a profound, detrimental influence on information security that costs organizations billions of U.S. dollars annually. As a result, organizations implement stringent security controls, which can inadvertently foster the behaviors that they are designed to deter. This research attempts to understand this phenomenon of increased internal computer abuses by applying causal reasoning theory to explain employees’ causal-search process following the implementation of information security measures. Our findings show how interpersonal and environmental factors influence insiders’ beliefs that the organization trusts them (i.e., attributed trust) and how low attributed trust perceptions drive computer abuse incidents subsequent to security changes. We also highlight the need for both managers and security researchers to assess the frequency with which employees encounter information security changes within dynamic, organizational environments. 
30|6-7||Rights violation detection in multi-level digital rights management system|A multi-level digital rights management (DRM) system consists of owner, multiple levels of distributors and consumers. The distributors and consumers are given redistribution and usage rights using redistribution and usage licenses respectively. However, both redistribution and usage licenses can be violated by a malicious distributor/consumer to bypass the permissions and constraints in them. Thus, it is required to detect such violations. In this paper, we deal with the redistribution rights violation using a process called license validation. The problem of validation becomes complex when there exists multiple redistribution licenses for the same content with the distributors. In such cases, large number of comparisons may be needed and it may become difficult in real time in a large and distributed multi-level DRM system. Hence, we propose a bit-vector transform based license organization technique to do the validation efficiently. The proposed license organization method is compatible with the DRM related tasks such as license revocation and constraint range revocation. Mathematical analysis and experimental results show that the proposed license organization technique can do the validation efficiently. 
30|6-7||Feature representation and selection in malicious code detection methods based on static system calls|Currently almost all static methods for detecting malicious code are signature-based, this leads the result that viruses can easily escape detection by simple mechanisms such as code obfuscation. In this paper, a behavior-based detection approach is proposed to address this problem. The behaviors of interest are defined as static system call sequences. Unlike the traditional approach, which derives system call sequences by running executables (i.e., dynamic system call sequences), this approach statically analyzes binary code to derive system call sequences. In this paper, a method for deriving static system call sequences is presented, and two automatic feature-selection methods based on n-grams are proposed. We use machine-learning methods, including the K-nearest neighbor, Support Vector Machine, and decision tree methods to classify executables. The proposed approach is compared with the dynamic detection approach using dynamic system call sequences. The experimental results show that the proposed approach has higher accuracy and a lower false positive rate than the dynamic detection approach. 
30|6-7||Toward cost-sensitive self-optimizing anomaly detection and response in autonomic networks|While anomaly detection and response play a significant role in attaining auto defense, one of core functionalities of autonomic networks, the design and deployment of Anomaly Detection and Response Systems (ADRS) herein is a non-trivial issue because of the special network characteristic, namely self-managing, which requires candidate ADRS to automatically and optimally balance performance objectives and potential negative consequence. In this paper, we propose a decision-theoretic framework to systematically analyze ADRS in autonomic networks, with an objective to achieve its cost-sensitive and self-optimizing operation. In particular, each ADRS agent is viewed as an autonomous entity, making decision as its local operating environment. A global reward signal is then used to quantify the performance of ADRS as a whole in terms of those identified metrics. Furthermore, the analytical framework serves as a basis for developing an adaptive, robust, and near-optimal prototype termed ARSoS, along with a reinforcement learning algorithm for approximately inferring the optimal behavior of a reputation-based ADRS in a specific autonomic network variant, mobile ad-hoc network. The performance of ARSoS is validated through extensive simulations. 
30|6-7||Correlating TCP/IP Packet contexts to detect stepping-stone intrusion|Stepping-stone intrusion is one of the most popular techniques for attacking other computers, and detecting this form of intrusion and resisting intruders’ evasion are critical security issues. In this paper, we propose a new approach to this problem by introducing packet context to help detect stepping-stone intrusion. Pearson product-moment correlation coefficient is introduced to correlate packet context. The proposed approach does not need a threshold, and it is easily implemented. The experimental results show that the proposed approach can detect stepping-stone intrusion and resist intruders’ time-jittering and chaff-perturbation manipulation to an extent. 
30|6-7||IFIP TCII - Aims, Scope and Technical Committee|
30|8|http://www.sciencedirect.com/science/journal/01674048/30/8|Contents|
30|8||Editorial for 30/8|
30|8||Distributed Court System for intrusion detection in mobile ad hoc networks|Securing routing layer functions in mobile ad hoc networks is an important issue, which includes many challenges like how to enhance detection accuracy when facing the highly dynamic characteristic of such networks, and how to distinguish malicious accusations under a totally autonomous structure. In this paper, we propose Distributed Court System (DCS), a complete Intrusion Detection System that intends to solve these challenges in a low-cost and robust way. We do not deploy any centralized entity, but rely on the collaboration among the nodes neighbouring the suspected node, to integrate information, improve the detection accuracy, and reject dissemination of malicious accusation. Through mathematical analysis and simulation, the proposed DCS is proved to be effective in a highly mobile and hostile network environment. 
30|8||Countering unauthorized code execution on commodity kernels: A survey of common interfaces allowing kernel code modification|Motivated by the goal of hardening operating system kernels against rootkits and related malware, we survey the common interfaces and methods which can be used to modify (either legitimately or maliciously) the kernel which is run on a commodity desktop computer. We also survey how these interfaces can be restricted or disabled. While we concentrate mainly on Linux, many of the methods for modifying kernel code also exist on other operating systems, some of which are discussed. 
30|8||Fair digital signing: The structural reliability of signed documents|The exchange of digitally signed data inherits all the problems related to the indeterminacy of human communication, which are further intensified by the legal implications of signing. One of the fundamental intrinsic weaknesses of digital signatures is that the signer creates a signature on a series of bits, which may be differently transformed and perceived by the verifier (or relying party), due to the inevitable differences in the intention and the purpose of the two agents. As a result, syntactic and semantic distance is introduced between a signer and a relying party. In this paper we suggest a framework that models the process of digital signing, using several virtual and interrelated levels of communication, thereby promoting the analytic and synthetic exploration of the entities and the transformations involved. Based on this exploration, it is possible to indicate the favorable conditions for mutual understanding between the signer and the relying party. We focus on the syntactic and presentation levels of the communication process and we introduce the notion of structural reliability of a syntactic component, as a measure of how securely and accurately a signed document can be used. It is argued that structural reliability depends on a quantitative metric, such as the structural informativeness along with other qualitative characteristics of the syntactic component. The structural reliability of several document representation protocols is evaluated and it is concluded that the higher the informativeness of the protocol, the less the semantic distance produced, provided that the communicating parties have the capacity to handle this protocol. 
30|8||An analysis of the statistical disclosure attack and receiver-bound cover|Anonymous communications provides an important privacy service by keeping passive eavesdroppers from linking communicating parties. However, an attacker can use long-term statistical analysis of traffic sent to and from such a system to link senders with their receivers. Cover traffic is an effective, but somewhat limited, counter strategy against this attack. Earlier work in this area proposes that privacy-sensitive users generate and send cover traffic to the system. However, users are not online all the time and cannot be expected to send consistent levels of cover traffic; use of inconsistent cover traffic drastically reduces its impact. We propose that the anonymity system generate cover traffic that mimics the sending patterns of users in the system. This receiver-bound cover (RBC) helps to make up for users that aren’t there, confusing the attacker. To study the statistical disclosure attack and different cover traffic methods, we introduce an analytical method to bound the time for an attacker to identify a contact of Alice with high probability. We use these bounds to show that cover traffic sent by Alice greatly increases the time for attacker success, especially as the amount of traffic from other users increases. Further, we show that RBC greatly enhances the defense, forcing the attacker to take additional time proportional to the amount of cover used. We also examine the effectiveness of the attack and cover traffic when the attacker can only observe part of the traffic in the system. We validate our analysis through simulations that extend to realistic social networks. When RBC is used in combination with user generated cover traffic, the attack takes a very long time to succeed. 
30|8||Analysis of update delays in signature-based network intrusion detection systems|Network Intrusion Detection Systems (NIDS) play a fundamental role on security policy deployment and help organizations in protecting their assets from network attacks. Signature-based NIDS rely on a set of known patterns to match malicious traffic. Accordingly, they are unable to detect a specific attack until a specific signature for the corresponding vulnerability is created, tested, released and deployed. Although vital, the delay in the updating process of these systems has not been studied in depth. This paper presents a comprehensive statistical analysis of this delay in relation to the vulnerability disclosure time, the updates of vulnerability detection systems (VDS), the software patching releases and the publication of exploits. The widely deployed NIDS Snort and its detection signatures release dates have been used. Results show that signature updates are typically available later than software patching releases. Moreover, Snort rules are generally released within the first 100 days from the vulnerability disclosure and most of the times exploits and the corresponding NIDS rules are published with little difference. Implications of these results are drawn in the context of security policy definition. This study can be easily kept up to date due to the methodology used. 
30|8||Swarm intelligence in intrusion detection: A survey|Intrusion Detection Systems (IDS) have nowadays become a necessary component of almost every security infrastructure. So far, many different approaches have been followed in order to increase the efficiency of IDS. Swarm Intelligence (SI), a relatively new bio-inspired family of methods, seeks inspiration in the behavior of swarms of insects or other animals. After applied in other fields with success SI started to gather the interest of researchers working in the field of intrusion detection. In this paper we explore the reasons that led to the application of SI in intrusion detection, and present SI methods that have been used for constructing IDS. A major contribution of this work is also a detailed comparison of several SI-based IDS in terms of efficiency. This gives a clear idea of which solution is more appropriate for each particular case. 
30|8||Constant round group key agreement protocols: A comparative study|The scope of this paper is to review and evaluate all constant round Group Key Agreement (GKA) protocols proposed so far in the literature. We have gathered all GKA protocols that require 1,2,3,4 and 5 rounds and examined their efficiency. In particular, we calculated each protocol’s computation and communication complexity and using proper assessments we compared their total energy cost. The evaluation of all protocols, interesting on its own, can also serve as a reference point for future works and contribute to the establishment of new, more efficient constant round protocols. 
30|8||A taxonomy of self-modifying code for obfuscation|Self-modifying code is frequently used as an additional layer of complexity when obfuscating code. Although it does not provide a provable level of obfuscation, it is generally assumed to make attacks more expensive. This paper attempts to quantify the cost of attacking self-modified code by defining a taxonomy for it and systematically categorising an adversary’s capabilities. A number of published methods and techniques for self-modifying code are then classified according to both the taxonomy and the model. 
30|8||Combining sketches and wavelet analysis for multi time-scale network anomaly detection|With the rapid development and the increasing complexity of computer and communication systems and networks, traditional security technologies and measures can not meet the demand for integrated and dynamic security solutions. In this scenario, the use of Intrusion Detection Systems has emerged as a key element in network security.In this paper we address the problem proposing a wavelet-based technique able to detect network anomalies almost in real-time. In more detail, our approach is based on the combined use of sketches and wavelet analysis to reveal the anomalies in data collected at the router level. Moreover, to improve the detection rate we propose a multi time-scale analysis. The performance analysis, presented in this paper, demonstrates the effectiveness of the proposed method. 
30|8||Enforcing privacy in e-commerce by balancing anonymity and trust|Privacy is a major concern in e-commerce. There exist two main paradigms to protect the customer’s privacy: one relies on the customer’s trust that the network will conform to his privacy policy, the other one insists on the customer’s anonymity. A new paradigm is advanced here as a natural balance between these two. It sees the customer act using his real identity but only circulate cover data that conceal the resources he requires. Privacy enforcement is thus shifted from the customer’s identity to his purchase preferences. The new paradigm is suitable for scenarios such as eBay purchases where trust that a network sticks to a privacy policy is problematic, while anonymity is either forbidden or impossible.The computation of cover data is done by a node other than the customer in order to minimize impact on the customer. That node will therefore see the customer’s private data that are used to compute the cover. This demands some technology to prevent the node from exposing private data. An existing protocol developed for self-enforcing privacy in the area of e-polls is thoroughly analysed and found somewhat weak in terms of fairness among its participants. A stronger version is designed and adopted, together with an innovative differential-privacy preserving function, in the new privacy paradigm. The strengthened e-poll protocol and the new differential-privacy preserving function, which strictly speaking only are side contributions of this paper, each appear as important as the new e-commerce privacy paradigm. 
30|8||The cyber threat landscape: Challenges and future research directions|Cyber threats are becoming more sophisticated with the blending of once distinct types of attack into more damaging forms. Increased variety and volume of attacks is inevitable given the desire of financially and criminally-motivated actors to obtain personal and confidential information, as highlighted in this paper. We describe how the Routine Activity Theory can be applied to mitigate these risks by reducing the opportunities for cyber crime to occur, making cyber crime more difficult to commit and by increasing the risks of detection and punishment associated with committing cyber crime. Potential research questions are also identified. 
30|8||Masquerade detection using profile hidden Markov models|In this paper, we consider the problem of masquerade detection, based on user-issued UNIX commands. We present a novel detection technique based on profile hidden Markov models (PHMMs). For comparison purposes, we implement an existing modeling technique based on hidden Markov models (HMMs). We compare these approaches and show that, in general, our PHMM technique is competitive with HMMs. However, the standard test data set lacks positional information. We conjecture that such positional information would give our PHMM a significant advantage over HMM-based detection. To lend credence to this conjecture, we generate a simulated data set that includes positional information. Based on this simulated data, experimental results show that our PHMM-based approach outperforms other techniques when limited training data is available. 
30|8||Roles in information security â A survey and classification of the research area|The concept of roles has been prevalent in the area of Information Security for more than 15 years already. It promises simplified and flexible user management, reduced administrative costs, improved security, as well as the integration of employees’ business functions into the IT administration. A comprehensive scientific literature collection revealed more than 1300 publications dealing with the application of sociological role theory in the context of Information Security up to now. Although there is an ANSI/NIST standard and an ISO standard proposal, a variety of competing models and interpretations of the role concept have developed. The major contribution of this survey is a categorization of the complete underlying set of publications into different classes. The main part of the work is investigating 32 identified research directions, evaluating their importance and analyzing research tendencies. An electronic bibliography including all surveyed publications together with the classification information is provided additionally. As a final contribution potential future developments in the area of role-research are considered. 
30|8||Windows driver memory analysis: A reverse engineering methodology|In a digital forensics examination, the capture and analysis of volatile data provides significant information on the state of the computer at the time of seizure. Memory analysis is a premier method of discovering volatile digital forensic information. While much work has been done in extracting forensic artifacts from Windows kernel structures, less focus has been paid to extracting information from Windows drivers. There are two reasons for this: (1) source code for one version of the Windows kernel (but not associated drivers) is available for educational use and (2) drivers are generally called asynchronously and contain no exported functions. Therefore, finding the handful of driver functions of interest out of the thousands of candidates makes reverse code engineering problematic at best. Developing a methodology to minimize the effort of analyzing these drivers, finding the functions of interest, and extracting the data structures of interest is highly desirable. This paper provides two contributions. First, it describes a general methodology for reverse code engineering of Windows drivers memory structures. Second it applies the methodology to tcpip.sys, a Windows driver that controls network connectivity. The result is the extraction from tcpip.sys of the data structures needed to determine current network connections and listeners from the 32 and 64 bit versions of Windows Vista and Windows 7.Manipulation (DKOM), tcpip.sys, Windows 7, Windows Vista. 2000 MSC: 60, 490. 
30|8||Enable delegation for RBAC with Secure Authorization Certificate|Our motivation in this paper is to explore a Secure Delegation Scheme that could keep access control information hidden through network transmission. This approach introduces the quasirandom structure, 3-Uniform Hypergraph, as the representation structure for authorization information. It generates a Secure Authorization Certificate (SAC) in place of an Attribute Certificate (AC) to enable both Role-based Access Control (RBAC) and a delegation process for hiding authorization information. We have two contributions in this regard: (1) a value-based delegation scheme and (2) a pattern-based RBAC. A Secure Delegation Scheme is based on the hashing values generated with the quasirandom structure. With this scheme, the delegation process will greatly reduce the risk of sensitive authorization information leakage for applications. In the case of pattern-based access, we introduce a new hash function using quasirandom structure to make a fingerprint1 for RBAC. The quasirandom structure derived from k-Uniform Hypergraph has measurable uniformity, which is an advantage over traditional hash functions. Another advantage is that it does not need to access the entire message context to generate the fingerprint which is essential for traditional hash functions such as MD5, SHA-1, etc. 
30|8||Estimating botnet virulence within mathematical models of botnet propagation dynamics|Mathematical models of botnet propagation dynamics are increasingly deemed to have potential for significant contribution to botnet mitigation. Botnet virulence, which comprises network vulnerability rate and network infection rate, is a key factor in those models. In this paper we discuss a practical approach that draws on epidemiological models in biology to estimate the botnet virulence in a network. Our research provides mathematical models of botnet propagation dynamics with concrete measures of botnet virulence, which make those models practical and hence employable in mitigation of real world botnets in a timely fashion. The approach is based on random sampling and follows a novel application of statistical learning and inference in a botnet-versus-network setting. We have implemented this research in the Matlab programming language. In this paper, we discuss an experimental evaluation of the effectiveness of this research with respect to botnet propagation dynamics realistically simulated in a GTNetS network simulation platform. 
30|8||Cybercrime: Understanding and addressing the concerns of stakeholders|Cybercrime and cybercriminal activities continue to impact communities as the steady growth of electronic information systems enables more online business. The collective views of sixty-six computer users and organizations, that have an exposure to cybercrime, were analyzed using concept analysis and mapping techniques in order to identify the major issues and areas of concern, and provide useful advice. The findings of the study show that a range of computing stakeholders have genuine concerns about the frequency of information security breaches and malware incursions (including the emergence of dangerous security and detection avoiding malware), the need for e-security awareness and education, the roles played by law and law enforcement, and the installation of current security software and systems. While not necessarily criminal in nature, some stakeholders also expressed deep concerns over the use of computers for cyberbullying, particularly where younger and school aged users are involved. The government’s future directions and recommendations for the technical and administrative management of cybercriminal activity were generally observed to be consistent with stakeholder concerns, with some users also taking practical steps to reduce cybercrime risks. 
30|8||E2VoIP2: Energy efficient voice over IP privacy|Due to the convergence of telecommunication technologies and pervasive computing, voice is increasingly being transmitted over IP networks, in what is commonly known as Voice over IP (VoIP). Despite many advantages offered by this technology, VoIP applications inherit many challenging characteristics from the underlying IP network related to quality of service and security concerns. Traditional ways to secure data over IP networks have negative effects on real-time applications and on power consumption, which is scarce in power-constrained handheld devices. In this work, a new codec-independent Energy Efficient Voice over IP Privacy (E2VoIP2) algorithm is devised to limit the overhead of the encryption process, without compromising the end-to-end confidentiality of the conversation. The design takes advantage of VoIP stream characteristics to encrypt selected packets using a secure algorithm, while relaxing the encryption procedure in-between these packets. We evaluated experimentally the difficulty of conducting known plaintext attacks on VoIP by demonstrating that a sound recorded simultaneously by different sources results in apparently random encoded files. Regarding E2VoIP2, experimental and simulation results show a substantial improvement in terms of the number of CPU cycles which results in a reduction of latency and a reduction in consumed power with respect to that of the SRTP. In addition, the proposed method is flexible in terms of the balance between security and power consumption. 
30|8||An interactive mobile SMS confirmation method using secret sharing technique|As we all know, Short Message Service (SMS) has brought about junk emails or nonsense messages coming from advertisement providers, called SMS spam. It does bother subscribers and make them distress to check SMS messages of mobile system. Statistically, each mobile subscriber receives an average number of 8.29 short messages every week. Thus, to furnish legitimate message service to the mobile subscribers, engineers have strived to figure out an interactive service system which can certify the user-participation in a communicatory session. If a system can verify whether the communicating party is human being or not, the machine tries can be detected to mitigate the risk. To realize this essential, we aim to develop an interactive SMS confirmation mechanism using the famous techniques – CAPTCHA and secret sharing. Experimental results show that it takes slight computation costs to complete the authentication including the identity verification and the check of user-participation. This has led to predominance that the new method is suitable for mobile environment. 
volume|issue|url|title|abstract
31|-|http://www.sciencedirect.com/science/journal/01674048/31|Contents|
31|-||Editorial|
31|-||Managing information security risks during new technology adoption|In the present study, we draw on previous system dynamics research on operational transition and change of vulnerability to investigate the role of incident response capability in controlling the severity of incidents during the adoption of new technology. Toward this end, we build a system dynamics model using the Norwegian Oil and Gas Industry as the context. The Norwegian Oil and Gas Industry has started to adopt new information communication technology to connect its offshore platforms, onshore control centers, and suppliers. In oil companies, the management is generally aware of the increasing risks associated with operational transition; however, to date, investment in incident response capability has not been highly prioritized because of the uncertainty related to risks and the present reactive mental model of security risk management. The model simulation shows that a reactive approach to security risk management might trap the organization into blindness to minor incidents and low incident response capability, which can lead to severe incidents. The system dynamics model can serve as a means to promote proactive investment in incident response capability. 
31|-||A secure and efficient discovery service system in EPCglobal network|In recent years, the Internet of Things (IOT) has drawn considerable attention from the industrial and research communities. Due to the vast amount of data generated through IOT devices and users, there is an urgent need for an effective search engine to help us make sense of this massive amount of data. With this motivation, we begin our initial works on developing a secure and efficient search engine (SecDS) based on EPC Discovery Services (EPCDS) for EPCglobal network, an integral part of IOT. SecDS is designed to provide a bridge between different partners of supply chains to share information while enabling them to find who is in possession of an item. The most important property of SecDS is: while efficiently processing user's search, it is also secure. In order to prevent unauthorized access to SecDS, an extended attribute-based access control model is proposed and implemented such that information belonging to different companies can be protected using different policies. We design, implement SecDS and conduct extensive experiments on it. The results validate the practicality and cost effectiveness of our design and implementations. 
31|-||A framework for quantitative evaluation of parallel control-flow obfuscation|Software obfuscation is intended to protect a program by thwarting reverse engineering. Several types of software obfuscation have been proposed, and control-flow obfuscation is a commonly adopted one. In this paper, we present a framework to evaluate parallel control-flow obfuscation, which raises difficulty of reverse engineering by increasing parallelism of a program. We also define a control flow graph of a program and some atomic operators for obfuscating transformations. The proposed framework comprises three phases: parsing, formalization and evaluation. A program is first parsed to a control flow graph. Then, we formalize a parallel control-flow obfuscating transformation based on our atomic operators. By selecting target code blocks in the control flow graph and applying obfuscating transformations to the target code blocks, the original program is then obfuscated. In the third phase, we define a measure to calculate the program complexity. The measure can be considered as a degree to which an obfuscating transformation can confuse a human trying to understand the obfuscated program. Such a measure can also be used as the base of the potency metric to estimate the capability of the obfuscated program against reverse engineering. Our novel framework helps efficiently examine a control-flow obfuscating transformation in a systematic manner and helps select an appropriate obfuscating transformation among a number of candidates to better protect a program. 
31|-||Access control for online social networks third party applications|With the development of Web 2.0 technologies, online social networks are able to provide open platforms to enable the seamless sharing of profile data to enable public developers to interface and extend the social network services as applications. At the same time, these open interfaces pose serious privacy concerns as third party applications are usually given access to the user profiles. Current related research has focused on mainly user-to-user interactions in social networks, and seems to ignore the third party applications. In this paper, we present an access control framework to manage third party applications. Our framework is based on enabling the user to specify the data attributes to be shared with the application and at the same time be able to specify the degree of specificity of the shared attributes. We model applications as finite state machines, and use the required user profile attributes as conditions governing the application execution. We formulate the minimal attribute generalization problem and we propose a solution that maps the problem to the shortest path problem to find the minimum set of attribute generalization required to access the application services. We assess the feasibility of our approach by developing a proof-of-concept implementation and by conducting user studies on a widely-used social network platform. 
31|-||Optimal mining on security labels for decentralized information flow control|Decentralized information flow control (DIFC) is a key innovation of traditional information flow control (IFC). When compared with IFC, DIFC provides new features including decentralized declassification, taint-tracking and privilege-transferring. These characteristics make DIFC more applicable than traditional IFC to the control of information flows in systems. This paper presents an optimal approach to the mining of security labels for DIFC. This approach can effectively improve DIFC's applicability and manageability in a wide variety of environments. We firstly design a novel policy description language to express security requirements in DIFC characterized assertions. Next, we prove that the problem of obtaining security labels from DIFC assertions is NP-complete. Based on logic programming and genetic algorithm, the proposed approach finally outputs optimal security labels separately for different DIFC systems in both small and large-scale environments. The objectives of this paper are to address two practical aspects of DIFC: (1) how to express security requirements by using DIFC characterized assertions; (2) how to obtain optimal DIFC labels to satisfy security requirements. The experimental results show that the proposed approach is effective in implementing fine-grained information control according to practical security requirements. 
31|-||A survey of electronic ticketing applied to transport|A wide variety of transport systems can benefit from the use of Electronic Ticketing (ET). ET systems are progressively introduced in transports systems, and produce a reduction of the associated economic costs and time intervals, and the control of the system is improved. However, the use of ET systems enables various privacy abuses both in real-time and retrospect since the anonymity of users is not always guaranteed and, therefore, users can be traced and their profiles of usual movements can be created. In our review article, we classify and describe the main proposals with special focus on the properties related to user privacy. 
31|-||Trusted Domain: A security platform for home automation|In the digital age of home automation and with the proliferation of mobile Internet access, the intelligent home and its devices should be accessible at any time from anywhere. There are many challenges such as security, privacy, ease of configuration, incompatible legacy devices, a wealth of wireless standards, limited resources of embedded systems, etc. Taking these challenges into account, we present a Trusted Domain home automation platform, which dynamically and securely connects heterogeneous networks of Short-Range Wireless devices via simple non-expert user interactions, and allows remote access via IP-based devices such as smartphones.The Trusted Domain platform fits existing legacy technologies by managing their interoperability and access controls, and it seeks to avoid the security issues of relying on third-party servers outside the home. It is a distributed system that enables secure end-to-end communication with home automation devices, and it supports device revocations as well as a structure of intersecting sets of nodes for scalability. Devices in the Trusted Domain are registered in a list that is distributed using a robust epidemic protocol optimized for constrained resources and network load sharing. The resource-intensive encryption operations are reduced to a minimum by sending short signed update queries and only synchronizing when necessary. An experiment on an embedded implementation examines timing, footprint, and behavioral properties of the protocol. The protocol has been formally verified by the UPPAAL model-checking tool. 
31|-||Selecting key management schemes for WSN applications|Key management in wireless sensor networks (WSN) is an active research topic. Due to the fact that a large number of key management schemes (KMS) have been proposed in the literature, it is not easy for a sensor network designer to know exactly which KMS best fits in a particular WSN application. In this article, we offer a comprehensive review on how the application requirements and the properties of various key management schemes influence each other. Based on this review, we show that the KMS plays a critical role in determining the security performance of a WSN network with given application requirements. We also develop a method that allows the network designers to select the most suitable KMS for a specific WSN network setting. In addition, the article also addresses the issues on the current state-of-the-art research on the KMS for homogeneous (i.e. non-hierarchical) networks to provide solutions for establishing link-layer keys in various WSN applications and scenarios. 
31|-||A Hierarchical Visibility theory for formal digital investigation of anti-forensic attacks|Among the leading topics of research in digital forensic investigation is the development of theoretical and scientifically proven techniques of incident analysis. However, two main problems, which remain unsolved in the literature, could lead the use of formal approaches of attack scenarios reconstruction and incident analysis to be inconclusive. The former is related to the absence of techniques to model and characterize anti-forensic attacks, and cope with the reconstruction of attack scenarios based on evidences compromised by these attacks. The latter is related to the lack of theoretical techniques usable during the preparation of systems to forensic analysis (i.e., the first phase of a forensic process that precedes the occurrence of an incident and the collection of evidences). These techniques are expected to determine the optimal set of security solutions to deploy so that the evidences to be generated further to a security incident would be sufficient to prove a wide range of anti-forensic attacks.In this paper we propose a formal approach, based on a novel theory of Hierarchical Visibility, allowing to forensically investigate security incidents that are conducted over complex systems and integrate anti-forensic attacks. We develop a formal logic-based model useful for the representation of complex systems and scenarios of attacks under different levels of abstractions, and the description of the deployed security solutions together with the evidences they generated. The theory of Hierarchical Visibility that we provide in this paper allows reasoning on anti-forensic attacks over complex systems, characterize situations under which they are provable, and prove their occurrence starting from incomplete evidences. An extension of the forensic process showing the use of Hierarchical Visibility theory to increase the number of provable anti-forensic attacks, is described. We illustrate the proposal using a case study related to the investigation of a denial of service attack over an SSH service. 
31|-||Power to the people? The evolving recognition of human aspects of security|It is perhaps unsurprising to find much of the focus in IT and computer security being drawn towards the technical aspects of the discipline. However, it is increasingly recognised that technology alone cannot deliver a complete solution, and there is also a tangible need to address human aspects. At the core, people must understand the threats they face and be able to use the protection available to them, and although this has not been entirely ignored, it has not received the level of attention that it merits either. Indeed, security surveys commonly reveal that the more directly user-facing aspects such as policy, training and education are prone to receiving significantly less attention than technical controls such as firewalls, antivirus and intrusion detection. The underlying reason for such disparity is that the human aspects are in many ways a more challenging problem to approach, not least because they cannot be easily targeted with a product-based solution. There is also a direct overlap into the technical area, with issues such as the usability and acceptability of technology solutions having a direct impact upon the actual protection that they are able to deliver.This paper explores these themes, highlighting the need for human aspects to form part of a holistic security strategy alongside the necessary technologies. Taking the specific examples of security awareness and two user-facing technical controls (user authentication and antivirus), the discussion examines how things have evolved to the present day and considers how they need to be positioned for the future. 
31|-||Study on poll-site voting and verification systems|Voting is an important part of the democratic process. The electorate makes a decision or expresses an opinion that is accepted by everyone. However, some individual or group may be interested in tampering with the elections process to force an outcome in their favor. Hence, controlling the whole voting process to ensure that it is performed correctly and according to current rules and law is, then, even more important. In this work, we present a review of existing verification systems for paper-based and electronic voting systems in supervised environments, from both academic and commercial worlds. To do so, we perform a fair comparison of a set of representative voting verification systems using an evaluation framework. We define this framework to be composed of several properties, covering important system areas, ranging from user interaction to security issues. Then, we model the natural evolution of verifiability issues on notable voting systems from academia and commerce which are influenced by restrictions on current laws and by the advance of technology. 
31|-||Wishful thinking|
31|-||Handbook on Securing Cyber-Physical Critical Infrastructure: Foundations and Challenges|
31|-||Announcement|
31|-||Call for papers|
31|1|http://www.sciencedirect.com/science/journal/01674048/31/1|Contents|
31|1||Editorial|
31|1||Android forensics|
31|1||A trust negotiation based security framework for service provisioning in load-balancing clusters|The OKKAM project aims at enabling the Web of Entities, a global digital space for publishing and managing information about entities. The project provides a scalable and sustainable infrastructure, called the Entity Name System (ENS), for the systematic reuse of global and unique entity identifiers. The ENS provides a collection of core services supporting entity identifiers pervasive reuse. The ENS is required to be reliable data intensive load-balancing cluster system for service provisioning.Given the project’s successful outcome, this paper presents the ENS security framework and how it enables scalable secure service provisioning underpinned by trust negotiation based access control. A detailed security performance evaluation is given, with supporting conclusions of scalable and efficient security design and implementation. 
31|1||On scrambling the BurrowsâWheeler transform to provide privacy in lossless compression|The usual way of ensuring the confidentiality of the compressed data is to encrypt it with a standard encryption algorithm. Although the computational cost of encryption is practically tolerable in most cases, the lack of flexibility to perform pattern matching on the compressed data due to the encryption level is the main disadvantage. Another alternative to provide privacy in compression is to alter the compression algorithms in such a way that the decompression requires the knowledge of some secret parameters. Securing the arithmetic and Huffman coders along with the dictionary based schemes have been previously studied, where Burrows–Wheeler transform (BWT) has not been addressed before in that sense. On BWT of an input data it is not possible to perform a successful search nor construct any part of it without the proper knowledge of the lexicographical ordering used in the construction. Based upon this observation, this study investigates methods to provide privacy in BWT by using a randomly selected permutation of the input symbols as the lexicographical order. The proposed technique aims to support pattern matching on compressed data, while still retaining the confidentiality. Unifying compression and security in a single step is also considered instead of the two-level compress-then-encrypt paradigm. 
31|1||A robust hashing algorithm based on SURF for video copy detection|To protect digital video from unauthorized use, video copy detection is an active research topic in the field of copyright control. For content-based copy detection, the key issue is to extract robust transformation-invariant feature. In this paper, a robust hashing algorithm based on speeded up robust feature (SURF) and ordinal measure (OM) is proposed for video copy detection. Since SURF is an invariant feature based on scale space theory, the local feature is extracted by SURF in a frame-by-frame manner. Every frame is divided into 4 × 4 blocks, and every block is traversed by Hilbert-order rasterization to count the number of SURF points. The Hash value is built by the difference of SURF points between adjacent blocks in Hilbert curve. Moreover, two special copy attacks, i.e., picture-in-picture and video flipping, are specifically discussed. Experimental results show the effectiveness of the proposed approach in both accuracy and efficiency. 
31|1||A new robust adjustable logo watermarking scheme|In this paper, a novel, yet simple, watermarking algorithm for image authentication is proposed using fractional wavelet packet transform (FRWPT) via singular value decomposition (SVD). Unlike the traditional watermarking schemes where the watermark is added to the transform coefficients, the proposed algorithm is based on embedding in the singular values (luminance) of the host image. To improve the fidelity, the perceptual quality of the watermarked image and to enhance the security of watermarking, we model an adjustable watermarking algorithm. The meaning of the word adjustable is that the watermark is embedded into the host image by taking two watermark embedding strengths, according to owner and some cryptographic conditions. Finally, a reliable watermark extraction algorithm is developed for the extraction of watermark from the distorted image. The feasibility of this method and its robustness against different kind of attacks are verified by computer simulations and comparison with the existing work. 
31|1||Tracing and revoking scheme for dynamic privileges against pirate rebroadcast|Broadcast encryption provides a convenient method to distribute digital content to subscribers over an insecure broadcast channel so that only the qualified users can recover the data. Currently, there are only two broadcast encryption schemes designed for users with different privileges. In these schemes, users with higher privileges can decrypt more contents than those with low privileges, which is quite suitable for applications with different service levels. However, both schemes do not provide traitor tracing strategy. In this paper, we present a traitor tracing and revoking system for different privileges against pirate rebroadcast. We improve the tree structure and the media key block proposed by Jin and Lotspiech to suitable for applications with dynamic services, and then combine them with dynamic traitor tracing and revoking method by Kiayias and Pehlivanoglu. Dynamic services mean the users can change their privileges dynamically and the broadcast center can upgrade to hold more/less privileges when needed for marketing, while in both previous schemes the numbers of privileges are determined when the systems are setup and users’ privileges are static. Our scheme uses subset cover method to trace and revoke users so that it can trace unlimited numbers of traitors and revoke unlimited numbers of users. 
31|1||Feature extraction and classification algorithm for detecting complex covert timing channel|Owing to the high variance of legitimate traffic, the detection of Covert Timing Channel (CTC) has become a challenging work. The combination of detection methods based on entropy and corrected conditional entropy has been proved an effective way for the detection against some typical CTCs. However, the methods cannot satisfy the detection of some complex CTCs. In this paper, based on wavelet transform and Support Vector Machine (SVM), a new approach is proposed to detect various kinds of CTCs inclusive of some complex CTCs. Our approach can extract the features of maximum entropies at different wavelet levels and the percentage of energy corresponding to the details at wavelet level 1, and then the features are put into multiclass SVM for classification. Moreover, also our approach is capable of detecting the CTC which has the ability to evade the entropy-based detection method. Finally, a sliding window scheme is successfully designed to detect the complex traffic which several kinds of CTCs are embedded in. 
31|1||Understanding information systems security policy compliance: An integration of the theory of planned behavior and the protection motivation theory|This research investigated information systems security policy (ISSP) compliance by drawing upon two relevant theories i.e. the theory of planned behavior (TPB) and the protection motivation theory (PMT). A research model that fused constituents of the aforementioned theories was proposed and validated. Relevant hypotheses were developed to test the research conceptualization. Data analysis was performed using the partial least squares (PLS) technique. Using a survey of 124 business managers and IS professionals, this study showed that factors such as self-efficacy, attitude toward compliance, subjective norms, response efficacy and perceived vulnerability positively influence ISSP behavioral compliance intentions of employees. The data analysis did not support perceived severity and response cost as being predictors of ISSP behavioral compliance intentions. The study’s implications for research and practice are discussed. 
31|1||Building safe PaaS clouds: A survey on security in multitenant software platforms|This paper surveys the risks brought by multitenancy in software platforms, along with the most prominent solutions proposed to address them. A multitenant platform hosts and executes software from several users (tenants). The platform must ensure that no malicious or faulty code from any tenant can interfere with the normal execution of other users’ code or with the platform itself. This security requirement is specially relevant in Platform-as-a-Service (PaaS) clouds. PaaS clouds offer an execution environment based on some software platform. Unless PaaS systems are deemed as safe environments users will be reluctant to trust them to run any relevant application. This requires to take into account how multitenancy is handled by the software platform used as the basis of the PaaS offer. This survey focuses on two technologies that are or will be the platform-of-choice in many PaaS clouds: Java and .NET. We describe the security mechanisms they provide, study their limitations as multitenant platforms and analyze the research works that try to solve those limitations. We include in this analysis some standard container technologies (such as Enterprise Java Beans) that can be used to standardize the hosting environment of PaaS clouds. Also we include a brief discussion of Operating Systems (OSs) traditional security capacities and why OSs are unlikely to be chosen as the basis of PaaS offers. Finally, we describe some research initiatives that reinforce security by monitoring the execution of untrusted code, whose results can be of interest in multitenant systems. 
31|1||Robustness of keystroke-dynamics based biometrics against synthetic forgeries|Biometric systems including keystroke-dynamics based authentication have been well studied in the literature. The attack model in biometrics typically considers impersonation attempts launched by human imposters. However, this attack model is not adequate, as advanced attackers may utilize programs to forge data. In this paper, we consider the effects of synthetic forgery attacks in the context of biometric authentication systems. Our study is performed in a concrete keystroke-dynamic authentication system.The main focus of our work is evaluating the security of keystroke-dynamics authentication against synthetic forgery attacks. Our analysis is performed in a remote authentication framework called TUBA that we design and implement for monitoring a user’s typing patterns. We evaluate the robustness of TUBA through experimental evaluation including two series of simulated bots. The keystroke sequences forged by the two bots are modeled using first-order Markov chains. Support vector machine is used for classification. Our results, based on 20 users’ keystroke data, are reported. Our work shows that keystroke dynamics is robust against the two specific types of synthetic forgery attacks studied, where attacker draws statistical samples from a pool of available keystroke dataset other than the target.We also describe TUBA’s use for detecting anomalous activities on remote hosts, and present its use in a specific cognition-based anomaly detection system. The use of TUBA provides high assurance on the information collected from the hosts and enables remote security diagnosis and monitoring. 
31|1||RIPsec â Using reputation-based multilayer security to protect MANETs|This paper examines the theory, application, and results for a Reputation-Based Internet Protocol Security (RIPsec) framework that provides security for a Mobile Ad-hoc Network (MANET) operating in a hostile environment. While there has been significant research in MANET security, the research has tended to address subsets of the overall security challenge. RIPsec leverages existing technologies to provide an overarching layered security framework that provides a more comprehensive security solution than existing approaches. Protection from external threats is provided in the form of encrypted links and encryption-wrapped nodes while internal threats are mitigated by behavior grading that assigns reputations to nodes based on their demonstrated participation in the routing process. End-to-end message security using public and private certificates protects against both internal and external threats. Network availability is improved by behavior grading and round-robin multipath routing.Simulation results showed that the number of routing errors sent in a MANET was reduced by an average of 52% when using RIPsec. The cost in network performance for the security provided by RIPsec was a reduction in throughput. However, the reduction was acceptable given the increase in security. The network load was also reduced, decreasing the overall traffic introduced into the MANET and permitting individual nodes to perform more work without overtaxing their limited resources.The RIPsec framework was analyzed to demonstrate its robustness against a number of well-known attacks against ad-hoc networks. Of the four features incorporated into RIPsec (encryption, IPsec transport mode, behavior grading, and multipath routing), three other frameworks incorporated two of the features (encryption and behavior grading), and the remaining eight frameworks only incorporated one of the four security features. The incorporation of all four security features at multiple levels makes RIPsec very robust against attacks. 
31|1||Malware target recognition via static heuristics|Organizations increasingly rely on the confidentiality, integrity and availability of their information and communications technologies to conduct effective business operations while maintaining their competitive edge. Exploitation of these networks via the introduction of undetected malware ultimately degrades their competitive edge, while taking advantage of limited network visibility and the high cost of analyzing massive numbers of programs. This article introduces the novel Malware Target Recognition (MaTR) system which combines the decision tree machine learning algorithm with static heuristic features for malware detection. By focusing on contextually important static heuristic features, this research demonstrates superior detection results. Experimental results on large sample datasets demonstrate near ideal malware detection performance (99.9+% accuracy) with low false positive (8.73e-4) and false negative rates (8.03e-4) at the same point on the performance curve. Test results against a set of publicly unknown malware, including potential advanced competitor tools, show MaTR’s superior detection rate (99%) versus the union of detections from three commercial antivirus products (60%). The resulting model is a fine granularity sensor with potential to dramatically augment cyberspace situation awareness. 
31|2|http://www.sciencedirect.com/science/journal/01674048/31/2|Contents|
31|2||Editorial|
31|2||Security evaluation of biometric keys|Biometric cryptosystems combine biometrics with cryptography by producing Biometric Cryptographic Keys (BCKs) to provide stronger security mechanisms while protecting against identity theft. The process of generating/binding biometric keys consists of a number of steps starting with a feature extraction procedure, the complexity of which depends on the specific biometric trait/scheme, followed often by user selected transformation to allow for revocability, and an error correction scheme to tolerate reasonable amount of intra-class variation. Each of these steps has its own effect on the security of the generated/bound key. Proper security evaluation must include thorough analysis of the security effect of each of these steps. We propose a comprehensive approach to BCK’s security evaluation that takes into consideration each of the steps involved in their construction. We first review existing BCKs and highlight that the analysis of their security is either insufficient or not provided. In addition to evaluating the correctness (i.e. error rates), and the generated/bound key size, we evaluate the randomness of biometric features employed in the process of key generation. Our proposal combines the Kullback–Leibler divergence and the discrimination entropy to formulate a new measure of the Entropy of Biometric Features (EBF), defined as the average number of bits that distinguishes a user from a given population. Then we rigorously evaluate the impact of using error correcting scheme on the security of BCKs to calculate the Effective Entropy of Biometric Features (EEBF). Finally, inherent individual differences of the EBFs will be discussed. Here, we focus on face-based BCKs, but this does not restrict the use of the proposed evaluation. This paper argues that current face-based BCKs are not secure enough for high level security applications, and demonstrates that the average EEBF of BCKs using PCA-based facial features is less than 20-bit even when applying a user-based randomization on biometric features. 
31|2||Performance of automated network vulnerability scanning at remediating security issues|This paper evaluates how large portion of an enterprises network security holes that would be remediated if one would follow the remediation guidelines provided by seven automated network vulnerability scanners. Remediation performance was assessed for both authenticated and unauthenticated scans. The overall findings suggest that a vulnerability scanner is a usable security assessment tool, given that credentials are available for the systems in the network. However, there are issues with the method: manual effort is needed to reach complete accuracy and the remediation guidelines are oftentimes very cumbersome to study. Results also show that a scanner more accurate in terms of remediating vulnerabilities generally also is better at detecting vulnerabilities, but is in turn also more prone to false alarms. This is independent of whether the scanner is provided system credentials or not. 
31|2||Toward a general defense against kernel queue hooking attacks|Kernel queue hooking (KQH) attacks achieve stealthy malicious function execution by embedding malicious hooks in dynamic kernel schedulable queues (K-Queues). Because they keep kernel code and persistent hooks intact, they can evade detection of state-of-the-art kernel integrity monitors. Moreover, they have been used by advanced malware such as the Rustock spam bot to achieve malicious goals. In this paper, we present a systematic defense against such novel attacks. We propose the Precise Lookahead Checking of function Pointers approach that checks the legitimacy of pending K-Queue callback requests by proactively checking function pointers that may be invoked by the callback function. To facilitate the derivation of specifications for any K-Queue, we build a unified static analysis framework and a toolset that can derive from kernel source code properties of legitimate K-Queue requests and turn them into source code for the runtime checker. We implement proof-of-concept runtime checkers for four K-Queues in Linux and perform a comprehensive experimental evaluation of these checkers, which shows that our defense is effective against KQH attacks. 
31|2||Multiple-File Remote Data Checking for cloud storage|Remote Data Checking (RDC) adds data possession or retrievability guarantee to cloud storage without downloading the whole data. The support for dynamic data updates is vital for the practical application of RDC. We define Multiple-File Remote Data Checking (MF-RDC), an RDC model suitable for the specific data update model of cloud storage. MF-RDC checks the intactness of a dynamic file group consisting of a growing number of static files. By checking a group of files aggregately, the overhead of the scheme can be significantly reduced. We propose constructions of two MF-RDC schemes: MF-PDP and MF-POR. An efficient and secure MF-PDP scheme that provides data possession guarantee is constructed from a single-file PDP scheme by combining homomorphic authenticators with virtual block indices. The scheme is amended to integrate with file encoding using adversarial error-correcting codes, producing the MF-POR scheme that provides data retrievability guarantee. We conduct rigorous security analysis of the schemes and perform experimental evaluation on our implementation. With an efficient implementation, the communication and computation overhead of the schemes is reduced from linear in the size of the data to near constant. The performance of the schemes is bounded by disk I/O rather than cryptographic computation. 
31|2||HIPAA Privacy Rule compliance: An interpretive study using Normanâs action theory|Using Reason’s GEMS typology to analyze publicly available reports of privacy breaches in the United States shows human error as the cause of a significant number of violations of HIPAA Privacy Rule. An interpretive study based on interviews of 15 privacy officers of major U.S. healthcare organizations reinforces this finding. Applying the Rating Scale Model to analyze these officers’ ranking of the underlying causes of human error suggests that such organizational factors as high workload and low morale impede HIPAA Privacy Rule compliance more than either poor skills or availability of technology resources. Contrary to the common belief that human error may be attributed primarily to an individual, the results suggest that the work environment is critical and that systemic limitations underlie errors made by employees. By applying a cognitive taxonomy of human errors based on Norman’s action theory, this paper gives healthcare organizations a framework for managing compliance with HIPAA Privacy Rule and operational strategies that help enforce this compliance, especially among the clinical staff. 
31|2||Unrealistic optimism on information security management|Information security is a critical issue that many firms face these days. While increasing incidents of information security breaches have generated extensive publicity, previous studies repeatedly expose low levels of managerial awareness and commitment, a key obstacle to achieving a good information security posture. The main motivation of our study emanates from this phenomenon that the increased vulnerability to information security breaches is coupled with the low level of managerial awareness and commitment regarding information security threats. We report this dissonance by addressing a cognitive bias called optimistic bias. Using a survey, we study if MIS executives are subject to such a bias in their vulnerability perceptions of information security. We find that they demonstrate optimistic bias in risk perception on information security domain. The extent of this optimistic bias is greater with a distant comparison target with fewer information sharing activities. This optimistic bias is also found to be related to perception of controllability with information security threats. In order to overcome the effects of optimistic bias, firms need more security awareness training and systematic treatments of security threats instead of relying on ad hoc approach to security measure implementation. 
31|2||A Hot Query Bank approach to improve detection performance against SQL injection attacks|SQL injection attacks (SQLIAs) exploit web sites by altering backend SQL statements through manipulating application input. With the growing popularity of web applications, such attacks have become a serious security threat to users and systems as well. Existing dynamic SQLIA detectors provide high detection accuracy yet may have ignored another focus: efficiency. Our research has found that inside most systems exist many hot queries that current SQLIA detectors have repeatedly verified. Such repetition causes unnecessary waste of system resources.The research has completed Hot Query Bank (HQB), a pilot design that can cooperate with the existing SQLIA detectors in web applications and enhance overall system performance. HQB simply records hot queries and skip the detector’s verification process on their next appearances. Algorithms for the design have been proposed. A series of simulated experiments has been conducted to observe the performance improved from the design with three respective detectors, SQLGuard, SQLrand, and PHPCheck.The results have illustrated that utilization of HQB can indeed improve system performance by 45% of execution time, regardless of different detectors being tested. With such improvement and robustness, the result promises to provide an add-on feature for SQLIA detectors in protecting web applications more efficiently. Future works include further validation of the design in a real web application environment, development of a standard interface to collaborate with web applications and detectors, etc. 
31|2||Security Risk Management: Building an Information Security Risk Management Program from the Ground Up|
31|2||Cyber Attacks|
31|2||Coding for Penetration Testers|
31|2||Formal security policy implementations in network firewalls|Network security should be based around formal security policies. From high-level natural language, non-technical, policies created by management, down to device and vendor specific policies, or configurations, written by network system administrators. There exists a multitude of research into policy-based network systems which has been undertaken. This paper provides an overview of the different type of policies relating to security in networks, and a taxonomy of the research into systems which have been proposed to support the network administrators in difficult tasks of creating, managing and deploying these policies. 
31|3|http://www.sciencedirect.com/science/journal/01674048/31/3|Contents|
31|3||Editorial|
31|3||Operational experiences with anomaly detection in backbone networks|Although network security is a crucial aspect for network operators, there are still very few works that have examined the anomalies present in large backbone networks and evaluated the performance of existing anomaly detection solutions in operational environments. The objective of this work is to fill this gap by reporting hands-on experience in the evaluation and deployment of an anomaly detection solution for the GÉANT backbone network. During this process, we analyzed three different commercial tools for anomaly detection and then deployed one of them for several months in the 18 points-of-presence of GÉANT. We first explain the general requirements that an anomaly detection system should satisfy from the point of view of a network operator. Afterwards, we describe the evaluation of the tools and present a study of the anomalies found in a continental backbone network after operationally using the finally deployed tool for half a year. We think that this first hand information can be of great interest to both professionals and researchers working on network security and can also guide future research towards more practical problems faced by network operators. 
31|3||Polite sender: A resource-saving spam email countermeasure based on sender responsibilities and recipient justifications|Currently, most of the existing spam countermeasures are deployed on the email recipient side. However, they cannot diminish the amount of wasteful traffic sent from the SMTP server and the wasteful data storage in the receiver's inbox incurred by spam emails. This paper presents an alternative approach on the sender side in order to overcome these problems and create a bandwidth-saving reduced-storage email system. Additional functions are added to the SMTP server on the sender side to examine whether should allow the particular email sender. If a proper authorization from the recipient has not been granted, the sending SMTP server will not forward the full email message. Instead, it sends the email header together with some additional inquiries for the recipient to authorize this particular sender. Once the authorization is granted, each pair of a given sender and receiver will be kept in a whitelist at the sending SMTP server. The proposed approach can be easily deployed without modifying the existing SMTP protocol stack. The experiment results based on a prototype and data analysis from real email servers demonstrate that the proposed scheme could drastically reduce the amount of wasteful traffic and storage associated with the annoying spam messages. 
31|3||A methodology for integrating access control policies within database development|Security in general and database protection from unauthorized access in particular, are crucial for organizations. While functional requirements are defined in the early stages of the development process, non-functional requirements such as security tend to be neglected or dealt with only at the end of the development process. Various efforts have been made to address this problem; however, none of them provide a complete framework to guide, enforce and verify the correct design of security policies, and eventually generate code from that design.We present a novel methodology that assists developers, in particular database designers, to design secure databases that comply with the organizational security policies that are related to access control. The methodology is applied in two main levels: organizational level and application development level. At the organizational level, which takes place before the development of a specific application, organizational policies are defined in the form of security patterns. These patterns encapsulate accumulated knowledge and best practices on security related problems. At the application development level, the data-related security requirements are defined as part of the data model. The security patterns, which have been defined at the organizational level, guide the definition and implementation of the security requirements. The correct implementation of the security patterns is verified during the design stage of the development process, before the automatic generation of the database code. The methodology is supported by a CASE tool that assists its implementation in the various stages. 
31|3||Applying security policies and service level agreement to IaaS service model to enhance security and transition|Over a decade ago, cloud computing became an important topic for small, medium and large businesses alike. The new concept promises scalability, security, cost reduction, portability and availability. While addressing the cloud concepts over the past several years, there have been intensive discussions about the importance of the different cloud computing service model. Moreover, there were lots of discussions about the risks in migrating to cloud computing. Therefore, this paper reviews the concept of cloud computing, security policies and concentrates on Infrastructure as a Service (IaaS) model. Also, the paper examines the risks encountered by implementing the Infrastructure as a Service (IaaS) model in organizations. Furthermore, the paper’s aim is to discuss the role of security policies, service level agreement (SLA) and compliance for enhancing the security of the IaaS service model by presenting several applicable policies. 
31|3||Evaluating a migration-based response to DoS attacks in a system of distributed auctions|Service migration is a possible approach for a class of Internet services to deal with the increasing frequency of denial-of-service (DoS) attacks. The basic idea is to entitle services to physically relocate to a different host after detecting an attack or as a preventive action. We examine the implications of this approach within the context of an automated and high-frequency English auction system, which can be particularly sensitive to degrading communications performance. The impact of the attack and the migration response are investigated in terms of the diminishing utility attainable by auctioneers, giving insight into the advantages and disadvantages of the migration approach as DoS defense. 
31|3||Have things changed now? An empirical study on input validation vulnerabilities in web applications|Web applications have become important services in our daily lives. Millions of users use web applications to obtain information, perform financial transactions, have fun, socialize, and communicate. Unfortunately, web applications are also frequently targeted by attackers. Recent data from SANS institute estimates that up to 60% of Internet attacks target web applications.In this paper, we perform an empirical analysis of a large number of web vulnerability reports with the aim of understanding how input validation flaws have evolved in the last decade. In particular, we are interested in finding out if developers are more aware of web security problems today than they used to be in the past. Our results suggest that the complexity of the attacks have not changed significantly and that many web problems are still simple in nature. Hence, despite awareness programs provided by organizations such as MITRE, SANS Institute and OWASP, application developers seem to be either not aware of these classes of vulnerabilities, or unable to implement effective countermeasures. Therefore, we believe that there is a growing need for languages and application platforms that attack the root of the problem and secure applications by design. 
31|3||Toward developing a systematic approach to generate benchmark datasets for intrusion detection|In network intrusion detection, anomaly-based approaches in particular suffer from accurate evaluation, comparison, and deployment which originates from the scarcity of adequate datasets. Many such datasets are internal and cannot be shared due to privacy issues, others are heavily anonymized and do not reflect current trends, or they lack certain statistical characteristics. These deficiencies are primarily the reasons why a perfect dataset is yet to exist. Thus, researchers must resort to datasets that are often suboptimal. As network behaviors and patterns change and intrusions evolve, it has very much become necessary to move away from static and one-time datasets toward more dynamically generated datasets which not only reflect the traffic compositions and intrusions of that time, but are also modifiable, extensible, and reproducible. In this paper, a systematic approach to generate the required datasets is introduced to address this need. The underlying notion is based on the concept of profiles which contain detailed descriptions of intrusions and abstract distribution models for applications, protocols, or lower level network entities. Real traces are analyzed to create profiles for agents that generate real traffic for HTTP, SMTP, SSH, IMAP, POP3, and FTP. In this regard, a set of guidelines is established to outline valid datasets, which set the basis for generating profiles. These guidelines are vital for the effectiveness of the dataset in terms of realism, evaluation capabilities, total capture, completeness, and malicious activity. The profiles are then employed in an experiment to generate the desirable dataset in a testbed environment. Various multi-stage attacks scenarios were subsequently carried out to supply the anomalous portion of the dataset. The intent for this dataset is to assist various researchers in acquiring datasets of this kind for testing, evaluation, and comparison purposes, through sharing the generated datasets and profiles. 
31|4|http://www.sciencedirect.com/science/journal/01674048/31/4|Contents|
31|4||Editorial|
31|4||Abstract interpretation-based semantic framework for software birthmark|Software birthmark is a promising technique for detecting software piracy. Currently, many software birthmarks have been proposed, but the evaluations of these birthmarks are mainly done through experiments and there is no theoretical framework, which makes it difficult to formally analyze and certify the effectiveness of software birthmarks. To solve this problem, a semantic framework for software birthmarks is proposed based on abstract interpretation in this paper. First, two models, which characterize the criteria for the copy relation and program transformation attacks respectively, are given by abstract interpretation. Then, based on these two models, the semantic definition of software birthmarks is presented, and the credibility and the resilience of software birthmarks are formally proved in the proposed semantic framework. Furthermore, software birthmarks are compared with respect to their credibilities and resilience in the lattice of abstract interpretation. Finally, the effectiveness of the proposed framework is demonstrated by evaluating and comparing two typical software birthmarks, the static API birthmark and the static n-gram birthmark. 
31|4||Performance analysis of Bayesian networks and neural networks in classification of file system activities|Precise comprehension of a file system state at any given time is vital for performing digital forensic analyses. To uncover evidence of the digital crime, the logical representation of file system activities helps reconstruct post-event timeline of the unauthorized or malicious accesses made on a system. This paper describes a comparative performance analysis of the Bayesian networks and neural networks techniques to classify the state of file system activities in terms of execution of applications based on the pattern of manipulation of specific files during certain period of time. In particular, this paper discusses the construction of a Bayesian networks and neural networks from the predetermined knowledge of the manipulation of file system artifacts and their corresponding metadata information by a set of software applications. The variability amongst the execution patterns of various applications indicate that the Bayesian network-based model is a more appropriate tool as compared to neural networks because of its ability to learn and detect patterns even from an incomplete dataset. The focus of this paper is to highlight intrinsic significance of the learning approach of Bayesian network methodology in comparison to the techniques used for supervised learning in ordinary neural networks. The paper also highlights the efficacy of Bayesian network technique to proficiently handle large volumes of datasets. 
31|4||MSABMS-based approach of detecting LDoS attack|Low-rate Denial of Service (LDoS) attacks exploit the deficiencies of the minimum RTO of TCP to send out attack packets in short-duration periodic pulses with low average volume traffic in order to throttle TCP throughput. It is hard to detect an LDoS attack by most available detection schemes, which are triggered by high-rate traffic based on time average statistics. In this paper, the method of Multiple Sampling Averaging Based on Missing Sampling (MSABMS) is used to detect LDoS attacks based on the model of small signal for the first time. In the proposed approach, statistics on the packets are taken within 30 s with the sampling interval of 10 ms (3000 sampling points in total), and the statistical results are compared with a threshold for identifying the LDoS attacks. Furthermore, an eigenvalue-estimating matrix is established to estimate the attack period after the detection of LDoS attacks. Simulation results in NS-2 environment show that the proposed approach can be used to detect the LDoS attack effectively. 
31|4||SCADA security in the light of Cyber-Warfare|Supervisory Control and Data Acquisition (SCADA) systems are deployed worldwide in many critical infrastructures ranging from power generation, over public transport to industrial manufacturing systems. Whilst contemporary research has identified the need for protecting SCADA systems, these information are disparate and do not provide a coherent view of the threats and the risks resulting from the tendency to integrate these once isolated systems into corporate networks that are prone to cyber attacks. This paper surveys ongoing research and provides a coherent overview of the threats, risks and mitigation strategies in the area of SCADA security. 
31|4||Encryption-based multilevel model for DBMS|In this paper, we propose an encryption-based multilevel model for database management systems. The proposed model is a combination of the Multilevel Relational (MLR) model and an encryption system. This encryption system encrypts each data in the tuple with different field-key according to a security class of the data element. Each field is decrypted individually by the field-key of which security class is higher than or equal to that of the encrypted field-key. The proposed model is characterized by three achievements: (1) utilizing an encryption system as an additional security layer over the multilevel security layer for the database, (2) reducing the multilevel database size, and (3) improving the response time of the data retrieval from the multilevel database. Also this paper summarizes our efforts in implementing a working multilevel secure database prototype. This prototype is used as a research tool for studying principles and mechanisms of the encryption-based multilevel model and multilevel secure database (MLS/DBMS) models (SeaView, Jajodia–Sandhu, Smith–Winslett, MLR, and Belief-Consistent Model). This prototype is implemented to be used to perform a series of experiments to measure the performance cost for applying encryption in multilevel database security. 
31|4||Dynamic risk-based decision methods for access control systems|In traditional multi-level security systems, trust and risk values are pre-computed. Any change in these values requires manual intervention of an administrator. In many dynamic environments, however, these values should be auto-adaptive, and auto-tunable according to the usage history of the users. Moreover, occasional exceptions on resource needs, which are common in dynamic environments like healthcare, should be allowed if the subjects show a positive record of use toward resources they acquired in the past. Conversely, access of authorized users, who have negative record, should be restricted. These requirements are not taken into consideration in existing risk-based access control systems. In order to overcome these shortcomings and to meet different sensitivity requirements of various applications, we propose two dynamic risk-based decision methods for access control systems. We provide theoretical and simulation-based analysis and evaluation of both schemes. Also, we analytically prove that the proposed methods, not only allow exceptions under certain controlled conditions, but uniquely restrict legitimate access of bad authorized users. 
31|4||Systematically breaking and fixing OpenID security: Formal analysis, semi-automated empirical evaluation, and practical countermeasures|OpenID 2.0 is a user-centric Web single sign-on protocol with over one billion OpenID-enabled user accounts, and tens of thousands of supporting websites. While the security of the protocol is clearly critical, so far its security analysis has only been done in a partial and ad-hoc manner. This paper presents the results of a systematic analysis of the protocol using both formal model checking and an empirical evaluation of 132 popular websites that support OpenID. Our formal analysis reveals that the protocol does not guarantee the authenticity and integrity of the authentication request, and it lacks contextual bindings among the protocol messages and the browser. The results of our empirical evaluation suggest that many OpenID-enabled websites are vulnerable to a series of cross-site request forgery attacks (CSRF) that either allow an attacker to stealthily force a victim user to sign into the OpenID supporting website and launch subsequent CSRF attacks (81%), or force a victim to sign in as the attacker in order to spoof the victim's personal information (77%). With additional capabilities (e.g., controlling a wireless access point), the adversary can impersonate the victim on 80% of the evaluated websites, and manipulate the victim's profile attributes by forging the extension parameters on 45% of those sites. Based on the insights from this analysis, we propose and evaluate a simple and scalable mitigation technique for OpenID-enabled websites, and an alternative man-in-the-middle defense mechanism for deployments of OpenID without SSL. 
31|4||PIN selection policies: Are they really effective?|Users have conflicting sets of requirements when it comes to choosing Personal Identification Numbers (PINs) for mobile phones or other systems that use PINs for authentication: the conflict lies between the ‘easy to remember’ usability requirement and the ‘hard to guess’ security requirement. Users often ignore the security requirement and choose PINs that are easy to remember and reuse, making it also easy for attackers to guess and compromise them. Just as the password strength is controlled through various password policies, PIN selection policies may be used to help users choose stronger PINs and meet various security requirements. An example policy would not allow the use of the most commonly selected PINs.An online user study was conducted to investigate the effectiveness of such PIN selection policies, requesting the participants to choose PINs under some carefully designed policies. The participants were also asked to record the memorability (remembrance difficulty) score of each PIN, indicating how easy/hard it was to remember the selected PIN. Based on the entropies calculated on the collected PINs and their memorability scores, this paper demonstrates that restricting some number of commonly used PINs (e.g. restricting the 200 most commonly used ones) is beneficial: this type of policy would significantly increase the randomness of PINs without incurring significant memorability overhead. Our results also showed that any PIN- or PIN-pattern-based blacklisting policy should be constructed with caution since the total PIN space may become too small, making it easier for attackers to guess PINs. 
31|4||CRiBAC: Community-centric role interaction based access control model|As one of the most efficient solutions to complex and large-scale problems, multi-agent cooperation has been in the limelight for the past few decades. Recently, many research projects have focused on context-aware cooperation to dynamically provide complex services. As cooperation in the multi-agent systems (MASs) becomes more common, guaranteeing the security of such cooperation takes on even greater importance. However, existing security models do not reflect the agents' unique features, including cooperation and context-awareness. In this paper, we propose a Community-based Role interaction-based Access Control model (CRiBAC) to allow secure cooperation in MASs. To do this, we refine and extend our preliminary RiBAC model, which was proposed earlier to support secure interactions among agents, by introducing a new concept of interaction permission, and then extend it to CRiBAC to support community-based cooperation among agents. We analyze potential problems related to interaction permissions and propose two approaches to address them. We also propose an administration model to facilitate administration of CRiBAC policies. Finally, we present the implementation of a prototype system based on a sample scenario to assess the proposed work and show its feasibility. 
31|4||FT-FW: A cluster-based fault-tolerant architecture for stateful firewalls|Nowadays, stateful firewalls are part of the critical infrastructure of the Internet. Basically, they help to protect network services and users against attackers by means of access control and protocol conformance checkings. However, stateful firewalls are problematic from the fault-tolerance perspective since they introduce a single point of failure in the network schema. In this work, we summarize and enhance our previous research efforts that aim to provide a full fault-tolerant solution for stateful firewalls. These efforts have focused on the design and the implementation of the cluster-based Fault-Tolerant stateful Firewall (FT-FW) architecture. We provide details on our proposed solution and we extensively evaluate important network performance and availability aspects that we did not cover so far. The evaluation experiments are based on our Free/OpenSource implementation that has become the most popular solution for Linux-based stateful firewalls.1 
31|4||domRBAC: An access control model for modern collaborative systems|Modern collaborative systems such as the Grid computing paradigm are capable of providing resource sharing between users and platforms. These collaborations need to be done in a transparent way among the participants of a virtual organization (VO). A VO may consist of hundreds of users and heterogeneous resources. In order to have a successful collaboration, a list of vital importance requirements should be fulfilled, viz. collaboration among domains, to ensure a secure environment during a collaboration, the ability to enforce usage constraints upon resources, and to manage the security policies in an easy and efficient way. In this article, we propose an enhanced role-based access control model entitled domRBAC for collaborative applications, which is based on the ANSI INCITS 359-2004 access control model. The domRBAC is capable of differentiating the security policies that need to be enforced in each domain and to support collaboration under secure inter-operation. Cardinality constraints along with context information are incorporated to provide the ability of applying simple usage management of resources for the first time in a role-based access control model. Furthermore, secure inter-operation is assured among collaborating domains during role assignment automatically and in real-time. Yet, domRBAC, as an RBAC approach, intrinsically inherits all of its virtues such as ease of management, and separation of duty relationships with the latter also being supported in multiple domains. As a proof of concept, we implement a simulator based on the definitions of our proposed access control model and conduct experimental studies to demonstrate the feasibility and performance of our approach. 
31|4||Bypassing information leakage protection with trusted applications|Insider threats are an increasing concern for most modern organizations. Information leakage is one of the most important insider threats, particularly according to its potential financial impact. Data Leakage Protection (DLP) systems have been developed to tackle this issue and they constitute the main solution to protect information systems against leaks. They work by tracking sensitive information flows and monitoring executed applications to ensure that sensitive information is not leaving the organization. However, current DLP systems do not fully consider that trusted applications represent a threat to sensitive information confidentiality. In this paper, we demonstrate how to use common trusted applications to evade current DLP systems. Thanks to its wide range, trusted applications such as Microsoft Excel can be transformed into standardized block ciphers. Information can thus be encrypted in such a way that current DLP techniques cannot detect that sensitive information is being leaked. This method could be used by non-skilled malicious insiders and leaves almost no traces. We have successfully tested our method against a well-known DLP solution from a commercial provider (TrendMicro LeakProof). Finally, we also analyze the proposed evasion technique from the malicious insider point of view and discuss some possible countermeasures to mitigate its use to steal information. 
31|4||QoP-ML: Quality of protection modelling language for cryptographic protocols|Cryptographic protocols can be realized on different levels of security. One can choose factors which have different impact on the overall system security. Traditionally, protocols have been configured with the strongest possible security mechanisms. Unfortunately, the strongest protection (especially in low resource devices) can lead to the denial of services. In such a situation the quality of protection models which scales the protection level depending on the specific requirements is used.In the article, we proposed the quality of protection modelling language (QoP-ML) which provides the modelling language for abstracting the cryptographic protocols. All of the security operations/mechanisms which are executed while running cryptographic protocols can be modelled with the QoP-ML. In the QoP-ML, the defined operations can be described by the security metrics which allow performing quality of protection evaluation. In the paper, the syntax and semantics of the Quality of Protection modelling language will be presented. Finally, the Needham–Schroeder public key protocol will be modelled by QoP-ML and their QoP evaluation will be discussed. 
31|4||Leveraging behavioral science to mitigate cyber security risk|Most efforts to improve cyber security focus primarily on incorporating new technological approaches in products and processes. However, a key element of improvement involves acknowledging the importance of human behavior when designing, building and using cyber security technology. In this survey paper, we describe why incorporating an understanding of human behavior into cyber security products and processes can lead to more effective technology. We present two examples: the first demonstrates how leveraging behavioral science leads to clear improvements, and the other illustrates how behavioral science offers the potential for significant increases in the effectiveness of cyber security. Based on feedback collected from practitioners in preliminary interviews, we narrow our focus to two important behavioral aspects: cognitive load and bias. Next, we identify proven and potential behavioral science findings that have cyber security relevance, not only related to cognitive load and bias but also to heuristics and behavioral science models. We conclude by suggesting several next steps for incorporating behavioral science findings in our technological design, development and use. 
31|4||Noncespaces: Using randomization to defeat cross-site scripting attacks|Cross-site scripting (XSS) vulnerabilities are among the most common and serious web application vulnerabilities. It is challenging to eliminate XSS vulnerabilities because it is difficult for web applications to sanitize all user input appropriately. We present Noncespaces, a technique that enables web clients to distinguish between trusted and untrusted content to prevent exploitation of XSS vulnerabilities. Using Noncespaces, a web application randomizes the the (X)HTML tags and attributes in each document before delivering it to the client. As long as the attacker is unable to guess the random mapping, the client can distinguish between trusted content created by the web application and untrusted content provided by an attacker. To implement Noncespaces with minimal changes to web applications, we leverage a popular web application architecture to automatically apply Noncespaces to static content processed through a popular PHP template engine. We design a policy language for Noncespaces, implement a training mode to assist policy development, and conduct extensive security testing of a generated policy for two large web applications to show the effectiveness of our technique. 
31|4||Private Cloud Computing: Consolidation, Virtualization, and Service-Oriented Infrastructure|
31|4||Penetration Tester's Open Source Toolkit|
31|4||Securing the Cloud: Cloud Computer Security Techniques and Tactics|
31|4||The Basics of Information Security: Understanding the Fundamentals of InfoSec in Theory and Practice|
31|4||Enterprise Security for the Executive|
31|4||Cyber Warfare â Techniques, Tactics and Tools for Security Practitioners|
31|4||Thor's Microsoft Security Bible|
31|4||XBOX 360 Forensics: A Digital Forensics Guide to Examining Artifacts|
31|4||Filler AD:IFIP|
31|5|http://www.sciencedirect.com/science/journal/01674048/31/5|Contents|
31|5||Editorial|
31|5||Incident response teams â Challenges in supporting the organisational security function|Incident response is a critical security function in organisations that aims to manage incidents in a timely and cost-effective manner. This research was motivated by previous case studies that suggested that the practice of incident response frequently did not result in the improvement of strategic security processes such as policy development and risk assessment. An exploratory in-depth case study was performed at a large global financial institution to examine shortcomings in the practice of incident response. The case study revealed the practice of incident response, in accordance with detailed best-practice guidelines, tended to adopt a narrow technical focus aimed at maintaining business continuity whilst neglecting strategic security concerns. The case study also revealed that the (limited) post-incident review process focused on ‘high-impact’ incidents rather than ‘high-learning’ (i.e. potentially useful incidents from a learning perspective) incidents and ‘near misses’. In response to this case study, we propose a new double-loop model for incident learning to address potential systemic corrective action in such areas as the risk assessment and policy development processes. 
31|5||Semantic adaptive microaggregation of categorical microdata|In the context of Statistical Disclosure Control, microaggregation is a privacy-preserving method aimed to mask sensitive microdata prior to publication. It iteratively creates clusters of, at least, k elements, and replaces them by their prototype so that they become k-indistinguishable (anonymous). This data transformation produces a loss of information with regards to the original dataset which affects the utility of masked data, so, the aim of microaggregation algorithms is to find the partition that minimises the information loss while ensuring a certain level of privacy. Most microaggregation methods, such as the MDAV algorithm, which is the focus of this paper, have been designed for numerical data. Extending them to support non-numerical (categorical) attributes is not straightforward because of the limitations on defining appropriate aggregation operators. Concretely, related works focused on the MDAV algorithm propose grouping data into groups with constrained size (or even fixed) and/or incorporate a basic categorical treatment of non-numerical data. This approach affects negatively the utility of the protected dataset because neither the distributional characteristics of data nor their underlying semantics are properly considered. In this paper, we propose a set of modifications to the MDAV algorithm focused on categorical microdata. Our approach has been evaluated and compared with related works when protecting real datasets with textual attribute values. Results show that our method produces masked datasets that better minimises the information loss resulting from the data transformation. 
31|5||Taxonomy of compliant information security behavior|This paper aims at surveying the extrinsic and intrinsic motivations that influence the propensity toward compliant information security behavior. Information security behavior refers to a set of core information security activities that have to be adhered to by end-users to maintain information security as defined by information security policies. The intention is to classify the research done on compliant information security behavior from an end-user perspective and arrange it as a taxonomy predicated on Self-Determination Theory (SDT). In addition, the relative significance of factors that contribute to compliant information security behavior is evaluated on the basis of empirical studies. The taxonomy will be valuable in providing a comprehensive overview of the factors that influence compliant information security behavior and in identifying areas that require further research. 
31|5||Securing distributed systems using patterns: A survey|Driven by expanding scientific computing and business enterprise needs, the last decade has seen a shift toward software paradigms in which distribution plays a central role. The increasing size, complexity and heterogeneity of the corresponding systems is accompanied by an increase of security vulnerabilities that require mitigation via combined security and software engineering strategies. In this respect security patterns, which build on the success of design patterns and software patterns more generally, are a tool of great value. In this paper we comprehensively survey the state-of-the-art in securing distributed systems using (security) patterns, considering both relevant patterns and methodologies for applying them. In the first part of the survey, we provide detailed reviews of our selected security patterns, classify the patterns using a multi-dimensional scheme and evaluate them according to a set of quality categories. This highlights deficiencies in the reviewed patterns and provides a basis for identifying new or “missing” patterns and pattern classes. The newly identified and surveyed patterns are a step forward in defining a pattern language for distributed computing. In the second part of the survey, we briefly review a number of pattern-based security methodologies and evaluate their maturity and appropriateness for securing distributed systems. 
31|5||Steganography for MP3 audio by exploiting the rule ofÂ window switching|MP3 audio is a promising carrier format for covert communication because of its popularization. In this paper, we propose an MP3 steganographic method by exploiting the rule of window switching during encoding. The method carries out embedding by establishing a mapping relationship between the secret bit and the encoding parameter, namely window type. The proposed algorithm is fully compliant with MP3 compression standard and the distortion caused by steganography can be controlled automatically by the distortion adjustment mechanism of the encoder. Experimental results demonstrate that the proposed method introduces insignificant perceptual distortion and is statistically undetectable for the attack of block size analysis. 
31|5||A second look at the performance of neural networks for keystroke dynamics using a publicly available dataset|Keystroke Dynamics, which is a biometric characteristic that depends on typing style of users, could be a viable alternative or a complementary technique for user authentication if tolerable error rates are achieved. Most of the earlier studies on Keystroke Dynamics were conducted with irreproducible evaluation conditions therefore comparing their experimental results are difficult, if not impossible. One of the few exceptions is the work done by Killourhy and Maxion, which made a dataset publicly available, developed a repeatable evaluation procedure and evaluated the performance of different methods using the same methodology. In their study, the error rate of neural networks was found to be one of the worst-performing. In this study, we have a second look at the performance of neural networks using the evaluation procedure and dataset same as in Killourhy and Maxion’s work. We find that performance of artificial neural networks can outperform all other methods by using negative examples. We conduct comparative tests of different algorithms for training neural networks and achieve an equal error rate of 7.73% with Levenberg–Marquardt backpropagation network, which is better than equal error rate of the best-performing method in Killourhy and Maxion’s work. 
31|5||WAVE-CUSUM: Improving CUSUM performance in network anomaly detection by means of wavelet analysis|The increasing number of network attacks causes growing problems for network operators and users. Thus, detecting anomalous traffic is of primary interest in IP networks management and many detection techniques, able to promptly reveal and identify network attacks, mainly detecting Heavy Changes in the network traffic, have been proposed. Among these, one of the most promising approach is based on the use of the CUSUM (CUmulative SUM). Nonetheless, CUSUM performance is strongly affected by its sensitivity to the presence of seasonal trends in the considered data.For this reason, in this paper we propose a novel detection method based on the idea of performing a pre-processing stage of the data by means of wavelets, aimed at filtering out such trends, before applying the CUSUM algorithm.The performance analysis, presented in the paper, demonstrates the efficiency of the proposed method, focusing on the performance improvements due to the pre-processing stage. 
31|5||A programmer's perspective|
31|5||Low Tech Hacking|
31|6|http://www.sciencedirect.com/science/journal/01674048/31/6|Contents|
31|6||Editorial|
31|6||An efficient handover authentication scheme with privacy preservation for IEEE 802.16m network|IEEE 802.16m is now under consideration by the International Telecommunication Union (ITU) to become the International Mobile Telecommunications (IMT)-Advanced standard. However, seamless and secure handover is one of the most challenging issues. Taking as reference our previous work, this paper presents a privacy-preserving fast handover authentication scheme based on pseudonym for IEEE 802.16m network. Since Mobile Station (MS) only provides a pseudonym in the initial authentication phase and changes its pseudonym in each handover authentication phase, this can protect the MS's identity privacy and allow MS to be untraceable. Moreover, each pseudonym is corresponding to a credential ticket generated by the previous service Base Station (BS) using a multi-BS group key, thus MS and the target BS can easily accomplish mutual authentication. In addition, our theoretical analysis and simulation indicate that our scheme outperforms previously reported schemes in terms of computation and communication overhead. 
31|6||Impact of HIPAA provisions on the stock market value of healthcare institutions, and information security and other information technology firms|Title 1 of the Health Insurance Portability and Accountability Act (HIPAA) was enacted to improve the portability of healthcare insurance coverage and Title II was intended to alleviate fraud and abuse. The development of a health information system was suggested in Title II of HIPAA as a means of promoting standardization to improve the efficiency of the healthcare system and ensure that electronic healthcare information is transferred securely and kept private. Since the legislation places the onus of providing the described improvements on healthcare institutions and part of these requirements relate to information technology (IT) and information security (IS), the process of complying with the legislation will necessitate acquiring products and services from IT/IS firms. From the viewpoint of stock market analysts, this increase in demand for IT/IS products and services has the potential to boost the profitability of public IT/IS firms, in turn positively enhancing their stock market valuation. Following the same logic, the legislation's compliance burdens shared by healthcare firms are expected to require hefty costs, thus potentially reducing the profitability of healthcare firms and reflecting negatively on their stock price. The intent of this paper is to evaluate the stock market reaction to the introduction of HIPAA legislation by evaluating the abnormal movement in the price of the stock of public healthcare institutions, IT, and IS firms. We conduct event-study analyses around the announcement dates of the various provisions of HIPAA. An event study is a standard statistical methodology used to determine whether the occurrence of a specific event or events results in a statistically significant reaction in financial markets. The advantage of the event study methodology for policy analysis is that it provides an anchor for determining value, which eliminates reliance on ad hoc judgments about the impact of specific events or policies on stock prices. While event studies have been conducted that examine the market effect of security and privacy breaches on firms, none has attempted to determine the impact, in terms of resulting market reaction, of the HIPAA legislation itself. The results of the study confirm the logic above, while also providing insight into specific stages of the legislative path of HIPAA. 
31|6||Engineering a secure mobile messaging framework|It is quite usual in the world of scientific software development to use, as black boxes, algorithmic software libraries without any prior assessment of their efficiency. This approach relies on the assumption that the experimental performance of these libraries, although correct, will match the theoretical expectation of their algorithmic counterparts.In this paper we discuss the case of SEESMS (Secure Extensible and Efficient SMS). It is a software framework that allows two peers to exchange encrypted and digitally signed SMS messages. The cryptographic part of SEESMS is implemented on top of the Java BC library (The Legion of Bouncy Castle, 2010), a widely used open-source library. The preliminary experimentations conducted on SEESMS, discussed in Castiglione et al. (2010), revealed some unexpected phenomena like the ECDSA-based cryptosystem being generally and significantly slower than the RSA-based equivalent. In this paper, we analyze these phenomena by profiling the code of SEESMS and expose the issues causing its bad performance. Then, we apply some algorithmic and programming optimizations techniques. The resulting code exhibits a significant performance boost with respect to the original implementation, and requires less memory in order to be run. 
31|6||Real time DDoS detection using fuzzy estimators|We propose a method for DDoS detection by constructing a fuzzy estimator on the mean packet inter arrival times. We divided the problem into two challenges, the first being the actual detection of the DDoS event taking place and the second being the identification of the offending IP addresses. We have imposed strict real time constraints for the first challenge and more relaxed constraints for the identification of addresses. Through empirical evaluation we confirmed that the detection can be completed within improved real time limits and that by using fuzzy estimators instead of crisp statistical descriptors we can avoid the shortcomings posed by assumptions on the model distribution of the traffic. In addition we managed to obtain results under a 3 sec detection window. 
31|6||Exploiting hash functions to intensify the remote user authentication scheme|Nowadays, the client–server model plays an important part in the Internet architecture. The procedure that a server authenticates a remote user securely has become a significant issue. Hence, many remote user authentication schemes are proposed. Hsiang and Shih pointed out the weaknesses of Yoon et al.'s scheme and proposed an improved scheme not only preserving original merits but also mending the weaknesses. The scheme protects against masquerading attacks, offline password guessing attacks and parallel session attacks. However, He et al. found that Hsiang et al.'s scheme is still vulnerable to password guessing attack, masquerading server attack and masquerading attack. Furthermore, we also found Hsiang–Shih's scheme could be threatened by a malicious insider that can originate an infringed account attack and a resembling account attack. In this paper, we first describe how a malicious insider carries out an infringed account attack and then present a resembling account attack on Hsiang–Shih's scheme. After that, we propose an improvement assisted by hashing functions to enhance Hsiang–Shih's scheme. 
31|6||Hacking: The Next Generation|
31|6||The Basics of Digital Forensics|
31|6||Inside Cyber Warfare: Mapping the Cyber Underworld|
31|6||Practical Malware Analysis: The Hands-On Guide to Dissecting Malicious Software|
31|7|http://www.sciencedirect.com/science/journal/01674048/31/7|Contents|
31|7||Editorial Computers and Security Special Issue IFIP/SEC 2010 âSecurity & Privacy â Silver Linings in the Cloudâ|
31|7||Understanding domain registration abuses|The ability to monetize domain names through resale or serving ad content has contributed to the rise of questionable practices in acquiring them, including domain-name speculation, tasting, and front running. In this paper, we perform one of the first comprehensive studies of these domain registration practices. In order to characterize the prevalence of domain-name speculation, we derive rules describing “hot” topics from popular Google search queries and apply these rules to a dataset containing all .com registrations for an eight-month period in 2008. We also study the extent of domain tasting throughout this time period and analyze the efficacy of ICANN policies intended to limit tasting activity. Finally, we automatically generate high-quality domain names related to current events in order to measure domain front running by registrars. The results of our experiments shed light on the methods and motivations behind these domain registration practices and, in some cases, underscore the difficulty in definitively measuring these questionable behaviors. 
31|7||Preserving privacy of feedback providers in decentralized reputation systems|Reputation systems make the users of a distributed application accountable for their behavior. The reputation of a user is computed as an aggregate of the feedback provided by other users in the system. Truthful feedback is clearly a prerequisite for computing a reputation score that accurately represents the behavior of a user. However, it has been observed that users often hesitate in providing truthful feedback, mainly due to the fear of retaliation. We present a decentralized privacy preserving reputation protocol that enables users to provide feedback in a private and thus uninhibited manner. The protocol has linear message complexity, which is an improvement over comparable decentralized reputation protocols. Moreover, the protocol allows users to quantify and maximize the probability that their privacy will be preserved. 
31|7||On-the-fly inlining of dynamic security monitors|How do we guarantee that a piece of code, possibly originating from third party, does not jeopardize the security of the underlying application? Language-based information-flow security considers programs that manipulate pieces of data at different sensitivity levels. Securing information flow in such programs remains an open challenge. Recently, considerable progress has been made on understanding dynamic monitoring for secure information flow. This paper presents a framework for inlining dynamic information-flow monitors. A novel feature of our framework is the ability to perform inlining on the fly. We consider a source language that includes dynamic code evaluation of strings whose content might not be known until runtime. To secure this construct, our inlining is done on the fly, at the string evaluation time, and, just like conventional offline inlining, requires no modification of the hosting runtime environment. We present a forma!lization for a simple language to show that the inlined code is secure: it satisfies a non-interference property. We also discuss practical considerations experimental results based on both manual and automatic code rewriting. 
31|7||A business-driven decomposition methodology for role mining|It is generally accepted that role mining – that is, the discovery of roles through the automatic analysis of data from existing access control systems – must count on business requirements to increase its effectiveness. Indeed, roles elicited without leveraging on business information are unlikely to be intelligible by system administrators. A business-oriented categorization of users and permissions (e.g., organizational units, job titles, cost centers, business processes, etc.) could help administrators identify the job profiles of users and, as a consequence, which roles should be assigned to them. Nonetheless, most of the existing role mining techniques yield roles that have no clear relationship with the business structure of the organization where the role mining is being applied. To face this problem, we propose a methodology that allows role engineers to leverage business information during the role finding process. The key idea is decomposing the dataset to analyze into several partitions, in a way that each partition is homogeneous from a business perspective. Each partition groups users or permissions with the same business categorization (e.g., all the users belonging to the same department, or all the permissions that support the execution of the same business process). Such partitions are then role-mined independently, hence achieving three main results: (1) elicited roles have a clearer relationship with business information; (2) mining algorithms do not seek to find commonalities among users with fundamentally different job profiles or among uncorrelated permissions; and, (3) any role mining algorithm can be used in conjunction with our approach. When several business attributes are available, analysts need to figure out which one produces the decomposition that leads to the most intelligible roles. In this paper, we describe three indexes that drive the decomposition process by measuring the quality of a given decomposition: entrustability, minability gain, and similarity gain. We compare these indexes, pointing out pros and cons. Finally, we apply our methodology on real enterprise data, showing its effectiveness and efficiency in supporting role engineering. 
32|-|http://www.sciencedirect.com/science/journal/01674048/32|Contents|
32|-||Editorial|
32|-||Design and formal security evaluation of NeMHIP: A new secure and efficient network mobility management protocol based on the Host Identity Protocol|NEtwork MObility Basic Support (NEMO BS) is a standardized protocol for managing the mobility of a set of nodes that move together as a whole while having continuous connectivity to the Internet through one or more Mobile Routers (MRs). Because it is based on Mobile IPv6 (MIPv6), it inherits the properties of MIPv6, such as the use of IPsec. However, NEMO BS does not address all the features required by the demanding Intelligent Transportation Systems (ITS) scenario to provide an integrated and global secure mobility management framework. In addition, unlike MIPv6, the routing in NEMO BS is suboptimal, which makes difficult the provision of an adequate service performance. These characteristics make the application of the NEMO BS protocol not optimum in this scenario. An interesting strategy to provide security and good service performance is to consider a protocol that establishes and maintains Security Associations (SAs), such as the Host Identity Protocol (HIP). Different HIP-based approaches have been defined. However, these HIP-based network mobility solutions still present unsolved issues. In this article, we present a secure and efficient network mobility protocol named NeMHIP. NeMHIP provides secure and optimum mobility management and efficient end-to-end confidentiality and integrity protection apart from the basic security properties inherited from HIP. To evaluate the security provisions of NeMHIP, we have conducted a belief-based formal evaluation. The results demonstrate that the defined security goals are achieved by the protocol. Furthermore, we have performed an automated formal evaluation to validate additional security aspects of NeMHIP. Thus, we have modeled NeMHIP using the AVISPA tool and assessed its security when an intruder is present. The results confirm that NeMHIP is a secure protocol that ensures end-to-end confidentiality and integrity without introducing security leaks to the basic HIP. Thus, we have addressed the need found in the literature for providing security and efficiency in the network mobility scenario. 
32|-||Tree-formed verification data for trusted platforms|The establishment of trust relationships to a computing platform relies on validation processes. Validation allows an external entity to build trust in the expected behaviour of the platform based on provided evidence of the platform's configuration. In a process like remote attestation, the ‘trusted’ platform submits verification data created during a start up process. These data consist of hardware-protected values of platform configuration registers, containing nested measurement values, e.g., hash values, of loaded or started components. Commonly, the register values are created in linear order by a hardware-secured operation. Fine-grained diagnosis of components, based on the linear order of verification data and associated measurement logs, is not optimal. We propose a method to use tree-formed verification data to validate a platform. Component measurement values represent leaves, and protected registers represent roots of a hash tree. We describe the basic mechanism of validating a platform using tree-formed measurement logs and root registers and show a logarithmic speed-up for the search of faults. Secure creation of a tree is possible using a limited number of hardware-protected registers and a single protected operation. In this way, the security of tree-formed verification data is maintained. 
32|-||A confidential and DoS-resistant multi-hop code dissemination protocol for wireless sensor networks|Due to the open environment in which Wireless Sensor Networks (WSNs) are typically deployed, it is important to be able to authenticate transmitted data. In some applications it is also required that the data be kept confidential in spite of message interception. Authentication and confidentiality are typically implemented through cryptographic operations which may be expensive in power consumption, making a protocol with these features vulnerable to attack by an adversary who transmits forged data, so forcing nodes to waste energy in identifying it as invalid. Additionally, in multi-hop code dissemination protocols, a sensor node is required to broadcast its program image when requested by its neighbors. An adversary could repeatedly send spurious program image requests to its neighbors, making them exhaust their energy reserves. In this paper, we present a new approach to achieve confidentiality in multi-hop code dissemination. We propose countermeasures against both these types of attack. Our approach is based on Deluge, an open source, state-of-the-art code dissemination protocol for WSNs. We provide theoretical analysis and simulation/experimental results which show that our approach outperforms earlier attempts when a malicious code injector is present, as well as a performance evaluation of latency, energy consumption, dissemination rate, and other factors (Liu and Ning, November 2006). 
32|-||Digital media triage with bulk data analysis and bulk_extractor|Bulk data analysis eschews file extraction and analysis, common in forensic practice today, and instead processes data in “bulk,” recognizing and extracting salient details (“features”) of use in the typical digital forensics investigation. This article presents the requirements, design and implementation of the bulk_extractor, a high-performance carving and feature extraction tool that uses bulk data analysis to allow the triage and rapid exploitation of digital media. Bulk data analysis and the bulk_extractor are designed to complement traditional forensic approaches, not replace them. The approach and implementation offer several important advances over today's forensic tools, including optimistic decompression of compressed data, context-based stop-lists, and the use of a “forensic path” to document both the physical location and forensic transformations necessary to reconstruct extracted evidence. The bulk_extractor is a stream-based forensic tool, meaning that it scans the entire media from beginning to end without seeking the disk head, and is fully parallelized, allowing it to work at the maximum I/O capabilities of the underlying hardware (provided that the system has sufficient CPU resources). Although bulk_extractor was developed as a research prototype, it has proved useful in actual police investigations, two of which this article recounts. 
32|-||The architecture of a digital forensic readiness management system|A coordinated approach to digital forensic readiness (DFR) in a large organisation requires the management and monitoring of a wide variety of resources, both human and technical. The resources involved in DFR in large organisations typically include staff from multiple departments and business units, as well as network infrastructure and computing platforms. The state of DFR within large organisations may therefore be adversely affected if the myriad human and technical resources involved are not managed in an optimal manner. This paper contributes to DFR by proposing the novel concept of a digital forensic readiness management system (DFRMS). The purpose of a DFRMS is to assist large organisations in achieving an optimal level of management for DFR. In addition to this, we offer an architecture for a DFRMS. This architecture is based on requirements for DFR that we ascertained from an exhaustive review of the DFR literature. We describe the architecture in detail and show that it meets the requirements set out in the DFR literature. The merits and disadvantages of the architecture are also discussed. Finally, we describe and explain an early prototype of a DFRMS. 
32|-||Future directions for behavioral information security research|Information Security (InfoSec) research is far reaching and includes many approaches to deal with protecting and mitigating threats to the information assets and technical resources available within computer based systems. Although a predominant weakness in properly securing information assets is the individual user within an organization, much of the focus of extant security research is on technical issues. The purpose of this paper is to highlight future directions for Behavioral InfoSec research, which is a newer, growing area of research. The ensuing paper presents information about challenges currently faced and future directions that Behavioral InfoSec researchers should explore. These areas include separating insider deviant behavior from insider misbehavior, approaches to understanding hackers, improving information security compliance, cross-cultural Behavioral InfoSec research, and data collection and measurement issues in Behavioral InfoSec research. 
32|-||From keyloggers to touchloggers: Take the rough with the smooth|The proliferation of touchscreen devices brings along several interesting research challenges. One of them is whether touchstroke-based analysis (similar to keylogging) can be a reliable means of profiling the user of a mobile device. Of course, in such a setting, the coin has two sides. First, one can employ the output produced by such a system to feed machine learning classifiers and later on intrusion detection engines. Second, aggressors can install touchloggers to harvest user's private data. This malicious option has been also extensively exploited in the past by legacy keyloggers under various settings, but has been scarcely assessed for soft keyboards. Compelled by these separate but interdependent aspects, we implement the first-known native and fully operational touchlogger for ultramodern smartphones and especially for those employing the proprietary iOS platform. The results we obtained for the first objective are very promising showing an accuracy in identifying misuses, and thus post-authenticating the user, in an amount that exceeds 99%. The virulent personality of such software when used maliciously is also demonstrated through real-use cases. 
32|-||On the detection of desynchronisation attacks against security protocols that use dynamic shared secrets|Many peer-to-peer security protocols in mobile communications utilise shared secrets. Synchronous storage of shared secrets is imperative for the successful operation of security protocols, as asynchronous storage of shared secrets may lead to service unavailability. Hence, update mechanisms must not only guarantee the secrecy of shared secrets, but also their synchrony.This paper addresses synchronisation weaknesses in security protocols for wireless communications. It is demonstrated that a wide range of protocols contain such weaknesses. A new class of attack, called suppress-and-desynchronise attack, is introduced that exploit these weaknesses. These new attacks desynchronise the shared secrets of principals by suppressing messages, resulting in a permanent denial of service condition.A verification system to model update mechanisms for shared secrets is introduced. Based on this verification system detection rules are developed that are able to detect synchronisation weaknesses that can be exploited by suppress-and-desynchronise attacks. Application of the detection rules to three security protocols results in the detection of hitherto unknown weaknesses. Consequently, these security protocols are susceptible to suppress-and-desynchronise attacks and details of mounting the attacks are presented. Finally, amendments to one of these protocols are proposed and application of the introduced formal system establishes the immunity of the amended protocol against suppress-and-desynchronise attacks. 
32|-||Systematic bug finding and fault localization enhanced with input data tracking|Fault localization (FL) is the process of debugging erroneous code and directing analysts to the root cause of the bug. With this in mind, we have developed a distributed, end-to-end fuzzing and analysis system that starts with a binary, identifies bugs, and subsequently localizes the bug's root cause. Our system does not require the test subject's source code, nor do we require a test suite. Our work focuses on an important class of bugs, memory corruption errors, which usually have software security implications. Thus, our approach appeals to software attack researchers as well. In addition to our bug hunting and analysis framework, we have enhanced code-coverage based fault localization by incorporating input data tainting and tracking using a light-weight binary instrumentation technique. By capturing code coverage and select input data usage, our new FL algorithm is able to better localize faults, and therefore better assist analysts. We report the application of our approach on large, real-world applications (Firefox and VLC), as well as the classic Siemens benchmark and other test programs. 
32|-||Exploring attack graph for cost-benefit security hardening: A probabilistic approach|The increasing complexity of today's computer systems, together with the rapid emergence of novel vulnerabilities, make security hardening a formidable challenge for security administrators. Although a large variety of tools and techniques are available for vulnerability analysis, the majority work at system or network level without explicit association with human and organizational factors. This article presents a middleware approach to bridge the gap between system-level vulnerabilities and organization-level security metrics, ultimately contributing to cost-benefit security hardening. In particular, our approach systematically integrates attack graph, a commonly used effective approach to representing and analyzing network vulnerabilities, and Hidden Markov Model (HMM) together, for exploring the probabilistic relation between system observations and states. More specifically, we modify and apply dependency attack graph to represent network assets and vulnerabilities (observations), which are then fed to HMM for estimating attack states, whereas their transitions are driven by a set of predefined cost factors associated with potential attacks and countermeasures. A heuristic searching algorithm is employed to automatically infer the optimal security hardening through cost-benefit analysis. We use a synthetic network scenario to illustrate our approach and evaluate its performance through a set of simulations. 
32|-||EVIV: An end-to-end verifiable Internet voting system|Traditionally, a country's electoral system requires the voter to vote at a specific day and place, which conflicts with the mobility usually seen in modern live styles. Thus, the widespread of Internet (mobile) broadband access can be seen as an opportunity to deal with this mobility problem, i.e. the adoption of an Internet voting system can make the live of voter's much more convenient; however, a widespread Internet voting systems adoption relies on the ability to develop trustworthy systems, i.e. systems that are verifiable and preserve the voter's privacy. Building such a system is still an open research problem.Our contribution is a new Internet voting system: EVIV, a highly sound End-to-end Verifiable Internet Voting system, which offers full voter's mobility and preserves the voter's privacy from the vote casting PC even if the voter votes from a public PC, such as a PC at a cybercafé or at a public library. Additionally, EVIV has private vote verification mechanisms, in which the voter just has to perform a simple match of two small strings (4–5 alphanumeric characters), that detect and protect against vote manipulations both at the insecure vote client platform and at the election server side. 
32|-||LSB matching steganalysis based on patterns of pixel differences and random embedding|This paper presents a novel method for detection of LSB matching steganography in grayscale images. This method is based on the analysis of the differences between neighboring pixels before and after random data embedding. In natural images, there is a strong correlation between adjacent pixels. This correlation is disturbed by LSB matching generating new types of correlations. The presented method generates patterns from these correlations and analyzes their variation when random data are hidden. The experiments performed for two different image databases show that the method yields better classification accuracy compared to prior art for both LSB matching and HUGO steganography. In addition, although the method is designed for the spatial domain, some experiments show its applicability also for detecting JPEG steganography. 
32|-||A universal system for fair non-repudiable certified e-mail without a trusted third party|Certified e-mail delivery is a quickly emerging field with significant legal benefits for e-commerce and e-government. Existing service-providers provide individual solutions that rely on a trusted third party and are in general limited in their scope and interoperability; as a consequence, they are unsuitable for cross-border application as required by transnational unions like the EU.In contrast to existing service-providers, we present a working system for certified e-mail that does not rely on a trusted third party for fair non-repudiation of receipt. We achieve fair non-repudiation of receipt through a novel protocol that involves splitting an encrypted message into a chain of parts, which the addressee gradually acquires, generating proof-of-receipt for each individual part. The protocol cryptographically prevents the addressee from obtaining the message in case they terminate the protocol prematurely.The universality of the presented system makes it feasible for unobtrusive operation using existing user agents and e-mail providers. The system is presented as a proof-of-concept implementation for the open-source Mozilla Thunderbird user agent. 
32|-||The state of the art of application restrictions and sandboxes: A survey of application-oriented access controls and their shortfalls|Under most widely-used security mechanisms the programs users run possess more authority than is strictly necessary, with each process typically capable of utilising all of the user's privileges. Consequently such security mechanisms often fail to protect against contemporary threats, such as previously unknown (‘zero-day’) malware and software vulnerabilities, as processes can misuse a user's privileges to behave maliciously. Application restrictions and sandboxes can mitigate threats that traditional approaches to access control fail to prevent by limiting the authority granted to each process. This developing field has become an active area of research, and a variety of solutions have been proposed. However, despite the seriousness of the problem and the security advantages these schemes provide, practical obstacles have restricted their adoption.This paper describes the motivation for application restrictions and sandboxes, presenting an in-depth review of the literature covering existing systems. This is the most comprehensive review of the field to date. The paper outlines the broad categories of existing application-oriented access control schemes, such as isolation and rule-based schemes, and discusses their limitations. Adoption of these schemes has arguably been impeded by workflow, policy complexity, and usability issues. The paper concludes with a discussion on areas for future work, and points a way forward within this developing field of research with recommendations for usability and abstraction to be considered to a further extent when designing application-oriented access controls. 
32|-||Security-related behavior in using information systems in the workplace: A review and synthesis|Security-related behavior in the workplace has recently drawn much attention from scholars in the information systems literature. Many studies, however, have reported inconsistent and sometimes contradictory results about the effects of some key factors such as sanctions. We argue that one of the reasons causing the inconsistent findings is the divergent conceptualizations of security-related behavior. In this paper, we conducted an extensive review of the divergent concepts. Many of the concepts overlap with each other on some dimensions and yet are different on others. By delineating and synthesizing the differences, we proposed a framework for conceptualizing security-related behavior. The framework can facilitate the development of consistent and comparable terms and concepts in future studies. Implications for research are also discussed. 
32|-||A middleware approach for outsourcing data securely|Businesses that provide data storage facilities on the internet (IDP) have exploded recently. Such businesses provide the following benefits to end users: a) anytime, anywhere access to data; b) low cost; and c) good quality of service. Examples of data storage providers include Amazon S3 service, Windows SkyDrive, Nirvarnix, etc.Users face two challenges in utilizing the storage infrastructures of the IDPs: a) Heterogeneity: Different IDPs provide different interfaces to application developers to store and fetch data with them due to lack of accepted standards; and b) Security: Data outsourced to IDPs is vulnerable to attacks from the internet thieves and from malicious employees of IDPs.In this paper, we present the design of iDataGuard, which is a client side interoperable security middleware that adapts to the heterogeneity of interfaces of IDPs and enforces security constraints on outsourced data. This significantly simplifies the effort for application development. To combat heterogeneity, iDataGuard incorporates an abstract service model that can be easily customized to individual IDPs. To address the security challenges, iDataGuard supports a security model that protects the confidentiality and integrity of outsourced data. We propose a novel indexing technique that allows search on the encrypted data stored at the IDPs. We illustrate the feasibility/efficacy of iDataGuard by implementing the middleware and executing it on two of the popular IDPs, Amazon S3 service and Gmail.com. 
32|-||Hacking VoIP|
32|-||Metasploit the Penetration Tester's Guide|
33|-|http://www.sciencedirect.com/science/journal/01674048/33|Contents|
33|-||Editorial|
33|-||Organizational power and information security rule compliance|This paper analyzes power relationships and the resulting failure in complying with information security rules. It argues that an inability to understand the intricate power relationships in the design and implementation of information security rules leads to a lack of compliance with the intended policy. The argument is conducted through an empirical, qualitative case study set in a Swedish Social Services organization. Our findings indicate that various dimensions of power and how these relate to information security rules ensure adequate compliance. This also helps to improve configuration of security rules through proactive information security management. 
33|-||TCP Ack storm DoS attacks|We present Ack-storm DoS attacks, a new family of DoS attacks exploiting a subtle design flaw in the core TCP specifications. The attacks can be launched by a very weak MitM attacker, which can only eavesdrop occasionally and spoof packets (a Weakling in the Middle (WitM)). The attacks can reach theoretically unlimited amplification; we measured amplification of over 400,000 against popular web sites before aborting our trial attack.Ack storm DoS attacks are practical. In fact, they are easy to deploy in large scale, especially considering the widespread availability of open wireless networks, allowing an attacker easy WitM abilities to thousands of connections. Storm attacks can be launched against the access network, e.g. blocking address to proxy web server, against web sites, or against the Internet backbone. Storm attacks work against TLS/SSL connections just as well as against unprotected TCP connections, but fails against IPSec or link-layer encrypted connections.We show that Ack-storm DoS attacks can be easily prevented, by a simple fix to TCP, in either client or server, or using a packet-filtering firewall. 
33|-||Distributed security policy conformance|Security policy conformance is a crucial issue in large-scale critical cyber-infrastructure. The complexity of these systems, insider attacks, and the possible speed of an attack on a system necessitate an automated approach to assure a basic level of protection.This paper presents Odessa, a resilient system for monitoring and validating compliance of networked systems to complex policies. To manage the scale of infrastructure systems and to avoid single points of failure or attack, Odessa distributes policy validation across many network nodes. Partial delegation enables the validation of component policies and of liveness at the edge nodes of the network using redundancy to increase security. Redundant distributed servers aggregate data to validate more complex policies. Our practical implementation of Odessa resists Byzantine failure of monitoring using an architecture that significantly increases scalability and attack resistance. 
33|-||An authentication flaw in browser-based Single Sign-On protocols: Impact and remediations|Browser-based Single Sign-On (SSO) protocols relieve the user from the burden of dealing with multiple credentials thereby improving the user experience and the security. In this paper we show that extreme care is required for specifying and implementing the prototypical browser-based SSO use case. We show that the main emerging SSO protocols, namely SAML SSO and OpenID, suffer from an authentication flaw that allows a malicious service provider to hijack a client authentication attempt or force the latter to access a resource without its consent or intention. This may have serious consequences, as evidenced by a Cross-Site Scripting attack that we have identified in the SAML-based SSO for Google Apps and in the SSO available in Novell Access Manager v.3.1. For instance, the attack allowed a malicious web server to impersonate a user on any Google application. We also describe solutions that can be used to mitigate and even solve the problem. 
33|-||A generic approach to prevent board flooding attacks inÂ coercion-resistant electronic voting schemes|This paper presents a generic approach to prevent board flooding attacks in remote electronic voting schemes providing coercion-resistance. A key property of these schemes is the possibility of casting invalid votes to the public bulletin board, which are indistinguishable from proper votes. Exactly this possibility is crucial for making these schemes coercion-resistant, but it also opens doors for flooding the bulletin board with an enormous amount of invalid votes, eventually spoiling the efficiency of the tallying process. To prevent such attacks, we present a generic enhancement for these schemes, in which we restrict the total amount of votes accepted by the public bulletin board. For this, voters receive a certain amount of posting tickets, each of which allowing its owner to post a single vote to the bulletin board. The list of all posting tickets is published along with the electoral register. Votes with no valid posting ticket are immediately rejected by the bulletin board. The maximum amount of postings accepted by the bulletin board is thus bounded by the total number of issued posting tickets. This prevents a massive board flooding attack with a very large number of invalid votes and thus guarantees the efficiency of the tallying phase. Except with respect to forced vote abstention, our enhancement preserves all properties of the existing scheme in use. Although coercion by forced vote abstention cannot be ruled out entirely, such attacks are at least not scalable to a considerable portion of the electorate. 
34|-|http://www.sciencedirect.com/science/journal/01674048/34|Contents|
34|-||Editorial|
34|-||Assessing security threat scenarios for utility-based reputation model in grids|Trust and reputation models play an important role in enabling trusted computations over large-scale distributed grids. Many models have been recently proposed and implemented within trust management systems. Nevertheless, the existing approaches usually assess performance of models in terms of resource management while less attention is paid to the analysis of security threat scenarios for such models. In this paper, we assess the most important and critical security threats for a utility-based reputation model in grids. The existing model is extended to address these threat scenarios. With simulations that were run using data collected from the EGEE Grid-Observatory project, we analyse efficiency of the utility-based reputation model against these threats. 
34|-||On the optimality of cooperative intrusion detection forÂ resource constrained wireless networks|The problem of cooperative intrusion detection in battery-powered wireless mesh and sensor networks is challenging, primarily because of the limited resources available to participating nodes. Although the problem has received some attention from the research community, little is known about the tradeoffs among different objectives, such as high network performance, low power consumption, low delay in information collection and high security effectiveness. This article proposes, to the best of our knowledge for the first time, cooperative intrusion detection functions that take into account multiple objectives simultaneously. We formulate the problem of identifying the type of intrusion detection function each node runs, as a multi-objective optimization problem, and propose solutions based on genetic algorithms. Through extensive simulations we demonstrate that our solutions are scalable to large networks, and are characterized by a small variance in the normalized fitness value of individual/single objectives and by a small attack detection/reporting delay. In a real implementation/evaluation we demonstrate that our cooperative intrusion detection system achieves a higher detection rate (93%) than state of art solutions. 
34|-||A computer forensic method for detecting timestamp forgery in NTFS|In this paper, we present a computer forensic method for detecting timestamp forgeries in the Windows NTFS file system. It is difficult to know precisely that the timestamps have been changed by only examining the timestamps of the file itself. If we can find the past timestamps before any changes to the file are made, this can act as evidence of file time forgery. The log records operate on files and leave large amounts of information in the $LogFile that can be used to reconstruct operations on the files and also used as forensic evidence. Log record with 0x07/0x07 opcode in the data part of Redo/Undo attribute has timestamps which contain past-and-present timestamps. The past-and-present timestamps can be decisive evidence to indicate timestamp forgery, as they contain when and how the timestamps were changed. We used file time change tools that can easily be found on Internet sites. The patterns of the timestamp change created by the tools are different compared to those of normal file operations. Seven file operations have ten timestamp change patterns in total by features of timestamp changes in the $STANDARD_INFORMATION attribute and the $FILE_NAME attribute. We made rule sets for detecting timestamp forgery based on using difference comparison between changes in timestamp patterns by the file time change tool and normal file operations. We apply the forensic rule sets for “.txt”, “.docx” and “.pdf” file types, and we show the effectiveness and validity of the proposed method. The importance of this research lies in the fact that we can find the past time in $LogFile, which gives decisive evidence of timestamp forgery. This makes the timestamp active evidence as opposed to simply being passive evidence. 
34|-||Delegate the smartphone user? Security awareness in smartphone platforms|Smartphone users increasingly download and install third-party applications from official application repositories. Attackers may use this centralized application delivery architecture as a security and privacy attack vector. This risk increases since application vetting mechanisms are often not in place and the user is delegated to authorize which functionality and protected resources are accessible by third-party applications. In this paper, we mount a survey to explore the security awareness of smartphone users who download applications from official application repositories (e.g. Google Play, Apple's App Store, etc.). The survey findings suggest a security complacency, as the majority of users trust the app repository, security controls are not enabled or not added, and users disregard security during application selection and installation. As a response to this security complacency we built a prediction model to identify users who trust the app repository. The model is assessed, evaluated and proved to be statistically significant and efficient. 
34|-||A taxonomy and survey of attacks on digital signatures|Non-repudiation is a desired property of current electronic transactions, by which a further repudiation of the commitments made by any involved party is prevented. Digital signatures are recognized by current standards and legislation as non-repudiation evidence that can be used to protect the parties involved in a transaction against the other's false denial about the occurrence of a certain event. However, the reliability of a digital signature should determine its capability to be used as valid evidence. The inevitability of vulnerabilities in technology and the non-negligible probability of an occurrence of security threats would make non-repudiation of evidence difficult to achieve. We consider that it is of the utmost importance to develop appropriate tools and methods to assist in designing and implementing secure systems in a way that reliable digital signatures can be produced. In this paper, a comprehensive taxonomy of attacks on digital signatures is presented, covering both the signature generation and verification phases. The taxonomy will enable a rigorous and systematic analysis of the causes that may subvert the signature reliability, allowing the identification of countermeasures of general applicability. In addition, an intensive survey of attacks classified under our taxonomy is given. 
34|-||Analyzing the security of Windows 7 and Linux for cloud computing|We review and analyze the major security features and concerns in deploying modern commodity operating systems such as Windows 7 and Linux 2.6.38 in a cloud computing environment. We identify the security weaknesses and open challenges of these two operating systems when deployed in the cloud environment. In particular, we examine and compare various operating system security features which are critical in providing a secure cloud. These security features include authentication, authorization and access control, physical memory protection, privacy and encryption of stored data, network access and firewalling capabilities, and virtual memory. 
34|-||Phishing detection and impersonated entity discovery using Conditional Random Field and Latent Dirichlet Allocation|Phishing is an attempt to steal users' personal and financial information such as passwords, social security and credit card numbers, via electronic communication such as e-mail and other messaging services. Attackers pretend to be from a legitimate organization and direct users to a fake website that resembles a legitimate website, which is then used to collect users' personal information. In this paper, we propose a novel methodology to detect phishing attacks and to discover the entity/organization that the attackers impersonate during phishing attacks. The proposed multi-stage methodology employs natural language processing and machine learning. The methodology first discovers (i) named entities, which includes names of people, organizations, and locations; and (ii) hidden topics, using (a) Conditional Random Field (CRF) and (b) Latent Dirichlet Allocation (LDA) operating on both phishing and non-phishing data. Utilizing topics and named entities as features, the next stage classifies each message as phishing or non-phishing using AdaBoost. For messages classified as phishing, the final stage discovers the impersonated entity using CRF. Experimental results show that the phishing classifier detects phishing attacks with no misclassification when the proportion of phishing emails is less than 20%. The F-measure obtained was 100%. Our approach also discovers the impersonated entity from messages that are classified as phishing, with a discovery rate of 88.1%. The automatic discovery of impersonated entity from phishing helps the legitimate organization to take down the offending phishing site. This protects their users from falling for phishing attacks, which in turn leads to satisfied customers. Automatic discovery of an impersonated entity also helps email service providers to collaborate with each other to exchange attack information and protect their customers. 
34|-||Special issue on trust in cyber, physical and social computing: Call for papers|
35|-|http://www.sciencedirect.com/science/journal/01674048/35|Contents|
35|-||Editorial|
35|-||ESPOONERBAC: Enforcing security policies in outsourced environments|Data outsourcing is a growing business model offering services to individuals and enterprises for processing and storing a huge amount of data. It is not only economical but also promises higher availability, scalability, and more effective quality of service than in-house solutions. Despite all its benefits, data outsourcing raises serious security concerns for preserving data confidentiality. There are solutions for preserving confidentiality of data while supporting search on the data stored in outsourced environments. However, such solutions do not support access policies to regulate access to a particular subset of the stored data.For complex user management, large enterprises employ Role-Based Access Controls (RBAC) models for making access decisions based on the role in which a user is active in. However, RBAC models cannot be deployed in outsourced environments as they rely on trusted infrastructure in order to regulate access to the data. The deployment of RBAC models may reveal private information about sensitive data they aim to protect. In this paper, we aim at filling this gap by proposing ESPOONERBAC for enforcing RBAC policies in outsourced environments. ESPOONERBAC enforces RBAC policies in an encrypted manner where a curious service provider may learn a very limited information about RBAC policies. We have implemented ESPOONERBAC and provided its performance evaluation showing a limited overhead, thus confirming viability of our approach. 
35|-||Modular square root puzzles: Design of non-parallelizable and non-interactive client puzzles|Denial of Service (DoS) attacks aiming to exhaust the resources of a server by overwhelming it with bogus requests have become a serious threat. Especially protocols that rely on public key cryptography and perform expensive authentication handshakes may be an easy target. A well-known countermeasure against resource depletion attacks are client puzzles. The victimized server demands from the clients to commit computing resources before it processes their requests. To get service, a client must solve a cryptographic puzzle and submit the right solution. Existing client puzzle schemes have some drawbacks. They are either parallelizable, coarse-grained or can be used only interactively. In case of interactive client puzzles where the server poses the challenge an attacker might mount a counterattack on the clients by injecting faked packets with bogus puzzle parameters bearing the server's sender address. In this paper we introduce a novel scheme for client puzzles which relies on the computation of square roots modulo a prime. Modular square root puzzles are non-parallelizable, i.e., the solution cannot be obtained faster than scheduled by distributing the puzzle to multiple machines or CPU cores, and they can be employed both interactively and non-interactively. Our puzzles provide polynomial granularity and compact solution and verification functions. Benchmark results demonstrate the feasibility of our approach to mitigate DoS attacks on hosts in 1 or even 10 Gbit networks. In addition, we show how to raise the efficiency of our puzzle scheme by introducing a bandwidth-based cost factor for the client. Furthermore, we also investigate the construction of client puzzles from modular cube roots. 
35|-||Budget-aware Role Based Access Control|The suitability of Role Based Access Control (RBAC) is being challenged in dynamic environments like healthcare. In an RBAC system, a user's legitimate access may be denied if their need has not been anticipated by the security administrator at the time of policy specification. Alternatively, even when the policy is correctly specified an authorised user may accidentally or intentionally misuse the granted permission. The heart of the challenge is the intrinsic unpredictability of users' operational needs as well as their incentives to misuse permissions. In this paper we propose a novel Budget-aware Role Based Access Control (B-RBAC) model that extends RBAC with the explicit notion of budget and cost, where users are assigned a limited budget through which they pay for the cost of permissions they need. We propose a model where the value of resources are explicitly defined and an RBAC policy is used as a reference point to discriminate the price of access permissions, as opposed to representing hard and fast rules for making access decisions. This approach has several desirable properties. It enables users to acquire unassigned permissions if they deem them necessary. However, users misuse capability is always bounded by their allocated budget and is further adjustable through the discrimination of permission prices. Finally, it provides a uniform mechanism for the detection and prevention of misuses. 
36|-|http://www.sciencedirect.com/science/journal/01674048/36|Contents|
36|-||Automatic and lightweight grammar generation for fuzz testing|Blackbox fuzz testing can only test a small portion of code when rigorously checking the well-formedness of input values. To overcome this problem, blackbox fuzz testing is performed using a grammar that delineates the format information of input values. However, it is almost impossible to manually construct a grammar if the input specifications are not known. We propose an alternative technique: the automatic generation of fuzzing grammars using API-level concolic testing. API-level concolic testing collects constraints at the library function level rather than the instruction level. While API-level concolic testing may be less accurate than instruction-level concolic testing, it is highly useful for speedily generating fuzzing grammars that enhance code coverage for real-world programs. To verify the feasibility of the proposed concept, we implemented the system for generating ActiveX control fuzzing grammars, named YMIR. The experiment results showed that the YMIR system was capable of generating fuzzing grammars that can raise branch coverage for ActiveX control using highly-structured input string by 15–50%. In addition, the YMIR system discovered two new vulnerabilities revealed only when input values are well-formed. Automatic fuzzing grammar generation through API-level concolic testing is not restricted to the testing of ActiveX controls; it should also be applicable to other string processing program whose source code is unavailable. 
36|-||New payment methods: A review of 2010â2012 FATF mutual evaluation reports|Traditionally, the financial sector is often seen as the gatekeepers of the Anti-Money Laundering/Counter Terrorism Financing (AML/CFT) regime. In recent years, new payment methods, particularly stored value prepaid cards and mobile money transfer systems, are increasingly been seen as a widely accepted payment method. However, they have also been highlighted as potential money laundering and terrorism financing instruments. This paper aims to provide an improved understanding of the money laundering and terrorism financing risk environment and hopefully, new payment method providers are better placed to manage new and emerging threats. A review of the compliance levels in 65 mutual evaluation (and follow-up) reports published by FATF in English between 1st of January 2010 and 31st of December 2012 suggests that there are still compliance issues in areas that might afford exploitative opportunities for transnational crime and terrorist networks – after all, global standards are only as strong as their weakest link. This can have detrimental effects on a country's national security through increasing risks of money laundering and financing of terrorism (e.g. due to regulatory arbitrage), and wastage due to the implementation of inappropriate regulatory measures. We conclude with a three-pronged evidence-based AML/CTF approach, with the aim of helping governments and key stakeholders to improve knowledge of the nature and dimensions to the problem, and of suitable risk management and mitigation strategies that would enable scarce resources in fighting money laundering and terrorism financing threats to be more effectively allocated and, hence, make the most impact. 
36|-||Validation of security protocol implementations from security objectives|Protocol security testing can verify and find the potential defects of protocols and their implementations to avoid possible threatening request attacks. It requires concrete experiment against a real, physical implementation. But with the growing complexity of the protocol, added to the multiplicity of possible malicious inputs, the combination of scenarios to be computed will increase to an explosive speed and become the main problem. To address this, we use the concept of Security Objectives to Protocol Security Testing, to generate the test cases on-the-fly. We propose the model, the approach and the algorithm for this protocol verification method and we present a case study with an authentication service. 
36|-||A novel iris and chaos-based random number generator|A novel kind of iris and chaotic-based random number generator (ICRNG) is developed from the unique randomness and unpredictability of iris. This method combines biometric feature extraction and random number generation in a novel way. Firstly, we get the non-deterministic source-iris image by iris acquisition equipment, then, we use chaotic function to eliminate the similar pattern in iris from the same person and get the unpredictable random sequence. We use auto-correlation function method and correlation coefficients method to show that ICRNG is linearly independent, then, we use BDS statistical test method to verify ICRNG is nonlinearly independent. Finally, the randomness of ICRNG is verified by histogram analysis, information analysis, sensitivity analysis, FIPS 140-2 tests and NIST SP 800-22 tests. Hence, ICRNG possess satisfactory performance and can be implemented on common PC platform. 
36|-||Using network-based text analysis to analyze trends in Microsoft's security innovations|As the use of networked computers and digital data increase, so have the reports of data compromise and malicious cyber-attacks. Increased use and reliance on technologies complicate the process of providing information security. This expanding complexity in supplying data security requirements coupled with the increased recognition of the value of information, have led to the need to quickly advance the information security area. In this paper, we examine the maturation of the information security area by analyzing the innovation activity of one of the largest and most ubiquitous information technology companies, Microsoft. We conduct a textual analysis of their patent application activity in the information security domain since the early 2000's using a novel text analysis approach based on concepts from social network analysis and algorithmic classification. We map our analysis to focal areas in information security and examine it against Microsoft's own history, in order to determine the depth and breadth of Microsoft's innovations. Our analysis shows the relevance of using a network-based text analysis. Specifically, we find that Microsoft has increasingly emphasized topics that fall into the identity and access management area. We also show that Microsoft's innovations in information security showed tremendous growth after their Trustworthy Computing Initiative was announced. In addition, we are able to determine areas of focus that correspond to Microsoft's major vulnerabilities. These findings indicate that while Microsoft is still actively, albeit not always successfully, fighting vulnerabilities in their products, they are quite vigorously and broadly innovating in the information security area. 
37|-|http://www.sciencedirect.com/science/journal/01674048/37|Contents|
37|-||Editorial|
37|-||A novel agent-based approach to detect sinkhole attacks inÂ wireless sensor networks|Nowadays, Wireless Sensor Networks (WSNs) are widely used in many areas, especially in military operations and monitoring applications. Their wireless nature makes them very attractive to attackers, so its security system plays a vital role. Due to the limitations on resources, such as energy and storage, the security mechanism of WSNs have to be considered differently from traditional networks. Over the past years researchers have encouraged the use of mobile agents as a new and smart paradigm for distributed applications to overcome the limitations of sensor nodes. In this paper a defensive mechanism will be proposed against sinkhole attacks using mobile agents. We use mobile agents to aware every node from its trusted neighbors through a three-step negotiation so they do not listen to the traffics generated by malicious nodes. We evaluate our work in terms of energy consumption, packet loss rate, throughput and agent overhead caused by mobility and communication. 
37|-||Mutual-friend based attacks in social network systems|Recently, we have seen a rapid growth of social networking systems (SNSs). In most SNSs, a user can configure his privacy settings to indicate who can or cannot see his friend list. Usually, SNSs, such as LinkedIn and Google Plus, also include a feature that allows a user to query mutual friends between him and any other user he can reach using the available public search feature in SNSs. While such a mutual friend feature is very helpful in letting users find new friends and connect to them, in this paper, we show that it also raises significant privacy concerns as an adversary can use it to find out some or all of the victim's friends, although, as per the privacy settings of the victim, the adversary is not authorized to see his friend list directly. We show that by using mutual friend queries, an attacker can launch privacy attacks that we refer to as mutual-friend based attacks to identify friends and distant neighbors of targeted users. We analyze these attacks and identify various attack structures that an attacker can use to build attack strategies, using which an attacker can identify a user's friends and his distant neighbors. Through simulations, we demonstrate that mutual-friend based attacks are effective. For instance, one of the simulation results show that an attacker using just one attacker node can identify more than 60% of a user's friends. 
37|-||Monitoring information security risks within health care|This paper presents an overview of possible risks to the security of health care data. These risks were detected with a novel approach to information security. It is based on the philosophy that information security risk monitoring should include human and societal factors, and that collaboration between organisations and experts is essential to gain knowledge about potential risks. The methodology uses a mixed methods approach including a quantitative analysis of historical security incident data and expert elicitation through a Delphi study. The result is an overview of the possible socio-technical risks that a panel of experts expect to materialise in health care organisations in the near future. These risks include (amongst others): staff leaving data assets unattended on the premises and these assets consequently go missing, staff sharing passwords to access patient data and staff sending email containing personal patient data to the wrong addressee thus disclosing data to unauthorised persons. The expert panel recognized risks from current discussion topics such as outsourcing, but these risks are still considered to appear less frequently than the more traditional information security risks. Furthermore, the panel did not estimate a high frequency of occurrence of socio-technical information security risks caused by new technologies such as cloud computing or RFID. 
37|-||A whitelist-based countermeasure scheme using a Bloom filter against SIP flooding attacks|Since SIP uses a text-based message format and is open to the public Internet, it is exposed to a number of potential threats of denial of service (DoS) by flooding attacks. Although several approaches have been proposed to detect and counteract SIP flooding attacks, most of these do not provide effective countervailing schemes to protect normal messages from abnormal ones after attacks have been detected. In addition, these approaches have some limitations in large user environments for SIP-based multimedia services. In this paper, a whitelist-based countermeasure scheme is proposed, to protect both normal SIP users and servers from malicious flooding attacks. To construct the whitelist, a Bloom filter approach is used, to reduce memory requirements and computational complexity. We use the non-membership ratio as a measure for the attack detection, instead of using the message rate usually used in conventional schemes. It is shown that the proposed method can provide more robust detection performances. 
37|-||A framework for prototyping and testing data-only rootkit attacks|Kernel rootkits—attacks which modify a running operating system kernel in order to hide an attacker's presence—are significant threats. Recent advances in rootkit defense technology will force rootkit threats to rely on only modifying kernel data structures without injecting and executing any new code; however these data-only kernel rootkit attacks are still both realistic and powerful. In this work we present DORF, a framework for prototyping and testing data-only rootkit attacks. DORF is an object-oriented framework that allows researchers to construct attacks that can be easily ported between various Linux distributions and versions. The current implementation of DORF contains a group of existing and new data-only attacks, and the portability of DORF is demonstrated by porting it to 6 different Linux distributions. The goal of DORF is to allow researchers to construct repeatable experiments with little effort, which will in turn advance research into data-only attacks and defenses. 
37|-||Active cyber defense with denial and deception: AÂ cyber-wargame experiment|In January 2012, MITRE performed a real-time, red team/blue team cyber-wargame experiment. This presented the opportunity to blend cyber-warfare with traditional mission planning and execution, including denial and deception tradecraft. The cyber-wargame was designed to test a dynamic network defense cyber-security platform being researched in The MITRE Corporation's Innovation Program called Blackjack, and to investigate the utility of using denial and deception to enhance the defense of information in command and control systems.The Blackjack tool failed to deny the adversary access to real information on the command and control mission system. The adversary had compromised a number of credentials without the computer network defenders' knowledge, and thereby observed both the real command and control mission system and the fake command and control mission system. However, traditional denial and deception techniques were effective in denying the adversary access to real information on the real command and control mission system, and instead provided the adversary with access to false information on a fake command and control mission system. 
37|-||Quality of security metrics and measurements|Quantification of information security can be used to obtain evidence to support decision-making about the security performance of software systems. Knowledge about the relational importance of the main quality criteria of security metrics can help build security metrology models based on practical needs. This paper presents the results of a quantitative security metrics expert survey of 141 respondents, and an associated interview study, regarding the prioritization of 19 quality criteria of security metrics identified in the literature. The interviews were used to validate the survey results and to obtain further information on the findings. The results identified three foundational quality criteria of security metrics: correctness, measurability, and meaningfulness. These criteria form the basis for credibility and sufficiency for security metrics and associated measurements. Moreover, usability was seen as an important criterion. The paper analyzes the foundational and related quality criteria and proposes a model of them. 
37|-||Defeating line-noise CAPTCHAs with multiple quadratic snakes|Optical character recognition (OCR) is one of the fundamental problems in artificial intelligence and image processing, but recent progress in OCR represents a security challenge for Web sites that throttle requests with image based CAPTCHAs (Completely Automated Public Turing Tests to Tell Computers and Humans Apart). A CAPTCHA is challenge-response test placed within web forms to determine whether the user is human. Unfortunately, algorithms capable of solving image based CAPTCHAs can be used to create spam accounts and design malicious denial of service (DoS) attacks, causing financial and social damage. The problem of defeating digital image CAPTCHAs is thus twofold. On the one hand, it is an important problem in artificial intelligence and image processing. On the other hand, publicly available CAPTCHAs that are not tested against state of the art machine recognition algorithms may make the systems vulnerable to attack by software bots.This paper considers a very important subclass of text CAPTCHAs, those characterized by salt and pepper noise combined with line (curve) noise. Thus far, attacks on CAPTCHAs with this type of noise have used relatively simple image processing methods with some success, but state-of-the-art segmentation methods have not been fully exploited. In this paper, we propose and benchmark two strong segmentation methods. The first method is a modification of a multiple quadratic snake proposed for road extraction from satellite images. The second competing method is a boundary tracing routine available in the OpenCV open source library.A first numerical experiment indicates excellent accuracy for both methods. A second experiment on human recognition shows that the CAPTCHAs used in the study are already near the threshold of being too hard for humans. Finally, a third numerical experiment presents a more difficult set of CAPTCHAs with the addition of anti-binarization methods. The snake-based method is shown to be more resilient to anti-binarization schemes than boundary tracing and state-of-the art projection-based attacks on CAPTCHAs.Since CAPTCHAs corrupted by small line noise are shown to be difficult for humans and relatively easy for our algorithm, CAPTCHA designers should introduce more challenging distortions into their CAPTCHAs, lest the security of systems based on them be compromised. 
37|-||Anonymous authentication for privacy-preserving IoT target-driven applications|The Internet of Things (IoT) will be formed by smart objects and services interacting autonomously and in real-time. As an application scenario, household smart meters will provide real-time neighborhood information which enables a smart community to cooperatively identify patterns, adapt consumption and improve overall quality of life, making the shared environment more sustainable. There is, in these types of settings, a major need toward securing all communications, placing equal effort on guaranteeing privacy properties (e.g., participant anonymity, unlinkability) as on assuring security properties (e.g., content authenticity). In this article, we present a fully decentralized anonymous authentication protocol aimed at encouraging the implementation of privacy-preserving IoT target-driven applications. The system is set up by an ad-hoc community of decentralized founding nodes. From then on, nodes can interact, being participants of cyber-physical systems, preserving full anonymity. We also present a performance and security analysis of the proposed system. 
37|-||Efficient authentication for fast handover in wireless mesh networks|We propose new authentication protocols to support fast handover in IEEE 802.11-based wireless mesh networks. The authentication server does not need to be involved in the handover authentication process. Instead, mesh access points directly authenticate mobile clients using tickets, avoiding multi-hop wireless communications in order to minimize the authentication delay. Numerical analysis and simulation results show that the proposed handover authentication protocol significantly outperforms IEEE 802.11 authentication in terms of authentication delay. 
37|-||Privacy-preserving publishing of opinion polls|Public opinion is the belief or thoughts of the public regarding a particular topic, especially one regarding politics, religion or social issues. Opinions may be sensitive since they may reflect a person's perspective, understanding, particular feelings, way of life, and desires. On one hand, public opinion is often collected through a central server which keeps a user profile for each participant and needs to publish this data for research purposes. On the other hand, such publishing of sensitive information without proper de-identification puts individuals' privacy at risk, thus opinions must be anonymized prior to publishing. While many anonymization approaches for tabular data with single sensitive attribute have been introduced, the proposed approaches do not readily apply to opinion polls. This is because opinions are generally collected on many issues, thus opinion databases have multiple sensitive attributes. Finding and enforcing anonymization models that work on datasets with multiple sensitive attributes while allowing risk analysis on the publisher side is not a well-studied problem. In this work, we identify the privacy problems regarding public opinions and propose a new probabilistic privacy model MSA-diversity, specifically defined on datasets with multiple sensitive attributes. We also present a heuristic anonymization technique to enforce MSA-diversity. Experimental results on real data show that our approach clearly outperforms the existing approaches in terms of anonymization accuracy. 
37|-||Security as a theoretical attribute construct|This paper provides an overview of the field of security metrics and discusses results of a survey of security experts on the topic. It describes a new framework for developing security metrics that focuses on effectiveness measures while maintaining measures of correctness. It introduces a view of security as a theoretical concept which encapsulates multiple aspects of a system. Viewing security as a theoretical attribute construct promotes the recognition that multiple characteristics and features of a system are required to make it secure. The view also motivates a sharp focus on system aspects which exhibit a measurable security attribute. The framework is illustrated with a case study. 
37|-||Enhancing IDS performance through comprehensive alert post-processing|Intrusion detection systems (IDS) are among the most common countermeasures against network attacks. In order to improve the alerts obtained from them, various methods of post-processing have been proposed. These methods usually try to alleviate specific drawbacks of intrusion detection. We propose a system that is a post-processing solution. The input of our system is a set of multiple IDS sensors alert sets. Each set's alerts are aggregated in order to improve their quality, before multiple alert sets merge into one general alert set. Then, a low clustering procedure allows the system to hypothesize about missed security events and to create relevant alerts. The main clustering phase comes next, before the final step, in which a clusters graph is generated to produce a high level presentation of the security events. The system has been tested using the DARPA 2000 dataset, as well as a live network dataset, and has produced satisfactory results. 
37|-||Onion routing circuit construction via latency graphs|The use of anonymity-based infrastructures and anonymisers is a plausible solution to mitigate privacy problems on the Internet. Tor (short for The onion router) is a popular low-latency anonymity system that can be installed as an end-user application on a wide range of operating systems to redirect the traffic through a series of anonymising proxy circuits. The construction of these circuits determines both the latency and the anonymity degree of the Tor anonymity system. While some circuit construction strategies lead to delays which are tolerated for activities like Web browsing, they can make the system vulnerable to linking attacks. We evaluate in this paper three classical strategies for the construction of Tor circuits, with respect to their de-anonymisation risk and latency performance. We then develop a new circuit selection algorithm that considerably reduces the success probability of linking attacks while keeping a good degree of performance. We finally conduct experiments on a real-world Tor deployment over PlanetLab. Our experimental results confirm the validity of our strategy and its performance increase for Web browsing. 
37|-||Secloud: A cloud-based comprehensive and lightweight security solution for smartphones|As smartphones are becoming more complex and powerful to provide better functionalities, concerns are increasing regarding security threats against their users. Since smartphones use a software architecture similar to PCs, they are vulnerable to the same classes of security risks. Unfortunately, smartphones are constrained by their limited resources that prevent the integration of advanced security monitoring solutions that work with traditional PCs. We propose Secloud, a cloud-based security solution for smartphone devices. Secloud emulates a registered smartphone device inside a designated cloud and keeps it synchronized by continuously passing the device inputs and network connections to the cloud. This allows Secloud to perform a resource-intensive security analysis on the emulated replica that would otherwise be infeasible to run on the device itself. We demonstrate the practical feasibility of Secloud through a prototype for Android devices and illustrate its resource effectiveness by comparing it with on-device solutions. 
38|-|http://www.sciencedirect.com/science/journal/01674048/38|Contents|
38|-||Cybercrime in the Digital Economy - Editorial|
38|-||Selecting a Cloud Service Provider in the age of cybercrime|The benefits of resorting to the cloud, as an efficient way to provide services, have long been recognised in the academic and industrial literature. However, as more and more companies are beginning to embrace the trend, it has also become clearer that the model offers unprecedented opportunities to cybercriminals: either by enabling them to compromise a myriad of services in a single shot or by allowing cyber-criminals to amplify their capabilities through a leverage of the technology offered by the cloud.This paper highlights the importance of an informed choice of a Cloud Service Provider (CSP) in minimising one's exposure to the insecurity of a cloud context. The paper proposes a well-defined approach, known as the Complete-Auditable-Reportable or C.A.RE, as a way to minimise one's exposure to the insecurity we live within the cloud. The C.A.RE approach helps to determine the adequacy of a CSP sponsored security by assessing its completeness in addressing most, if not all, risks that a service may be exposed to; the potential of that security to be adapted upon the identification of a security vulnerability during an audit, and how transparently such information is shared with the concerned Cloud Service Consumer (CSC). A level of assurance is associated to each of the C.A.RE parameters in order to help determine the overall trustworthiness of a CSP.The analysis and comparison of the C.A.RE approach to a well-known guideline as the Cloud Service Security Alliance guidelines, reveals that C.A.RE offers a clear and efficient way in determining a Trusted Cloud Service. 
38|-||Smart control of operational threats in control substations|Any deliberate or unsuitable operational action in control tasks of critical infrastructures, such as energy generation, transmission and distribution systems that comprise sub-domains of a Smart Grid, could have a significant impact on the digital economy: without energy, the digital economy cannot live. In addition, the vast majority of these types of critical systems are configured in isolated locations where their control depends on the ability of a few, supposedly trustworthy, human operators. However, this assumption of reliability is not always true. Malicious human operators (criminal insiders) might take advantage of these situations to intentionally manipulate the critical nature of the underlying infrastructure. These criminal actions could be not attending to emergency events, inadequately responding to incidents or trying to alter the normal behaviour of the system with malicious actions. For this reason, in this paper we propose a smart response mechanism that controls human operators' operational threats at all times. Moreover, the design of this mechanism allows the system to be able to not only evaluate by itself, the situation of a particular scenario but also to take control when areas are totally unprotected and/or isolated. The response mechanism, which is based on Industrial Wireless Sensor Networks (IWSNs) for the constant monitoring of observed critical infrastructures, on reputation for controlling human operators' actions, and on the ISA100.11a standard for alarm management, has been implemented and simulated to evaluate its feasibility for critical contexts. 
38|-||Investigating phishing victimization with the HeuristicâSystematic Model: A theoretical framework and an exploration|To the extent that phishing has become a serious threat to information security, there has been rather limited theory-grounded research on this burgeoning phenomenon. In this paper, we develop a theoretical model of victimization by phishing based on the Heuristic–Systematic Model of information processing. We argue that the Heuristic–Systematic Model offers an ideal theoretical framework for investigating the psychological mechanism underlying the effectiveness of phishing attacks. An exploratory experiment is presented to validate the research model based on the theory. 
38|-||A game theoretic defence framework against DoS/DDoS cyber attacks|Game-theoretic approaches have been previously employed in the research area of network security in order to explore the interaction between an attacker and a defender during a Distributed Denial of Service (DDoS) attack scenario. Existing literature investigates payoffs and optimal strategies for both parties, in order to provide the defender with an optimal defence strategy. In this paper, we model a DDoS attack as a one-shot, non-cooperative, zero-sum game. We extend previous work by incorporating in our model a richer set of options available to the attacker compared to what has been previously achieved. We investigate multiple permutations in terms of the cost to perform an attack, the number of attacking nodes, malicious traffic probability distributions and their parameters. We analytically demonstrate that there exists a single optimal strategy available to the defender. By adopting it, the defender sets an upper boundary to attacker payoff, which can only be achieved if the attacker is a rational player. For all other attack strategies (those adopted by irrational attackers), attacker payoff will be lower than this boundary. We preliminary validate this model via simulations with the ns2 network simulator. The simulated environment replicates the analytical model's parameters and the results confirm our model's accuracy. 
38|-||Smartphone sensor data as digital evidence|The proliferation of smartphones introduces new opportunities in digital forensics. One of the reasons is that smartphones are usually equipped with sensors (e.g. accelerometer, proximity sensor, etc.), hardware which can be used to infer the user's context. This context may be useful in a digital investigation, as it can aid in the rejection or acceptance of an alibi, or even reveal a suspect's actions or activities. Nonetheless, sensor data are volatile, thus are not available in post-mortem analysis. Thus, the only way to timely acquire them, in case such a need arises during a digital investigation, is by software that collects them when they are generated by the suspect's actions. In this paper we examine the feasibility of ad-hoc data acquisition from smartphone sensors by implementing a device agent for their collection in Android, as well as a protocol for their transfer. Then, we discuss our experience regarding the data collection of smartphone sensors, as well as legal and ethical issues that arise from their collection. Finally, we describe scenarios regarding the agent's preparation and use in a digital investigation. 
38|-||Assessing the genuineness of events in runtime monitoring of cyber systems|Monitoring security properties of cyber systems at runtime is necessary if the preservation of such properties cannot be guaranteed by formal analysis of their specification. It is also necessary if the runtime interactions between their components that are distributed over different types of local and wide area networks cannot be fully analyzed before putting the systems in operation. The effectiveness of runtime monitoring depends on the trustworthiness of the runtime system events, which are analyzed by the monitor. In this paper, we describe an approach for assessing the trustworthiness of such events. Our approach is based on the generation of possible explanations of runtime events based on a diagnostic model of the system under surveillance using abductive reasoning, and the confirmation of the validity of such explanations and the runtime events using belief based reasoning. The assessment process that we have developed based on this approach has been implemented as part of the EVEREST runtime monitoring framework and has been evaluated in a series of simulations that are discussed in the paper. 
38|-||From information security to cyber security|The term cyber security is often used interchangeably with the term information security. This paper argues that, although there is a substantial overlap between cyber security and information security, these two concepts are not totally analogous. Moreover, the paper posits that cyber security goes beyond the boundaries of traditional information security to include not only the protection of information resources, but also that of other assets, including the person him/herself. In information security, reference to the human factor usually relates to the role(s) of humans in the security process. In cyber security this factor has an additional dimension, namely, the humans as potential targets of cyber attacks or even unknowingly participating in a cyber attack. This additional dimension has ethical implications for society as a whole, since the protection of certain vulnerable groups, for example children, could be seen as a societal responsibility. 
38|-||Integrated digital forensic process model|Digital forensics is an established research and application field. Various process models exist describing the steps and processes to follow during digital forensic investigations. During such investigations, it is not only the digital evidence itself that needs to prevail in a court of law; the process followed and terminology used should also be rigorous and generally accepted within the digital forensic community. Different investigators have been refining their own investigative methods, resulting in a variety of digital forensic process models. This paper proposes a standardized Digital Forensic Process Model to aid investigators in following a uniform approach in digital forensic investigations. 
39|PA|http://www.sciencedirect.com/science/journal/01674048/39/part/PA|Contents|
39|PA||Editorial|
39|PA||Botnet detection based on traffic behavior analysis and flow intervals|Botnets represent one of the most serious cybersecurity threats faced by organizations today. Botnets have been used as the main vector in carrying many cyber crimes reported in the recent news. While a significant amount of research has been accomplished on botnet analysis and detection, several challenges remain unaddressed, such as the ability to design detectors which can cope with new forms of botnets. In this paper, we propose a new approach to detect botnet activity based on traffic behavior analysis by classifying network traffic behavior using machine learning. Traffic behavior analysis methods do not depend on the packets payload, which means that they can work with encrypted network communication protocols. Network traffic information can usually be easily retrieved from various network devices without affecting significantly network performance or service availability. We study the feasibility of detecting botnet activity without having seen a complete network flow by classifying behavior based on time intervals. Using existing datasets, we show experimentally that it is possible to identify the presence of existing and unknown botnets activity with high accuracy even with very small time windows. 
39|PA||Behavior-based tracking: Exploiting characteristic patterns inÂ DNS traffic|We review and evaluate three techniques that allow a passive adversary to track users who have dynamic IP addresses based on characteristic behavioral patterns, i.e., without cookies or similar techniques. For this purpose we consider 1-Nearest-Neighbor classifiers, a Multinomial Naïve Bayes classifier and pattern mining techniques based on the criteria support and lift.For evaluation we focus on the case of a curious DNS resolver. Therefore, we analyze the effectiveness of the techniques using a common, large-scale dataset that contains the DNS queries issued by more than 3600 users over the course of two months. We find that behavior-based tracking is feasible: The best technique can link up to 85.4% of the surfing sessions of all users on a day-to-day basis. Moreover, for tracking to be effective only the most significant features or the most popular hostnames have to be considered.Our results indicate that users can degrade accuracy by changing their IP addresses more frequently, e.g., every few minutes. On the other hand, we find that the previously proposed DNS “range query” obfuscation techniques cannot prevent tracking reliably.Our findings are not limited to DNS traffic. Behavior-based tracking can be implemented by any adversary that has access to the web requests issued by users or their machines. 
39|PA||Covert communications through network configuration messages|Covert channels are a form of hidden communication that may violate the integrity of systems. Since their birth in Multi-Level Security systems in the early 70's they have evolved considerably, such that new solutions have appeared for computer networks mainly due to vague protocols specifications. In this paper we concentrate on short-range covert channels and analyze the opportunities of concealing data in various extensively used protocols today. From this analysis we observe several features that can be effectively exploited for subliminal data transmission in the Dynamic Host Configuration Protocol (DHCP). The result is a proof-of-concept implementation, HIDE_DHCP, which integrates three different covert channels each of which accommodate to different stealthiness and capacity requirements. Finally, we provide a theoretical and experimental analysis of this tool in terms of its reliability, capacity, and detectability. 
39|PA||Enforcing dynamic write privileges in data outsourcing|Users and companies are more and more resorting to external providers for storing their data and making them available to others. Since data sharing is typically selective (i.e., accesses to certain data should be allowed only to authorized users), there is the problem of enforcing authorizations on the outsourced data. Recently proposed approaches based on selective encryption provide convenient enforcement of read privileges, but are not directly applicable for supporting write privileges.In this paper, we extend selective encryption approaches to the support of write privileges. Our proposal enriches the approach based on key derivation of existing solutions and complements it with a hash-based approach for supporting write privileges. Enforcement of write privileges and of possible policy updates relies on the – controlled – cooperation of the external provider. Our solution also allows the data owner and the users to verify the integrity of the outsourced data. 
39|PA||Management of stateful firewall misconfiguration|Firewall configurations are evolving into dynamic policies that depend on protocol states. As a result, stateful configurations tend to be much more error prone. Some errors occur on configurations that only contain stateful rules. Others may affect those holding both stateful and stateless rules. Such situations lead to configurations in which actions on certain packets are conducted by the firewall, while other related actions are not. We address automatic solutions to handle these problems. Permitted states and transitions of connection-oriented protocols (in essence, on any layer) are encoded as automata. Flawed rules are identified and potential modifications are provided in order to get consistent configurations. We validate the feasibility of our proposal based on a proof of concept prototype that automatically parses existing firewall configuration files and handles the discovery of flawed rules according to our approach. 
39|PA||A framework for risk assessment in access control systems|We describe a framework for risk assessment specifically within the context of risk-based access control systems, which make authorization decisions by determining the security risk associated with access requests and weighing such security risk against operational needs together with situational conditions. Our framework estimates risk as a product of threat and impact scores. The framework that we describe includes four different approaches for conducting threat assessment: an object sensitivity-based approach, a subject trustworthiness-based approach and two additional approaches which are based on the difference between object sensitivity and subject trustworthiness. We motivate each of the four approaches with a series of examples. We also identify and formally describe the properties that are to be satisfied within each approach. Each of these approaches results in different threat orderings, and can be chosen based on the context of applications or preference of organizations. We also propose formulae to estimate the threat of subject–object accesses within each of the four approaches of our framework.We then demonstrate the application of our threat assessment framework for estimating the risk of access requests, which are initiated by subjects to perform certain actions on data objects, by using the methodology of NIST Special Publication 800-30. We show that risk estimates for access requests actually differ based on the threat assessment approach that has been chosen. Therefore, organizations must make prudent judgement while selecting a threat assessment function for risk-based access control systems. 
39|PA||Breaking and fixing the Android Launching Flow|The security model of the Android OS is based on the effective combination of a number of well-known security mechanisms (e.g. statically defined permissions for applications, the isolation offered by the Dalvik Virtual Machine, and the well-known Linux discretionary access control model). Although each security mechanism has been extensively tested and proved to be effective in isolation, their combination may suffer from unexpected security flaws. We show that this is actually the case by presenting a severe vulnerability in Android related to the application launching flow. This vulnerability is based on a security flaw affecting a kernel-level socket (namely, the Zygote socket). We also present an exploit of the vulnerability that allows a malicious application to mount a severe Denial-of-Service attack that makes the Android devices become totally unresponsive. Besides explaining the vulnerability (which affects all versions of Android up to version 4.0.3) we propose two fixes. One of the two fixes has been adopted in the official release of Android, starting with version 4.1. We empirically assess the impact of the vulnerability as well as the efficacy of the countermeasures on the end user. We conclude by extending our security analysis to the whole set of sockets, showing that other sockets do not suffer from the same vulnerability as the Zygote one. 
39|PB|http://www.sciencedirect.com/science/journal/01674048/39/part/PB|Contents|
39|PB||A privacy-aware continuous authentication scheme for proximity-based access control|Continuous authentication is mainly associated with the use of biometrics to guarantee that a resource is being accessed by the same user throughout the usage period. Wireless devices can also serve as a supporting technology for continuous authentication or even as a complete alternative to biometrics when accessing proximity-based services.In this paper we present the implementation of a secure, non-invasive continuous authentication scheme supported by the use of Wearable Wireless Devices (WWD), which allow users to gain access to proximity-based services while preserving their privacy. Additionally we devise an improved scheme that circumvents some of the limitations of our implementation. 
39|PB||A framework for continuous, transparent mobile device authentication|We address two distinct problems with de facto mobile device authentication, as provided by a password or sketch. Firstly, device activity is permitted on an all-or-nothing basis, depending on whether the user successfully authenticates at the beginning of a session. This ignores the fact that tasks performed on a mobile device have a range of sensitivities, depending on the nature of the data and services accessed. Secondly, users are forced to re-authenticate frequently due to the bursty nature that characterizes mobile device use. Owners react to this by disabling the mechanism, or by choosing a weak “secret”. To address both issues, we propose an extensible Transparent Authentication Framework that integrates multiple behavioral biometrics with conventional authentication to implement an effortless and continuous authentication mechanism. Our security and usability evaluation of the proposed framework showed that a legitimate device owner can perform all device tasks, while being asked to authenticate explicitly 67% less often than without a transparent authentication method. Furthermore, our evaluation showed that attackers are soon denied access to on-device tasks as their behavioral biometrics are collected. Our results support the creation of a working prototype of our framework, and provide support for further research into transparent authentication on mobile devices. 
39|PB||Gait and activity recognition using commercial phones|This paper presents the results of applying gait and activity recognition on a commercially available mobile smartphone, where both data collection and real-time analysis was done on the phone. The collected data was also transferred to a computer for further analysis and comparison of various distance metrics and machine learning techniques. In our experiment 5 users created each 3 templates on the phone, where the templates were related to different walking speeds. The system was tested for correct identification of the user or the walking activity with 20 new users and with the 5 enrolled users. The activities are recognised correctly with an accuracy of over 99%. For gait recognition the phone learned the individual features of the 5 enrolled participants at the various walk speeds, enabling the phone to afterwards identify the current user. The new Cross Dynamic Time Warping (DTW) Metric gives the best performance for gait recognition where users are identified correctly in 89.3% of the cases and the false positive probability is as low as 1.4%. 
39|PB||Don't make excuses! Discouraging neutralization to reduce IT policy violation|Past research on information technology (IT) security training and awareness has focused on informing employees about security policies and formal sanctions for violating those policies. However, research suggests that deterrent sanctions may not be the most powerful influencer of employee violations. Often, employees use rationalizations, termed neutralization techniques, to overcome the effects of deterrence when deciding whether or not to violate a policy. Therefore, neutralization techniques often are stronger than sanctions in predicting employee behavior. For this study, we examine “denial of injury,” “metaphor of the ledger,” and “defense of necessity” as relevant justifications for violating password policies that are commonly used in organizations as used in (Siponen and Vance, 2010). Initial research on neutralization in IS security has shown that results are consistent regardless of which type of neutralization is considered (Siponen and Vance, 2010). In this study, we investigate whether IT security communication focused on mitigating neutralization, rather than deterrent sanctions, can reduce intentions to violate security policies. Additionally, considering the effects of message framing in persuading individuals against security policy violations are largely unexamined, we predict that negatively-framed communication will be more persuasive than positively-framed communication. We test our hypotheses using the factorial survey method. Our results suggest that security communication and training that focuses on neutralization techniques is just as effective as communication that focuses on deterrent sanctions in persuading employees not to violate policies, and that both types of framing are equally effective. 
39|PB||Dynamic traffic awareness statistical model for firewall performance enhancement|Firewall is considered to be one of the most important security components in today's IP network architectures. Firewall performance has a significant impact on the overall network performance. In this paper, we propose a mechanism to improve firewall performance, using network traffic behavior and packet filtering statistics. Upon certain threshold qualification (Chi-square test), the proposed mechanism allows optimizing the filtering rules order and their corresponding fields order according to the divergence of the traffic behavior. That is, if the firewall system is stable, then the same current filtering rules and/or rule-fields orders are used for filtering the next network traffic window. Otherwise, an update of the filtering rules and/or rule-fields orders is required for filtering the next network traffic window. The numerical results obtained by simulation demonstrate that the proposed mechanism allow to improve significantly the firewall performance in terms of cumulative packet processing time even for small security policies. This improvement is a result of the minimization of the overhead corresponding to the frequency of updating the rule/field structures, as well as of using the optimum traffic window size. 
39|PB||A comprehensive study of multiple deductions-based algebraic trace driven cache attacks on AES|Existing trace driven cache attacks (TDCAs) can only analyze the cache events in the first two rounds or the last round of AES, which limits the efficiency of the attacks. Recently, Zhao et al. proposed the multiple deductions-based algebraic side-channel attack (MDASCA) to cope with the errors in leakage measurements and to exploit new leakage models. Their preliminary results showed that MDASCA can improve TDCAs and attack the AES implemented with a compact lookup table of 256 bytes. This paper performs a comprehensive study of MDASCA-based TDCAs (MDATDCA) on most of the AES implementations that are widely used. First, the key recovery in TDCA is depicted by an abstract model regardless of the specific attack techniques. Then, the previous work of TDCAs on AES is classified into three types and its limitations are analyzed. How to utilize the cache events with MDATDCA is presented and the overhead is also calculated. To evaluate MDATDCA on AES, this paper constructs a mathematical model to estimate the maximal number of leakage rounds that can be utilized and the minimal number of cache traces required for a successful MDATDCA. Extensive experiments are conducted under different implementations, attack scenarios and key lengths of AES. The experimental results are consistent with the theoretical analysis. Many improvements are achieved. For the first time, we show that TDCAs on AES-192 and AES-256 become possible with the MDATDCA technique. Our work attests that combining TDCAs with algebraic techniques is a very efficient way to improve cache attacks. 
39|PB||Cyber-physical security metric inference in smart grid critical infrastructures based on system administrators' responsive behavior|To protect complex power-grid control networks, efficient security assessment techniques are required. However, efficiently making sure that calculated security measures match the expert knowledge is a challenging endeavor. In this paper, we present EliMet, a framework that combines information from different sources and estimates the extent to which a control network meets its security objective. Initially, EliMet passively observes system operators' online reactive behavior against security incidents, and accordingly refines the calculated security measure values. To make the values comply with the expert knowledge, EliMet actively queries operators regarding those states for which sufficient information was not gained during the passive observation. Finally, EliMet makes use of the estimated security measure values for predictive situational awareness by ranking potential cyber-physical contingencies that the security administrators should plan for upfront. Our experimental results show that EliMet can optimally make use of prior knowledge as well as automated inference techniques to minimize human involvement and efficiently deduce the expert knowledge regarding individual states of that particular system. 
39|PB||AMTRAC: An administrative model for temporal role-based access control|Over the years, Role Based Access Control (RBAC) has received significant attention in system security and administration. The Temporal Role Based Access Control (TRBAC) model is an extension of RBAC that allows one to specify periodic enabling and disabling of roles in a role enabling base (REB). While decentralized administration and delegation of administrative responsibilities in large RBAC systems is managed using an administrative role based access control model like ARBAC97, no administrative model for TRBAC has yet been proposed. In this paper, we introduce such a model and name it AMTRAC (Administrative Model for Temporal Role based Access Control). AMTRAC defines a broad range of relations that control user-role assignment, role-permission assignment, role–role assignment and role enabling base assignment. Since the first three are similar to those in ARBAC97, the role enabling base assignment component has been discussed in detail in this paper. The different ways by which role enabling conditions of regular roles can be modified are first explained. We then show how to specify which of the administrative roles are authorized to modify the role enabling conditions of any regular role. An exhaustive set of commands for authorization enforcement along with their pre and postconditions is also presented. Together, this would facilitate practical deployment and security analysis of TRBAC systems. 
39|PB||Caller-REP: Detecting unwanted calls with caller social strength|Voice over IP (VoIP) is a cost effective mechanism for telemarketers and criminals to generate bulk spam calls. A challenge in managing a VoIP network is to detect spam calls without user involvement or content analysis. In this paper we present a novel content independent, non-intrusive approach based on caller trust and reputation to block spam callers in a VoIP network. Our approach uses call duration, interaction rate, and caller out-degree distribution to establish a trust network between VoIP users and computes the global reputation of a caller across the network. Our approach uses historical information for automatically determining a global reputation threshold below which a caller is declared as socially non-connected and as a spammer. No VoIP data-set is available for testing the detection mechanism. We verify the accuracy of our approach with synthetic data that we generate by randomly varying the call duration, call rate, and out-degree distributions of spammers and legitimate users. This evaluation shows that our approach can automatically detect spam callers in a network. Our approach achieves a false positive rate of less than 10% and true positive rate of almost 80% in the first two days even in the presence of a significant number of spammers. This increases to a true positive rate of 99% and drops a false positive rate to less than 2% on the third day. In a network with no spammers, our approach achieves a false positive rate of less than 10%. In a network heavily saturated with more than 60% of spam callers, our approach achieves a true positive rate of 98% and no false positives. We compare the performance of our approach with a closely related spam detection approach named Call-Rank. The results show that our approach outperforms Call-Rank in terms of detection accuracy and detection time. 
39|PB||An adaptive risk management and access control framework to mitigate insider threats|Insider Attacks are one of the most dangerous threats organizations face today. An insider attack occurs when a person authorized to perform certain actions in an organization decides to abuse the trust, and harm the organization. These attacks may negatively impact the reputation of the organization, its productivity, and may produce losses in revenue and clients. Avoiding insider attacks is a daunting task. While it is necessary to provide privileges to employees so they can perform their jobs efficiently, providing too many privileges may backfire when users accidentally or intentionally abuse their privileges. Hence, finding a middle ground, where the necessary privileges are provided and malicious usage are avoided, is necessary. In this paper, we propose a framework that extends the role-based access control (RBAC) model by incorporating a risk assessment process, and the trust the system has on its users. Our framework adapts to suspicious changes in users' behavior by removing privileges when users' trust falls below a certain threshold. This threshold is computed based on a risk assessment process that includes the risk due to inference of unauthorized information. We use a Coloured-Petri net to detect inferences. We also redefine the existing role activation problem, and propose an algorithm that reduces the risk exposure. We propose a methodology to help administrators managing inference threats. We present experimental evaluation to validate our work. 
39|PB||Efficient intrusion detection using representative instances|Because of their feasibility and effectiveness, artificial intelligence-based intrusion detection systems attract considerable interest from researchers. However, when confronted with large-scale data sets, many artificial intelligence-based intrusion detection systems could suffer from a high computational burden, even though the feature selection method can help to reduce the computational complexity. To improve the efficiency, we propose a representative instance selection method to preprocess the original data set before training a classifier, which is independent of the learning algorithm that is used for constructing the intrusion detection system. In this study, a new metric is introduced to measure the representative power of an instance with respect to its class. Based on an implementation of representativeness, we select the most representative instance in each subset divided by a novel centroid-based partitioning strategy, and then, we utilise the result as training data to build various intrusion detection models efficiently. Experimental results on a labelled flow-based data set introduced in 2009 show that ANN, KNN, SVM and Liblinear learning with a largely reduced set of representative instances can not only achieve high efficiency in detecting network attacks but also provide comparable detection performance in terms of the detection rate, precision, F-score and accuracy, as compared with four corresponding classifiers built with the original large data set. 
39|PB||SHADuDT: Secure hypervisor-based anomaly detection using danger theory|Intrusion Detection based upon learning methods is an attractive approach in research community. These researches have two critical concerns: secure information gathering and accurate detection method. Here we used system calls together with their arguments as a suitable pattern for describing behavior of each process. In security applications, these patterns must be collected safely, so we proposed SHADuDT, a secure and robust hypervisor-based architecture for system call intercepting and information gathering that utilizes the second generation of Artificial Immune Systems (AIS) as intrusion detection method. Generally intrusion detection based on AISs fall into two categories. The first generation of AIS is inspired from adaptive immune reactions but the second one that is called danger theory focuses on both of these reactions to build a more biologically-realistic model of Human Immune System.Here we presented a novel Algorithm in Danger Theory field as SHADuDT detection method (SHADuDT_DM) for anomaly detection and utilized hypervisor architecture for SHADuDT secure auditor (SHADuDT_SA) to guarantee the safety of information gathering. We evaluated SHADuDT architecture through several criteria and compared its detection method with classic AIS methods for anomaly detection. These Evaluation results show considerable improvements in terms of detection performance and false alarm rates while keeping low overheads in execution time and memory by using the advantages of both hypervisor technology and Artificial Immune Systems. 
39|PB||Analysis of dictionary methods for PIN selection|Personal Identification Numbers (PINs) are commonly used as an authentication mechanism. An important security requirement is that PINs should be hard to guess. On the other hand, remembering several random PINs can be difficult task for a user. We evaluate several dictionary-based methods of choosing the PIN. To assess their resistance to guessing attacks, we use entropy, covering of the PIN space, guesswork, marginal guesswork, and marginal success rate metrics. With respect to these metrics, most of the evaluated methods are far from ideal ones. Positive results are obtained by a more involved morphing method, and the technique of the reduced dictionary. We also discuss two methods for constructing easy to remember PIN words for randomly chosen PINs. 
39|PB||Establishing initial trust in autonomous Delay Tolerant Networks without centralised PKI|A Delay Tolerant Network (DTN) is a network where nodes can be highly mobile, with long message delay times, forming dynamic and fragmented networks. Conventional centralised network security mechanisms are unsuitable in such networks, therefore distributed security solutions are more desirable in DTN implementations. Establishing effective trust in distributed systems with no centralised Public Key Infrastructure (PKI) such as the Pretty Good Privacy (PGP) scheme, usually requires human intervention. In this paper, we build and compare different decentralised trust systems for autonomous DTN. We utilise a public key distribution model based on the Web of Trust principle, and employ a simple Leverage of Common Friends (LCF) trust system to establish initial trust in autonomous DTN. We compare this system with two other scenarios (no trust and random trust) for autonomous establishment of initial trust. Comparisons are based on the time it takes to disperse the trust and resilience of the system against a malicious node distributing malicious and False Public Keys. Our results show that the LCF trust system mitigates the distribution of false malicious public keys by 40%. LCF takes 44% longer to distribute 50% of the public keys compared when using no trust system, but is 16% faster in comparison to the random trust method. 
39|PB||A fast malware detection algorithm based on objective-oriented association mining|Objective-oriented association (OOA) mining has been successfully applied in malware detection. One problem of OOA mining is that the number of association rules is very large, and many of the rules are redundant and have little capacity to distinguish malware from benign files. This circumstance seriously affects the running speed of OOA for malware detection. In this paper, an API (Application Programming Interface)-based association mining method is proposed for detecting malware. To increase the detection speed of the OOA, different strategies are presented: to improve the rule quality, criteria for API selection are proposed to remove APIs that cannot become frequent items; to find association rules that have strong discrimination power, we define the rule utility to evaluate the association rules; and to improve the detection accuracy, a classification method based on multiple association rules is adopted. The experiments show that the proposed strategies can significantly improve the running speed of OOA. In our experiments the time cost for data mining is reduced by thirty-two percent, and the time cost for classification is reduced by fifty percent. 
39|PB||An integrated framework combining Bio-Hashed minutiae template and PKCS15 compliant card for a better secure management of fingerprint cancelable templates|We address in this paper the problem of privacy in fingerprint biometric systems. Today, cancelable techniques have been proposed to deal with this issue. Ideally, such transforms are one-way. However, even if they are with provable security, they remain vulnerable when the user-specific key that achieves cancelability property is stolen. The prominence of the cancelable template confidentiality to maintain the irreversibility property was also demonstrated for many proposed constructions. To prevent possible coming attacks, it becomes important to securely manage this key as well as the transformed template in order to avoid them being leaked simultaneously and thus compromised. To better manage the user credentials of cancelable constructs, we propose a new solution combining a trusted architecture and a cancelable fingerprint template. Therefore, a Bio-Hashed minutiae template based on a chip matching algorithm is proposed. A pkcs15 compliant cancelable biometric system for fingerprint privacy preserving is implemented on a smartcard. This closed system satisfies the safe management of the sensitive templates. The proposed solution is proved to be well resistant to different attacks. 
39|PB||Identifying android malicious repackaged applications by thread-grained system call sequences|Android security has become highly desirable since adversaries can easily repackage malicious codes into various benign applications and spread these malicious repackaged applications (MRAs). Most MRA detection mechanisms on Android focus on detecting a specific family of MRAs or requiring the original benign application to compare with the malicious ones. This work proposes a new mechanism, SCSdroid (System Call Sequence Droid), which adopts the thread-grained system call sequences activated by applications. The concept is that even if MRAs can be camouflaged as benign applications, their malicious behavior would still appear in the system call sequences. SCSdroid extracts the truly malicious common subsequences from the system call sequences of MRAs belonging to the same family. Therefore, these extracted common subsequences can be used to identify any evaluated application without requiring the original benign application. Experimental results show that SCSdroid falsely detected only two applications among 100 evaluated benign applications, and falsely detected only one application among 49 evaluated malicious applications. As a result, SCSdroid achieved up to 95.97% detection accuracy, i.e., 143 correct detections among 149 applications. 
39|PB||Trust management system design for the Internet of Things: A context-aware and multi-service approach|This work proposes a new trust management system (TMS) for the Internet of Things (IoT). The wide majority of these systems are today bound to the assessment of trustworthiness with respect to a single function. As such, they cannot use past experiences related to other functions. Even those that support multiple functions hide this heterogeneity by regrouping all past experiences into a single metric. These restrictions are detrimental to the adaptation of TMSs to today's emerging M2M and IoT architectures, which are characterized with heterogeneity in nodes, capabilities and services. To overcome these limitations, we design a context-aware and multi-service trust management system fitting the new requirements of the IoT. Simulation results show the good performance of the proposed system and especially highlight its ability to deter a class of common attacks designed to target trust management systems. 
39|PB||APFS: Adaptive Probabilistic Filter Scheduling against distributed denial-of-service attacks|Distributed denial-of-service (DDoS) attacks are considered to be among the most crucial security challenges in current networks because they significantly disrupt the availability of a service by consuming extreme amount of resource and/or by creating link congestions. One type of countermeasure against DDoS attacks is a filter-based approach where filter-based intermediate routers within the network coordinate with each other to filter undesired flows. The key to success for this approach is effective filter propagation and management techniques. However, existing filter-based approaches do not consider effective filter propagation and management. In this paper, we define three necessary properties for a viable DDoS solution: how to practically propagate filters, how to place filters to effective filter routers, and how to manage filters to maximize the efficacy of the defense. We propose a novel mechanism, called Adaptive Probabilistic Filter Scheduling (APFS), that effectively defends against DDoS attacks and also satisfies the three necessary properties. In APFS, a filter router adaptively calculates its own marking probability based on three factors: 1) hop count from a sender, 2) the filter router's resource availability, and 3) the filter router's link degree. That is, a filter router that is closer to attackers, has more available resources, or has more connections to neighbors inserts its marking with a higher probability. These three factors lead a victim to receive more markings from more effective filter routers, and thus, filters are quickly distributed to effective filter routers. Moreover, each filter router manages multiple filters using a filter scheduling policy that allows it to selectively keep the most effective filters depending on attack situations. Experimental results show that APFS has a faster filter propagation and a higher attack blocking ratio than existing approaches that use fixed marking probability. In addition, APFS has a 44% higher defense effectiveness than existing filter-based approaches that do not use a filter scheduling policy. 
39|PB||Dual-use open source security software in organizations â Dilemma: Help or hinder?|Dual-use technology can be used for both peaceful and harmful purposes. While the new type of anonymous, invisible and devastating security threats (malware, worms and viruses) shape contemporary warfare, organizations are challenged by the undefined risks of open source dual-use security tools. The dual-use dilemma is very important. It has not received adequate academic focus: questions such as increased or decreased risk, facilitation of security breaches, and the impact on security awareness have not been clarified or studied. This research closes existing gaps by studying the open source dual-use security software challenges that organizations should consider when using this technology. We utilize a triangulation approach with three independent data sources to conduct a detailed analysis of this phenomenon. Our study has found that the dual-use technology has both positive and negative effects on information system security. The ease of use of the dual-use security software facilitates security breaches and enterprises are using vulnerable open source security libraries and frameworks to develop their own in-house applications. On a positive note, open source dual-use security software is used as a powerful defense tool against attackers. Our study also found that security awareness is the key to maintaining the right level of information security risk in the dual-use context. Dual-use can also be of a great help to organizations in leveraging their information system security. 
39|PB||CISOs and organisational culture: Their own worst enemy?|Many large organisations now have a Chief Information Security Officer (CISO1). While it may seem obvious that their role is to define and deliver organisational security goals, there has been little discussion on what makes a CISO able to deliver this effectively. In this paper, we report the results from 5 in-depth interviews with CISOs, which were analysed using organisational behaviour theory. The results show that the CISOs struggle to gain credibility within their organisation due to: a perceived lack of power, confusion about their role identity, and their inability to engage effectively with employees. We conclude that as the CISO role continues to develop CISOs need to reflect on effective ways of achieving credibility in their organisations and, in particular, to work on communicating with employees and engaging them in security initiatives. We also identify a key responsibility for effective CISOs; that is to remove the blockages that prevent information security from becoming ‘business as usual’ rather than a specialist function. For researchers, our findings offer a new piece of the emerging picture of human factors in information security initiatives. 
39|PB||Usage control in SIP-based multimedia delivery|The Session Initiation Protocol (SIP) is an application layer signaling protocol for the creation, modification and termination of multimedia sessions and VoIP calls with one or more participants. While SIP operates in highly dynamic environments, in the current version its authorization support is based on traditional access control models. The main problem these models face is that they were designed many years ago, and under some circumstances they tend to be inadequate in modern highly dynamic environments. Usage Control (UCON), instead, is a model that supports the same operations as traditional access control models do, but it further enhances them with novel ones. In previous work, an architecture supporting continuous authorizations in SIP, based on the UCON model, was presented. In this article, an authorization support implementing the whole UCON model, including authorizations, obligations and conditions, has been integrated in a SIP system. Moreover, a testbed has been set up to experimentally evaluate the performance of the proposed security mechanism. 
39|PB||Deriving common malware behavior through graph clustering|Detection of malicious software (malware) continues to be a problem as hackers devise new ways to evade available methods. The proliferation of malware and malware variants requires new advanced methods to detect them. This paper proposes a method to construct a common behavioral graph representing the execution behavior of a family of malware instances. The method generates one common behavioral graph by clustering a set of individual behavioral graphs, which represent kernel objects and their attributes based on system call traces. The resulting common behavioral graph has a common path, called HotPath, which is observed in all the malware instances in the same family. The proposed method shows high detection rates and false positive rates close to 0%. The derived common behavioral graph is highly scalable regardless of new instances added. It is also robust against system call attacks. 
39|PB||Inference attacks against trust-based onion routing: Trust degree to the rescue|Trust-based onion routing enhances anonymity protection by means of constructing onion circuits using trust-based routers. However, attackers who have the knowledge of a priori trust distributions are still capable of largely reducing the anonymity protected by trust-based circuits. The root cause is that these attackers have a high probability to guess the users who initiate trust-based circuits through the routers trusted by few other users (i.e., inference attacks). In this paper, we uncover trust degree, an essential feature of routing anonymity that is effective in defeating inference attacks but has been overlooked in the design of existing trust-based onion routing. We conduct an isolated model based analysis to understand why the trust degree is effective and how it can be used to resist inference attacks. Our major contributions are three-fold. First, we present a model to exclusively reason about inference attacks in trust-based onion routing. This model isolates the anonymity compromised by inference attacks from other attacks (e.g., correlation-like attacks), and hence derives an exclusive design space that reveals trust degree as the key feature against inference attacks. Second, to show the usefulness of our model, we design a new routing algorithm by taking into account of trust degree. Our algorithm can protect anonymity against inference attacks without sacrificing the capability against attackers' routers. Third, we compare trust-based routing algorithms with and without considering trust degree using real-world social networking datasets. These comparisons present evidence to confirm the effectiveness of trust degree in defeating inference attacks under real-world settings. 
39|PB||Understanding the violation of IS security policy in organizations: An integrated model based on social control and deterrence theory|It is widely agreed that a large amount of information systems (IS) security incidents occur in the workplace because employees subvert existing IS Security Policy (ISSP). In order to understand the factors that constrain employees from deviance and violation of the organizational ISSP, past work has traditionally viewed this issue through the lens of formal deterrence mechanisms; we postulated that we could better explain employees' ISSP violation behaviours through considering both formal and informal control factors as well as through considering existing deterrence theory. We therefore developed a theoretical model based on both deterrence and social bond theories rooted in a social control perspective to better understand employee behaviour in this context. The model is validated using survey data of 185 employees. Our empirical results highlight that both formal and informal controls have a significant effect on employees' ISSP violation intentions. To be specific, employees' social bonding is found to have mixed impacts on the employee's intention to violate ISSP. Social pressures exerted by subjective norms and co-worker behaviours also significantly influence employees' ISSP violation intentions. In analyzing the formal sanctions, the perceived severity of sanctions was found to be significant while, perceived certainty of those sanctions was not. We discuss the key implications of our findings for managers and researchers and the implications for professional practice. 
39|PB||Creditability-based weighted voting for reducing false positives and negatives in intrusion detection|False positives (FPs) and false negatives (FNs) happen in every Intrusion Detection System (IDS). How often they occur is regarded as a measurement of the accuracy of the system. Frequent occurrences of FPs not only reduce the throughput of an IDS as FPs block the normal traffic and also degrade its trustworthiness. It is also difficult to eradicate all FNs from an IDS. One way to overcome the shortcomings of a single IDS is to employ multiple IDSs in its place and leverage the different capabilities and domain knowledge of these systems. Nonetheless, making a correct intrusion decision based on the outcomes of multiple IDSs has been a challenging task, as different IDSs may respond differently to the same packet trace. In this paper, we propose a method to reduce FPs and FNs by applying a creditability-based weighted voting (CWV) scheme to the outcomes of multiple IDSs. First, the CWV scheme evaluates the creditability of each individual IDS by monitoring its response to a large collection of pre-recorded packet traces containing various types of intrusions. For each IDS, our scheme then assigns different weights to each intrusion type according to its FP and FN ratios. Later, after their operations, the outcomes of individual IDSs are merged using a weighted voting scheme. In benchmarking tests, our CWV-based multiple IDSs demonstrated significant improvement in accuracy and efficiency when compared with multiple IDSs employing an ordinary majority voting (MV) scheme. The accuracy is the percentage of whole traces that are determined accurately, while the efficiency indicates that the voting algorithm performs better on reducing both FP and FN ratios. The CWV scheme achieved 95% accuracy and 94% efficiency while the MV scheme produced only 66% accuracy and 41% efficiency; the average percentages of FP/FN reduction were 21% and 58% respectively. 
39|PB||DNS amplification attack revisited|It is without doubt that the Domain Name System (DNS) is one of the most decisive elements of the Internet infrastructure; even a slight disruption to the normal operation of a DNS server could cause serious impairment to network services and thus hinder access to network resources. Hence, it is straightforward that DNS nameservers are constantly under the threat of Denial of Service (DoS) attacks. This paper presents a new, stealthy from the attacker's viewpoint, flavor of DNSSEC-powered amplification attack that takes advantage of the vast number of DNS forwarders out there. Specifically, for augmenting the amplification factor, the attacker utilizes only those forwarders that support DNSSEC-related resource records and advertize a large DNS size packet. The main benefits of the presented attack scenario as compared to that of the typical amplification attack are: (a) The revocation of the need of the aggressor to control a botnet, and (b) the elimination of virtually all traces that may be used toward disclosing the attacker's actions, true identity and geographical location. The conducted experiments taking into consideration three countries, namely Greece, Ireland and Portugal demonstrate that with a proper but simple planning and a reasonable amount of resources, a determined perpetrator is able to create a large torrent of bulky DNS packets towards its target. In the context of the present study this is translated to a maximum amplification factor of 44. 
39|PB||Co-operative user identity verification using an Authentication Aura|IT usage today is typified by users that operate across multiple devices, including traditional desktop PCs, laptops, tablets and smartphones. As a consequence, users can regularly find themselves having a variety of devices open concurrently, and with even the most basic security in place, there is a resultant need to repeatedly authenticate, which can potentially represent a source of hindrance and frustration for the user. Building upon previous work by the authors that proposed a novel approach to user authentication, called an Authentication Aura, this paper investigates the latent security potential contained in surrounding devices in everyday life and how this may be used to augment security. An experiment has been undertaken to ascertain the technological infrastructure, devices and inert objects that surround individuals throughout the day to establish whether or not these items might be utilised within an authentication solution. The experiment involved twenty volunteers, over a 14-day period, and resulted in a dataset of 1.23 million recorded observations. Using the data provided by the experiment as a basis for a simulation, it investigated how confidence in the user's identity is influenced by these familiar everyday possessions and how their own authentication status can be ‘leveraged’ to negate the need to repeatedly manually authenticate. The simulation suggests a potential reduction of 74.04% in the daily number of required authentications for a user operating a device once every 30 min, with a 10-min screen lock in place. Ultimately, it confirms that during device activation it is possible to remove the need to authenticate with the Authentication Aura providing sufficient confidence. 
40|-|http://www.sciencedirect.com/science/journal/01674048/40|Contents|
40|-||Editorial|
40|-||The passing of a pioneer|
40|-||Model checking authorization requirements in business processes|Business processes are usually expected to meet high level authorization requirements (e.g., Separation of Duty). Since violation of authorization requirements may lead to economic losses and/or legal implications, ensuring that a business process meets them is of paramount importance. Previous work showed that model checking can be profitably used to check authorization requirements in business processes. However, building formal models that simultaneously account for both the workflow and the access control policy is a time consuming and error-prone activity. In this paper we present a new approach to model checking authorization requirements in business processes that allows for the separate specification of the workflow and of the associated access control policy while retaining the ability to carry out a fully automatic analysis of the business process. To illustrate the effectiveness of the approach we describe its application to a Loan Origination Process subject to an RBAC access control policy featuring conditional permission assignments and delegation. 
40|-||A comprehensive and efficacious architecture for detecting phishing webpages|Phishing is a web-based criminal act. Phishing sites lure sensitive information from naive online users by camouflaging themselves as trustworthy entities. Phishing is considered an annoying threat in the field of electronic commerce. Due to the short lifespan of phishing webpages and the rapid advancement of phishing techniques, maintaining blacklists, white-lists or employing solely heuristics-based approaches are not particularly effective. The impact of phishing can be largely mitigated by adopting a suitable combination of all these techniques. In this study, the characteristics of legitimate and phishing webpages were investigated in depth, and based on this analysis, we proposed heuristics to extract 15 features from such webpages. These heuristic results were fed as an input to a trained machine learning algorithm to detect phishing sites. Before applying heuristics to the webpages, we used two preliminary screening modules in this system. The first module, the preapproved site identifier, checks webpages against a private white-list maintained by the user, and the second module, the Login Form Finder, classifies webpages as legitimate when there are no login forms present. These modules help to reduce superfluous computation in the system and in addition reducing the rate of false positives without compromising on the false negatives. By using all of these modules, we are able to classify webpages with 99.8% precision and a 0.4% of false positive rate. The experimental results indicate that this method is efficient for protecting users from online identity attacks. 
40|-||Security and compliance challenges in complex IT outsourcing arrangements: A multi-stakeholder perspective|Complex IT outsourcing arrangements promise numerous benefits such as increased cost predictability and reduced costs, higher flexibility and scalability upon demand. Organizations trying to realize these benefits, however, face several security and compliance challenges. In this article, we investigate the pressure to take action with respect to such challenges and discuss avenues toward promising responses. We collected perceptions on security and compliance challenges from multiple stakeholders by means of a series of interviews and an online survey, first, to analyze the current and future relevance of the challenges as well as potential adverse effects on organizational performance and, second, to discuss the nature and scope of potential responses. The survey participants confirmed the current and future relevance of the six challenges auditing clouds, managing heterogeneity of services, coordinating involved parties, managing relationships between clients and vendors, localizing and migrating data and coping with lack of security awareness. Additionally, they perceived these challenges as affecting organizational performance adversely in case they are not properly addressed. Responses in form of organizational measures were considered more promising than technical ones concerning all challenges except localizing and migrating data, for which the opposite was true. Balancing relational and contractual governance as well as employing specific client and vendor capabilities is essential for the success of IT outsourcing arrangements, yet do not seem sufficient to overcome the investigated challenges. Innovations connecting the technical perspective of utility software with the business perspective of application software relevant for security and compliance management, however, nourish the hope that the benefits associated with complex IT outsourcing arrangements can be realized in the foreseeable future whilst addressing the security and compliance challenges. 
40|-||Towards a distributed secure in-vehicle communication architecture for modern vehicles|Modern automotive vehicles are becoming a collection of interconnected embedded subsystems, where the mechanical parts are controlled by electronic ones and the vehicle is transformed into a mobile information system. However, the industry standards for in-vehicle communication are not following long-established computer security policies. This trend not only makes vehicles prone to thefts and automated attacks, but also endangers passengers safety.This paper analyzes current practices and standards of the automotive industry, highlighting several vulnerabilities that stress the need to change the way that in-vehicle communication is handled. To this end, we present a novel vehicle security architecture that supports two new features; users with different access rights and roles, and mutual authentication of ECUs. These features can enable a more distributed security architecture and prevent many attacks, or at least trigger adequate alarms to detect and mitigate them, or allow backtracking. 
40|-||Extended DMTP: A new protocol for improved graylist categorization|The proportion of spam has significantly increased in recent years. This paper proposes an extended differentiated mail transfer protocol (extended DMTP, namely EDMTP) to deal with the problem that the graylist is unclearly categorized in DMTP. Considering the difficulty in promoting the common problem of protocol, we design EDMTP-based schemes on SMTA (sending Mail Transfer Agent) and RMTA (receiving Mail Transfer Agent). Our new schemes accord with open close principle and do not need changing the original mail infrastructure. In addition, we design experiments to compare EDMTP with DMTP. Simulation results demonstrate that EDMTP can reduce the number of envelopes and the proportion of spam envelopes and therefore significantly improve the performance of graylist. Moreover, compared to the current SMTP-based e-mail system, our proposed EDMTP-based e-mail system can effectively decrease the traffic usage of spam. 
40|-||An advanced persistent threat in 3G networks: Attacking the home network from roaming networks|The HLR/AuC is considered to be one of the most important network elements of a 3G network. It can serve up to five million subscribers and at least one transaction with HLR/AuC is required for every single phone call or data session. This paper presents experimental results and observations that can be exploited to perform a novel distributed denial of service attack in 3G networks that targets the availability of the HLR/AuC. More specifically, first we present an experiment in which we identified and proved some zero-day vulnerabilities of the 3G network that can be exploited by malicious actors to mount various attacks. For the purpose of our experiment, we have used off-the-shelf infrastructure and software, without any specialized modification. Based on the observations of the experiment, we reveal an Advanced Persistent Threat (APT) in 3G networks that aims to flood an HLR/AuC of a mobile operator. We also prove that the discovered APT can be performed in a trivial manner using commodity hardware and software, which is widely and affordably available. 
40|-||A framework for generating realistic traffic for Distributed Denial-of-Service attacks and Flash Events|An intrinsic challenge associated with evaluating proposed techniques for detecting Distributed Denial-of-Service (DDoS) attacks and distinguishing them from Flash Events (FEs) is the extreme scarcity of publicly available real-word traffic traces. Those available are either heavily anonymised or too old to accurately reflect the current trends in DDoS attacks and FEs. This paper proposes a traffic generation and testbed framework for synthetically generating different types of realistic DDoS attacks, FEs and other benign traffic traces, and monitoring their effects on the target. Using only modest hardware resources, the proposed framework, consisting of a customised software traffic generator, ‘Botloader’, is capable of generating a configurable mix of two-way traffic, for emulating either large-scale DDoS attacks, FEs or benign traffic traces that are experimentally reproducible. Botloader uses IP-aliasing, a well-known technique available on most computing platforms, to create thousands of interactive UDP/TCP endpoints on a single computer, each bound to a unique IP-address, to emulate large numbers of simultaneous attackers or benign clients. 
40|-||Framework and principles for active cyber defense|This essay offers a broad view of active defense derived from the concept of active air and missile defense. This view admits a range of cyber defenses, many of which are widely deployed and considered essential in today's threat environment. Instead of equating active defense to hacking back, this wider interpretation lends itself to distinguishing different types of active defense and the legal and ethical issues they raise. The essay will review the concepts of active and passive air and missile defenses, apply them to cyberspace, describe a framework for distinguishing different types of active cyber defense, and finally suggest legal and ethical principles for conducting active cyber defense. 
volume|issue|url|title|abstract
41|-|http://www.sciencedirect.com/science/journal/01674048/41|Contents|
41|-||Editorial|
41|-||Consistency and enforcement of access rules in cooperative data sharing environment|In this paper we consider the situation where a set of enterprises need to collaborate to provide rich services to their clients. An enterprise may need information from several other collaborating parties to satisfy its business requirements. Such collaboration often requires controlled access to one another's data, which we assume is stored in standard relational form. We assume that a set of access rules is given to the parties to regulate the data sharing, and such rules are defined over the join operations over the relational data. It is expected that the access rules will be designed according to business needs of the involved enterprises and although some negotiation between them will be involved, only a comprehensive analysis of the rules can uncover all issues of consistency between rules and their adequacy in answering the authorized queries (which we call enforceability). In this paper, we provide such an analysis and provide algorithms for checking and removing inconsistency, checking for rule enforceability, and minimally updating the rules to ensure enforceability whenever possible using only the existing parties. The involvement of specialized third parties for consistency and enforcement purposes is not addressed in this paper. 
41|-||CPBAC: Property-based access control model for secure cooperation in online social networks|The rapid growth of online social networks (OSNs) has brought a revolutionary change in the way geographically dispersed people interact and cooperate with each other toward achieving some common goals. Recently, new ways of ad-hoc cooperation have been demonstrated during the hurricane Irene and the earthquake in Japan. In such emergency situations, OSNs have already taken a significant role as alternative social media that support altruistic information sharing and cooperation among people. However, existing cooperation approaches have not been well-organized and are highly vulnerable to security threats such as a disclosure of users' identities and the leakage of other private data because of the lack of secure cooperation mechanisms. To support secure and effective cooperation in OSNs, in this paper, we propose the CPBAC (Community-centric Property Based Access Control) model, which extends the existing CRiBAC (Community-centric Role interaction Based Access Control) model for use in OSNs to support cooperation among users. To verify the feasibility of the proposed model, we have implemented a prototype application on Facebook and have demonstrated its applicability with two working examples. 
41|-||Identifying hidden social circles for advanced privacy configuration|With the dramatic increase of users on social network websites, the needs to assist users to manage their large number of contacts as well as providing privacy protection become more and more evident. Unfortunately, limited tools are available to address such needs and reduce users' workload on managing their social relationships. To tackle this issue, we propose an approach to facilitate online social network users to group their contacts into social circles with common interests. Further, we leverage the social group practice to automate the privacy setting process for users who add new contacts or upload new data items. We evaluate our approach using real-world data collected through a user study. The study also includes an analysis of the properties that are most critical for privacy related decisions. 
41|-||A formal proximity model for RBAC systems|To combat the threat of information leakage through pervasive access, researchers have proposed several extensions to the popular role-based access control (RBAC) model. Such extensions can incorporate contextual features, such as location, into the policy decision in an attempt to restrict access to trustworthy settings. In many cases, though, such extensions fail to reflect the true threat, which is the presence or absence of other users, rather than absolute locations. For instance, for location-aware separation of duty, it is more important to ensure that two people are in the same room, rather than in a designated, pre-defined location. Prox-RBAC was proposed as an extension to consider the relative proximity of other users with the help of a pervasive monitoring infrastructure. However, that work offered only an informal view of proximity, and unnecessarily restricted the domain to spatial concerns. In this work, we present a more rigorous definition of proximity based on formal topological relations. In addition, we show that this definition can be applied to several additional domains, such as social networks, communication channels, attributes, and time; thus, our policy model and language is more flexible and powerful than the previous work. In addition to proposing the model, we present a number of theoretical results for such systems, including a complexity analysis, templates for cryptographic protocols, and proofs of security features. 
41|-||Representation and querying of unfair evaluations in social rating systems|Social rating systems are subject to unfair evaluations. Users may try to individually or collaboratively promote or demote a product. Detecting unfair evaluations, mainly massive collusive attacks as well as honest looking intelligent attacks, is still a real challenge for collusion detection systems. In this paper, we study the impact of unfair evaluations in online rating systems. First, we study the individual unfair evaluations and their impact on the reputation of people calculated by social rating systems. We then propose a method for detecting collaborative unfair evaluations, also known as collusion. The proposed model uses frequent itemset mining technique to detect the candidate collusion groups and sub-groups. We use several indicators to identify collusion groups and to estimate how destructive such colluding groups can be. The approaches presented in this paper have been implemented in prototype tools, and experimentally validated on synthetic and real-world datasets. 
42|-|http://www.sciencedirect.com/science/journal/01674048/42|Contents|
42|-||Editorial|
42|-||Cancelable multi-biometrics: Mixing iris-codes based on adaptive bloom filters|In this work adaptive Bloom filter-based transforms are applied in order to mix binary iris biometric templates at feature level, where iris-codes are obtained from both eyes of a single subject. The irreversible mixing transform, which generates alignment-free templates, obscures information present in different iris-codes. In addition, the transform is parameterized in order to achieve unlinkability, implementing cancelable multi-biometrics. Experiments which are carried out on the IITD Iris Database version 1.0 confirm the soundness of the proposed approach, (1) maintaining biometric performance at equal error rates below 0.5% for different feature extraction methods and fusion scenarios and (2) achieving a compression of mixed templates down to 10% of original size. 
42|-||Covert Computation â Hiding code in code through compile-time obfuscation|Recently, the concept of semantic-aware malware detection has been proposed in the literature. Instead of relying on a syntactic analysis (i.e., comparison of a program to pre-generated signatures of malware samples), semantic-aware malware detection tries to model the effects a malware sample has on the machine. Thus, it does not depend on a specific syntactic implementation. For this purpose a model of the underlying machine is used. While it is possible to construct more and more precise models of hardware architectures, we show that there are ways to implement hidden functionality based on side effects in the microprocessor that are difficult to cover with a model. In this paper we give a comprehensive analysis of side effects in the x86 architecture and describe an implementation concept based on the idea of compile-time obfuscation, where obfuscating transformations are applied to the code at compile time. Finally, we provide an evaluation based on a prototype implementation to show the practicability of our approach and estimate complexity and space overhead using actual malware samples. 
42|-||Protecting organizational competitive advantage: A knowledge leakage perspective|The strategic management literature emphasizes the importance of protecting organizational knowledge and information, especially in terms of maintaining competitive advantage. We synthesized several mechanisms from the literature that organizations could deploy to protect their knowledge and information. An Australian field study investigated how and to what extent these mechanisms were deployed in 11 knowledge-intensive organizations. The study revealed surprising findings: firstly, there was no evidence of a systematic and comprehensive management approach to the identification and protection of knowledge assets. Approaches were often haphazard, driven in a bottom-up fashion with much of the responsibility delegated to individual employees and knowledge owners. Secondly, concerns about confidentiality of organizations' operational data (e.g., client details), often crowded out managerial attention to protecting organizations' own knowledge and information assets. Based on these observations, we outline several implications for future research, including the need for more comprehensive frameworks to address knowledge leakage from a strategic perspective. 
42|-||WorSE: A Workbench for Model-based Security Engineering|IT systems with sophisticated security requirements increasingly apply problem-specific security policies for specifying, analyzing, and implementing security properties. Due to their key role for defining and enforcing strategic security concepts, security polices are extremely critical, and quality assets such as policy correctness or policy consistency are essential objectives in policy engineering.This paper argues for a tool-supported policy engineering approach to increase the efficiency and quality of security policy making. The paper's general topic is WorSE, a policy engineering workbench encompassing the automation of engineering steps, pre-built model patterns, integrated plausibility checks, and model analysis tools; the paper especially focuses on tools supporting model engineering and model analysis, and describes their theoretical foundations and practical application. 
42|-||Smartphone information security awareness: A victim of operational pressures|Smartphone information security awareness describes the knowledge, attitude and behaviour that employees apply to the security of the organisational information that they access, process and store on their smartphone devices. The surge in the number of smartphone devices connecting to organisational systems and used to process organisational data has enabled a new level of operational efficiency. While employees are aware of the benefits they enjoy by bringing their personal devices into the workplace, managers too are aware of the benefits of having a constantly connected workforce. Unfortunately, those aware of the risks to information security do not share an equal level of enthusiasm. These devices are owned by employees who are not adequately skilled to configure the security settings for acceptable security of that information. Moreover, routine information security awareness programmes, even if applied, gradually fade into the daily rush of operations from the day they are completed.This paper explores the factors which influence these oscillating levels of information security awareness. By applying an adapted version of an awareness model from the domain of accident prevention, the factors which cause diminishing awareness levels are exposed. Subsequently, information security awareness emerges as a symptom of such factors. Through geometrical modelling of the boundaries and pressures that govern our daily operations, an awareness model emerges. This model ensures that organisations are better equipped to monitor their information security awareness position, their boundaries and the daily pressures affecting the organisation, thus allowing them to design better integrated policies and procedures to encourage safe operating limits. The model is evaluated using a theory evaluation framework through an expert review process. 
42|-||Evaluating the privacy of Android mobile applications under forensic analysis|In this paper, we investigate and evaluate through experimental analysis the possibility of recovering authentication credentials of mobile applications from the volatile memory of Android mobile devices. Throughout the carried experiments and analysis, we have, exclusively, used open-source and free forensic tools. Overall, the contribution of this paper is threefold. First, it thoroughly, examines thirteen (13) mobile applications, which represent four common application categories that elaborate sensitive users' data, whether it is possible to recover authentication credentials from the physical memory of mobile devices, following thirty (30) different scenarios. Second, it explores in the considered applications, if we can discover patterns and expressions that indicate the exact position of authentication credentials in a memory dump. Third, it reveals a set of critical observations regarding the privacy of Android mobile applications and devices. 
42|-||An automated system for rapid and secure device sanitization|Public and private organizations face the challenges of protecting their networks from cyber-attacks, while reducing the amount of time and money spent on Information Technology. Organizations can reduce their expenditures by reusing server, switch and router hardware, but they must use reliable and efficient methods of sanitizing these devices before they can be redeployed. The sanitization process removes proprietary, sensitive or classified data, as well as persistent malware from a device prior to reuse. The Johns Hopkins University Applied Physics Laboratory has developed an automated, rapid, and secure method for sanitizing servers, switches and routers. This sanitization method was implemented and tested on several different types of network devices during the Cyber Measurement & Analysis Center project, which was funded under Phases I and II of the DARPA (2008) National Cyber Range program. The performance of the automated sanitization system was excellent with an order of magnitude reduction in the time required to sanitize servers, routers and switches, and a significant improvement in the effectiveness of the sanitization process through the addition of persistent malware removal. 
42|-||Characterization and classification of malicious Web traffic|Web systems commonly face unique set of vulnerabilities and security threats due to their high exposure, access by browsers, and integration with databases. This study is focused on characterization and classification of malicious cyber activities aimed at Web systems. The empirical analysis is based on three datasets, each in duration of four to five months, collected by high-interaction honeypots which ran fully functional three-tier Web systems. We first explore the types and prevalence of malicious scans and attacks to Web systems, and the extent to which these malicious activities differ in different periods of time or on Web servers running different services. In addition to descriptive statistical analysis, we include an inferential statistical analysis of the malicious session attributes, such as duration, number of requests and bytes transferred in a session. Then, we use supervised machine learning methods to classify attacker activities to two classes: vulnerability scans and attacks. Our main observations include the following: (1) Some characteristics of the malicious Web traffic were invariant across different servers and time periods, such as for example the dominant use of the search-based strategy for attacking the servers and the heavy-tailed behavior of session attributes. (2) On the other side, servers running different services experienced almost complementary profiles of vulnerability scan and attack types. (3) Supervised learning methods efficiently distinguished attack sessions from vulnerability scan sessions, with high probability of detection and very low probability of false alarms. (4) Decision tree based methods J48 and PART performed better than SVM across all datasets. (5) Attacks differed from vulnerability scans only in a small number of session attributes; depending on the dataset, classification of malicious activities can be performed using from four to six features without significantly affecting learners' performance compared to when all 43 features were used. 
42|-||Trusted Online Social Network (OSN) services with optimal data management|Online Social Network (OSN) services have rapidly grown into a wide network and offer users a variety of benefits. However, they also bring new threats and privacy issues to the community. Unfortunately, there are attackers that attempt to expose OSN users' private information or conceal the information that the user desire to share with other users. Therefore, in this research we develop a framework that can provide trusted data management in OSN services. We first define the data types in OSN services and the states of shared data with respect to Optimal, Under-shared, Over-shared, and Hybrid states. We also identify the facilitating, detracting, and preventive parameters that are responsible for the state transition of the data. In a reliable OSN service, we address that a user should be able to set up his or her desired level of information sharing with a certain group of other users. However, it is not always clear to the ordinary users how to determine how much information they should reveal to others. In order to support such a decision, we propose an approach for helping OSN users to determine their optimum levels of information sharing, taking into consideration the payoffs (potential Reward or Cost) based on the Markov decision process (MDP). As an extension of the MDP-based approach, we also introduce a game theoretic approach, considering the interactions of OSN users and attackers with conflicting interests whose decisions affect each other's. Finally, after developing the framework for the optimal data sharing on OSNs, we conduct several experiments with attack simulation based on the proposed ideas and discuss the results. Our proposed approach has the capability to allow a large amount of variables to be altered to suit particular setups that an organization might have. 
42|-||TinyLock: Affordable defense against smudge attacks on smartphone pattern lock systems|A pattern lock system is a widely used graphical password mechanism in today's mobile computing environment. To unlock a smartphone, a user draws a memorized graphical pattern with a finger on a flat touchscreen whereas the finger actually leaves its oily residues, also called smudges, on the surface of the touchscreen. The smudges can be exploited by adversaries to reproduce the secret pattern. Unfortunately, however, security is still dependent on a user's behavior that is to carefully remove them after use. In this paper, we study an affordable defense to resist the smudge attacks without losing the ease-of-use property of the pattern lock system and without demanding user's attentional behavior after use. We present TinyLock as our main result. TinyLock is a simple tweak of the user interface under the existing pattern lock paradigm but it can effectively resist the smudge attacks. Furthermore, TinyLock can be more resilient to shoulder-surfing attacks than the contemporary pattern lock systems. Our user study shows that TinyLock can significantly improve security of the pattern lock system while incurring minimal cost increase in terms of unlocking time. 
42|-||Achieving an effective, scalable and privacy-preserving data sharing service in cloud computing|Data sharing in the cloud, fueled by favorable trends in cloud technology, is emerging as a promising technique for allowing users to conveniently access data. However, the growing number of enterprises and customers who stores their data in cloud servers is increasingly challenging users' privacy and the security of data. This paper focuses on providing a dependable and secure cloud data sharing service that allows users dynamic access to their data. In order to achieve this, we propose an effective, scalable and flexible privacy-preserving data policy with semantic security, by utilizing ciphertext policy attribute-based encryption (CP-ABE) combined with identity-based encryption (IBE) techniques. In addition to ensuring robust data sharing security, our policy succeeds in preserving the privacy of cloud users and supports efficient and secure dynamic operations including, but not limited to, file creation, user revocation and modification of user attributes. Security analysis indicates that the proposed policy is secure under the generic bilinear group model in the random oracle model and enforces fine-grained access control, full collusion resistance and backward secrecy. Furthermore, performance analysis and experimental results show that the overheads are as light as possible. 
42|-||Determining employee awareness using the Human Aspects of Information Security Questionnaire (HAIS-Q)|It is increasingly acknowledged that many threats to an organisation's computer systems can be attributed to the behaviour of computer users. To quantify these human-based information security vulnerabilities, we are developing the Human Aspects of Information Security Questionnaire (HAIS-Q). The aim of this paper was twofold. The first aim was to outline the conceptual development of the HAIS-Q, including validity and reliability testing. The second aim was to examine the relationship between knowledge of policy and procedures, attitude towards policy and procedures and behaviour when using a work computer. Results from 500 Australian employees indicate that knowledge of policy and procedures had a stronger influence on attitude towards policy and procedure than self-reported behaviour. This finding suggests that training and education will be more effective if it outlines not only what is expected (knowledge) but also provides an understanding of why this is important (attitude). Plans for future research to further develop and test the HAIS-Q are outlined. 
42|-||GARS: Real-time system for identification, assessment and control of cyber grooming attacks|In this paper, the Grooming Attack Recognition System (GARS) is presented. The main objectives of GARS are the real-time identification, assessment and control of cyber grooming attacks in favor of child protection. The system utilizes the processes of document classification, personality recognition, user history and exposure time recording to calculate specific risks children are exposed to during chat conversations. The above processes are repeated after each new message and three of them feed corresponding fuzzy logic controllers that provide particular but homogenized risk values as outputs. The weighted sum of the particular risk values results in a total value that indicates the current cyber grooming risk the child is exposed to, as the conversation evolves. Depending on predefined thresholds, the total risk value can be used to trigger alarms for various scopes (children, parents, etc). The practical use of GARS is demonstrated with a case study based on real grooming dialogs. Furthermore, an evaluation of the proposed approach through the discussion of applicability and performance results is discussed. 
42|-||Automatic Defense Against Zero-day Polymorphic Worms in Communication Networks|
43|-|http://www.sciencedirect.com/science/journal/01674048/43|Contents|
43|-||Editorial|
43|-||Mobile malware detection through analysis of deviations in application network behavior|In this paper we present a new behavior-based anomaly detection system for detecting meaningful deviations in a mobile application's network behavior. The main goal of the proposed system is to protect mobile device users and cellular infrastructure companies from malicious applications by: (1) identification of malicious attacks or masquerading applications installed on a mobile device, and (2) identification of republished popular applications injected with a malicious code (i.e., repackaging). More specifically, we attempt to detect a new type of mobile malware with self-updating capabilities that were recently found on the official Google Android marketplace. Malware of this type cannot be detected using the standard signatures approach or by applying regular static or dynamic analysis methods. The detection is performed based on the application's network traffic patterns only. For each application, a model representing its specific traffic pattern is learned locally (i.e., on the device). Semi-supervised machine-learning methods are used for learning the normal behavioral patterns and for detecting deviations from the application's expected behavior. These methods were implemented and evaluated on Android devices. The evaluation experiments demonstrate that: (1) various applications have specific network traffic patterns and certain application categories can be distinguished by their network patterns; (2) different levels of deviation from normal behavior can be detected accurately; (3) in the case of self-updating malware, original (benign) and infected versions of an application have different and distinguishable network traffic patterns that in most cases, can be detected within a few minutes after the malware is executed while presenting very low false alarms rate; and (4) local learning is feasible and has a low performance overhead on mobile devices. 
43|-||Evaluation model for knowledge sharing in information security professional virtual community|Knowledge sharing has been proven to have affirmative effects on both the education and business sectors. Nevertheless, many professional virtual communities (PVC) have failed due to reasons, such as the low willingness of members to share knowledge with other members. In addition, it is not explicitly evident whether knowledge sharing in information security is able to reduce risk. To date, there have been relatively few empirical studies concerning the effects of knowledge sharing and its capability to reduce risk in information security communities. This paper proposes a model that is composed of two main parts. The first part is the Triandis theory, which is adapted to understand and foster the determinants of knowledge sharing behavior in PVCs. The second part explores the quantitative relationship between knowledge sharing and security risk reduction expectation. One hundred and forty-two members from the LinkedIn information security groups participated in this study. PLS analysis shows that perceived consequences, affect, and facilitating conditions have significant effects on knowledge sharing behavior. In contrast, social factors have shown insignificant effects on knowledge sharing behavior in information security communities. The results of the study demonstrate that there is a positive and strong relationship between knowledge sharing behavior and information security risk reduction expectation. 
43|-||On fingerprinting probing activities|Motivated by recent cyber attacks that were facilitated through probing, limited cyber security intelligence and the lack of accuracy that is provided by scanning detection systems, this paper presents a new approach to fingerprint probing activity. It investigates whether the perceived traffic refers to probing activities and which exact scanning technique is being employed to perform the probing. Further, this work strives to examine probing traffic dimensions to infer the ‘machinery’ of the scan; whether the probing is random or follows a certain predefined pattern; which probing strategy is being employed; and whether the probing activity is generated from a software tool or from a worm/bot. The approach leverages a number of statistical techniques, probabilistic distribution methods and observations in an attempt to understand and analyze probing activities. To prevent evasion, the approach formulates this matter as a change point detection problem that yielded motivating results. Evaluations performed using 55 GB of real darknet traffic shows that the extracted inferences exhibit promising accuracy and can generate significant insights that could be used for mitigation purposes. 
43|-||Static analysis based invariant detection for commodity operating systems|Recent interest in runtime attestation requires modeling of a program's runtime behavior to formulate its integrity properties. In this paper, we study the possibility of employing static source code analysis to derive integrity models of a commodity operating systems kernel. We develop a precise and static analysis-based data invariant detection tool that overcomes several technical challenges: field-sensitivity, array-sensitivity, and pointer analysis. We apply our tool to Linux kernel 2.4.32 and Windows Research Kernel (WRK). For Linux kernel 2.4.32, our tool identifies 284,471 data invariants that are critical to its runtime integrity, e.g., we use them to detect ten real-world Linux rootkits. Furthermore, comparison with the result of a dynamic invariant detector reveals 17,182 variables that can cause false alarms for the dynamic detector in the constant invariants category. Our tool also works successfully for WRK and reports 202,992 invariants, which we use to detect nine real-world Windows malware and one synthetic Windows malware. When compared with a dynamic invariant detector, we see similar results in terms of false alarms. Our experience suggests that static analysis is a viable option for automated integrity property derivation, and it can have very low false positive rate and very low false negative rate (e.g., for the constant invariants of WRK, the false positive rate is one out of 100,822 and the false negative rate is 0.007% or seven out of 100,822). 
43|-||An exploratory investigation of message-person congruence in information security awareness campaigns|In this study, we sought to answer the question of whether certain information security awareness message themes are more or less effective for different types of individuals based on their personality traits. We considered five message themes (deterrence, morality, regret, feedback, and incentive) as they relate to seven personality traits (the Big Five, Machiavellianism, and social desirability). Our survey analysis of 293 users provides evidence that security awareness message effectiveness does vary based on personality, but not always as one would expect. Depending on certain personality traits, some security messages appear beneficial to security efforts, whereas other personality traits make the individual less receptive to certain message types and therefore security messages may backfire in terms of achieving their intended effect. Our exploratory results can assist practitioners in identifying a best fit between security awareness themes and individual users based on their personality profile. 
43|-||User identification and authentication using multi-modal behavioral biometrics|Biometric computer authentication has an advantage over password and access card authentication in that it is based on something you are, which is not easily copied or stolen. One way of performing biometric computer authentication is to use behavioral tendencies associated with how a user interacts with the computer. However, behavioral biometric authentication accuracy rates are worse than more traditional authentication methods. This article presents a behavioral biometric system that fuses user data from keyboard, mouse, and Graphical User Interface (GUI) interactions. Combining the modalities results in a more accurate authentication decision based on a broader view of the user's computer activity while requiring less user interaction to train the system than previous work. Testing over 31 users shows that fusion techniques significantly improve behavioral biometric authentication accuracy over single modalities on their own. Between the two fusion techniques presented, feature fusion and an ensemble based classification method, the ensemble method performs the best with a False Acceptance Rate (FAR) of 2.10% and a False Rejection Rate (FRR) 2.24%. 
43|-||Information security knowledge sharing inÂ organizations: Investigating the effect ofÂ behavioral information security governance andÂ national culture|This paper presents an empirical investigation on what behavioral information security governance factors drives the establishment of information security knowledge sharing in organizations. Data was collected from organizations located in different geographic regions of the world, and the amount of data collected from two countries – namely, USA and Sweden – allowed us to investigate if the effect of behavioral information security governance factors on the establishment of security knowledge sharing differs based on national culture.The study followed a mixed methods research design, wherein qualitative data was collected to both establish the study's research model and develop a survey instrument that was distributed to 578 information security executives. The results suggest that processes to coordinate implemented security knowledge sharing mechanisms have a major direct influence on the establishment of security knowledge sharing in organizations; the effect of organizational structure (e.g., centralized security function to develop and deploy uniform firm-wide policies, and use of steering committees to facilitate information security planning) is slightly weaker, while business-based information security management has no significant direct effect on security knowledge sharing. A mediation analysis revealed that the reason for the nonsignificant direct relation between business-based information security management and security knowledge sharing is the fully mediating effect of coordinating information security processes. Thus, the results disentangles the interrelated influences of behavioral information security governance factors on security knowledge sharing by showing that information security governance sets the platform to establish security knowledge sharing, and coordinating processes realize the effect of both the structure of the information security function and the alignment of information security management with business needs.A multigroup analysis identified that national culture had a significant moderating effect on the association between four of the six proposed relations. In Sweden – which is seen as a less individualist, feminine country – managers tend to focus their efforts on implementing controls that are aligned with business activities and employees' need; monitoring the effectiveness of the implemented controls, and assuring that the controls are not too obtrusive to the end-user. On the contrary, US organizations establish security knowledge sharing in their organization through formal arrangements and structures. These results imply that Swedish managers perceive it to be important to involve, or at least know how their employees cope with the decisions that have been made, thus favoring local participation in information security management, while US managers may feel the need to have more central control when running their information security function.The findings suggest that national culture should be taken into consideration in future studies – in particular when investigating organizations operating in a global environment – and understand how it affects behaviors and decision-making. 
43|-||Nothing ventured, nothing gained. Profiles of online activity, cyber-crime exposure, and security measures of end-users in European Union|We use large-scale survey data from the Eurobarometer 77.2/2012 to explore variability in online activity, cyber-crime exposure, and security measures of end-users in European Union (EU27). While cyber-security is a high-priority activity for security experts and researchers, end-users conduct it in the context of their daily lives, as a socially accountable and resource-limited activity. We argue that end-users' security behaviors should be analyzed in relation to their experiences of online victimization, in the context of their routine activities. An ecological analysis at country level indicates that societies with widespread Internet use support cultures of higher cyber-security. They also expose daily Internet users to higher cyber-crime risks, but this positive correlation is weaker, with Romania and Hungary as two notable exceptions of high average exposure with low overall Internet use. Given the negative feedback loops between security responses, exposure to cyber-crime, and online activity, we find that, at individual level, linear causal modeling on survey data is impractical, and we propose classification analysis as a better tool for capturing variability. We use K-means cluster analysis to identify five types of end-users' orientation towards security in the context of their activity: ‘explorer’, ‘reactive’, ‘prudent’, ‘lucky’, and ‘occasional’ users, and we discuss their profiles of online activities and experiences. ‘Prudent’ users are relatively neglected in public campaigns for Internet security. Classification analysis is a productive tool for understanding end-users' security orientations through survey data and for informing public interventions. 
43|-||On the adoption of anomaly detection for packed executable filtering|Malware packing is a common technique employed to hide malicious code and to avoid static analysis. In order to fully inspect the contents of the executable, unpacking techniques must be applied. Unfortunately, generic unpacking is computationally expensive. For this reason, it is important to filter binaries in order to correctly handle them. In previous work, we proposed the adoption of anomaly detection for the classification of packed and not packed binaries using features based on the Portable Executable structure. In this paper, we extend this work and thoroughly evaluate the method with a different dataset and two different feature sets, rendering new conclusions. While anomaly detection is reaffirmed as a sound method for the discrimination of packed and not packed binaries, Portable Executable structure based features present limitations to distinguish custom packed files from not packed files. 
43|-||Obscuring users' identity in VoIP/IMS environments|Next Generation Networks bring together wired and wireless architectures, under the umbrella of an all IP architecture. Architectures such as the IP Multimedia Subsystem (IMS) offer advanced services at very low cost but also inherit IP infrastructure's security and privacy issues. The utilized signalling protocol (i.e. Session Initiation Protocol) and the related specifications are both overlooking users' privacy, leaving public and private identities unprotected to eavesdroppers. Existing solutions require either the existence of a public key infrastructure or the establishment of the appropriate mechanism for managing symmetric keys.We propose a novel one-time identity mechanism for obscuring users' real identity against eavesdroppers. The solution exploits the advantages of commutative functions, enabling the communicating parties to exchange data without pre-established keys nor any modification in the infrastructure. All participating entities generate one-time random identities providing in this way unlinkability and anonymity services as well.We evaluate the proposed mechanism through an open source IMS platform. Results have provided evidence that the client's response times are not considerably affected by the proposed mechanism, while the overhead imposed to the IMS core is negligible. 
43|-||SoNeUCONABC, an expressive usage control model for Web-Based Social Networks|In the era of hyper-connectivity Web-Based Social Networks (WBSNs) are demanding applications. They facilitate the interaction of huge amounts of users and the development of appropriate Access Control Models (ACMs) is an arising necessity. Particularly, the development of WBSNs ACMs with expressive power and capable of managing access control along the whole usage process is the challenge pursued. To contribute on this issue, first, 23 proposals have been analysed and second, SoNeUCONABC, an expressive usage control model for WBSNs, is proposed. It extends UCONABC ( Park, 2003) including relationships management and it is formally defined, specifying entities and elements involved and an access control policy language. Moreover, policy construction is carefully detailed by using regular expressions and access control enforcement functions are described. Finally, the evaluation shows, theoretically, the significant expressive power of SoNeUCONABC and, empirically, the feasibility of its implementation by the development of a proof of concept system. 
43|-||A Bug Hunter's Diary|
43|-||EFM: Enhancing the performance of signature-based network intrusion detection systems using enhanced filter mechanism|Signature-based network intrusion detection systems (NIDSs) have been widely deployed in current network security infrastructure. However, these detection systems suffer from some limitations such as network packet overload, expensive signature matching and massive false alarms in a large-scale network environment. In this paper, we aim to develop an enhanced filter mechanism (named EFM) to comprehensively mitigate these issues, which consists of three major components: a context-aware blacklist-based packet filter, an exclusive signature matching component and a KNN-based false alarm filter. The experiments, which were conducted with two data sets and in a network environment, demonstrate that our proposed EFM can overall enhance the performance of a signature-based NIDS such as Snort in the aspects of packet filtration, signature matching improvement and false alarm reduction without affecting network security. 
43|-||Permission based Android security: Issues and countermeasures|Android security has been a hot spot recently in both academic research and public concerns due to numerous instances of security attacks and privacy leakage on Android platform. Android security has been built upon a permission based mechanism which restricts accesses of third-party Android applications to critical resources on an Android device. Such permission based mechanism is widely criticized for its coarse-grained control of application permissions and difficult management of permissions by developers, marketers, and end-users. In this paper, we investigate the arising issues in Android security, including coarse granularity of permissions, incompetent permission administration, insufficient permission documentation, over-claim of permissions, permission escalation attack, and TOCTOU (Time of Check to Time of Use) attack. We illustrate the relationships among these issues, and investigate the existing countermeasures to address these issues. In particular, we provide a systematic review on the development of these countermeasures, and compare them according to their technical features. Finally, we propose several methods to further mitigate the risk in Android security. 
44|-|http://www.sciencedirect.com/science/journal/01674048/44|Contents|
44|-||Is Anti-virus Really Dead?|
44|-||A situation awareness model for information security risk management|Information security risk management (ISRM) is the primary means by which organizations preserve the confidentiality, integrity and availability of information resources. A review of ISRM literature identified deficiencies in the practice of information security risk assessment that inevitably lead to poor decision-making and inadequate or inappropriate security strategies. In this conceptual paper, we propose a situation aware ISRM (SA-ISRM) process model to complement the information security risk management process. Our argument is that the model addresses the aforementioned deficiencies through an enterprise-wide collection, analysis and reporting of risk-related information. The SA-ISRM model is adapted from Endsley's situation awareness model and has been refined using our findings from a case study of the US national security intelligence enterprise. 
44|-||Survey of certificate usage in distributed access control|Access control is an important building block in many distributed applications, and several solutions, both centralised and distributed, have been proposed and used for such applications. Certificates are particularly well suited to distributed systems and have been used in several ways.In this paper, we survey the certificate landscape from 2000 onwards. Our emphasis is on authorisation certificates and SPKI in particular. 
44|-||Implementing a database encryption solution, design and implementation issues|In this paper, we analyze and compare five traditional architectures for database encryption. We show that existing architectures may provide a high level of security, but have a significant impact on performance and impose major changes to the application layer, or may be transparent to the application layer and provide high performance, but have several fundamental security weaknesses. We suggest a sixth novel architecture that was not considered before. The new architecture is based on placing the encryption module inside the database management software (DBMS), just above the database cache, and using a dedicated technique to encrypt each database value together with its coordinates. These two properties allow our new architecture to achieve a high level of data security while offering enhanced performance and total transparency to the application layer. We also explain how each architecture can be implemented in a commercial, open source DBMS. We evaluate the performance of the various architectures both analytically and through extensive experimentation. Our performance evaluation results demonstrate that in most realistic scenarios, i.e., where only a part of the database content is stored in the database cache, the suggested architecture outperforms the others. 
44|-||Hybrid k-Anonymity|Anonymization-based privacy protection ensures that published data cannot be linked back to an individual. The most common approach in this domain is to apply generalizations on the private data in order to maintain a privacy standard such as k-anonymity. While generalization-based techniques preserve truthfulness, relatively small output space of such techniques often results in unacceptable utility loss especially when privacy requirements are strict. In this paper, we introduce the hybrid generalizations which are formed by not only generalizations but also the data relocation mechanism. Data relocation involves changing certain data cells to further populate small groups of tuples that are indistinguishable with each other. This allows us to create anonymizations of finer granularity confirming to the underlying privacy standards. Data relocation serves as a tradeoff between utility and truthfulness and we provide an input parameter to control this tradeoff. Experiments on real data show that allowing a relatively small number of relocations increases utility with respect to heuristic metrics and query answering accuracy. 
44|-||Securing Cloud and Mobility|
44|-||Control flow-based opcode behavior analysis for Malware detection|Opcode sequences from decompiled executables have been employed to detect malware. Currently, opcode sequences are extracted using text-based methods, and the limitation of this method is that the extracted opcode sequences cannot represent the true behaviors of an executable. To solve this issue, we present a control flow-based method to extract executable opcode behaviors. The behaviors extracted by this method can fully represent the behavior characteristics of an executable. To verify the efficiency of control flow-based behaviors, we perform a comparative study of the two types of opcode behavior analysis methods. The experimental results indicate that the proposed control flow-based method has a higher overall accuracy and a lower false positive rate. 
44|-||Unintended disclosure of information: Inference attacks by third-party extensions to Social Network Systems|Popularity of Social Network Systems (SNSs) has significantly increased in recent years, raising serious concerns for the privacy of users. Such concerns arise partly because SNS providers allow third-party extensions to access their users' information through an Application Programming Interface (API). Typical permission-based protection mechanisms restrict direct access to user data. However, once an extension has been authorized by a user to access some data in a user’s profile, there is no more control on how that extension uses the data. A malicious extension may try to infer other information based on the legitimately accessible information. If an extension is not supposed to know the inferred information, then this information leakage process is called an inference attack. Due to the large number of users who subscribe to third-party extensions in SNSs, even an inference attack with only a moderate success rate can put the privacy of a large number of users at risk. In addition, inference attacks are not only a privacy violation, they could also be used as the building blocks for more dangerous security attacks, such as identity theft and phishing attacks. In this work, we conduct a comprehensive empirical study to assess the feasibility and accuracy of inference attacks that are launched from the extension API of SNSs. We devise an analytical framework for assessing the success rate of sample inference attacks, and discuss two further attack scenarios in which inference attacks are employed as building blocks. The significance of this work is in thoroughly discussing how inference attacks could happen in practice via the extension API of SNSs, and highlighting the clear and present danger of even the naively crafted inference attacks. 
44|-||Propagation model of smartphone worms based on semi-Markov process and social relationship graph|Smartphone applications are getting more and more popular and pervasive in our daily life, and are also attractive to malware writers due to their limited computing source and vulnerabilities. At the same time, we possess limited understanding of our opponents in cyberspace. In this paper, we investigate the propagation model of SMS/MMS-based worms through integrating semi-Markov process and social relationship graph. In our modeling, we use semi-Markov process to characterize state transition among mobile nodes, and hire social network theory, a missing element in many previous works, to enhance the proposed mobile malware propagation model. In order to evaluate the proposed models, we have developed a specific software, and collected a large scale real-world data for this purpose. The extensive experiments indicate that the proposed models and algorithms are effective and practical. 
44|-||Detecting SQL injection attacks using query result size|Web applications are becoming an essential part of our everyday lives, with many of our activities dependent on the functionality and security of these applications. Web applications are ubiquitous, perform mission critical tasks, and handle sensitive user data. As the scale of these applications grows, injection vulnerabilities, such as SQL injections, become major security challenges. Most of these vulnerabilities stem from a lack of input validation; that is, web applications use malicious input as part of a sensitive operation without properly checking or sanitizing the input values. SQL injection attacks target databases that are accessible through a web front-end; moreover, they take advantage of flaws in the input validation logic of web components. In this paper, we exhibit a novel scheme that automatically transforms web applications, rendering them safe against SQL injection attacks. Our technique dynamically analyzes the developer-intended query result size for any input, and detects attacks by comparing this against the result of the actual query. We implement this technique in a tool for protecting Java-based web applications. An experimental evaluation demonstrates that our technique is effective against SQL injection vulnerabilities. 
45|-|http://www.sciencedirect.com/science/journal/01674048/45|Contents|
45|-||Taxonomy of intrusion risk assessment and response system|In recent years, we have seen notable changes in the way attackers infiltrate computer systems compromising their functionality. Research in intrusion detection systems aims to reduce the impact of these attacks. In this paper, we present a taxonomy of Intrusion Response Systems (IRS) and Intrusion Risk Assessment (IRA), two important components of an intrusion detection solution. We achieve this by classifying a number of studies published during the last two decades. We discuss the key features of existing IRS and IRA. We show how characterizing security risks and choosing the right countermeasures are an important and challenging part of designing an IRS and an IRA. Poorly designed IRS and IRA may reduce network performance and wrongly disconnect users from a network. We propose techniques on how to address these challenges and highlight the need for a comprehensive defense mechanism approach. We believe that this taxonomy will open up interesting areas for future research in the growing field of intrusion risk assessment and response systems. 
45|-||Uniform DoS traceback|Denial of service (DoS) is a significant security challenge in the Internet. Identifying the attackers so that their attack traffic can be blocked at source is one strategy that can be used to mitigate DoS attacks. However, determining the source can be difficult due to the inherent connectionless nature of IP. Traceback using various marking schemes that overload, mostly unused, fields in the IP header are promising techniques to identify the source of the attack. This paper shows that the marking probability used in two existing techniques: probabilistic packet marking (PPM) and dynamic probabilistic packet marking (DPPM) are not optimal and derives an optimal marking scheme called uniform probabilistic packet marking (UPPM). The performance of UPPM is shown to be improved compared to PPM and DPPM by performing comparative numerical analysis. One significant advantage of UPPM over these earlier techniques is that it performs marking at the level of autonomous systems (ASs) rather than at every router. This has advantages both in terms of marking overhead and allowing the optimal formulation of marking probability by utilizing metrics readily available from BGP-4, the inter-AS routing protocol. 
45|-||An approach for profiling phishing activities|Phishing attacks continue unabated to plague Internet users and trick them into providing personal and confidential information to phishers. In this paper, an approach for email-born phishing detection based on profiling and clustering techniques is proposed. We formulate the profiling problem as a clustering problem using various features present in the phishing emails as feature vectors and generate profiles based on clustering predictions. These predictions are further utilized to generate complete profiles of the emails. We carried out extensive experimental analysis of the proposed approach in order to evaluate its effectiveness to various factors such as sensitivity to the type of data, number of data sizes and cluster sizes. We compared the performance of the proposed approach against the Modified Global Kmeans (MGKmeans) approach. The results show that the proposed approach is efficient as compared to the baseline approach. 
45|-||Information security incident management: Current practice as reported in the literature|This paper reports results of a systematic literature review on current practice and experiences with incident management, covering a wide variety of organisations. Identified practices are summarised according to the incident management phases of ISO/IEC 27035. The study shows that current practice and experience seem to be in line with the standard. We identify some inspirational examples that will be useful for organisations looking to improve their practices, and highlight which recommended practices generally are challenging to follow. We provide suggestions for addressing the challenges, and present identified research needs within information security incident management. 
45|-||A multi-level approach to understanding the impact of cyber crime on the financial sector|This paper puts forward a multi-level model, based on system dynamics methodology, to understand the impact of cyber crime on the financial sector. Consistent with recent findings, our results show that strong dynamic relationships, amongst tangible and intangible factors, affect cyber crime cost and occur at different levels of society and value network. Specifically, shifts in financial companies' strategic priorities, having the protection of customer trust and loyalty as a key objective, together with considerations related to market positioning vis-à-vis competitors are important factors in determining the cost of cyber crime. Most of these costs are not driven by the number of cyber crime incidents experienced by financial companies but rather by the way financial companies choose to go about in protecting their business interests and market positioning in the presence of cyber crime. Financial companies' strategic behaviour as response to cyber crime, especially in regard to over-spending on defence measures and chronic under-reporting, has also an important consequence at overall sector and society levels, potentially driving the cost of cyber crime even further upwards. Unwanted consequences, such as weak policing, weak international frameworks for tackling cyber attacks and increases in the jurisdictional arbitrage opportunities for cyber criminals can all increase the cost of cyber crime, while inhibiting integrated and effective measures to address the problem. 
45|-||Bluetooth Command and Control channel|Bluetooth is popular technology for short-range communications and is incorporated in mobile devices such as smartphones, tablet computers and laptops. Vulnerabilities associated with Bluetooth technology led to improved security measures surrounding Bluetooth connections. Besides the improvement in security features, Bluetooth technology is still plagued by vulnerability exploits. This paper explores the development of a physical Bluetooth C&C channel, moving beyond previous research that mostly relied on simulations. In order to develop a physical channel, certain requirements must be fulfilled and specific aspects regarding Bluetooth technology must be taken into consideration. To measure performance, the newly designed Bluetooth C&C channel is executed in a controlled environment using the Android operating system as a development platform. The results show that a physical Bluetooth C&C channel is indeed possible and the paper concludes by identifying potential strengths and weaknesses of the new channel. 
45|-||On the security of text-based 3D CAPTCHAs|CAPTCHAs have become a standard security mechanism that are used to deter automated abuse of online services intended for humans. However, many existing CAPTCHA schemes to date have been successfully broken. As such, a number of CAPTCHA developers have explored alternative methods of designing CAPTCHAs. 3D CAPTCHAs is a design alternative that has been proposed to overcome the limitations of traditional CAPTCHAs. These CAPTCHAs are designed to capitalize on the human visual system's natural ability to perceive 3D objects from an image. The underlying security assumption is that it is difficult for a computer program to identify the 3D content. This paper investigates the robustness of text-based 3D CAPTCHAs. In particular, we examine three existing text-based 3D CAPTCHA schemes that are currently deployed on a number of websites. While the direct use of Optical Character Recognition (OCR) software is unable to correctly solve these text-based 3D CAPTCHA challenges, we highlight certain patterns in the 3D CAPTCHAs can be exploited to identify important information within the CAPTCHA. By extracting this information, this paper demonstrates that automated attacks can be used to solve these 3D CAPTCHAs with a high degree of success. 
45|-||An empirical comparison of botnet detection methods|The results of botnet detection methods are usually presented without any comparison. Although it is generally accepted that more comparisons with third-party methods may help to improve the area, few papers could do it. Among the factors that prevent a comparison are the difficulties to share a dataset, the lack of a good dataset, the absence of a proper description of the methods and the lack of a comparison methodology. This paper compares the output of three different botnet detection methods by executing them over a new, real, labeled and large botnet dataset. This dataset includes botnet, normal and background traffic. The results of our two methods (BClus and CAMNEP) and BotHunter were compared using a methodology and a novel error metric designed for botnet detections methods. We conclude that comparing methods indeed helps to better estimate how good the methods are, to improve the algorithms, to build better datasets and to build a comparison methodology. 
45|-||Ontology for attack detection: An intelligent approach to web application security|Conventional detection techniques struggle to keep up with the inherent complexity of web application design and hence the ever growing variety of attacks that can exploit it. Security frameworks modeled using an ontological approach are a promising new line of defense that can be highly effective in detecting zero day and sophisticated web application attacks because they can capture the context of the contents of information such as HTML pages or in-line scripts and have the ability to filter these contents by taking into consideration their consequences to the target applications. The goal of this article is to demonstrate how an ontology-engineering methodology may be systematically applied for designing and evaluating such security systems. A detailed ontological model is shown that caters to the generalized working of web applications, the underlying communication protocols and attacks. More specifically the proposed ontological model because it captures the context can not only detect HTTP protocol specification attacks but also helps focus only on specific portions of the request and response where a malicious script is possible. The model also captures the context of important attacks, the various technologies used by the hackers, source, target and vulnerabilities exploited by the attack, impact on system components and controls for mitigation. A comprehensive and best metrics suite for ontology evaluation has been used for assessing the quality of proposed model which includes correctness, accuracy, consistency, soundness, task orientation, completeness, conciseness, expandability, reusability, clarity, integrity, efficiency and expressiveness. The proposed model ranked well against the above mentioned metrics. Moreover a prototype attack detection system based upon the proposed model showed improved performance and detection rate and low rate of false positives while detecting OWASP's top ten listed web attacks. 
45|-||Soft biometrics for keystroke dynamics: Profiling individuals while typing passwords|This paper presents a new profiling approach of individuals based on soft biometrics for keystroke dynamics. Soft biometric traits are unique representation of a person, which can be in a form of physical, behavioural or biological human characteristics that differentiate between him/her into a group people (e.g. gender, age, height, colour, race etc.). Keystroke dynamics is a behavioural biometric modality to recognise how a person types on a keyboard. In this paper, we consider the following soft traits: the hand category (i.e. if the user types with one or two hands), the gender category, the age category and the handedness category. For this purpose, we collected a new database. Two cases are studied: static passwords and free text. By combining machine learning and fusion process, the results are promising. 
45|-||Performance evaluation of anomaly-detection algorithms for mouse dynamics|Mouse dynamics—the analysis of mouse operating behaviors to identify users—has been proposed for detecting impostors. Since many anomaly-detection algorithms have been proposed for this task, it is natural to ask how well these algorithms perform and how they compare with each other (e.g., to identify promising research directions). This paper presents a performance-evaluation study of a range of anomaly-detection algorithms in mouse dynamics on an equal basis. We collected a mouse-dynamics data set consisting of 17,400 samples from 58 subjects, developed a repeatable evaluation methodology, and implemented and evaluated 17 detectors from the mouse-dynamics and pattern-recognition literatures. Performance is measured in terms of detection accuracy, sensitivity to training sample size, usability with respect to sample length, and scalability with respect to the number of users (user space). The six top-performing detectors achieve equal-error rates between 8.81% and 11.63% with a detection time of 6.1 s; detector performance improves as training sample size and sample length increase and becomes saturated gradually; detector performance decreases as user space becomes large, but only small fluctuations with the error range are apparent when the space size exceeds a certain number. Along with the shared data and evaluation methodology, the results constitute a benchmark for comparing detectors and measuring progress. 
45|-||Complexity is dead, long live complexity! How software can help service providers manage security and compliance|Service providers expected to see a simplification regarding security and compliance management as standards and best practice were applied to complex information technology (IT) outsourcing arrangements. However, security and compliance management became even more complex and is presenting greater challenges to service providers than ever before. In this article, we focus on the work practices of service providers dealing with complex and transitory security requirements and distributed IT infrastructures. Based on the results of semi-structured interviews followed by a think-aloud study, we first describe specific requirements to be met by software supporting security and compliance management in complex IT outsourcing arrangements, and discuss the extent to which existing software already meets them. We show that existing software, which is primarily designed for in-house settings, fails to meet requirements of complex IT outsourcing arrangements such as (1) the use of standardized and formal descriptions of security requirements and configurations, (2) the definition of a interface allowing to exchange messages and to delegate tasks, (3) the provision of mechanisms for designing and implementing a configuration for specific security requirements across organizational boundaries, (4) the provision of mechanisms for verifying and approving the enforcement of these security requirements, and (5) the provision of mechanisms for searching and browsing security requirements, configurations and links between them. We then propose a software architecture that claims to be capable of meeting those requirements and outline how this claim was evaluated by means of another think-aloud study in which potential end users were asked to perform a series of tasks using a prototypical implementation of the architecture. The results of the evaluation confirm that the software meets the described requirements and suggests that it facilitates the management of security and compliance in complex IT outsourcing arrangements. 
45|-||A practical solution for sealed bid and multi-currency auctions|This paper introduces a sealed bid and multi-currency auction using secure multiparty computation (SMC). Two Boolean functions, a comparison and multiplication function, have been designed as required to apply SMC. These functions are applied without revealing any information, not even to trusted third parties such as the auctioneer. A type of Zero Knowledge proof, discreet proof, has been implemented with three variants, interactive, regular and reduced non interactive proofs. These proofs make it possible to verify the correctness of the functions whilst preserving the privacy of the bid values. Moreover, a system performance evaluation of the proposal has been realized on heterogeneous platforms, including a mobile platform. The evaluation concludes that our proposal is practical even on mobile platforms. 
45|-||Location leakage in distance bounding: Why location privacy does not work|In many cases, we can only have access to a service by proving we are sufficiently close to a particular location (e.g. in automobile or building access control). In these cases, proximity can be guaranteed through signal attenuation. However, by using additional transmitters an attacker can relay signals between the prover and the verifier. Distance-bounding protocols are the main countermeasure against such attacks; however, such protocols may leak information regarding the location of the prover and/or the verifier who run the distance-bounding protocol.In this paper, we consider a formal model for location privacy in the context of distance-bounding. In particular, our contributions are threefold: we first define a security game for location privacy in distance bounding; secondly, we define an adversarial model for this game, with two adversary classes; finally, we assess the feasibility of attaining location privacy for distance-bounding protocols. Concretely, we prove that for protocols with a beginning or a termination, it is theoretically impossible to achieve location privacy for either of the two adversary classes, in the sense that there always exists a polynomially-bounded adversary winning the security game. However, for so-called limited adversaries, who cannot see the location of arbitrary provers, carefully chosen parameters do, in practice, enable computational location privacy. 
45|-||Towards optimal noise distribution forÂ privacy preserving in data aggregation|In aggregation applications, individual privacy is a crucial factor to determine the effectiveness, for which the noise-addition method (i.e., a random noise value is added to the true value) is a simple yet powerful approach. However, improper additive noise could result in bias for the aggregate result. It demands an optimal noise distribution to reduce the deviation. In this paper, we develop a mathematical framework to derive the optimal noise distribution that provides privacy protection under the constraint of a limited value deviation. Specifically, we first derive a generic system dynamic function that the optimal noise distribution must satisfy, and further investigate the special case that the original values obey Gaussian distribution. Then we detailedly investigate the general cases that the original values obey arbitrary continuous distribution, which can be expressed by Gaussian Mixture Model (GMM). Our theoretical analysis suggests that for the Gaussian input Gaussian distribution is the optimal solution, and for the general input, the optimal solution is composed of infinite number of Gaussian components. We further find the general term formula of the components, which reduces the number of unknowns from infinite to three, i.e. the parameters of the first component (variance, expectation, weight). Based on it, we investigate the properties and propose an algorithm in order to calculate the asymptotically optimal solution composed of finite Gaussian components. The numerical evaluation shows that the results has little deviation to the optimal solutions. 
45|-||Selection of Candidate Support Vectors in incremental SVM for network intrusion detection|In an Incremental Support Vector Machine classification, the data objects labelled as non-support vectors by the previous classification are re-used as training data in the next classification along with new data samples verified by Karush–Kuhn–Tucker (KKT) condition. This paper proposes Half-partition strategy of selecting and retaining non-support vectors of the current increment of classification – named as Candidate Support Vectors (CSV) – which are likely to become support vectors in the next increment of classification. This research work also designs an algorithm named the Candidate Support Vector based Incremental SVM (CSV-ISVM) algorithm that implements the proposed strategy and materializes the whole process of incremental SVM classification. This work also suggests modifications to the previously proposed concentric-ring method and reserved set strategy. Performance of the proposed method is evaluated with experiments and also by comparing it with other ISVM techniques. Experimental results and performance analyses show that the proposed algorithm CSV-ISVM is better than general ISVM classifications for real-time network intrusion detection. 
45|-||HTTP attack detection using n-gram analysis|Previous research has shown that byte-level analysis of network traffic can be useful for network intrusion detection and traffic analysis. Such an approach does not require any knowledge of applications running on web servers or any pre-processing of incoming data.In this paper, we apply three n-gram techniques to the problem of HTTP attack detection. The goal is to provide a first line of defense by filtering the vast majority of benign HTTP traffic, leaving only a relatively small amount of suspect traffic for more costly processing. We analyze these n-gram techniques in terms of accuracy and performance. Our results show that we can attain equal or better detection rates at considerably less cost, in comparison to a previously developed HMM-based technique. We also apply these techniques to a highly realistic dataset consisting of four recent attacks and show that we obtain equally strong results in this case. Overall, these results indicate that this type of byte-level analysis is highly effective and practical. 
45|-||Design guidelines for security protocols toÂ prevent replay & parallel session attacks|This work is concerned with the design of security protocols. These protocols are susceptible to intruder attacks and their security compromised if weaknesses in the protocols' design are evident. In this paper a new analysis is presented on the reasons why security protocols are vulnerable to replay and parallel session attack and based on this analysis a new set of design guidelines to ensure resistance to these attacks is proposed. The guidelines are general purpose so as to encompass a wide spectrum of security protocols.Further, an empirical study on the effectiveness of the proposed guidelines is carried out on a set of protocols, incorporating those that are known to be vulnerable to replay or parallel session attacks as well as some amended versions that are known to be free of these weaknesses. The goal of this study is to establish conformance of the set of protocols with the proposed design guidelines. The results of the study show that any protocol following the design guidelines can be considered free of weaknesses exploitable by replay or parallel session attacks. On the other hand, if non-conformance of a protocol with the design guidelines is determined, then the protocol is vulnerable to replay or parallel session attacks. 
45|-||Shadow IT â A view from behind the curtain|Shadow IT is a currently misunderstood and relatively unexplored phenomena. It represents all hardware, software, or any other solutions used by employees inside of the organisational ecosystem which have not received any formal IT department approval. But how much do we know about this phenomenon? What is behind the curtain? Is security in organisations jeopardised? In the research study reported here, we conducted an in-depth analysis of the organisational Shadow IT software database, reporting the view from behind the curtain. The study used triangulation approach to investigate the Shadow IT phenomena and its findings open Pandora's Box as they lay a new picture of what Shadow IT looks like from the software perspective. Our study revealed that greynet, content apps, and utility tools are the most used shadow systems. This study offers important insights on the Shadow IT phenomena for information management professionals and provides new research directions for academia. 
45|-||A comparative analysis of detection metrics for covert timing channels|Methods to detect covert timing channels (CTCs) can be categorized into three broad classes: shape tests which include the Kolmogorov–Smirnov (KS) test, entropy tests which include first order entropy test, corrected conditional entropy (CCE) test, and Kullback–Leibler (KL) divergence test, and regularity tests. This paper contributes towards understanding and advancing the state-of-the-art of CTC detection methods. First, we present a detailed analysis of the performance of the well-known tests that are used to detect three main types of CTCs, namely, JitterBug, model-based CTC (MB-CTC) and time-replay CTC (TR-CTC). The performance analysis is carried out in an enterprise-like setting, employing large traffic traces. The detection methods are compared with respect to their applicability, computational complexity, and the classification rates for the three types of CTCs. In addition to evaluating the existing methods, we propose a new shape test based on the Welch's t-test and compare its performance with existing detection methods. We show that the classification rate of Welch's t-test is at least at par with other existing detection methods while having a relatively lower computational cost. The results also show that the Welch's t-test outperforms the CCE test in detecting JitterBug, while the CCE test has a better performance in detecting the TR-CTC. Furthermore, both tests perform comparably on the MB-CTC. Finally, we study the feasibility of using a multi-feature SVM classifier to increase the classification rate. We show that by combining the Welch's t-test we are able to increase the classification rate of MB-CTCs from 0.67 (using a single regularity measure) to 0.94. 
45|-||Time and space interval record schedule consistency analysis for atomic items without interactions in open spaces with stationary locations|Attacks on systems often produce records that are distinguishable from normal records because, by the nature of the subversions they undertake, they produce records that the system could not produce under normal operation. This paper outlines a basis for understanding and determining one class of such discernible subversion inconsistencies associated with time and space interval record schedule consistency analysis for atomic items in open spaces without interactions as a method of questioned digital record examination. It starts with a brief introduction to the issues and description of the specific problem at hand, develops an approach to solving the problem, and identifies an algorithm for near-linear time detection of inconsistency or demonstration of a feasible schedule for special cases likely to occur in real-world record-keeping. 
45|-||Securing cloud and mobility: A practitioner's guide|
46|-|http://www.sciencedirect.com/science/journal/01674048/46|Contents|
46|-||RAPID: Traffic-agnostic intrusion detection for resource-constrained wireless mesh networks|Due to the recent increased interest in wireless mesh networks (WMN), their security challenges have become of paramount importance. An important security mechanism for WMN, intrusion detection, has received considerable attention from the research community. Recent results show that traditional monitoring mechanisms are not applicable to real-world WMN due to their constrained resources (memory and processing power), which result in high false negative rates since only a few IDS functions can be activated on monitoring nodes. Cooperative solutions, on the other hand, have high communication overhead and detection delay when the traffic load is high. A practical traffic-aware IDS solution was recently proposed for resource-constrained WMN, however, traffic-awareness might not be feasible for some WMN applications. This article proposes a traffic-agnostic IDS solution that uses a link-coverage approach to monitor both local and backbone WMN traffic. Using real-world experiments and extensive simulations, we show that our proposed IDS solutions outperform traffic-aware IDS solutions while incurring lower computation and communication overhead. 
46|-||Cyber situational awareness â A systematic review of the literature|Cyber situational awareness is attracting much attention. It features prominently in the national cyber strategies of many countries, and there is a considerable body of research dealing with it. However, until now, there has been no systematic and up-to-date review of the scientific literature on cyber situational awareness.This article presents a review of cyber situational awareness, based on systematic queries in four leading scientific databases. 102 articles were read, clustered, and are succinctly described in the paper. The findings are discussed from the perspective of both national cyber strategies and science, and some directions for future research are examined. 
46|-||Toward a secure and usable cloud-based password manager for web browsers|Web users are confronted with the daunting challenges of creating, remembering, and using more and more strong passwords than ever before in order to protect their valuable assets on different websites. Password manager, particularly Browser-based Password Manager (BPM), is one of the most popular approaches designed to address these challenges by saving users' passwords and later automatically filling the login forms on behalf of users. Fortunately, all the five most popular Web browsers have provided password managers as a useful built-in feature. In this paper, we uncover the vulnerabilities of existing BPMs and analyze how they can be exploited by attackers to crack users' saved passwords. Moreover, we propose a novel Cloud-based Storage-Free BPM (CSF-BPM) design to achieve a high level of security with the desired confidentiality, integrity, and availability properties. We have implemented a CSF-BPM system into Firefox and evaluated its correctness, performance, and usability. Our evaluation results and analysis demonstrate that CSF-BPM can be efficiently and conveniently used. We believe CSF-BPM is a rational design that can also be integrated into other popular browsers to make the online experience of Web users more secure, convenient, and enjoyable. 
46|-||Decision support for releasing anonymised data|For legal and privacy reasons it is often prescribed that data bases containing sensitive personal data can be published only in anonymised form. History shows, however, that the privacy of anonymised data in many cases is easily broken by de-anonymisation attacks. This paper defines guiding principles for decisions about releasing anonymised data and provides a simple process for analysing de-anonymisation risk and for making decisions about publishing anonymised personal data. At the heart of this process is an information-theoretic de-anonymisation feasibility limit that is independent of the details of both the anonymisation procedure and the adversarial de-anonymisation algorithms. This feasibility limit relates the adversarial mutual information of the anonymised data and the attacker's background information to the number of records in the anonymised data base and the acceptable risk of privacy violations. Based on this result, we explain, discuss and exemplify the process for making decisions about releasing anonymised data. 
46|-||Enhancing the detection of metamorphic malware using call graphs|Malware stands for malicious software. It is software that is designed with a harmful intent. A malware detector is a system that attempts to identify malware using Application Programming Interface (API) call graph technique and/or other techniques. API call graph techniques follow two main steps, namely, transformation of malware samples into an API call graph using API call graph construction algorithm, and matching the constructed graph against existing malware call graph samples using graph matching algorithm. A major issue facing malware API call graph construction algorithms is building a precise call graph from information collected about malware samples. On the other hand call graph matching is an NP-complete problem and is slow because of computational complexity. In this study, a malware detection system based on API call graph is proposed. In the proposed system, each malware sample is represented as an API call graph. API call graph construction algorithm is used to transform input malware samples into API call graph by integrating API calls and operating system resource to represent graph nodes. Moreover, the dependence between different types of nodes is identified and represented using graph edges. After that, graph matching algorithm is used to calculate similarity between the input sample and malware API call graph samples that are stored in a database. The graph matching algorithm is based on an enhanced graph edit distance algorithm that simplifies the computational complexity using a greedy approach to select best common subgraphs from the integrating API call graph with high similarity, which helps in terms of detecting metamorphic malware. Experimental results on 514 malware samples demonstrate that the proposed system has 98% accuracy and 0 false positive rates. Detailed comparisons against other detection methods have been carried out and significant improvement over them is shown. 
46|-||Stable web spam detection using features based on lexical items|Web spam is a method of manipulating search engines results by improving ranks of spam pages. It takes various forms and lacks a consistent definition. Web spam detectors use machine learning techniques to detect spam. However, the detectors are mostly verified on data sets coming from the same year as the learning sets. In this paper we compared Support Vector Machine classifiers trained and tested on WEBSPAM–UK data sets from different years. To obtain stable results we proposed new lexical-based features. The HTML document – transformed into a text without HTML tags, a set of visible symbols, and a list of links including the ones from tags – gave information about weird combinations of letters; consonant clusters; statistics on syllables, words, and sentences; and the Gunning Fog Index. Using data collected in 2006 as a learning set, we obtained very stable accuracy among years. This choice of the training set reduced the sensitivity in 2007, but that can be improved by managing the acceptance threshold. Finally, we proved that the balance between the sensitivity and the specificity measured by the Area Under the Curve (AUC) is improved by our selection of features. 
46|-||An unsupervised anomaly-based detection approach for integrity attacks on SCADA systems|Supervisory Control and Data Acquisition (SCADA) systems are a core part of industrial systems, such as smart grid power and water distribution systems. In recent years, such systems become highly vulnerable to cyber attacks. The design of efficient and accurate data-driven anomaly detection models become an important topic of interest relating to the development of SCADA-specific Intrusion Detection Systems (IDSs) to counter cyber attacks. This paper proposes two novel techniques: (i) an automatic identification of consistent and inconsistent states of SCADA data for any given system, and (ii) an automatic extraction of proximity detection rules from identified states. During the identification phase, the density factor for the k-nearest neighbours of an observation is adapted to compute its inconsistency score. Then, an optimal inconsistency threshold is calculated to separate inconsistent from consistent observations. During the extraction phase, the well-known fixed-width clustering technique is extended to extract proximity-detection rules, which forms a small and most-representative data set for both inconsistent and consistent behaviours in the training data set. Extensive experiments were carried out both on real as well as simulated data sets, and we show that the proposed techniques provide significant accuracy and efficiency in detecting cyber attacks, compared to three well-known anomaly detection approaches. 
46|-||New X.509-based mechanisms for fair anonymity management|Privacy has become a major concern in the Internet, resulting in an increased popularity of anonymizing systems aimed to protect users' identities. However, service providers sometimes interpret this anonymity as a risk, since dishonest users may take advantage of it. A possible solution is to create a practical implementation of fairness mechanisms to reach an equilibrium between anonymity and its different types of revocation. Furthermore, in order to reach a wide acceptance, any new mechanism must be easily deployable in current systems and must be adaptable (from the functionality perspective) to the needs that may arise in different situations. To that end, we propose a set of extensions to the CRL and OCSP procedures of the X.509 infrastructure, and a new protocol for easing the task of providing evidence of illegitimate actions. On one hand, the adaptability of our scheme relies on the already widely deployed X.509 infrastructure. On the other hand, the functionality provided by our proposal is mainly built upon group signatures, which gives it a vast variety of schemes to choose from, depending on the specific needs that may arise. 
46|-||Stealing bandwidth from BitTorrent seeders|BitTorrent continues to comprise the largest fraction of Internet traffic. While significant progress has been made in understanding the BitTorrent choking mechanism, its security vulnerabilities have not been investigated thoroughly. This paper presents an experimental analysis of bandwidth attacks against different choking algorithms in the BitTorrent seed state. We reveal a simple exploit that allows malicious peers to receive a considerably higher download rate than contributing leechers, therefore introducing significant efficiency degradations for benign peers. We show the damage caused by the proposed attack in two different environments: a lab testbed comprising 32 peers and a PlanetLab testbed with 300 peers. Our results show that 3 malicious peers can degrade the download rate up to 414.99% for all peers. Combined with a Sybil attack that consists of as many attackers as leechers, it is possible to degrade the download rate by more than 1000%. We propose a novel choking algorithm which is immune against bandwidth attacks and a countermeasure against the revealed attack. 
46|-||Cylindrical Coordinates Security Visualization for multiple domain command and control botnet detection|The botnets are one of the most dangerous species of network-based attack. They cause severe network disruptions through massive coordinated attacks nowadays and the results of this disruption frequently cost enterprises large sums in financial losses. In this paper, we make an in-depth investigation on the issue of botnet detection and present a new security visualization tool for visualizing botnet behaviors on DNS traffic. The core mechanism is developed with the objective of enabling users to recognize security threats promptly and mitigate the damages by only visualizing DNS traffic in cylindrical coordinates. We compare our visualization method with existing ones and the experimental results show that ours has greater perceptual efficiency. The ideas and results of this study will contribute toward designing an advanced visualization technique that offers better security. Also, the approach proposed in this study can be utilized to derive new and valuable insights in security aspects from the complex correlations of Big Data. 
46|-||Security analysis of temporal RBAC under an administrative model|Security analysis of access control models is critical to confirm whether they ensure certain security properties. Administrative models specify the rules for state transition for any given access control model. While security analysis of role-based access control (RBAC) systems has been done using administrative models, work on security analysis of its temporal, spatial and spatio-temporal extensions has so far not considered the presence of any corresponding administrative model. In this paper, we present a methodology for performing security analysis of temporal RBAC (TRBAC) where state changes occur using the relations defined in a recently proposed administrative model named as AMTRAC (Administrative Model for Temporal Role-based Access Control). We initially define a number of security properties for TRBAC. These properties along with a representation of the TRBAC system and the administrative relations in AMTRAC are then formally specified using Alloy, a first order logic based language. Subsequently, validity of the specified properties is analyzed using the Alloy analyzer. We study the impact of the number of roles, users and temporal elements of TRBAC as well as various relations defined in AMTRAC on the time taken for security analysis. 
46|-||Introduction to Computer and Network Security: Navigating Shades of Gray|
47|-|http://www.sciencedirect.com/science/journal/01674048/47|Contents|
47|-||Editorial: Special issue on trust in cyber, physical and social computing|
47|-||Analysis on the acceptance of Global Trust Management for unwanted traffic control based on game theory|The Internet has witnessed an incredible growth in its pervasive use and brought unprecedented convenience to its users. However, an increasing amount of unwanted traffic, such as spam and malware, severely burdens both users and Internet service providers (ISPs), which arouses wide public concern. A Global Trust Management (GTM) system was proposed and demonstrated to be accurate, robust and effective on unwanted traffic control in our previous work (22 and 23). But its acceptance by network entities (ISPs and hosts) is crucial to its practical deployment and final success. In this paper, we investigate the acceptance conditions of the GTM system using game theory. Considering the selfish nature of network entities, we address our problem as a social dilemma. To enhance cooperation among network entities, a public-goods-based GTM game is formulated with a trust-based punishment mechanism that can provide the incentives of behaving cooperatively for network entities. Meanwhile, the conditions of the adoption of GTM system are figured out. We also carry out a number of simulations to illustrate the acceptance conditions of the GTM system in practical deployment, and show the effectiveness of the trust-based punishment mechanism. Furthermore, suggestions for ISPs cooperating with antivirus vendors are put forward. 
47|-||Evaluating and comparing the quality of access control in different operating systems|Access control mechanisms (ACMs) have been widely used by operating systems (OSes) to protect information security. However, it is often challenging to evaluate and compare the quality of protection (QoP) of ACMs, especially when they are deployed on different OS platforms. This article presents an approach to quantitatively measure and compare the quality of ACMs, which provides useful information to support OS administrators and users to choose ACMs that fit with their security needs.We introduce the notion of vulnerability profiles to capture the weakness of ACMs in protecting against malicious attacks, based on which vulnerability coefficients are computed as the numeric and platform-independent measurement of the QoP of ACMs. The approach combines the grey system theory and an independent vulnerability scoring system to infer complete vulnerability profiles and to calculate fair and objective vulnerability coefficients for ACMs. We implement a prototype called ACVAL based on the approach, and apply it to four mainstream ACMs. The results show that ACVAL is effective in evaluating and comparing ACMs across different OSes, a feature particularly useful to administrators of heterogeneous IT systems. To the best of our knowledge, our approach is the first to quantitative measurement and comparison of ACMs across OSes. 
47|-||CooPeD: Co-owned Personal Data management|With the spread of Web-Based Social Networks (WBSNs) managing access to data is a challenging matter. Providing personalized, fine-grained access control is essential to build trusted WBSNs. WBSNs data can be associated with owners and co-owners, namely users who upload the data and users who are linked to uploaded data respectively. Thus, a privacy-friendly WBSN must allow users the management of elements related to them. In this regard, CooPeD (Co-owned Personal Data management), a system that deals with co-ownership management of decomposable objects, is proposed. CooPeD is formed by a model and a mechanism. CooPeD is developed on the bases of SoNeUCONABC usage control model. Particularly, an extension of SoNeUCONABC is proposed to support co-ownership management by means of access control and administrative management. In CooPeD's mechanism objects, decomposed in parts, are attached to owners and co-owners who individually set their access control preferences. The evaluation of CooPeD consists of three parts. Firstly, a feasibility analysis for different architectures of CooPeD's model and mechanism, as well as of CooPeD's mechanism in Facebook is performed. Secondly, a prototype proves the feasibility of implementing CooPeD. Lastly, a survey study assesses the acceptance of CooPeD. 
47|-||Effective detection of vulnerable and malicious browser extensions|Unsafely coded browser extensions can compromise the security of a browser, making them attractive targets for attackers as a primary vehicle for conducting cyber-attacks. Among others, the three factors making vulnerable extensions a high-risk security threat for browsers include: i) the wide popularity of browser extensions, ii) the similarity of browser extensions with web applications, and iii) the high privilege of browser extension scripts. Furthermore, mechanisms that specifically target to mitigate browser extension-related attacks have received less attention as opposed to solutions that have been deployed for common web security problems (such as SQL injection, XSS, logic flaws, client-side vulnerabilities, drive-by-download, etc.). To address these challenges, recently some techniques have been proposed to defend extension-related attacks. These techniques mainly focus on information flow analysis to capture suspicious data flows, impose privilege restriction on API calls by malicious extensions, apply digital signatures to monitor process and memory level activities, and allow browser users to specify policies in order to restrict the operations of extensions.This article presents a model-based approach to detect vulnerable and malicious browser extensions by widening and complementing the existing techniques. We observe and utilize various common and distinguishing characteristics of benign, vulnerable, and malicious browser extensions. These characteristics are then used to build our detection models, which are based on the Hidden Markov Model constructs. The models are well trained using a set of features extracted from a number of browser extensions together with user supplied specifications. Along the course of this study, one of the main challenges we encountered was the lack of vulnerable and malicious extension samples. To address this issue, based on our previous knowledge on testing web applications and heuristics obtained from available vulnerable and malicious extensions, we have defined rules to generate training samples. The approach is implemented in a prototype tool and evaluated using a number of Mozilla Firefox extensions. Our evaluation indicated that the approach not only detects known vulnerable and malicious extensions, but also identifies previously undetected extensions with a negligible performance overhead. 
47|-||Happy faces considered trustworthy irrespective of perceiver's mood: Challenges to the mood congruency effect|Interpersonal trust is affected by the emotional states and facial expressions of the interacting parties. This study investigated the interactive (combined) effects of emotions and facial expressions by simultaneously manipulating both variables. Three laboratory experiments were conducted using a face evaluation task (i.e., choosing the most trustworthy face from a face pair) with happy, sad, and neutral faces. Experiment 1 tested the effect of facial expressions on trust judgments. Experiments 2 and 3 manipulated both facial expressions and perceiver's emotions. The results of the three experiments showed that happy faces were more often chosen as the most trustworthy relative to neutral and sad faces, and that perceiver's emotions did not influence trust judgment. These results indicated that other's facial expressions as dominant social cue would bias interpersonal trust, which might cause security issue in online social networks. 
48|-|http://www.sciencedirect.com/science/journal/01674048/48|Contents|
48|-||Vulnerabilities and mitigation techniques toning in the cloud: A cost and vulnerabilities coverage optimization approach using Cuckoo search algorithm with LÃ©vy flights|Information and Communication Technology (ICT) security issues have been a major concern for decades. Today's ICT infrastructure faces sophisticated attacks using combinations of multiple vulnerabilities to penetrate networks with devastating impact. With the recent rise of cloud computing as a new utility computing paradigm, organizations have been considering it as a viable option to outsource major IT services in order to cut costs. Some organizations have opted for a private or hybrid cloud to take advantage of the emerging technologies and services. However, ICT security issues have to be appropriately mitigated. This research proposes a cloud security framework and an approach for vulnerabilities coverage and cost optimization using Cuckoo search algorithm with Lévy flights as random walks. The objective is to mitigate an identified set of vulnerabilities using a selected set of techniques when minimizing cost and maximizing coverage. The results show that Cloud Computing providers and organizations implementing cloud technology within their premises can effectively balance IT security coverage and cost using the proposed approach. 
48|-||Implementing information security best practices on software lifecycle processes: The ISO/IEC 15504 Security Extension|The ISO/IEC 15504 international standard can be aligned with the ISO/IEC 27000 information security management framework. During the research conducted all the existing relations between ISO/IEC 15504-5 software development base practices and ISO/IEC 27002 security controls have been analysed and the ISO/IEC 15504 Security Extension has been developed. This extension details the changes that software companies should make in the software lifecycle processes for the successful implementation of the related security controls. To attain our research objectives, we evaluate the ISO/IEC 15504 Security Extension through case studies in a sample of software development organizations. This study follows the design science research paradigm that is based on constructive research. 
48|-||Combating advanced persistent threats: From network event correlation to incident detection|An advanced persistent threat (also known as APT) is a deliberately slow-moving cyberattack that is applied to quietly compromise interconnected information systems without revealing itself. APTs often use a variety of attack methods to get unauthorized system access initially and then gradually spread throughout the network. In contrast to traditional attacks, they are not used to interrupt services but primarily to steal intellectual property, sensitive internal business and legal documents and other data. If an attack on a system is successful, timely detection is of paramount importance to mitigate its impact and prohibit APTs from further spreading. However, recent security incidents, such as Operation Shady Rat, Operation Red October or the discovery of MiniDuke – just to name a few – have impressively demonstrated that current security mechanisms are mostly insufficient to prohibit targeted and customized attacks. This paper therefore proposes a novel anomaly detection approach which is a promising basis for modern intrusion detection systems. In contrast to other common approaches, which apply a kind of black-list approach and consider only actions and behaviour that match to well-known attack patterns and signatures of malware traces, our system works with a white-list approach. Our anomaly detection technique keeps track of system events, their dependencies and occurrences, and thus learns the normal system behaviour over time and reports all actions that differ from the created system model. In this work, we describe this system in theory and show evaluation results from a pilot study under real-world conditions. 
48|-||On the limits of engine analysis for cheating detection in chess|The integrity of online games has important economic consequences for both the gaming industry and players of all levels, from professionals to amateurs. Where there is a high likelihood of cheating, there is a loss of trust and players will be reluctant to participate — particularly if this is likely to cost them money.Chess is a game that has been established online for around 25 years and is played over the Internet commercially. In that environment, where players are not physically present “over the board” (OTB), chess is one of the most easily exploitable games by those who wish to cheat, because of the widespread availability of very strong chess-playing programs. Allegations of cheating even in OTB games have increased significantly in recent years, and even led to recent changes in the laws of the game that potentially impinge upon players’ privacy.In this work, we examine some of the difficulties inherent in identifying the covert use of chess-playing programs purely from an analysis of the moves of a game. Our approach is to deeply examine a large collection of games where there is confidence that cheating has not taken place, and analyse those that could be easily misclassified.We conclude that there is a serious risk of finding numerous “false positives” and that, in general, it is unsafe to use just the moves of a single game as prima facie evidence of cheating. We also demonstrate that it is impossible to compute definitive values of the figures currently employed to measure similarity to a chess-engine for a particular game, as values inevitably vary at different depths and, even under identical conditions, when multi-threading evaluation is used. 
48|-||Exfiltrating data from Android devices|Modern mobile devices have security capabilities built into the native operating system, which are generally designed to ensure the security of personal or corporate data stored on the device, both at rest and in transit. In recent times, there has been interest from researchers and governments in securing as well as exfiltrating data stored on such devices (e.g. the high profile PRISM program involving the US Government). In this paper, we propose an adversary model for Android covert data exfiltration, and demonstrate how it can be used to construct a mobile data exfiltration technique (MDET) to covertly exfiltrate data from Android devices. Two proof-of-concepts were implemented to demonstrate the feasibility of exfiltrating data via SMS and inaudible audio transmission using standard mobile devices. 
48|-||Managing XACML systems in distributed environments through Meta-Policies|Policy-based authorization systems have been largely deployed nowadays to control different privileges over a big amount of resources within a security domain. With policies it is possible to reach a fine-grained level of expressiveness to state proper responses of a system against multiple access control requests. In this context, XACML has achieved a big popularity between both industry and academy as a standard for the definition of access control policies, as well as an architecture for the evaluation of authorization requests and for the issuing of authorization decisions. However, the applicability of XACML is still not clear in collaborative and distributed environments composed of several security domains sharing the access control over some specific resources. Such a circumstance manifests when many security domains can simultaneously define the behavior that a resource will have upon received authorization requests, like for instance an organization with many subsidiaries, a company with a service virtualization business model, etc. In this paper we propose a solution to reach an effective distributed policy management considering that a number of policies in one domain may be confidential. To this end, the default XACML architecture has been redefined in order to use i) Master and Slave PAPs to communicate security domains, ii) Meta-Policies to define privileges over access control policies (the policies become the managed resources) and iii) SAML extensions to protect the policy management messages which flow between security domains. The experiments and the defined scenarios in the paper prove the validity of the proposed solution. 
48|-||Towards complexity analysis of User Authorization Query problem in RBAC|The User Authorization Query (UAQ) problem for RBAC is to determine whether there exists an optimum set of roles to be activated to provide a particular set of permissions requested by a user. It is a key issue related to efficiently handling users' access requests. Previous definitions of the UAQ problem have considered only the optimization objective for the number of permissions whereas the optimization objective for the number of roles, which is also equally important, has been largely ignored. Moreover, little attention has been given to the computational complexity of the UAQ problem that considers the optimization objectives for both the numbers of permissions and roles. In this paper, we propose a more comprehensive definition of the UAQ problem, which includes irreducibility, role-cardinality and permission-cardinality constraints, and consider both these optimization objectives together. In particular, we study the computational complexity of the UAQ problem by dividing it into three subcases: exact match, safe match and available match, and show that many instances in each subcase with additional constraints are intractable. We also propose an approach for solving the intractable cases of the UAQ problem; the proposed approach incorporates static pruning, preprocessing and the depth-first search based algorithm to significantly reduce the running time. 
48|-||A data hiding scheme using pixel value differencing and improving exploiting modification directions|The fundamental requirements of information hiding systems are good visual quality, high hiding capacity, robustness and steganographic security. In this paper, we propose a new data hiding method which can increase the steganographic security of a data hiding scheme because it is less detectable by RS detection attack and the steganalytic histogram attack of pixel-value difference. In our method, a cover image is first mapped into a 1D pixels sequence by Hilbert filling curve and then divided into non-overlapping embedding units with two consecutive pixels. Because the human eyes tolerate more changes in edge and texture areas than in smooth areas, and pixel pairs in these areas often possess larger differences, the method exploits pixel value differences (PVD) to estimate the base of digits to be embedded into pixel pairs. Pixel pairs with larger differences are embedded with digits in larger base than those pixel pairs with smaller differences to maximize the payload and image quality. By using an optimization problem to solve the overflow/underflow problem, minimal distortion of the pixel ternaries cause by data embedding can be obtained. The experimental results show our method not only to enhance the embedding rate and good embedding capacity but also to keep stego-image quality. 
48|-||Insecurity of an anonymous authentication for privacy-preserving IoT target-driven applications|The Internet of Things (IoT) will be formed by smart objects and services interacting autonomously and in real-time. Recently, Alcaide et al. proposed a fully decentralized anonymous authentication protocol for privacy-preserving IoT target-driven applications. Their system is set up by an ad-hoc community of decentralized founding nodes. Nodes can interact, being participants of cyberphysical systems, preserving full anonymity. In this study, we point out that their protocol is insecure. The adversary can cheat the data collectors by impersonating a legitimate user. 
48|-||Authentication graphs: Analyzing user behavior within an enterprise network|User authentication over the network builds a foundation of trust within large-scale computer networks. The collection of this network authentication activity provides valuable insight into user behavior within an enterprise network. Representing this authentication data as a set of user-specific graphs and graph features, including time-constrained attributes, enables novel and comprehensive analysis opportunities. We show graph-based approaches to user classification and intrusion detection with practical results. We also show a method for assessing network authentication trust risk and cyber attack mitigation within an enterprise network using bipartite authentication graphs. We demonstrate the value of these graph-based approaches on a real-world authentication data set collected from an enterprise network. 
48|-||Transaction authentication using complementary colors|In this paper, we introduce a transaction authentication solution that provides compatibility with any banking transactions. Our solution is based on a novel visual cryptographic scheme that supports multiple uses of a single static share, unlike existing techniques in the literature of visual cryptography. To support multiple uses, we utilize complementary colors in the subtractive color mixing model. In our solution, a user is asked to overlap an issued visual cryptography card on an encrypted image that is rendered on a screen to check transaction information and transaction authentication number. We analyze the security of our scheme against various attacks and show its effectiveness using a usability study. 
48|-||The professionalisation of information security: Perspectives of UK practitioners|In response to the increased “cyber” threats to business, the UK and US Governments are taking steps to develop the training and professional identity of information security practitioners. The ambition of the UK Government is to drive the creation of a recognised profession, in order to attract technology graduates and others into the practice of cyber-security. Although much has been written by state bodies and industry commentators alike on this topic, we believe this qualitative study is the first empirical academic work investigating attitudes to that professionalisation amongst information security workers. The results are contextualised using concepts from the literature in the fields of professionalisation and social topics in information security.Despite the movement to establish professional status for their industry, these practitioners showed mixed levels of support for further professionalisation, with a distinctly wary attitude towards full regulation and licensing and an explicit rejection of elitist and exclusive models of profession. Whereas the UK Government looks to establish “professional” status in order to attract entrants, such status in itself was seen to be of little import to those already working in the area. In addition there are significant tensions between managers embracing business- and human-centred security and those more interested in the technical practice of executing policy.While these tensions continue, the results suggest that state attempts artificially to catalyse the professionalisation process for this group would be precipitate. Historically such projects have risen from the front line; ambitions to move the industry in that direction might see more success by identifying and delegating control to a single regulatory body, founded and respected by the people it aims eventually to regulate. 
48|-||Leakage-resilient password entry: Challenges, design, and evaluation|Password leakage is one of the most serious threats for password-based user authentication. Although this problem has been extensively investigated over the last two decades, there is still no widely adopted solution. In this paper, we attempt to systematically understand the challenges behind this problem and investigate the feasibility of solving it. Since password leakage usually happens when a password is input during authentication, we focus on designing leakage-resilient password entry (LRPE) schemes in this study. We develop a broad set of design criteria and use them to construct a practical LRPE scheme named CoverPad, which not only improves leakage resilience but also retains most usability benefits of legacy passwords. Its practicability is further verified by an extended user study. 
48|-||A framework for metamorphic malware analysis and real-time detection|Metamorphism is a technique that mutates the binary code using different obfuscations. It is difficult to write a new metamorphic malware and in general malware writers reuse old malware. To hide detection the malware writers change the obfuscations (syntax) more than the behavior (semantic) of such a new malware. On this assumption and motivation, this paper presents a new framework named MARD for Metamorphic Malware Analysis and Real-Time Detection. As part of the new framework, to build a behavioral signature and detect metamorphic malware in real-time, we propose two novel techniques, named ACFG (Annotated Control Flow Graph) and SWOD-CFWeight (Sliding Window of Difference and Control Flow Weight). Unlike other techniques, ACFG provides a faster matching of CFGs, without compromising detection accuracy; it can handle malware with smaller CFGs, and contains more information and hence provides more accuracy than a CFG. SWOD-CFWeight mitigates and addresses key issues in current techniques, related to the change of the frequencies of opcodes, such as the use of different compilers, compiler optimizations, operating systems and obfuscations. The size of SWOD can change, which gives anti-malware tool developers the ability to select appropriate parameter values to further optimize malware detection. CFWeight captures the control flow semantics of a program to an extent that helps detect metamorphic malware in real-time. Experimental evaluation of the two proposed techniques, using an existing dataset, achieved detection rates in the range 94%–99.6%. Compared to ACFG, SWOD-CFWeight significantly improves the detection time, and is suitable to be used where the time for malware detection is more important as in real-time (practical) anti-malware applications. 
48|-||Deceiving entropy based DoS detection|Denial of Service (DoS) attacks disable network services for legitimate users. As a result of growing dependence on the Internet by both the general public and service providers, the availability of Internet services has become a concern. While DoS attacks cause inconvenience for users, and revenue loss for service providers; their effects on critical infrastructures like the smart grid and public utilities could be catastrophic. For example, an attack on a smart grid system can cause cascaded power failures and lead to a major blackout. Researchers have proposed approaches for detecting these attacks in the past decade. Anomaly based DoS detection is the most common. The detector uses network traffic statistics; such as the entropy of incoming packet header fields (e.g. source IP addresses or protocol type). It calculates the observed statistical feature and triggers an alarm if an extreme deviation occurs. Entropy features are common in recent DDoS detection publications. They are also one of the most effective features for detecting these attacks. However, intrusion detection systems (IDS) using entropy based detection approaches can be a victim of spoofing attacks. An attacker can sniff the network and calculate background traffic entropy before a (D)DoS attack starts. They can then spoof attack packets to keep the entropy value in the expected range during the attack. This paper explains the vulnerability of entropy based network monitoring systems. We present a proof of concept entropy spoofing attack and show that by exploiting this vulnerability, the attacker can avoid detection or degrade detection performance to an unacceptable level. 
48|-||Detection of malicious PDF files and directions for enhancements: A state-of-the art survey|Initial penetration is one of the first steps of an Advanced Persistent Threat (APT) attack, and it is considered one of the most significant means of initiating cyber-attacks aimed at organizations. Such an attack usually results in the loss of sensitive and confidential information. Because email communication is an integral part of daily business operations, APT attackers frequently leverage email as an attack vector for initial penetration of the targeted organization. Emails allow the attacker to deliver malicious attachments or links to malicious websites. Attackers usually use social engineering in order to make the recipient open the malicious email, open the attachment, or press a link. Existing defensive solutions within organizations prevent executables from entering organizational networks via emails, therefore, recent APT attacks tend to attach non-executable files (PDF, MS Office etc.) which are widely used in organizations and mistakenly considered less suspicious or malicious. This article surveys existing academic methods for the detection of malicious PDF files. The article outlines an Active Learning framework and highlights the correlation between structural incompatibility of PDF files and their likelihood of maliciousness. Finally, we provide comparisons, insights and conclusions, as well as avenues for future research in order to enhance the detection of malicious PDFs. 
48|-||Measuring user satisfaction with information security practices|Information security is a major concern of organizational management. Security solutions based on technical aspects alone are insufficient to protect corporate data. Successful information security depends on appropriate user behavior while using information systems. User satisfaction is widely used to measure the success of information systems. The objective of this research is to develop a model to measure user satisfaction with information security practices. An instrument was developed based on this model. A survey was conducted, and 173 valid responses were obtained. Structural equation modeling was used for the data analysis. The results indicated that users understand the benefits of information security practices, but the use of information systems with security controls is considered a complex matter, which reduces information systems productivity. The measurement of the user satisfaction with information security practices is a starting point to diagnose the behavior of users in relation to information security, providing metrics to management evaluate the investment in information security training and awareness program. 
48|-||Comparing intention to avoid malware across contexts in a BYOD-enabled Australian university: A Protection Motivation Theory approach|Malware have been regarded as a persistent threat to both individuals and organisations due to its wide spread via various means of infection. With the increasing use of personal mobile devices and the trending adoption of Bring Your Own Device (BYOD) practices, this threat has become even more versatile and dreadful as it could hide behind the users' typical and daily Internet activities. The importance of investigating whether the user's intention to perform malware avoidance behaviours would change across multiple contexts is emphasised. Consequently, this study determines the contributing factors and compares their impacts on such intention by extending Protection Motivation Theory in two different contexts. A total of 252 Australian higher education students were surveyed when using mobile devices such as smartphone, laptop and tablet at home and at a BYOD-enabled university. Paired t-test, Bayesian structural equation modelling, and revised z-test were employed for data analysis. The empirical findings reveal that intention to perform malware avoidance behaviours differed across the contexts. Furthermore, the researchers found perceptions of self-efficacy and vulnerability to have different impacts on such intention and other variables in the model. As a result, such findings suggested developing community of practice and repeated trainings to maintain the users' confidence in their own abilities to cope with malware threats. Message that focuses on the threats' consequences was suggested to improve home users' intention to avoid malware, along with a number of factors that could be critical to designing information security education programs. Moreover, these implications particularly address information security management at educational institutions that adopt BYOD policy. Finally, theoretical contributions include an extended model based on Protection Motivation Theory that reflects the users' intention to avoid malware threats in BYOD context, from which directions for future research were also provided. 
49|-|http://www.sciencedirect.com/science/journal/01674048/49|Contents|
49|-||Decision Diagrams for XACML Policy Evaluation and Management|One of the primary challenges to apply the XACML access control policy language in applications is the performance problem of policy evaluation engines, particularly when they experience a great number of policies. Some existing works attempted to solve this problem, but only for some particular use-cases: either supporting simple policies with equality comparisons or predefined attribute values. Due to the lack of carefully checking the XACML model, they did not have original policy evaluation semantics. Therefore, they cannot handle errors containing indeterminate decisions, or ignore the critical attribute setting that leads to potential missing attribute attacks. In this paper, we build up the XACML logical model and propose a decision diagram approach using the data interval partition aggregation. It can parse and transform complex logical expressions in policies into decision tree structures, which efficiently improve the policy evaluation performance. Our approach can also be applied to solve other policy management problems such as policy redundancy detection, policy testings and comparisons, or authorization reverse queries. 
49|-||Towards efficient certificate status validations with E-ADOPT in mobile ad hoc networks|Each public key infrastructure needs an efficient certificate status validation method to exclude the revoked certificates from network. In this paper, we present a novel certificate validation scheme called E-ADOPT or Enhanced-ADOPT which utilizes a new kind of certificate status information. In this solution, we modify the OCSP response messages to carry information about the accusations issued against the certificate and this additional security information helps the client nodes to tune the OCSP results refresh rate more intelligently. As a result, client node can mitigate the certificate status information inconsistency problem with lower overheads and conduct more effective certificate status validations in MANET. Simulation results demonstrate that by appending accusation-related information to the OCSP responses, our solution achieves better results. 
49|-||Spherical microaggregation: Anonymizing sparse vector spaces|Unstructured texts are a very popular data type and still widely unexplored in the privacy preserving data mining field. We consider the problem of providing public information about a set of confidential documents. To that end we have developed a method to protect a Vector Space Model (VSM), to make it public even if the documents it represents are private. This method is inspired by microaggregation, a popular protection method from statistical disclosure control, and adapted to work with sparse and high dimensional data sets. 
49|-||A survey of information security incident handling in the cloud|Incident handling strategy is one key strategy to mitigate risks to the confidentiality, integrity and availability (CIA) of organisation assets, as well as minimising loss (e.g. financial, reputational and legal) particularly as organisations move to the cloud. In this paper, we surveyed existing incident handling and digital forensic literature with the aims of contributing to the knowledge gap(s) in handling incidents in the cloud environment. 139 English language publications between January 2009 and May 2014 were located by searching various sources including the websites of standard bodies (e.g. National Institute of Standards and Technology) and academic databases (e.g. Google Scholar, IEEEXplore, ACM Digital Library, Springer and ScienceDirect). We then propose a conceptual cloud incident handling model that brings together incident handling, digital forensic and the Capability Maturity Model for Services to more effectively handle incidents for organisations using the cloud. A discussion of open research issues concludes this survey. 
49|-||Cyber warfare: Issues and challenges|The topic of cyber warfare is a vast one, with numerous sub topics receiving attention from the research community. We first examine the most basic question of what cyber warfare is, comparing existing definitions to find common ground or disagreements. We discover that there is no widely adopted definition and that the terms cyber war and cyber warfare are not well enough differentiated. To address these issues, we present a definition model to help define both cyber warfare and cyber war. The paper then identifies nine research challenges in cyber warfare and analyses contemporary work carried out in each. We conclude by making suggestions on how the field may best be progressed by future efforts. 
49|-||Detecting fake anti-virus software distribution webpages|Attackers are continually seeking novel methods to distribute malware. Among various approaches, fake Anti-Virus (AV) attacks represent an active trend for malware distribution. In a fake AV attack, attackers disguise malware as legitimate anti-virus software and convince users to install it. As web browsers become the most popular applications for users to access online resources, webpages have become the dominating means to launch fake AV attacks. In this paper, we presented an automated and effective detection system, namely DART, to identify fake AV webpages in the Internet. We proposed a collection of novel features to characterize an unknown webpage and then integrate them using statistical classifiers. These features focus on profiling a fake AV webpage from three aspects that are fundamentally important for its success, thereby resulting in the high detection accuracy and implying resistance against evasion attempts. We have performed extensive evaluation based on real fake AV webpages that are collected from the Internet. Experimental results have demonstrated that DART can accomplish a high detection rate of 90.4% at an extremely low false positive rate of 0.2%. 
49|-||A novel methodology towards a trusted environment in mashup web applications|A mashup is a web-based application developed through aggregation of data from different public external or internal sources (including trusted and untrusted). Mashup introduces an open environment that is exposed to many security vulnerabilities, threats and risks. These weaknesses will bring security to the forefront when developing mashup applications and will require new ways of identifying and managing said risks. The primary goal of this paper is to present a client side mashup security framework to ensure that the sources for mashup applications are tested and secured against malicious intrusions. This framework is based on risk analysis and mashup source classification that will examine, analyze and evaluate the data transitions between the server-side and the client-side. Risk filtering using data mining suggests a new data mining technique also be utilized to enhance the quality of the risk analysis by removing most of the false risks. This approach is called the Risk Filtering Data Mining algorithm (RFDM). The RFDM framework deals with three types of clusters (trusted, untrusted and hesitation or unknown) to handle the hesitation clusters. Our proposal is to employ Atanassov's Instuitionistic Fuzzy Sets (A-IFs) as it improves the results of an URL. Finally, the results would be evaluated based on five experimental measures generated by a confusion matrix, namely: Accuracy (AC), recall or true positive rate (TP), precision (P), F-measure (considers both precision and recall) and FÎ². 
49|-||An efficient grouping method and error probability analysis for RO-PUFs|Physical Unclonable Functions (PUFs) are primitives that have wide usage areas in information security. Ordering based Ring Oscillator (RO)-PUFs have been introduced recently to overcome the robustness and area efficiency issues related to PUF implementations. With this approach, 100% robust outputs are generated, providing a solution for cryptographic key generation. High entropy extraction with relatively few ROs is also achieved, resulting in high area utilization of the PUF circuit. Frequency threshold determination is the most critical step in ordering based RO-PUFs, and determines a trade-off between area efficiency and robustness. In this work, we overview an efficient grouping method for RO-PUFs and analyze the error vulnerability of PUFs based on the frequency threshold determination. Next, we analyze the length of groups used in such PUF circuits and determine the symbol error probability. In addition to these, we demonstrate the relationship between the symbol error probability and bit error probability. We also investigate the bit error probability based on the wrong determination of the frequency threshold in ordering based RO-PUFs. Finally, a trade-off between area usage and robustness is presented for identification applications. 
49|-||Towards more pro-active access control in computer systems and networks|Access control is a core security technology which has been widely used in computer systems and networks to protect sensitive information and critical resources and to counter malicious attacks. Although many access control models have been developed in the past, such as discretionary access control (DAC), mandatory access control (MAC) and role-based access control (RBAC), these models are designed primarily as a defensive measure in that they are used for examining access requests and making authorization decisions based on established access control policies. As the result, even after a malicious access is identified, the requester can still keep issuing more malicious access requests without much fear of punitive consequences from the access control system in subsequent accesses. Such access control may be acceptable in closed systems and networks but is not adequate in open systems and networks where the real identities and other critical information about requesters may not be known to the systems and networks. In this paper, we propose to design pro-active access control so that access control systems can respond to malicious access pro-actively to suit the needs of open systems and networks. We will first apply some established principles in the Game Theory to analyze current access control models to identify the limitations that make them inadequate in open systems and networks. To design pro-active access control (PAC), we incorporate a constraint mechanism that includes feedback and evaluation components and show based on the Game Theory how to make such access control respond to malicious access in a pro-active manner. We also present a framework design of PAC and demonstrate through the implementation of trust-based access control the feasibility of design, implementation and application of pro-active access control. Such kind of models and mechanisms can serve as the foundation for the design of access control systems that will be made more effective in deterring malicious attacks in open systems and networks. 
49|-||SENTINEL: Securing Legacy Firefox Extensions|A poorly designed web browser extension with a security vulnerability may expose the whole system to an attacker. Therefore, attacks directed at “benign-but-buggy” extensions, as well as extensions that have been written with malicious intent, pose significant security threats to a system running such components. Recent studies have indeed shown that many Firefox extensions are over-privileged, making them attractive attack targets. Unfortunately, users currently do not have many options when it comes to protecting themselves from extensions that may potentially be malicious. Once installed and executed, the extension is considered trusted.This paper introduces SENTINEL, a policy enforcer for the Firefox browser that gives fine-grained control to the user over the actions of existing JavaScript Firefox extensions. The user is able to define policies (or use predefined ones) and block common attacks such as data exfiltration, remote code execution, saved password theft, preference modification, phishing, browser window clickjacking, and namespace collision exploits. Our evaluation of SENTINEL shows that our prototype implementation can effectively prevent concrete, real-world Firefox extension attacks without a detrimental impact on the user's browsing experience. 
49|-||Improving the information security culture through monitoring and implementation actions illustrated through a case study|The human aspect, together with technology and process controls, needs to be considered as part of an information security programme. Current and former employees are still regarded as one of the root causes of information security incidents. One way of addressing the human aspect is to embed an information security culture where the interaction of employees with information assets contributes to the protection of these assets. In other words, it is critical to improve the information security culture in organisations such that the behaviour of employees is in compliance with information security and related information processing policies and regulatory requirements. This can be achieved by assessing, monitoring and influencing an information security culture. An information security culture can be assessed by using an approach such as an information security culture assessment (ISCA). The empirical data derived from an ISCA can be used to influence the information security culture by focussing on developmental areas, of which awareness and training programmes are a critical facet.In this paper we discuss a case study of an international financial institution at which ISCA was conducted at four intervals over a period of eight years, across twelve countries. Comparative and multivariate analyses were conducted to establish whether the information security culture improved from one assessment to the next based on the developmental actions implemented. One of the key actions implemented was training and awareness focussing on the critical dimensions identified by ISCA. The information security culture improved from one assessment to the next, with the most positive results in the fourth assessment.This research illustrates that the theoretical ISCA tool previously developed can be implemented successfully in organisations to positively influence the information security culture. Empirical evidence is provided supporting the effectiveness of ISCA in the context of identified shortcomings in the organisation's information security culture. In addition, empirical evidence is presented indicating that information security training and awareness is a significant factor in positively influencing an information security culture when applied in the context of ISCA. 
49|-||Personality, attitudes, and intentions: Predicting initial adoption of information security behavior|Investigations of computer user behavior become especially important when behaviors like security software adoption affect organizational information resource security, but adoption antecedents remain elusive. Technology adoption studies typically predict behavioral outcomes by investigating the relationship between attitudes and intentions, though intention may not be the best predictor of actual behavior. Personality constructs have recently been found to explain even more variance in behavior, thus providing insights into user behavior. This research incorporates conscientiousness and agreeableness into a conceptual model of security software use. Attitudinal constructs perceived ease of use and perceived usefulness were linked with behavioral intent, while the relationship between intent and actual use was found to be moderated by conscientiousness and agreeableness. The results that the moderating effect of personality greatly increases the amount of variance explained in actual use. 
49|-||A Permission verification approach for android mobile applications|Mobile applications build part of their security and privacy on a declarative permission model. In this approach mobile applications, to get access to sensitive resources, have to define the corresponding permissions in a manifest. However, mobile applications may request access to permissions that they do not require for their execution (over-privileges) and offer opportunities to malicious software to gain access to otherwise inaccessible resources. In this paper, we investigate on the declarative permissions model on which security and privacy services of Android rely upon. We propose a practical and efficient permission certification technique, in the direction of risk management assessment. We combine both runtime information and static analysis to profile mobile applications and identify if they are over-privileged or follow the least privilege principle. We demonstrate a transparent solution that neither requires modification to the underlying framework, nor access to the applications' original source code. We assess the effectiveness of our approach, using a randomly selected varied set of mobile applications. Results show that our approach can accurately identify whether an application is over-privileged or not, whilst at the same time guaranteeing the need of declaring specific permissions in the manifest. 
49|-||RTECA: Real time episode correlation algorithm for multi-step attack scenarios detection|Today, from information security perspective, prevention methods are not enough solely. Early Warning Systems (EWSs) are in the category of reactive methods. These systems are complementing Intrusion Detection Systems (IDSs) where their main goals include early detection of potential malicious behavior in large scale environments such as national level. An important process in EWSs is the analysis and correlation of alerts aggregated from the installed sensors (e.g., IDSs, IP telescopes, and botnet detection systems). In this paper, an efficient framework for alert correlation in EWSs is proposed. The framework includes a correlation scheme based on a combination of statistical and stream mining techniques. The method works real-time by extracting critical episodes from sequences of alerts, which could be part of multi-step attack scenarios. A Causal Correlation Matrix (CCM) is used for encoding correlation strength between the alert types in attack scenarios. Experimental results show that the framework is efficient enough in detecting known attack scenarios and new attack strategies. The results also show that the system is able to predict the next steps of running attack scenaris up to 95% of accuracy under special circumstances. 
49|-||Towards privacy-preserving reputation management for hybrid broadcast broadband applications|Hybrid Broadcast Broadband TV (HbbTV) is an industry standard aimed to provide a platform combining TV services with Internet services, using connected TVs and set-top boxes. It enables the possibility for vendors to offer applications directly to the users, introducing new entertainment services such as streaming of video on demand, games, social networking, etc. As a consequence, tons of applications are available for users to directly download and consume through the so-called application stores, despite the potential trust and security issues arising due to the decentralized nature of these environments.Reputation management systems are usually deployed to handle trust in such dynamic scenarios, whereas they could also be used to evaluate and rate the applications from the users' point of view, and even provide customized rankings. Nevertheless, they require the application stores to know information related to the installed applications and the provided recommendations of the users, hence compromising their privacy.In this paper we present a privacy-preserving reputation management framework to be integrated within the HbbTV context. We make use of identity management and extend homomorphic encryption techniques to avoid the application stores and other relying parties determining the recommendations provided by the users, yet being able to compute customized reputation values based on the similarity between users' recommendations. 
49|-||Privacy leakage analysis in online social networks|Online Social Networks (OSNs) have become one of the major platforms for social interactions, such as building up relationship, sharing personal experiences, and providing other services. The wide adoption of OSNs raises privacy concerns due to personal data shared online. Privacy control mechanisms have been deployed in popular OSNs for users to determine who can view their personal information. However, user's sensitive information could still be leaked even when privacy rules are properly configured. We investigate the effectiveness of privacy control mechanisms against privacy leakage from the perspective of information flow. Our analysis reveals that the existing privacy control mechanisms do not protect the flow of personal information effectively. By examining representative OSNs including Facebook, Google+, and Twitter, we discover a series of privacy exploits. We find that most of these exploits are inherent due to the conflicts between privacy control and OSN functionalities. The conflicts reveal that the effectiveness of privacy control may not be guaranteed as most OSN users expect. We provide remedies for OSN users to mitigate the risk of involuntary information leakage in OSNs. Finally, we discuss the costs and implications of resolving the privacy exploits. 
49|-||Profiling user-trigger dependence for Android malware detection|As mobile computing becomes an integral part of the modern user experience, malicious applications have infiltrated open marketplaces for mobile platforms. Malware apps stealthily launch operations to retrieve sensitive user or device data or abuse system resources. We describe a highly accurate classification approach for detecting malicious Android apps. Our method statically extracts a data-flow feature on how user inputs trigger sensitive API invocations, a property referred to as the user-trigger dependence. Our evaluation with 1433 malware apps and 2684 free popular apps gives a classification accuracy (2.1% false negative rate and 2.0% false positive rate) that is better than, or at least competitive against, the state-of-the-art. Our method also discovers new malicious apps in the Google Play market that cannot be detected by virus scanning tools. Our thesis in this mobile app classification work is to advocate the approach of benign property enforcement, i.e., extracting unique behavioral properties from benign programs and designing corresponding classification policies. 
49|-||Hacking and Penetration Testing with Low Power Devices|
50|-|http://www.sciencedirect.com/science/journal/01674048/50|Contents|
50|-||Intrusion alert prioritisation and attack detection using post-correlation analysis|Event Correlation used to be a widely used technique for interpreting alert logs and discovering network attacks. However, due to the scale and complexity of today's networks and attacks, alert logs produced by these modern networks are much larger in volume and difficult to analyse. In this research we show that adding post-correlation methods can be used alongside correlation to significantly improve the analysis of alert logs.We proposed a new framework titled A Comprehensive System for Analysing Intrusion Alerts (ACSAnIA). The post-correlation methods include a new prioritisation metric based on anomaly detection and a novel approach to clustering events using correlation knowledge. One of the key benefits of the framework is that it significantly reduces false-positive alerts and it adds contextual information to true-positive alerts.We evaluated the post-correlation methods of ACSAnIA using data from a 2012 cyber range experiment carried out by industrial partners of the British Telecom Security Practice Team. In one scenario, our results show that false-positives were successfully reduced by 97% and in another scenario, 16%. It also showed that clustering correlated alerts aided in attack detection.The proposed framework is also being developed and integrated into a pre-existing Visual Analytic tool developed by the British Telecom SATURN Research Team for the analysis of cyber security data. 
50|-||Integrity, authenticity, non-repudiation, and proof of existence for long-term archiving: A survey|The world increasingly depends on archives to store digital documents, such as land registers and medical records, for long periods of time. For stored documents to remain trustworthy, archives must provide proofs that a document existed on a certain date and has not been changed since. In addition, in many cases, the origin of the document must be verifiable and the originator must not be able to repudiate that she is the originator. In this paper, we survey the solutions that provide the above protection goals in the long term. We analyze and compare the solutions with respect to their functionalities (which protection goals do they achieve?), the trust assumptions they require, and their performance. From this analysis and comparison, we deduce deficiencies of the current solutions and important research problems that must be solved in order to come up with protection solutions that are even more satisfactory. 
50|-||The ultimate control flow transfer in a Java based smart card|Recently, researchers published several attacks on smart cards. Among these, software attacks are the most affordable, they do not require specific hardware (laser, EM probe, etc.). Such attacks succeed to modify a sensitive system element which offers access to the smart card assets. To prevent that, smart card manufacturers embed dedicated countermeasures that aim to protect the sensitive system elements. We present a generic approach based on a Control Flow Transfer (CFT) attack to modify the Java Card program counter. This attack is built on a type confusion using the couple of instructions jsr/ret. Evaluated on different Java Cards, this new attack is a generic CFT exploitation that succeeds on each attacked cards. We present several countermeasures proposed by the literature or implemented by smart card designers and for all of them we explain how to bypass them. Then, we propose to use Attack Countermeasure Tree to develop an effective and affordable countermeasure for this attack. 
50|-||Relay and jammer selection schemes for improving physical layer security in two-way cooperative networks|This paper is concerned with the relay and jammers selection in two-way cooperative networks to improve their physical layer security. Three different categories of selection schemes are proposed which are; selection schemes without jamming, selection schemes with conventional jamming and selection schemes with controlled jamming. The selection process is analyzed for two different network models; single eavesdropper model and multiple cooperating and non-cooperating eavesdroppers’ model. The proposed schemes select three intermediate nodes during two communication phases and use the Decode-and-Forward (DF) strategy to assist the sources to deliver their data to the corresponding destinations. The performance of the proposed schemes is analyzed in terms of ergodic secrecy rate and secrecy outage probability metrics. The obtained results show that the selection schemes with jamming outperform the schemes without jamming when the intermediate nodes are distributed dispersedly between sources and eavesdropper nodes. However, when the intermediate nodes cluster gets close to one of the sources, they are not superior any more due to the strong interference on the destination nodes. Therefore, a hybrid scheme which switches between selection schemes with jamming and schemes without jamming is introduced to overcome the negative effects of interference. Finally, a comparison between relay and jammers selection schemes in both one-way and two-way cooperative networks is given in terms of both secrecy metrics. The obtained results reveal that, despite the presence of cooperating eavesdroppers, the proposed selection schemes are still able to improve both the secrecy rate and the secrecy outage probability of the two-way cooperative networks. 
50|-||Selecting a trusted cloud service provider for your SaaS program|Software as a Service (SaaS) offers major business and IT benefits that organizations are looking to take advantage of. SaaS adoption presents serious and unique security risks. Moving a company's sensitive data into the hands of cloud providers expands and complicates the risk landscape in which the organization operates.This paper highlights the significance and ramifications of a structured selection of a Cloud Service Provider (CSP) in achieving the required assurance level based on an organization's specific security posture. This paper proposes a holistic model, known as the Function, Auditability, Governability and Interoperability or FAGI, as an approach to help a Cloud Service Consumer (CSC) to engage and select a trusted CSP through four major decisions: Selecting a safe cloud that has adequate security functions; Choosing an auditable cloud via third-party certifications/assessments or self tests; Picking out a governable cloud that provides the required transparency; Opting for a portable cloud that ensures the desired portability.A case study reveals the FAGI approach offers an objective and efficient way to choose a qualified and trusted cloud service and in turn saves CSCs' time, effort, and grief. 
50|-||DP-Apriori: A differentially private frequent itemset mining algorithm based on transaction splitting|In this paper, we study the problem of designing a differentially private FIM algorithm which can simultaneously provide a high level of data utility and a high level of data privacy. This task is very challenging due to the possibility of long transactions. A potential solution is to limit the cardinality of transactions by truncating long transactions. However, such approach might cause too much information loss and result in poor performance. To limit the cardinality of transactions while reducing the information loss, we argue that long transactions should be split rather than truncated. To this end, we propose a transaction splitting based differentially private FIM algorithm, which is referred to as DP-Apriori. In particular, a smart weighted splitting technique is proposed to divide long transactions into sub-transactions whose cardinality is no more than a specified number of items. In addition, to offset the information loss caused by transaction splitting, a support estimation technique is devised to estimate the actual support of itemsets in the original database. Through privacy analysis, we show that our DP-Apriori algorithm is ÉÉ-differentially private. Extensive experiments on real-world datasets illustrate that DP-Apriori substantially outperforms the state-of-the-art techniques. 
50|-||SECO: Secure and scalable data collaboration services in cloud computing|Cloud storage services enable users to remotely store their data and eliminate excessive local installation of software and hardware. There is an increasing trend of outsourcing enterprise data to the cloud for efficient data storage and management. However, this introduces many new challenges toward data security. One critical issue is how to enable a secure data collaboration service including data access and update in cloud computing. A data collaboration service is to support the availability and consistency of the shared data among multi-users. In this paper, we propose a secure, efficient and scalable data collaboration scheme SECO. In SECO, we employ a multi-level hierarchical identity based encryption (HIBE) to guarantee data confidentiality against untrusted cloud. This paper is the first attempt to explore secure cloud data collaboration services that precludes information leakage and enables a one-to-many encryption paradigm, data writing operation and fine-grained access control simultaneously. Security analysis indicates that the SECO is semantically secure against adaptive chosen ciphertext attacks (IND-ID-CCA) in the random oracle model, and enforces fine-grained access control, collusion resistance and backward secrecy. Extensive performance analysis and experimental results show that SECO is highly efficient and has only low overhead on computation, communication and storage. 
volume|issue|url|title|abstract
51|-|http://www.sciencedirect.com/science/journal/01674048/51|Contents|
51|-||A practical off-line taint analysis framework and its application in reverse engineering of file format|This paper presents FlowWalker, a novel dynamic taint analysis framework that aims to extract the complete taint data flow while eliminating the bottlenecks that occur in existing tools, with applications to file-format reverse engineering. The framework proposes a multi-taint-tag assembly-level taint propagation strategy. FlowWalker separates taint tracking operations from execution with an off-line structure, utilizes memory-mapped files to enhance I/O efficiency, processes taint paths during virtual execution playback, and uses parallelization and pipelining mechanisms to achieve speedup. Based on the semantic correlations implied by the taint path information, this paper presents an algorithm for extracting the structures of unknown file formats. According to test data, the overall program runtime ranges from 92.98% to 208.01% of the length of the underlying instrumentation alone, while the speed enhancement is 60% compared to another well-featured tool in Windows. Medium-complexity file formats are correctly partitioned, and the constant fields are extracted. Due to its efficiency and scalability, FlowWalker can address the needs of further security-related research. 
51|-||Stealth attacks: An extended insight into the obfuscation effects on Android malware|In order to effectively evade anti-malware solutions, Android malware authors are progressively resorting to automatic obfuscation strategies. Recent works have shown, on small-scale experiments, the possibility of evading anti-malware engines by applying simple obfuscation transformations on previously detected malware samples. In this paper, we provide a large-scale experiment in which the detection performances of a high number of anti-malware solutions are tested against two different sets of malware samples that have been obfuscated according to different strategies. Moreover, we show that anti-malware engines search for possible malicious content inside assets and entry-point classes. We also provide a temporal analysis of the detection performances of anti-malware engines to verify if their resilience has improved since 2013. Finally, we show how, by manipulating the area of the Android executable that contains the strings used by the application, it is possible to deceive anti-malware engines so that they will identify legitimate samples as malware. On one hand, the attained results show that anti-malware systems have improved their resilience against trivial obfuscation techniques. On the other hand, more complex changes to the application executable have proved to be still effective against detection. Thus, we claim that a deeper static (or dynamic) analysis of the application is needed to improve the robustness of such systems. 
51|-||Time series modeling of vulnerabilities|Vulnerability prediction models forecast future vulnerabilities and can be used to assess security risks and estimate the resources needed for handling potential security breaches. Although several vulnerability prediction models have been proposed, such models have shortcomings and do not consider trend, level, and seasonality components of vulnerabilities. Through time series analysis, this study built predictive models for five popular web browsers: Chrome, Firefox, Internet Explorer, Safari and Opera and for all reported vulnerabilities elsewhere. Results showed that time series models provide a good fit to our vulnerability datasets and can be useful for vulnerability prediction. Results also suggested that the level of the series is the best estimator of the prediction models. 
51|-||Incorporating attacker capabilities in risk estimation and mitigation|The risk exposure of a given threat to an information system is a function of the likelihood of the threat and the severity of its impacts. Existing methods for estimating threat likelihood assume that the attacker is able to cause a given threat, that exploits existing vulnerabilities, if s/he has the required opportunities (e.g., sufficient attack time) and means (e.g., tools and skills), which is not true; often, s/he can perform an attack and cause the related threat only if s/he has the ability to access related resources (objects) of the system that allow to do so. This paper proposes a risk estimation method that incorporates attacker capabilities in estimating the likelihood of threats as conditions for using the means and opportunities, demonstrates the use of the proposed risk estimation method through two examples: video conferencing systems and connected vehicles, shows that changing attacker capabilities changes the risks of the threats, and compares the uncertainty of experts in evaluating the likelihood of threats considering and not considering attacker capabilities for two experiments. The results of the experiments suggest that experts are less uncertain about their estimations of threat likelihoods when they consider attacker capabilities. 
52|-|http://www.sciencedirect.com/science/journal/01674048/52|Decentralized detection of network attacks through P2P data clustering of SNMP data|The goal of Network Intrusion Detection Systems (NIDSs) is to protect against attacks by inspecting network traffic packets, for instance, looking for anomalies and signatures of known attacks. This paper illustrates an approach to attack detection that analyzes just the standard statistics automatically generated by the Simple Network Management Protocol (SNMP) through unsupervised distributed data mining algorithms. We describe the design of a decentralized system composed of a peer-to-peer network of monitoring stations: each of them continuously gathers SNMP statistical observations about the network traffic and runs a distributed data clustering algorithm in cooperation with other stations. This progressively leads to the construction of a traffic model capable to detect undergoing attacks on later observations, including potentially previously unknown attacks. To estimate the accuracy of the described system, we performed an extensive number of distributed data clustering processing on data sets of SNMP observations generated from real traffic. 
52|-||Gaithashing: A two-factor authentication scheme based on gait features|Recently, gait recognition has attracted much attention as a biometric feature for real-time person authentication. The main advantage of gait is that it can be observed at a distance in an unobtrusive manner. However, the security of an authentication system, based only on gait features, can be easily broken. A malicious actor can observe the gait of an unsuspicious person and extract the related biometric template in a trivial manner and without being noticed. Another major issue of gait as an identifier has to do with their high intra-variance, since human silhouettes can be significantly modified, when for example the user holds a bag or wears a coat. This paper proposes gaithashing, a two-factor authentication that interpolates between the security features of biohash and the recognition capabilities of gait features to provide a high accuracy and secure authentication system. A novel characteristic of gaithashing is that it enrolls three different human silhouettes types. During authentication, the new extracted gait features and the enrollment ones are fused using weighted sums. By selecting appropriate weight values, the proposed scheme eliminates the noise and distortions caused by different silhouette types and achieves to authenticate a user independently of his/her silhouette. Apart from high accuracy, the proposed scheme provides revocability in case of a biometric template compromise. The performance of the proposed scheme is evaluated by carrying out a comprehensive set of experiments. Numerical results show that gaithashing outperforms existing solutions in terms of authentication performance, while at the same time achieves to secure the gait features. 
52|-||Hypervisor-based malware protection with AccessMiner|In this paper we discuss the design and implementation of AccessMiner, a system-centric behavioral malware detector. Our system is designed to model the general interactions between benign programs and the underlying operating system (OS). In this way, AccessMiner is able to capture which, and how, OS resources are used by normal applications and detect anomalous behavior in real-time.The advantage of our approach is that it does not require to be trained on malicious samples, and therefore it is able to provide a general detection solution that can be used to protect against both known and unknown malware. To make the system more resilient against tampering from sophisticated attackers, AccessMiner is implemented as a custom hypervisor that sits below the operating system. In this paper we discuss the implementation details and the technical solutions we adopted to optimize the performances and reduce the impact of the system.Our experiments show that in a stable environment AccessMiner can provide a high level of protection (around 90% detection rate with zero false positives) with an acceptable overhead – similar to the one that can be experienced in a state of the art virtual machine environment. 
52|-||Dytaint: The implementation of a novel lightweight 3-state dynamic taint analysis framework for x86 binary programs|By analyzing information flow at runtime, dynamic taint analysis can precisely detect a wide range of vulnerabilities of software. However, it suffers from substantial runtime overhead and is incapable of discovering potential threats. Yet, realistically, the interested analyst doesn't have access to the source code of the malware. Therefore, the task of software flaw tracking becomes rather complicated. In order to cope with these issues, this paper proposes Dytaint, a novel lightweight 3-state dynamic taint analysis framework, for diagnosing more software vulnerabilities with lower runtime overhead. The framework works for the x86 binary executables and requires no special hardware assistance. Besides the tainted and the untainted states that are discussed by many popularly used taint analysis tools, the third state, controlled-taint state, is proposed to detect more types of software vulnerabilities. The new Chaining Hash Table which reduces the space for storing taint information without increasing the accessing time is also incorporated in the framework. Furthermore, two mechanisms, namely, the irrelevant API filtering based on the function recognition method and basic block handling, are introduced to optimize the runtime performance of our framework. The testing results by running SPEC CINT2006 benchmarks and various popular software have demonstrated that Dytaint is efficient which incurs only 3.1 times overhead to the native on average and practical which is able to discover not only all the real threats but also most of the potential ones. 
52|-||Digital forensic readiness: Expert perspectives on a theoretical framework|Modern organizations need to develop ‘digital forensic readiness’ to comply with their legal, contractual, regulatory, security and operational obligations. A review of academic and practitioner literature revealed a lack of comprehensive and coherent guidance on how forensic readiness can be achieved. This is compounded by the lack of maturity in the discourse of digital forensics rooted in the informal definitions of key terms and concepts. In this paper we validate and refine a digital forensic readiness framework through a series of expert focus groups. Drawing on the deliberations of experts in the focus groups, we discuss the critical issues facing practitioners in achieving digital forensic readiness. 
52|-||Security Busters: Web browser security vs. rogue sites|URL blacklists are used by the majority of modern web browsers as a means to protect users from rogue web sites, i.e. those serving malware and/or hosting phishing scams. There is a plethora of URL blacklists/reputation services, out of which Google's Safe Browsing and Microsoft's SmartScreen stand out as the two most commonly used ones. Frequently, such lists are the only safeguard web browsers implement against such threats. In this paper, we examine the level of protection that is offered by popular web browsers on iOS, Android and desktop (Windows) platforms, against a large set of phishing and malicious URL. The results reveal that most browsers – especially those for mobile devices – offer limited protection against such threats. As a result, we propose and evaluate a countermeasure, which can be used to significantly improve the level of protection offered to the users, regardless of the web browser or platform they are using. 
52|-||Toward protecting control flow confidentiality in cloud-based computation|Cloud based computation services have grown in popularity in recent years. Cloud users can deploy an arbitrary computation cluster to public clouds and execute their programs on that remote cluster to reduce infrastructure investment and maintenance costs. However, how to leverage cloud resources while keeping the computation confidential is a new challenge to be explored. In this paper, we propose runtime control flow obfuscation (RCFO) to protect the control flow confidentiality of outsourced programs. RCFO transforms an outsourced program into two parts: the public program running on the untrusted public cloud and the private program running on the trusted private cloud. By hiding parts of the control flow information in the private program and inserting fake branch statements into the public program, RCFO raises the bar for static and dynamic analysis-based reverse engineering attacks. Based on RCFO, we implement a system called MRDisguiser to protect cloud-based MapReduce services. We perform experiments on a real MapReduce service, Amazon Elastic MapReduce. The experimental results indicate that MRDisguiser is compatible with current cloud-based MapReduce services, and incurs moderate performance overhead. Specifically, when the obfuscation degree increases from 0 to 1.0, the average performance overhead is between 14.9% and 33.2%. 
52|-||Analyzing the role of cognitive and cultural biases in the internalization of information security policies: Recommendations for information security awareness programs|Standards and best practices for information security awareness programs focus on the content and processes of the programs, without taking into consideration how individuals internalize security-related information and how individuals make security related decisions. Relevant literature, however has identified that individual perceptions, beliefs, and biases significantly influence security policy compliance behavior. Security awareness programs need, therefore, to be aligned with the factors affecting the internalization of the communicated security objectives. This paper explores the role of cognitive and cultural biases in shaping information security perceptions and behaviors. We draw upon related literature from contiguous disciplines (namely behavioral economics and health and safety research) to develop a conceptual framework and analyze the role of cognitive and cultural biases in information security behavior. We discuss the implications of biases for security awareness programs and provide a set of recommendations for planning and implementing awareness programs, and for designing the related material. This paper opens new avenues for information security awareness research with regard to security decision making and proposes practical recommendations for planning and delivering security awareness programs, so as to exploit and alleviate the effect of cognitive and cultural biases on shaping risk perceptions and security behavior. 
52|-||Effect of network infrastructure factors on information system risk judgments|Little is known about how perceived network topology factors, which are common components of information system risk metrics, impact human judgments of risk. Using a half-fractional factorial design, this study experimentally manipulated five perceivable network topology factors (network partitioning, network diversity, wireless status, network footprint and connectivity) to assess the relationship between these factors and network risk judgments. The consistency of network risk ratings and rankings were evaluated for each of the 16 network topologies across a sample of 55 network security professionals who reviewed these topologies. Three robust significant main effects (network partitioning, wireless status, and connectivity) and one significant interaction (network partitioning X wireless status) were found. While some topologies were consistently rated and ranked as significantly more risky than others, there was some variability in ratings at each main effect level as well as the spread of the mean ratings between the two main effect levels (e.g., wireless and wired). We discuss the implications of our findings with respect to network risk metric rigor. 
53|-|http://www.sciencedirect.com/science/journal/01674048/53|A taxonomy for privacy enhancing technologies|Privacy-enhancing technologies (PETs) belong to a class of technical measures which aim at preserving the privacy of individuals or groups of individuals. Numerous PETs have been proposed for all kinds of purposes, but are difficult to be compared with each other. The challenge here lies in the fact that information privacy is a comprehensive concept with solutions being diverse, with different focus and aims. As existing taxonomies cover information security-related aspects, while neglecting privacy-specific properties, this work aims at filling this gap by describing a universal taxonomy of PETs where the taxonomy aspects are selected such that they allow the categorization of PETs in different dimensions and properties to cover a wide area of privacy (e.g., user privacy, data privacy). It provides the reader with a tool for the systematic comparison of different PETs. This helps in identifying limitations of existing PETs, complementary technologies, and potential research directions. To demonstrate its applicability, the proposed taxonomy is applied to a set of key technologies covering different disciplines such as data anonymization, privacy-preserving data querying, communication protection, and identity hiding. 
53|-||An expert-based investigation of the Common Vulnerability Scoring System|The Common Vulnerability Scoring System (CVSS) is the most widely used standard for quantifying the severity of security vulnerabilities. For instance, all vulnerabilities in the US National Vulnerability Database are scored according to this system. Unfortunately, it is largely unexplored whether or not its scores are accurate. This paper studies this property through a survey with opinions by 384 experts, covering more than 3000 vulnerabilities. The results show that the mean disagreement between the judgments of the experts and the CVSS Base Score is −0.38, with a variance of 4.46 (on a scale from 0 to 10). The direction of this difference depends on the type of vulnerability that is concerned. The experts then suggest a number of possible revisions to the CVSS that could explain this difference. 
53|-||When Mice devour the Elephants: A DDoS attack against size-based scheduling schemes in the internet|Size-based scheduling (SBS) has been shown to offer significant performance improvement in Web servers and routers. However, most of the performance benefits offered by SBS rely on the premise that the scheduler will interact with a “well behaved” heavy tailed job size distribution. In this paper we design an attack that degrades the performance of an SBS scheduler by subjecting it to a job size distribution which violates the core traffic properties from which SBS derives its strengths. Through theoretical work and a wide range of experiments, we demonstrate the lethality of the attack against routers that use SBS. Additionally, we cite evidence that indicates why the tools and practical manoeuvres required to carry out the attack on a live network are within the reach of adversaries. As flavors of SBS begin to grace the Internet, the paper provides a timely cautionary note on the challenges that SBS could face if widely deployed without specialized defense mechanisms. 
53|-||On statistical distance based testing of pseudo random sequences and experiments with PHP and Debian OpenSSL|NIST SP800-22 (2010) proposed the state of the art statistical testing techniques for testing the quality of (pseudo) random generators. However, it is easy to construct natural functions that are considered as GOOD pseudorandom generators by the NIST SP800-22 test suite though the output of these functions is easily distinguishable from the uniform distribution. This paper proposes solutions to address this challenge by using statistical distance based testing techniques. We carried out both NIST tests and LIL based tests on commonly deployed pseudorandom generators such as the standard C linear congruential generator, Mersenne Twister pseudorandom generator, and Debian Linux (CVE-2008-0166) pseudorandom generator with OpenSSL 0.9.8c-1. Based on experimental results, we illustrate the advantages of our LIL based testing over NIST testing. It is known that Debian Linux (CVE-2008-0166) pseudorandom generator based on OpenSSL 0.9.8c-1 is flawed and the output sequences are predictable. Our LIL tests on these sequences discovered the flaws in Debian Linux implementation. However, NIST SP800-22 test suite is not able to detect this flaw using the NIST recommended parameters. It is concluded that NIST SP800-22 test suite is not sufficient and distance based LIL test techniques be included in statistical testing practice. It is also recommended that all pseudorandom generator implementations be comprehensively tested using state-of-the-art statistically robust testing tools. 
53|-||Information security conscious care behaviour formation in organizations|Today, the Internet can be considered to be a basic commodity, similar to electricity, without which many businesses simply cannot operate. However, information security for both private and business aspects is important. Experts believe that technology cannot solely guarantee a secure environment for information. Users' behaviour should be considered as an important factor in this domain. The Internet is a huge network with great potential for information security breaches. Hackers use different methods to change confidentiality, integrity, and the availability of information in line with their benefits, while users intentionally or through negligence are a great threat for information security. Sharing their account information, downloading any software from the Internet, writing passwords on sticky paper, and using social security numbers as a username or password are examples of their mistakes. Users' negligence, ignorance, lack of awareness, mischievous, apathy and resistance are usually the reasons for security breaches. Users' poor information security behaviour is the main problem in this domain and the presented model endeavours to reduce the risk of users' behaviour in this realm. The results of structural equation modelling (SEM) showed that Information Security Awareness, Information Security Organization Policy, Information Security Experience and Involvement, Attitude towards information security, Subjective Norms, Threat Appraisal, and Information Security Self-efficacy have a positive effect on users' behaviour. However, Perceived Behavioural Control does not affect their behaviour significantly. The Protection Motivation Theory and Theory of Planned Behaviour were applied as the backbone of the research model. 
53|-||Security of Software Defined Networks: AÂ survey|Software Defined Networking (SDN) has emerged as a new network architecture for dealing with network dynamics through software-enabled control. While SDN is promoting many new network applications, security has become an important concern. This paper provides an extensive survey on SDN security. We discuss the security threats to SDN according to their effects, i.e., Spoofing, Tampering, Repudiation, Information disclosure, Denial of Service, and Elevation of Privilege. We also review a wide range of SDN security controls, such as firewalls, IDS/IPS, access control, auditing, and policy management. We describe several pathways of how SDN is evolving. 
53|-||Statistical dynamic splay tree filters towards multilevel firewall packet filtering enhancement|Network Firewalls are considered to be one of the most important security components in today's IP network architectures. Performance of firewalls has significant impact on the overall network performance. Firewalls should be able to sustain a very high throughput and ensure network services availability. In this paper, we propose an analytical dynamic multilevel early packet filtering mechanism to enhance firewall performance. The proposed mechanism uses statistical splay tree filters that utilize traffic characteristics to minimize packet filtering time. The statistical splay tree filters are reordered according to the network traffic divergence upon certain threshold qualification (Chi–Square Test). That is, the proposed mechanism is able to decide whether or not there is a need to update the dynamic splay tree filters' order for filtering the next network traffic window and predict the best order pattern. Furthermore, the importance of optimizing packet rejection and acceptance is done through the multilevel packet filtering process; where in each level, unwanted packets are rejected as early as possible. The proposed mechanism can also be considered as a device protection mechanism against denial of service (DoS) attacks targeting the default filtering rule. Early packet acceptance is done using the splay tree data structure which adapts dynamically according to network traffic flows. Consequently, repeated packets will have less memory accesses and therefore reduce the overall packets filtering time as demonstrated in the evaluation section. 
53|-||Information Security Behaviour Profiling Framework (ISBPF) for student mobile phone users|The mobile phone has become a necessity for many students; however it also exposes them to security threats that may result in a loss of information. In developing countries, large numbers of students are at a disadvantage because they have limited access to information relating to information security threats, unlike their counterparts in more developed societies who can readily access this information from sources such as the Internet. In addition, the developmental environment is plagued with challenges like access to the Internet and limited access to computers.The poor security behaviour exhibited by student users of mobile phones, which was confirmed by the findings of this study, is of particular interest in the context of the university in this study as most undergraduate students are offered a computer-related course which covers certain information security-related principles. This study addressed the problem of low information security awareness levels among student mobile phone users with the objective of levering security awareness as a way of stimulating safer security behaviours among mobile phone users. The resulting changes in awareness were then juxtaposed against behavioural intent changes in the proposed Information Security Behaviour Profiling Framework. Action research was selected as the appropriate methodology for this study, which saw mobile phone users exposed to various awareness campaigns with their behaviour observed before and after these interventions. This paper proposes a framework which can be used to forecast the information security behaviour profiles of student mobile phone users.The findings of this study uncover a possible link between information security awareness and information security behavioural intention, which translates into the security behaviour profiles exhibited by mobile phone users. By using the proposed framework to examine the association between information security awareness and mobile phone user security behavioural intent, this paper proposes a new method for tracking and categorising student mobile phone user security behavioural profiles. While the level of student information security awareness and behavioural intent were measured, no comparison was made between the awareness interventions implemented as part of this action research study. 
53|-||Input extraction via motion-sensor behavior analysis on smartphones|Smartphone onboard sensors, such as the accelerometer and gyroscope, have greatly facilitated people's life, but these sensors may bring potential security and privacy risk. This paper presents an empirical study of analyzing the characteristics of accelerometer and magnetometer data to infer users' input on Android smartphones. The rationale behind is that the touch input actions in different positions would cause different levels of posture and motion change of the smartphone. In this work, an Android application was run as a background process to monitor data of motion sensors. Accelerometer data were analyzed to detect the occurrence of input actions on touchscreen. Then the magnetometer data were fused with accelerometer data for inferring the positions of user inputs on touchscreen. Through the mapping relationship from input positions and common layouts of keyboard or number pad, one can easily obtain the inputs. Analyses were conducted using data from three types of smartphones and across various operational scenarios. The results indicated that users' inputs can be accurately inferred from the sensor data, with the accuracies of 100% for input-action detection and 80% for input inference in some cases. Additional experiments on the effect of smartphone screen size, sampling rate, and training data size were provided to further examine the reliability and practicability of our approach. These findings suggest that readings from accelerometer and magnetometer data could be a powerful side channel for inferring user inputs. 
