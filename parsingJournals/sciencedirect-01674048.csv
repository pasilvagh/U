volume|issue|url|title|abstract
31|1|http://www.sciencedirect.com/science/journal/01674048/31/1|Contents|
31|1||Editorial|
31|1||Android forensics|
31|1||A trust negotiation based security framework for service provisioning in load-balancing clusters|The OKKAM project aims at enabling the Web of Entities, a global digital space for publishing and managing information about entities. The project provides a scalable and sustainable infrastructure, called the Entity Name System (ENS), for the systematic reuse of global and unique entity identifiers. The ENS provides a collection of core services supporting entity identifiers pervasive reuse. The ENS is required to be reliable data intensive load-balancing cluster system for service provisioning.
31|1||On scrambling the BurrowsâWheeler transform to provide privacy in lossless compression|The usual way of ensuring the confidentiality of the compressed data is to encrypt it with a standard encryption algorithm. Although the computational cost of encryption is practically tolerable in most cases, the lack of flexibility to perform pattern matching on the compressed data due to the encryption level is the main disadvantage. Another alternative to provide privacy in compression is to alter the compression algorithms in such a way that the decompression requires the knowledge of some secret parameters. Securing the arithmetic and Huffman coders along with the dictionary based schemes have been previously studied, where Burrows–Wheeler transform (BWT) has not been addressed before in that sense. On BWT of an input data it is not possible to perform a successful search nor construct any part of it without the proper knowledge of the lexicographical ordering used in the construction. Based upon this observation, this study investigates methods to provide privacy in BWT by using a randomly selected permutation of the input symbols as the lexicographical order. The proposed technique aims to support pattern matching on compressed data, while still retaining the confidentiality. Unifying compression and security in a single step is also considered instead of the two-level compress-then-encrypt paradigm.
31|1||A robust hashing algorithm based on SURF for video copy detection|To protect digital video from unauthorized use, video copy detection is an active research topic in the field of copyright control. For content-based copy detection, the key issue is to extract robust transformation-invariant feature. In this paper, a robust hashing algorithm based on speeded up robust feature (SURF) and ordinal measure (OM) is proposed for video copy detection. Since SURF is an invariant feature based on scale space theory, the local feature is extracted by SURF in a frame-by-frame manner. Every frame is divided into 4 × 4 blocks, and every block is traversed by Hilbert-order rasterization to count the number of SURF points. The Hash value is built by the difference of SURF points between adjacent blocks in Hilbert curve. Moreover, two special copy attacks, i.e., picture-in-picture and video flipping, are specifically discussed. Experimental results show the effectiveness of the proposed approach in both accuracy and efficiency.
31|1||A new robust adjustable logo watermarking scheme|In this paper, a novel, yet simple, watermarking algorithm for image authentication is proposed using fractional wavelet packet transform (FRWPT) via singular value decomposition (SVD). Unlike the traditional watermarking schemes where the watermark is added to the transform coefficients, the proposed algorithm is based on embedding in the singular values (luminance) of the host image. To improve the fidelity, the perceptual quality of the watermarked image and to enhance the security of watermarking, we model an adjustable watermarking algorithm. The meaning of the word adjustable is that the watermark is embedded into the host image by taking two watermark embedding strengths, according to owner and some cryptographic conditions. Finally, a reliable watermark extraction algorithm is developed for the extraction of watermark from the distorted image. The feasibility of this method and its robustness against different kind of attacks are verified by computer simulations and comparison with the existing work.
31|1||Tracing and revoking scheme for dynamic privileges against pirate rebroadcast|Broadcast encryption provides a convenient method to distribute digital content to subscribers over an insecure broadcast channel so that only the qualified users can recover the data. Currently, there are only two broadcast encryption schemes designed for users with different privileges. In these schemes, users with higher privileges can decrypt more contents than those with low privileges, which is quite suitable for applications with different service levels. However, both schemes do not provide traitor tracing strategy. In this paper, we present a traitor tracing and revoking system for different privileges against pirate rebroadcast. We improve the tree structure and the media key block proposed by Jin and Lotspiech to suitable for applications with dynamic services, and then combine them with dynamic traitor tracing and revoking method by Kiayias and Pehlivanoglu. Dynamic services mean the users can change their privileges dynamically and the broadcast center can upgrade to hold more/less privileges when needed for marketing, while in both previous schemes the numbers of privileges are determined when the systems are setup and users’ privileges are static. Our scheme uses subset cover method to trace and revoke users so that it can trace unlimited numbers of traitors and revoke unlimited numbers of users.
31|1||Feature extraction and classification algorithm for detecting complex covert timing channel|Owing to the high variance of legitimate traffic, the detection of Covert Timing Channel (CTC) has become a challenging work. The combination of detection methods based on entropy and corrected conditional entropy has been proved an effective way for the detection against some typical CTCs. However, the methods cannot satisfy the detection of some complex CTCs. In this paper, based on wavelet transform and Support Vector Machine (SVM), a new approach is proposed to detect various kinds of CTCs inclusive of some complex CTCs. Our approach can extract the features of maximum entropies at different wavelet levels and the percentage of energy corresponding to the details at wavelet level 1, and then the features are put into multiclass SVM for classification. Moreover, also our approach is capable of detecting the CTC which has the ability to evade the entropy-based detection method. Finally, a sliding window scheme is successfully designed to detect the complex traffic which several kinds of CTCs are embedded in.
31|1||Understanding information systems security policy compliance: An integration of the theory of planned behavior and the protection motivation theory|This research investigated information systems security policy (ISSP) compliance by drawing upon two relevant theories i.e. the theory of planned behavior (TPB) and the protection motivation theory (PMT). A research model that fused constituents of the aforementioned theories was proposed and validated. Relevant hypotheses were developed to test the research conceptualization. Data analysis was performed using the partial least squares (PLS) technique. Using a survey of 124 business managers and IS professionals, this study showed that factors such as self-efficacy, attitude toward compliance, subjective norms, response efficacy and perceived vulnerability positively influence ISSP behavioral compliance intentions of employees. The data analysis did not support perceived severity and response cost as being predictors of ISSP behavioral compliance intentions. The study’s implications for research and practice are discussed.
31|1||Building safe PaaS clouds: A survey on security in multitenant software platforms|This paper surveys the risks brought by multitenancy in software platforms, along with the most prominent solutions proposed to address them. A multitenant platform hosts and executes software from several users (tenants). The platform must ensure that no malicious or faulty code from any tenant can interfere with the normal execution of other users’ code or with the platform itself. This security requirement is specially relevant in Platform-as-a-Service (PaaS) clouds. PaaS clouds offer an execution environment based on some software platform. Unless PaaS systems are deemed as safe environments users will be reluctant to trust them to run any relevant application. This requires to take into account how multitenancy is handled by the software platform used as the basis of the PaaS offer. This survey focuses on two technologies that are or will be the platform-of-choice in many PaaS clouds: Java and .NET. We describe the security mechanisms they provide, study their limitations as multitenant platforms and analyze the research works that try to solve those limitations. We include in this analysis some standard container technologies (such as Enterprise Java Beans) that can be used to standardize the hosting environment of PaaS clouds. Also we include a brief discussion of Operating Systems (OSs) traditional security capacities and why OSs are unlikely to be chosen as the basis of PaaS offers. Finally, we describe some research initiatives that reinforce security by monitoring the execution of untrusted code, whose results can be of interest in multitenant systems.
31|1||Robustness of keystroke-dynamics based biometrics against synthetic forgeries|Biometric systems including keystroke-dynamics based authentication have been well studied in the literature. The attack model in biometrics typically considers impersonation attempts launched by human imposters. However, this attack model is not adequate, as advanced attackers may utilize programs to forge data. In this paper, we consider the effects of synthetic forgery attacks in the context of biometric authentication systems. Our study is performed in a concrete keystroke-dynamic authentication system.
31|1||RIPsec â Using reputation-based multilayer security to protect MANETs|This paper examines the theory, application, and results for a Reputation-Based Internet Protocol Security (RIPsec) framework that provides security for a Mobile Ad-hoc Network (MANET) operating in a hostile environment. While there has been significant research in MANET security, the research has tended to address subsets of the overall security challenge. RIPsec leverages existing technologies to provide an overarching layered security framework that provides a more comprehensive security solution than existing approaches. Protection from external threats is provided in the form of encrypted links and encryption-wrapped nodes while internal threats are mitigated by behavior grading that assigns reputations to nodes based on their demonstrated participation in the routing process. End-to-end message security using public and private certificates protects against both internal and external threats. Network availability is improved by behavior grading and round-robin multipath routing.
31|1||Malware target recognition via static heuristics|Organizations increasingly rely on the confidentiality, integrity and availability of their information and communications technologies to conduct effective business operations while maintaining their competitive edge. Exploitation of these networks via the introduction of undetected malware ultimately degrades their competitive edge, while taking advantage of limited network visibility and the high cost of analyzing massive numbers of programs. This article introduces the novel Malware Target Recognition (MaTR) system which combines the decision tree machine learning algorithm with static heuristic features for malware detection. By focusing on contextually important static heuristic features, this research demonstrates superior detection results. Experimental results on large sample datasets demonstrate near ideal malware detection performance (99.9+% accuracy) with low false positive (8.73e-4) and false negative rates (8.03e-4) at the same point on the performance curve. Test results against a set of publicly unknown malware, including potential advanced competitor tools, show MaTR’s superior detection rate (99%) versus the union of detections from three commercial antivirus products (60%). The resulting model is a fine granularity sensor with potential to dramatically augment cyberspace situation awareness.
31|2|http://www.sciencedirect.com/science/journal/01674048/31/2|Contents|
31|2||Editorial|
31|2||Security evaluation of biometric keys|Biometric cryptosystems combine biometrics with cryptography by producing Biometric Cryptographic Keys (BCKs) to provide stronger security mechanisms while protecting against identity theft. The process of generating/binding biometric keys consists of a number of steps starting with a feature extraction procedure, the complexity of which depends on the specific biometric trait/scheme, followed often by user selected transformation to allow for revocability, and an error correction scheme to tolerate reasonable amount of intra-class variation. Each of these steps has its own effect on the security of the generated/bound key. Proper security evaluation must include thorough analysis of the security effect of each of these steps. We propose a comprehensive approach to BCK’s security evaluation that takes into consideration each of the steps involved in their construction. We first review existing BCKs and highlight that the analysis of their security is either insufficient or not provided. In addition to evaluating the correctness (i.e. error rates), and the generated/bound key size, we evaluate the randomness of biometric features employed in the process of key generation. Our proposal combines the Kullback–Leibler divergence and the discrimination entropy to formulate a new measure of the Entropy of Biometric Features (EBF), defined as the average number of bits that distinguishes a user from a given population. Then we rigorously evaluate the impact of using error correcting scheme on the security of BCKs to calculate the Effective Entropy of Biometric Features (EEBF). Finally, inherent individual differences of the EBFs will be discussed. Here, we focus on face-based BCKs, but this does not restrict the use of the proposed evaluation. This paper argues that current face-based BCKs are not secure enough for high level security applications, and demonstrates that the average EEBF of BCKs using PCA-based facial features is less than 20-bit even when applying a user-based randomization on biometric features.
31|2||Performance of automated network vulnerability scanning at remediating security issues|This paper evaluates how large portion of an enterprises network security holes that would be remediated if one would follow the remediation guidelines provided by seven automated network vulnerability scanners. Remediation performance was assessed for both authenticated and unauthenticated scans. The overall findings suggest that a vulnerability scanner is a usable security assessment tool, given that credentials are available for the systems in the network. However, there are issues with the method: manual effort is needed to reach complete accuracy and the remediation guidelines are oftentimes very cumbersome to study. Results also show that a scanner more accurate in terms of remediating vulnerabilities generally also is better at detecting vulnerabilities, but is in turn also more prone to false alarms. This is independent of whether the scanner is provided system credentials or not.
31|2||Toward a general defense against kernel queue hooking attacks|Kernel queue hooking (KQH) attacks achieve stealthy malicious function execution by embedding malicious hooks in dynamic kernel schedulable queues (K-Queues). Because they keep kernel code and persistent hooks intact, they can evade detection of state-of-the-art kernel integrity monitors. Moreover, they have been used by advanced malware such as the Rustock spam bot to achieve malicious goals. In this paper, we present a systematic defense against such novel attacks. We propose the Precise Lookahead Checking of function Pointers approach that checks the legitimacy of pending K-Queue callback requests by proactively checking function pointers that may be invoked by the callback function. To facilitate the derivation of specifications for any K-Queue, we build a unified static analysis framework and a toolset that can derive from kernel source code properties of legitimate K-Queue requests and turn them into source code for the runtime checker. We implement proof-of-concept runtime checkers for four K-Queues in Linux and perform a comprehensive experimental evaluation of these checkers, which shows that our defense is effective against KQH attacks.
31|2||Multiple-File Remote Data Checking for cloud storage|Remote Data Checking (RDC) adds data possession or retrievability guarantee to cloud storage without downloading the whole data. The support for dynamic data updates is vital for the practical application of RDC. We define Multiple-File Remote Data Checking (MF-RDC), an RDC model suitable for the specific data update model of cloud storage. MF-RDC checks the intactness of a dynamic file group consisting of a growing number of static files. By checking a group of files aggregately, the overhead of the scheme can be significantly reduced. We propose constructions of two MF-RDC schemes: MF-PDP and MF-POR. An efficient and secure MF-PDP scheme that provides data possession guarantee is constructed from a single-file PDP scheme by combining homomorphic authenticators with virtual block indices. The scheme is amended to integrate with file encoding using adversarial error-correcting codes, producing the MF-POR scheme that provides data retrievability guarantee. We conduct rigorous security analysis of the schemes and perform experimental evaluation on our implementation. With an efficient implementation, the communication and computation overhead of the schemes is reduced from linear in the size of the data to near constant. The performance of the schemes is bounded by disk I/O rather than cryptographic computation.
31|2||HIPAA Privacy Rule compliance: An interpretive study using Normanâs action theory|Using Reason’s GEMS typology to analyze publicly available reports of privacy breaches in the United States shows human error as the cause of a significant number of violations of HIPAA Privacy Rule. An interpretive study based on interviews of 15 privacy officers of major U.S. healthcare organizations reinforces this finding. Applying the Rating Scale Model to analyze these officers’ ranking of the underlying causes of human error suggests that such organizational factors as high workload and low morale impede HIPAA Privacy Rule compliance more than either poor skills or availability of technology resources. Contrary to the common belief that human error may be attributed primarily to an individual, the results suggest that the work environment is critical and that systemic limitations underlie errors made by employees. By applying a cognitive taxonomy of human errors based on Norman’s action theory, this paper gives healthcare organizations a framework for managing compliance with HIPAA Privacy Rule and operational strategies that help enforce this compliance, especially among the clinical staff.
31|2||Unrealistic optimism on information security management|Information security is a critical issue that many firms face these days. While increasing incidents of information security breaches have generated extensive publicity, previous studies repeatedly expose low levels of managerial awareness and commitment, a key obstacle to achieving a good information security posture. The main motivation of our study emanates from this phenomenon that the increased vulnerability to information security breaches is coupled with the low level of managerial awareness and commitment regarding information security threats. We report this dissonance by addressing a cognitive bias called optimistic bias. Using a survey, we study if MIS executives are subject to such a bias in their vulnerability perceptions of information security. We find that they demonstrate optimistic bias in risk perception on information security domain. The extent of this optimistic bias is greater with a distant comparison target with fewer information sharing activities. This optimistic bias is also found to be related to perception of controllability with information security threats. In order to overcome the effects of optimistic bias, firms need more security awareness training and systematic treatments of security threats instead of relying on ad hoc approach to security measure implementation.
31|2||A Hot Query Bank approach to improve detection performance against SQL injection attacks|SQL injection attacks (SQLIAs) exploit web sites by altering backend SQL statements through manipulating application input. With the growing popularity of web applications, such attacks have become a serious security threat to users and systems as well. Existing dynamic SQLIA detectors provide high detection accuracy yet may have ignored another focus: efficiency. Our research has found that inside most systems exist many hot queries that current SQLIA detectors have repeatedly verified. Such repetition causes unnecessary waste of system resources.
31|2||Security Risk Management: Building an Information Security Risk Management Program from the Ground Up|
31|2||Cyber Attacks|
31|2||Coding for Penetration Testers|
31|2||Formal security policy implementations in network firewalls|Network security should be based around formal security policies. From high-level natural language, non-technical, policies created by management, down to device and vendor specific policies, or configurations, written by network system administrators. There exists a multitude of research into policy-based network systems which has been undertaken. This paper provides an overview of the different type of policies relating to security in networks, and a taxonomy of the research into systems which have been proposed to support the network administrators in difficult tasks of creating, managing and deploying these policies.
31|3|http://www.sciencedirect.com/science/journal/01674048/31/3|Contents|
31|3||Editorial|
31|3||Operational experiences with anomaly detection in backbone networks|Although network security is a crucial aspect for network operators, there are still very few works that have examined the anomalies present in large backbone networks and evaluated the performance of existing anomaly detection solutions in operational environments. The objective of this work is to fill this gap by reporting hands-on experience in the evaluation and deployment of an anomaly detection solution for the GÉANT backbone network. During this process, we analyzed three different commercial tools for anomaly detection and then deployed one of them for several months in the 18 points-of-presence of GÉANT. We first explain the general requirements that an anomaly detection system should satisfy from the point of view of a network operator. Afterwards, we describe the evaluation of the tools and present a study of the anomalies found in a continental backbone network after operationally using the finally deployed tool for half a year. We think that this first hand information can be of great interest to both professionals and researchers working on network security and can also guide future research towards more practical problems faced by network operators.
31|3||Polite sender: A resource-saving spam email countermeasure based on sender responsibilities and recipient justifications|Currently, most of the existing spam countermeasures are deployed on the email recipient side. However, they cannot diminish the amount of wasteful traffic sent from the SMTP server and the wasteful data storage in the receiver's inbox incurred by spam emails. This paper presents an alternative approach on the sender side in order to overcome these problems and create a bandwidth-saving reduced-storage email system. Additional functions are added to the SMTP server on the sender side to examine whether should allow the particular email sender. If a proper authorization from the recipient has not been granted, the sending SMTP server will not forward the full email message. Instead, it sends the email header together with some additional inquiries for the recipient to authorize this particular sender. Once the authorization is granted, each pair of a given sender and receiver will be kept in a whitelist at the sending SMTP server. The proposed approach can be easily deployed without modifying the existing SMTP protocol stack. The experiment results based on a prototype and data analysis from real email servers demonstrate that the proposed scheme could drastically reduce the amount of wasteful traffic and storage associated with the annoying spam messages.
31|3||A methodology for integrating access control policies within database development|Security in general and database protection from unauthorized access in particular, are crucial for organizations. While functional requirements are defined in the early stages of the development process, non-functional requirements such as security tend to be neglected or dealt with only at the end of the development process. Various efforts have been made to address this problem; however, none of them provide a complete framework to guide, enforce and verify the correct design of security policies, and eventually generate code from that design.
31|3||Applying security policies and service level agreement to IaaS service model to enhance security and transition|Over a decade ago, cloud computing became an important topic for small, medium and large businesses alike. The new concept promises scalability, security, cost reduction, portability and availability. While addressing the cloud concepts over the past several years, there have been intensive discussions about the importance of the different cloud computing service model. Moreover, there were lots of discussions about the risks in migrating to cloud computing. Therefore, this paper reviews the concept of cloud computing, security policies and concentrates on Infrastructure as a Service (IaaS) model. Also, the paper examines the risks encountered by implementing the Infrastructure as a Service (IaaS) model in organizations. Furthermore, the paper’s aim is to discuss the role of security policies, service level agreement (SLA) and compliance for enhancing the security of the IaaS service model by presenting several applicable policies.
31|3||Evaluating a migration-based response to DoS attacks in a system of distributed auctions|Service migration is a possible approach for a class of Internet services to deal with the increasing frequency of denial-of-service (DoS) attacks. The basic idea is to entitle services to physically relocate to a different host after detecting an attack or as a preventive action. We examine the implications of this approach within the context of an automated and high-frequency English auction system, which can be particularly sensitive to degrading communications performance. The impact of the attack and the migration response are investigated in terms of the diminishing utility attainable by auctioneers, giving insight into the advantages and disadvantages of the migration approach as DoS defense.
31|3||Have things changed now? An empirical study on input validation vulnerabilities in web applications|Web applications have become important services in our daily lives. Millions of users use web applications to obtain information, perform financial transactions, have fun, socialize, and communicate. Unfortunately, web applications are also frequently targeted by attackers. Recent data from SANS institute estimates that up to 60% of Internet attacks target web applications.
31|3||Toward developing a systematic approach to generate benchmark datasets for intrusion detection|In network intrusion detection, anomaly-based approaches in particular suffer from accurate evaluation, comparison, and deployment which originates from the scarcity of adequate datasets. Many such datasets are internal and cannot be shared due to privacy issues, others are heavily anonymized and do not reflect current trends, or they lack certain statistical characteristics. These deficiencies are primarily the reasons why a perfect dataset is yet to exist. Thus, researchers must resort to datasets that are often suboptimal. As network behaviors and patterns change and intrusions evolve, it has very much become necessary to move away from static and one-time datasets toward more dynamically generated datasets which not only reflect the traffic compositions and intrusions of that time, but are also modifiable, extensible, and reproducible. In this paper, a systematic approach to generate the required datasets is introduced to address this need. The underlying notion is based on the concept of profiles which contain detailed descriptions of intrusions and abstract distribution models for applications, protocols, or lower level network entities. Real traces are analyzed to create profiles for agents that generate real traffic for HTTP, SMTP, SSH, IMAP, POP3, and FTP. In this regard, a set of guidelines is established to outline valid datasets, which set the basis for generating profiles. These guidelines are vital for the effectiveness of the dataset in terms of realism, evaluation capabilities, total capture, completeness, and malicious activity. The profiles are then employed in an experiment to generate the desirable dataset in a testbed environment. Various multi-stage attacks scenarios were subsequently carried out to supply the anomalous portion of the dataset. The intent for this dataset is to assist various researchers in acquiring datasets of this kind for testing, evaluation, and comparison purposes, through sharing the generated datasets and profiles.
31|4|http://www.sciencedirect.com/science/journal/01674048/31/4|Contents|
31|4||Editorial|
31|4||Abstract interpretation-based semantic framework for software birthmark|Software birthmark is a promising technique for detecting software piracy. Currently, many software birthmarks have been proposed, but the evaluations of these birthmarks are mainly done through experiments and there is no theoretical framework, which makes it difficult to formally analyze and certify the effectiveness of software birthmarks. To solve this problem, a semantic framework for software birthmarks is proposed based on abstract interpretation in this paper. First, two models, which characterize the criteria for the copy relation and program transformation attacks respectively, are given by abstract interpretation. Then, based on these two models, the semantic definition of software birthmarks is presented, and the credibility and the resilience of software birthmarks are formally proved in the proposed semantic framework. Furthermore, software birthmarks are compared with respect to their credibilities and resilience in the lattice of abstract interpretation. Finally, the effectiveness of the proposed framework is demonstrated by evaluating and comparing two typical software birthmarks, the static API birthmark and the static n-gram birthmark.
31|4||Performance analysis of Bayesian networks and neural networks in classification of file system activities|Precise comprehension of a file system state at any given time is vital for performing digital forensic analyses. To uncover evidence of the digital crime, the logical representation of file system activities helps reconstruct post-event timeline of the unauthorized or malicious accesses made on a system. This paper describes a comparative performance analysis of the Bayesian networks and neural networks techniques to classify the state of file system activities in terms of execution of applications based on the pattern of manipulation of specific files during certain period of time. In particular, this paper discusses the construction of a Bayesian networks and neural networks from the predetermined knowledge of the manipulation of file system artifacts and their corresponding metadata information by a set of software applications. The variability amongst the execution patterns of various applications indicate that the Bayesian network-based model is a more appropriate tool as compared to neural networks because of its ability to learn and detect patterns even from an incomplete dataset. The focus of this paper is to highlight intrinsic significance of the learning approach of Bayesian network methodology in comparison to the techniques used for supervised learning in ordinary neural networks. The paper also highlights the efficacy of Bayesian network technique to proficiently handle large volumes of datasets.
31|4||MSABMS-based approach of detecting LDoS attack|Low-rate Denial of Service (LDoS) attacks exploit the deficiencies of the minimum RTO of TCP to send out attack packets in short-duration periodic pulses with low average volume traffic in order to throttle TCP throughput. It is hard to detect an LDoS attack by most available detection schemes, which are triggered by high-rate traffic based on time average statistics. In this paper, the method of Multiple Sampling Averaging Based on Missing Sampling (MSABMS) is used to detect LDoS attacks based on the model of small signal for the first time. In the proposed approach, statistics on the packets are taken within 30 s with the sampling interval of 10 ms (3000 sampling points in total), and the statistical results are compared with a threshold for identifying the LDoS attacks. Furthermore, an eigenvalue-estimating matrix is established to estimate the attack period after the detection of LDoS attacks. Simulation results in NS-2 environment show that the proposed approach can be used to detect the LDoS attack effectively.
31|4||SCADA security in the light of Cyber-Warfare|Supervisory Control and Data Acquisition (SCADA) systems are deployed worldwide in many critical infrastructures ranging from power generation, over public transport to industrial manufacturing systems. Whilst contemporary research has identified the need for protecting SCADA systems, these information are disparate and do not provide a coherent view of the threats and the risks resulting from the tendency to integrate these once isolated systems into corporate networks that are prone to cyber attacks. This paper surveys ongoing research and provides a coherent overview of the threats, risks and mitigation strategies in the area of SCADA security.
31|4||Encryption-based multilevel model for DBMS|In this paper, we propose an encryption-based multilevel model for database management systems. The proposed model is a combination of the Multilevel Relational (MLR) model and an encryption system. This encryption system encrypts each data in the tuple with different field-key according to a security class of the data element. Each field is decrypted individually by the field-key of which security class is higher than or equal to that of the encrypted field-key. The proposed model is characterized by three achievements: (1) utilizing an encryption system as an additional security layer over the multilevel security layer for the database, (2) reducing the multilevel database size, and (3) improving the response time of the data retrieval from the multilevel database. Also this paper summarizes our efforts in implementing a working multilevel secure database prototype. This prototype is used as a research tool for studying principles and mechanisms of the encryption-based multilevel model and multilevel secure database (MLS/DBMS) models (SeaView, Jajodia–Sandhu, Smith–Winslett, MLR, and Belief-Consistent Model). This prototype is implemented to be used to perform a series of experiments to measure the performance cost for applying encryption in multilevel database security.
31|4||Dynamic risk-based decision methods for access control systems|In traditional multi-level security systems, trust and risk values are pre-computed. Any change in these values requires manual intervention of an administrator. In many dynamic environments, however, these values should be auto-adaptive, and auto-tunable according to the usage history of the users. Moreover, occasional exceptions on resource needs, which are common in dynamic environments like healthcare, should be allowed if the subjects show a positive record of use toward resources they acquired in the past. Conversely, access of authorized users, who have negative record, should be restricted. These requirements are not taken into consideration in existing risk-based access control systems. In order to overcome these shortcomings and to meet different sensitivity requirements of various applications, we propose two dynamic risk-based decision methods for access control systems. We provide theoretical and simulation-based analysis and evaluation of both schemes. Also, we analytically prove that the proposed methods, not only allow exceptions under certain controlled conditions, but uniquely restrict legitimate access of bad authorized users.
31|4||Systematically breaking and fixing OpenID security: Formal analysis, semi-automated empirical evaluation, and practical countermeasures|OpenID 2.0 is a user-centric Web single sign-on protocol with over one billion OpenID-enabled user accounts, and tens of thousands of supporting websites. While the security of the protocol is clearly critical, so far its security analysis has only been done in a partial and ad-hoc manner. This paper presents the results of a systematic analysis of the protocol using both formal model checking and an empirical evaluation of 132 popular websites that support OpenID. Our formal analysis reveals that the protocol does not guarantee the authenticity and integrity of the authentication request, and it lacks contextual bindings among the protocol messages and the browser. The results of our empirical evaluation suggest that many OpenID-enabled websites are vulnerable to a series of cross-site request forgery attacks (CSRF) that either allow an attacker to stealthily force a victim user to sign into the OpenID supporting website and launch subsequent CSRF attacks (81%), or force a victim to sign in as the attacker in order to spoof the victim's personal information (77%). With additional capabilities (e.g., controlling a wireless access point), the adversary can impersonate the victim on 80% of the evaluated websites, and manipulate the victim's profile attributes by forging the extension parameters on 45% of those sites. Based on the insights from this analysis, we propose and evaluate a simple and scalable mitigation technique for OpenID-enabled websites, and an alternative man-in-the-middle defense mechanism for deployments of OpenID without SSL.
31|4||PIN selection policies: Are they really effective?|Users have conflicting sets of requirements when it comes to choosing Personal Identification Numbers (PINs) for mobile phones or other systems that use PINs for authentication: the conflict lies between the ‘easy to remember’ usability requirement and the ‘hard to guess’ security requirement. Users often ignore the security requirement and choose PINs that are easy to remember and reuse, making it also easy for attackers to guess and compromise them. Just as the password strength is controlled through various password policies, PIN selection policies may be used to help users choose stronger PINs and meet various security requirements. An example policy would not allow the use of the most commonly selected PINs.
31|4||CRiBAC: Community-centric role interaction based access control model|As one of the most efficient solutions to complex and large-scale problems, multi-agent cooperation has been in the limelight for the past few decades. Recently, many research projects have focused on context-aware cooperation to dynamically provide complex services. As cooperation in the multi-agent systems (MASs) becomes more common, guaranteeing the security of such cooperation takes on even greater importance. However, existing security models do not reflect the agents' unique features, including cooperation and context-awareness. In this paper, we propose a Community-based Role interaction-based Access Control model (CRiBAC) to allow secure cooperation in MASs. To do this, we refine and extend our preliminary RiBAC model, which was proposed earlier to support secure interactions among agents, by introducing a new concept of interaction permission, and then extend it to CRiBAC to support community-based cooperation among agents. We analyze potential problems related to interaction permissions and propose two approaches to address them. We also propose an administration model to facilitate administration of CRiBAC policies. Finally, we present the implementation of a prototype system based on a sample scenario to assess the proposed work and show its feasibility.
31|4||FT-FW: A cluster-based fault-tolerant architecture for stateful firewalls|Nowadays, stateful firewalls are part of the critical infrastructure of the Internet. Basically, they help to protect network services and users against attackers by means of access control and protocol conformance checkings. However, stateful firewalls are problematic from the fault-tolerance perspective since they introduce a single point of failure in the network schema. In this work, we summarize and enhance our previous research efforts that aim to provide a full fault-tolerant solution for stateful firewalls. These efforts have focused on the design and the implementation of the cluster-based Fault-Tolerant stateful Firewall (FT-FW) architecture. We provide details on our proposed solution and we extensively evaluate important network performance and availability aspects that we did not cover so far. The evaluation experiments are based on our Free/OpenSource implementation that has become the most popular solution for Linux-based stateful firewalls.1
31|4||domRBAC: An access control model for modern collaborative systems|Modern collaborative systems such as the Grid computing paradigm are capable of providing resource sharing between users and platforms. These collaborations need to be done in a transparent way among the participants of a virtual organization (VO). A VO may consist of hundreds of users and heterogeneous resources. In order to have a successful collaboration, a list of vital importance requirements should be fulfilled, viz. collaboration among domains, to ensure a secure environment during a collaboration, the ability to enforce usage constraints upon resources, and to manage the security policies in an easy and efficient way. In this article, we propose an enhanced role-based access control model entitled domRBAC for collaborative applications, which is based on the ANSI INCITS 359-2004 access control model. The domRBAC is capable of differentiating the security policies that need to be enforced in each domain and to support collaboration under secure inter-operation. Cardinality constraints along with context information are incorporated to provide the ability of applying simple usage management of resources for the first time in a role-based access control model. Furthermore, secure inter-operation is assured among collaborating domains during role assignment automatically and in real-time. Yet, domRBAC, as an RBAC approach, intrinsically inherits all of its virtues such as ease of management, and separation of duty relationships with the latter also being supported in multiple domains. As a proof of concept, we implement a simulator based on the definitions of our proposed access control model and conduct experimental studies to demonstrate the feasibility and performance of our approach.
31|4||Bypassing information leakage protection with trusted applications|Insider threats are an increasing concern for most modern organizations. Information leakage is one of the most important insider threats, particularly according to its potential financial impact. Data Leakage Protection (DLP) systems have been developed to tackle this issue and they constitute the main solution to protect information systems against leaks. They work by tracking sensitive information flows and monitoring executed applications to ensure that sensitive information is not leaving the organization. However, current DLP systems do not fully consider that trusted applications represent a threat to sensitive information confidentiality. In this paper, we demonstrate how to use common trusted applications to evade current DLP systems. Thanks to its wide range, trusted applications such as Microsoft Excel can be transformed into standardized block ciphers. Information can thus be encrypted in such a way that current DLP techniques cannot detect that sensitive information is being leaked. This method could be used by non-skilled malicious insiders and leaves almost no traces. We have successfully tested our method against a well-known DLP solution from a commercial provider (TrendMicro LeakProof). Finally, we also analyze the proposed evasion technique from the malicious insider point of view and discuss some possible countermeasures to mitigate its use to steal information.
31|4||QoP-ML: Quality of protection modelling language for cryptographic protocols|Cryptographic protocols can be realized on different levels of security. One can choose factors which have different impact on the overall system security. Traditionally, protocols have been configured with the strongest possible security mechanisms. Unfortunately, the strongest protection (especially in low resource devices) can lead to the denial of services. In such a situation the quality of protection models which scales the protection level depending on the specific requirements is used.
31|4||Leveraging behavioral science to mitigate cyber security risk|Most efforts to improve cyber security focus primarily on incorporating new technological approaches in products and processes. However, a key element of improvement involves acknowledging the importance of human behavior when designing, building and using cyber security technology. In this survey paper, we describe why incorporating an understanding of human behavior into cyber security products and processes can lead to more effective technology. We present two examples: the first demonstrates how leveraging behavioral science leads to clear improvements, and the other illustrates how behavioral science offers the potential for significant increases in the effectiveness of cyber security. Based on feedback collected from practitioners in preliminary interviews, we narrow our focus to two important behavioral aspects: cognitive load and bias. Next, we identify proven and potential behavioral science findings that have cyber security relevance, not only related to cognitive load and bias but also to heuristics and behavioral science models. We conclude by suggesting several next steps for incorporating behavioral science findings in our technological design, development and use.
31|4||Noncespaces: Using randomization to defeat cross-site scripting attacks|Cross-site scripting (XSS) vulnerabilities are among the most common and serious web application vulnerabilities. It is challenging to eliminate XSS vulnerabilities because it is difficult for web applications to sanitize all user input appropriately. We present Noncespaces, a technique that enables web clients to distinguish between trusted and untrusted content to prevent exploitation of XSS vulnerabilities. Using Noncespaces, a web application randomizes the the (X)HTML tags and attributes in each document before delivering it to the client. As long as the attacker is unable to guess the random mapping, the client can distinguish between trusted content created by the web application and untrusted content provided by an attacker. To implement Noncespaces with minimal changes to web applications, we leverage a popular web application architecture to automatically apply Noncespaces to static content processed through a popular PHP template engine. We design a policy language for Noncespaces, implement a training mode to assist policy development, and conduct extensive security testing of a generated policy for two large web applications to show the effectiveness of our technique.
31|4||Private Cloud Computing: Consolidation, Virtualization, and Service-Oriented Infrastructure|
31|4||Penetration Tester's Open Source Toolkit|
31|4||Securing the Cloud: Cloud Computer Security Techniques and Tactics|
31|4||The Basics of Information Security: Understanding the Fundamentals of InfoSec in Theory and Practice|
31|4||Enterprise Security for the Executive|
31|4||Cyber Warfare â Techniques, Tactics and Tools for Security Practitioners|
31|4||Thor's Microsoft Security Bible|
31|4||XBOX 360 Forensics: A Digital Forensics Guide to Examining Artifacts|
31|4||Filler AD:IFIP|
31|5|http://www.sciencedirect.com/science/journal/01674048/31/5|Contents|
31|5||Editorial|
31|5||Incident response teams â Challenges in supporting the organisational security function|Incident response is a critical security function in organisations that aims to manage incidents in a timely and cost-effective manner. This research was motivated by previous case studies that suggested that the practice of incident response frequently did not result in the improvement of strategic security processes such as policy development and risk assessment. An exploratory in-depth case study was performed at a large global financial institution to examine shortcomings in the practice of incident response. The case study revealed the practice of incident response, in accordance with detailed best-practice guidelines, tended to adopt a narrow technical focus aimed at maintaining business continuity whilst neglecting strategic security concerns. The case study also revealed that the (limited) post-incident review process focused on ‘high-impact’ incidents rather than ‘high-learning’ (i.e. potentially useful incidents from a learning perspective) incidents and ‘near misses’. In response to this case study, we propose a new double-loop model for incident learning to address potential systemic corrective action in such areas as the risk assessment and policy development processes.
31|5||Semantic adaptive microaggregation of categorical microdata|In the context of Statistical Disclosure Control, microaggregation is a privacy-preserving method aimed to mask sensitive microdata prior to publication. It iteratively creates clusters of, at least, k elements, and replaces them by their prototype so that they become k-indistinguishable (anonymous). This data transformation produces a loss of information with regards to the original dataset which affects the utility of masked data, so, the aim of microaggregation algorithms is to find the partition that minimises the information loss while ensuring a certain level of privacy. Most microaggregation methods, such as the MDAV algorithm, which is the focus of this paper, have been designed for numerical data. Extending them to support non-numerical (categorical) attributes is not straightforward because of the limitations on defining appropriate aggregation operators. Concretely, related works focused on the MDAV algorithm propose grouping data into groups with constrained size (or even fixed) and/or incorporate a basic categorical treatment of non-numerical data. This approach affects negatively the utility of the protected dataset because neither the distributional characteristics of data nor their underlying semantics are properly considered. In this paper, we propose a set of modifications to the MDAV algorithm focused on categorical microdata. Our approach has been evaluated and compared with related works when protecting real datasets with textual attribute values. Results show that our method produces masked datasets that better minimises the information loss resulting from the data transformation.
31|5||Taxonomy of compliant information security behavior|This paper aims at surveying the extrinsic and intrinsic motivations that influence the propensity toward compliant information security behavior. Information security behavior refers to a set of core information security activities that have to be adhered to by end-users to maintain information security as defined by information security policies. The intention is to classify the research done on compliant information security behavior from an end-user perspective and arrange it as a taxonomy predicated on Self-Determination Theory (SDT). In addition, the relative significance of factors that contribute to compliant information security behavior is evaluated on the basis of empirical studies. The taxonomy will be valuable in providing a comprehensive overview of the factors that influence compliant information security behavior and in identifying areas that require further research.
31|5||Securing distributed systems using patterns: A survey|Driven by expanding scientific computing and business enterprise needs, the last decade has seen a shift toward software paradigms in which distribution plays a central role. The increasing size, complexity and heterogeneity of the corresponding systems is accompanied by an increase of security vulnerabilities that require mitigation via combined security and software engineering strategies. In this respect security patterns, which build on the success of design patterns and software patterns more generally, are a tool of great value. In this paper we comprehensively survey the state-of-the-art in securing distributed systems using (security) patterns, considering both relevant patterns and methodologies for applying them. In the first part of the survey, we provide detailed reviews of our selected security patterns, classify the patterns using a multi-dimensional scheme and evaluate them according to a set of quality categories. This highlights deficiencies in the reviewed patterns and provides a basis for identifying new or “missing” patterns and pattern classes. The newly identified and surveyed patterns are a step forward in defining a pattern language for distributed computing. In the second part of the survey, we briefly review a number of pattern-based security methodologies and evaluate their maturity and appropriateness for securing distributed systems.
31|5||Steganography for MP3 audio by exploiting the rule ofÂ window switching|MP3 audio is a promising carrier format for covert communication because of its popularization. In this paper, we propose an MP3 steganographic method by exploiting the rule of window switching during encoding. The method carries out embedding by establishing a mapping relationship between the secret bit and the encoding parameter, namely window type. The proposed algorithm is fully compliant with MP3 compression standard and the distortion caused by steganography can be controlled automatically by the distortion adjustment mechanism of the encoder. Experimental results demonstrate that the proposed method introduces insignificant perceptual distortion and is statistically undetectable for the attack of block size analysis.
31|5||A second look at the performance of neural networks for keystroke dynamics using a publicly available dataset|Keystroke Dynamics, which is a biometric characteristic that depends on typing style of users, could be a viable alternative or a complementary technique for user authentication if tolerable error rates are achieved. Most of the earlier studies on Keystroke Dynamics were conducted with irreproducible evaluation conditions therefore comparing their experimental results are difficult, if not impossible. One of the few exceptions is the work done by Killourhy and Maxion, which made a dataset publicly available, developed a repeatable evaluation procedure and evaluated the performance of different methods using the same methodology. In their study, the error rate of neural networks was found to be one of the worst-performing. In this study, we have a second look at the performance of neural networks using the evaluation procedure and dataset same as in Killourhy and Maxion’s work. We find that performance of artificial neural networks can outperform all other methods by using negative examples. We conduct comparative tests of different algorithms for training neural networks and achieve an equal error rate of 7.73% with Levenberg–Marquardt backpropagation network, which is better than equal error rate of the best-performing method in Killourhy and Maxion’s work.
31|5||WAVE-CUSUM: Improving CUSUM performance in network anomaly detection by means of wavelet analysis|The increasing number of network attacks causes growing problems for network operators and users. Thus, detecting anomalous traffic is of primary interest in IP networks management and many detection techniques, able to promptly reveal and identify network attacks, mainly detecting Heavy Changes in the network traffic, have been proposed. Among these, one of the most promising approach is based on the use of the CUSUM (CUmulative SUM). Nonetheless, CUSUM performance is strongly affected by its sensitivity to the presence of seasonal trends in the considered data.
31|5||A programmer's perspective|
31|5||Low Tech Hacking|
31|6|http://www.sciencedirect.com/science/journal/01674048/31/6|Contents|
31|6||Editorial|
31|6||An efficient handover authentication scheme with privacy preservation for IEEE 802.16m network|IEEE 802.16m is now under consideration by the International Telecommunication Union (ITU) to become the International Mobile Telecommunications (IMT)-Advanced standard. However, seamless and secure handover is one of the most challenging issues. Taking as reference our previous work, this paper presents a privacy-preserving fast handover authentication scheme based on pseudonym for IEEE 802.16m network. Since Mobile Station (MS) only provides a pseudonym in the initial authentication phase and changes its pseudonym in each handover authentication phase, this can protect the MS's identity privacy and allow MS to be untraceable. Moreover, each pseudonym is corresponding to a credential ticket generated by the previous service Base Station (BS) using a multi-BS group key, thus MS and the target BS can easily accomplish mutual authentication. In addition, our theoretical analysis and simulation indicate that our scheme outperforms previously reported schemes in terms of computation and communication overhead.
31|6||Impact of HIPAA provisions on the stock market value of healthcare institutions, and information security and other information technology firms|Title 1 of the Health Insurance Portability and Accountability Act (HIPAA) was enacted to improve the portability of healthcare insurance coverage and Title II was intended to alleviate fraud and abuse. The development of a health information system was suggested in Title II of HIPAA as a means of promoting standardization to improve the efficiency of the healthcare system and ensure that electronic healthcare information is transferred securely and kept private. Since the legislation places the onus of providing the described improvements on healthcare institutions and part of these requirements relate to information technology (IT) and information security (IS), the process of complying with the legislation will necessitate acquiring products and services from IT/IS firms. From the viewpoint of stock market analysts, this increase in demand for IT/IS products and services has the potential to boost the profitability of public IT/IS firms, in turn positively enhancing their stock market valuation. Following the same logic, the legislation's compliance burdens shared by healthcare firms are expected to require hefty costs, thus potentially reducing the profitability of healthcare firms and reflecting negatively on their stock price. The intent of this paper is to evaluate the stock market reaction to the introduction of HIPAA legislation by evaluating the abnormal movement in the price of the stock of public healthcare institutions, IT, and IS firms. We conduct event-study analyses around the announcement dates of the various provisions of HIPAA. An event study is a standard statistical methodology used to determine whether the occurrence of a specific event or events results in a statistically significant reaction in financial markets. The advantage of the event study methodology for policy analysis is that it provides an anchor for determining value, which eliminates reliance on ad hoc judgments about the impact of specific events or policies on stock prices. While event studies have been conducted that examine the market effect of security and privacy breaches on firms, none has attempted to determine the impact, in terms of resulting market reaction, of the HIPAA legislation itself. The results of the study confirm the logic above, while also providing insight into specific stages of the legislative path of HIPAA.
31|6||Engineering a secure mobile messaging framework|It is quite usual in the world of scientific software development to use, as black boxes, algorithmic software libraries without any prior assessment of their efficiency. This approach relies on the assumption that the experimental performance of these libraries, although correct, will match the theoretical expectation of their algorithmic counterparts.
31|6||Real time DDoS detection using fuzzy estimators|We propose a method for DDoS detection by constructing a fuzzy estimator on the mean packet inter arrival times. We divided the problem into two challenges, the first being the actual detection of the DDoS event taking place and the second being the identification of the offending IP addresses. We have imposed strict real time constraints for the first challenge and more relaxed constraints for the identification of addresses. Through empirical evaluation we confirmed that the detection can be completed within improved real time limits and that by using fuzzy estimators instead of crisp statistical descriptors we can avoid the shortcomings posed by assumptions on the model distribution of the traffic. In addition we managed to obtain results under a 3 sec detection window.
31|6||Exploiting hash functions to intensify the remote user authentication scheme|Nowadays, the client–server model plays an important part in the Internet architecture. The procedure that a server authenticates a remote user securely has become a significant issue. Hence, many remote user authentication schemes are proposed. Hsiang and Shih pointed out the weaknesses of Yoon et al.'s scheme and proposed an improved scheme not only preserving original merits but also mending the weaknesses. The scheme protects against masquerading attacks, offline password guessing attacks and parallel session attacks. However, He et al. found that Hsiang et al.'s scheme is still vulnerable to password guessing attack, masquerading server attack and masquerading attack. Furthermore, we also found Hsiang–Shih's scheme could be threatened by a malicious insider that can originate an infringed account attack and a resembling account attack. In this paper, we first describe how a malicious insider carries out an infringed account attack and then present a resembling account attack on Hsiang–Shih's scheme. After that, we propose an improvement assisted by hashing functions to enhance Hsiang–Shih's scheme.
31|6||Hacking: The Next Generation|
31|6||The Basics of Digital Forensics|
31|6||Inside Cyber Warfare: Mapping the Cyber Underworld|
31|6||Practical Malware Analysis: The Hands-On Guide to Dissecting Malicious Software|
31|7|http://www.sciencedirect.com/science/journal/01674048/31/7|Contents|
31|7||Editorial Computers and Security Special Issue IFIP/SEC 2010 âSecurity & Privacy â Silver Linings in the Cloudâ|
31|7||Understanding domain registration abuses|The ability to monetize domain names through resale or serving ad content has contributed to the rise of questionable practices in acquiring them, including domain-name speculation, tasting, and front running. In this paper, we perform one of the first comprehensive studies of these domain registration practices. In order to characterize the prevalence of domain-name speculation, we derive rules describing “hot” topics from popular Google search queries and apply these rules to a dataset containing all .com registrations for an eight-month period in 2008. We also study the extent of domain tasting throughout this time period and analyze the efficacy of ICANN policies intended to limit tasting activity. Finally, we automatically generate high-quality domain names related to current events in order to measure domain front running by registrars. The results of our experiments shed light on the methods and motivations behind these domain registration practices and, in some cases, underscore the difficulty in definitively measuring these questionable behaviors.
31|7||Preserving privacy of feedback providers in decentralized reputation systems|Reputation systems make the users of a distributed application accountable for their behavior. The reputation of a user is computed as an aggregate of the feedback provided by other users in the system. Truthful feedback is clearly a prerequisite for computing a reputation score that accurately represents the behavior of a user. However, it has been observed that users often hesitate in providing truthful feedback, mainly due to the fear of retaliation. We present a decentralized privacy preserving reputation protocol that enables users to provide feedback in a private and thus uninhibited manner. The protocol has linear message complexity, which is an improvement over comparable decentralized reputation protocols. Moreover, the protocol allows users to quantify and maximize the probability that their privacy will be preserved.
31|7||On-the-fly inlining of dynamic security monitors|How do we guarantee that a piece of code, possibly originating from third party, does not jeopardize the security of the underlying application? Language-based information-flow security considers programs that manipulate pieces of data at different sensitivity levels. Securing information flow in such programs remains an open challenge. Recently, considerable progress has been made on understanding dynamic monitoring for secure information flow. This paper presents a framework for inlining dynamic information-flow monitors. A novel feature of our framework is the ability to perform inlining on the fly. We consider a source language that includes dynamic code evaluation of strings whose content might not be known until runtime. To secure this construct, our inlining is done on the fly, at the string evaluation time, and, just like conventional offline inlining, requires no modification of the hosting runtime environment. We present a forma!lization for a simple language to show that the inlined code is secure: it satisfies a non-interference property. We also discuss practical considerations experimental results based on both manual and automatic code rewriting.
31|7||A business-driven decomposition methodology for role mining|It is generally accepted that role mining – that is, the discovery of roles through the automatic analysis of data from existing access control systems – must count on business requirements to increase its effectiveness. Indeed, roles elicited without leveraging on business information are unlikely to be intelligible by system administrators. A business-oriented categorization of users and permissions (e.g., organizational units, job titles, cost centers, business processes, etc.) could help administrators identify the job profiles of users and, as a consequence, which roles should be assigned to them. Nonetheless, most of the existing role mining techniques yield roles that have no clear relationship with the business structure of the organization where the role mining is being applied. To face this problem, we propose a methodology that allows role engineers to leverage business information during the role finding process. The key idea is decomposing the dataset to analyze into several partitions, in a way that each partition is homogeneous from a business perspective. Each partition groups users or permissions with the same business categorization (e.g., all the users belonging to the same department, or all the permissions that support the execution of the same business process). Such partitions are then role-mined independently, hence achieving three main results: (1) elicited roles have a clearer relationship with business information; (2) mining algorithms do not seek to find commonalities among users with fundamentally different job profiles or among uncorrelated permissions; and, (3) any role mining algorithm can be used in conjunction with our approach. When several business attributes are available, analysts need to figure out which one produces the decomposition that leads to the most intelligible roles. In this paper, we describe three indexes that drive the decomposition process by measuring the quality of a given decomposition: entrustability, minability gain, and similarity gain. We compare these indexes, pointing out pros and cons. Finally, we apply our methodology on real enterprise data, showing its effectiveness and efficiency in supporting role engineering.
31|8|http://www.sciencedirect.com/science/journal/01674048/31/8|Contents|
31|8||Editorial|
31|8||Managing information security risks during new technology adoption|In the present study, we draw on previous system dynamics research on operational transition and change of vulnerability to investigate the role of incident response capability in controlling the severity of incidents during the adoption of new technology. Toward this end, we build a system dynamics model using the Norwegian Oil and Gas Industry as the context. The Norwegian Oil and Gas Industry has started to adopt new information communication technology to connect its offshore platforms, onshore control centers, and suppliers. In oil companies, the management is generally aware of the increasing risks associated with operational transition; however, to date, investment in incident response capability has not been highly prioritized because of the uncertainty related to risks and the present reactive mental model of security risk management. The model simulation shows that a reactive approach to security risk management might trap the organization into blindness to minor incidents and low incident response capability, which can lead to severe incidents. The system dynamics model can serve as a means to promote proactive investment in incident response capability.
31|8||A secure and efficient discovery service system in EPCglobal network|In recent years, the Internet of Things (IOT) has drawn considerable attention from the industrial and research communities. Due to the vast amount of data generated through IOT devices and users, there is an urgent need for an effective search engine to help us make sense of this massive amount of data. With this motivation, we begin our initial works on developing a secure and efficient search engine (SecDS) based on EPC Discovery Services (EPCDS) for EPCglobal network, an integral part of IOT. SecDS is designed to provide a bridge between different partners of supply chains to share information while enabling them to find who is in possession of an item. The most important property of SecDS is: while efficiently processing user's search, it is also secure. In order to prevent unauthorized access to SecDS, an extended attribute-based access control model is proposed and implemented such that information belonging to different companies can be protected using different policies. We design, implement SecDS and conduct extensive experiments on it. The results validate the practicality and cost effectiveness of our design and implementations.
31|8||A framework for quantitative evaluation of parallel control-flow obfuscation|Software obfuscation is intended to protect a program by thwarting reverse engineering. Several types of software obfuscation have been proposed, and control-flow obfuscation is a commonly adopted one. In this paper, we present a framework to evaluate parallel control-flow obfuscation, which raises difficulty of reverse engineering by increasing parallelism of a program. We also define a control flow graph of a program and some atomic operators for obfuscating transformations. The proposed framework comprises three phases: parsing, formalization and evaluation. A program is first parsed to a control flow graph. Then, we formalize a parallel control-flow obfuscating transformation based on our atomic operators. By selecting target code blocks in the control flow graph and applying obfuscating transformations to the target code blocks, the original program is then obfuscated. In the third phase, we define a measure to calculate the program complexity. The measure can be considered as a degree to which an obfuscating transformation can confuse a human trying to understand the obfuscated program. Such a measure can also be used as the base of the potency metric to estimate the capability of the obfuscated program against reverse engineering. Our novel framework helps efficiently examine a control-flow obfuscating transformation in a systematic manner and helps select an appropriate obfuscating transformation among a number of candidates to better protect a program.
31|8||Access control for online social networks third party applications|With the development of Web 2.0 technologies, online social networks are able to provide open platforms to enable the seamless sharing of profile data to enable public developers to interface and extend the social network services as applications. At the same time, these open interfaces pose serious privacy concerns as third party applications are usually given access to the user profiles. Current related research has focused on mainly user-to-user interactions in social networks, and seems to ignore the third party applications. In this paper, we present an access control framework to manage third party applications. Our framework is based on enabling the user to specify the data attributes to be shared with the application and at the same time be able to specify the degree of specificity of the shared attributes. We model applications as finite state machines, and use the required user profile attributes as conditions governing the application execution. We formulate the minimal attribute generalization problem and we propose a solution that maps the problem to the shortest path problem to find the minimum set of attribute generalization required to access the application services. We assess the feasibility of our approach by developing a proof-of-concept implementation and by conducting user studies on a widely-used social network platform.
31|8||Optimal mining on security labels for decentralized information flow control|Decentralized information flow control (DIFC) is a key innovation of traditional information flow control (IFC). When compared with IFC, DIFC provides new features including decentralized declassification, taint-tracking and privilege-transferring. These characteristics make DIFC more applicable than traditional IFC to the control of information flows in systems. This paper presents an optimal approach to the mining of security labels for DIFC. This approach can effectively improve DIFC's applicability and manageability in a wide variety of environments. We firstly design a novel policy description language to express security requirements in DIFC characterized assertions. Next, we prove that the problem of obtaining security labels from DIFC assertions is NP-complete. Based on logic programming and genetic algorithm, the proposed approach finally outputs optimal security labels separately for different DIFC systems in both small and large-scale environments. The objectives of this paper are to address two practical aspects of DIFC: (1) how to express security requirements by using DIFC characterized assertions; (2) how to obtain optimal DIFC labels to satisfy security requirements. The experimental results show that the proposed approach is effective in implementing fine-grained information control according to practical security requirements.
31|8||A survey of electronic ticketing applied to transport|A wide variety of transport systems can benefit from the use of Electronic Ticketing (ET). ET systems are progressively introduced in transports systems, and produce a reduction of the associated economic costs and time intervals, and the control of the system is improved. However, the use of ET systems enables various privacy abuses both in real-time and retrospect since the anonymity of users is not always guaranteed and, therefore, users can be traced and their profiles of usual movements can be created. In our review article, we classify and describe the main proposals with special focus on the properties related to user privacy.
31|8||Trusted Domain: A security platform for home automation|In the digital age of home automation and with the proliferation of mobile Internet access, the intelligent home and its devices should be accessible at any time from anywhere. There are many challenges such as security, privacy, ease of configuration, incompatible legacy devices, a wealth of wireless standards, limited resources of embedded systems, etc. Taking these challenges into account, we present a Trusted Domain home automation platform, which dynamically and securely connects heterogeneous networks of Short-Range Wireless devices via simple non-expert user interactions, and allows remote access via IP-based devices such as smartphones.
31|8||Selecting key management schemes for WSN applications|Key management in wireless sensor networks (WSN) is an active research topic. Due to the fact that a large number of key management schemes (KMS) have been proposed in the literature, it is not easy for a sensor network designer to know exactly which KMS best fits in a particular WSN application. In this article, we offer a comprehensive review on how the application requirements and the properties of various key management schemes influence each other. Based on this review, we show that the KMS plays a critical role in determining the security performance of a WSN network with given application requirements. We also develop a method that allows the network designers to select the most suitable KMS for a specific WSN network setting. In addition, the article also addresses the issues on the current state-of-the-art research on the KMS for homogeneous (i.e. non-hierarchical) networks to provide solutions for establishing link-layer keys in various WSN applications and scenarios.
31|8||A Hierarchical Visibility theory for formal digital investigation of anti-forensic attacks|Among the leading topics of research in digital forensic investigation is the development of theoretical and scientifically proven techniques of incident analysis. However, two main problems, which remain unsolved in the literature, could lead the use of formal approaches of attack scenarios reconstruction and incident analysis to be inconclusive. The former is related to the absence of techniques to model and characterize anti-forensic attacks, and cope with the reconstruction of attack scenarios based on evidences compromised by these attacks. The latter is related to the lack of theoretical techniques usable during the preparation of systems to forensic analysis (i.e., the first phase of a forensic process that precedes the occurrence of an incident and the collection of evidences). These techniques are expected to determine the optimal set of security solutions to deploy so that the evidences to be generated further to a security incident would be sufficient to prove a wide range of anti-forensic attacks.
31|8||Power to the people? The evolving recognition of human aspects of security|It is perhaps unsurprising to find much of the focus in IT and computer security being drawn towards the technical aspects of the discipline. However, it is increasingly recognised that technology alone cannot deliver a complete solution, and there is also a tangible need to address human aspects. At the core, people must understand the threats they face and be able to use the protection available to them, and although this has not been entirely ignored, it has not received the level of attention that it merits either. Indeed, security surveys commonly reveal that the more directly user-facing aspects such as policy, training and education are prone to receiving significantly less attention than technical controls such as firewalls, antivirus and intrusion detection. The underlying reason for such disparity is that the human aspects are in many ways a more challenging problem to approach, not least because they cannot be easily targeted with a product-based solution. There is also a direct overlap into the technical area, with issues such as the usability and acceptability of technology solutions having a direct impact upon the actual protection that they are able to deliver.
31|8||Study on poll-site voting and verification systems|Voting is an important part of the democratic process. The electorate makes a decision or expresses an opinion that is accepted by everyone. However, some individual or group may be interested in tampering with the elections process to force an outcome in their favor. Hence, controlling the whole voting process to ensure that it is performed correctly and according to current rules and law is, then, even more important. In this work, we present a review of existing verification systems for paper-based and electronic voting systems in supervised environments, from both academic and commercial worlds. To do so, we perform a fair comparison of a set of representative voting verification systems using an evaluation framework. We define this framework to be composed of several properties, covering important system areas, ranging from user interaction to security issues. Then, we model the natural evolution of verifiability issues on notable voting systems from academia and commerce which are influenced by restrictions on current laws and by the advance of technology.
31|8||Wishful thinking|
31|8||Handbook on Securing Cyber-Physical Critical Infrastructure: Foundations and Challenges|
31|8||Announcement|
31|8||Call for papers|
32|-|http://www.sciencedirect.com/science/journal/01674048/32|Contents|
32|-||Editorial|
32|-||Design and formal security evaluation of NeMHIP: A new secure and efficient network mobility management protocol based on the Host Identity Protocol|NEtwork MObility Basic Support (NEMO BS) is a standardized protocol for managing the mobility of a set of nodes that move together as a whole while having continuous connectivity to the Internet through one or more Mobile Routers (MRs). Because it is based on Mobile IPv6 (MIPv6), it inherits the properties of MIPv6, such as the use of IPsec. However, NEMO BS does not address all the features required by the demanding Intelligent Transportation Systems (ITS) scenario to provide an integrated and global secure mobility management framework. In addition, unlike MIPv6, the routing in NEMO BS is suboptimal, which makes difficult the provision of an adequate service performance. These characteristics make the application of the NEMO BS protocol not optimum in this scenario. An interesting strategy to provide security and good service performance is to consider a protocol that establishes and maintains Security Associations (SAs), such as the Host Identity Protocol (HIP). Different HIP-based approaches have been defined. However, these HIP-based network mobility solutions still present unsolved issues. In this article, we present a secure and efficient network mobility protocol named NeMHIP. NeMHIP provides secure and optimum mobility management and efficient end-to-end confidentiality and integrity protection apart from the basic security properties inherited from HIP. To evaluate the security provisions of NeMHIP, we have conducted a belief-based formal evaluation. The results demonstrate that the defined security goals are achieved by the protocol. Furthermore, we have performed an automated formal evaluation to validate additional security aspects of NeMHIP. Thus, we have modeled NeMHIP using the AVISPA tool and assessed its security when an intruder is present. The results confirm that NeMHIP is a secure protocol that ensures end-to-end confidentiality and integrity without introducing security leaks to the basic HIP. Thus, we have addressed the need found in the literature for providing security and efficiency in the network mobility scenario.
32|-||Tree-formed verification data for trusted platforms|The establishment of trust relationships to a computing platform relies on validation processes. Validation allows an external entity to build trust in the expected behaviour of the platform based on provided evidence of the platform's configuration. In a process like remote attestation, the ‘trusted’ platform submits verification data created during a start up process. These data consist of hardware-protected values of platform configuration registers, containing nested measurement values, e.g., hash values, of loaded or started components. Commonly, the register values are created in linear order by a hardware-secured operation. Fine-grained diagnosis of components, based on the linear order of verification data and associated measurement logs, is not optimal. We propose a method to use tree-formed verification data to validate a platform. Component measurement values represent leaves, and protected registers represent roots of a hash tree. We describe the basic mechanism of validating a platform using tree-formed measurement logs and root registers and show a logarithmic speed-up for the search of faults. Secure creation of a tree is possible using a limited number of hardware-protected registers and a single protected operation. In this way, the security of tree-formed verification data is maintained.
32|-||A confidential and DoS-resistant multi-hop code dissemination protocol for wireless sensor networks|Due to the open environment in which Wireless Sensor Networks (WSNs) are typically deployed, it is important to be able to authenticate transmitted data. In some applications it is also required that the data be kept confidential in spite of message interception. Authentication and confidentiality are typically implemented through cryptographic operations which may be expensive in power consumption, making a protocol with these features vulnerable to attack by an adversary who transmits forged data, so forcing nodes to waste energy in identifying it as invalid. Additionally, in multi-hop code dissemination protocols, a sensor node is required to broadcast its program image when requested by its neighbors. An adversary could repeatedly send spurious program image requests to its neighbors, making them exhaust their energy reserves. In this paper, we present a new approach to achieve confidentiality in multi-hop code dissemination. We propose countermeasures against both these types of attack. Our approach is based on Deluge, an open source, state-of-the-art code dissemination protocol for WSNs. We provide theoretical analysis and simulation/experimental results which show that our approach outperforms earlier attempts when a malicious code injector is present, as well as a performance evaluation of latency, energy consumption, dissemination rate, and other factors (Liu and Ning, November 2006).
32|-||Digital media triage with bulk data analysis and bulk_extractor|Bulk data analysis eschews file extraction and analysis, common in forensic practice today, and instead processes data in “bulk,” recognizing and extracting salient details (“features”) of use in the typical digital forensics investigation. This article presents the requirements, design and implementation of the bulk_extractor, a high-performance carving and feature extraction tool that uses bulk data analysis to allow the triage and rapid exploitation of digital media. Bulk data analysis and the bulk_extractor are designed to complement traditional forensic approaches, not replace them. The approach and implementation offer several important advances over today's forensic tools, including optimistic decompression of compressed data, context-based stop-lists, and the use of a “forensic path” to document both the physical location and forensic transformations necessary to reconstruct extracted evidence. The bulk_extractor is a stream-based forensic tool, meaning that it scans the entire media from beginning to end without seeking the disk head, and is fully parallelized, allowing it to work at the maximum I/O capabilities of the underlying hardware (provided that the system has sufficient CPU resources). Although bulk_extractor was developed as a research prototype, it has proved useful in actual police investigations, two of which this article recounts.
32|-||The architecture of a digital forensic readiness management system|A coordinated approach to digital forensic readiness (DFR) in a large organisation requires the management and monitoring of a wide variety of resources, both human and technical. The resources involved in DFR in large organisations typically include staff from multiple departments and business units, as well as network infrastructure and computing platforms. The state of DFR within large organisations may therefore be adversely affected if the myriad human and technical resources involved are not managed in an optimal manner. This paper contributes to DFR by proposing the novel concept of a digital forensic readiness management system (DFRMS). The purpose of a DFRMS is to assist large organisations in achieving an optimal level of management for DFR. In addition to this, we offer an architecture for a DFRMS. This architecture is based on requirements for DFR that we ascertained from an exhaustive review of the DFR literature. We describe the architecture in detail and show that it meets the requirements set out in the DFR literature. The merits and disadvantages of the architecture are also discussed. Finally, we describe and explain an early prototype of a DFRMS.
32|-||Future directions for behavioral information security research|Information Security (InfoSec) research is far reaching and includes many approaches to deal with protecting and mitigating threats to the information assets and technical resources available within computer based systems. Although a predominant weakness in properly securing information assets is the individual user within an organization, much of the focus of extant security research is on technical issues. The purpose of this paper is to highlight future directions for Behavioral InfoSec research, which is a newer, growing area of research. The ensuing paper presents information about challenges currently faced and future directions that Behavioral InfoSec researchers should explore. These areas include separating insider deviant behavior from insider misbehavior, approaches to understanding hackers, improving information security compliance, cross-cultural Behavioral InfoSec research, and data collection and measurement issues in Behavioral InfoSec research.
32|-||From keyloggers to touchloggers: Take the rough with the smooth|The proliferation of touchscreen devices brings along several interesting research challenges. One of them is whether touchstroke-based analysis (similar to keylogging) can be a reliable means of profiling the user of a mobile device. Of course, in such a setting, the coin has two sides. First, one can employ the output produced by such a system to feed machine learning classifiers and later on intrusion detection engines. Second, aggressors can install touchloggers to harvest user's private data. This malicious option has been also extensively exploited in the past by legacy keyloggers under various settings, but has been scarcely assessed for soft keyboards. Compelled by these separate but interdependent aspects, we implement the first-known native and fully operational touchlogger for ultramodern smartphones and especially for those employing the proprietary iOS platform. The results we obtained for the first objective are very promising showing an accuracy in identifying misuses, and thus post-authenticating the user, in an amount that exceeds 99%. The virulent personality of such software when used maliciously is also demonstrated through real-use cases.
32|-||On the detection of desynchronisation attacks against security protocols that use dynamic shared secrets|Many peer-to-peer security protocols in mobile communications utilise shared secrets. Synchronous storage of shared secrets is imperative for the successful operation of security protocols, as asynchronous storage of shared secrets may lead to service unavailability. Hence, update mechanisms must not only guarantee the secrecy of shared secrets, but also their synchrony.
32|-||Systematic bug finding and fault localization enhanced with input data tracking|Fault localization (FL) is the process of debugging erroneous code and directing analysts to the root cause of the bug. With this in mind, we have developed a distributed, end-to-end fuzzing and analysis system that starts with a binary, identifies bugs, and subsequently localizes the bug's root cause. Our system does not require the test subject's source code, nor do we require a test suite. Our work focuses on an important class of bugs, memory corruption errors, which usually have software security implications. Thus, our approach appeals to software attack researchers as well. In addition to our bug hunting and analysis framework, we have enhanced code-coverage based fault localization by incorporating input data tainting and tracking using a light-weight binary instrumentation technique. By capturing code coverage and select input data usage, our new FL algorithm is able to better localize faults, and therefore better assist analysts. We report the application of our approach on large, real-world applications (Firefox and VLC), as well as the classic Siemens benchmark and other test programs.
32|-||Exploring attack graph for cost-benefit security hardening: A probabilistic approach|The increasing complexity of today's computer systems, together with the rapid emergence of novel vulnerabilities, make security hardening a formidable challenge for security administrators. Although a large variety of tools and techniques are available for vulnerability analysis, the majority work at system or network level without explicit association with human and organizational factors. This article presents a middleware approach to bridge the gap between system-level vulnerabilities and organization-level security metrics, ultimately contributing to cost-benefit security hardening. In particular, our approach systematically integrates attack graph, a commonly used effective approach to representing and analyzing network vulnerabilities, and Hidden Markov Model (HMM) together, for exploring the probabilistic relation between system observations and states. More specifically, we modify and apply dependency attack graph to represent network assets and vulnerabilities (observations), which are then fed to HMM for estimating attack states, whereas their transitions are driven by a set of predefined cost factors associated with potential attacks and countermeasures. A heuristic searching algorithm is employed to automatically infer the optimal security hardening through cost-benefit analysis. We use a synthetic network scenario to illustrate our approach and evaluate its performance through a set of simulations.
32|-||EVIV: An end-to-end verifiable Internet voting system|Traditionally, a country's electoral system requires the voter to vote at a specific day and place, which conflicts with the mobility usually seen in modern live styles. Thus, the widespread of Internet (mobile) broadband access can be seen as an opportunity to deal with this mobility problem, i.e. the adoption of an Internet voting system can make the live of voter's much more convenient; however, a widespread Internet voting systems adoption relies on the ability to develop trustworthy systems, i.e. systems that are verifiable and preserve the voter's privacy. Building such a system is still an open research problem.
32|-||LSB matching steganalysis based on patterns of pixel differences and random embedding|This paper presents a novel method for detection of LSB matching steganography in grayscale images. This method is based on the analysis of the differences between neighboring pixels before and after random data embedding. In natural images, there is a strong correlation between adjacent pixels. This correlation is disturbed by LSB matching generating new types of correlations. The presented method generates patterns from these correlations and analyzes their variation when random data are hidden. The experiments performed for two different image databases show that the method yields better classification accuracy compared to prior art for both LSB matching and HUGO steganography. In addition, although the method is designed for the spatial domain, some experiments show its applicability also for detecting JPEG steganography.
32|-||A universal system for fair non-repudiable certified e-mail without a trusted third party|Certified e-mail delivery is a quickly emerging field with significant legal benefits for e-commerce and e-government. Existing service-providers provide individual solutions that rely on a trusted third party and are in general limited in their scope and interoperability; as a consequence, they are unsuitable for cross-border application as required by transnational unions like the EU.
32|-||The state of the art of application restrictions and sandboxes: A survey of application-oriented access controls and their shortfalls|Under most widely-used security mechanisms the programs users run possess more authority than is strictly necessary, with each process typically capable of utilising all of the user's privileges. Consequently such security mechanisms often fail to protect against contemporary threats, such as previously unknown (‘zero-day’) malware and software vulnerabilities, as processes can misuse a user's privileges to behave maliciously. Application restrictions and sandboxes can mitigate threats that traditional approaches to access control fail to prevent by limiting the authority granted to each process. This developing field has become an active area of research, and a variety of solutions have been proposed. However, despite the seriousness of the problem and the security advantages these schemes provide, practical obstacles have restricted their adoption.
32|-||Security-related behavior in using information systems in the workplace: A review and synthesis|Security-related behavior in the workplace has recently drawn much attention from scholars in the information systems literature. Many studies, however, have reported inconsistent and sometimes contradictory results about the effects of some key factors such as sanctions. We argue that one of the reasons causing the inconsistent findings is the divergent conceptualizations of security-related behavior. In this paper, we conducted an extensive review of the divergent concepts. Many of the concepts overlap with each other on some dimensions and yet are different on others. By delineating and synthesizing the differences, we proposed a framework for conceptualizing security-related behavior. The framework can facilitate the development of consistent and comparable terms and concepts in future studies. Implications for research are also discussed.
32|-||A middleware approach for outsourcing data securely|Businesses that provide data storage facilities on the internet (IDP) have exploded recently. Such businesses provide the following benefits to end users: a) anytime, anywhere access to data; b) low cost; and c) good quality of service. Examples of data storage providers include Amazon S3 service, Windows SkyDrive, Nirvarnix, etc.
32|-||Hacking VoIP|
32|-||Metasploit the Penetration Tester's Guide|
33|-|http://www.sciencedirect.com/science/journal/01674048/33|Contents|
33|-||Editorial|
33|-||Organizational power and information security rule compliance|This paper analyzes power relationships and the resulting failure in complying with information security rules. It argues that an inability to understand the intricate power relationships in the design and implementation of information security rules leads to a lack of compliance with the intended policy. The argument is conducted through an empirical, qualitative case study set in a Swedish Social Services organization. Our findings indicate that various dimensions of power and how these relate to information security rules ensure adequate compliance. This also helps to improve configuration of security rules through proactive information security management.
33|-||TCP Ack storm DoS attacks|We present Ack-storm DoS attacks, a new family of DoS attacks exploiting a subtle design flaw in the core TCP specifications. The attacks can be launched by a very weak MitM attacker, which can only eavesdrop occasionally and spoof packets (a Weakling in the Middle (WitM)). The attacks can reach theoretically unlimited amplification; we measured amplification of over 400,000 against popular web sites before aborting our trial attack.
33|-||Distributed security policy conformance|Security policy conformance is a crucial issue in large-scale critical cyber-infrastructure. The complexity of these systems, insider attacks, and the possible speed of an attack on a system necessitate an automated approach to assure a basic level of protection.
33|-||An authentication flaw in browser-based Single Sign-On protocols: Impact and remediations|Browser-based Single Sign-On (SSO) protocols relieve the user from the burden of dealing with multiple credentials thereby improving the user experience and the security. In this paper we show that extreme care is required for specifying and implementing the prototypical browser-based SSO use case. We show that the main emerging SSO protocols, namely SAML SSO and OpenID, suffer from an authentication flaw that allows a malicious service provider to hijack a client authentication attempt or force the latter to access a resource without its consent or intention. This may have serious consequences, as evidenced by a Cross-Site Scripting attack that we have identified in the SAML-based SSO for Google Apps and in the SSO available in Novell Access Manager v.3.1. For instance, the attack allowed a malicious web server to impersonate a user on any Google application. We also describe solutions that can be used to mitigate and even solve the problem.
33|-||A generic approach to prevent board flooding attacks inÂ coercion-resistant electronic voting schemes|This paper presents a generic approach to prevent board flooding attacks in remote electronic voting schemes providing coercion-resistance. A key property of these schemes is the possibility of casting invalid votes to the public bulletin board, which are indistinguishable from proper votes. Exactly this possibility is crucial for making these schemes coercion-resistant, but it also opens doors for flooding the bulletin board with an enormous amount of invalid votes, eventually spoiling the efficiency of the tallying process. To prevent such attacks, we present a generic enhancement for these schemes, in which we restrict the total amount of votes accepted by the public bulletin board. For this, voters receive a certain amount of posting tickets, each of which allowing its owner to post a single vote to the bulletin board. The list of all posting tickets is published along with the electoral register. Votes with no valid posting ticket are immediately rejected by the bulletin board. The maximum amount of postings accepted by the bulletin board is thus bounded by the total number of issued posting tickets. This prevents a massive board flooding attack with a very large number of invalid votes and thus guarantees the efficiency of the tallying phase. Except with respect to forced vote abstention, our enhancement preserves all properties of the existing scheme in use. Although coercion by forced vote abstention cannot be ruled out entirely, such attacks are at least not scalable to a considerable portion of the electorate.
34|-|http://www.sciencedirect.com/science/journal/01674048/34|Contents|
34|-||Editorial|
34|-||Assessing security threat scenarios for utility-based reputation model in grids|Trust and reputation models play an important role in enabling trusted computations over large-scale distributed grids. Many models have been recently proposed and implemented within trust management systems. Nevertheless, the existing approaches usually assess performance of models in terms of resource management while less attention is paid to the analysis of security threat scenarios for such models. In this paper, we assess the most important and critical security threats for a utility-based reputation model in grids. The existing model is extended to address these threat scenarios. With simulations that were run using data collected from the EGEE Grid-Observatory project, we analyse efficiency of the utility-based reputation model against these threats.
34|-||On the optimality of cooperative intrusion detection forÂ resource constrained wireless networks|The problem of cooperative intrusion detection in battery-powered wireless mesh and sensor networks is challenging, primarily because of the limited resources available to participating nodes. Although the problem has received some attention from the research community, little is known about the tradeoffs among different objectives, such as high network performance, low power consumption, low delay in information collection and high security effectiveness. This article proposes, to the best of our knowledge for the first time, cooperative intrusion detection functions that take into account multiple objectives simultaneously. We formulate the problem of identifying the type of intrusion detection function each node runs, as a multi-objective optimization problem, and propose solutions based on genetic algorithms. Through extensive simulations we demonstrate that our solutions are scalable to large networks, and are characterized by a small variance in the normalized fitness value of individual/single objectives and by a small attack detection/reporting delay. In a real implementation/evaluation we demonstrate that our cooperative intrusion detection system achieves a higher detection rate (93%) than state of art solutions.
34|-||A computer forensic method for detecting timestamp forgery in NTFS|In this paper, we present a computer forensic method for detecting timestamp forgeries in the Windows NTFS file system. It is difficult to know precisely that the timestamps have been changed by only examining the timestamps of the file itself. If we can find the past timestamps before any changes to the file are made, this can act as evidence of file time forgery. The log records operate on files and leave large amounts of information in the $LogFile that can be used to reconstruct operations on the files and also used as forensic evidence. Log record with 0x07/0x07 opcode in the data part of Redo/Undo attribute has timestamps which contain past-and-present timestamps. The past-and-present timestamps can be decisive evidence to indicate timestamp forgery, as they contain when and how the timestamps were changed. We used file time change tools that can easily be found on Internet sites. The patterns of the timestamp change created by the tools are different compared to those of normal file operations. Seven file operations have ten timestamp change patterns in total by features of timestamp changes in the $STANDARD_INFORMATION attribute and the $FILE_NAME attribute. We made rule sets for detecting timestamp forgery based on using difference comparison between changes in timestamp patterns by the file time change tool and normal file operations. We apply the forensic rule sets for “.txt”, “.docx” and “.pdf” file types, and we show the effectiveness and validity of the proposed method. The importance of this research lies in the fact that we can find the past time in $LogFile, which gives decisive evidence of timestamp forgery. This makes the timestamp active evidence as opposed to simply being passive evidence.
34|-||Delegate the smartphone user? Security awareness in smartphone platforms|Smartphone users increasingly download and install third-party applications from official application repositories. Attackers may use this centralized application delivery architecture as a security and privacy attack vector. This risk increases since application vetting mechanisms are often not in place and the user is delegated to authorize which functionality and protected resources are accessible by third-party applications. In this paper, we mount a survey to explore the security awareness of smartphone users who download applications from official application repositories (e.g. Google Play, Apple's App Store, etc.). The survey findings suggest a security complacency, as the majority of users trust the app repository, security controls are not enabled or not added, and users disregard security during application selection and installation. As a response to this security complacency we built a prediction model to identify users who trust the app repository. The model is assessed, evaluated and proved to be statistically significant and efficient.
34|-||A taxonomy and survey of attacks on digital signatures|Non-repudiation is a desired property of current electronic transactions, by which a further repudiation of the commitments made by any involved party is prevented. Digital signatures are recognized by current standards and legislation as non-repudiation evidence that can be used to protect the parties involved in a transaction against the other's false denial about the occurrence of a certain event. However, the reliability of a digital signature should determine its capability to be used as valid evidence. The inevitability of vulnerabilities in technology and the non-negligible probability of an occurrence of security threats would make non-repudiation of evidence difficult to achieve. We consider that it is of the utmost importance to develop appropriate tools and methods to assist in designing and implementing secure systems in a way that reliable digital signatures can be produced. In this paper, a comprehensive taxonomy of attacks on digital signatures is presented, covering both the signature generation and verification phases. The taxonomy will enable a rigorous and systematic analysis of the causes that may subvert the signature reliability, allowing the identification of countermeasures of general applicability. In addition, an intensive survey of attacks classified under our taxonomy is given.
34|-||Analyzing the security of Windows 7 and Linux for cloud computing|We review and analyze the major security features and concerns in deploying modern commodity operating systems such as Windows 7 and Linux 2.6.38 in a cloud computing environment. We identify the security weaknesses and open challenges of these two operating systems when deployed in the cloud environment. In particular, we examine and compare various operating system security features which are critical in providing a secure cloud. These security features include authentication, authorization and access control, physical memory protection, privacy and encryption of stored data, network access and firewalling capabilities, and virtual memory.
34|-||Phishing detection and impersonated entity discovery using Conditional Random Field and Latent Dirichlet Allocation|Phishing is an attempt to steal users' personal and financial information such as passwords, social security and credit card numbers, via electronic communication such as e-mail and other messaging services. Attackers pretend to be from a legitimate organization and direct users to a fake website that resembles a legitimate website, which is then used to collect users' personal information. In this paper, we propose a novel methodology to detect phishing attacks and to discover the entity/organization that the attackers impersonate during phishing attacks. The proposed multi-stage methodology employs natural language processing and machine learning. The methodology first discovers (i) named entities, which includes names of people, organizations, and locations; and (ii) hidden topics, using (a) Conditional Random Field (CRF) and (b) Latent Dirichlet Allocation (LDA) operating on both phishing and non-phishing data. Utilizing topics and named entities as features, the next stage classifies each message as phishing or non-phishing using AdaBoost. For messages classified as phishing, the final stage discovers the impersonated entity using CRF. Experimental results show that the phishing classifier detects phishing attacks with no misclassification when the proportion of phishing emails is less than 20%. The F-measure obtained was 100%. Our approach also discovers the impersonated entity from messages that are classified as phishing, with a discovery rate of 88.1%. The automatic discovery of impersonated entity from phishing helps the legitimate organization to take down the offending phishing site. This protects their users from falling for phishing attacks, which in turn leads to satisfied customers. Automatic discovery of an impersonated entity also helps email service providers to collaborate with each other to exchange attack information and protect their customers.
34|-||Special issue on trust in cyber, physical and social computing: Call for papers|
35|-|http://www.sciencedirect.com/science/journal/01674048/35|Contents|
35|-||Editorial|
35|-||ESPOONERBAC: Enforcing security policies in outsourced environments|Data outsourcing is a growing business model offering services to individuals and enterprises for processing and storing a huge amount of data. It is not only economical but also promises higher availability, scalability, and more effective quality of service than in-house solutions. Despite all its benefits, data outsourcing raises serious security concerns for preserving data confidentiality. There are solutions for preserving confidentiality of data while supporting search on the data stored in outsourced environments. However, such solutions do not support access policies to regulate access to a particular subset of the stored data.
35|-||Modular square root puzzles: Design of non-parallelizable and non-interactive client puzzles|Denial of Service (DoS) attacks aiming to exhaust the resources of a server by overwhelming it with bogus requests have become a serious threat. Especially protocols that rely on public key cryptography and perform expensive authentication handshakes may be an easy target. A well-known countermeasure against resource depletion attacks are client puzzles. The victimized server demands from the clients to commit computing resources before it processes their requests. To get service, a client must solve a cryptographic puzzle and submit the right solution. Existing client puzzle schemes have some drawbacks. They are either parallelizable, coarse-grained or can be used only interactively. In case of interactive client puzzles where the server poses the challenge an attacker might mount a counterattack on the clients by injecting faked packets with bogus puzzle parameters bearing the server's sender address. In this paper we introduce a novel scheme for client puzzles which relies on the computation of square roots modulo a prime. Modular square root puzzles are non-parallelizable, i.e., the solution cannot be obtained faster than scheduled by distributing the puzzle to multiple machines or CPU cores, and they can be employed both interactively and non-interactively. Our puzzles provide polynomial granularity and compact solution and verification functions. Benchmark results demonstrate the feasibility of our approach to mitigate DoS attacks on hosts in 1 or even 10 Gbit networks. In addition, we show how to raise the efficiency of our puzzle scheme by introducing a bandwidth-based cost factor for the client. Furthermore, we also investigate the construction of client puzzles from modular cube roots.
35|-||Budget-aware Role Based Access Control|The suitability of Role Based Access Control (RBAC) is being challenged in dynamic environments like healthcare. In an RBAC system, a user's legitimate access may be denied if their need has not been anticipated by the security administrator at the time of policy specification. Alternatively, even when the policy is correctly specified an authorised user may accidentally or intentionally misuse the granted permission. The heart of the challenge is the intrinsic unpredictability of users' operational needs as well as their incentives to misuse permissions. In this paper we propose a novel Budget-aware Role Based Access Control (B-RBAC) model that extends RBAC with the explicit notion of budget and cost, where users are assigned a limited budget through which they pay for the cost of permissions they need. We propose a model where the value of resources are explicitly defined and an RBAC policy is used as a reference point to discriminate the price of access permissions, as opposed to representing hard and fast rules for making access decisions. This approach has several desirable properties. It enables users to acquire unassigned permissions if they deem them necessary. However, users misuse capability is always bounded by their allocated budget and is further adjustable through the discrimination of permission prices. Finally, it provides a uniform mechanism for the detection and prevention of misuses.
36|-|http://www.sciencedirect.com/science/journal/01674048/36|Contents|
36|-||Automatic and lightweight grammar generation for fuzz testing|Blackbox fuzz testing can only test a small portion of code when rigorously checking the well-formedness of input values. To overcome this problem, blackbox fuzz testing is performed using a grammar that delineates the format information of input values. However, it is almost impossible to manually construct a grammar if the input specifications are not known. We propose an alternative technique: the automatic generation of fuzzing grammars using API-level concolic testing. API-level concolic testing collects constraints at the library function level rather than the instruction level. While API-level concolic testing may be less accurate than instruction-level concolic testing, it is highly useful for speedily generating fuzzing grammars that enhance code coverage for real-world programs. To verify the feasibility of the proposed concept, we implemented the system for generating ActiveX control fuzzing grammars, named YMIR. The experiment results showed that the YMIR system was capable of generating fuzzing grammars that can raise branch coverage for ActiveX control using highly-structured input string by 15–50%. In addition, the YMIR system discovered two new vulnerabilities revealed only when input values are well-formed. Automatic fuzzing grammar generation through API-level concolic testing is not restricted to the testing of ActiveX controls; it should also be applicable to other string processing program whose source code is unavailable.
36|-||New payment methods: A review of 2010â2012 FATF mutual evaluation reports|Traditionally, the financial sector is often seen as the gatekeepers of the Anti-Money Laundering/Counter Terrorism Financing (AML/CFT) regime. In recent years, new payment methods, particularly stored value prepaid cards and mobile money transfer systems, are increasingly been seen as a widely accepted payment method. However, they have also been highlighted as potential money laundering and terrorism financing instruments. This paper aims to provide an improved understanding of the money laundering and terrorism financing risk environment and hopefully, new payment method providers are better placed to manage new and emerging threats. A review of the compliance levels in 65 mutual evaluation (and follow-up) reports published by FATF in English between 1st of January 2010 and 31st of December 2012 suggests that there are still compliance issues in areas that might afford exploitative opportunities for transnational crime and terrorist networks – after all, global standards are only as strong as their weakest link. This can have detrimental effects on a country's national security through increasing risks of money laundering and financing of terrorism (e.g. due to regulatory arbitrage), and wastage due to the implementation of inappropriate regulatory measures. We conclude with a three-pronged evidence-based AML/CTF approach, with the aim of helping governments and key stakeholders to improve knowledge of the nature and dimensions to the problem, and of suitable risk management and mitigation strategies that would enable scarce resources in fighting money laundering and terrorism financing threats to be more effectively allocated and, hence, make the most impact.
36|-||Validation of security protocol implementations from security objectives|Protocol security testing can verify and find the potential defects of protocols and their implementations to avoid possible threatening request attacks. It requires concrete experiment against a real, physical implementation. But with the growing complexity of the protocol, added to the multiplicity of possible malicious inputs, the combination of scenarios to be computed will increase to an explosive speed and become the main problem. To address this, we use the concept of Security Objectives to Protocol Security Testing, to generate the test cases on-the-fly. We propose the model, the approach and the algorithm for this protocol verification method and we present a case study with an authentication service.
36|-||A novel iris and chaos-based random number generator|A novel kind of iris and chaotic-based random number generator (ICRNG) is developed from the unique randomness and unpredictability of iris. This method combines biometric feature extraction and random number generation in a novel way. Firstly, we get the non-deterministic source-iris image by iris acquisition equipment, then, we use chaotic function to eliminate the similar pattern in iris from the same person and get the unpredictable random sequence. We use auto-correlation function method and correlation coefficients method to show that ICRNG is linearly independent, then, we use BDS statistical test method to verify ICRNG is nonlinearly independent. Finally, the randomness of ICRNG is verified by histogram analysis, information analysis, sensitivity analysis, FIPS 140-2 tests and NIST SP 800-22 tests. Hence, ICRNG possess satisfactory performance and can be implemented on common PC platform.
36|-||Using network-based text analysis to analyze trends in Microsoft's security innovations|As the use of networked computers and digital data increase, so have the reports of data compromise and malicious cyber-attacks. Increased use and reliance on technologies complicate the process of providing information security. This expanding complexity in supplying data security requirements coupled with the increased recognition of the value of information, have led to the need to quickly advance the information security area. In this paper, we examine the maturation of the information security area by analyzing the innovation activity of one of the largest and most ubiquitous information technology companies, Microsoft. We conduct a textual analysis of their patent application activity in the information security domain since the early 2000's using a novel text analysis approach based on concepts from social network analysis and algorithmic classification. We map our analysis to focal areas in information security and examine it against Microsoft's own history, in order to determine the depth and breadth of Microsoft's innovations. Our analysis shows the relevance of using a network-based text analysis. Specifically, we find that Microsoft has increasingly emphasized topics that fall into the identity and access management area. We also show that Microsoft's innovations in information security showed tremendous growth after their Trustworthy Computing Initiative was announced. In addition, we are able to determine areas of focus that correspond to Microsoft's major vulnerabilities. These findings indicate that while Microsoft is still actively, albeit not always successfully, fighting vulnerabilities in their products, they are quite vigorously and broadly innovating in the information security area.
37|-|http://www.sciencedirect.com/science/journal/01674048/37|Contents|
37|-||Editorial|
37|-||A novel agent-based approach to detect sinkhole attacks inÂ wireless sensor networks|Nowadays, Wireless Sensor Networks (WSNs) are widely used in many areas, especially in military operations and monitoring applications. Their wireless nature makes them very attractive to attackers, so its security system plays a vital role. Due to the limitations on resources, such as energy and storage, the security mechanism of WSNs have to be considered differently from traditional networks. Over the past years researchers have encouraged the use of mobile agents as a new and smart paradigm for distributed applications to overcome the limitations of sensor nodes. In this paper a defensive mechanism will be proposed against sinkhole attacks using mobile agents. We use mobile agents to aware every node from its trusted neighbors through a three-step negotiation so they do not listen to the traffics generated by malicious nodes. We evaluate our work in terms of energy consumption, packet loss rate, throughput and agent overhead caused by mobility and communication.
37|-||Mutual-friend based attacks in social network systems|Recently, we have seen a rapid growth of social networking systems (SNSs). In most SNSs, a user can configure his privacy settings to indicate who can or cannot see his friend list. Usually, SNSs, such as LinkedIn and Google Plus, also include a feature that allows a user to query mutual friends between him and any other user he can reach using the available public search feature in SNSs. While such a mutual friend feature is very helpful in letting users find new friends and connect to them, in this paper, we show that it also raises significant privacy concerns as an adversary can use it to find out some or all of the victim's friends, although, as per the privacy settings of the victim, the adversary is not authorized to see his friend list directly. We show that by using mutual friend queries, an attacker can launch privacy attacks that we refer to as mutual-friend based attacks to identify friends and distant neighbors of targeted users. We analyze these attacks and identify various attack structures that an attacker can use to build attack strategies, using which an attacker can identify a user's friends and his distant neighbors. Through simulations, we demonstrate that mutual-friend based attacks are effective. For instance, one of the simulation results show that an attacker using just one attacker node can identify more than 60% of a user's friends.
37|-||Monitoring information security risks within health care|This paper presents an overview of possible risks to the security of health care data. These risks were detected with a novel approach to information security. It is based on the philosophy that information security risk monitoring should include human and societal factors, and that collaboration between organisations and experts is essential to gain knowledge about potential risks. The methodology uses a mixed methods approach including a quantitative analysis of historical security incident data and expert elicitation through a Delphi study. The result is an overview of the possible socio-technical risks that a panel of experts expect to materialise in health care organisations in the near future. These risks include (amongst others): staff leaving data assets unattended on the premises and these assets consequently go missing, staff sharing passwords to access patient data and staff sending email containing personal patient data to the wrong addressee thus disclosing data to unauthorised persons. The expert panel recognized risks from current discussion topics such as outsourcing, but these risks are still considered to appear less frequently than the more traditional information security risks. Furthermore, the panel did not estimate a high frequency of occurrence of socio-technical information security risks caused by new technologies such as cloud computing or RFID.
37|-||A whitelist-based countermeasure scheme using a Bloom filter against SIP flooding attacks|Since SIP uses a text-based message format and is open to the public Internet, it is exposed to a number of potential threats of denial of service (DoS) by flooding attacks. Although several approaches have been proposed to detect and counteract SIP flooding attacks, most of these do not provide effective countervailing schemes to protect normal messages from abnormal ones after attacks have been detected. In addition, these approaches have some limitations in large user environments for SIP-based multimedia services. In this paper, a whitelist-based countermeasure scheme is proposed, to protect both normal SIP users and servers from malicious flooding attacks. To construct the whitelist, a Bloom filter approach is used, to reduce memory requirements and computational complexity. We use the non-membership ratio as a measure for the attack detection, instead of using the message rate usually used in conventional schemes. It is shown that the proposed method can provide more robust detection performances.
37|-||A framework for prototyping and testing data-only rootkit attacks|Kernel rootkits—attacks which modify a running operating system kernel in order to hide an attacker's presence—are significant threats. Recent advances in rootkit defense technology will force rootkit threats to rely on only modifying kernel data structures without injecting and executing any new code; however these data-only kernel rootkit attacks are still both realistic and powerful. In this work we present DORF, a framework for prototyping and testing data-only rootkit attacks. DORF is an object-oriented framework that allows researchers to construct attacks that can be easily ported between various Linux distributions and versions. The current implementation of DORF contains a group of existing and new data-only attacks, and the portability of DORF is demonstrated by porting it to 6 different Linux distributions. The goal of DORF is to allow researchers to construct repeatable experiments with little effort, which will in turn advance research into data-only attacks and defenses.
37|-||Active cyber defense with denial and deception: AÂ cyber-wargame experiment|In January 2012, MITRE performed a real-time, red team/blue team cyber-wargame experiment. This presented the opportunity to blend cyber-warfare with traditional mission planning and execution, including denial and deception tradecraft. The cyber-wargame was designed to test a dynamic network defense cyber-security platform being researched in The MITRE Corporation's Innovation Program called Blackjack, and to investigate the utility of using denial and deception to enhance the defense of information in command and control systems.
37|-||Quality of security metrics and measurements|Quantification of information security can be used to obtain evidence to support decision-making about the security performance of software systems. Knowledge about the relational importance of the main quality criteria of security metrics can help build security metrology models based on practical needs. This paper presents the results of a quantitative security metrics expert survey of 141 respondents, and an associated interview study, regarding the prioritization of 19 quality criteria of security metrics identified in the literature. The interviews were used to validate the survey results and to obtain further information on the findings. The results identified three foundational quality criteria of security metrics: correctness, measurability, and meaningfulness. These criteria form the basis for credibility and sufficiency for security metrics and associated measurements. Moreover, usability was seen as an important criterion. The paper analyzes the foundational and related quality criteria and proposes a model of them.
37|-||Defeating line-noise CAPTCHAs with multiple quadratic snakes|Optical character recognition (OCR) is one of the fundamental problems in artificial intelligence and image processing, but recent progress in OCR represents a security challenge for Web sites that throttle requests with image based CAPTCHAs (Completely Automated Public Turing Tests to Tell Computers and Humans Apart). A CAPTCHA is challenge-response test placed within web forms to determine whether the user is human. Unfortunately, algorithms capable of solving image based CAPTCHAs can be used to create spam accounts and design malicious denial of service (DoS) attacks, causing financial and social damage. The problem of defeating digital image CAPTCHAs is thus twofold. On the one hand, it is an important problem in artificial intelligence and image processing. On the other hand, publicly available CAPTCHAs that are not tested against state of the art machine recognition algorithms may make the systems vulnerable to attack by software bots.
37|-||Anonymous authentication for privacy-preserving IoT target-driven applications|The Internet of Things (IoT) will be formed by smart objects and services interacting autonomously and in real-time. As an application scenario, household smart meters will provide real-time neighborhood information which enables a smart community to cooperatively identify patterns, adapt consumption and improve overall quality of life, making the shared environment more sustainable. There is, in these types of settings, a major need toward securing all communications, placing equal effort on guaranteeing privacy properties (e.g., participant anonymity, unlinkability) as on assuring security properties (e.g., content authenticity). In this article, we present a fully decentralized anonymous authentication protocol aimed at encouraging the implementation of privacy-preserving IoT target-driven applications. The system is set up by an ad-hoc community of decentralized founding nodes. From then on, nodes can interact, being participants of cyber-physical systems, preserving full anonymity. We also present a performance and security analysis of the proposed system.
37|-||Efficient authentication for fast handover in wireless mesh networks|We propose new authentication protocols to support fast handover in IEEE 802.11-based wireless mesh networks. The authentication server does not need to be involved in the handover authentication process. Instead, mesh access points directly authenticate mobile clients using tickets, avoiding multi-hop wireless communications in order to minimize the authentication delay. Numerical analysis and simulation results show that the proposed handover authentication protocol significantly outperforms IEEE 802.11 authentication in terms of authentication delay.
37|-||Privacy-preserving publishing of opinion polls|Public opinion is the belief or thoughts of the public regarding a particular topic, especially one regarding politics, religion or social issues. Opinions may be sensitive since they may reflect a person's perspective, understanding, particular feelings, way of life, and desires. On one hand, public opinion is often collected through a central server which keeps a user profile for each participant and needs to publish this data for research purposes. On the other hand, such publishing of sensitive information without proper de-identification puts individuals' privacy at risk, thus opinions must be anonymized prior to publishing. While many anonymization approaches for tabular data with single sensitive attribute have been introduced, the proposed approaches do not readily apply to opinion polls. This is because opinions are generally collected on many issues, thus opinion databases have multiple sensitive attributes. Finding and enforcing anonymization models that work on datasets with multiple sensitive attributes while allowing risk analysis on the publisher side is not a well-studied problem. In this work, we identify the privacy problems regarding public opinions and propose a new probabilistic privacy model MSA-diversity, specifically defined on datasets with multiple sensitive attributes. We also present a heuristic anonymization technique to enforce MSA-diversity. Experimental results on real data show that our approach clearly outperforms the existing approaches in terms of anonymization accuracy.
37|-||Security as a theoretical attribute construct|This paper provides an overview of the field of security metrics and discusses results of a survey of security experts on the topic. It describes a new framework for developing security metrics that focuses on effectiveness measures while maintaining measures of correctness. It introduces a view of security as a theoretical concept which encapsulates multiple aspects of a system. Viewing security as a theoretical attribute construct promotes the recognition that multiple characteristics and features of a system are required to make it secure. The view also motivates a sharp focus on system aspects which exhibit a measurable security attribute. The framework is illustrated with a case study.
37|-||Enhancing IDS performance through comprehensive alert post-processing|Intrusion detection systems (IDS) are among the most common countermeasures against network attacks. In order to improve the alerts obtained from them, various methods of post-processing have been proposed. These methods usually try to alleviate specific drawbacks of intrusion detection. We propose a system that is a post-processing solution. The input of our system is a set of multiple IDS sensors alert sets. Each set's alerts are aggregated in order to improve their quality, before multiple alert sets merge into one general alert set. Then, a low clustering procedure allows the system to hypothesize about missed security events and to create relevant alerts. The main clustering phase comes next, before the final step, in which a clusters graph is generated to produce a high level presentation of the security events. The system has been tested using the DARPA 2000 dataset, as well as a live network dataset, and has produced satisfactory results.
37|-||Onion routing circuit construction via latency graphs|The use of anonymity-based infrastructures and anonymisers is a plausible solution to mitigate privacy problems on the Internet. Tor (short for The onion router) is a popular low-latency anonymity system that can be installed as an end-user application on a wide range of operating systems to redirect the traffic through a series of anonymising proxy circuits. The construction of these circuits determines both the latency and the anonymity degree of the Tor anonymity system. While some circuit construction strategies lead to delays which are tolerated for activities like Web browsing, they can make the system vulnerable to linking attacks. We evaluate in this paper three classical strategies for the construction of Tor circuits, with respect to their de-anonymisation risk and latency performance. We then develop a new circuit selection algorithm that considerably reduces the success probability of linking attacks while keeping a good degree of performance. We finally conduct experiments on a real-world Tor deployment over PlanetLab. Our experimental results confirm the validity of our strategy and its performance increase for Web browsing.
37|-||Secloud: A cloud-based comprehensive and lightweight security solution for smartphones|As smartphones are becoming more complex and powerful to provide better functionalities, concerns are increasing regarding security threats against their users. Since smartphones use a software architecture similar to PCs, they are vulnerable to the same classes of security risks. Unfortunately, smartphones are constrained by their limited resources that prevent the integration of advanced security monitoring solutions that work with traditional PCs. We propose Secloud, a cloud-based security solution for smartphone devices. Secloud emulates a registered smartphone device inside a designated cloud and keeps it synchronized by continuously passing the device inputs and network connections to the cloud. This allows Secloud to perform a resource-intensive security analysis on the emulated replica that would otherwise be infeasible to run on the device itself. We demonstrate the practical feasibility of Secloud through a prototype for Android devices and illustrate its resource effectiveness by comparing it with on-device solutions.
38|-|http://www.sciencedirect.com/science/journal/01674048/38|Contents|
38|-||Cybercrime in the Digital Economy - Editorial|
38|-||Selecting a Cloud Service Provider in the age of cybercrime|The benefits of resorting to the cloud, as an efficient way to provide services, have long been recognised in the academic and industrial literature. However, as more and more companies are beginning to embrace the trend, it has also become clearer that the model offers unprecedented opportunities to cybercriminals: either by enabling them to compromise a myriad of services in a single shot or by allowing cyber-criminals to amplify their capabilities through a leverage of the technology offered by the cloud.
38|-||Smart control of operational threats in control substations|
38|-||Investigating phishing victimization with the HeuristicâSystematic Model: A theoretical framework and an exploration|To the extent that phishing has become a serious threat to information security, there has been rather limited theory-grounded research on this burgeoning phenomenon. In this paper, we develop a theoretical model of victimization by phishing based on the Heuristic–Systematic Model of information processing. We argue that the Heuristic–Systematic Model offers an ideal theoretical framework for investigating the psychological mechanism underlying the effectiveness of phishing attacks. An exploratory experiment is presented to validate the research model based on the theory.
38|-||A game theoretic defence framework against DoS/DDoS cyber attacks|Game-theoretic approaches have been previously employed in the research area of network security in order to explore the interaction between an attacker and a defender during a Distributed Denial of Service (DDoS) attack scenario. Existing literature investigates payoffs and optimal strategies for both parties, in order to provide the defender with an optimal defence strategy. In this paper, we model a DDoS attack as a one-shot, non-cooperative, zero-sum game. We extend previous work by incorporating in our model a richer set of options available to the attacker compared to what has been previously achieved. We investigate multiple permutations in terms of the cost to perform an attack, the number of attacking nodes, malicious traffic probability distributions and their parameters. We analytically demonstrate that there exists a single optimal strategy available to the defender. By adopting it, the defender sets an upper boundary to attacker payoff, which can only be achieved if the attacker is a rational player. For all other attack strategies (those adopted by irrational attackers), attacker payoff will be lower than this boundary. We preliminary validate this model via simulations with the ns2 network simulator. The simulated environment replicates the analytical model's parameters and the results confirm our model's accuracy.
38|-||Smartphone sensor data as digital evidence|The proliferation of smartphones introduces new opportunities in digital forensics. One of the reasons is that smartphones are usually equipped with sensors (e.g. accelerometer, proximity sensor, etc.), hardware which can be used to infer the user's context. This context may be useful in a digital investigation, as it can aid in the rejection or acceptance of an alibi, or even reveal a suspect's actions or activities. Nonetheless, sensor data are volatile, thus are not available in post-mortem analysis. Thus, the only way to timely acquire them, in case such a need arises during a digital investigation, is by software that collects them when they are generated by the suspect's actions. In this paper we examine the feasibility of ad-hoc data acquisition from smartphone sensors by implementing a device agent for their collection in Android, as well as a protocol for their transfer. Then, we discuss our experience regarding the data collection of smartphone sensors, as well as legal and ethical issues that arise from their collection. Finally, we describe scenarios regarding the agent's preparation and use in a digital investigation.
38|-||Assessing the genuineness of events in runtime monitoring of cyber systems|Monitoring security properties of cyber systems at runtime is necessary if the preservation of such properties cannot be guaranteed by formal analysis of their specification. It is also necessary if the runtime interactions between their components that are distributed over different types of local and wide area networks cannot be fully analyzed before putting the systems in operation. The effectiveness of runtime monitoring depends on the trustworthiness of the runtime system events, which are analyzed by the monitor. In this paper, we describe an approach for assessing the trustworthiness of such events. Our approach is based on the generation of possible explanations of runtime events based on a diagnostic model of the system under surveillance using abductive reasoning, and the confirmation of the validity of such explanations and the runtime events using belief based reasoning. The assessment process that we have developed based on this approach has been implemented as part of the EVEREST runtime monitoring framework and has been evaluated in a series of simulations that are discussed in the paper.
38|-||From information security to cyber security|The term cyber security is often used interchangeably with the term information security. This paper argues that, although there is a substantial overlap between cyber security and information security, these two concepts are not totally analogous. Moreover, the paper posits that cyber security goes beyond the boundaries of traditional information security to include not only the protection of information resources, but also that of other assets, including the person him/herself. In information security, reference to the human factor usually relates to the role(s) of humans in the security process. In cyber security this factor has an additional dimension, namely, the humans as potential targets of cyber attacks or even unknowingly participating in a cyber attack. This additional dimension has ethical implications for society as a whole, since the protection of certain vulnerable groups, for example children, could be seen as a societal responsibility.
38|-||Integrated digital forensic process model|Digital forensics is an established research and application field. Various process models exist describing the steps and processes to follow during digital forensic investigations. During such investigations, it is not only the digital evidence itself that needs to prevail in a court of law; the process followed and terminology used should also be rigorous and generally accepted within the digital forensic community. Different investigators have been refining their own investigative methods, resulting in a variety of digital forensic process models. This paper proposes a standardized Digital Forensic Process Model to aid investigators in following a uniform approach in digital forensic investigations.
39|PA|http://www.sciencedirect.com/science/journal/01674048/39/part/PA|Contents|
39|PA||Editorial|
39|PA||Botnet detection based on traffic behavior analysis and flow intervals|Botnets represent one of the most serious cybersecurity threats faced by organizations today. Botnets have been used as the main vector in carrying many cyber crimes reported in the recent news. While a significant amount of research has been accomplished on botnet analysis and detection, several challenges remain unaddressed, such as the ability to design detectors which can cope with new forms of botnets. In this paper, we propose a new approach to detect botnet activity based on traffic behavior analysis by classifying network traffic behavior using machine learning. Traffic behavior analysis methods do not depend on the packets payload, which means that they can work with encrypted network communication protocols. Network traffic information can usually be easily retrieved from various network devices without affecting significantly network performance or service availability. We study the feasibility of detecting botnet activity without having seen a complete network flow by classifying behavior based on time intervals. Using existing datasets, we show experimentally that it is possible to identify the presence of existing and unknown botnets activity with high accuracy even with very small time windows.
39|PA||Behavior-based tracking: Exploiting characteristic patterns inÂ DNS traffic|We review and evaluate three techniques that allow a passive adversary to track users who have dynamic IP addresses based on characteristic behavioral patterns, i.e., without cookies or similar techniques. For this purpose we consider 1-Nearest-Neighbor classifiers, a Multinomial Naïve Bayes classifier and pattern mining techniques based on the criteria support and lift.
39|PA||Covert communications through network configuration messages|Covert channels are a form of hidden communication that may violate the integrity of systems. Since their birth in Multi-Level Security systems in the early 70's they have evolved considerably, such that new solutions have appeared for computer networks mainly due to vague protocols specifications. In this paper we concentrate on short-range covert channels and analyze the opportunities of concealing data in various extensively used protocols today. From this analysis we observe several features that can be effectively exploited for subliminal data transmission in the Dynamic Host Configuration Protocol (DHCP). The result is a proof-of-concept implementation, HIDE_DHCP, which integrates three different covert channels each of which accommodate to different stealthiness and capacity requirements. Finally, we provide a theoretical and experimental analysis of this tool in terms of its reliability, capacity, and detectability.
39|PA||Enforcing dynamic write privileges in data outsourcing|Users and companies are more and more resorting to external providers for storing their data and making them available to others. Since data sharing is typically selective (i.e., accesses to certain data should be allowed only to authorized users), there is the problem of enforcing authorizations on the outsourced data. Recently proposed approaches based on selective encryption provide convenient enforcement of read privileges, but are not directly applicable for supporting write privileges.
39|PA||Management of stateful firewall misconfiguration|Firewall configurations are evolving into dynamic policies that depend on protocol states. As a result, stateful configurations tend to be much more error prone. Some errors occur on configurations that only contain stateful rules. Others may affect those holding both stateful and stateless rules. Such situations lead to configurations in which actions on certain packets are conducted by the firewall, while other related actions are not. We address automatic solutions to handle these problems. Permitted states and transitions of connection-oriented protocols (in essence, on any layer) are encoded as automata. Flawed rules are identified and potential modifications are provided in order to get consistent configurations. We validate the feasibility of our proposal based on a proof of concept prototype that automatically parses existing firewall configuration files and handles the discovery of flawed rules according to our approach.
39|PA||A framework for risk assessment in access control systems|We describe a framework for risk assessment specifically within the context of risk-based access control systems, which make authorization decisions by determining the security risk associated with access requests and weighing such security risk against operational needs together with situational conditions. Our framework estimates risk as a product of threat and impact scores. The framework that we describe includes four different approaches for conducting threat assessment: an object sensitivity-based approach, a subject trustworthiness-based approach and two additional approaches which are based on the difference between object sensitivity and subject trustworthiness. We motivate each of the four approaches with a series of examples. We also identify and formally describe the properties that are to be satisfied within each approach. Each of these approaches results in different threat orderings, and can be chosen based on the context of applications or preference of organizations. We also propose formulae to estimate the threat of subject–object accesses within each of the four approaches of our framework.
39|PA||Breaking and fixing the Android Launching Flow|The security model of the Android OS is based on the effective combination of a number of well-known security mechanisms (e.g. statically defined permissions for applications, the isolation offered by the Dalvik Virtual Machine, and the well-known Linux discretionary access control model). Although each security mechanism has been extensively tested and proved to be effective in isolation, their combination may suffer from unexpected security flaws. We show that this is actually the case by presenting a severe vulnerability in Android related to the application launching flow. This vulnerability is based on a security flaw affecting a kernel-level socket (namely, the Zygote socket). We also present an exploit of the vulnerability that allows a malicious application to mount a severe Denial-of-Service attack that makes the Android devices become totally unresponsive. Besides explaining the vulnerability (which affects all versions of Android up to version 4.0.3) we propose two fixes. One of the two fixes has been adopted in the official release of Android, starting with version 4.1. We empirically assess the impact of the vulnerability as well as the efficacy of the countermeasures on the end user. We conclude by extending our security analysis to the whole set of sockets, showing that other sockets do not suffer from the same vulnerability as the Zygote one.
39|PB|http://www.sciencedirect.com/science/journal/01674048/39/part/PB|Contents|
39|PB||A privacy-aware continuous authentication scheme for proximity-based access control|Continuous authentication is mainly associated with the use of biometrics to guarantee that a resource is being accessed by the same user throughout the usage period. Wireless devices can also serve as a supporting technology for continuous authentication or even as a complete alternative to biometrics when accessing proximity-based services.
39|PB||A framework for continuous, transparent mobile device authentication|We address two distinct problems with de facto mobile device authentication, as provided by a password or sketch. Firstly, device activity is permitted on an all-or-nothing basis, depending on whether the user successfully authenticates at the beginning of a session. This ignores the fact that tasks performed on a mobile device have a range of sensitivities, depending on the nature of the data and services accessed. Secondly, users are forced to re-authenticate frequently due to the bursty nature that characterizes mobile device use. Owners react to this by disabling the mechanism, or by choosing a weak “secret”. To address both issues, we propose an extensible Transparent Authentication Framework that integrates multiple behavioral biometrics with conventional authentication to implement an effortless and continuous authentication mechanism. Our security and usability evaluation of the proposed framework showed that a legitimate device owner can perform all device tasks, while being asked to authenticate explicitly 67% less often than without a transparent authentication method. Furthermore, our evaluation showed that attackers are soon denied access to on-device tasks as their behavioral biometrics are collected. Our results support the creation of a working prototype of our framework, and provide support for further research into transparent authentication on mobile devices.
39|PB||Gait and activity recognition using commercial phones|This paper presents the results of applying gait and activity recognition on a commercially available mobile smartphone, where both data collection and real-time analysis was done on the phone. The collected data was also transferred to a computer for further analysis and comparison of various distance metrics and machine learning techniques. In our experiment 5 users created each 3 templates on the phone, where the templates were related to different walking speeds. The system was tested for correct identification of the user or the walking activity with 20 new users and with the 5 enrolled users. The activities are recognised correctly with an accuracy of over 99%. For gait recognition the phone learned the individual features of the 5 enrolled participants at the various walk speeds, enabling the phone to afterwards identify the current user. The new Cross Dynamic Time Warping (DTW) Metric gives the best performance for gait recognition where users are identified correctly in 89.3% of the cases and the false positive probability is as low as 1.4%.
39|PB||Don't make excuses! Discouraging neutralization to reduce IT policy violation|Past research on information technology (IT) security training and awareness has focused on informing employees about security policies and formal sanctions for violating those policies. However, research suggests that deterrent sanctions may not be the most powerful influencer of employee violations. Often, employees use rationalizations, termed neutralization techniques, to overcome the effects of deterrence when deciding whether or not to violate a policy. Therefore, neutralization techniques often are stronger than sanctions in predicting employee behavior. For this study, we examine “denial of injury,” “metaphor of the ledger,” and “defense of necessity” as relevant justifications for violating password policies that are commonly used in organizations as used in (Siponen and Vance, 2010). Initial research on neutralization in IS security has shown that results are consistent regardless of which type of neutralization is considered (Siponen and Vance, 2010). In this study, we investigate whether IT security communication focused on mitigating neutralization, rather than deterrent sanctions, can reduce intentions to violate security policies. Additionally, considering the effects of message framing in persuading individuals against security policy violations are largely unexamined, we predict that negatively-framed communication will be more persuasive than positively-framed communication. We test our hypotheses using the factorial survey method. Our results suggest that security communication and training that focuses on neutralization techniques is just as effective as communication that focuses on deterrent sanctions in persuading employees not to violate policies, and that both types of framing are equally effective.
39|PB||Dynamic traffic awareness statistical model for firewall performance enhancement|Firewall is considered to be one of the most important security components in today's IP network architectures. Firewall performance has a significant impact on the overall network performance. In this paper, we propose a mechanism to improve firewall performance, using network traffic behavior and packet filtering statistics. Upon certain threshold qualification (Chi-square test), the proposed mechanism allows optimizing the filtering rules order and their corresponding fields order according to the divergence of the traffic behavior. That is, if the firewall system is stable, then the same current filtering rules and/or rule-fields orders are used for filtering the next network traffic window. Otherwise, an update of the filtering rules and/or rule-fields orders is required for filtering the next network traffic window. The numerical results obtained by simulation demonstrate that the proposed mechanism allow to improve significantly the firewall performance in terms of cumulative packet processing time even for small security policies. This improvement is a result of the minimization of the overhead corresponding to the frequency of updating the rule/field structures, as well as of using the optimum traffic window size.
39|PB||A comprehensive study of multiple deductions-based algebraic trace driven cache attacks on AES|Existing trace driven cache attacks (TDCAs) can only analyze the cache events in the first two rounds or the last round of AES, which limits the efficiency of the attacks. Recently, Zhao et al. proposed the multiple deductions-based algebraic side-channel attack (MDASCA) to cope with the errors in leakage measurements and to exploit new leakage models. Their preliminary results showed that MDASCA can improve TDCAs and attack the AES implemented with a compact lookup table of 256 bytes. This paper performs a comprehensive study of MDASCA-based TDCAs (MDATDCA) on most of the AES implementations that are widely used. First, the key recovery in TDCA is depicted by an abstract model regardless of the specific attack techniques. Then, the previous work of TDCAs on AES is classified into three types and its limitations are analyzed. How to utilize the cache events with MDATDCA is presented and the overhead is also calculated. To evaluate MDATDCA on AES, this paper constructs a mathematical model to estimate the maximal number of leakage rounds that can be utilized and the minimal number of cache traces required for a successful MDATDCA. Extensive experiments are conducted under different implementations, attack scenarios and key lengths of AES. The experimental results are consistent with the theoretical analysis. Many improvements are achieved. For the first time, we show that TDCAs on AES-192 and AES-256 become possible with the MDATDCA technique. Our work attests that combining TDCAs with algebraic techniques is a very efficient way to improve cache attacks.
39|PB||Cyber-physical security metric inference in smart grid critical infrastructures based on system administrators' responsive behavior|To protect complex power-grid control networks, efficient security assessment techniques are required. However, efficiently making sure that calculated security measures match the expert knowledge is a challenging endeavor. In this paper, we present EliMet, a framework that combines information from different sources and estimates the extent to which a control network meets its security objective. Initially, EliMet passively observes system operators' online reactive behavior against security incidents, and accordingly refines the calculated security measure values. To make the values comply with the expert knowledge, EliMet actively queries operators regarding those states for which sufficient information was not gained during the passive observation. Finally, EliMet makes use of the estimated security measure values for predictive situational awareness by ranking potential cyber-physical contingencies that the security administrators should plan for upfront. Our experimental results show that EliMet can optimally make use of prior knowledge as well as automated inference techniques to minimize human involvement and efficiently deduce the expert knowledge regarding individual states of that particular system.
39|PB||AMTRAC: An administrative model for temporal role-based access control|Over the years, Role Based Access Control (RBAC) has received significant attention in system security and administration. The Temporal Role Based Access Control (TRBAC) model is an extension of RBAC that allows one to specify periodic enabling and disabling of roles in a role enabling base (REB). While decentralized administration and delegation of administrative responsibilities in large RBAC systems is managed using an administrative role based access control model like ARBAC97, no administrative model for TRBAC has yet been proposed. In this paper, we introduce such a model and name it AMTRAC (Administrative Model for Temporal Role based Access Control). AMTRAC defines a broad range of relations that control user-role assignment, role-permission assignment, role–role assignment and role enabling base assignment. Since the first three are similar to those in ARBAC97, the role enabling base assignment component has been discussed in detail in this paper. The different ways by which role enabling conditions of regular roles can be modified are first explained. We then show how to specify which of the administrative roles are authorized to modify the role enabling conditions of any regular role. An exhaustive set of commands for authorization enforcement along with their pre and postconditions is also presented. Together, this would facilitate practical deployment and security analysis of TRBAC systems.
39|PB||Caller-REP: Detecting unwanted calls with caller social strength|Voice over IP (VoIP) is a cost effective mechanism for telemarketers and criminals to generate bulk spam calls. A challenge in managing a VoIP network is to detect spam calls without user involvement or content analysis. In this paper we present a novel content independent, non-intrusive approach based on caller trust and reputation to block spam callers in a VoIP network. Our approach uses call duration, interaction rate, and caller out-degree distribution to establish a trust network between VoIP users and computes the global reputation of a caller across the network. Our approach uses historical information for automatically determining a global reputation threshold below which a caller is declared as socially non-connected and as a spammer. No VoIP data-set is available for testing the detection mechanism. We verify the accuracy of our approach with synthetic data that we generate by randomly varying the call duration, call rate, and out-degree distributions of spammers and legitimate users. This evaluation shows that our approach can automatically detect spam callers in a network. Our approach achieves a false positive rate of less than 10% and true positive rate of almost 80% in the first two days even in the presence of a significant number of spammers. This increases to a true positive rate of 99% and drops a false positive rate to less than 2% on the third day. In a network with no spammers, our approach achieves a false positive rate of less than 10%. In a network heavily saturated with more than 60% of spam callers, our approach achieves a true positive rate of 98% and no false positives. We compare the performance of our approach with a closely related spam detection approach named Call-Rank. The results show that our approach outperforms Call-Rank in terms of detection accuracy and detection time.
39|PB||An adaptive risk management and access control framework to mitigate insider threats|Insider Attacks are one of the most dangerous threats organizations face today. An insider attack occurs when a person authorized to perform certain actions in an organization decides to abuse the trust, and harm the organization. These attacks may negatively impact the reputation of the organization, its productivity, and may produce losses in revenue and clients. Avoiding insider attacks is a daunting task. While it is necessary to provide privileges to employees so they can perform their jobs efficiently, providing too many privileges may backfire when users accidentally or intentionally abuse their privileges. Hence, finding a middle ground, where the necessary privileges are provided and malicious usage are avoided, is necessary. In this paper, we propose a framework that extends the role-based access control (RBAC) model by incorporating a risk assessment process, and the trust the system has on its users. Our framework adapts to suspicious changes in users' behavior by removing privileges when users' trust falls below a certain threshold. This threshold is computed based on a risk assessment process that includes the risk due to inference of unauthorized information. We use a Coloured-Petri net to detect inferences. We also redefine the existing role activation problem, and propose an algorithm that reduces the risk exposure. We propose a methodology to help administrators managing inference threats. We present experimental evaluation to validate our work.
39|PB||Efficient intrusion detection using representative instances|Because of their feasibility and effectiveness, artificial intelligence-based intrusion detection systems attract considerable interest from researchers. However, when confronted with large-scale data sets, many artificial intelligence-based intrusion detection systems could suffer from a high computational burden, even though the feature selection method can help to reduce the computational complexity. To improve the efficiency, we propose a representative instance selection method to preprocess the original data set before training a classifier, which is independent of the learning algorithm that is used for constructing the intrusion detection system. In this study, a new metric is introduced to measure the representative power of an instance with respect to its class. Based on an implementation of representativeness, we select the most representative instance in each subset divided by a novel centroid-based partitioning strategy, and then, we utilise the result as training data to build various intrusion detection models efficiently. Experimental results on a labelled flow-based data set introduced in 2009 show that ANN, KNN, SVM and Liblinear learning with a largely reduced set of representative instances can not only achieve high efficiency in detecting network attacks but also provide comparable detection performance in terms of the detection rate, precision, F-score and accuracy, as compared with four corresponding classifiers built with the original large data set.
39|PB||SHADuDT: Secure hypervisor-based anomaly detection using danger theory|Intrusion Detection based upon learning methods is an attractive approach in research community. These researches have two critical concerns: secure information gathering and accurate detection method. Here we used system calls together with their arguments as a suitable pattern for describing behavior of each process. In security applications, these patterns must be collected safely, so we proposed SHADuDT, a secure and robust hypervisor-based architecture for system call intercepting and information gathering that utilizes the second generation of Artificial Immune Systems (AIS) as intrusion detection method. Generally intrusion detection based on AISs fall into two categories. The first generation of AIS is inspired from adaptive immune reactions but the second one that is called danger theory focuses on both of these reactions to build a more biologically-realistic model of Human Immune System.
39|PB||Analysis of dictionary methods for PIN selection|Personal Identification Numbers (PINs) are commonly used as an authentication mechanism. An important security requirement is that PINs should be hard to guess. On the other hand, remembering several random PINs can be difficult task for a user. We evaluate several dictionary-based methods of choosing the PIN. To assess their resistance to guessing attacks, we use entropy, covering of the PIN space, guesswork, marginal guesswork, and marginal success rate metrics. With respect to these metrics, most of the evaluated methods are far from ideal ones. Positive results are obtained by a more involved morphing method, and the technique of the reduced dictionary. We also discuss two methods for constructing easy to remember PIN words for randomly chosen PINs.
39|PB||Establishing initial trust in autonomous Delay Tolerant Networks without centralised PKI|A Delay Tolerant Network (DTN) is a network where nodes can be highly mobile, with long message delay times, forming dynamic and fragmented networks. Conventional centralised network security mechanisms are unsuitable in such networks, therefore distributed security solutions are more desirable in DTN implementations. Establishing effective trust in distributed systems with no centralised Public Key Infrastructure (PKI) such as the Pretty Good Privacy (PGP) scheme, usually requires human intervention. In this paper, we build and compare different decentralised trust systems for autonomous DTN. We utilise a public key distribution model based on the Web of Trust principle, and employ a simple Leverage of Common Friends (LCF) trust system to establish initial trust in autonomous DTN. We compare this system with two other scenarios (no trust and random trust) for autonomous establishment of initial trust. Comparisons are based on the time it takes to disperse the trust and resilience of the system against a malicious node distributing malicious and False Public Keys. Our results show that the LCF trust system mitigates the distribution of false malicious public keys by 40%. LCF takes 44% longer to distribute 50% of the public keys compared when using no trust system, but is 16% faster in comparison to the random trust method.
39|PB||A fast malware detection algorithm based on objective-oriented association mining|Objective-oriented association (OOA) mining has been successfully applied in malware detection. One problem of OOA mining is that the number of association rules is very large, and many of the rules are redundant and have little capacity to distinguish malware from benign files. This circumstance seriously affects the running speed of OOA for malware detection. In this paper, an API (Application Programming Interface)-based association mining method is proposed for detecting malware. To increase the detection speed of the OOA, different strategies are presented: to improve the rule quality, criteria for API selection are proposed to remove APIs that cannot become frequent items; to find association rules that have strong discrimination power, we define the rule utility to evaluate the association rules; and to improve the detection accuracy, a classification method based on multiple association rules is adopted. The experiments show that the proposed strategies can significantly improve the running speed of OOA. In our experiments the time cost for data mining is reduced by thirty-two percent, and the time cost for classification is reduced by fifty percent.
39|PB||An integrated framework combining Bio-Hashed minutiae template and PKCS15 compliant card for a better secure management of fingerprint cancelable templates|We address in this paper the problem of privacy in fingerprint biometric systems. Today, cancelable techniques have been proposed to deal with this issue. Ideally, such transforms are one-way. However, even if they are with provable security, they remain vulnerable when the user-specific key that achieves cancelability property is stolen. The prominence of the cancelable template confidentiality to maintain the irreversibility property was also demonstrated for many proposed constructions. To prevent possible coming attacks, it becomes important to securely manage this key as well as the transformed template in order to avoid them being leaked simultaneously and thus compromised. To better manage the user credentials of cancelable constructs, we propose a new solution combining a trusted architecture and a cancelable fingerprint template. Therefore, a Bio-Hashed minutiae template based on a chip matching algorithm is proposed. A pkcs15 compliant cancelable biometric system for fingerprint privacy preserving is implemented on a smartcard. This closed system satisfies the safe management of the sensitive templates. The proposed solution is proved to be well resistant to different attacks.
39|PB||Identifying android malicious repackaged applications by thread-grained system call sequences|Android security has become highly desirable since adversaries can easily repackage malicious codes into various benign applications and spread these malicious repackaged applications (MRAs). Most MRA detection mechanisms on Android focus on detecting a specific family of MRAs or requiring the original benign application to compare with the malicious ones. This work proposes a new mechanism, SCSdroid (System Call Sequence Droid), which adopts the thread-grained system call sequences activated by applications. The concept is that even if MRAs can be camouflaged as benign applications, their malicious behavior would still appear in the system call sequences. SCSdroid extracts the truly malicious common subsequences from the system call sequences of MRAs belonging to the same family. Therefore, these extracted common subsequences can be used to identify any evaluated application without requiring the original benign application. Experimental results show that SCSdroid falsely detected only two applications among 100 evaluated benign applications, and falsely detected only one application among 49 evaluated malicious applications. As a result, SCSdroid achieved up to 95.97% detection accuracy, i.e., 143 correct detections among 149 applications.
39|PB||Trust management system design for the Internet of Things: A context-aware and multi-service approach|This work proposes a new trust management system (TMS) for the Internet of Things (IoT). The wide majority of these systems are today bound to the assessment of trustworthiness with respect to a single function. As such, they cannot use past experiences related to other functions. Even those that support multiple functions hide this heterogeneity by regrouping all past experiences into a single metric. These restrictions are detrimental to the adaptation of TMSs to today's emerging M2M and IoT architectures, which are characterized with heterogeneity in nodes, capabilities and services. To overcome these limitations, we design a context-aware and multi-service trust management system fitting the new requirements of the IoT. Simulation results show the good performance of the proposed system and especially highlight its ability to deter a class of common attacks designed to target trust management systems.
39|PB||APFS: Adaptive Probabilistic Filter Scheduling against distributed denial-of-service attacks|Distributed denial-of-service (DDoS) attacks are considered to be among the most crucial security challenges in current networks because they significantly disrupt the availability of a service by consuming extreme amount of resource and/or by creating link congestions. One type of countermeasure against DDoS attacks is a filter-based approach where filter-based intermediate routers within the network coordinate with each other to filter undesired flows. The key to success for this approach is effective filter propagation and management techniques. However, existing filter-based approaches do not consider effective filter propagation and management. In this paper, we define three necessary properties for a viable DDoS solution: how to practically propagate filters, how to place filters to effective filter routers, and how to manage filters to maximize the efficacy of the defense. We propose a novel mechanism, called Adaptive Probabilistic Filter Scheduling (APFS), that effectively defends against DDoS attacks and also satisfies the three necessary properties. In APFS, a filter router adaptively calculates its own marking probability based on three factors: 1) hop count from a sender, 2) the filter router's resource availability, and 3) the filter router's link degree. That is, a filter router that is closer to attackers, has more available resources, or has more connections to neighbors inserts its marking with a higher probability. These three factors lead a victim to receive more markings from more effective filter routers, and thus, filters are quickly distributed to effective filter routers. Moreover, each filter router manages multiple filters using a filter scheduling policy that allows it to selectively keep the most effective filters depending on attack situations. Experimental results show that APFS has a faster filter propagation and a higher attack blocking ratio than existing approaches that use fixed marking probability. In addition, APFS has a 44% higher defense effectiveness than existing filter-based approaches that do not use a filter scheduling policy.
39|PB||Dual-use open source security software in organizations â Dilemma: Help or hinder?|Dual-use technology can be used for both peaceful and harmful purposes. While the new type of anonymous, invisible and devastating security threats (malware, worms and viruses) shape contemporary warfare, organizations are challenged by the undefined risks of open source dual-use security tools. The dual-use dilemma is very important. It has not received adequate academic focus: questions such as increased or decreased risk, facilitation of security breaches, and the impact on security awareness have not been clarified or studied. This research closes existing gaps by studying the open source dual-use security software challenges that organizations should consider when using this technology. We utilize a triangulation approach with three independent data sources to conduct a detailed analysis of this phenomenon. Our study has found that the dual-use technology has both positive and negative effects on information system security. The ease of use of the dual-use security software facilitates security breaches and enterprises are using vulnerable open source security libraries and frameworks to develop their own in-house applications. On a positive note, open source dual-use security software is used as a powerful defense tool against attackers. Our study also found that security awareness is the key to maintaining the right level of information security risk in the dual-use context. Dual-use can also be of a great help to organizations in leveraging their information system security.
39|PB||CISOs and organisational culture: Their own worst enemy?|Many large organisations now have a Chief Information Security Officer (CISO1). While it may seem obvious that their role is to define and deliver organisational security goals, there has been little discussion on what makes a CISO able to deliver this effectively. In this paper, we report the results from 5 in-depth interviews with CISOs, which were analysed using organisational behaviour theory. The results show that the CISOs struggle to gain credibility within their organisation due to: a perceived lack of power, confusion about their role identity, and their inability to engage effectively with employees. We conclude that as the CISO role continues to develop CISOs need to reflect on effective ways of achieving credibility in their organisations and, in particular, to work on communicating with employees and engaging them in security initiatives. We also identify a key responsibility for effective CISOs; that is to remove the blockages that prevent information security from becoming ‘business as usual’ rather than a specialist function. For researchers, our findings offer a new piece of the emerging picture of human factors in information security initiatives.
39|PB||Usage control in SIP-based multimedia delivery|The Session Initiation Protocol (SIP) is an application layer signaling protocol for the creation, modification and termination of multimedia sessions and VoIP calls with one or more participants. While SIP operates in highly dynamic environments, in the current version its authorization support is based on traditional access control models. The main problem these models face is that they were designed many years ago, and under some circumstances they tend to be inadequate in modern highly dynamic environments. Usage Control (UCON), instead, is a model that supports the same operations as traditional access control models do, but it further enhances them with novel ones. In previous work, an architecture supporting continuous authorizations in SIP, based on the UCON model, was presented. In this article, an authorization support implementing the whole UCON model, including authorizations, obligations and conditions, has been integrated in a SIP system. Moreover, a testbed has been set up to experimentally evaluate the performance of the proposed security mechanism.
39|PB||Deriving common malware behavior through graph clustering|Detection of malicious software (malware) continues to be a problem as hackers devise new ways to evade available methods. The proliferation of malware and malware variants requires new advanced methods to detect them. This paper proposes a method to construct a common behavioral graph representing the execution behavior of a family of malware instances. The method generates one common behavioral graph by clustering a set of individual behavioral graphs, which represent kernel objects and their attributes based on system call traces. The resulting common behavioral graph has a common path, called HotPath, which is observed in all the malware instances in the same family. The proposed method shows high detection rates and false positive rates close to 0%. The derived common behavioral graph is highly scalable regardless of new instances added. It is also robust against system call attacks.
39|PB||Inference attacks against trust-based onion routing: Trust degree to the rescue|Trust-based onion routing enhances anonymity protection by means of constructing onion circuits using trust-based routers. However, attackers who have the knowledge of a priori trust distributions are still capable of largely reducing the anonymity protected by trust-based circuits. The root cause is that these attackers have a high probability to guess the users who initiate trust-based circuits through the routers trusted by few other users (i.e., inference attacks). In this paper, we uncover trust degree, an essential feature of routing anonymity that is effective in defeating inference attacks but has been overlooked in the design of existing trust-based onion routing. We conduct an isolated model based analysis to understand why the trust degree is effective and how it can be used to resist inference attacks. Our major contributions are three-fold. First, we present a model to exclusively reason about inference attacks in trust-based onion routing. This model isolates the anonymity compromised by inference attacks from other attacks (e.g., correlation-like attacks), and hence derives an exclusive design space that reveals trust degree as the key feature against inference attacks. Second, to show the usefulness of our model, we design a new routing algorithm by taking into account of trust degree. Our algorithm can protect anonymity against inference attacks without sacrificing the capability against attackers' routers. Third, we compare trust-based routing algorithms with and without considering trust degree using real-world social networking datasets. These comparisons present evidence to confirm the effectiveness of trust degree in defeating inference attacks under real-world settings.
39|PB||Understanding the violation of IS security policy in organizations: An integrated model based on social control and deterrence theory|It is widely agreed that a large amount of information systems (IS) security incidents occur in the workplace because employees subvert existing IS Security Policy (ISSP). In order to understand the factors that constrain employees from deviance and violation of the organizational ISSP, past work has traditionally viewed this issue through the lens of formal deterrence mechanisms; we postulated that we could better explain employees' ISSP violation behaviours through considering both formal and informal control factors as well as through considering existing deterrence theory. We therefore developed a theoretical model based on both deterrence and social bond theories rooted in a social control perspective to better understand employee behaviour in this context. The model is validated using survey data of 185 employees. Our empirical results highlight that both formal and informal controls have a significant effect on employees' ISSP violation intentions. To be specific, employees' social bonding is found to have mixed impacts on the employee's intention to violate ISSP. Social pressures exerted by subjective norms and co-worker behaviours also significantly influence employees' ISSP violation intentions. In analyzing the formal sanctions, the perceived severity of sanctions was found to be significant while, perceived certainty of those sanctions was not. We discuss the key implications of our findings for managers and researchers and the implications for professional practice.
39|PB||Creditability-based weighted voting for reducing false positives and negatives in intrusion detection|False positives (FPs) and false negatives (FNs) happen in every Intrusion Detection System (IDS). How often they occur is regarded as a measurement of the accuracy of the system. Frequent occurrences of FPs not only reduce the throughput of an IDS as FPs block the normal traffic and also degrade its trustworthiness. It is also difficult to eradicate all FNs from an IDS. One way to overcome the shortcomings of a single IDS is to employ multiple IDSs in its place and leverage the different capabilities and domain knowledge of these systems. Nonetheless, making a correct intrusion decision based on the outcomes of multiple IDSs has been a challenging task, as different IDSs may respond differently to the same packet trace. In this paper, we propose a method to reduce FPs and FNs by applying a creditability-based weighted voting (CWV) scheme to the outcomes of multiple IDSs. First, the CWV scheme evaluates the creditability of each individual IDS by monitoring its response to a large collection of pre-recorded packet traces containing various types of intrusions. For each IDS, our scheme then assigns different weights to each intrusion type according to its FP and FN ratios. Later, after their operations, the outcomes of individual IDSs are merged using a weighted voting scheme. In benchmarking tests, our CWV-based multiple IDSs demonstrated significant improvement in accuracy and efficiency when compared with multiple IDSs employing an ordinary majority voting (MV) scheme. The accuracy is the percentage of whole traces that are determined accurately, while the efficiency indicates that the voting algorithm performs better on reducing both FP and FN ratios. The CWV scheme achieved 95% accuracy and 94% efficiency while the MV scheme produced only 66% accuracy and 41% efficiency; the average percentages of FP/FN reduction were 21% and 58% respectively.
39|PB||DNS amplification attack revisited|It is without doubt that the Domain Name System (DNS) is one of the most decisive elements of the Internet infrastructure; even a slight disruption to the normal operation of a DNS server could cause serious impairment to network services and thus hinder access to network resources. Hence, it is straightforward that DNS nameservers are constantly under the threat of Denial of Service (DoS) attacks. This paper presents a new, stealthy from the attacker's viewpoint, flavor of DNSSEC-powered amplification attack that takes advantage of the vast number of DNS forwarders out there. Specifically, for augmenting the amplification factor, the attacker utilizes only those forwarders that support DNSSEC-related resource records and advertize a large DNS size packet. The main benefits of the presented attack scenario as compared to that of the typical amplification attack are: (a) The revocation of the need of the aggressor to control a botnet, and (b) the elimination of virtually all traces that may be used toward disclosing the attacker's actions, true identity and geographical location. The conducted experiments taking into consideration three countries, namely Greece, Ireland and Portugal demonstrate that with a proper but simple planning and a reasonable amount of resources, a determined perpetrator is able to create a large torrent of bulky DNS packets towards its target. In the context of the present study this is translated to a maximum amplification factor of 44.
39|PB||Co-operative user identity verification using an Authentication Aura|IT usage today is typified by users that operate across multiple devices, including traditional desktop PCs, laptops, tablets and smartphones. As a consequence, users can regularly find themselves having a variety of devices open concurrently, and with even the most basic security in place, there is a resultant need to repeatedly authenticate, which can potentially represent a source of hindrance and frustration for the user. Building upon previous work by the authors that proposed a novel approach to user authentication, called an Authentication Aura, this paper investigates the latent security potential contained in surrounding devices in everyday life and how this may be used to augment security. An experiment has been undertaken to ascertain the technological infrastructure, devices and inert objects that surround individuals throughout the day to establish whether or not these items might be utilised within an authentication solution. The experiment involved twenty volunteers, over a 14-day period, and resulted in a dataset of 1.23 million recorded observations. Using the data provided by the experiment as a basis for a simulation, it investigated how confidence in the user's identity is influenced by these familiar everyday possessions and how their own authentication status can be ‘leveraged’ to negate the need to repeatedly manually authenticate. The simulation suggests a potential reduction of 74.04% in the daily number of required authentications for a user operating a device once every 30 min, with a 10-min screen lock in place. Ultimately, it confirms that during device activation it is possible to remove the need to authenticate with the Authentication Aura providing sufficient confidence.
40|-|http://www.sciencedirect.com/science/journal/01674048/40|Contents|
40|-||Editorial|
40|-||The passing of a pioneer|
40|-||Model checking authorization requirements in business processes|Business processes are usually expected to meet high level authorization requirements (e.g., Separation of Duty). Since violation of authorization requirements may lead to economic losses and/or legal implications, ensuring that a business process meets them is of paramount importance. Previous work showed that model checking can be profitably used to check authorization requirements in business processes. However, building formal models that simultaneously account for both the workflow and the access control policy is a time consuming and error-prone activity. In this paper we present a new approach to model checking authorization requirements in business processes that allows for the separate specification of the workflow and of the associated access control policy while retaining the ability to carry out a fully automatic analysis of the business process. To illustrate the effectiveness of the approach we describe its application to a Loan Origination Process subject to an RBAC access control policy featuring conditional permission assignments and delegation.
40|-||A comprehensive and efficacious architecture for detecting phishing webpages|Phishing is a web-based criminal act. Phishing sites lure sensitive information from naive online users by camouflaging themselves as trustworthy entities. Phishing is considered an annoying threat in the field of electronic commerce. Due to the short lifespan of phishing webpages and the rapid advancement of phishing techniques, maintaining blacklists, white-lists or employing solely heuristics-based approaches are not particularly effective. The impact of phishing can be largely mitigated by adopting a suitable combination of all these techniques. In this study, the characteristics of legitimate and phishing webpages were investigated in depth, and based on this analysis, we proposed heuristics to extract 15 features from such webpages. These heuristic results were fed as an input to a trained machine learning algorithm to detect phishing sites. Before applying heuristics to the webpages, we used two preliminary screening modules in this system. The first module, the preapproved site identifier, checks webpages against a private white-list maintained by the user, and the second module, the Login Form Finder, classifies webpages as legitimate when there are no login forms present. These modules help to reduce superfluous computation in the system and in addition reducing the rate of false positives without compromising on the false negatives. By using all of these modules, we are able to classify webpages with 99.8% precision and a 0.4% of false positive rate. The experimental results indicate that this method is efficient for protecting users from online identity attacks.
40|-||Security and compliance challenges in complex IT outsourcing arrangements: A multi-stakeholder perspective|Complex IT outsourcing arrangements promise numerous benefits such as increased cost predictability and reduced costs, higher flexibility and scalability upon demand. Organizations trying to realize these benefits, however, face several security and compliance challenges. In this article, we investigate the pressure to take action with respect to such challenges and discuss avenues toward promising responses. We collected perceptions on security and compliance challenges from multiple stakeholders by means of a series of interviews and an online survey, first, to analyze the current and future relevance of the challenges as well as potential adverse effects on organizational performance and, second, to discuss the nature and scope of potential responses. The survey participants confirmed the current and future relevance of the six challenges auditing clouds, managing heterogeneity of services, coordinating involved parties, managing relationships between clients and vendors, localizing and migrating data and coping with lack of security awareness. Additionally, they perceived these challenges as affecting organizational performance adversely in case they are not properly addressed. Responses in form of organizational measures were considered more promising than technical ones concerning all challenges except localizing and migrating data, for which the opposite was true. Balancing relational and contractual governance as well as employing specific client and vendor capabilities is essential for the success of IT outsourcing arrangements, yet do not seem sufficient to overcome the investigated challenges. Innovations connecting the technical perspective of utility software with the business perspective of application software relevant for security and compliance management, however, nourish the hope that the benefits associated with complex IT outsourcing arrangements can be realized in the foreseeable future whilst addressing the security and compliance challenges.
40|-||Towards a distributed secure in-vehicle communication architecture for modern vehicles|Modern automotive vehicles are becoming a collection of interconnected embedded subsystems, where the mechanical parts are controlled by electronic ones and the vehicle is transformed into a mobile information system. However, the industry standards for in-vehicle communication are not following long-established computer security policies. This trend not only makes vehicles prone to thefts and automated attacks, but also endangers passengers safety.
40|-||Extended DMTP: A new protocol for improved graylist categorization|The proportion of spam has significantly increased in recent years. This paper proposes an extended differentiated mail transfer protocol (extended DMTP, namely EDMTP) to deal with the problem that the graylist is unclearly categorized in DMTP. Considering the difficulty in promoting the common problem of protocol, we design EDMTP-based schemes on SMTA (sending Mail Transfer Agent) and RMTA (receiving Mail Transfer Agent). Our new schemes accord with open close principle and do not need changing the original mail infrastructure. In addition, we design experiments to compare EDMTP with DMTP. Simulation results demonstrate that EDMTP can reduce the number of envelopes and the proportion of spam envelopes and therefore significantly improve the performance of graylist. Moreover, compared to the current SMTP-based e-mail system, our proposed EDMTP-based e-mail system can effectively decrease the traffic usage of spam.
40|-||An advanced persistent threat in 3G networks: Attacking the home network from roaming networks|The HLR/AuC is considered to be one of the most important network elements of a 3G network. It can serve up to five million subscribers and at least one transaction with HLR/AuC is required for every single phone call or data session. This paper presents experimental results and observations that can be exploited to perform a novel distributed denial of service attack in 3G networks that targets the availability of the HLR/AuC. More specifically, first we present an experiment in which we identified and proved some zero-day vulnerabilities of the 3G network that can be exploited by malicious actors to mount various attacks. For the purpose of our experiment, we have used off-the-shelf infrastructure and software, without any specialized modification. Based on the observations of the experiment, we reveal an Advanced Persistent Threat (APT) in 3G networks that aims to flood an HLR/AuC of a mobile operator. We also prove that the discovered APT can be performed in a trivial manner using commodity hardware and software, which is widely and affordably available.
40|-||A framework for generating realistic traffic for Distributed Denial-of-Service attacks and Flash Events|An intrinsic challenge associated with evaluating proposed techniques for detecting Distributed Denial-of-Service (DDoS) attacks and distinguishing them from Flash Events (FEs) is the extreme scarcity of publicly available real-word traffic traces. Those available are either heavily anonymised or too old to accurately reflect the current trends in DDoS attacks and FEs. This paper proposes a traffic generation and testbed framework for synthetically generating different types of realistic DDoS attacks, FEs and other benign traffic traces, and monitoring their effects on the target. Using only modest hardware resources, the proposed framework, consisting of a customised software traffic generator, ‘Botloader’, is capable of generating a configurable mix of two-way traffic, for emulating either large-scale DDoS attacks, FEs or benign traffic traces that are experimentally reproducible. Botloader uses IP-aliasing, a well-known technique available on most computing platforms, to create thousands of interactive UDP/TCP endpoints on a single computer, each bound to a unique IP-address, to emulate large numbers of simultaneous attackers or benign clients.
40|-||Framework and principles for active cyber defense|This essay offers a broad view of active defense derived from the concept of active air and missile defense. This view admits a range of cyber defenses, many of which are widely deployed and considered essential in today's threat environment. Instead of equating active defense to hacking back, this wider interpretation lends itself to distinguishing different types of active defense and the legal and ethical issues they raise. The essay will review the concepts of active and passive air and missile defenses, apply them to cyberspace, describe a framework for distinguishing different types of active cyber defense, and finally suggest legal and ethical principles for conducting active cyber defense.
||||
volume|issue|url|title|abstract
41|-|http://www.sciencedirect.com/science/journal/01674048/41|Contents|
41|-||Editorial|
41|-||Consistency and enforcement of access rules in cooperative data sharing environment|In this paper we consider the situation where a set of enterprises need to collaborate to provide rich services to their clients. An enterprise may need information from several other collaborating parties to satisfy its business requirements. Such collaboration often requires controlled access to one another's data, which we assume is stored in standard relational form. We assume that a set of access rules is given to the parties to regulate the data sharing, and such rules are defined over the join operations over the relational data. It is expected that the access rules will be designed according to business needs of the involved enterprises and although some negotiation between them will be involved, only a comprehensive analysis of the rules can uncover all issues of consistency between rules and their adequacy in answering the authorized queries (which we call enforceability). In this paper, we provide such an analysis and provide algorithms for checking and removing inconsistency, checking for rule enforceability, and minimally updating the rules to ensure enforceability whenever possible using only the existing parties. The involvement of specialized third parties for consistency and enforcement purposes is not addressed in this paper.
41|-||CPBAC: Property-based access control model for secure cooperation in online social networks|The rapid growth of online social networks (OSNs) has brought a revolutionary change in the way geographically dispersed people interact and cooperate with each other toward achieving some common goals. Recently, new ways of ad-hoc cooperation have been demonstrated during the hurricane Irene and the earthquake in Japan. In such emergency situations, OSNs have already taken a significant role as alternative social media that support altruistic information sharing and cooperation among people. However, existing cooperation approaches have not been well-organized and are highly vulnerable to security threats such as a disclosure of users' identities and the leakage of other private data because of the lack of secure cooperation mechanisms. To support secure and effective cooperation in OSNs, in this paper, we propose the CPBAC (Community-centric Property Based Access Control) model, which extends the existing CRiBAC (Community-centric Role interaction Based Access Control) model for use in OSNs to support cooperation among users. To verify the feasibility of the proposed model, we have implemented a prototype application on Facebook and have demonstrated its applicability with two working examples.
41|-||Identifying hidden social circles for advanced privacy configuration|With the dramatic increase of users on social network websites, the needs to assist users to manage their large number of contacts as well as providing privacy protection become more and more evident. Unfortunately, limited tools are available to address such needs and reduce users' workload on managing their social relationships. To tackle this issue, we propose an approach to facilitate online social network users to group their contacts into social circles with common interests. Further, we leverage the social group practice to automate the privacy setting process for users who add new contacts or upload new data items. We evaluate our approach using real-world data collected through a user study. The study also includes an analysis of the properties that are most critical for privacy related decisions.
41|-||A formal proximity model for RBAC systems|To combat the threat of information leakage through pervasive access, researchers have proposed several extensions to the popular role-based access control (RBAC) model. Such extensions can incorporate contextual features, such as location, into the policy decision in an attempt to restrict access to trustworthy settings. In many cases, though, such extensions fail to reflect the true threat, which is the presence or absence of other users, rather than absolute locations. For instance, for location-aware separation of duty, it is more important to ensure that two people are in the same room, rather than in a designated, pre-defined location. Prox-RBAC was proposed as an extension to consider the relative proximity of other users with the help of a pervasive monitoring infrastructure. However, that work offered only an informal view of proximity, and unnecessarily restricted the domain to spatial concerns. In this work, we present a more rigorous definition of proximity based on formal topological relations. In addition, we show that this definition can be applied to several additional domains, such as social networks, communication channels, attributes, and time; thus, our policy model and language is more flexible and powerful than the previous work. In addition to proposing the model, we present a number of theoretical results for such systems, including a complexity analysis, templates for cryptographic protocols, and proofs of security features.
41|-||Representation and querying of unfair evaluations in social rating systems|Social rating systems are subject to unfair evaluations. Users may try to individually or collaboratively promote or demote a product. Detecting unfair evaluations, mainly massive collusive attacks as well as honest looking intelligent attacks, is still a real challenge for collusion detection systems. In this paper, we study the impact of unfair evaluations in online rating systems. First, we study the individual unfair evaluations and their impact on the reputation of people calculated by social rating systems. We then propose a method for detecting collaborative unfair evaluations, also known as collusion. The proposed model uses frequent itemset mining technique to detect the candidate collusion groups and sub-groups. We use several indicators to identify collusion groups and to estimate how destructive such colluding groups can be. The approaches presented in this paper have been implemented in prototype tools, and experimentally validated on synthetic and real-world datasets.
42|-|http://www.sciencedirect.com/science/journal/01674048/42|Contents|
42|-||Editorial|
42|-||Cancelable multi-biometrics: Mixing iris-codes based on adaptive bloom filters|In this work adaptive Bloom filter-based transforms are applied in order to mix binary iris biometric templates at feature level, where iris-codes are obtained from both eyes of a single subject. The irreversible mixing transform, which generates alignment-free templates, obscures information present in different iris-codes. In addition, the transform is parameterized in order to achieve unlinkability, implementing cancelable multi-biometrics. Experiments which are carried out on the IITD Iris Database version 1.0 confirm the soundness of the proposed approach, (1) maintaining biometric performance at equal error rates below 0.5% for different feature extraction methods and fusion scenarios and (2) achieving a compression of mixed templates down to 10% of original size.
42|-||Covert Computation â Hiding code in code through compile-time obfuscation|Recently, the concept of semantic-aware malware detection has been proposed in the literature. Instead of relying on a syntactic analysis (i.e., comparison of a program to pre-generated signatures of malware samples), semantic-aware malware detection tries to model the effects a malware sample has on the machine. Thus, it does not depend on a specific syntactic implementation. For this purpose a model of the underlying machine is used. While it is possible to construct more and more precise models of hardware architectures, we show that there are ways to implement hidden functionality based on side effects in the microprocessor that are difficult to cover with a model. In this paper we give a comprehensive analysis of side effects in the x86 architecture and describe an implementation concept based on the idea of compile-time obfuscation, where obfuscating transformations are applied to the code at compile time. Finally, we provide an evaluation based on a prototype implementation to show the practicability of our approach and estimate complexity and space overhead using actual malware samples.
42|-||Protecting organizational competitive advantage: A knowledge leakage perspective|The strategic management literature emphasizes the importance of protecting organizational knowledge and information, especially in terms of maintaining competitive advantage. We synthesized several mechanisms from the literature that organizations could deploy to protect their knowledge and information. An Australian field study investigated how and to what extent these mechanisms were deployed in 11 knowledge-intensive organizations. The study revealed surprising findings: firstly, there was no evidence of a systematic and comprehensive management approach to the identification and protection of knowledge assets. Approaches were often haphazard, driven in a bottom-up fashion with much of the responsibility delegated to individual employees and knowledge owners. Secondly, concerns about confidentiality of organizations' operational data (e.g., client details), often crowded out managerial attention to protecting organizations' own knowledge and information assets. Based on these observations, we outline several implications for future research, including the need for more comprehensive frameworks to address knowledge leakage from a strategic perspective.
42|-||WorSE: A Workbench for Model-based Security Engineering|IT systems with sophisticated security requirements increasingly apply problem-specific security policies for specifying, analyzing, and implementing security properties. Due to their key role for defining and enforcing strategic security concepts, security polices are extremely critical, and quality assets such as policy correctness or policy consistency are essential objectives in policy engineering.
42|-||Smartphone information security awareness: A victim of operational pressures|Smartphone information security awareness describes the knowledge, attitude and behaviour that employees apply to the security of the organisational information that they access, process and store on their smartphone devices. The surge in the number of smartphone devices connecting to organisational systems and used to process organisational data has enabled a new level of operational efficiency. While employees are aware of the benefits they enjoy by bringing their personal devices into the workplace, managers too are aware of the benefits of having a constantly connected workforce. Unfortunately, those aware of the risks to information security do not share an equal level of enthusiasm. These devices are owned by employees who are not adequately skilled to configure the security settings for acceptable security of that information. Moreover, routine information security awareness programmes, even if applied, gradually fade into the daily rush of operations from the day they are completed.
42|-||Evaluating the privacy of Android mobile applications under forensic analysis|In this paper, we investigate and evaluate through experimental analysis the possibility of recovering authentication credentials of mobile applications from the volatile memory of Android mobile devices. Throughout the carried experiments and analysis, we have, exclusively, used open-source and free forensic tools. Overall, the contribution of this paper is threefold. First, it thoroughly, examines thirteen (13) mobile applications, which represent four common application categories that elaborate sensitive users' data, whether it is possible to recover authentication credentials from the physical memory of mobile devices, following thirty (30) different scenarios. Second, it explores in the considered applications, if we can discover patterns and expressions that indicate the exact position of authentication credentials in a memory dump. Third, it reveals a set of critical observations regarding the privacy of Android mobile applications and devices.
42|-||An automated system for rapid and secure device sanitization|Public and private organizations face the challenges of protecting their networks from cyber-attacks, while reducing the amount of time and money spent on Information Technology. Organizations can reduce their expenditures by reusing server, switch and router hardware, but they must use reliable and efficient methods of sanitizing these devices before they can be redeployed. The sanitization process removes proprietary, sensitive or classified data, as well as persistent malware from a device prior to reuse. The Johns Hopkins University Applied Physics Laboratory has developed an automated, rapid, and secure method for sanitizing servers, switches and routers. This sanitization method was implemented and tested on several different types of network devices during the Cyber Measurement & Analysis Center project, which was funded under Phases I and II of the DARPA (2008) National Cyber Range program. The performance of the automated sanitization system was excellent with an order of magnitude reduction in the time required to sanitize servers, routers and switches, and a significant improvement in the effectiveness of the sanitization process through the addition of persistent malware removal.
42|-||Characterization and classification of malicious Web traffic|Web systems commonly face unique set of vulnerabilities and security threats due to their high exposure, access by browsers, and integration with databases. This study is focused on characterization and classification of malicious cyber activities aimed at Web systems. The empirical analysis is based on three datasets, each in duration of four to five months, collected by high-interaction honeypots which ran fully functional three-tier Web systems. We first explore the types and prevalence of malicious scans and attacks to Web systems, and the extent to which these malicious activities differ in different periods of time or on Web servers running different services. In addition to descriptive statistical analysis, we include an inferential statistical analysis of the malicious session attributes, such as duration, number of requests and bytes transferred in a session. Then, we use supervised machine learning methods to classify attacker activities to two classes: vulnerability scans and attacks. Our main observations include the following: (1) Some characteristics of the malicious Web traffic were invariant across different servers and time periods, such as for example the dominant use of the search-based strategy for attacking the servers and the heavy-tailed behavior of session attributes. (2) On the other side, servers running different services experienced almost complementary profiles of vulnerability scan and attack types. (3) Supervised learning methods efficiently distinguished attack sessions from vulnerability scan sessions, with high probability of detection and very low probability of false alarms. (4) Decision tree based methods J48 and PART performed better than SVM across all datasets. (5) Attacks differed from vulnerability scans only in a small number of session attributes; depending on the dataset, classification of malicious activities can be performed using from four to six features without significantly affecting learners' performance compared to when all 43 features were used.
42|-||Trusted Online Social Network (OSN) services with optimal data management|Online Social Network (OSN) services have rapidly grown into a wide network and offer users a variety of benefits. However, they also bring new threats and privacy issues to the community. Unfortunately, there are attackers that attempt to expose OSN users' private information or conceal the information that the user desire to share with other users. Therefore, in this research we develop a framework that can provide trusted data management in OSN services. We first define the data types in OSN services and the states of shared data with respect to Optimal, Under-shared, Over-shared, and Hybrid states. We also identify the facilitating, detracting, and preventive parameters that are responsible for the state transition of the data. In a reliable OSN service, we address that a user should be able to set up his or her desired level of information sharing with a certain group of other users. However, it is not always clear to the ordinary users how to determine how much information they should reveal to others. In order to support such a decision, we propose an approach for helping OSN users to determine their optimum levels of information sharing, taking into consideration the payoffs (potential Reward or Cost) based on the Markov decision process (MDP). As an extension of the MDP-based approach, we also introduce a game theoretic approach, considering the interactions of OSN users and attackers with conflicting interests whose decisions affect each other's. Finally, after developing the framework for the optimal data sharing on OSNs, we conduct several experiments with attack simulation based on the proposed ideas and discuss the results. Our proposed approach has the capability to allow a large amount of variables to be altered to suit particular setups that an organization might have.
42|-||TinyLock: Affordable defense against smudge attacks on smartphone pattern lock systems|A pattern lock system is a widely used graphical password mechanism in today's mobile computing environment. To unlock a smartphone, a user draws a memorized graphical pattern with a finger on a flat touchscreen whereas the finger actually leaves its oily residues, also called smudges, on the surface of the touchscreen. The smudges can be exploited by adversaries to reproduce the secret pattern. Unfortunately, however, security is still dependent on a user's behavior that is to carefully remove them after use. In this paper, we study an affordable defense to resist the smudge attacks without losing the ease-of-use property of the pattern lock system and without demanding user's attentional behavior after use. We present TinyLock as our main result. TinyLock is a simple tweak of the user interface under the existing pattern lock paradigm but it can effectively resist the smudge attacks. Furthermore, TinyLock can be more resilient to shoulder-surfing attacks than the contemporary pattern lock systems. Our user study shows that TinyLock can significantly improve security of the pattern lock system while incurring minimal cost increase in terms of unlocking time.
42|-||Achieving an effective, scalable and privacy-preserving data sharing service in cloud computing|Data sharing in the cloud, fueled by favorable trends in cloud technology, is emerging as a promising technique for allowing users to conveniently access data. However, the growing number of enterprises and customers who stores their data in cloud servers is increasingly challenging users' privacy and the security of data. This paper focuses on providing a dependable and secure cloud data sharing service that allows users dynamic access to their data. In order to achieve this, we propose an effective, scalable and flexible privacy-preserving data policy with semantic security, by utilizing ciphertext policy attribute-based encryption (CP-ABE) combined with identity-based encryption (IBE) techniques. In addition to ensuring robust data sharing security, our policy succeeds in preserving the privacy of cloud users and supports efficient and secure dynamic operations including, but not limited to, file creation, user revocation and modification of user attributes. Security analysis indicates that the proposed policy is secure under the generic bilinear group model in the random oracle model and enforces fine-grained access control, full collusion resistance and backward secrecy. Furthermore, performance analysis and experimental results show that the overheads are as light as possible.
42|-||Determining employee awareness using the Human Aspects of Information Security Questionnaire (HAIS-Q)|It is increasingly acknowledged that many threats to an organisation's computer systems can be attributed to the behaviour of computer users. To quantify these human-based information security vulnerabilities, we are developing the Human Aspects of Information Security Questionnaire (HAIS-Q). The aim of this paper was twofold. The first aim was to outline the conceptual development of the HAIS-Q, including validity and reliability testing. The second aim was to examine the relationship between knowledge of policy and procedures, attitude towards policy and procedures and behaviour when using a work computer. Results from 500 Australian employees indicate that knowledge of policy and procedures had a stronger influence on attitude towards policy and procedure than self-reported behaviour. This finding suggests that training and education will be more effective if it outlines not only what is expected (knowledge) but also provides an understanding of why this is important (attitude). Plans for future research to further develop and test the HAIS-Q are outlined.
42|-||GARS: Real-time system for identification, assessment and control of cyber grooming attacks|In this paper, the Grooming Attack Recognition System (GARS) is presented. The main objectives of GARS are the real-time identification, assessment and control of cyber grooming attacks in favor of child protection. The system utilizes the processes of document classification, personality recognition, user history and exposure time recording to calculate specific risks children are exposed to during chat conversations. The above processes are repeated after each new message and three of them feed corresponding fuzzy logic controllers that provide particular but homogenized risk values as outputs. The weighted sum of the particular risk values results in a total value that indicates the current cyber grooming risk the child is exposed to, as the conversation evolves. Depending on predefined thresholds, the total risk value can be used to trigger alarms for various scopes (children, parents, etc). The practical use of GARS is demonstrated with a case study based on real grooming dialogs. Furthermore, an evaluation of the proposed approach through the discussion of applicability and performance results is discussed.
42|-||Automatic Defense Against Zero-day Polymorphic Worms in Communication Networks|
43|-|http://www.sciencedirect.com/science/journal/01674048/43|Contents|
43|-||Editorial|
43|-||Mobile malware detection through analysis of deviations in application network behavior|In this paper we present a new behavior-based anomaly detection system for detecting meaningful deviations in a mobile application's network behavior. The main goal of the proposed system is to protect mobile device users and cellular infrastructure companies from malicious applications by: (1) identification of malicious attacks or masquerading applications installed on a mobile device, and (2) identification of republished popular applications injected with a malicious code (i.e., repackaging). More specifically, we attempt to detect a new type of mobile malware with self-updating capabilities that were recently found on the official Google Android marketplace. Malware of this type cannot be detected using the standard signatures approach or by applying regular static or dynamic analysis methods. The detection is performed based on the application's network traffic patterns only. For each application, a model representing its specific traffic pattern is learned locally (i.e., on the device). Semi-supervised machine-learning methods are used for learning the normal behavioral patterns and for detecting deviations from the application's expected behavior. These methods were implemented and evaluated on Android devices. The evaluation experiments demonstrate that: (1) various applications have specific network traffic patterns and certain application categories can be distinguished by their network patterns; (2) different levels of deviation from normal behavior can be detected accurately; (3) in the case of self-updating malware, original (benign) and infected versions of an application have different and distinguishable network traffic patterns that in most cases, can be detected within a few minutes after the malware is executed while presenting very low false alarms rate; and (4) local learning is feasible and has a low performance overhead on mobile devices.
43|-||Evaluation model for knowledge sharing in information security professional virtual community|Knowledge sharing has been proven to have affirmative effects on both the education and business sectors. Nevertheless, many professional virtual communities (PVC) have failed due to reasons, such as the low willingness of members to share knowledge with other members. In addition, it is not explicitly evident whether knowledge sharing in information security is able to reduce risk. To date, there have been relatively few empirical studies concerning the effects of knowledge sharing and its capability to reduce risk in information security communities. This paper proposes a model that is composed of two main parts. The first part is the Triandis theory, which is adapted to understand and foster the determinants of knowledge sharing behavior in PVCs. The second part explores the quantitative relationship between knowledge sharing and security risk reduction expectation. One hundred and forty-two members from the LinkedIn information security groups participated in this study. PLS analysis shows that perceived consequences, affect, and facilitating conditions have significant effects on knowledge sharing behavior. In contrast, social factors have shown insignificant effects on knowledge sharing behavior in information security communities. The results of the study demonstrate that there is a positive and strong relationship between knowledge sharing behavior and information security risk reduction expectation.
43|-||On fingerprinting probing activities|Motivated by recent cyber attacks that were facilitated through probing, limited cyber security intelligence and the lack of accuracy that is provided by scanning detection systems, this paper presents a new approach to fingerprint probing activity. It investigates whether the perceived traffic refers to probing activities and which exact scanning technique is being employed to perform the probing. Further, this work strives to examine probing traffic dimensions to infer the ‘machinery’ of the scan; whether the probing is random or follows a certain predefined pattern; which probing strategy is being employed; and whether the probing activity is generated from a software tool or from a worm/bot. The approach leverages a number of statistical techniques, probabilistic distribution methods and observations in an attempt to understand and analyze probing activities. To prevent evasion, the approach formulates this matter as a change point detection problem that yielded motivating results. Evaluations performed using 55 GB of real darknet traffic shows that the extracted inferences exhibit promising accuracy and can generate significant insights that could be used for mitigation purposes.
43|-||Static analysis based invariant detection for commodity operating systems|Recent interest in runtime attestation requires modeling of a program's runtime behavior to formulate its integrity properties. In this paper, we study the possibility of employing static source code analysis to derive integrity models of a commodity operating systems kernel. We develop a precise and static analysis-based data invariant detection tool that overcomes several technical challenges: field-sensitivity, array-sensitivity, and pointer analysis. We apply our tool to Linux kernel 2.4.32 and Windows Research Kernel (WRK). For Linux kernel 2.4.32, our tool identifies 284,471 data invariants that are critical to its runtime integrity, e.g., we use them to detect ten real-world Linux rootkits. Furthermore, comparison with the result of a dynamic invariant detector reveals 17,182 variables that can cause false alarms for the dynamic detector in the constant invariants category. Our tool also works successfully for WRK and reports 202,992 invariants, which we use to detect nine real-world Windows malware and one synthetic Windows malware. When compared with a dynamic invariant detector, we see similar results in terms of false alarms. Our experience suggests that static analysis is a viable option for automated integrity property derivation, and it can have very low false positive rate and very low false negative rate (e.g., for the constant invariants of WRK, the false positive rate is one out of 100,822 and the false negative rate is 0.007% or seven out of 100,822).
43|-||An exploratory investigation of message-person congruence in information security awareness campaigns|In this study, we sought to answer the question of whether certain information security awareness message themes are more or less effective for different types of individuals based on their personality traits. We considered five message themes (deterrence, morality, regret, feedback, and incentive) as they relate to seven personality traits (the Big Five, Machiavellianism, and social desirability). Our survey analysis of 293 users provides evidence that security awareness message effectiveness does vary based on personality, but not always as one would expect. Depending on certain personality traits, some security messages appear beneficial to security efforts, whereas other personality traits make the individual less receptive to certain message types and therefore security messages may backfire in terms of achieving their intended effect. Our exploratory results can assist practitioners in identifying a best fit between security awareness themes and individual users based on their personality profile.
43|-||User identification and authentication using multi-modal behavioral biometrics|Biometric computer authentication has an advantage over password and access card authentication in that it is based on something you are, which is not easily copied or stolen. One way of performing biometric computer authentication is to use behavioral tendencies associated with how a user interacts with the computer. However, behavioral biometric authentication accuracy rates are worse than more traditional authentication methods. This article presents a behavioral biometric system that fuses user data from keyboard, mouse, and Graphical User Interface (GUI) interactions. Combining the modalities results in a more accurate authentication decision based on a broader view of the user's computer activity while requiring less user interaction to train the system than previous work. Testing over 31 users shows that fusion techniques significantly improve behavioral biometric authentication accuracy over single modalities on their own. Between the two fusion techniques presented, feature fusion and an ensemble based classification method, the ensemble method performs the best with a False Acceptance Rate (FAR) of 2.10% and a False Rejection Rate (FRR) 2.24%.
43|-||Information security knowledge sharing inÂ organizations: Investigating the effect ofÂ behavioral information security governance andÂ national culture|This paper presents an empirical investigation on what behavioral information security governance factors drives the establishment of information security knowledge sharing in organizations. Data was collected from organizations located in different geographic regions of the world, and the amount of data collected from two countries – namely, USA and Sweden – allowed us to investigate if the effect of behavioral information security governance factors on the establishment of security knowledge sharing differs based on national culture.
43|-||Nothing ventured, nothing gained. Profiles of online activity, cyber-crime exposure, and security measures of end-users in European Union|We use large-scale survey data from the Eurobarometer 77.2/2012 to explore variability in online activity, cyber-crime exposure, and security measures of end-users in European Union (EU27). While cyber-security is a high-priority activity for security experts and researchers, end-users conduct it in the context of their daily lives, as a socially accountable and resource-limited activity. We argue that end-users' security behaviors should be analyzed in relation to their experiences of online victimization, in the context of their routine activities. An ecological analysis at country level indicates that societies with widespread Internet use support cultures of higher cyber-security. They also expose daily Internet users to higher cyber-crime risks, but this positive correlation is weaker, with Romania and Hungary as two notable exceptions of high average exposure with low overall Internet use. Given the negative feedback loops between security responses, exposure to cyber-crime, and online activity, we find that, at individual level, linear causal modeling on survey data is impractical, and we propose classification analysis as a better tool for capturing variability. We use K-means cluster analysis to identify five types of end-users' orientation towards security in the context of their activity: ‘explorer’, ‘reactive’, ‘prudent’, ‘lucky’, and ‘occasional’ users, and we discuss their profiles of online activities and experiences. ‘Prudent’ users are relatively neglected in public campaigns for Internet security. Classification analysis is a productive tool for understanding end-users' security orientations through survey data and for informing public interventions.
43|-||On the adoption of anomaly detection for packed executable filtering|Malware packing is a common technique employed to hide malicious code and to avoid static analysis. In order to fully inspect the contents of the executable, unpacking techniques must be applied. Unfortunately, generic unpacking is computationally expensive. For this reason, it is important to filter binaries in order to correctly handle them. In previous work, we proposed the adoption of anomaly detection for the classification of packed and not packed binaries using features based on the Portable Executable structure. In this paper, we extend this work and thoroughly evaluate the method with a different dataset and two different feature sets, rendering new conclusions. While anomaly detection is reaffirmed as a sound method for the discrimination of packed and not packed binaries, Portable Executable structure based features present limitations to distinguish custom packed files from not packed files.
43|-||Obscuring users' identity in VoIP/IMS environments|Next Generation Networks bring together wired and wireless architectures, under the umbrella of an all IP architecture. Architectures such as the IP Multimedia Subsystem (IMS) offer advanced services at very low cost but also inherit IP infrastructure's security and privacy issues. The utilized signalling protocol (i.e. Session Initiation Protocol) and the related specifications are both overlooking users' privacy, leaving public and private identities unprotected to eavesdroppers. Existing solutions require either the existence of a public key infrastructure or the establishment of the appropriate mechanism for managing symmetric keys.
43|-||SoNeUCONABC, an expressive usage control model for Web-Based Social Networks|In the era of hyper-connectivity Web-Based Social Networks (WBSNs) are demanding applications. They facilitate the interaction of huge amounts of users and the development of appropriate Access Control Models (ACMs) is an arising necessity. Particularly, the development of WBSNs ACMs with expressive power and capable of managing access control along the whole usage process is the challenge pursued. To contribute on this issue, first, 23 proposals have been analysed and second, SoNeUCONABC, an expressive usage control model for WBSNs, is proposed. It extends UCONABC ( Park, 2003) including relationships management and it is formally defined, specifying entities and elements involved and an access control policy language. Moreover, policy construction is carefully detailed by using regular expressions and access control enforcement functions are described. Finally, the evaluation shows, theoretically, the significant expressive power of SoNeUCONABC and, empirically, the feasibility of its implementation by the development of a proof of concept system.
43|-||A Bug Hunter's Diary|
43|-||EFM: Enhancing the performance of signature-based network intrusion detection systems using enhanced filter mechanism|Signature-based network intrusion detection systems (NIDSs) have been widely deployed in current network security infrastructure. However, these detection systems suffer from some limitations such as network packet overload, expensive signature matching and massive false alarms in a large-scale network environment. In this paper, we aim to develop an enhanced filter mechanism (named EFM) to comprehensively mitigate these issues, which consists of three major components: a context-aware blacklist-based packet filter, an exclusive signature matching component and a KNN-based false alarm filter. The experiments, which were conducted with two data sets and in a network environment, demonstrate that our proposed EFM can overall enhance the performance of a signature-based NIDS such as Snort in the aspects of packet filtration, signature matching improvement and false alarm reduction without affecting network security.
43|-||Permission based Android security: Issues and countermeasures|Android security has been a hot spot recently in both academic research and public concerns due to numerous instances of security attacks and privacy leakage on Android platform. Android security has been built upon a permission based mechanism which restricts accesses of third-party Android applications to critical resources on an Android device. Such permission based mechanism is widely criticized for its coarse-grained control of application permissions and difficult management of permissions by developers, marketers, and end-users. In this paper, we investigate the arising issues in Android security, including coarse granularity of permissions, incompetent permission administration, insufficient permission documentation, over-claim of permissions, permission escalation attack, and TOCTOU (Time of Check to Time of Use) attack. We illustrate the relationships among these issues, and investigate the existing countermeasures to address these issues. In particular, we provide a systematic review on the development of these countermeasures, and compare them according to their technical features. Finally, we propose several methods to further mitigate the risk in Android security.
44|-|http://www.sciencedirect.com/science/journal/01674048/44|Contents|
44|-||Is Anti-virus Really Dead?|
44|-||A situation awareness model for information security risk management|Information security risk management (ISRM) is the primary means by which organizations preserve the confidentiality, integrity and availability of information resources. A review of ISRM literature identified deficiencies in the practice of information security risk assessment that inevitably lead to poor decision-making and inadequate or inappropriate security strategies. In this conceptual paper, we propose a situation aware ISRM (SA-ISRM) process model to complement the information security risk management process. Our argument is that the model addresses the aforementioned deficiencies through an enterprise-wide collection, analysis and reporting of risk-related information. The SA-ISRM model is adapted from Endsley's situation awareness model and has been refined using our findings from a case study of the US national security intelligence enterprise.
44|-||Survey of certificate usage in distributed access control|Access control is an important building block in many distributed applications, and several solutions, both centralised and distributed, have been proposed and used for such applications. Certificates are particularly well suited to distributed systems and have been used in several ways.
44|-||Implementing a database encryption solution, design and implementation issues|In this paper, we analyze and compare five traditional architectures for database encryption. We show that existing architectures may provide a high level of security, but have a significant impact on performance and impose major changes to the application layer, or may be transparent to the application layer and provide high performance, but have several fundamental security weaknesses. We suggest a sixth novel architecture that was not considered before. The new architecture is based on placing the encryption module inside the database management software (DBMS), just above the database cache, and using a dedicated technique to encrypt each database value together with its coordinates. These two properties allow our new architecture to achieve a high level of data security while offering enhanced performance and total transparency to the application layer. We also explain how each architecture can be implemented in a commercial, open source DBMS. We evaluate the performance of the various architectures both analytically and through extensive experimentation. Our performance evaluation results demonstrate that in most realistic scenarios, i.e., where only a part of the database content is stored in the database cache, the suggested architecture outperforms the others.
44|-||Hybrid k-Anonymity|Anonymization-based privacy protection ensures that published data cannot be linked back to an individual. The most common approach in this domain is to apply generalizations on the private data in order to maintain a privacy standard such as k-anonymity. While generalization-based techniques preserve truthfulness, relatively small output space of such techniques often results in unacceptable utility loss especially when privacy requirements are strict. In this paper, we introduce the hybrid generalizations which are formed by not only generalizations but also the data relocation mechanism. Data relocation involves changing certain data cells to further populate small groups of tuples that are indistinguishable with each other. This allows us to create anonymizations of finer granularity confirming to the underlying privacy standards. Data relocation serves as a tradeoff between utility and truthfulness and we provide an input parameter to control this tradeoff. Experiments on real data show that allowing a relatively small number of relocations increases utility with respect to heuristic metrics and query answering accuracy.
44|-||Securing Cloud and Mobility|
44|-||Control flow-based opcode behavior analysis for Malware detection|Opcode sequences from decompiled executables have been employed to detect malware. Currently, opcode sequences are extracted using text-based methods, and the limitation of this method is that the extracted opcode sequences cannot represent the true behaviors of an executable. To solve this issue, we present a control flow-based method to extract executable opcode behaviors. The behaviors extracted by this method can fully represent the behavior characteristics of an executable. To verify the efficiency of control flow-based behaviors, we perform a comparative study of the two types of opcode behavior analysis methods. The experimental results indicate that the proposed control flow-based method has a higher overall accuracy and a lower false positive rate.
44|-||Unintended disclosure of information: Inference attacks by third-party extensions to Social Network Systems|Popularity of Social Network Systems (SNSs) has significantly increased in recent years, raising serious concerns for the privacy of users. Such concerns arise partly because SNS providers allow third-party extensions to access their users' information through an Application Programming Interface (API). Typical permission-based protection mechanisms restrict direct access to user data. However, once an extension has been authorized by a user to access some data in a user’s profile, there is no more control on how that extension uses the data. A malicious extension may try to infer other information based on the legitimately accessible information. If an extension is not supposed to know the inferred information, then this information leakage process is called an inference attack. Due to the large number of users who subscribe to third-party extensions in SNSs, even an inference attack with only a moderate success rate can put the privacy of a large number of users at risk. In addition, inference attacks are not only a privacy violation, they could also be used as the building blocks for more dangerous security attacks, such as identity theft and phishing attacks. In this work, we conduct a comprehensive empirical study to assess the feasibility and accuracy of inference attacks that are launched from the extension API of SNSs. We devise an analytical framework for assessing the success rate of sample inference attacks, and discuss two further attack scenarios in which inference attacks are employed as building blocks. The significance of this work is in thoroughly discussing how inference attacks could happen in practice via the extension API of SNSs, and highlighting the clear and present danger of even the naively crafted inference attacks.
44|-||Propagation model of smartphone worms based on semi-Markov process and social relationship graph|Smartphone applications are getting more and more popular and pervasive in our daily life, and are also attractive to malware writers due to their limited computing source and vulnerabilities. At the same time, we possess limited understanding of our opponents in cyberspace. In this paper, we investigate the propagation model of SMS/MMS-based worms through integrating semi-Markov process and social relationship graph. In our modeling, we use semi-Markov process to characterize state transition among mobile nodes, and hire social network theory, a missing element in many previous works, to enhance the proposed mobile malware propagation model. In order to evaluate the proposed models, we have developed a specific software, and collected a large scale real-world data for this purpose. The extensive experiments indicate that the proposed models and algorithms are effective and practical.
44|-||Detecting SQL injection attacks using query result size|Web applications are becoming an essential part of our everyday lives, with many of our activities dependent on the functionality and security of these applications. Web applications are ubiquitous, perform mission critical tasks, and handle sensitive user data. As the scale of these applications grows, injection vulnerabilities, such as SQL injections, become major security challenges. Most of these vulnerabilities stem from a lack of input validation; that is, web applications use malicious input as part of a sensitive operation without properly checking or sanitizing the input values. SQL injection attacks target databases that are accessible through a web front-end; moreover, they take advantage of flaws in the input validation logic of web components. In this paper, we exhibit a novel scheme that automatically transforms web applications, rendering them safe against SQL injection attacks. Our technique dynamically analyzes the developer-intended query result size for any input, and detects attacks by comparing this against the result of the actual query. We implement this technique in a tool for protecting Java-based web applications. An experimental evaluation demonstrates that our technique is effective against SQL injection vulnerabilities.
45|-|http://www.sciencedirect.com/science/journal/01674048/45|Contents|
45|-||Taxonomy of intrusion risk assessment and response system|In recent years, we have seen notable changes in the way attackers infiltrate computer systems compromising their functionality. Research in intrusion detection systems aims to reduce the impact of these attacks. In this paper, we present a taxonomy of Intrusion Response Systems (IRS) and Intrusion Risk Assessment (IRA), two important components of an intrusion detection solution. We achieve this by classifying a number of studies published during the last two decades. We discuss the key features of existing IRS and IRA. We show how characterizing security risks and choosing the right countermeasures are an important and challenging part of designing an IRS and an IRA. Poorly designed IRS and IRA may reduce network performance and wrongly disconnect users from a network. We propose techniques on how to address these challenges and highlight the need for a comprehensive defense mechanism approach. We believe that this taxonomy will open up interesting areas for future research in the growing field of intrusion risk assessment and response systems.
45|-||Uniform DoS traceback|Denial of service (DoS) is a significant security challenge in the Internet. Identifying the attackers so that their attack traffic can be blocked at source is one strategy that can be used to mitigate DoS attacks. However, determining the source can be difficult due to the inherent connectionless nature of IP. Traceback using various marking schemes that overload, mostly unused, fields in the IP header are promising techniques to identify the source of the attack. This paper shows that the marking probability used in two existing techniques: probabilistic packet marking (PPM) and dynamic probabilistic packet marking (DPPM) are not optimal and derives an optimal marking scheme called uniform probabilistic packet marking (UPPM). The performance of UPPM is shown to be improved compared to PPM and DPPM by performing comparative numerical analysis. One significant advantage of UPPM over these earlier techniques is that it performs marking at the level of autonomous systems (ASs) rather than at every router. This has advantages both in terms of marking overhead and allowing the optimal formulation of marking probability by utilizing metrics readily available from BGP-4, the inter-AS routing protocol.
45|-||An approach for profiling phishing activities|Phishing attacks continue unabated to plague Internet users and trick them into providing personal and confidential information to phishers. In this paper, an approach for email-born phishing detection based on profiling and clustering techniques is proposed. We formulate the profiling problem as a clustering problem using various features present in the phishing emails as feature vectors and generate profiles based on clustering predictions. These predictions are further utilized to generate complete profiles of the emails. We carried out extensive experimental analysis of the proposed approach in order to evaluate its effectiveness to various factors such as sensitivity to the type of data, number of data sizes and cluster sizes. We compared the performance of the proposed approach against the Modified Global Kmeans (MGKmeans) approach. The results show that the proposed approach is efficient as compared to the baseline approach.
45|-||Information security incident management: Current practice as reported in the literature|This paper reports results of a systematic literature review on current practice and experiences with incident management, covering a wide variety of organisations. Identified practices are summarised according to the incident management phases of ISO/IEC 27035. The study shows that current practice and experience seem to be in line with the standard. We identify some inspirational examples that will be useful for organisations looking to improve their practices, and highlight which recommended practices generally are challenging to follow. We provide suggestions for addressing the challenges, and present identified research needs within information security incident management.
45|-||A multi-level approach to understanding the impact of cyber crime on the financial sector|This paper puts forward a multi-level model, based on system dynamics methodology, to understand the impact of cyber crime on the financial sector. Consistent with recent findings, our results show that strong dynamic relationships, amongst tangible and intangible factors, affect cyber crime cost and occur at different levels of society and value network. Specifically, shifts in financial companies' strategic priorities, having the protection of customer trust and loyalty as a key objective, together with considerations related to market positioning vis-à-vis competitors are important factors in determining the cost of cyber crime. Most of these costs are not driven by the number of cyber crime incidents experienced by financial companies but rather by the way financial companies choose to go about in protecting their business interests and market positioning in the presence of cyber crime. Financial companies' strategic behaviour as response to cyber crime, especially in regard to over-spending on defence measures and chronic under-reporting, has also an important consequence at overall sector and society levels, potentially driving the cost of cyber crime even further upwards. Unwanted consequences, such as weak policing, weak international frameworks for tackling cyber attacks and increases in the jurisdictional arbitrage opportunities for cyber criminals can all increase the cost of cyber crime, while inhibiting integrated and effective measures to address the problem.
45|-||Bluetooth Command and Control channel|Bluetooth is popular technology for short-range communications and is incorporated in mobile devices such as smartphones, tablet computers and laptops. Vulnerabilities associated with Bluetooth technology led to improved security measures surrounding Bluetooth connections. Besides the improvement in security features, Bluetooth technology is still plagued by vulnerability exploits. This paper explores the development of a physical Bluetooth C&C channel, moving beyond previous research that mostly relied on simulations. In order to develop a physical channel, certain requirements must be fulfilled and specific aspects regarding Bluetooth technology must be taken into consideration. To measure performance, the newly designed Bluetooth C&C channel is executed in a controlled environment using the Android operating system as a development platform. The results show that a physical Bluetooth C&C channel is indeed possible and the paper concludes by identifying potential strengths and weaknesses of the new channel.
45|-||On the security of text-based 3D CAPTCHAs|CAPTCHAs have become a standard security mechanism that are used to deter automated abuse of online services intended for humans. However, many existing CAPTCHA schemes to date have been successfully broken. As such, a number of CAPTCHA developers have explored alternative methods of designing CAPTCHAs. 3D CAPTCHAs is a design alternative that has been proposed to overcome the limitations of traditional CAPTCHAs. These CAPTCHAs are designed to capitalize on the human visual system's natural ability to perceive 3D objects from an image. The underlying security assumption is that it is difficult for a computer program to identify the 3D content. This paper investigates the robustness of text-based 3D CAPTCHAs. In particular, we examine three existing text-based 3D CAPTCHA schemes that are currently deployed on a number of websites. While the direct use of Optical Character Recognition (OCR) software is unable to correctly solve these text-based 3D CAPTCHA challenges, we highlight certain patterns in the 3D CAPTCHAs can be exploited to identify important information within the CAPTCHA. By extracting this information, this paper demonstrates that automated attacks can be used to solve these 3D CAPTCHAs with a high degree of success.
45|-||An empirical comparison of botnet detection methods|The results of botnet detection methods are usually presented without any comparison. Although it is generally accepted that more comparisons with third-party methods may help to improve the area, few papers could do it. Among the factors that prevent a comparison are the difficulties to share a dataset, the lack of a good dataset, the absence of a proper description of the methods and the lack of a comparison methodology. This paper compares the output of three different botnet detection methods by executing them over a new, real, labeled and large botnet dataset. This dataset includes botnet, normal and background traffic. The results of our two methods (BClus and CAMNEP) and BotHunter were compared using a methodology and a novel error metric designed for botnet detections methods. We conclude that comparing methods indeed helps to better estimate how good the methods are, to improve the algorithms, to build better datasets and to build a comparison methodology.
45|-||Ontology for attack detection: An intelligent approach to web application security|Conventional detection techniques struggle to keep up with the inherent complexity of web application design and hence the ever growing variety of attacks that can exploit it. Security frameworks modeled using an ontological approach are a promising new line of defense that can be highly effective in detecting zero day and sophisticated web application attacks because they can capture the context of the contents of information such as HTML pages or in-line scripts and have the ability to filter these contents by taking into consideration their consequences to the target applications. The goal of this article is to demonstrate how an ontology-engineering methodology may be systematically applied for designing and evaluating such security systems. A detailed ontological model is shown that caters to the generalized working of web applications, the underlying communication protocols and attacks. More specifically the proposed ontological model because it captures the context can not only detect HTTP protocol specification attacks but also helps focus only on specific portions of the request and response where a malicious script is possible. The model also captures the context of important attacks, the various technologies used by the hackers, source, target and vulnerabilities exploited by the attack, impact on system components and controls for mitigation. A comprehensive and best metrics suite for ontology evaluation has been used for assessing the quality of proposed model which includes correctness, accuracy, consistency, soundness, task orientation, completeness, conciseness, expandability, reusability, clarity, integrity, efficiency and expressiveness. The proposed model ranked well against the above mentioned metrics. Moreover a prototype attack detection system based upon the proposed model showed improved performance and detection rate and low rate of false positives while detecting OWASP's top ten listed web attacks.
45|-||Soft biometrics for keystroke dynamics: Profiling individuals while typing passwords|This paper presents a new profiling approach of individuals based on soft biometrics for keystroke dynamics. Soft biometric traits are unique representation of a person, which can be in a form of physical, behavioural or biological human characteristics that differentiate between him/her into a group people (e.g. gender, age, height, colour, race etc.). Keystroke dynamics is a behavioural biometric modality to recognise how a person types on a keyboard. In this paper, we consider the following soft traits: the hand category (i.e. if the user types with one or two hands), the gender category, the age category and the handedness category. For this purpose, we collected a new database. Two cases are studied: static passwords and free text. By combining machine learning and fusion process, the results are promising.
45|-||Performance evaluation of anomaly-detection algorithms for mouse dynamics|Mouse dynamics—the analysis of mouse operating behaviors to identify users—has been proposed for detecting impostors. Since many anomaly-detection algorithms have been proposed for this task, it is natural to ask how well these algorithms perform and how they compare with each other (e.g., to identify promising research directions). This paper presents a performance-evaluation study of a range of anomaly-detection algorithms in mouse dynamics on an equal basis. We collected a mouse-dynamics data set consisting of 17,400 samples from 58 subjects, developed a repeatable evaluation methodology, and implemented and evaluated 17 detectors from the mouse-dynamics and pattern-recognition literatures. Performance is measured in terms of detection accuracy, sensitivity to training sample size, usability with respect to sample length, and scalability with respect to the number of users (user space). The six top-performing detectors achieve equal-error rates between 8.81% and 11.63% with a detection time of 6.1 s; detector performance improves as training sample size and sample length increase and becomes saturated gradually; detector performance decreases as user space becomes large, but only small fluctuations with the error range are apparent when the space size exceeds a certain number. Along with the shared data and evaluation methodology, the results constitute a benchmark for comparing detectors and measuring progress.
45|-||Complexity is dead, long live complexity! How software can help service providers manage security and compliance|Service providers expected to see a simplification regarding security and compliance management as standards and best practice were applied to complex information technology (IT) outsourcing arrangements. However, security and compliance management became even more complex and is presenting greater challenges to service providers than ever before. In this article, we focus on the work practices of service providers dealing with complex and transitory security requirements and distributed IT infrastructures. Based on the results of semi-structured interviews followed by a think-aloud study, we first describe specific requirements to be met by software supporting security and compliance management in complex IT outsourcing arrangements, and discuss the extent to which existing software already meets them. We show that existing software, which is primarily designed for in-house settings, fails to meet requirements of complex IT outsourcing arrangements such as (1) the use of standardized and formal descriptions of security requirements and configurations, (2) the definition of a interface allowing to exchange messages and to delegate tasks, (3) the provision of mechanisms for designing and implementing a configuration for specific security requirements across organizational boundaries, (4) the provision of mechanisms for verifying and approving the enforcement of these security requirements, and (5) the provision of mechanisms for searching and browsing security requirements, configurations and links between them. We then propose a software architecture that claims to be capable of meeting those requirements and outline how this claim was evaluated by means of another think-aloud study in which potential end users were asked to perform a series of tasks using a prototypical implementation of the architecture. The results of the evaluation confirm that the software meets the described requirements and suggests that it facilitates the management of security and compliance in complex IT outsourcing arrangements.
45|-||A practical solution for sealed bid and multi-currency auctions|This paper introduces a sealed bid and multi-currency auction using secure multiparty computation (SMC). Two Boolean functions, a comparison and multiplication function, have been designed as required to apply SMC. These functions are applied without revealing any information, not even to trusted third parties such as the auctioneer. A type of Zero Knowledge proof, discreet proof, has been implemented with three variants, interactive, regular and reduced non interactive proofs. These proofs make it possible to verify the correctness of the functions whilst preserving the privacy of the bid values. Moreover, a system performance evaluation of the proposal has been realized on heterogeneous platforms, including a mobile platform. The evaluation concludes that our proposal is practical even on mobile platforms.
45|-||Location leakage in distance bounding: Why location privacy does not work|In many cases, we can only have access to a service by proving we are sufficiently close to a particular location (e.g. in automobile or building access control). In these cases, proximity can be guaranteed through signal attenuation. However, by using additional transmitters an attacker can relay signals between the prover and the verifier. Distance-bounding protocols are the main countermeasure against such attacks; however, such protocols may leak information regarding the location of the prover and/or the verifier who run the distance-bounding protocol.
45|-||Towards optimal noise distribution forÂ privacy preserving in data aggregation|In aggregation applications, individual privacy is a crucial factor to determine the effectiveness, for which the noise-addition method (i.e., a random noise value is added to the true value) is a simple yet powerful approach. However, improper additive noise could result in bias for the aggregate result. It demands an optimal noise distribution to reduce the deviation. In this paper, we develop a mathematical framework to derive the optimal noise distribution that provides privacy protection under the constraint of a limited value deviation. Specifically, we first derive a generic system dynamic function that the optimal noise distribution must satisfy, and further investigate the special case that the original values obey Gaussian distribution. Then we detailedly investigate the general cases that the original values obey arbitrary continuous distribution, which can be expressed by Gaussian Mixture Model (GMM). Our theoretical analysis suggests that for the Gaussian input Gaussian distribution is the optimal solution, and for the general input, the optimal solution is composed of infinite number of Gaussian components. We further find the general term formula of the components, which reduces the number of unknowns from infinite to three, i.e. the parameters of the first component (variance, expectation, weight). Based on it, we investigate the properties and propose an algorithm in order to calculate the asymptotically optimal solution composed of finite Gaussian components. The numerical evaluation shows that the results has little deviation to the optimal solutions.
45|-||Selection of Candidate Support Vectors in incremental SVM for network intrusion detection|In an Incremental Support Vector Machine classification, the data objects labelled as non-support vectors by the previous classification are re-used as training data in the next classification along with new data samples verified by Karush–Kuhn–Tucker (KKT) condition. This paper proposes Half-partition strategy of selecting and retaining non-support vectors of the current increment of classification – named as Candidate Support Vectors (CSV) – which are likely to become support vectors in the next increment of classification. This research work also designs an algorithm named the Candidate Support Vector based Incremental SVM (CSV-ISVM) algorithm that implements the proposed strategy and materializes the whole process of incremental SVM classification. This work also suggests modifications to the previously proposed concentric-ring method and reserved set strategy. Performance of the proposed method is evaluated with experiments and also by comparing it with other ISVM techniques. Experimental results and performance analyses show that the proposed algorithm CSV-ISVM is better than general ISVM classifications for real-time network intrusion detection.
45|-||HTTP attack detection using n-gram analysis|Previous research has shown that byte-level analysis of network traffic can be useful for network intrusion detection and traffic analysis. Such an approach does not require any knowledge of applications running on web servers or any pre-processing of incoming data.
45|-||Design guidelines for security protocols toÂ prevent replay & parallel session attacks|This work is concerned with the design of security protocols. These protocols are susceptible to intruder attacks and their security compromised if weaknesses in the protocols' design are evident. In this paper a new analysis is presented on the reasons why security protocols are vulnerable to replay and parallel session attack and based on this analysis a new set of design guidelines to ensure resistance to these attacks is proposed. The guidelines are general purpose so as to encompass a wide spectrum of security protocols.
45|-||Shadow IT â A view from behind the curtain|Shadow IT is a currently misunderstood and relatively unexplored phenomena. It represents all hardware, software, or any other solutions used by employees inside of the organisational ecosystem which have not received any formal IT department approval. But how much do we know about this phenomenon? What is behind the curtain? Is security in organisations jeopardised? In the research study reported here, we conducted an in-depth analysis of the organisational Shadow IT software database, reporting the view from behind the curtain. The study used triangulation approach to investigate the Shadow IT phenomena and its findings open Pandora's Box as they lay a new picture of what Shadow IT looks like from the software perspective. Our study revealed that greynet, content apps, and utility tools are the most used shadow systems. This study offers important insights on the Shadow IT phenomena for information management professionals and provides new research directions for academia.
45|-||A comparative analysis of detection metrics for covert timing channels|Methods to detect covert timing channels (CTCs) can be categorized into three broad classes: shape tests which include the Kolmogorov–Smirnov (KS) test, entropy tests which include first order entropy test, corrected conditional entropy (CCE) test, and Kullback–Leibler (KL) divergence test, and regularity tests. This paper contributes towards understanding and advancing the state-of-the-art of CTC detection methods. First, we present a detailed analysis of the performance of the well-known tests that are used to detect three main types of CTCs, namely, JitterBug, model-based CTC (MB-CTC) and time-replay CTC (TR-CTC). The performance analysis is carried out in an enterprise-like setting, employing large traffic traces. The detection methods are compared with respect to their applicability, computational complexity, and the classification rates for the three types of CTCs. In addition to evaluating the existing methods, we propose a new shape test based on the Welch's t-test and compare its performance with existing detection methods. We show that the classification rate of Welch's t-test is at least at par with other existing detection methods while having a relatively lower computational cost. The results also show that the Welch's t-test outperforms the CCE test in detecting JitterBug, while the CCE test has a better performance in detecting the TR-CTC. Furthermore, both tests perform comparably on the MB-CTC. Finally, we study the feasibility of using a multi-feature SVM classifier to increase the classification rate. We show that by combining the Welch's t-test we are able to increase the classification rate of MB-CTCs from 0.67 (using a single regularity measure) to 0.94.
45|-||Time and space interval record schedule consistency analysis for atomic items without interactions in open spaces with stationary locations|Attacks on systems often produce records that are distinguishable from normal records because, by the nature of the subversions they undertake, they produce records that the system could not produce under normal operation. This paper outlines a basis for understanding and determining one class of such discernible subversion inconsistencies associated with time and space interval record schedule consistency analysis for atomic items in open spaces without interactions as a method of questioned digital record examination. It starts with a brief introduction to the issues and description of the specific problem at hand, develops an approach to solving the problem, and identifies an algorithm for near-linear time detection of inconsistency or demonstration of a feasible schedule for special cases likely to occur in real-world record-keeping.
45|-||Securing cloud and mobility: A practitioner's guide|
46|-|http://www.sciencedirect.com/science/journal/01674048/46|Contents|
46|-||RAPID: Traffic-agnostic intrusion detection for resource-constrained wireless mesh networks|Due to the recent increased interest in wireless mesh networks (WMN), their security challenges have become of paramount importance. An important security mechanism for WMN, intrusion detection, has received considerable attention from the research community. Recent results show that traditional monitoring mechanisms are not applicable to real-world WMN due to their constrained resources (memory and processing power), which result in high false negative rates since only a few IDS functions can be activated on monitoring nodes. Cooperative solutions, on the other hand, have high communication overhead and detection delay when the traffic load is high. A practical traffic-aware IDS solution was recently proposed for resource-constrained WMN, however, traffic-awareness might not be feasible for some WMN applications. This article proposes a traffic-agnostic IDS solution that uses a link-coverage approach to monitor both local and backbone WMN traffic. Using real-world experiments and extensive simulations, we show that our proposed IDS solutions outperform traffic-aware IDS solutions while incurring lower computation and communication overhead.
46|-||Cyber situational awareness â A systematic review of the literature|Cyber situational awareness is attracting much attention. It features prominently in the national cyber strategies of many countries, and there is a considerable body of research dealing with it. However, until now, there has been no systematic and up-to-date review of the scientific literature on cyber situational awareness.
46|-||Toward a secure and usable cloud-based password manager for web browsers|Web users are confronted with the daunting challenges of creating, remembering, and using more and more strong passwords than ever before in order to protect their valuable assets on different websites. Password manager, particularly Browser-based Password Manager (BPM), is one of the most popular approaches designed to address these challenges by saving users' passwords and later automatically filling the login forms on behalf of users. Fortunately, all the five most popular Web browsers have provided password managers as a useful built-in feature. In this paper, we uncover the vulnerabilities of existing BPMs and analyze how they can be exploited by attackers to crack users' saved passwords. Moreover, we propose a novel Cloud-based Storage-Free BPM (CSF-BPM) design to achieve a high level of security with the desired confidentiality, integrity, and availability properties. We have implemented a CSF-BPM system into Firefox and evaluated its correctness, performance, and usability. Our evaluation results and analysis demonstrate that CSF-BPM can be efficiently and conveniently used. We believe CSF-BPM is a rational design that can also be integrated into other popular browsers to make the online experience of Web users more secure, convenient, and enjoyable.
46|-||Decision support for releasing anonymised data|For legal and privacy reasons it is often prescribed that data bases containing sensitive personal data can be published only in anonymised form. History shows, however, that the privacy of anonymised data in many cases is easily broken by de-anonymisation attacks. This paper defines guiding principles for decisions about releasing anonymised data and provides a simple process for analysing de-anonymisation risk and for making decisions about publishing anonymised personal data. At the heart of this process is an information-theoretic de-anonymisation feasibility limit that is independent of the details of both the anonymisation procedure and the adversarial de-anonymisation algorithms. This feasibility limit relates the adversarial mutual information of the anonymised data and the attacker's background information to the number of records in the anonymised data base and the acceptable risk of privacy violations. Based on this result, we explain, discuss and exemplify the process for making decisions about releasing anonymised data.
46|-||Enhancing the detection of metamorphic malware using call graphs|Malware stands for malicious software. It is software that is designed with a harmful intent. A malware detector is a system that attempts to identify malware using Application Programming Interface (API) call graph technique and/or other techniques. API call graph techniques follow two main steps, namely, transformation of malware samples into an API call graph using API call graph construction algorithm, and matching the constructed graph against existing malware call graph samples using graph matching algorithm. A major issue facing malware API call graph construction algorithms is building a precise call graph from information collected about malware samples. On the other hand call graph matching is an NP-complete problem and is slow because of computational complexity. In this study, a malware detection system based on API call graph is proposed. In the proposed system, each malware sample is represented as an API call graph. API call graph construction algorithm is used to transform input malware samples into API call graph by integrating API calls and operating system resource to represent graph nodes. Moreover, the dependence between different types of nodes is identified and represented using graph edges. After that, graph matching algorithm is used to calculate similarity between the input sample and malware API call graph samples that are stored in a database. The graph matching algorithm is based on an enhanced graph edit distance algorithm that simplifies the computational complexity using a greedy approach to select best common subgraphs from the integrating API call graph with high similarity, which helps in terms of detecting metamorphic malware. Experimental results on 514 malware samples demonstrate that the proposed system has 98% accuracy and 0 false positive rates. Detailed comparisons against other detection methods have been carried out and significant improvement over them is shown.
46|-||Stable web spam detection using features based on lexical items|Web spam is a method of manipulating search engines results by improving ranks of spam pages. It takes various forms and lacks a consistent definition. Web spam detectors use machine learning techniques to detect spam. However, the detectors are mostly verified on data sets coming from the same year as the learning sets. In this paper we compared Support Vector Machine classifiers trained and tested on WEBSPAM–UK data sets from different years. To obtain stable results we proposed new lexical-based features. The HTML document – transformed into a text without HTML tags, a set of visible symbols, and a list of links including the ones from tags – gave information about weird combinations of letters; consonant clusters; statistics on syllables, words, and sentences; and the Gunning Fog Index. Using data collected in 2006 as a learning set, we obtained very stable accuracy among years. This choice of the training set reduced the sensitivity in 2007, but that can be improved by managing the acceptance threshold. Finally, we proved that the balance between the sensitivity and the specificity measured by the Area Under the Curve (AUC) is improved by our selection of features.
46|-||An unsupervised anomaly-based detection approach for integrity attacks on SCADA systems|Supervisory Control and Data Acquisition (SCADA) systems are a core part of industrial systems, such as smart grid power and water distribution systems. In recent years, such systems become highly vulnerable to cyber attacks. The design of efficient and accurate data-driven anomaly detection models become an important topic of interest relating to the development of SCADA-specific Intrusion Detection Systems (IDSs) to counter cyber attacks. This paper proposes two novel techniques: (i) an automatic identification of consistent and inconsistent states of SCADA data for any given system, and (ii) an automatic extraction of proximity detection rules from identified states. During the identification phase, the density factor for the k-nearest neighbours of an observation is adapted to compute its inconsistency score. Then, an optimal inconsistency threshold is calculated to separate inconsistent from consistent observations. During the extraction phase, the well-known fixed-width clustering technique is extended to extract proximity-detection rules, which forms a small and most-representative data set for both inconsistent and consistent behaviours in the training data set. Extensive experiments were carried out both on real as well as simulated data sets, and we show that the proposed techniques provide significant accuracy and efficiency in detecting cyber attacks, compared to three well-known anomaly detection approaches.
46|-||New X.509-based mechanisms for fair anonymity management|Privacy has become a major concern in the Internet, resulting in an increased popularity of anonymizing systems aimed to protect users' identities. However, service providers sometimes interpret this anonymity as a risk, since dishonest users may take advantage of it. A possible solution is to create a practical implementation of fairness mechanisms to reach an equilibrium between anonymity and its different types of revocation. Furthermore, in order to reach a wide acceptance, any new mechanism must be easily deployable in current systems and must be adaptable (from the functionality perspective) to the needs that may arise in different situations. To that end, we propose a set of extensions to the CRL and OCSP procedures of the X.509 infrastructure, and a new protocol for easing the task of providing evidence of illegitimate actions. On one hand, the adaptability of our scheme relies on the already widely deployed X.509 infrastructure. On the other hand, the functionality provided by our proposal is mainly built upon group signatures, which gives it a vast variety of schemes to choose from, depending on the specific needs that may arise.
46|-||Stealing bandwidth from BitTorrent seeders|BitTorrent continues to comprise the largest fraction of Internet traffic. While significant progress has been made in understanding the BitTorrent choking mechanism, its security vulnerabilities have not been investigated thoroughly. This paper presents an experimental analysis of bandwidth attacks against different choking algorithms in the BitTorrent seed state. We reveal a simple exploit that allows malicious peers to receive a considerably higher download rate than contributing leechers, therefore introducing significant efficiency degradations for benign peers. We show the damage caused by the proposed attack in two different environments: a lab testbed comprising 32 peers and a PlanetLab testbed with 300 peers. Our results show that 3 malicious peers can degrade the download rate up to 414.99% for all peers. Combined with a Sybil attack that consists of as many attackers as leechers, it is possible to degrade the download rate by more than 1000%. We propose a novel choking algorithm which is immune against bandwidth attacks and a countermeasure against the revealed attack.
46|-||Cylindrical Coordinates Security Visualization for multiple domain command and control botnet detection|The botnets are one of the most dangerous species of network-based attack. They cause severe network disruptions through massive coordinated attacks nowadays and the results of this disruption frequently cost enterprises large sums in financial losses. In this paper, we make an in-depth investigation on the issue of botnet detection and present a new security visualization tool for visualizing botnet behaviors on DNS traffic. The core mechanism is developed with the objective of enabling users to recognize security threats promptly and mitigate the damages by only visualizing DNS traffic in cylindrical coordinates. We compare our visualization method with existing ones and the experimental results show that ours has greater perceptual efficiency. The ideas and results of this study will contribute toward designing an advanced visualization technique that offers better security. Also, the approach proposed in this study can be utilized to derive new and valuable insights in security aspects from the complex correlations of Big Data.
46|-||Security analysis of temporal RBAC under an administrative model|Security analysis of access control models is critical to confirm whether they ensure certain security properties. Administrative models specify the rules for state transition for any given access control model. While security analysis of role-based access control (RBAC) systems has been done using administrative models, work on security analysis of its temporal, spatial and spatio-temporal extensions has so far not considered the presence of any corresponding administrative model. In this paper, we present a methodology for performing security analysis of temporal RBAC (TRBAC) where state changes occur using the relations defined in a recently proposed administrative model named as AMTRAC (Administrative Model for Temporal Role-based Access Control). We initially define a number of security properties for TRBAC. These properties along with a representation of the TRBAC system and the administrative relations in AMTRAC are then formally specified using Alloy, a first order logic based language. Subsequently, validity of the specified properties is analyzed using the Alloy analyzer. We study the impact of the number of roles, users and temporal elements of TRBAC as well as various relations defined in AMTRAC on the time taken for security analysis.
46|-||Introduction to Computer and Network Security: Navigating Shades of Gray|
47|-|http://www.sciencedirect.com/science/journal/01674048/47|Contents|
47|-||Editorial: Special issue on trust in cyber, physical and social computing|
47|-||Analysis on the acceptance of Global Trust Management for unwanted traffic control based on game theory|The Internet has witnessed an incredible growth in its pervasive use and brought unprecedented convenience to its users. However, an increasing amount of unwanted traffic, such as spam and malware, severely burdens both users and Internet service providers (ISPs), which arouses wide public concern. A Global Trust Management (GTM) system was proposed and demonstrated to be accurate, robust and effective on unwanted traffic control in our previous work (22 and 23). But its acceptance by network entities (ISPs and hosts) is crucial to its practical deployment and final success. In this paper, we investigate the acceptance conditions of the GTM system using game theory. Considering the selfish nature of network entities, we address our problem as a social dilemma. To enhance cooperation among network entities, a public-goods-based GTM game is formulated with a trust-based punishment mechanism that can provide the incentives of behaving cooperatively for network entities. Meanwhile, the conditions of the adoption of GTM system are figured out. We also carry out a number of simulations to illustrate the acceptance conditions of the GTM system in practical deployment, and show the effectiveness of the trust-based punishment mechanism. Furthermore, suggestions for ISPs cooperating with antivirus vendors are put forward.
47|-||Evaluating and comparing the quality of access control in different operating systems|Access control mechanisms (ACMs) have been widely used by operating systems (OSes) to protect information security. However, it is often challenging to evaluate and compare the quality of protection (QoP) of ACMs, especially when they are deployed on different OS platforms. This article presents an approach to quantitatively measure and compare the quality of ACMs, which provides useful information to support OS administrators and users to choose ACMs that fit with their security needs.
47|-||CooPeD: Co-owned Personal Data management|With the spread of Web-Based Social Networks (WBSNs) managing access to data is a challenging matter. Providing personalized, fine-grained access control is essential to build trusted WBSNs. WBSNs data can be associated with owners and co-owners, namely users who upload the data and users who are linked to uploaded data respectively. Thus, a privacy-friendly WBSN must allow users the management of elements related to them. In this regard, CooPeD (Co-owned Personal Data management), a system that deals with co-ownership management of decomposable objects, is proposed. CooPeD is formed by a model and a mechanism. CooPeD is developed on the bases of SoNeUCONABC usage control model. Particularly, an extension of SoNeUCONABC is proposed to support co-ownership management by means of access control and administrative management. In CooPeD's mechanism objects, decomposed in parts, are attached to owners and co-owners who individually set their access control preferences. The evaluation of CooPeD consists of three parts. Firstly, a feasibility analysis for different architectures of CooPeD's model and mechanism, as well as of CooPeD's mechanism in Facebook is performed. Secondly, a prototype proves the feasibility of implementing CooPeD. Lastly, a survey study assesses the acceptance of CooPeD.
47|-||Effective detection of vulnerable and malicious browser extensions|Unsafely coded browser extensions can compromise the security of a browser, making them attractive targets for attackers as a primary vehicle for conducting cyber-attacks. Among others, the three factors making vulnerable extensions a high-risk security threat for browsers include: i) the wide popularity of browser extensions, ii) the similarity of browser extensions with web applications, and iii) the high privilege of browser extension scripts. Furthermore, mechanisms that specifically target to mitigate browser extension-related attacks have received less attention as opposed to solutions that have been deployed for common web security problems (such as SQL injection, XSS, logic flaws, client-side vulnerabilities, drive-by-download, etc.). To address these challenges, recently some techniques have been proposed to defend extension-related attacks. These techniques mainly focus on information flow analysis to capture suspicious data flows, impose privilege restriction on API calls by malicious extensions, apply digital signatures to monitor process and memory level activities, and allow browser users to specify policies in order to restrict the operations of extensions.
47|-||Happy faces considered trustworthy irrespective of perceiver's mood: Challenges to the mood congruency effect|Interpersonal trust is affected by the emotional states and facial expressions of the interacting parties. This study investigated the interactive (combined) effects of emotions and facial expressions by simultaneously manipulating both variables. Three laboratory experiments were conducted using a face evaluation task (i.e., choosing the most trustworthy face from a face pair) with happy, sad, and neutral faces. Experiment 1 tested the effect of facial expressions on trust judgments. Experiments 2 and 3 manipulated both facial expressions and perceiver's emotions. The results of the three experiments showed that happy faces were more often chosen as the most trustworthy relative to neutral and sad faces, and that perceiver's emotions did not influence trust judgment. These results indicated that other's facial expressions as dominant social cue would bias interpersonal trust, which might cause security issue in online social networks.
48|-|http://www.sciencedirect.com/science/journal/01674048/48|Contents|
48|-||Vulnerabilities and mitigation techniques toning in the cloud: A cost and vulnerabilities coverage optimization approach using Cuckoo search algorithm with LÃ©vy flights|Information and Communication Technology (ICT) security issues have been a major concern for decades. Today's ICT infrastructure faces sophisticated attacks using combinations of multiple vulnerabilities to penetrate networks with devastating impact. With the recent rise of cloud computing as a new utility computing paradigm, organizations have been considering it as a viable option to outsource major IT services in order to cut costs. Some organizations have opted for a private or hybrid cloud to take advantage of the emerging technologies and services. However, ICT security issues have to be appropriately mitigated. This research proposes a cloud security framework and an approach for vulnerabilities coverage and cost optimization using Cuckoo search algorithm with Lévy flights as random walks. The objective is to mitigate an identified set of vulnerabilities using a selected set of techniques when minimizing cost and maximizing coverage. The results show that Cloud Computing providers and organizations implementing cloud technology within their premises can effectively balance IT security coverage and cost using the proposed approach.
48|-||Implementing information security best practices on software lifecycle processes: The ISO/IEC 15504 Security Extension|The ISO/IEC 15504 international standard can be aligned with the ISO/IEC 27000 information security management framework. During the research conducted all the existing relations between ISO/IEC 15504-5 software development base practices and ISO/IEC 27002 security controls have been analysed and the ISO/IEC 15504 Security Extension has been developed. This extension details the changes that software companies should make in the software lifecycle processes for the successful implementation of the related security controls. To attain our research objectives, we evaluate the ISO/IEC 15504 Security Extension through case studies in a sample of software development organizations. This study follows the design science research paradigm that is based on constructive research.
48|-||Combating advanced persistent threats: From network event correlation to incident detection|An advanced persistent threat (also known as APT) is a deliberately slow-moving cyberattack that is applied to quietly compromise interconnected information systems without revealing itself. APTs often use a variety of attack methods to get unauthorized system access initially and then gradually spread throughout the network. In contrast to traditional attacks, they are not used to interrupt services but primarily to steal intellectual property, sensitive internal business and legal documents and other data. If an attack on a system is successful, timely detection is of paramount importance to mitigate its impact and prohibit APTs from further spreading. However, recent security incidents, such as Operation Shady Rat, Operation Red October or the discovery of MiniDuke – just to name a few – have impressively demonstrated that current security mechanisms are mostly insufficient to prohibit targeted and customized attacks. This paper therefore proposes a novel anomaly detection approach which is a promising basis for modern intrusion detection systems. In contrast to other common approaches, which apply a kind of black-list approach and consider only actions and behaviour that match to well-known attack patterns and signatures of malware traces, our system works with a white-list approach. Our anomaly detection technique keeps track of system events, their dependencies and occurrences, and thus learns the normal system behaviour over time and reports all actions that differ from the created system model. In this work, we describe this system in theory and show evaluation results from a pilot study under real-world conditions.
48|-||On the limits of engine analysis for cheating detection in chess|The integrity of online games has important economic consequences for both the gaming industry and players of all levels, from professionals to amateurs. Where there is a high likelihood of cheating, there is a loss of trust and players will be reluctant to participate — particularly if this is likely to cost them money.
48|-||Exfiltrating data from Android devices|Modern mobile devices have security capabilities built into the native operating system, which are generally designed to ensure the security of personal or corporate data stored on the device, both at rest and in transit. In recent times, there has been interest from researchers and governments in securing as well as exfiltrating data stored on such devices (e.g. the high profile PRISM program involving the US Government). In this paper, we propose an adversary model for Android covert data exfiltration, and demonstrate how it can be used to construct a mobile data exfiltration technique (MDET) to covertly exfiltrate data from Android devices. Two proof-of-concepts were implemented to demonstrate the feasibility of exfiltrating data via SMS and inaudible audio transmission using standard mobile devices.
48|-||Managing XACML systems in distributed environments through Meta-Policies|Policy-based authorization systems have been largely deployed nowadays to control different privileges over a big amount of resources within a security domain. With policies it is possible to reach a fine-grained level of expressiveness to state proper responses of a system against multiple access control requests. In this context, XACML has achieved a big popularity between both industry and academy as a standard for the definition of access control policies, as well as an architecture for the evaluation of authorization requests and for the issuing of authorization decisions. However, the applicability of XACML is still not clear in collaborative and distributed environments composed of several security domains sharing the access control over some specific resources. Such a circumstance manifests when many security domains can simultaneously define the behavior that a resource will have upon received authorization requests, like for instance an organization with many subsidiaries, a company with a service virtualization business model, etc. In this paper we propose a solution to reach an effective distributed policy management considering that a number of policies in one domain may be confidential. To this end, the default XACML architecture has been redefined in order to use i) Master and Slave PAPs to communicate security domains, ii) Meta-Policies to define privileges over access control policies (the policies become the managed resources) and iii) SAML extensions to protect the policy management messages which flow between security domains. The experiments and the defined scenarios in the paper prove the validity of the proposed solution.
48|-||Towards complexity analysis of User Authorization Query problem in RBAC|The User Authorization Query (UAQ) problem for RBAC is to determine whether there exists an optimum set of roles to be activated to provide a particular set of permissions requested by a user. It is a key issue related to efficiently handling users' access requests. Previous definitions of the UAQ problem have considered only the optimization objective for the number of permissions whereas the optimization objective for the number of roles, which is also equally important, has been largely ignored. Moreover, little attention has been given to the computational complexity of the UAQ problem that considers the optimization objectives for both the numbers of permissions and roles. In this paper, we propose a more comprehensive definition of the UAQ problem, which includes irreducibility, role-cardinality and permission-cardinality constraints, and consider both these optimization objectives together. In particular, we study the computational complexity of the UAQ problem by dividing it into three subcases: exact match, safe match and available match, and show that many instances in each subcase with additional constraints are intractable. We also propose an approach for solving the intractable cases of the UAQ problem; the proposed approach incorporates static pruning, preprocessing and the depth-first search based algorithm to significantly reduce the running time.
48|-||A data hiding scheme using pixel value differencing and improving exploiting modification directions|The fundamental requirements of information hiding systems are good visual quality, high hiding capacity, robustness and steganographic security. In this paper, we propose a new data hiding method which can increase the steganographic security of a data hiding scheme because it is less detectable by RS detection attack and the steganalytic histogram attack of pixel-value difference. In our method, a cover image is first mapped into a 1D pixels sequence by Hilbert filling curve and then divided into non-overlapping embedding units with two consecutive pixels. Because the human eyes tolerate more changes in edge and texture areas than in smooth areas, and pixel pairs in these areas often possess larger differences, the method exploits pixel value differences (PVD) to estimate the base of digits to be embedded into pixel pairs. Pixel pairs with larger differences are embedded with digits in larger base than those pixel pairs with smaller differences to maximize the payload and image quality. By using an optimization problem to solve the overflow/underflow problem, minimal distortion of the pixel ternaries cause by data embedding can be obtained. The experimental results show our method not only to enhance the embedding rate and good embedding capacity but also to keep stego-image quality.
48|-||Insecurity of an anonymous authentication for privacy-preserving IoT target-driven applications|The Internet of Things (IoT) will be formed by smart objects and services interacting autonomously and in real-time. Recently, Alcaide et al. proposed a fully decentralized anonymous authentication protocol for privacy-preserving IoT target-driven applications. Their system is set up by an ad-hoc community of decentralized founding nodes. Nodes can interact, being participants of cyberphysical systems, preserving full anonymity. In this study, we point out that their protocol is insecure. The adversary can cheat the data collectors by impersonating a legitimate user.
48|-||Authentication graphs: Analyzing user behavior within an enterprise network|User authentication over the network builds a foundation of trust within large-scale computer networks. The collection of this network authentication activity provides valuable insight into user behavior within an enterprise network. Representing this authentication data as a set of user-specific graphs and graph features, including time-constrained attributes, enables novel and comprehensive analysis opportunities. We show graph-based approaches to user classification and intrusion detection with practical results. We also show a method for assessing network authentication trust risk and cyber attack mitigation within an enterprise network using bipartite authentication graphs. We demonstrate the value of these graph-based approaches on a real-world authentication data set collected from an enterprise network.
48|-||Transaction authentication using complementary colors|In this paper, we introduce a transaction authentication solution that provides compatibility with any banking transactions. Our solution is based on a novel visual cryptographic scheme that supports multiple uses of a single static share, unlike existing techniques in the literature of visual cryptography. To support multiple uses, we utilize complementary colors in the subtractive color mixing model. In our solution, a user is asked to overlap an issued visual cryptography card on an encrypted image that is rendered on a screen to check transaction information and transaction authentication number. We analyze the security of our scheme against various attacks and show its effectiveness using a usability study.
48|-||The professionalisation of information security: Perspectives of UK practitioners|In response to the increased “cyber” threats to business, the UK and US Governments are taking steps to develop the training and professional identity of information security practitioners. The ambition of the UK Government is to drive the creation of a recognised profession, in order to attract technology graduates and others into the practice of cyber-security. Although much has been written by state bodies and industry commentators alike on this topic, we believe this qualitative study is the first empirical academic work investigating attitudes to that professionalisation amongst information security workers. The results are contextualised using concepts from the literature in the fields of professionalisation and social topics in information security.
48|-||Leakage-resilient password entry: Challenges, design, and evaluation|Password leakage is one of the most serious threats for password-based user authentication. Although this problem has been extensively investigated over the last two decades, there is still no widely adopted solution. In this paper, we attempt to systematically understand the challenges behind this problem and investigate the feasibility of solving it. Since password leakage usually happens when a password is input during authentication, we focus on designing leakage-resilient password entry (LRPE) schemes in this study. We develop a broad set of design criteria and use them to construct a practical LRPE scheme named CoverPad, which not only improves leakage resilience but also retains most usability benefits of legacy passwords. Its practicability is further verified by an extended user study.
48|-||A framework for metamorphic malware analysis and real-time detection|Metamorphism is a technique that mutates the binary code using different obfuscations. It is difficult to write a new metamorphic malware and in general malware writers reuse old malware. To hide detection the malware writers change the obfuscations (syntax) more than the behavior (semantic) of such a new malware. On this assumption and motivation, this paper presents a new framework named MARD for Metamorphic Malware Analysis and Real-Time Detection. As part of the new framework, to build a behavioral signature and detect metamorphic malware in real-time, we propose two novel techniques, named ACFG (Annotated Control Flow Graph) and SWOD-CFWeight (Sliding Window of Difference and Control Flow Weight). Unlike other techniques, ACFG provides a faster matching of CFGs, without compromising detection accuracy; it can handle malware with smaller CFGs, and contains more information and hence provides more accuracy than a CFG. SWOD-CFWeight mitigates and addresses key issues in current techniques, related to the change of the frequencies of opcodes, such as the use of different compilers, compiler optimizations, operating systems and obfuscations. The size of SWOD can change, which gives anti-malware tool developers the ability to select appropriate parameter values to further optimize malware detection. CFWeight captures the control flow semantics of a program to an extent that helps detect metamorphic malware in real-time. Experimental evaluation of the two proposed techniques, using an existing dataset, achieved detection rates in the range 94%–99.6%. Compared to ACFG, SWOD-CFWeight significantly improves the detection time, and is suitable to be used where the time for malware detection is more important as in real-time (practical) anti-malware applications.
48|-||Deceiving entropy based DoS detection|Denial of Service (DoS) attacks disable network services for legitimate users. As a result of growing dependence on the Internet by both the general public and service providers, the availability of Internet services has become a concern. While DoS attacks cause inconvenience for users, and revenue loss for service providers; their effects on critical infrastructures like the smart grid and public utilities could be catastrophic. For example, an attack on a smart grid system can cause cascaded power failures and lead to a major blackout. Researchers have proposed approaches for detecting these attacks in the past decade. Anomaly based DoS detection is the most common. The detector uses network traffic statistics; such as the entropy of incoming packet header fields (e.g. source IP addresses or protocol type). It calculates the observed statistical feature and triggers an alarm if an extreme deviation occurs. Entropy features are common in recent DDoS detection publications. They are also one of the most effective features for detecting these attacks. However, intrusion detection systems (IDS) using entropy based detection approaches can be a victim of spoofing attacks. An attacker can sniff the network and calculate background traffic entropy before a (D)DoS attack starts. They can then spoof attack packets to keep the entropy value in the expected range during the attack. This paper explains the vulnerability of entropy based network monitoring systems. We present a proof of concept entropy spoofing attack and show that by exploiting this vulnerability, the attacker can avoid detection or degrade detection performance to an unacceptable level.
48|-||Detection of malicious PDF files and directions for enhancements: A state-of-the art survey|Initial penetration is one of the first steps of an Advanced Persistent Threat (APT) attack, and it is considered one of the most significant means of initiating cyber-attacks aimed at organizations. Such an attack usually results in the loss of sensitive and confidential information. Because email communication is an integral part of daily business operations, APT attackers frequently leverage email as an attack vector for initial penetration of the targeted organization. Emails allow the attacker to deliver malicious attachments or links to malicious websites. Attackers usually use social engineering in order to make the recipient open the malicious email, open the attachment, or press a link. Existing defensive solutions within organizations prevent executables from entering organizational networks via emails, therefore, recent APT attacks tend to attach non-executable files (PDF, MS Office etc.) which are widely used in organizations and mistakenly considered less suspicious or malicious. This article surveys existing academic methods for the detection of malicious PDF files. The article outlines an Active Learning framework and highlights the correlation between structural incompatibility of PDF files and their likelihood of maliciousness. Finally, we provide comparisons, insights and conclusions, as well as avenues for future research in order to enhance the detection of malicious PDFs.
48|-||Measuring user satisfaction with information security practices|Information security is a major concern of organizational management. Security solutions based on technical aspects alone are insufficient to protect corporate data. Successful information security depends on appropriate user behavior while using information systems. User satisfaction is widely used to measure the success of information systems. The objective of this research is to develop a model to measure user satisfaction with information security practices. An instrument was developed based on this model. A survey was conducted, and 173 valid responses were obtained. Structural equation modeling was used for the data analysis. The results indicated that users understand the benefits of information security practices, but the use of information systems with security controls is considered a complex matter, which reduces information systems productivity. The measurement of the user satisfaction with information security practices is a starting point to diagnose the behavior of users in relation to information security, providing metrics to management evaluate the investment in information security training and awareness program.
48|-||Comparing intention to avoid malware across contexts in a BYOD-enabled Australian university: A Protection Motivation Theory approach|Malware have been regarded as a persistent threat to both individuals and organisations due to its wide spread via various means of infection. With the increasing use of personal mobile devices and the trending adoption of Bring Your Own Device (BYOD) practices, this threat has become even more versatile and dreadful as it could hide behind the users' typical and daily Internet activities. The importance of investigating whether the user's intention to perform malware avoidance behaviours would change across multiple contexts is emphasised. Consequently, this study determines the contributing factors and compares their impacts on such intention by extending Protection Motivation Theory in two different contexts. A total of 252 Australian higher education students were surveyed when using mobile devices such as smartphone, laptop and tablet at home and at a BYOD-enabled university. Paired t-test, Bayesian structural equation modelling, and revised z-test were employed for data analysis. The empirical findings reveal that intention to perform malware avoidance behaviours differed across the contexts. Furthermore, the researchers found perceptions of self-efficacy and vulnerability to have different impacts on such intention and other variables in the model. As a result, such findings suggested developing community of practice and repeated trainings to maintain the users' confidence in their own abilities to cope with malware threats. Message that focuses on the threats' consequences was suggested to improve home users' intention to avoid malware, along with a number of factors that could be critical to designing information security education programs. Moreover, these implications particularly address information security management at educational institutions that adopt BYOD policy. Finally, theoretical contributions include an extended model based on Protection Motivation Theory that reflects the users' intention to avoid malware threats in BYOD context, from which directions for future research were also provided.
49|-|http://www.sciencedirect.com/science/journal/01674048/49|Contents|
49|-||Decision Diagrams for XACML Policy Evaluation and Management|One of the primary challenges to apply the XACML access control policy language in applications is the performance problem of policy evaluation engines, particularly when they experience a great number of policies. Some existing works attempted to solve this problem, but only for some particular use-cases: either supporting simple policies with equality comparisons or predefined attribute values. Due to the lack of carefully checking the XACML model, they did not have original policy evaluation semantics. Therefore, they cannot handle errors containing indeterminate decisions, or ignore the critical attribute setting that leads to potential missing attribute attacks. In this paper, we build up the XACML logical model and propose a decision diagram approach using the data interval partition aggregation. It can parse and transform complex logical expressions in policies into decision tree structures, which efficiently improve the policy evaluation performance. Our approach can also be applied to solve other policy management problems such as policy redundancy detection, policy testings and comparisons, or authorization reverse queries.
49|-||Towards efficient certificate status validations with E-ADOPT in mobile ad hoc networks|Each public key infrastructure needs an efficient certificate status validation method to exclude the revoked certificates from network. In this paper, we present a novel certificate validation scheme called E-ADOPT or Enhanced-ADOPT which utilizes a new kind of certificate status information. In this solution, we modify the OCSP response messages to carry information about the accusations issued against the certificate and this additional security information helps the client nodes to tune the OCSP results refresh rate more intelligently. As a result, client node can mitigate the certificate status information inconsistency problem with lower overheads and conduct more effective certificate status validations in MANET. Simulation results demonstrate that by appending accusation-related information to the OCSP responses, our solution achieves better results.
49|-||Spherical microaggregation: Anonymizing sparse vector spaces|Unstructured texts are a very popular data type and still widely unexplored in the privacy preserving data mining field. We consider the problem of providing public information about a set of confidential documents. To that end we have developed a method to protect a Vector Space Model (VSM), to make it public even if the documents it represents are private. This method is inspired by microaggregation, a popular protection method from statistical disclosure control, and adapted to work with sparse and high dimensional data sets.
49|-||A survey of information security incident handling in the cloud|Incident handling strategy is one key strategy to mitigate risks to the confidentiality, integrity and availability (CIA) of organisation assets, as well as minimising loss (e.g. financial, reputational and legal) particularly as organisations move to the cloud. In this paper, we surveyed existing incident handling and digital forensic literature with the aims of contributing to the knowledge gap(s) in handling incidents in the cloud environment. 139 English language publications between January 2009 and May 2014 were located by searching various sources including the websites of standard bodies (e.g. National Institute of Standards and Technology) and academic databases (e.g. Google Scholar, IEEEXplore, ACM Digital Library, Springer and ScienceDirect). We then propose a conceptual cloud incident handling model that brings together incident handling, digital forensic and the Capability Maturity Model for Services to more effectively handle incidents for organisations using the cloud. A discussion of open research issues concludes this survey.
49|-||Cyber warfare: Issues and challenges|The topic of cyber warfare is a vast one, with numerous sub topics receiving attention from the research community. We first examine the most basic question of what cyber warfare is, comparing existing definitions to find common ground or disagreements. We discover that there is no widely adopted definition and that the terms cyber war and cyber warfare are not well enough differentiated. To address these issues, we present a definition model to help define both cyber warfare and cyber war. The paper then identifies nine research challenges in cyber warfare and analyses contemporary work carried out in each. We conclude by making suggestions on how the field may best be progressed by future efforts.
49|-||Detecting fake anti-virus software distribution webpages|Attackers are continually seeking novel methods to distribute malware. Among various approaches, fake Anti-Virus (AV) attacks represent an active trend for malware distribution. In a fake AV attack, attackers disguise malware as legitimate anti-virus software and convince users to install it. As web browsers become the most popular applications for users to access online resources, webpages have become the dominating means to launch fake AV attacks. In this paper, we presented an automated and effective detection system, namely DART, to identify fake AV webpages in the Internet. We proposed a collection of novel features to characterize an unknown webpage and then integrate them using statistical classifiers. These features focus on profiling a fake AV webpage from three aspects that are fundamentally important for its success, thereby resulting in the high detection accuracy and implying resistance against evasion attempts. We have performed extensive evaluation based on real fake AV webpages that are collected from the Internet. Experimental results have demonstrated that DART can accomplish a high detection rate of 90.4% at an extremely low false positive rate of 0.2%.
49|-||A novel methodology towards a trusted environment in mashup web applications|A mashup is a web-based application developed through aggregation of data from different public external or internal sources (including trusted and untrusted). Mashup introduces an open environment that is exposed to many security vulnerabilities, threats and risks. These weaknesses will bring security to the forefront when developing mashup applications and will require new ways of identifying and managing said risks. The primary goal of this paper is to present a client side mashup security framework to ensure that the sources for mashup applications are tested and secured against malicious intrusions. This framework is based on risk analysis and mashup source classification that will examine, analyze and evaluate the data transitions between the server-side and the client-side. Risk filtering using data mining suggests a new data mining technique also be utilized to enhance the quality of the risk analysis by removing most of the false risks. This approach is called the Risk Filtering Data Mining algorithm (RFDM). The RFDM framework deals with three types of clusters (trusted, untrusted and hesitation or unknown) to handle the hesitation clusters. Our proposal is to employ Atanassov's Instuitionistic Fuzzy Sets (A-IFs) as it improves the results of an URL. Finally, the results would be evaluated based on five experimental measures generated by a confusion matrix, namely: Accuracy (AC), recall or true positive rate (TP), precision (P), F-measure (considers both precision and recall) and FÎ².
49|-||An efficient grouping method and error probability analysis for RO-PUFs|Physical Unclonable Functions (PUFs) are primitives that have wide usage areas in information security. Ordering based Ring Oscillator (RO)-PUFs have been introduced recently to overcome the robustness and area efficiency issues related to PUF implementations. With this approach, 100% robust outputs are generated, providing a solution for cryptographic key generation. High entropy extraction with relatively few ROs is also achieved, resulting in high area utilization of the PUF circuit. Frequency threshold determination is the most critical step in ordering based RO-PUFs, and determines a trade-off between area efficiency and robustness. In this work, we overview an efficient grouping method for RO-PUFs and analyze the error vulnerability of PUFs based on the frequency threshold determination. Next, we analyze the length of groups used in such PUF circuits and determine the symbol error probability. In addition to these, we demonstrate the relationship between the symbol error probability and bit error probability. We also investigate the bit error probability based on the wrong determination of the frequency threshold in ordering based RO-PUFs. Finally, a trade-off between area usage and robustness is presented for identification applications.
49|-||Towards more pro-active access control in computer systems and networks|Access control is a core security technology which has been widely used in computer systems and networks to protect sensitive information and critical resources and to counter malicious attacks. Although many access control models have been developed in the past, such as discretionary access control (DAC), mandatory access control (MAC) and role-based access control (RBAC), these models are designed primarily as a defensive measure in that they are used for examining access requests and making authorization decisions based on established access control policies. As the result, even after a malicious access is identified, the requester can still keep issuing more malicious access requests without much fear of punitive consequences from the access control system in subsequent accesses. Such access control may be acceptable in closed systems and networks but is not adequate in open systems and networks where the real identities and other critical information about requesters may not be known to the systems and networks. In this paper, we propose to design pro-active access control so that access control systems can respond to malicious access pro-actively to suit the needs of open systems and networks. We will first apply some established principles in the Game Theory to analyze current access control models to identify the limitations that make them inadequate in open systems and networks. To design pro-active access control (PAC), we incorporate a constraint mechanism that includes feedback and evaluation components and show based on the Game Theory how to make such access control respond to malicious access in a pro-active manner. We also present a framework design of PAC and demonstrate through the implementation of trust-based access control the feasibility of design, implementation and application of pro-active access control. Such kind of models and mechanisms can serve as the foundation for the design of access control systems that will be made more effective in deterring malicious attacks in open systems and networks.
49|-||SENTINEL: Securing Legacy Firefox Extensions|A poorly designed web browser extension with a security vulnerability may expose the whole system to an attacker. Therefore, attacks directed at “benign-but-buggy” extensions, as well as extensions that have been written with malicious intent, pose significant security threats to a system running such components. Recent studies have indeed shown that many Firefox extensions are over-privileged, making them attractive attack targets. Unfortunately, users currently do not have many options when it comes to protecting themselves from extensions that may potentially be malicious. Once installed and executed, the extension is considered trusted.
49|-||Improving the information security culture through monitoring and implementation actions illustrated through a case study|The human aspect, together with technology and process controls, needs to be considered as part of an information security programme. Current and former employees are still regarded as one of the root causes of information security incidents. One way of addressing the human aspect is to embed an information security culture where the interaction of employees with information assets contributes to the protection of these assets. In other words, it is critical to improve the information security culture in organisations such that the behaviour of employees is in compliance with information security and related information processing policies and regulatory requirements. This can be achieved by assessing, monitoring and influencing an information security culture. An information security culture can be assessed by using an approach such as an information security culture assessment (ISCA). The empirical data derived from an ISCA can be used to influence the information security culture by focussing on developmental areas, of which awareness and training programmes are a critical facet.
49|-||Personality, attitudes, and intentions: Predicting initial adoption of information security behavior|Investigations of computer user behavior become especially important when behaviors like security software adoption affect organizational information resource security, but adoption antecedents remain elusive. Technology adoption studies typically predict behavioral outcomes by investigating the relationship between attitudes and intentions, though intention may not be the best predictor of actual behavior. Personality constructs have recently been found to explain even more variance in behavior, thus providing insights into user behavior. This research incorporates conscientiousness and agreeableness into a conceptual model of security software use. Attitudinal constructs perceived ease of use and perceived usefulness were linked with behavioral intent, while the relationship between intent and actual use was found to be moderated by conscientiousness and agreeableness. The results that the moderating effect of personality greatly increases the amount of variance explained in actual use.
49|-||A Permission verification approach for android mobile applications|Mobile applications build part of their security and privacy on a declarative permission model. In this approach mobile applications, to get access to sensitive resources, have to define the corresponding permissions in a manifest. However, mobile applications may request access to permissions that they do not require for their execution (over-privileges) and offer opportunities to malicious software to gain access to otherwise inaccessible resources. In this paper, we investigate on the declarative permissions model on which security and privacy services of Android rely upon. We propose a practical and efficient permission certification technique, in the direction of risk management assessment. We combine both runtime information and static analysis to profile mobile applications and identify if they are over-privileged or follow the least privilege principle. We demonstrate a transparent solution that neither requires modification to the underlying framework, nor access to the applications' original source code. We assess the effectiveness of our approach, using a randomly selected varied set of mobile applications. Results show that our approach can accurately identify whether an application is over-privileged or not, whilst at the same time guaranteeing the need of declaring specific permissions in the manifest.
49|-||RTECA: Real time episode correlation algorithm for multi-step attack scenarios detection|Today, from information security perspective, prevention methods are not enough solely. Early Warning Systems (EWSs) are in the category of reactive methods. These systems are complementing Intrusion Detection Systems (IDSs) where their main goals include early detection of potential malicious behavior in large scale environments such as national level. An important process in EWSs is the analysis and correlation of alerts aggregated from the installed sensors (e.g., IDSs, IP telescopes, and botnet detection systems). In this paper, an efficient framework for alert correlation in EWSs is proposed. The framework includes a correlation scheme based on a combination of statistical and stream mining techniques. The method works real-time by extracting critical episodes from sequences of alerts, which could be part of multi-step attack scenarios. A Causal Correlation Matrix (CCM) is used for encoding correlation strength between the alert types in attack scenarios. Experimental results show that the framework is efficient enough in detecting known attack scenarios and new attack strategies. The results also show that the system is able to predict the next steps of running attack scenaris up to 95% of accuracy under special circumstances.
49|-||Towards privacy-preserving reputation management for hybrid broadcast broadband applications|Hybrid Broadcast Broadband TV (HbbTV) is an industry standard aimed to provide a platform combining TV services with Internet services, using connected TVs and set-top boxes. It enables the possibility for vendors to offer applications directly to the users, introducing new entertainment services such as streaming of video on demand, games, social networking, etc. As a consequence, tons of applications are available for users to directly download and consume through the so-called application stores, despite the potential trust and security issues arising due to the decentralized nature of these environments.
49|-||Privacy leakage analysis in online social networks|Online Social Networks (OSNs) have become one of the major platforms for social interactions, such as building up relationship, sharing personal experiences, and providing other services. The wide adoption of OSNs raises privacy concerns due to personal data shared online. Privacy control mechanisms have been deployed in popular OSNs for users to determine who can view their personal information. However, user's sensitive information could still be leaked even when privacy rules are properly configured. We investigate the effectiveness of privacy control mechanisms against privacy leakage from the perspective of information flow. Our analysis reveals that the existing privacy control mechanisms do not protect the flow of personal information effectively. By examining representative OSNs including Facebook, Google+, and Twitter, we discover a series of privacy exploits. We find that most of these exploits are inherent due to the conflicts between privacy control and OSN functionalities. The conflicts reveal that the effectiveness of privacy control may not be guaranteed as most OSN users expect. We provide remedies for OSN users to mitigate the risk of involuntary information leakage in OSNs. Finally, we discuss the costs and implications of resolving the privacy exploits.
49|-||Profiling user-trigger dependence for Android malware detection|As mobile computing becomes an integral part of the modern user experience, malicious applications have infiltrated open marketplaces for mobile platforms. Malware apps stealthily launch operations to retrieve sensitive user or device data or abuse system resources. We describe a highly accurate classification approach for detecting malicious Android apps. Our method statically extracts a data-flow feature on how user inputs trigger sensitive API invocations, a property referred to as the user-trigger dependence. Our evaluation with 1433 malware apps and 2684 free popular apps gives a classification accuracy (2.1% false negative rate and 2.0% false positive rate) that is better than, or at least competitive against, the state-of-the-art. Our method also discovers new malicious apps in the Google Play market that cannot be detected by virus scanning tools. Our thesis in this mobile app classification work is to advocate the approach of benign property enforcement, i.e., extracting unique behavioral properties from benign programs and designing corresponding classification policies.
49|-||Hacking and Penetration Testing with Low Power Devices|
50|-|http://www.sciencedirect.com/science/journal/01674048/50|Contents|
50|-||Intrusion alert prioritisation and attack detection using post-correlation analysis|Event Correlation used to be a widely used technique for interpreting alert logs and discovering network attacks. However, due to the scale and complexity of today's networks and attacks, alert logs produced by these modern networks are much larger in volume and difficult to analyse. In this research we show that adding post-correlation methods can be used alongside correlation to significantly improve the analysis of alert logs.
50|-||Integrity, authenticity, non-repudiation, and proof of existence for long-term archiving: A survey|The world increasingly depends on archives to store digital documents, such as land registers and medical records, for long periods of time. For stored documents to remain trustworthy, archives must provide proofs that a document existed on a certain date and has not been changed since. In addition, in many cases, the origin of the document must be verifiable and the originator must not be able to repudiate that she is the originator. In this paper, we survey the solutions that provide the above protection goals in the long term. We analyze and compare the solutions with respect to their functionalities (which protection goals do they achieve?), the trust assumptions they require, and their performance. From this analysis and comparison, we deduce deficiencies of the current solutions and important research problems that must be solved in order to come up with protection solutions that are even more satisfactory.
50|-||The ultimate control flow transfer in a Java based smart card|Recently, researchers published several attacks on smart cards. Among these, software attacks are the most affordable, they do not require specific hardware (laser, EM probe, etc.). Such attacks succeed to modify a sensitive system element which offers access to the smart card assets. To prevent that, smart card manufacturers embed dedicated countermeasures that aim to protect the sensitive system elements. We present a generic approach based on a Control Flow Transfer (CFT) attack to modify the Java Card program counter. This attack is built on a type confusion using the couple of instructions jsr/ret. Evaluated on different Java Cards, this new attack is a generic CFT exploitation that succeeds on each attacked cards. We present several countermeasures proposed by the literature or implemented by smart card designers and for all of them we explain how to bypass them. Then, we propose to use Attack Countermeasure Tree to develop an effective and affordable countermeasure for this attack.
50|-||Relay and jammer selection schemes for improving physical layer security in two-way cooperative networks|This paper is concerned with the relay and jammers selection in two-way cooperative networks to improve their physical layer security. Three different categories of selection schemes are proposed which are; selection schemes without jamming, selection schemes with conventional jamming and selection schemes with controlled jamming. The selection process is analyzed for two different network models; single eavesdropper model and multiple cooperating and non-cooperating eavesdroppers’ model. The proposed schemes select three intermediate nodes during two communication phases and use the Decode-and-Forward (DF) strategy to assist the sources to deliver their data to the corresponding destinations. The performance of the proposed schemes is analyzed in terms of ergodic secrecy rate and secrecy outage probability metrics. The obtained results show that the selection schemes with jamming outperform the schemes without jamming when the intermediate nodes are distributed dispersedly between sources and eavesdropper nodes. However, when the intermediate nodes cluster gets close to one of the sources, they are not superior any more due to the strong interference on the destination nodes. Therefore, a hybrid scheme which switches between selection schemes with jamming and schemes without jamming is introduced to overcome the negative effects of interference. Finally, a comparison between relay and jammers selection schemes in both one-way and two-way cooperative networks is given in terms of both secrecy metrics. The obtained results reveal that, despite the presence of cooperating eavesdroppers, the proposed selection schemes are still able to improve both the secrecy rate and the secrecy outage probability of the two-way cooperative networks.
50|-||Selecting a trusted cloud service provider for your SaaS program|Software as a Service (SaaS) offers major business and IT benefits that organizations are looking to take advantage of. SaaS adoption presents serious and unique security risks. Moving a company's sensitive data into the hands of cloud providers expands and complicates the risk landscape in which the organization operates.
50|-||DP-Apriori: A differentially private frequent itemset mining algorithm based on transaction splitting|In this paper, we study the problem of designing a differentially private FIM algorithm which can simultaneously provide a high level of data utility and a high level of data privacy. This task is very challenging due to the possibility of long transactions. A potential solution is to limit the cardinality of transactions by truncating long transactions. However, such approach might cause too much information loss and result in poor performance. To limit the cardinality of transactions while reducing the information loss, we argue that long transactions should be split rather than truncated. To this end, we propose a transaction splitting based differentially private FIM algorithm, which is referred to as DP-Apriori. In particular, a smart weighted splitting technique is proposed to divide long transactions into sub-transactions whose cardinality is no more than a specified number of items. In addition, to offset the information loss caused by transaction splitting, a support estimation technique is devised to estimate the actual support of itemsets in the original database. Through privacy analysis, we show that our DP-Apriori algorithm is ÉÉ-differentially private. Extensive experiments on real-world datasets illustrate that DP-Apriori substantially outperforms the state-of-the-art techniques.
50|-||SECO: Secure and scalable data collaboration services in cloud computing|Cloud storage services enable users to remotely store their data and eliminate excessive local installation of software and hardware. There is an increasing trend of outsourcing enterprise data to the cloud for efficient data storage and management. However, this introduces many new challenges toward data security. One critical issue is how to enable a secure data collaboration service including data access and update in cloud computing. A data collaboration service is to support the availability and consistency of the shared data among multi-users. In this paper, we propose a secure, efficient and scalable data collaboration scheme SECO. In SECO, we employ a multi-level hierarchical identity based encryption (HIBE) to guarantee data confidentiality against untrusted cloud. This paper is the first attempt to explore secure cloud data collaboration services that precludes information leakage and enables a one-to-many encryption paradigm, data writing operation and fine-grained access control simultaneously. Security analysis indicates that the SECO is semantically secure against adaptive chosen ciphertext attacks (IND-ID-CCA) in the random oracle model, and enforces fine-grained access control, collusion resistance and backward secrecy. Extensive performance analysis and experimental results show that SECO is highly efficient and has only low overhead on computation, communication and storage.
||||
volume|issue|url|title|abstract
51|-|http://www.sciencedirect.com/science/journal/01674048/51|Contents|
51|-||A practical off-line taint analysis framework and its application in reverse engineering of file format|This paper presents FlowWalker, a novel dynamic taint analysis framework that aims to extract the complete taint data flow while eliminating the bottlenecks that occur in existing tools, with applications to file-format reverse engineering. The framework proposes a multi-taint-tag assembly-level taint propagation strategy. FlowWalker separates taint tracking operations from execution with an off-line structure, utilizes memory-mapped files to enhance I/O efficiency, processes taint paths during virtual execution playback, and uses parallelization and pipelining mechanisms to achieve speedup. Based on the semantic correlations implied by the taint path information, this paper presents an algorithm for extracting the structures of unknown file formats. According to test data, the overall program runtime ranges from 92.98% to 208.01% of the length of the underlying instrumentation alone, while the speed enhancement is 60% compared to another well-featured tool in Windows. Medium-complexity file formats are correctly partitioned, and the constant fields are extracted. Due to its efficiency and scalability, FlowWalker can address the needs of further security-related research.
51|-||Stealth attacks: An extended insight into the obfuscation effects on Android malware|In order to effectively evade anti-malware solutions, Android malware authors are progressively resorting to automatic obfuscation strategies. Recent works have shown, on small-scale experiments, the possibility of evading anti-malware engines by applying simple obfuscation transformations on previously detected malware samples. In this paper, we provide a large-scale experiment in which the detection performances of a high number of anti-malware solutions are tested against two different sets of malware samples that have been obfuscated according to different strategies. Moreover, we show that anti-malware engines search for possible malicious content inside assets and entry-point classes. We also provide a temporal analysis of the detection performances of anti-malware engines to verify if their resilience has improved since 2013. Finally, we show how, by manipulating the area of the Android executable that contains the strings used by the application, it is possible to deceive anti-malware engines so that they will identify legitimate samples as malware. On one hand, the attained results show that anti-malware systems have improved their resilience against trivial obfuscation techniques. On the other hand, more complex changes to the application executable have proved to be still effective against detection. Thus, we claim that a deeper static (or dynamic) analysis of the application is needed to improve the robustness of such systems.
51|-||Time series modeling of vulnerabilities|Vulnerability prediction models forecast future vulnerabilities and can be used to assess security risks and estimate the resources needed for handling potential security breaches. Although several vulnerability prediction models have been proposed, such models have shortcomings and do not consider trend, level, and seasonality components of vulnerabilities. Through time series analysis, this study built predictive models for five popular web browsers: Chrome, Firefox, Internet Explorer, Safari and Opera and for all reported vulnerabilities elsewhere. Results showed that time series models provide a good fit to our vulnerability datasets and can be useful for vulnerability prediction. Results also suggested that the level of the series is the best estimator of the prediction models.
51|-||Incorporating attacker capabilities in risk estimation and mitigation|The risk exposure of a given threat to an information system is a function of the likelihood of the threat and the severity of its impacts. Existing methods for estimating threat likelihood assume that the attacker is able to cause a given threat, that exploits existing vulnerabilities, if s/he has the required opportunities (e.g., sufficient attack time) and means (e.g., tools and skills), which is not true; often, s/he can perform an attack and cause the related threat only if s/he has the ability to access related resources (objects) of the system that allow to do so. This paper proposes a risk estimation method that incorporates attacker capabilities in estimating the likelihood of threats as conditions for using the means and opportunities, demonstrates the use of the proposed risk estimation method through two examples: video conferencing systems and connected vehicles, shows that changing attacker capabilities changes the risks of the threats, and compares the uncertainty of experts in evaluating the likelihood of threats considering and not considering attacker capabilities for two experiments. The results of the experiments suggest that experts are less uncertain about their estimations of threat likelihoods when they consider attacker capabilities.
52|-|http://www.sciencedirect.com/science/journal/01674048/52|Decentralized detection of network attacks through P2P data clustering of SNMP data|The goal of Network Intrusion Detection Systems (NIDSs) is to protect against attacks by inspecting network traffic packets, for instance, looking for anomalies and signatures of known attacks. This paper illustrates an approach to attack detection that analyzes just the standard statistics automatically generated by the Simple Network Management Protocol (SNMP) through unsupervised distributed data mining algorithms. We describe the design of a decentralized system composed of a peer-to-peer network of monitoring stations: each of them continuously gathers SNMP statistical observations about the network traffic and runs a distributed data clustering algorithm in cooperation with other stations. This progressively leads to the construction of a traffic model capable to detect undergoing attacks on later observations, including potentially previously unknown attacks. To estimate the accuracy of the described system, we performed an extensive number of distributed data clustering processing on data sets of SNMP observations generated from real traffic.
52|-||Gaithashing: A two-factor authentication scheme based on gait features|Recently, gait recognition has attracted much attention as a biometric feature for real-time person authentication. The main advantage of gait is that it can be observed at a distance in an unobtrusive manner. However, the security of an authentication system, based only on gait features, can be easily broken. A malicious actor can observe the gait of an unsuspicious person and extract the related biometric template in a trivial manner and without being noticed. Another major issue of gait as an identifier has to do with their high intra-variance, since human silhouettes can be significantly modified, when for example the user holds a bag or wears a coat. This paper proposes gaithashing, a two-factor authentication that interpolates between the security features of biohash and the recognition capabilities of gait features to provide a high accuracy and secure authentication system. A novel characteristic of gaithashing is that it enrolls three different human silhouettes types. During authentication, the new extracted gait features and the enrollment ones are fused using weighted sums. By selecting appropriate weight values, the proposed scheme eliminates the noise and distortions caused by different silhouette types and achieves to authenticate a user independently of his/her silhouette. Apart from high accuracy, the proposed scheme provides revocability in case of a biometric template compromise. The performance of the proposed scheme is evaluated by carrying out a comprehensive set of experiments. Numerical results show that gaithashing outperforms existing solutions in terms of authentication performance, while at the same time achieves to secure the gait features.
52|-||Hypervisor-based malware protection with AccessMiner|In this paper we discuss the design and implementation of AccessMiner, a system-centric behavioral malware detector. Our system is designed to model the general interactions between benign programs and the underlying operating system (OS). In this way, AccessMiner is able to capture which, and how, OS resources are used by normal applications and detect anomalous behavior in real-time.
52|-||Dytaint: The implementation of a novel lightweight 3-state dynamic taint analysis framework for x86 binary programs|By analyzing information flow at runtime, dynamic taint analysis can precisely detect a wide range of vulnerabilities of software. However, it suffers from substantial runtime overhead and is incapable of discovering potential threats. Yet, realistically, the interested analyst doesn't have access to the source code of the malware. Therefore, the task of software flaw tracking becomes rather complicated. In order to cope with these issues, this paper proposes Dytaint, a novel lightweight 3-state dynamic taint analysis framework, for diagnosing more software vulnerabilities with lower runtime overhead. The framework works for the x86 binary executables and requires no special hardware assistance. Besides the tainted and the untainted states that are discussed by many popularly used taint analysis tools, the third state, controlled-taint state, is proposed to detect more types of software vulnerabilities. The new Chaining Hash Table which reduces the space for storing taint information without increasing the accessing time is also incorporated in the framework. Furthermore, two mechanisms, namely, the irrelevant API filtering based on the function recognition method and basic block handling, are introduced to optimize the runtime performance of our framework. The testing results by running SPEC CINT2006 benchmarks and various popular software have demonstrated that Dytaint is efficient which incurs only 3.1 times overhead to the native on average and practical which is able to discover not only all the real threats but also most of the potential ones.
52|-||Digital forensic readiness: Expert perspectives on a theoretical framework|Modern organizations need to develop ‘digital forensic readiness’ to comply with their legal, contractual, regulatory, security and operational obligations. A review of academic and practitioner literature revealed a lack of comprehensive and coherent guidance on how forensic readiness can be achieved. This is compounded by the lack of maturity in the discourse of digital forensics rooted in the informal definitions of key terms and concepts. In this paper we validate and refine a digital forensic readiness framework through a series of expert focus groups. Drawing on the deliberations of experts in the focus groups, we discuss the critical issues facing practitioners in achieving digital forensic readiness.
52|-||Security Busters: Web browser security vs. rogue sites|URL blacklists are used by the majority of modern web browsers as a means to protect users from rogue web sites, i.e. those serving malware and/or hosting phishing scams. There is a plethora of URL blacklists/reputation services, out of which Google's Safe Browsing and Microsoft's SmartScreen stand out as the two most commonly used ones. Frequently, such lists are the only safeguard web browsers implement against such threats. In this paper, we examine the level of protection that is offered by popular web browsers on iOS, Android and desktop (Windows) platforms, against a large set of phishing and malicious URL. The results reveal that most browsers – especially those for mobile devices – offer limited protection against such threats. As a result, we propose and evaluate a countermeasure, which can be used to significantly improve the level of protection offered to the users, regardless of the web browser or platform they are using.
52|-||Toward protecting control flow confidentiality in cloud-based computation|Cloud based computation services have grown in popularity in recent years. Cloud users can deploy an arbitrary computation cluster to public clouds and execute their programs on that remote cluster to reduce infrastructure investment and maintenance costs. However, how to leverage cloud resources while keeping the computation confidential is a new challenge to be explored. In this paper, we propose runtime control flow obfuscation (RCFO) to protect the control flow confidentiality of outsourced programs. RCFO transforms an outsourced program into two parts: the public program running on the untrusted public cloud and the private program running on the trusted private cloud. By hiding parts of the control flow information in the private program and inserting fake branch statements into the public program, RCFO raises the bar for static and dynamic analysis-based reverse engineering attacks. Based on RCFO, we implement a system called MRDisguiser to protect cloud-based MapReduce services. We perform experiments on a real MapReduce service, Amazon Elastic MapReduce. The experimental results indicate that MRDisguiser is compatible with current cloud-based MapReduce services, and incurs moderate performance overhead. Specifically, when the obfuscation degree increases from 0 to 1.0, the average performance overhead is between 14.9% and 33.2%.
52|-||Analyzing the role of cognitive and cultural biases in the internalization of information security policies: Recommendations for information security awareness programs|Standards and best practices for information security awareness programs focus on the content and processes of the programs, without taking into consideration how individuals internalize security-related information and how individuals make security related decisions. Relevant literature, however has identified that individual perceptions, beliefs, and biases significantly influence security policy compliance behavior. Security awareness programs need, therefore, to be aligned with the factors affecting the internalization of the communicated security objectives. This paper explores the role of cognitive and cultural biases in shaping information security perceptions and behaviors. We draw upon related literature from contiguous disciplines (namely behavioral economics and health and safety research) to develop a conceptual framework and analyze the role of cognitive and cultural biases in information security behavior. We discuss the implications of biases for security awareness programs and provide a set of recommendations for planning and implementing awareness programs, and for designing the related material. This paper opens new avenues for information security awareness research with regard to security decision making and proposes practical recommendations for planning and delivering security awareness programs, so as to exploit and alleviate the effect of cognitive and cultural biases on shaping risk perceptions and security behavior.
52|-||Effect of network infrastructure factors on information system risk judgments|Little is known about how perceived network topology factors, which are common components of information system risk metrics, impact human judgments of risk. Using a half-fractional factorial design, this study experimentally manipulated five perceivable network topology factors (network partitioning, network diversity, wireless status, network footprint and connectivity) to assess the relationship between these factors and network risk judgments. The consistency of network risk ratings and rankings were evaluated for each of the 16 network topologies across a sample of 55 network security professionals who reviewed these topologies. Three robust significant main effects (network partitioning, wireless status, and connectivity) and one significant interaction (network partitioning X wireless status) were found. While some topologies were consistently rated and ranked as significantly more risky than others, there was some variability in ratings at each main effect level as well as the spread of the mean ratings between the two main effect levels (e.g., wireless and wired). We discuss the implications of our findings with respect to network risk metric rigor.
53|-|http://www.sciencedirect.com/science/journal/01674048/53|A taxonomy for privacy enhancing technologies|Privacy-enhancing technologies (PETs) belong to a class of technical measures which aim at preserving the privacy of individuals or groups of individuals. Numerous PETs have been proposed for all kinds of purposes, but are difficult to be compared with each other. The challenge here lies in the fact that information privacy is a comprehensive concept with solutions being diverse, with different focus and aims. As existing taxonomies cover information security-related aspects, while neglecting privacy-specific properties, this work aims at filling this gap by describing a universal taxonomy of PETs where the taxonomy aspects are selected such that they allow the categorization of PETs in different dimensions and properties to cover a wide area of privacy (e.g., user privacy, data privacy). It provides the reader with a tool for the systematic comparison of different PETs. This helps in identifying limitations of existing PETs, complementary technologies, and potential research directions. To demonstrate its applicability, the proposed taxonomy is applied to a set of key technologies covering different disciplines such as data anonymization, privacy-preserving data querying, communication protection, and identity hiding.
53|-||An expert-based investigation of the Common Vulnerability Scoring System|The Common Vulnerability Scoring System (CVSS) is the most widely used standard for quantifying the severity of security vulnerabilities. For instance, all vulnerabilities in the US National Vulnerability Database are scored according to this system. Unfortunately, it is largely unexplored whether or not its scores are accurate. This paper studies this property through a survey with opinions by 384 experts, covering more than 3000 vulnerabilities. The results show that the mean disagreement between the judgments of the experts and the CVSS Base Score is −0.38, with a variance of 4.46 (on a scale from 0 to 10). The direction of this difference depends on the type of vulnerability that is concerned. The experts then suggest a number of possible revisions to the CVSS that could explain this difference.
53|-||When Mice devour the Elephants: A DDoS attack against size-based scheduling schemes in the internet|Size-based scheduling (SBS) has been shown to offer significant performance improvement in Web servers and routers. However, most of the performance benefits offered by SBS rely on the premise that the scheduler will interact with a “well behaved” heavy tailed job size distribution. In this paper we design an attack that degrades the performance of an SBS scheduler by subjecting it to a job size distribution which violates the core traffic properties from which SBS derives its strengths. Through theoretical work and a wide range of experiments, we demonstrate the lethality of the attack against routers that use SBS. Additionally, we cite evidence that indicates why the tools and practical manoeuvres required to carry out the attack on a live network are within the reach of adversaries. As flavors of SBS begin to grace the Internet, the paper provides a timely cautionary note on the challenges that SBS could face if widely deployed without specialized defense mechanisms.
53|-||On statistical distance based testing of pseudo random sequences and experiments with PHP and Debian OpenSSL|NIST SP800-22 (2010) proposed the state of the art statistical testing techniques for testing the quality of (pseudo) random generators. However, it is easy to construct natural functions that are considered as GOOD pseudorandom generators by the NIST SP800-22 test suite though the output of these functions is easily distinguishable from the uniform distribution. This paper proposes solutions to address this challenge by using statistical distance based testing techniques. We carried out both NIST tests and LIL based tests on commonly deployed pseudorandom generators such as the standard C linear congruential generator, Mersenne Twister pseudorandom generator, and Debian Linux (CVE-2008-0166) pseudorandom generator with OpenSSL 0.9.8c-1. Based on experimental results, we illustrate the advantages of our LIL based testing over NIST testing. It is known that Debian Linux (CVE-2008-0166) pseudorandom generator based on OpenSSL 0.9.8c-1 is flawed and the output sequences are predictable. Our LIL tests on these sequences discovered the flaws in Debian Linux implementation. However, NIST SP800-22 test suite is not able to detect this flaw using the NIST recommended parameters. It is concluded that NIST SP800-22 test suite is not sufficient and distance based LIL test techniques be included in statistical testing practice. It is also recommended that all pseudorandom generator implementations be comprehensively tested using state-of-the-art statistically robust testing tools.
53|-||Information security conscious care behaviour formation in organizations|Today, the Internet can be considered to be a basic commodity, similar to electricity, without which many businesses simply cannot operate. However, information security for both private and business aspects is important. Experts believe that technology cannot solely guarantee a secure environment for information. Users' behaviour should be considered as an important factor in this domain. The Internet is a huge network with great potential for information security breaches. Hackers use different methods to change confidentiality, integrity, and the availability of information in line with their benefits, while users intentionally or through negligence are a great threat for information security. Sharing their account information, downloading any software from the Internet, writing passwords on sticky paper, and using social security numbers as a username or password are examples of their mistakes. Users' negligence, ignorance, lack of awareness, mischievous, apathy and resistance are usually the reasons for security breaches. Users' poor information security behaviour is the main problem in this domain and the presented model endeavours to reduce the risk of users' behaviour in this realm. The results of structural equation modelling (SEM) showed that Information Security Awareness, Information Security Organization Policy, Information Security Experience and Involvement, Attitude towards information security, Subjective Norms, Threat Appraisal, and Information Security Self-efficacy have a positive effect on users' behaviour. However, Perceived Behavioural Control does not affect their behaviour significantly. The Protection Motivation Theory and Theory of Planned Behaviour were applied as the backbone of the research model.
53|-||Security of Software Defined Networks: AÂ survey|Software Defined Networking (SDN) has emerged as a new network architecture for dealing with network dynamics through software-enabled control. While SDN is promoting many new network applications, security has become an important concern. This paper provides an extensive survey on SDN security. We discuss the security threats to SDN according to their effects, i.e., Spoofing, Tampering, Repudiation, Information disclosure, Denial of Service, and Elevation of Privilege. We also review a wide range of SDN security controls, such as firewalls, IDS/IPS, access control, auditing, and policy management. We describe several pathways of how SDN is evolving.
53|-||Statistical dynamic splay tree filters towards multilevel firewall packet filtering enhancement|Network Firewalls are considered to be one of the most important security components in today's IP network architectures. Performance of firewalls has significant impact on the overall network performance. Firewalls should be able to sustain a very high throughput and ensure network services availability. In this paper, we propose an analytical dynamic multilevel early packet filtering mechanism to enhance firewall performance. The proposed mechanism uses statistical splay tree filters that utilize traffic characteristics to minimize packet filtering time. The statistical splay tree filters are reordered according to the network traffic divergence upon certain threshold qualification (Chi–Square Test). That is, the proposed mechanism is able to decide whether or not there is a need to update the dynamic splay tree filters' order for filtering the next network traffic window and predict the best order pattern. Furthermore, the importance of optimizing packet rejection and acceptance is done through the multilevel packet filtering process; where in each level, unwanted packets are rejected as early as possible. The proposed mechanism can also be considered as a device protection mechanism against denial of service (DoS) attacks targeting the default filtering rule. Early packet acceptance is done using the splay tree data structure which adapts dynamically according to network traffic flows. Consequently, repeated packets will have less memory accesses and therefore reduce the overall packets filtering time as demonstrated in the evaluation section.
