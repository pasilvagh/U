volume|issue|url|title|abstract
41|1|http://www.sciencedirect.com/science/journal/01641212/41/1|Editorial|
41|1||Software engineering practices in Singapore|A study on software engineering practices in Singapore was conducted in 1995. The study was conducted to gain an insight into the extent to which software engineering practices have been adopted by organisations in Singapore, the benefits these organisations have realised, the problems they face in implementing these practices and their perceptions on issues prevalent in the software engineering field. In the context of the study, “software engineering practices” refer to the models, techniques and tools used in the software engineering process. The study was conducted through a self-administered mail questionnaire survey sent to a number of organisations in Singapore. A total of 54 organisations responded to the survey. The results showed that the majority of organisations had adopted a formal system development methodology but relatively few were using software engineering tools. The respondents' perceptions towards the benefits and problems of implementing software engineering practices validated to a large extent the findings of other practitioners and researchers in the field.
41|1||Investments in reusable software. A study of software reuse investment success factors|This research supports the thesis that there is a set of success factors which are common across organizations and have some predictability relationships to software reuse. For completeness, this research also investigated to see if software reuse had a predictive relationship to productivity and quality. The individual success factors were grouped into the following categories: management commitment, investment strategy, business strategy, technology transfer, organizational structure, process maturity, product-line approach, software architecture, availability of components, and quality of components. A questionnaire was developed to measure software reuse capability, productivity, quality, and the set of software reuse success factors. A survey was conducted to determine the state-of-the practice. The data from the survey was statistically analyzed to evaluate the relationships among reuse capability, productivity, quality, and the individual software reuse success factors. The results of the analysis showed some of the success factors to have a predictive relationship to software reuse capability. Software reuse capability also had a predictive relationship to productivity and quality. Based on the research results, the leading indicators of software reuse capability are: product-line approach, architecture which standardizes interfaces and data formats, common software architecture across the product-line, design for manufacturing approach, domain engineering, management which understands reuse issues, software reuse advocate(s) in senior management, state-of-the-art tools and methods, precedence of reusing high level software artifacts such as requirements and design versus just code reuse, and trace end-user requirements to the components which support them.
41|1||Analyzing existing software for software reuse|This paper describes an automated method to support analysis of existing software at the start of a systematic reuse initiative. By determining a baseline level of informal reuse, this method can be used to help identify promising domains for initial implementation of systematic reuse and provide information that may affect calculation of return on investment in reuse. It can also be used in conjunction with domain analysis to help identify ideas for reusable components. The method draws on techniques developed for detecting plagiarized student programs. The paper presents results of three case studies that used a prototype tool called SoftKin to test this approach on commercial application software. The case studies indicate that the results from plagiarism detection transfer nicely to the task of analyzing existing software for reuse.
41|1||The use of immense memories for database systems: A pragmatic approach|We examine the cost and performance advantages of immense main memories and of the use of extended storage units for a wide range of database applications. Most database systems are required to guarantee a close to absolute degree of data integrity. This requirement is a serious limitation on the usage of main memories as the primary repository of data because of the vulnerability of the content to errors, particularly, software related errors. Current research often neglects this point and focuses instead on the technological issues of the silicon memories content volatility. Transaction processing performance is mostly evaluated on the basis of throughput (i.e., transaction per second) provided that the average response time does not exceed 1 s. Most update intensive applications can be classified as transaction processing and therefore driven by throughput considerations. We will show that large memories have little effect on throughput. Given this point of view, there is very little (if any) performance advantage in using main memories for transaction processing. Consequently, the potential of the immense memory system is for improving the performance of those (non-updating) applications that execute complex queries. The coexistence of query applications with the updating transactions is a realistic view of most business systems as well as many engineering and scientific applications. By taking these facts into consideration, a new vision of in-memory databases is created. In this approach, hardware architecture changes are shown to be less important than the use of proper algorithms and software systems.
41|1||A simulation model for the primary copy strategy in distributed INGRES|In distributed INGRES, the primary copy strategy based on two-phase locking is used to solve the concurrency control (cc) problem. To observe the behaviors of the transactions in distributed INGRES, a simulation model is proposed and a simulation is conducted in this paper. Furthermore, to observe how the primary copy sites for fragments influence the performance of distributed INGRES, we derive a method to select the primary copy sites such that the system has shorter average processing time per transaction, higher throughput, and fewer deadlocks. The experimental results obtained from the simulation show that the primary copy sites selected by the method generate the best results under low conflict ratio or long inter-arrival time.
41|1||Satisfying timing constraints of real-time databases|A real-time database has deadlines for processing transactions. Approximate query processing (AQP) has been presented as a strategy to satisfy these timing constraints by providing approximate answers to queries at the deadline instead of missing deadlines. In order to produce approximate answers, semantic information is maintained about the database and a computational overhead is required. In this paper the performance of AQP is examined to determine its effect on satisfying the timing constraints of real-time databases. Query and update transactions are modeled as periodic tasks with hard deadlines. A lock-based concurrency control is used and the effect of workload characteristics, such as the scheduling algorithm, the number of transactions and the percentage of update transactions, are examined. We compare the number of missed deadlines and approximate answers produced during AQP to the number of missed deadlines occurring during traditional query processing (TQP). Results demonstrate that despite the overhead, fewer deadlines are missed during approximate query processing than during TQP.
41|2|http://www.sciencedirect.com/science/journal/01641212/41/2|Editor's Corner: Software runaways â Some surprising findings1|
41|2||Software design guidelines for event-driven programming|This paper deals with software design principles and guidelines to improve the reusability and maintainability of event-driven (E-D) programs. The paper examines how well the principles of structured software design from the procedural programming field can be applied to the event-driven environment. Taking into account the unique characteristics of event-driven programming (E-DP), additional guidelines that are specific to this field are proposed. The guidelines presented here deal with modularizing event procedures, graphical representation of E-D programs, sharing data between event/general procedures, using user-defined objects, and developing cohesive procedures and forms.
41|2||Dynamic navigation in multiple view software specifications and designs|This paper describes an approach for dynamic navigation among multiple view software specifications and designs. Each view contains graphical and/or textual information about a particular aspect of the system. The paper describes an approach for facilitating access to a specification/design model by mapping the multiple views of the model to an integrated underlying representation and providing a Dynamic Navigation System for navigating through this representation in an application-independent and method-independent fashion. The paper also describes a hypermedia prototype that provides dynamic navigation among the multiple views using different search strategies and paths.
41|2||Promoting business policies in object-oriented methods1|Business policies have been proposed to bridge the gap between business and information system professionals, and at the same time, for easing system evolution. So far, however, most approaches to business policies have been biased towards providing a structural perspective. Here, we argue that there is much to be gained from moving the business-policy idea to a behavioral setting such as the one used in most object-oriented methods. This paper proposes a division of behavioral domain features into two orthogonal dimensions depending on the stability of these features: the event dimension which mainly corresponds to state-transition diagrams that are rarely changed, and the policy dimension which describes restrictions and dependencies among elements on the event dimension that routinely evolve with time. This explicit and separate description of business policies allows the changing of these policies without impacting unnecessarily on the underlying domain, thus easing requirement modifications and finally, enhancing software evolution. The paper addresses policy identification, description and implementation.
41|2||An object-oriented implementation of an entity relationship model|During systems design and implementation, high level models need to be suitably transformed to suit the lower level constructs supported by the implementation language. In cases where an entity relationship (ER) model needs to be implemented in an object-oriented environment, substantial coding is required. This is because many of the concepts in the entity relationship model are not directly supported in object-oriented languages. This tedious and error prone coding can be substantially eliminated. Much of the required codes can be consolidated into a layer of generic ER classes that directly represent the various concepts found in the entity relationship model. Specific ER models for application domains can then be directly modeled as subclasses of the generic ER classes, with minimal coding. Besides reducing coding, these classes also eliminate the transformation task otherwise required of the implementor. The design of the generic ER classes is described and discussed.
41|2||Software reuse using C++ classes: The question of inheritance|The purpose of this research is to explore the productivity benefits gained by software reuse in the object-oriented paradigm. There are two types of object-oriented software reuse: black box reuse (class reuse without modification) and white box reuse (class reuse by deriving a new class from and existing one (inheritance)). Each type is examined for its effect on programmer productivity using two related experiments. The results of these experiments show that black box reuse increases programmer productivity, but the benefits of white box reuse are less clear. In order to derive new classes effectively, the parent (library) class must be understood. This research suggests that black box reuse in the object-oriented paradigm should be encouraged in all programmers, but white box reuse should be performed by a library specialist (manager) to remove the overhead of understanding the library's implementation in order to reuse it effectively.
41|2||Estimation of effort and complexity: An object-oriented case study|The metrication of object-oriented software systems is still an underdeveloped part within the domain of the object paradigm. An empirical investigation aimed at finding appropriate measures and establishing simple, yet usable and cost-effective models for estimation and control of object-oriented system projects, was undertaken on a set of object-oriented projects implemented in a stable environment. First, the measures available were screened for possible correlations; then, the models suitable for estimation were derived and discussed. Effort was found to correlate well with the total number of classes and the total number of methods, both of which are known at the end of the design phase. A number of other models for estimation of the source code complexity were also defined.
41|3|http://www.sciencedirect.com/science/journal/01641212/41/3|Editorial|
41|3||A toolset to support the construction and animation of formal specifications|Model-based specification languages, in particular Z, have been widely used to provide a precise and unambiguous statement of the proposed system as perceived by the developer. However, for many complex specifications, developers cannot themselves be sure about the “intended behaviour” of the specification constructed. This paper reports on an approach and toolset that enables a developer to construct a Z specification using the wiZe editor, demonstrate its properties by transforming it into an executable form using the ZAL animation system, and explore its adequacy by animating a variety of scenarios. The application of the approach and toolset is demonstrated on a specification of a telephone network to illustrate that specification validation could be carried out incrementally during development through investigative scenarios to assess the adequacy of the specification. It is claimed that this interaction, when used in peer review or by the individual developer, can provide enhanced accessibility, a better understood and possibly an improved specification.
41|3||A practical language and toolkit for high-integrity tools|This paper shows how a safe interface to heap storage, based on garbage collection as provided in implementations of pure functional languages, can be combined with imperative languages. It also shows how expressive notation from functional languages, such as algebraic data types and equational definition of functions with pattern matching, can be adopted. The paper argues that the resulting combination is appropriate for the construction of high-integrity tools, based on an assessment against the same criteria as have been used for assessing the suitability of imperative languages for producing high-integrity software.
41|3||A task migration algorithm for heterogeneous distributed computing systems|Most load balancing schemes are based on the assumption that all processors in a distributed computing system (DCS) have similar processing power and workload. However, as technology continues to advance, networks have become more likely to contain processors of different capabilities and configurations. The proposed load balancing method is based on the coterie, a tool first used in mutual exclusion and vote assignment. This paper describes how the unique properties of coteries allow the underlying protocol to be tailored to the capabilities of the individual nodes in a connected network. In general, each processor communicates with only a subset of the other processors in the network, keeping message traffic relatively low. The coterie allows these subsets to be constructed according to a more thoughtful method – one that is designed to better meet the needs of networks of varying characteristics. It is also shown that the proposed method will always perform task migration whenever possible i.e., if at least one lightly loaded processor and at least one heavily loaded processor exist in the network, task migration will occur. Further, an extension of our algorithm to provide fault tolerance is given: when a processor fails, the network can continue to function with a modified system configuration.
41|3||Task routing and resequencing in a multiprocessor system|
41|3||History, principles and application of the SPRINT method|This paper presents the history and the principles of the SPRINT method and some experiences with its application. It is a method for the development of embedded control software of audio/video systems based on the formal language COLD. The method is developed and used in Philips. It is an integrated approach combining three key techniques in software development: specification, prototyping and reuse.
42|1|http://www.sciencedirect.com/science/journal/01641212/42/1|Editorial|
42|1||Measuring the comprehensibility of Z specifications|The effects of natural language comments, meaningful variable names, and structure on the comprehensibility of Z specifications are investigated through a designed experiment conducted with a range of undergraduate and post-graduate student subjects. The times taken on three assessment questions are analysed and related to the abilities of the students as indicated by their total score, with the result that stronger students need less time than weaker students to complete the assessment. Individual question scores, and total score, are then analysed and the influence of comments, naming, structure and level of student's class are determined. In the whole experimental group, only meaningful naming significantly enhances comprehension. In contrast, for those obtaining the best score of 3/3 the only significant factor is commenting. Finally, the subjects' ratings of the five specifications used in the study in terms of their perceived comprehensibility have been analysed. Comments, naming and structure are again found to be of importance in the group when analysed as a whole, but in the sub-group of best performing subjects only the comments had an effect on perceived comprehensibility.
42|1||How to write comments suitable for automatic software indexing1|This paper proposes a strategy for writing the comments (that is, natural-language text) embedded in the source code of software components. The novelty of the strategy is that it suggests to proceed step by step starting from given specifications. At the end of the writing process we have, besides the comments themselves, a set of indices forming a short description of the software component (its profile), which is much easier to manipulate than the full text. By storing profiles into a database a software catalog is built. The availability of software catalogs is strategic to locate reusable components matching specific requirements.
42|1||A classification scheme for software modules|The current abstract view of the detailed architecture of a software program simply focuses on a set of modules that interact with each other. The pictorial view of the architecture of modules abstracts to a directed, connected, acyclic graph whose nodes are modules and whose arcs reveal the reference dependency relationship. No distinction exists among the modules. This paper provides a decided distinction. This paper introduces a new, formal, objective classification scheme of ten classes for modules. The process of fashioning the classification scheme applies the software principle of information hiding to three internal structural components of a module. These components are the variable, the data structure, and the subprogram. Two conceptual constructs enable information hiding. One is the encapsulation of a data structure (object). The other is the inaccessible section of a module which is the section that no external module can reference. Each module belongs to exactly one class. The assignment of a module to a class is objective, not subjective. The scheme opens up the opportunity for a deeper study into each class. The classes of the scheme are subjectively ranked into four categories, ranging from best to worst. The paper lists five benefits of this classification scheme, and relates these benefits to three application areas.
42|1||âParamita wisdomâ password authentication scheme without verification tables|
42|1||An adaptive exponentiation method|Exponentiation of large positive integer with a 512-bit exponent is the basis of several well-known cryptographic algorithms. In this paper, an adaptive method for improving the performance of the m-ary method is proposed and analyzed. Due to the efficient utilization of partial results, it is useful for systems with varied exponent and base. This method is based on two ideas. Firstly, for base x, a few of the exponentiations with smaller exponents are precomputed on-line, that is, x2,x3,…,x(2w−1) are precomputed, where w is an optimization parameter. Secondly, a number of used partial results will be determined and stored in a look-up table during the computation. Assume that squaring is free as compared with multiplication, depending on the numbers of precomputations and partial results, the proposed method on average gives a 26–40% time reduction as compared with the m-ary method. On the other hand, it does require little temporary storage for the used partial results.
42|1||Thread migration and its applications in distributed shared memory systems1|In this paper we describe the way thread migration can be carried in distributed shared memory (DSM) systems. We discuss the advantages of multi-threading in DSM systems and the importance of preempted dynamic thread migration. The proposed solution is implemented in MILLIPEDE: an environment for parallel programming over a network of (personal) computers. MILLIPEDE implements transparent computation migration mechanism: a mobile computation thread in a MILLIPEDE application can be suspended almost at every point during its lifetime and be resumed on another host. This mechanism can be used to better utilize system resources and improve performance by balancing the load and solving ping-pong situations of memory objects, and to provide user ownership on his workstation. We describe how some of these are implemented in the MILLIPEDE system. MILLIPEDE, including its thread migration module, is fully implemented in user-mode (currently on Windows-NT) using the standard operating system APIs.
42|1||Effort estimation and prediction of object-oriented systems1|Due to the growing diffusion of the object-oriented paradigm (OOP) and the need of maintaining under control the process of software development, industries are looking for metrics capable of producing satisfactory effort estimations and predictions. These metrics have to produce results with a known confidence since the early phases of software life-cycle in order to establish a process of prediction and correction of costs. To this end, specific metrics are needed in order to maintain under control object-oriented system development. In this paper, new complexity and size metrics for effort evaluation and prediction are presented and compared with respect to the most important metrics proposed for the same purpose in the literature. The validation of the most important of these metrics is also reported.
42|2|http://www.sciencedirect.com/science/journal/01641212/42/2|Editorial|
42|2||A quantitative approach for assessing the priorities of software quality requirements|There are two challenges in software quality requirements specification and analysis: (a) software requirements are usually imprecise; (b) software quality requirements often conflict with each other. Software quality requirements are often prioritized to resolve their conflicts. However, the priorities of quality requirements are very difficult to assess and determine subjectively. In this paper, a quantitative approach is developed to assess the relative priority of requirements based on the trade-off analysis between conflicting requirements. One technique is based on the trade-off analysis between the satisfaction degrees of requirements and another is based on the marginal rate of substitution in decision science, which specifies the maximal amount of a decision attribute the customer is willing to sacrifice for a unit increase in another decision attribute. A technique is then introduced to transform relative priorities into numeric priorities of requirements so that they can be used in aggregation of requirements. The analytic approach developed in this paper enables an objective assessment of priorities of the requirements, which provides a solid formal foundation for resolving conflicts by prioritizing conflicting requirements.
42|2||An empirical study of the LSS specification toolkit in use|The Lightweight Specification System (LSS) toolkit assists in the development of logic programs, using a variety of high level specification methods. Many other high level specification systems impose a single, uniform view of how specification should proceed. In practice, there is normally no single understanding of how to describe specifications – there are instead a variety of different forms of description which have evolved from the work practices of various domains. Any attempt to disturb these work practices in a radical way will, naturally, meet with resistance unless those who must be educated in new methods can see clearly that they will benefit (soon) from their efforts. LSS addresses this problem by providing a collection of comparatively simple independent tools, each of which is directed at a particular community of users who might reasonably be expected to adjust to the tool without excessive effort. In this sense, LSS is lightweight – it is intended to be easy to pick up. Communication between LSS tools is achieved by using Horn Clause logic as a standard language, although users of some of the tools are buffered from the logical details by interfaces targeted at the appropriate group of users. This allows the products of specification from some of the tools to be used as the basis for more detailed specification (perhaps by other people) using other tools. This paper summarises the current LSS system and describes the results of an experiment in applying it to a substantial software engineering task: the specification of one of its own tools.
42|2||Specifications in software prototyping1|We explore the use of software specifications for software prototyping. This paper describes a process model for software prototyping, and shows how specifications can be used to support such a process via a cellular mobile phone switch example.
42|2||Testing object-oriented programs: from formal specification to test scenario generation|New problems associated with the testing and maintenance of object-oriented programs (OOPs) have been introduced with the dramatically increasing use of OOPs over the past decade. Testing OOPs beyond the class level has been rarely discussed. This paper presents an approach performing high level testing for OOPs based on formal specifications and usage profiles. The behavior of a software system is specified in an object-oriented formal specification. A state model provides a complementary representation of the dynamic behavior. In the model, a state represents the cumulative results of the system behavior. Probability distributions are used to derive the anticipated operation sequences of a program from the state model. An enhanced state transition diagram (ESTD) is used to describe the state model, which incorporates hierarchy, usage and parameter information. This paper describes the construction of state transition diagrams (STDs) based on the formal specification, and the derivation of test scenarios from the ESTD.
42|2||An integrated environment for reuse reengineering C code|The paper presents an integrated environment implemented in Prolog for reuse reengineering existing C systems. Different tools developed in the RE2 project are integrated in the environment through sharing a fine-grained representation for C programs, the Combined C Graph (CCG). Different views of a system can be abstracted and visualised from the data-base of Prolog facts implementing its CCG representation. Software metric tools evaluate the reengineering costs, while reengineering operations are expressed as transformation rules and a symbolic executor allows the production of the reusable module's specification.
42|2||Recombining changes to software specifications1|This paper proposes a model of software changes for supporting the evolution of software prototypes. We decompose software evolution steps into primitive substeps that correspond to monotonic specification changes. This structure is used to rearrange chronological derivation sequences into idealized conceptual derivation structures containing only meaning-extending changes, and to automatically combine different changes to a specification. A set of examples illustrates the ideas.
42|2||Using design abstractions to visualize, quantify, and restructure software1|During design or maintenance, software developers often use intuition, rather than an objective set of criteria, to determine or recapture the design structure of a software system. A decision process based on intuition alone can miss alternative design options that are easier to implement, test, maintain, and reuse. The concept of design-level cohesion can provide both visual and quantitative guidance for comparing alternative software designs. The visual support can supplement human intuition; an ordinal design-level cohesion measure provides objective criteria for comparing alternative design structures. The process for visualizing and quantifying design-level cohesion can be readily automated and can be used to re-engineer software.
42|3|http://www.sciencedirect.com/science/journal/01641212/42/3|Editorial|
42|3||DRDB: Towards distributed real-time database services for time-critical active applications1|Many real-time systems are now being used in safety-critical applications, in which human lives or expensive machinery may be at stake. As real-time systems continue to evolve, their applications become more complex, and often require timely and predictable access to massive amounts of data. This need for advanced data management functionalities in real-time systems poses formidable intellectual and engineering challenges to database researchers and practitioners. Transactions in real-time database systems should be scheduled considering both data consistency and timing constraints. In addition, a real-time database must adapt to changes in the operating environment. The effects of scheduling decisions, and concurrency control mechanisms on real-time database systems have typically been demonstrated in a simulated environment. In this paper we present a real-time database server, called DRDB, which provides an operational platform for research in real-time distributed database issues. Current research issues involving the development of run-time estimates for use in scheduling decisions, temporal functionality, and our efforts in supporting active applications are also discussed.
42|3||Transaction management mechanisms for active and real-time databases: A comprehensive protocol and a performance study|Active and real-time databases (ARTDB) have a variety of applications in electronic brokerages in financial markets, stock trading, network management and manufacturing process control. Transaction processing (TP) in ARTDB is extremely complicated since transactions may trigger other real-time transactions to an arbitrary depth with various types of dependencies (coupling modes). Therefore, transaction processing must be cognizant of not only the time deadlines but also the types of semantic dependencies with other transactions. The conflict resolution between two transactions cannot be considered in isolation since affecting one transaction may affect every other semantically dependent transaction. Similarly, transaction scheduling needs to be compatible with the concurrency control to avoid unnecessary restarts. In this paper we argue that transaction pre-analysis using the pre-declaration paradigm is an efficient mechanism to integrate the various issues of transaction processing such as concurrency control, scheduling, and semantic dependencies. The pre-analysis is possible since in many applications transactions repeat from a set of transaction classes, and the conflicts can be easily determined at a logical level by partitioning relations into mutually exclusive subset (e.g., by stock-id in financial applications). We develop a pre-analysis based transaction processing mechanism called OCCWB. OCCWB is an extended optimistic concurrency control protocol with blocking that combines the benefits of both optimistic and lock based protocols. Such an approach also has an implicit overload management mechanism required in many applications. OCCWB consists of four phases, namely, transaction pre-analysis, serialization ordering, priority adjustment and priority wait. Our protocol is validated using simulation and is shown to outperform existing protocols under various workload and parameter settings.
42|3||Buffer management in real-time active database systems|Real-time, active database systems (RTADBSs) have attracted the attention of researchers in recent times. Such systems are envisioned as control systems for environments as diverse as process control, network management and automated financial trading. Sensors distributed throughout the system report the state of the system to the database. Unacceptable state reports typically result in corrective actions being triggered with deadlines. Thus RTADBSs incorporate both real-time as well as active characteristics. In this paper we study buffer management in RTADBSs. Buffer management is recognized as not being a well studied area in real-time systems. As a result of our work, we postulate (Prefetching Anticipatorily and Priority basEd Replacement) (PAPER), a new buffer management scheme that relies on two strategies: prefetching and priority based buffer replacement. We report the result of studies of the performance of PAPER, as compared to that of existing buffer management algorithms. The insights derived from this paper impact both real-time database systems as well as RTADBSs.
42|3||Transaction processing in distributed active real-time database systems|An active real-time database system (ARTDBS) is designed to provide timely response to the critical situations that are defined on database states. Although a number of studies have already addressed various issues in ARTDBSs, little attention has been paid to scheduling transactions in a distributed ARTDBS environment. In this paper,2 we describe a detailed performance model of a distributed ARTDBS and investigate various performance issues in time-cognizant transaction processing in ARTDBSs. The experiments conducted evaluate the performance under various types of active workload and different distributed transaction-processing architectures. The performance metric used in the evaluations is the fraction of transactions that violate their timing constraints. We also describe and evaluate a nested transaction execution scheme that improves the real-time performance under high levels of active workload.
42|3||Real-time event control in active databases|Most existing languages for the specification of active rules are based on the ECA (event-condition-action) paradigm. Usually, rules specified by such languages are triggered by simple events like the update of a tuple in a relational database. We present a specification language that can be used to specify real-time triggering conditions in terms of complex event patterns. Our specification language can be used to formulate complex, triggering conditions for active rules in terms of event patterns that involve sequences, alternations, iterations, and parallel compositions. Also, the language can be used to specify soft, real-time constraints with respect to the minimum/maximum time required/allowed between pairs of events. Our language is designed to be flexible and general in the sense that it can be combined with most ECA rule languages. We formally define the syntax and semantics of the language and we illustrate its use by means of examples.
42|3||An analysis of lock-based and optimistic concurrency control protocols in multiprocessor real-time databases|Previous studies, e.g., Haritsa et al. (Haritsa, J.R., Livny, M., Carey, M., 1990. Proceedings of Ninth ACM Symposium on Principles of Database systems) have shown that optimistic concurrency control (OCC) generally performs better than lock-based protocols in disk-based real-time database systems (RTDBS). In this paper we compare the two concurrency control protocols in both disk-based and memory-resident multiprocessor RTDBS. Based on simulation, we analyze the intrinsic behaviors of the two protocols. The result of our performance evaluation experiments show that different characteristics of the two environments indeed have great impact on the protocols' performance. We identify such system characteristics and expose the weaknesses of traditional OCC and lock-based protocols. To improve performance, a new protocol, called Two Phase Locking-Lock Write All (2PL-LW), is proposed. We show that 2PL-LW performs better than the traditional protocols in meeting transaction deadlines in both disk-based and memory-resident RTDBS.
43|1|http://www.sciencedirect.com/science/journal/01641212/43/1|Editorial|
43|1||A criticism on the capture-and-recapture method for software reliability assurance|There are not a few literature claiming that the capture-and-recapture method (CR method) can estimate the number of remaining software bugs during the test process and thus enables us to decide when to stop testing. Although the decision when to stop testing can only be made when there is ample confidence that the number of the remaining bugs is less than a certain limit, most of the literature just makes a point estimate of it using a simple proportional formula without considering the level of errors. Some of the literature touches on the fact that the recaptured bug number is given by the hypergeometric distribution, but none uses the confidence interval estimate effectively. This paper derives a formula giving a confidence interval estimate for the seeded bug ratio. Then it presents a virtual experiment to apply the CR method. The paper concludes that the CR method cannot be effectively applied to software testing because it overestimates the number of bugs to be detected before testing can be stopped, and that the CR method is not applicable at all to the final phase of testing because it is very difficult to make so many bugs to be seeded that are equivalent to those remaining in the target software.
43|1||Reverse engineering of software threads: A design recovery technique for large multi-process systems1|Many software systems currently being deployed in both military and industrial domains serve their users through a large number of concurrent processes. Such systems are expensive and time consuming to build and thus cannot be lightly discarded when circumstances change. Instead, the systems must be able to evolve to meet new challenges. But a software system cannot evolve without an understanding of its design, and such understanding is easily lost when the developers disperse. A particularly difficult problem may be to understand how the many processes cooperate to produce each functionality that the user needs. This paper presents a novel, but relatively simple method for using traces of inter-process messages to recover functional software design threads from a large, multi-process system. The method is illustrated with a preliminary case study of the Joint STARS battle management software. The method seems to be effective in identifying the “as built” threads, and in providing unexpected insights even to Software Engineers who are relatively familiar with the system. Design thread recovery for Joint STARS is feasible because instrumentation to capture inter-process messages was designed into the original system. Our results suggest that the designers of new systems should give thought to including such instrumentation since the cost is small and the future economic benefits in easier evolution could be very large.
43|1||How well do experienced software developers predict software change?|Requirements-driven impact analysis (rdia) identifies the set of software entities needed to be changed to implement a new requirement in an existing system. The input is a set of requirements and the existing system. The output is, for each requirement, a set of software entities that have to be changed. The output is used as input to many project-planning activities, for example cost estimation based on change volume. This paper quantifies how well experienced software developers predict change by conducting rdia, where rdia in this case is the general activity of predicting changes based on change request. The means has been an empirical study of rdia in the industrial object-oriented pmr-project. rdia has been carried out in two releases, R4 and R6, of this project as a normal part of project developers' work. This in-depth case-study has been carried out over four years and in close contact with project developers. The correctness of the prediction is high while problems with underprediction have been identified – many more classes than predicted are changed. We have also found that project developers are unaware of their own positive and negative capabilities in predicting change.
43|1||A study of atomic action schemes intended for standard Ada|Although the number of proposals discussing various atomic action schemes is increasing, these schemes are very rarely used in designing practical applications. To a large extent, this is accounted for by the gap existing between the languages used in research and the standard or widely spread languages (e.g. C, C++, Ada 83, Ada 95, Java) employed by practitioners. Moreover, very often researchers extend languages with new features or invent new languages to express their ideas better. Even though these approaches seem to be quite natural, they widen the gap between practice and research. To bridge this gap, we should consider fault tolerance schemes in terms of a standard language, taking the language itself for granted. The question which we believe should be addressed is how to use/implement a particular scheme in these languages rather than how to modify the language. Only in this way the schemes could be used directly and the application domains of atomic action schemes extended. The main intention of this paper is to summarise research that has been done in the last years in designing various atomic action and conversation schemes in Ada 83 and Ada 95. This should give a fuller picture of the existing schemes for researchers and help practitioners to choose the appropriate schemes. We would also like to raise and discuss some questions concerned with moving fault tolerance schemes into standard languages and environments. Finally, we intend to discuss the likely directions of future research in this area.
43|1||Exploiting and-or parallelism in Prolog: The OASys computational model and abstract architecture|Different forms of parallelism have been extensively investigated over the last few years in logic programs and a number of systems have been proposed. Or/And System (OASys) is an experimental parallel Prolog system that exploits and-or-parallelism and comprises a computational model, a compiler, an abstract machine and an emulator. OASys computational model combines the two types of parallelism considering each alternative path as a totally independent computation which consists of a conjunction of determinate subgoals. It is based on distributed scheduling and supports recomputation of paths as well as stack copying. The system features modular design, high distribution and minimal inter-processor communication. This paper presents briefly the computational model and describes the abstract machine discussing data representation, memory organization, instruction set, operation and synchronization. Finally performance results obtained by the single-processor implementation and the multiple-processor emulation are discussed.
43|1||An assessment of systems and software engineering scholars and institutions (1993â1997)|
43|1||A narrative history and description of MOSIS software|This article explores the major software systems used by MOSIS since its inception in 1980, developed by the author. The MOSIS project (Metal Oxide Silicon Implementation System) has served the nation by turning VLSI (microchip) designs, submitted over e-mail, into fully packaged chips, sent back to the user via US mail. Before MOSIS, chip designers were faced with a prohibitive fabrication expense, and the daunting tasks of augmenting their designs with various tedious geometries and coordinating the various vendors ranging from mask making for the fabrication of silicon to packaging. The major innovation has been the “sharing of silicon”. The various “packers” that implement silicon sharing are explored, as well as the other requisite software.
43|2|http://www.sciencedirect.com/science/journal/01641212/43/2|Editorial|
43|2||A case study of evolution in object oriented and heterogeneous architectures|In order to properly understand a technology, it is important to know not only its strengths but also its limits. In this paper, we investigate the limits of object oriented technology. We present a case study in which we compare two different architectures for the same program – a student registration system which belongs to the domain of reactive repositories. We started our case study with a model produced by Object Modeling Technique (OMT), and used it for two different implementations: a homogeneous one (HOA) and a heterogeneous one (HTA). The HOA results in an object oriented system consisting of classes and their dependencies, while the HTA combines traditional imperative programming with a relational database. We evaluated the evolvability of both resulting programs by adding new use cases. HTA proved to be easier to evolve than HOA. In the paper, we discuss possible reasons for this finding.
43|2||TARILAN: an embedded functional data processing language|TARILAN is a functional programming language, specialized for data processing tasks. Its intended application field is in complex data processing tasks where flexibility is more important than high performance. The flexibility is the result of a separation of the business rules that control the transformation from the transformation itself. The present work shows how a subset of functional and logic programming can be used to express complex transformation rules. The TARILAN system is now in use for more than two years and proves to be a powerful environment in practice.
43|2||Humanâcomputer interaction: Interdisciplinary roots and trends|Methodology, theory, and practice in the field of Human–Computer Interaction (HCI) all share the goal of producing interactive software that can be used efficiently, effectively, safely, and with satisfaction. HCI is cross-disciplinary in its conduct and multidisciplinary in its roots. The central concept of HCI is usability, ease of use plus usefulness. Achieving good usability requires attention to both product and development process, particularly for the user interaction design, which should serve as requirements for the user interface software component. This paper reviews some of the theory and modeling supporting the practice of HCI, development life cycles and activities, and much of the practice that constitutes “usability engineering”. Future application areas of interest in HCI include new interaction styles, virtual environments, the World Wide Web, information visualization, and wearable computing.
43|2||F-metric: a WWW-based framework for intelligent formulation and analysis of metric queries|As an organization matures, quantitative techniques are employed to make software project management more systematic, informed and under control. This is typically done through the collection and analysis of software metrics to measure the performance of projects. While many organizations utilize software metrics to analyze project issues and answer management queries, the manner in which such queries are answered varies from organization to organization. As the number of metrics grows, interpretation of raw metrics in the context of management goals becomes difficult. The method and tool that we developed attempts to bridge the gap between the project managers' mental model of a project and raw software metrics. Based on industrial surveys, we identified types of queries about the project progress, resources and schedule that project managers often ask during project execution. Next, we built a conceptual model for project metrics relevant to those queries, justifying our selection of a core set of test and evaluation metrics. We defined a project management query language (PMQL for short), based on concepts of an extended Entity-Relationship model, which we used to formulate project queries at a conceptual level. Finally, we present a flexible WWW-based framework which can be used by experts to formulate and update management queries in PMQL, modify heuristics based on which queries are evaluated, as well as to dynamically display solutions to existing queries. The flexibility of our framework means that new queries may be added, heuristics to queries may be changed, and display of results can be changed on-the-fly via mutual cooperation between managers and technical experts of the organization without any changes to code.
43|2||Comparative study and categorization of high-level petri nets|The graphical formalism of Petri Nets (PNs) is established on a strong mathematical foundation that can be applied in systems specification, analysis and verification. However, classical (low-level) models suffer from the state explosion problem as resulting PNs become larger. Thus, their ability to represent and analyze realistic large scale systems is reduced. High-level PNs have been introduced in order to extend the modeling power of low-level models. This paper presents an assessment of high-level PNs from an engineering perspective. A set of categories is proposed for classifying several extensions presented in the literature. Models which belong to the same category are compared by discussing the formalism, the descriptive power and the inherent limitations of each. All categories are compared using a set of general criteria including compactness, ease of analysis, degree of supporting refinement/abstraction and specifying communication. The modeling power of representative models of each category is discussed by presenting illustrative application examples.
43|3|http://www.sciencedirect.com/science/journal/01641212/43/3|Editorial|
43|3||User cognitive representations: The case for an object oriented model|This study empirically investigates the effect on user performance using object oriented cognitive structures. The results indicate improvement in the quality (measured by response time and error rate) of decisions for subjects who were monitored in an experimental setting. These findings indicate the presence of object oriented properties in user cognition. Such an inference puts the theorized mechanisms of human information processing such as, cognitive economy, and limited storage space, in proper perspective. The study discusses implications of these findings for requirements analysis, user-interface design, and training.
43|3||Combining OMT with a prototyping approach|Using the OMT software engineering methodology, a designer starts by identifying objects from the informal requirements specification. By identifying the relations between these objects, a global object model can be produced. Then, in an iterative process, this initial model is refined into an actual program. In this paper, we argue that the OMT methodology does not really help to analyze the requirements of a new proposed parallel system, and suggest combining OMT with the ProSet-Linda prototyping approach to requirements analysis of parallel systems to overcome some of the problems. The experience with this combination for the analysis of the requirements on a specific parallel system – a configurable hospital communication server – is discussed.
43|3||The ProSet-Linda approach to prototyping parallel systems|Parallel programming is conceptually harder to undertake and to understand than sequential programming, because a programmer often has to manage the coexistence and coordination of multiple parallel activities. Prototyping is used to explore the essential features of a proposed system through practical experimentation before its actual implementation to make the correct design choices early in the process of software development. Approaches to prototyping parallel algorithms with very high-level parallel programming languages intend to alleviate the development of parallel algorithms. To make parallel programming easier, early experimentation with alternate algorithm choices or problem decompositions for parallel applications is suggested. This paper presents the ProSet-Linda approach which has been designed for prototyping parallel systems.
43|3||On heuristics for optimal configuration of hierarchical distributed monitoring systems|This paper considers the problem of optimal configuration of the monitoring units in a hierarchical distributed monitoring system. A hierarchical distributed monitoring system consists of a hierarchy of monitoring units which are grouped and distributed onto the physical network. The architecture lends itself to parallel processing, reducing the complexity of distributed monitoring caused by factors such as collection and processing of the large quantities of monitoring data. Furthermore, the topology-specific partitioning of the monitoring units allow complex, topology-specific events to be monitored and evaluated in a natural and efficient way. The optimal configuration problem is concerned with finding an optimal hierarchical partition of the monitoring units such that the total processing cost is the minimum. It is a NP-complete problem. In this paper, we study the heuristics for obtaining near-optimal grouping of monitoring units. Simulation of heuristic algorithms for mesh and hypercube networks are presented. The results suggest further system topology specific heuristics. Although the paper is targeted at distributed monitoring, we believe that the results can also be applied to other hierarchical control problems in distributed computing.
43|3||A taxonomy of distributed termination detection algorithms|An important problem in the field of distributed systems is that of detecting the termination of a distributed computation. Distributed termination detection (DTD) is a difficult problem due to the fact that there is no simple way of gaining knowledge of the global state of the system. Of the algorithms proposed in the last 15 years, there are many similarities. We have categorized these algorithms based on the following factors: algorithm type (e.g., wave, credit-recovery), required network topology, algorithm symmetry, required process knowledge, communication protocol (synchronous or asynchronous), communication channel behavior (first-in first-out (FIFO) or non-FIFO), message optimality, and fault tolerance. This methodology is intended to guide future research in DTD algorithms (since research continues on these algorithms) as well as to provide a classification survey for this area.
43|3||On using similarity for concurrency control in real-time database systems|Most of the proposed concurrency control protocols for real-time database systems (RTDBS) are based on serializability theorem. Owing to the unique characteristics of real-time database applications and the importance of satisfying the timing constraint of the transactions, serializable concurrency control protocols are not suitable for RTDBS for most cases. In this paper, another notion of correctness, similarity, is used for concurrency control in a RTDBS, for instance, a stock trading database system. Similarity is a less restrictive notion comparing with serializability. By studying the correctness requirements of the stock trading database applications, a real-time two phase locking protocol, High Priority 2 Phase Locking (H2PL) is re-defined based on similarity. Although the new protocol cannot ensure serializability, the concurrency of the system is higher and the amount of inconsistency in the database is tolerable. On the other hand, the performance of the whole system can be much improved.
43|3||An effective model for composition of secure systems|In this paper we present a composable security property based on the principle of an existing one that was developed for the non-interference requirement and used to deal with composition of components linked as a chain. This new property is more general in the sense that it can handle composition of components connected as a tree, and allow each component and their system to meet different security requirements. The applicability of the new property is then assessed based on a case study of security analysis of a file system. Useful observations derived from this case study are finally discussed which can help to develop cost-effective approaches to design and evaluation of secure systems.
44|1|http://www.sciencedirect.com/science/journal/01641212/44/1|Editorial|
44|1||Improving the performance of distributed shared memory systems via parallel file input/output|
44|1||Classifying and alleviating the communication overheads in matrix computations on large-scale NUMA multiprocessors1|
44|1||Prototyping experiences with classical IP and ARP over signaled ATM connections|This paper discusses a prototyping effort in the classical internet protocol (IP) and address resolution protocol (ARP) over asynchronous transfer mode (ATM) model. It introduces the important features of the involved protocols, motivates the interaction between IP and ATM, and examines several possible scenarios for it. The design and implementation of a driver prototype for an ATM network adapter are described. The prototype utilizes and dynamically manages signaled ATM connections in a way that is transparent to user processes. The paper shares our experiences from the design and implementation of this prototype, such as the lessons learned how to deal with problems we ran into that were not anticipated. The lessons of this paper are applicable to a wide range of software prototyping efforts, not only the system described.
44|1||Applying complexity measures to rule-based prolog programs|Four measures are developed and empirically tested that attempt to quantify the complexity of rule-based Prolog programs in terms of measuring the dependency between rules. The measures are lines of code (LOC), number of rules (NR), number of unique predicate nodes (UPN), and McCabe's cyclomatic complexity model, v(G). The UPN and v(G) measures are developed by reducing a Prolog program to its most abstract graphical form and then counting the nodes and arcs represented by the graph. The four measures are tested in terms of their ability to distinguish between a set of 80 professionally developed programs, divided into groups of error-prone and error-free programs. Unpaired student's t-tests showed that UPN is a significantly better measure of complexity (Ï=0.002–0.006) than any of the other measures (Ï=0.096–0.609). Finally, by plotting the cumulative frequency of errors across the sample, it is possible to observe a threshold point of around 35 ± 5 UPN, above which Prolog programs contain significantly more errors (Ï=0.000).
44|1||High-performance Arabic character recognition|Many Arabic character recognition systems have been proposed since the early eighties. Most systems reported high recognition rates, however, they overlooked a very important factor in the process; the speed factor. In this paper, a high-performance Arabic character recognition system is introduced. The goal of the system is to maximize both accuracy and speed. The goal has been achieved through developing a high-accuracy sequential Arabic character recognition system and, then mapping it into a multi-processing environment. Experimental results show that the multi-processing environment is very promising in enhancing a sequential Arabic character recognition system performance.
44|1||A spatial match retrieval mechanism for symbolic pictures|Spatial relationships are important ingredients of spatial access methods in retrieval mechanism systems for pictorial or multimedia databases. We suggest a one-to-one mapping method of retrieving the symbolic pictures in a pictorial database based upon the spatial relationships among the objects in the picture. In our mechanism, each picture or query is transformed into a set of ordered triples (Oi,Oj,rij)'s, where Oi and Oj are two objects and rij is the spatial relationship between Oi and Oj. Then we construct a one-to-one mapping table for all (Oi,Oj,rij)'s of all the pictures in the pictorial database. By searching the preconstructed mapping table for all of the (Oi,Oj,rij)'s associated with a query, the desired pictures can be easily determined. Besides, the proposed spatial match retrieval approach can be easily embedded into various systems such as parallelism systems or distributed database system.
44|2|http://www.sciencedirect.com/science/journal/01641212/44/2|Editorial|
44|2||A comparative evaluation of CASE tools|CASE tools are complex software products offering many different features. Systems professionals evaluated various CASE products from a feature and attribute basis. Each product has a different mix of strengths and weaknesses as perceived by the end user. Specific CASE tools support different steps of the applications development process as well as varying methodologies. The complexity of software development, diversity of tools and features leads to several questions. What CASE features are being used by systems developers? What areas of CASE tolls need improvement? Are some CASE tools or product attributes considered to be better than others?
44|2||CASE and software maintenance practices in Singapore|Many claims have been made about the benefits of CASE and many problems have been reported about software maintenance. This paper discusses two aspects from a survey recently conducted in Singapore on Software Engineering Practices. These two aspects include: how organisations in Singapore use CASE tools and how organisations in Singapore practise software maintenance. The survey indicated that the level of usage of CASE tools in Singapore is fairly low. Organizations in Singapore are generally well aware of the benefits and problems in CASE. The major barriers to the use of CASE tools were the high cost of implementing CASE tools and the long learning curve to use CASE tools effectively. On the practice of software maintenance, the survey found that the greatest problem with it was staff turnover.
44|2||Supporting software maintenance with software engineering tools: A Computed taskâtechnology fit analysis|Management has turned to software engineering tools designed to support software maintenance as a potential solution to maintenance productivity and quality problems. Once adopted by an organisation, however, these tools are often not used. A research model, based on a task–technology fit (TTF) model, is developed to explain the factors which lead to the use of the software maintenance support tools. Our model, which examines the nature of the fit between software tool functionality and maintenance task demands, consists of a TTF model augmented with a model of the maintenance task and a model of software maintenance tool functionality. Such an augmented model is necessary for moving beyond isolated exploratory studies of maintenance and for building on the existing research in software development and software tool utilization. Using this model, fit between maintenance task and technology characteristics is computed for two dimensions of fit derived from the task and technology models. Tests of hypotheses derived from the model demonstrate that task–technology fit, computed using methods for computing strategic fit, is associated with increased tool utilization. Our findings provide direction for the development of better maintenance support tools. This research extends the application and usefulness of TTF models and maintenance task and technology models so that future software maintenance research can build on tested models.
44|2||A method for the identification of reusable units through the reengineering of legacy code|This paper describes a method for reengineering legacy systems into potential reuse candidates so that they can eventually be replaced by more flexible and maintainable software. The method consists of 10 steps to obtain the reuse candidates and employs both the analysis of code and the assistance of domain specialists. The inclusion of non-technical staff within a project team has placed communication constraints on the method. To aid the communication process between technical and non-technical staff, the method relies heavily on the use of graphical representations for each of the stages of the analysis process. This paper includes examples of the typical graphical representation that could be obtained from performing many of the stages of the method. The method has been geared towards COBOL business applications, but some indications are given of how the method could also be applied to systems written in C. Twelve case studies have been used to validate varying aspects of the method. The most detailed of these case studies is described within this paper. The initial steps of the method are concerned with SECTION dominance. Results have been promising, however, the necessity to perform detailed data analysis has been shown.The data analysis steps are performed at the later stages of the method. In addition, the necessity for commercial strength tools to reduce the time to perform steps would be required to make the method more acceptable for very large systems. Finally, this paper looks at some ways in which the method benefits software engineering and makes some recommendations as to how the method can be improved.
44|2||A comparison of measurement and defect characteristics of new and legacy software systems|This paper compares the quality characteristics of a large legacy software system and a new one. Defect fixes applied to specific modules during testing are used as the direct quality metric. Indirect quality indicators used in this paper include software metrics of various product and process attributes, including design, size, change, and complexity. We analyze and compare the measurement results by examining their individual distributions, the correlations between defects and quality indicators, and tree-based models linking defects to quality indicators. In both these systems, most of the defects are found to be concentrated on relatively few high-defect modules, which points to the need for appropriate risk identification techniques so that defect removal effort can be focused on those high-defect modules for effective quality improvement. In addition, defects in the legacy system are more closely related to change and data complexity metrics; while defects in the new system are more closely related to various design metrics. These results demonstrate different measurement characteristics for these two types of software systems, and suggest that different quality analysis and improvement methods may be more appropriate and effective for different kinds of software systems.
44|2||Metrics for object-oriented software projects|This paper puts forward an analysis of two software projects developed at the Jet Propulsion Laboratory (JPL). The two projects are The Micro Generic Controller (UGC) and The Sequence Generator (SEQGEN). Both projects use the object-oriented paradigm and the C++ programming language. Object-oriented metrics and other measures are implemented to analyze and compare the two projects.
44|2||Another metric suite for object-oriented programming|Chidamber and Kemerer (C&K) proposed six metrics for object-oriented programming. Discussions about the metrics were also reported. Recently, Kitchenham and her colleagues proposed a framework to validate software metrics. This paper evaluates C&K metrics by using Kitchenham's metric-evaluation framework and finds deficiencies in some of the C&K metrics. In order to remedy the deficiencies, this paper proposes another metric suite for object-oriented programming. The new metric suite, also six in number, includes Number of Ancestor Classes (NAC), Number of Local Methods (NLM), Class Method Complexity (CMC), Number of Descendent Classes (NDC), Coupling Through Abstract Data Type (CTA), and Coupling Through Message Passing (CTM).
44|2||A study of the sensitivity of software release time|Software release time determination is of great importance to software developers. In order to predict the amount of testing needed and to estimate the release time at which a certain reliability target is met, probabilistic models can be used to predict the reliability level achieved during the software testing. However, most of the results assume that the model parameters are known which is not true since they have to be estimated based on the information available. In many cases, they are inaccurate because of the small amount of data available. Hence, it is useful to know the sensitivity of software release time with respect to the estimated parameters so that attention can be paid to those parameters that affect the release time significantly. In this paper, the sensitivity issue is studied for a commonly used software reliability model. The study can be used, for example, if an overestimation of a parameter implies an underestimation of the release time which can be costly as more failures are experienced by the consumers, we should try not to overestimate that parameter. Also, if a parameter affects the release time more than others, it is important to have this parameter estimated as accurately as possible. The interval estimation of release time is recommended to avoid further excessive adjustment of release time.
44|3|http://www.sciencedirect.com/science/journal/01641212/44/3|Editorial|
44|3||Cognitive design elements to support the construction of a mental model during software exploration|The scope of software visualization tools which exist for the navigation, analysis and presentation of software information varies widely. One class of tools, which we refer to as Software exploration tools, provides graphical representations of static software structures linked to textual views of the program source code and documentation. This paper describes a hierarchy of cognitive issues which should be considered during the design of a software exploration tool. The hierarchy of cognitive design elements is derived through the examination of program comprehension cognitive models. Examples of how existing tools address each of these issues are provided. In addition, this paper demonstrates how these cognitive design elements may be applied to the design of an effective interface for software exploration.
44|3||Assessing the maintenance process through replicated, controlled experiments|This work describes a controlled experiment comparing maintenance process derived from two different paradigms: Quick Fix (Q.F.) and Iterative Enhancement (I.E.). It has been repeated twice with undergraduate students and once with professional developers (I.E.). The first time served to improve the material for successive replications. The experiment aimed to ascertain the quality of the maintenance process in terms of the correctness and completeness of the modifications made and whether Q.F. processes are more timely than I.E. From the results of the experiments it would seem that on the one hand, the Q.F. is not appreciably more timely than I.E., while on the other, it results in lesser correctness, completeness and traceability. Thus, Q.F. damages the comprehensibility of the system more then I.E. does. In any case, the software quality must be safeguarded after maintenance, as even I.E. is not entirely free from harmful effects.
44|3||Identifying objects in legacy systems using design metrics|Many organisations are migrating towards object-oriented technology. However, owing to the business value of legacy software, new object-oriented development has to be weighed against salvaging strategies. The incremental migration of procedurally oriented systems to object-oriented platforms seems to be a feasible approach, although it must be considered as risky as redevelopment. This approach uses reverse engineering activities to abstract an object-oriented model from legacy code. The paper presents a method for decomposing legacy systems into objects. The identification of objects is centred around persistent data stores, such as files or tables in the database, while programs and routines are candidates for implementing the object methods. Associating the methods to the objects is achieved by optimising selected object-oriented design metrics. The rationale behind this choice is that the object-oriented decomposition of a legacy system should not result in a poor design, as this would make the re-engineered system more difficult to maintain.
44|3||Points-to analysis for program understanding|Program understanding activities are more difficult for programs written in languages (such as C) that heavily make use of pointers for data structure manipulation, because the programmer needs to build a mental model of the memory use and of the pointers to its locations. Pointers also pose additional problems to the tools supporting program understanding, since they introduce additional dependences that have to be accounted for. This paper extends the flow insensitive context insensitive points-to analysis (PTA) algorithm proposed by Steensgaard, to cover arbitrary combinations of pointer dereferences, array subscripts and field selections. It exhibits interesting properties, among which scalability resulting from the low complexity and good performances is one. The results of the analysis are valuable by themselves, as their graphical display represents the points-to links between locations. They are also integrated with other program understanding techniques like, e.g., call graph construction, slicing, plan recognition and architectural recovery. The use of this algorithm in the framework of the program understanding environment CANTO is discussed.
44|3||Evaluation of the ITOC information system design recovery tool: a case study|Most contemporary fourth-generation languages (4GLs) are tightly coupled with the database and other subsystems provided by the vendor. As a result, organisations wishing to change database vendors are typically forced to rewrite their applications using the new vendor's 4GL. The anticipated cost of this redevelopment can deter an organisation from changing vendors, hence denying it the benefits that would otherwise result, for example, the exploitation of more sophisticated database technology. If tools existed that could reduce the rewriting effort, the option of changing database vendors would become more economically feasible. The ITOC project is a large collaborative research initiative between the Centre for Software Maintenance at the University of Queensland and Oracle Corporation. The primary goal of the project is to develop tools to assist in the migration of 4GL information system applications. This assistance is in the form of both automated reverse engineering and support for legacy system program comprehension. A tool resulting from the project has been utilised to recover design information from several deployed commercial applications. This information is presented in a graphical form using existing tools to facilitate both program comprehension, and also modification of the design prior to forward engineering if desired. This paper describes the tool, evaluates its result when applied to these applications and provides insight into the development of “industrial strength” information system re-engineering tools.
44|3||Using Belbin's leadership role to improve team effectiveness: An empirical investigation|This paper presents a controlled experiment conducted with senior software engineering students that demonstrates the utility of forming teams based on R. Meredith Belbin's set of team roles. The overall research effort focuses on the general utility of Belbin's roles in improving the effectiveness of teams, which can be viewed in two ways: performance and team viability. Performance effectiveness, which is the focus of this paper, clearly addresses a team's productivity. To address this problem, the first phase of the total research project consists of a controlled experiment that demonstrates that teams containing leadership roles perform better than teams that do not have this role filled. In a laboratory setting, a number of teams were formed that contained a single leader; others were formed that had no leader or multiple leaders. The results of this experiment are positive; they demonstrate that indeed Belbin's roles provide useful information to form teams. The specific conclusion of this controlled experiment is that a single leader on a team improves a team's performance over teams having multiple leaders or no leader. In other words, as one would expect, the mean time-to-completion for the leaderless teams was significantly greater than the teams with leaders. This means that Belbin's roles can be utilized in formation of new teams as well as in evaluation of extant teams, making certain that a team has a single leader. Both of these aspects, formation and evaluation, are extremely useful to managers of software programmers.
44|3||Ethical considerations of the software-dependent organization|Software use offers organizations many benefits, however it also makes the organization vulnerable to the potentially harmful effects which can result from software failure. While much has been written about the direct results of software failure, little examination of its long term consequences has taken place. A basis for such an examination can be found in the body of literature related to ethics. For example, Moor examines issues of computer related ethics based on the nature and impact of computer technology. Other writers such as Jones identify causes and consequences of ethical and unethical behavior within the organization. This short piece uses an ethical perspective to undertake a preliminary examination of software failure and its effects on the organization.
45|1|http://www.sciencedirect.com/science/journal/01641212/45/1|Editorial|
45|1||Assessment of a renewal process experimented in the field|This work describes a case study which demonstrates the efficacy of a renewal process of a very old system, judged from the viewpoint of users and managers. Data were collected over three years in a real environment to assess the effectiveness of the process, performed on a banking software system. The effects attributable to the progressive ageing of a system perceived by its users and managers were first individuated. After the process, such successes as a drop in the costs of maintenance and an increase in transferability and testability of the programs were recorded. There were also some failures, notably insufficient improvement in the quality of the programs and in their capacity to be modified. Finally, some conclusions are drawn as to the opportunity of subdividing the renewal process into two macrophases, reverse engineering and restoration, which integrate the process activities generating the benefits. Although these are conceptually distinguishable, they must be executed together to obtain the desired benefits from the renewal process.
45|1||Toward an integration of data flow and domain testing|
45|1||OBJECTIVE: a benchmark for object-oriented active database systems|
45|1||Satisfying temporal consistency constraints of real-time databases|In addition to timing constraints, a real-time database has temporal consistency constraints for its temporal data. The temporal consistency constraints require the data to represent a state of the real-world that is up-to-date and also require data to represent past states of the real-world with values that are close in time. Factors, such as concurrency control, can cause transactions to miss their deadlines and data to become temporally inconsistent. Approximate query processing (AQP) has been proposed as a strategy to satisfy the timing constraints of real-time databases. Approximate answers are provided by AQP if it is not possible to produce an exact answer by a specified deadline. In this paper, we examine the temporal consistency of the data during traditional and AQP. Four metrics of temporal consistency are utilized to compare the age and dispersion of the data during traditional query processing (TQP) versus approximate query processing. Simulation results identify factors, such as the concurrency control algorithm, the number of transactions and the percentage of query transactions, that affect the temporal inconsistency of the data.
45|1||Architecture-driven modeling of real-time concurrent systems with applications in FMS1|Petri nets have become increasingly popular for flexible manufacturing systems (FMS) modeling and control because they accurately capture the concurrent, non-deterministic and time-dependent properties of the systems. While offering many advantages, conventional Petri net models suffer from some serious problems that limit their usability as design models for complex FMS. Central to these problems is the lack of an engineering support for incremental design, refinement, and analysis of large-scale systems. In this paper, we present an architecture-driven approach for the modeling and design of FMS that effectively addresses the problems while leveraging the strengths of Petri nets. The approach has two major aspects: The first is a Net-based and Object-based Architectural Model (NOAM) that introduces a well-founded architectural framework into the Petri nets notation and lays a foundation to support formal design. The second is a modeling method based on NOAM that uses architecture decomposition and refinement as the basis to reduce design complexity, to provide smooth transition from informal to formal design, and to support incremental refinement and analysis. A case study using NOAM for FMS modeling is provided to show the applicability of our approach.
45|2|http://www.sciencedirect.com/science/journal/01641212/45/2|Editorial|
45|2||Making distributed applications manageable through instrumentation1|
45|2||A classification of multicast mechanisms: implementations and applications|This paper presents a taxonomy of multicast protocols in distributed computing systems. It classifies multicast protocols based on a four-dimensional model which is derived from the analysis of both application requirements and design goals of multicast mechanisms. The four dimensions are atomicity, ordering, real-time delivery and fault tolerance. Multicast protocols can be described by parameters in these four dimensions with different degrees of strength. This classification model captures the main features of all multicast mechanisms. It can help users to: (1) describe, analyze and evaluate multicast mechanisms; (2) specify clearly the required multicast properties; (3) build more structured multicast mechanisms and reduce the complexity of implementations; (4) design flexible multicast mechanisms which customize users' needs at run-time.
45|2||Communicating object group and protocols for distributed systems|Distributed communicating object groups, relevant algorithms and object communication protocols are presented. The distributed communicating object group system (called DCO) is built on the basis of hierarchy group communication protocols for networks. The system can be used as a specification and design environment for distributed fault-tolerant object-oriented (OO) applications. By combining distributed process group with the powerful OO concepts, the novelty of DCO lies in its provision of methodology for construction of communicating object groups, reliable communication protocols, group membership and management. The mechanisms of grouping user objects enable application objects to communicate transparently over network. Communication details implemented by underlying group multicast protocol are thus hidden from users. This makes the objects completely responsible for their modification, protection, execution and communication. DCO is used to relieve application designers from difficulty of designing object communication and reliability. Multiple layers of object group protocols are given and their design and implementation experiences are presented.
45|2||QoS specification and mapping for distributed multimedia systems: A survey of issues|The drastic improvement of network performance in terms of throughput and error rate coupled with the continuous increase in power of end-systems have led to the emergence of a variety of distributed Multimedia (MM) applications. The performance levels required by the distributed MM systems are expressed as Quality of Service (QoS). Network QoS support alone cannot guarantee the real-time performance required for distributed MM applications. Successful operation of distributed MM systems depends on the guaranteed support of all intervening layers including the end-systems. The guaranteed performance of these layers in turn is affected by the efficiency of the support provided by: QoS specification, QoS mapping or translation, QoS negotiation and re-negotiation, Resource monitoring and adaptation, and QoS architectures. Most people usually have a good understanding of a certain layer but not the others. However, to be able to deliver MM QoS, developers need to have a better understanding of several layers. This paper focuses on QoS specification and mapping and aims to provide a survey of issues for those working on MM QoS so that they have a better view on these two areas which are important to MM QoS.
45|2||Rapid prototyping of distributed algorithms|Rapid prototyping is expected to be especially effective in designing, evaluating, and tuning new software whose real-time performance and quality of service are difficult to determine analytically. Distributed algorithms and protocols fall into this category of software. In this paper we present the design and implementation of a rapid prototyping system for distributed algorithms. The system provides a library of interprocess communication routines and supports dynamic reconfiguration of network topology, automatic process loading and performance monitoring of the prototyped algorithms. We discuss our experiences in developing the rapid prototyping system on a transputer network and describe examples of using the system.
45|2||Fast and simple decomposition techniques for the reliability analysis of interconnection networks|To study and analyze the reliability of complex systems such as multistage interconnection networks (MINs) and hierarchical interconnection networks (HINs), traditional techniques such as simulation, the Markov chain modeling, and probabilistic techniques have been widely used. Unfortunately, these traditional approaches become intractable when dealing with real-life applications. For the purpose of reliability analysis, the Petri net (PN) theory can also be used, but the major drawback of PN models is the state space explosion with increasing model complexity. Therefore, in this paper, we propose two analytical decomposition techniques for computing (approximately) the transient state space solution of large stochastic PN (SPN) models of MINs and HINs. In the first technique, a large scale SPN model is partitioned into smaller submodels. These submodels are compressed and combined to calculate the entire net. Throughout our reliability analysis of a simple example, we will understand how to decompose an entire net of a modeled system into subnets and what kinds of quantities are required to pass from one subnet to another. The modeling power of this technique is illustrated through the reliability analysis of an 8 × 8 Omega network as a practical example of MINs. In the second approach, we use the topological properties of HINs to build an SPN generic modeling methodology to aid the reliability evaluation of different HIN configurations. Finally, we illustrate and discuss the numerical reliability results obtained from the proposed techniques. It has been shown that the suggested techniques give results quite close to those obtained by the exact method with an enormous saving in computation time and memory usage.
45|3|http://www.sciencedirect.com/science/journal/01641212/45/3|Editorial|
45|3||The efficacy of matching information systems development methodologies with application characteristics â an empirical study|An experimental study was conducted to determine whether a data-centered system development methodology produced system designs superior to those from a process-centered methodology when applied to a data-intensive system problem. Experimental subjects (N=30) were advanced undergraduate systems analysis students who were given standardization training in system development techniques and then randomly divided into two groups. One group was taught to use a data-centered methodology and the other a process-centered methodology. Both groups then “solved” a system design problem that was intentionally designed to be data-intensive. The resulting designs were systematically scored for quality using a panel of system development experts. Results showed that the system designs produced by the data-centered group were not significantly better (p < 0.05) than those produced by the process-centered group. The quality of the system designs by both of the methodology assisted groups were found to be significantly superior (p < 0.01) to those produced by the group using no methodology whatsoever. This suggests that the development of system development methodologies that are geared toward specific application development technologies (hypertext, object-oriented, or rapid prototyping, for example) may not have merit and that further investment in “strongly typed” development methodologies may not be productive.
45|3||An assessment of systems analysis and design courses|Are colleges and universities adequately preparing their information systems graduates to perform successfully as practicing systems analysts? In this paper the relative importance of systems analysis and design tasks and skills as taught in undergraduate systems analysis and design courses is compared to the importance placed on these tasks and skills by practicing systems analysts. Whereas some concurrence is found, the number and nature of the differences may prompt educators to consider adjustments to the focus of their courses.
45|3||A cognitive information processing and information theory approach to diagram clarity: A synthesis and experimental investigation|Diagrams are unquestionably powerful tools. A multitude of studies investigate the relative usefulness of different types of diagrams (e.g. Larkin and Simon, 1987; Vessey and Galletta, 1991; Vessey and Conger, 1994; Jones and Schkade, 1995). However, analysis and research directed at empirically identifying and synthesizing basic components of diagram clarity, are rare. This study integrates isolated findings related to diagram clarity and experimentally investigates their application. The research utilized comes primarily from the Cognitive Information Processing (CIP) and Information Theory (IT) streams. Accordingly, the approach here is termed CIPIT. The set of CIPIT rules presented here represent a foundational framework for basic diagram clarity. The utility of the CIPIT rules are investigated experimentally via applying the CIPIT rules to one type of diagramming technique, the Data Flow Diagram (DFD). The experiment is carried out to investigate the clarity of a CIPIT-based DFD relative to a non-CIPIT DFD. Diagram clarity is measured objectively in terms of both speed of comprehension and the amount of comprehension. A questionnaire using likert-scale items is used to capture users' attitudes about the two diagrams. The results indicate a strong significant effect (increased clarity) for the CIPIT-based DFD.
45|3||Joint application design (JAD) in practice|Joint application design (JAD) is an intuitively appealing systems development method that encompasses approaches for enhancing user participation, expediting development, and improving the quality of specifications. Although the JAD method has been widely acclaimed, little is actually known about its effectiveness in practice. This paper reports findings of a field study at three organizations in which JAD practices were examined to assess how practice influenced outcomes of JAD use. Findings suggest that the organizations realized modest improvements in systems development outcomes by using the JAD method. JAD use was most effective in small, clearly focused projects and less effective in large, complex projects. Findings also suggest that practices for using JAD at the research sites reduced its potential effectiveness by limiting user participation, focusing workshop exercises on developers' analytical models and terminology, and applying the JAD method prematurely in large projects. Adaptations of the JAD method in practice stemmed from unresolved conflicts between assumptions underlying the espoused JAD method and the status quo IS development approach at each organization. Implications for the effectiveness of the JAD method as a software process improvement are considered.
45|3||Expressing casual relationships in conceptual database schemas|Conceptual schema design is a crucial phase in the database design process. The quality of the final database (regardless of logical implementation model) is dependent largely upon the quality of the conceptual schema. Since conceptual schemas serve as formal representations of the requirements specification for a database, it is critical that a schema capture the requirements as completely and unambiguously as possible. Many studies have shown that semantic models, such as the Extended Entity–Relationship model, are better for conceptual database design than traditional models such as relational, hierarchical, and network models. This is primarily because of their ability to capture explicitly many “natural” cognitive relationship types that are likely to occur in requirements specifications, e.g., association, generalization/specialization, and aggregation. However, the relationships that can be specified in a semantic model represent only a subset of the relationships that are likely to be used by people in describing an application environment. Thus, using current semantic models for conceptual database design may result in abstractions of application environments in which some important information from the requirements is either not represented or is represented inappropriately.This paper seeks to help bridge the gap between requirements specifications and data modeling by hypothesizing the need for supporting additional cognitive relationship types in conceptual models. In the paper, we demonstrate the need for one such relationship type, causation. Specifically, we investigate the effects of the lack of constructs in semantic models for capturing causation on analysts' ability to express causal relationships mentioned in a requirements document.We found that subjects not familiar with data modeling expressed causal relationships better in their representations than did subjects who had some prior exposure to data modeling. This seems to indicate that the lack of constructs for capturing causation in semantic models hinders the ability of people trained in data modeling techniques to recognize and express causal relationships in conceptual schemas. The results also suggest the need to develop semantic models that provide constructs for capturing causation and other cognitive relationships.
45|3||Usability inspection in contract-based systems development â A contextual assessment|
46|1|http://www.sciencedirect.com/science/journal/01641212/46/1|Examining the effects of the âApplication Revolutionâ1|
46|1||A concept of designing cheater identification methods for secret sharing|A new concept of designing cheater identification methods for secret sharing is proposed in this paper. System constructors can apply digital signature algorithm under this proposed concept to construct a practical cheater identification method which is both efficient and simple. It is convenient for a system which already contains a digital signature algorithm and needs to detect or identify cheaters. The security of the constructed method is dependent on the selected digital signature algorithm. The bit length of the public value of the cheater identification method which follows the proposed concept is independent on the total number of the participants in the secret sharing scheme. System constructors do not need to change the public value when he wants to include new participants into the system or to remove certain participants from the system. There exist efficient and secure commercial products for digital signature algorithm such as RSA. The proposed concept can be implemented in computer systems.
46|1||A phased reuse adoption model|Reuse is easy to understand but challenging to institute. Instituting reuse can be made easier by a reuse adoption model. The Software Productivity Consortium's reuse adoption model has been used widely and successfully in the five years of its existence. However, time and experience have suggested ways to improve the model. This paper describes the Consortium's new model. Whereas the old model was process-based and independent of a particular reuse methodology, the new model is phased and integrates the Consortium's Synthesis methodology. This paper discusses the old model and how it motivated the phased model. It then presents the phased model and describes experiences with it.
46|1||Technical controlling and software process improvement|Successful process improvement depends heavily on a close tracking of actual results versus the improvement plan. For concerting different reengineering activities, process managers must implement a proactive, forward-looking oversight mechanism designed to ensure that the various ongoing projects within the company operate in a performance zone that provides the expected business value. Technical controlling of software projects is introduced as a comprehensive controlling activity consisting of analyzing and interpreting project information for strategy formulation, planning and tracking activities, decision-making, and cost accounting. For practical guidelines this article focuses on introducing and maintaining a corporate program for technical controlling in a highly distributed large organization. Experiences are shared that show how technical controlling closely relates and thus supports an ongoing software process improvement initiative.
46|1||R-nets for the performance evaluation of hard real-time systems|Time Petri net (TPN) is a powerful technique for studying the specification, verification, and temporal behavior of real-time distributed systems. For evaluating the performance of these systems, in this paper, we extend the TPNs by distributing probability density functions (pdfs) with uniform distributions over the time intervals associated with the transitions of the net. These pdfs are used to determine the state transition probabilities of branching from the current state to the possible next states. Thus, the reachability graph obtained from the extended TPN can be interpreted as a Markov process. We call this extended TPN a Real-net or R-net. To improve the modeling capability of R-net, we allow its structure to contain multiple tokens, multiple arcs, inhibitor arcs, and two types of transitions: real-time transitions and zero-time transitions. The R-net methodology is illustrated through a simple example. In view of the important applications for which R-net is used, a performance model for a fault-tolerant real-time multiprocessor (FRMP) system is developed and analyzed. The FRMP system is used as the central computer in air traffic control applications.
46|1||Optimal task allocation in distributed systems by graph matching and state space search|
46|1||A displacement addressing method for letter-oriented keys|In this paper, a new method for minimal perfect hashing is proposed. Our scheme can process a rather large set of keywords and does not care the problem of letter-selection on each keyword. Our scheme employs the displacement of letters on each keyword to set up a minimal perfect hashing function. According to our experiments, up to one thousand keywords could be handled easily. Moreover, our scheme could be employed to handle more large key set if we subdivide such a key set into several proper-size subsets and apply our hashing function to them, respectively.
46|2-3|http://www.sciencedirect.com/science/journal/01641212/46/2-3|Editorial|
46|2-3||Software process simulation modeling: Why? What? How?|Software process simulation modeling is increasingly being used to address a variety of issues from the strategic management of software development, to supporting process improvements, to software project management training. The scope of software process simulation applications ranges from narrow focused portions of the life cycle to longer term product evolutionary models with broad organizational impacts. This article provides an overview of work being conducted in this field. It identifies the questions and issues that simulation can be used to address (`why'), the scope and variables that can be usefully simulated (`what'), and the modeling approaches and techniques that can be most productively employed (`how'). It includes a summary of the papers in this special issue of the Journal of Systems and Software, which were presented at the First International Silver Falls Workshop on Software Process Simulation Modeling (ProSim'98). It also provides a framework that helps characterize work in this field, and applies this new characterization scheme to many of the articles in this special issue. This paper concludes by offering some guidance in selecting a simulation modeling approach for practical application, and recommending some issues warranting additional research.
46|2-3||Simulation in support of CMM-based process improvement|This paper shows how simulation can be of significant benefit in supporting software process improvement and, in particular, how it supports the capability maturity model (Paulk et al., 1993. Capability maturity model for software, version 1.1. Software Engineering Institute Report CMU/SEI-93-TR-24). In the latter context it is shown that simulation can be used to support improvement at all five levels of the CMM. Simulation capability at each CMM level incrementally builds upon the simulation capabilities of the preceding levels, and match the needs of the software engineering practices at that level.
46|2-3||Quantitative modeling for the interactive simulation of software projects|The research project Software Engineering Simulation by Animated Models (SESAM) aims at providing a training environment for future project managers. The basic idea is to create a model of the software development process that can be interpreted by a simulator. A student using the simulator can control the simulated project interactively, managing it more or less successfully. In this paper, the simulation system and the model description language are presented. Then a new simulation model, the Quality Assurance (QA) model, is introduced. This model covers all activities of software development from requirements' specification to product delivery. The model especially emphasizes the effects of quality assurance activities. Finally, simulation results obtained by executing the QA model are presented and discussed.
46|2-3||The impact of feedback in the global software process|After briefly reviewing the difficulties of achieving major improvement in the process of global software development and maintenance, software evolution, the paper introduces the FEAST hypothesis. This states that such problems may, at least in part, be due to the feedback nature of that process and that this is, generally, overlooked in seeking such improvement. Results to date from the FEAST/1 project support the hypothesis and also the laws of software evolution as recognised and refined over some 25 years. The project is exploring the phenomenon in depth by identifying, modelling and simulating the evolutionary behaviour of a number of industrial software projects using both black box and system dynamics techniques. It is now demonstrating the presence and impact of feedback on process behaviour and improvement, and deriving guidelines for software process planning and management.
46|2-3||Integration of system dynamics modelling with descriptive process modelling and goal-oriented measurement|One of the obstacles that seem to impede a more frequent application of the modelling and simulation approach system dynamics (SD) in the software engineering community is the lack of well-defined and repeatable procedures for generating or using information that (a) stems from real industrial software development practice and (b) is suitable for SD model building. This problem can be resolved, at least partially, by combining SD modelling with already existing and commonly used static modelling methods, namely software process modelling and measurement-based quantitative modelling. To illustrate the feasibility of such a combination, in this paper, an approach is proposed that complements SD modelling with descriptive process modelling and goal-oriented measurement performed according to the principles of the well-established goal/question/metric (GQM) paradigm. The new approach is called integrated measurement, modelling and simulation (IMMoS). It originates in lessons learnt from an extensive industrial SD modelling activity.
46|2-3||Strategies for lifecycle concurrency and iteration â A system dynamics approach|Increasingly fierce commercial pressures necessitate the use of advanced software lifecycle techniques to meet growing demands on both product time-to-market and business performance. Two significant methods of achieving such improved cycle-time capability are concurrent software engineering and staged-delivery. Concurrent software engineering exploits the potential for simultaneous performance of development activities between projects, product deliveries, development phases, and individual tasks. Staged-delivery enables lifecycle iteration to supply defined chunks of product functionality at pre-planned intervals. Used effectively, these techniques provide a powerful route to reduced cycle-times, increased product quality and, potentially, lower development costs. However, the degree and manner in which these techniques should be applied remains an area for active research.This paper identifies some of the issues and open problems of incremental lifecycle management by reference to the development of aeroengine control systems within Rolls-Royce plc. We explain why system dynamics is a promising technique for evaluating strategies for lifecycle concurrency and iteration.
46|2-3||Software process simulation to achieve higher CMM levels|Organizations interested in improving their process performance face two key questions:
46|2-3||Software process simulation for reliability management|This paper describes the use of a process simulator to support software project planning and management. The modeling approach here focuses on software reliability, but is just as applicable to other software quality factors, as well as to cost and schedule factors. The process simulator was developed as a part of a decision support system for assisting project managers in planning or tailoring the software development process, in a quality driven manner. The original simulator was developed using the system dynamics approach. As the model evolved by applying it to a real software development project, a need arose to incorporate the concepts of discrete event modeling. The system dynamics model and discrete event models each have unique characteristics that make them more applicable in specific situations. The continuous model can be used for project planning and for predicting the effect of management and reliability engineering decisions. It can also be used as a training tool for project managers. The discrete event implementation is more detailed and therefore more applicable to project tracking and control. In this paper the structure of the system dynamics model is presented. The use of the discrete event model to construct a software reliability prediction model for an army project, the Crusader, is described in detail.
46|2-3||Experience with software process simulation and modeling|In this paper, I describe an approach and experiences in developing and applying simulation and modeling technologies to software processes. Processes for both software development and use have been investigated. The focus of this paper is organized around three topics for software process simulation and modeling. First, I describe an approach and examples of software simulation and modeling as investigated with knowledge-based process engineering environment developed at USC. Second, I describe how by focusing on process modeling, analysis and simulation, we are led to expand the scope of work with software processes toward a more comprehensive software process life cycle engineering. Third, I describe some of the lessons learned from applying modeling and simulation concepts, techniques and tools to software processes in a variety of organizational settings. Conclusions then stress the complementary value arising form the use of both qualitative and quantitative technologies for software process simulation and modeling.
46|2-3||Software process white box modelling for FEAST/1|This paper describes a high-level system dynamics model of a real-world software evolution process. This process is implementing embedded software elements of a defence system composed of hardware and software components. The model is one of the outputs of the FEAST/1 project, which is investigating the role and effect of feedback in the global software process. The simple feedback-based model, which has resulted from a top to down modelling approach, demonstrates the influence of the global process on the evolution of the software specification and implementation. Model outputs closely simulate actual and expected metrics for the real-world project. It is concluded inter alia that feedback external to a software production process may significantly influence that process.
46|2-3||Modeling the FedEx IT division: a system dynamics approach to strategic IT planning|This paper describes the development of a macro-scale model that predicts staffing, training and infrastructure funding over a five year period for the FedEx Information Technology Division. A system dynamics model was built using regressions on business, system and productivity metrics coupled with business projections. The model generated expected demands for development, support and infrastructure rollout. IT management was provided with a dashboard of strategic factors that could be adjusted to determine the impact on workloads and productivity of the IT organization. Model outputs were judged acceptable by management for order-of-magnitude budget and planning estimates. More important, the exercise galvanized IT management's commitment to process improvements and architecture and focussed attention on human resource strategies.
47|1|http://www.sciencedirect.com/science/journal/01641212/47/1|Of counting schemes and end-of-century partying1|
47|1||Information technology push/pull reactions|Innovation has not only become the domain of a few progressive enterprises but the key to success of many others. Innovative changes in management practices can assist in ensuring survival in an increasingly competitive world.
47|1||A multi-agent tuple-space based problem solving framework|Many real-life problems are inherently distributed. The applications may be spatially distributed, such as interpreting and integrating data from spatially distributed sources. It is also possible to have the applications being functionally distributed, such as bringing together a number of specialized medical-diagnosis systems on a particularly difficult case. In addition, the applications might be temporally distributed, as in a factory where the production line consists of several work areas, each having an expert system responsible for scheduling orders. Research in cooperating intelligent systems has led to the development of many computational models ( , , , , ,  and  Sycara, 1989) for coordinating several intelligent systems for solving complex problems involving diverse knowledge and activity. In this paper, a system for coordinating problem solving activities of multiple intelligent agents using the tuple space model of computation is described. The tuple space based problem solving framework is implemented on an Intel Hypercube iPSC/2 allowing multiple rule-based systems performing their dedicated tasks in parallel. The tasks are interrelated, in other words, there exists a partial order among the tasks which have to be performed.
47|1||A header-population based flow control for multicomputer networks|Wormhole routing is a very popular cut-through switching technique used in multicomputers due to its small network latency and simplicity in hardware. Although wormhole routing offers high throughput in an uncongested network, its throughput sharply drops whenever there is a congestion or blocking. In this paper, we present a new flow control technique, called header population based flow control (HP), for direct networks employing wormhole switching. When the traffic of a node increases, HP takes two actions: (1) it prevents the local processor of the node from injecting a new packet if the number of header flits in the router reaches a given injection threshold, and (2) among those header flits requesting the same output channel of the router, it selects the header flit whose buffer is filled with more flits if the number of header flits in the router reaches a given selection threshold. We have conducted extensive simulations to compare the network performance with and without HP in the networks using wormhole switching. Our simulation results show that HP improves network throughput significantly at high network traffic without sacrificing network latency. This paper also introduces an analytical model and derives a closed form expression for network latency when HP is used along with wormhole switching in direct networks. This model is validated through simulations and is found to agree closely with simulations.
47|1||Spare capacity assignment in telecom networks using path restoration and further improvement using traffic splitting1|Link restoration and path restoration are the two major techniques used for telecom network restoration in DCS (Digital Cross-connect System) mesh networks. Upon any link failure, for rerouting to be possible, sufficient spare capacity must exist in the links of the chosen alternate paths. Path restoration results in lower additional cost compared to link restoration. Path restoration planning corresponds to the multi-commodity flow problem, which is computationally NP-hard. In this paper, we present the details of an approximation scheme for the path based restoration planning problem and compare the performance of link and path restoration schemes. These schemes have been incorporated into the network planning tool developed at Alcatel Network Systems.
47|1||Partition search filter and its performance analysis|A search filter is used to filter out the items that do not exist in the search space. For a search filter, the false drop probability and the average testing time are two important factors estimated. Bloom filter and Random filter are two kinds of well-known search filters proposed by Bloom and by Wang et al., respectively. Chang and Leu had proved that Random filter does not guarantee to be superior to Bloom filter. In other words, both Random and Bloom filters have their own fittest performance conditions. This paper proposes a new search filter mechanism, Partition filter, which improves the above two methods with respect to the two criteria, the false drop probability and the average testing time, by taking the advantages of Bloom and Random filters.
47|1||Using individual prefixes in B+-trees|In 1985 the authors developed an original tree structure, based on B+-trees, in which each key keeps track of the prefix it shares with its immediate preceding key using only one byte. This is considered to be a substantial space and search-time saving approach. These kind of trees have been successfully implemented and have been used in applications such as information retrieval, general data bases, information systems, library administration systems and statistical analysis. This paper presents the structure of these trees, explains how the prefix is used to save space and time and gives a comparative performance study with respect to similar trees and approaches.
47|1||Synthesis of communications protocol converters using the timed Petri net model|The proliferation of heterogeneous, distributed computer networks has led to an urgent need for constructing reliable and efficient communication protocol converter, to facilitate the internetworking between such networks. Most existing converter design methods are based on the communicating finite state machine (CFSM) model as the formal description technique to describe the protocols, their services and the converter design. Two drawbacks of CFSM model are the state explosion problem and the inability of the model to express concurrent behaviors of protocols and services. These drawbacks may be overcome by using Petri nets as the formal description technique. Moreover, a reliable converter must perform its conversion functions in a timely manner satisfying the timing constraints of both protocol architectures. Our paper adopts the Timed Petri net (TPN) model to fulfill this requirement. The paper also highlights the dynamicity of the derived converter. We illustrate the converter design method using an example showing the dynamicity and timeliness features of the converter.
47|2-3|http://www.sciencedirect.com/science/journal/01641212/47/2-3|Editorial|
47|2-3||Editorial|
47|2-3||Intelligent mobile agents in large distributed autonomous cooperative systems|A large distributed autonomous cooperative system is a system that provides fundamental services for integrating high-level services in a large distributed system with local autonomy for individual system platforms and processes. This paper discusses the role and the functions of Intelligent Mobile Agents (IMAs) in large distributed autonomous cooperative systems. Rather than providing services to a user at the application level, IMAs considered in this paper are deemed an integral part of system level software and perform tasks that are considered central to the distributed system. A variety of solutions to problems that are inherent to the distributed nature of the computing infrastructure may be implemented through a system of IMAs. These problems include, but are by no means limited to, load balancing, scheduling, information retrieval and management, distributed decision support, routing and flow control, security and intrusion detection. In this paper we discuss some of characteristics of IMA based systems central to solving some of these problems. The effectiveness of IMAs in large distributed systems clearly depends on the design and implementation of the underlying IMA architecture. This paper discusses the features that must be provided by the IMA infrastructure in support of IMAs that are an integral part of the large distributed autonomous cooperative system.
47|2-3||Towards more industrially relevant academic researches into testing of communicating systems1|Academic researches have made significant advances in the generation of test sequences from formal specifications and in the development of computer-aided test tools with the aims of improving the effectiveness of testing communicating systems. However, this state-of-the-art research is not necessarily the state-of-the-practice; these methods and tools are seldom used in the communications industry. The main reason is that they do not quite address the problems facing testers in the industry; and testers generally regard academic testing techniques impractical and irrelevant to solving real problems. There is a big gap between testing practice and research results published in journals and reported at conferences. This paper presents an argument that academic researches into testing of communicating systems need to become more industrially relevant, describes some means by which this change can be facilitated and suggests some research topics that are relevant to the industry. It aims to help effect of change in the direction of academic researches into testing of communicating systems.
47|2-3||Evaluation techniques for improving the quality of very large software systems in a cost-effective way|Evaluation techniques for creating software systems that are both very large and very high quality are discussed. Central to this approach are careful architecture assessment, the automation of test case generation to make it efficient and tester-independent, and ways of minimizing the cost of regression testing to encourage that it be done thoroughly and systematically.
47|2-3||Consistency management in a process environment|Software inconsistency is primarily caused by changes. Changing a software product may cause other products to change. Moreover, changing a part of product (sub-product) usually causes other parts to change too. This paper covers software consistency management supports of advanced process environment research (Aper) by (1) decomposing software products into sub-products and establishing relationships among products and sub-products, and (2) defining trigger processes and consistency conditions in relationships. When a (sub-)product is changed, relationships can be traced to identify the affected ones. Trigger processes then dictate developers to handle the affected ones, which normally need to change accordingly. Meanwhile, consistency conditions should be kept among (sub-)products. Violation of the conditions will result in exceptions, which require handling by developers.
47|2-3||Understanding and improving technology transfer in software engineering|Technology transfer in software engineering involves more than a new idea or evidence that it works. This paper illustrates how technology transfer requires a good idea, the generation of evidence, analysis of that evidence, good packaging and support, and careful consideration of the audience for the technology. By learning from other disciplines, we present a model of technology transfer that can be tailored to a particular organisation's needs.
47|2-3||The distributed object computing paradigm: concepts and applications|The computing world is shifting toward a new paradigm, the Distributed Object Computing (DOC) paradigm. This paradigm exploits the benefits of object-oriented technology and uses the Internet and its communications infrastructure as a vehicle for the delivery of a wide range of sophisticated value-added distributed services. Applications of this paradigm are characterised as being open, client/server and multi-tiered applications using collaborating distributed objects. The integration of the Common Object Request Broker Architecture (CORBA) and object-oriented programming languages such as Java will facilitate the introduction and deployment of such applications in a platform-independent distributed environment. In this paper, we introduce both CORBA and Java as tools to facilitate the development of DOC-based applications, and we present examples from two areas of applications, namely, telecom and electronic commerce (EC) applications.
47|2-3||Rethinking the modes of software engineering research|Over the years, Software Engineering has provided software developers with a number of significant innovations that have changed the way we conceive and develop software. This tremendous growth has made it possible the development and operation of extremely complex applications such as air traffic control systems, worldwide airline reservation services, avionics and company-wide information systems. Despite this indisputable success, there are important issues and difficulties that we as researchers and practitioners should take into account to further promote the development of the Software Engineering discipline. They are related to the approach and the attitude we adopt in carrying out our research work. This paper briefly discusses some issues particularly critical to address the limitations and difficulties we are facing. The paper does not have the ambition of being complete and comprehensive. Nor does it propose precise solutions, which indeed require the committed and collective effort of the entire Software Engineering community. It just aims at proposing some arguments and observations for stimulating the discussion and reflection on the future of Software Engineering research.
47|2-3||Towards an evaluative framework for software process improvement models|Today the modus operandi for software development is heavily process-oriented. This is based on the premise that there has to be a quality process in order to produce quality software. There are number of quality models for software development called Software Process Improvement (SPI) models, which address this important issue. As these models differ in their characteristics it is important that there be a basis to evaluate them effectively. Such an evaluation is important from the perspective of understanding the particular model in terms of its structure, its benefits, etc. For the evaluation to be comprehensive and systematic, it is important to have a framework (or benchmark). This article defines such a framework which includes the most important criteria for the evaluation of the SPI models.
47|2-3||Software metrics: successes, failures and new directions|The history of software metrics is almost as old as the history of software engineering. Yet, the extensive research and literature on the subject has had little impact on industrial practice. This is worrying given that the major rationale for using metrics is to improve the software engineering decision making process from a managerial and technical perspective. Industrial metrics activity is invariably based around metrics that have been around for nearly 30 years (notably Lines of Code or similar size counts, and defects counts). While such metrics can be considered as massively successful given their popularity, their limitations are well known, and mis-applications are still common. The major problem is in using such metrics in isolation. We argue that it is possible to provide genuinely improved management decision support systems based on such simplistic metrics, but only by adopting a less isolationist approach. Specifically, we feel it is important to explicitly model: (a) cause and effect relationships and (b) uncertainty and combination of evidence. Our approach uses Bayesian Belief nets, which are increasingly seen as the best means of handling decision-making under uncertainty. The approach is already having an impact in Europe.
47|2-3||Integrated design patterns for database applications|Design patterns describe solutions for design problems that occur repeatedly in the development of software systems. Software system builders increasingly recognize the importance of exploiting design knowledge and successful experiences in the engineering of new systems. As a result, design patterns have started to gain popularity in capturing software design knowledge and software designs themselves. Currently, design patterns have addressed some of the object-oriented design limitations such as the difficulty in specifying reusable designs. Database applications have both structural and dynamic properties. However, design patterns for structural and dynamic properties are usually discussed separately without much integration. In this paper, we propose design patterns to provide solutions for some important common requirements in database applications. Most of these patterns address the structural and dynamic properties of database applications on an integrated basis. These patterns can be used in the design process as well as the design verification process.
47|2-3||Testing evolving software1|Regression testing, which attempts to validate modified software and ensure that no new errors are introduced into previously tested code, is used extensively during maintenance of evolving software. Despite efforts to reduce its cost, regression testing remains one of the most expensive activities performed during a software system's lifetime. Because regression testing is important and expensive, many researchers have focused on ways to make it more efficient and effective. Research on regression testing spans a wide variety of topics, including test environments and automation, capture-playback mechanisms, regression-test selection, coverage identification, test suite maintenance, regression testability, and regression-testing process. This paper discusses the state of the art in several important aspects of regression testing, and presents some promising areas for future research.
47|2-3||Repetitive failure, feedback and the lost art of diagnosis|This paper highlights a growing problem as software systems become more and more tightly coupled and complex, that of poor diagnostic capability. Diagnosis has never been particularly sophisticated in software systems, often being an ad hoc process in which programmers receive no training. As a result, significant numbers of failures in modern systems cannot be diagnosed in the sense of being uniquely related to one or more faults and as such they continue to fail. As a result, modern software systems are unique amongst modern engineering systems is being characterised by repetitive and frequently avoidable failure.
47|2-3||Measurement and continuous improvement of software reliability throughout software life-cycle1|Software reliability is one of the most important aspects of software quality. However, most of the existing techniques focus on the system testing and other late stages of development life-cycle. Therefore, they are not effective for early problem identification and reliability improvement. This paper describes our recent work in establishing predictive linkage between software reliability and other entities that we can measure and control early in the development cycle, and using such predictive relations to drive continuous reliability improvement.
48|1|http://www.sciencedirect.com/science/journal/01641212/48/1|Editorial|
48|1||Object-oriented methods: current practices and attitudes|A total of 160 experienced object-oriented (OO) developers and OO trainees responded to a survey covering current practices and attitudes toward various OO methods, in particular and some aspects of OO systems development, in general. The survey probed into developers' training in OO analysis and design (OOAD) methods; familiarity, preference and use of OOAD methods; the use of OO tools; and attitudes toward OOAD methods. The purpose of this paper is to report the findings in these areas. Overall, (1) trainees are relying more on universities and less on self-instruction compared to existing developers; (2) the Booch, OMT and Jacobson methods are known by, preferred by, and used by developers more than other methods; (3) automated tools are used regularly by less than 50% of developers; and (4) attitudes toward OOAD methods by developers and trainees are favorable, although some differences do exist. The results are important for understanding the current state of, and attitudes toward, OOAD methods as interest in this area continues to grow and mature.
48|1||Multi-method research: An empirical investigation of object-oriented technology|There is a general acceptance that software engineering research should be supported by empirical evaluation. To make real progress researchers must address the difficulties caused by the human-intensive nature of software development as well as experimental validity. This paper proposes the use of multi-method empirical research programs, as an alternative to `single-shot' empirical studies, to help address these problems.
48|1||Formal methods in object oriented business modelling|Most of BPR methodologies lack the formal underpining to ensure the logical consistency of their business models. Agent Relationship Morphism Analysis (ARMA) is a BPR methodology in which modelling of the business environment is achieved with the use of three perspectives: the structural, behavioural and process. A technique called Agent Relationship Modeling (ARM) has been developed for modelling the structural perspective. The more dynamic organizational concepts are described in the behavioural and process perspectives. These perspectives are modeled in a technique called Agent/Object Lifecycles (ALCs/OLCs). In this paper we present the application of ARMA in a petrochemical industry. The conceptual part of ARMA allowed us to articulate our ideas by creating an informal model of the situation. Then a formal model was developed that enforced rigour to the modelling exercise introducing the concept of business rules in agent and object classes. Finally, we present the verification of the ARM and ALC/OLC models in order to secure that the resulting formal models are logically consistent.
48|1||Class diversity support in object-oriented languages|We start with a general approach to introducing software fault tolerance (SFT) into object-oriented (OO) systems [Xu, J., Randell, B., Rubira, C.M.F., Stroud, R.J., 1995. Toward an object-oriented approach to software fault-tolerance. In: Avreski, D. (Ed.) Fault-Tolerant Parallel and Distributed Systems. DEEE CS Press, Silver Spring, MD.] and proceed in two directions. The first one is the use of SFT schemes within standard OO languages. New questions which arise when we are dealing with these languages are addressed. Our intention is to thoroughly analyse all engineering steps which allow diversity to be introduced in systems programmed in these languages. Some new general problems are spotted and discussed as well. The second direction is dealing with version concurrency and distributedness in a general way. We investigate providing SFT by class diversity, which is the most general way of designing diverse software in OO systems. We concentrate on N-version programming (NVP) and give an exhaustive discussion of this approach. One of the main reasons for this choice is that we have come to believe that the general approach which allows a unified discussion of all SFT schemes is rather restrictive because it does not properly address the differences between these schemes which represent their essences and the most difficult parts of their implementation and support. Our intention is to discuss the use of NVP in OO terms and to outline all novelties arising from this. The re-usability of SFT features is a key point in our approach. One of the conclusions we have arrived at is that, generally speaking, the entire states of version objects should be compared to detect and mask the faulty one. We propose unifying in one component features dealing with adjudication and faulty object recovery because these functionalities have a lot in common. Our approach is demonstrated using Ada 95.
48|1||Two practical systems for classification of three-dimensional objects|The main goals of practical vision systems are the fast and accurate recovery of the three-dimensional description of an object that produced a two-dimensional image. In this paper, we propose two practical systems (called VISION1 and VISION2) for the classification of three-dimensional objects using a set of their two-dimensional images that adhere to these goals. VISION1 is an on-line recognition system utilizing a colored TV camera and the necessary interface equipment to supply a two-dimensional image to a digital computer for processing and classification. It essentially relies on shape-from-X techniques to compute the depth. In addition it exploits automatic rotation of the object to generate a training sample set, and extraction of 12 two-dimensional invariant features to train an artificial neural network (ANN) with. Then VISION1 recalls its trained ANN for classification of objects. The proposed VISION1 is simple, fast, uses less man-power and provides storage space at lower cost than comparable vision systems (Sahibsingh, A.D., Kenneth, J.B., Robert, B.M., Aircraft identification by moment invariants. IEEE Transactions on computers, vol. c-26 (1), pp. 39–45). On the other hand, VISION2 system follows the classical approach of vision theory developed by Marr, D., 1982 (Vision – a Computational Investigation into the Human Presentation and Processing of Visual Information. Freeman, San Francisco, CA). VISION2 depends upon image segmentation algorithms, shape-from-X techniques, and the extraction of 11 three-dimensional invariant features to train an ANN with. Thus VISION2 is very efficient even more accurate than VISION1. Both VISION1 and VISION2 may be used in different manufacturing environments for classification of objects. Experimental testing was conducted through a sample test of 112 real images for four different classes of objects. The reported results support our claim that the two proposed systems are practical, fast and accurate.
48|1||Erratum|
48|1||Erratum|
48|2|http://www.sciencedirect.com/science/journal/01641212/48/2|Editorial|
48|2||Test set size minimization and fault detection effectiveness: A case study in a space application|An important question in software testing is whether it is reasonable to apply coverage-based criteria as a filter to reduce the size of a test set. An empirical study was conducted using a test set minimization technique to explore the effect of reducing the size of a test set, while keeping block coverage constant, on the fault detection strength of the resulting minimized test set. Two types of test sets were examined. For those with respect to a fixed size, no test case screening was conducted during the generation, whereas for those with respect to a fixed coverage, each subsequent test case had to improve the overall coverage in order to be included. The study reveals that regardless of how a test set is generated, with or without any test case screening, block minimized test sets have a size/effectiveness advantage, in terms of a significant reduction in test set size and with almost the same fault detection effectiveness, over the original non-minimized test sets.
48|2||Integration in software intensive systems|This article is concerned with systems integration and its impact on software intensive projects. It contains a review and a taxonomy of integration concepts as applied to software intensive systems. We identify pertinent technical integration issues and review and classify integration models, strategies, mechanisms and architectures. We argue that integration is part of the design activity and propose existing best integration practice for project management and engineering of large software intensive systems.
48|2||Factors affecting the design of load balancing algorithms in distributed systems|In a distributed environment made up of several computing units, it is necessary to exploit the system's greater calculation resources more efficiently by distributing the workload over the various machines in the system. The aim of distributed allocation can therefore be to balance the load generated in the single machines so as to improve the response time and throughput of the system as a whole. This paper deals with the problem of allocating processes with the aim of achieving load balancing in a heterogeneous distributed system. Some strategies are proposed based on adaptive heuristic approaches which can be followed in real systems. The aim is to point out which factors mainly influence a good workload distribution and which parameters can be tuned to achieve the best allocation. In order to measure the host's workload here the concept of relaxation is introduced, and some algorithms which take this parameter into account are discussed. In addition, the contribution of the knowledge of the process nature to the allocation strategy has been investigated too. The solutions proposed have all been implemented and evaluated on a real heterogeneous distributed system in order to assess how each parameter, in combination with the others, contributes to determining the best allocation decision.
48|2||An automatic implementation of the ISO FTAM protocol based on an integrated specification of Estelle and ASN.1|There is a huge demand for methods that enable communication protocols to be speedily implemented. Application layer Protocol Data Units (PDUs) are specified in Abstract Syntax Notation One (ASN.1). Any application protocol specification is not complete until it includes the PDU type definitions and operations for manipulating, comparing and assigning ASN.1 values. Given that internationally standardized Formal Description Techniques (FDTs) do not readily support ASN.1, an automatic implementation of an application protocol cannot be easily achieved; its development can thus be a long and error-prone task. This paper describes the details of an automatic implementation of the ISO FTAM protocol using an integrated Estelle and ASN.1 specification and the software system, EASE (Estelle and ASN.1 Software Environment) (Lai, R., Lo, A., 1996a. EASE: A software environment for automatic implementation of application protocol, Software Practice and Experience, John Wiley and Sons, 26 (1), 83–103) which supports such a specification, demonstrating how this long and error-prone process can be addressed; an evaluation of the work done is also included. It presents a success story of the use of FDTs, in particular Estelle, for the speedy implementation of an application protocol so that developers in the communications industry have more confidence in using FDTs for their communication software developments.
48|2||A formal approach for generating oo specifications from natural language|The requirements analysis process is essential to software development. The success or failure of a software system can be said to largely depend on the quality of this activity. A formal and disciplined process is therefore necessary for requirements analysis. In this paper, we present an approach that is based on the formal definition of relations between linguistic and OO conceptual structures as a basis for a formal and disciplined problem analysis process. This process is based on two components, conceptual model formalization and OO model construction. The first provides formal rules to identify the key components of conceptual models, and the second, provides a set of definite steps to guide the analyst in model construction. We also present some conclusions concerning the application of our approach versus the standard OMT approach by a group of students at our university.
48|2||User perception of expert system advice|Expert Systems represent a major investment to many organizations. To capitalize on the investment requires that users take advantage of the advice provided. This study examines novice decision makers in a constructed setting to see if they perceive an expert system to have value in the decision process and to determine if expert systems can be favorably viewed. Results indicate that the subjects find the expert system in the study to be structured, satisfactory and an enhancement to the decision process. But even though a positive impression is made by the system, the users did not feel a sense of accomplishment. Management can build expert systems expecting the decision to be well received, but should not expect a stimulating task environment without additional design effort.
48|3|http://www.sciencedirect.com/science/journal/01641212/48/3|Editorial|
48|3||Achieving non-repudiation of Web based transactions1|In this paper, we describe our approach to achieve non-repudiation for World Wide Web (WWW) based transactions. We designed and implemented protocols for preparing digital signatures on the server as well as the client machine. In our design, we use the popular Pretty Good Privacy (PGP) software for preparing and verifying digital signatures. The key-contribution is the deployment of a special purpose HTTP server, called signing server, on the client machine to communicate with the WWW browser. A signing server is specialized to handle digital signatures. This paper discusses the design and implementation of the signing server protocol to achieve non-repudiation transactions in a WWW based employee information system. The technique of deploying special purpose HTTP servers on the client machine has many applications beyond this. It inter-operates with all types of browsers and is an attractive alternative to browser dependent plug-ins.
48|3||Stateful relational database gateways for the World Wide Web|The World Wide Web is currently considered as the most appropriate software platform for the deployment of applications in wide area networks (telematics) as well as corporate intranets. Such applications are, in the majority of cases, tightly coupled with legacy databases hosted by relational management systems. However, the nature of the database enabled WWW systems is quite different from that of classical database applications developed with tools like 4GLs, Forms, Menus, etc. The basic difference lies in the stateless character of the WWW. The `user session' concept, encountered in conventional database applications, does not apply in the WWW environment. Instead, interaction with the information server is accomplished through a series of hits (request–response interactions) which are treated independently. This paper presents an architecture for the deployment of stateful database gateways for WWW servers. Although the server still treats each individual hit independently, state information maintained in the WWW browser as well as in specialized agents that operate behind the WWW server renders the WWW appropriate for a `session-aware' database application. The effort required to port an existing `session-aware' database application to the WWW environment is minimal.
48|3||Effective fault tolerance for agent-based cluster computing|Long-lived parallel applications running on workstation clusters are vulnerable to single-node or multiple-node failures. Fault recovery is therefore important in avoiding immature program termination. However, much of the runtime overhead imposed by fault tolerance schemes is generally due to the cost of transferring the checkpoint states of applications by disk I/O operations. In this paper, we propose a fault tolerant model in which disk I/O operations are not required because checkpoint states are transferred between replicated parallel applications. We also describe how the resource consumption of the replicated applications can be effective. To achieve this, applications are constructed as computation agents that can reconfigure dynamically according to resource availability. The fault tolerant model has been implemented and tested on a workstation cluster and a Fujitsu AP3000 multi-processor machine. The measurements of our experiments have showed that efficient fault tolerance can be achieved by replicating parallel applications on workstation clusters.
48|3||A faultâtolerant object service on CORBA|The Common Object Request Broker Architecture (CORBA), is a major industrial standard for distributed object-based applications. Today's large-scale CORBA applications have to deal with object crashes, node failures, networks partitioning and unpredictable communication delays. Existing efforts to enhance the CORBA reliability can be roughly categorized into three approaches: integration approach, interception approach and service approach. Each approach has its own merits and prices. In this paper, we propose a service approach solution called Object Fault–tolerance Service (OFS). Solutions that adopt the service approach usually specify their service in terms of CORBA IDL interfaces. The implementations of such solutions in general do not modify the ORB infrastructure or IDL language mappings, and thus applications developed with those systems appear to be more portable. OFS differs from other service approach solutions in that OFS does not assume underlying support of reliable group communication. Applications with advance registration can rely on OFS for detection of object and node crashes, and for customized recovery. In this paper, we first present the service specification of OFS. We then give the system architecture of an OFS implementation. This OFS implementation is developed on the Solaris 2.5 platform and with IONA's Orbix 2.0. The performance evaluation of the OFS implementation is also presented. The preliminary experiments indicate that OFS overhead is minimal and client objects experience little response delay when a service object is under OFS surveillance.
48|3||Cleaning policies in mobile computers using flash memory|Flash memory is small, lightweight, shock-resistant, nonvolatile, and consumes little power. Flash memory therefore shows promise for use in storage devices for consumer electronics, mobile computers and embedded systems. However, flash memory cannot be overwritten unless erased in advance. Erase operations are slow that usually decrease system performance, and consume power. The number of erase cycles is also limited. For power conservation, better system performance, and longer flash memory lifetime, system support for erasure management is necessary. In this paper, we use the non-update-in-place scheme to implement a flash memory server and propose a new cleaning policy to reduce the number of erase operations needed and to evenly wear out flash memory. The policy uses a fine-grained method to effectively cluster hot data and cold data in order to reduce cleaning overhead. A wear-leveling algorithm is also proposed. Performance evaluations show that erase operations are significantly reduced and flash memory is evenly worn. Though the proposed fine-grained separation scheme is targeted at flash memory-based systems, it can be applied to other storage systems as well.
48|3||A reserved bandwidth video smoothing algorithm for MPEG transmission|MPEG video transmission will make up a significant portion of the workload on future computer networks. These variable bit rate (VBR) compressed videos are known to exhibit significant, multiple-time-scale bit rate variability. To such bursty traffic, it is always a compromise between providing a good quality of service (QoS) and a high utilization on the reserved bandwidth. One can utilize an optimal video smoothing algorithm as suggested in Salehi et al. (1996) to smooth out the data transmission from a server to a client so that the bandwidth utilization can be improved. However, such smoothing techniques concentrate their efforts on smoothing the transmission and worry less about resource requirements, maximum bandwidth needed and the overall bandwidth utilization. In this paper, we present a smoothing algorithm for transmitting stored MPEG-I video stream under a user-defined maximum network bandwidth. We also consider the impact of the startup latency, and the client's buffer size on the network utilization. Our algorithm differs from the others in that we do video smoothing under a pre-defined reserved bandwidth. At the cost of deleting some B-frames within a short period of time, the reserved bandwidth utilization can be improved dramatically without sacrificing much on the video quality.
49|1|http://www.sciencedirect.com/science/journal/01641212/49/1|Editorial|
49|1||A conceptual basis for feature engineering|The gulf between the user and the developer perspectives lead to difficulties in producing successful software systems. Users are focused on the problem domain, where the system's features are the primary concern. Developers are focused on the solution domain, where the system's life-cycle artifacts are key. Presently, there is little understanding of how to narrow this gulf.
49|1||Measurement processes are software, too1|Software process improvement and measurement are closely linked: measures are the only way to prove improvements in a process. Despite this link, and the interest in process improvement, measurement is not widely applied in industrial software production. This paper describes a method designed to guide the definition, implementation and operation of measurement processes. The method, which builds upon Fenton's measurement framework and GQM, starts from the point that measuring a software process is in its turn a process in the software process. The three basic ideas of the method are derived from this assumption: the measurement process should reuse and suitably adapt the same phases of the software process: requirements definition, design, implementation, etc. A descriptive process model should be the essential starting point of a measurement process. Many concepts and tools which derive from the object oriented approach should be effectively used in the measurement process. An experimental application in an industrial process has shown that building the process model was the hardest part of the measurement process, and that it has improved the quality of measurement by reducing misunderstandings. Object oriented concepts and tools make it possible to automate certain tasks (for instance the definition of the schema of the measurement database) and to improve robustness against changes in the measurement process.
49|1||Cost estimation based on business models|Software development requires early and accurate cost estimation in order to enhance likely success. System complexity needs to be measured and then correlated with development effort. One of the best known approaches to such measurement-based estimation in the area of Information Systems is Function Point Analysis (FPA). Although it is reasonably well used in practice, FPA has been shown to be formally ambiguous and to have some serious practical deficiencies as well, mainly in the context of newly emerged object-oriented modeling approaches. This paper reports results from an empirical study undertaken in Swiss industry covering 36 projects. We observed that a new formally sound approach, the System Meter (SM) method, which explicitly takes reuse into account, predicts effort substantially better than FPA.
49|1||Software reliability prediction incorporating information from a similar project|Although there are many models for the prediction of software reliability using the failure data collected during testing, the estimation is usually inaccurate, especially at the early stages of the testing phase, and hence many practitioners are hesitant to use software reliability models. On the other hand, the traditional software reliability growth models do not make use of information from earlier or similar projects. For example, software systems today are usually an improvement or modification of an earlier version or at least within the same application domain, which implies that some information should be available from similar projects. In this paper we study some approaches for the estimation of software reliability by incorporating information from a similar project. In particular, we use the Goel–Okumoto model and assume the same value of the fault detection rate. The other parameter is then estimated based on the available testing data. For an actual set of data, our approach provides much more stable estimates and when the traditional maximum likelihood estimates exist and are reasonable, our results are very close to that from a statistical point of view. In addition, our approach does not require numerical algorithm to update the estimate and hence it is convenient to use.
49|1||Generating test data from SOFL specifications1|Software testing can only be formalized and quantified when a solid basis for test generation can be defined. Tests are commonly generated from the source code, control flow graphs, design representations, and specifications/requirements. Formal specifications represent a significant opportunity for testing because they precisely describe  functions the software is supposed to provide in a form that can be easily manipulated. This paper presents a new method for generating tests from formal specifications. This method is comprehensive in specification coverage, applies at several levels of abstraction, and can be highly automated. The paper applies the method to SOFL specifications, describes the technique, and demonstrates the application on a case study. A preliminary evaluation using a code-level coverage criterion (mutation testing), indicates that the method can result in very effective tests.
49|1||A performance model interchange format1|A performance model interchange format (PMIF) provides a mechanism whereby system model information may be transferred among performance modeling tools. The PMIF allows diverse tools to exchange information and requires only that the importing and exporting tools support the PMIF. This paper presents the definition of a PMIF by describing a meta-model of the information requirements and the transfer format derived from it. It describes how tool developers can implement the PMIF, how the model interchange via export and import works in practice, and how the PMIF can be extended. A simple case study illustrates the format. The paper concludes with the current status of the PMIF, lessons learned, some suggestions for extensions, and current work in progress.
49|1||An assessment of systems and software engineering scholars and institutions (1994â1998)|
49|1||Editorial|
49|1||Quality in use: Meeting user needs for quality|There is an increasing demand for software that matches real user needs in a working environment. The paper describes the new framework for software product quality developed for ISO/IEC 9126-1: internal quality (static properties of the code), external quality (behaviour of the software when it is executed) and quality in use (the extent to which the software meets the needs of the user). Quality in use is a broader view of the ergonomic concept of usability in ISO 9241-11 (1998). Achieving quality in use requires a user-centred design process which has cultural, strategic and technical implications.
49|1||An automatic approach of domain test data generation|It is usually observed in software testing that input data near the boundary of a domain partition are more sensitive to program faults and should be carefully checked. A formalization of this intuitive observation is domain testing. However, as an implementable testing technique, it has some difficulties. One of which is related to the test data generation. Since domain testing requires that the ON and OFF points be exactly on or near the boundary of a path domain, they can hardly be given manually. But how to automatically generate these test points efficiently is unclear. Traditionally there exist two different ideas. One is to use a static approach that computes the ON and OFF points by a series of algebraic operations. The other is to use a dynamic “search” method that determines an ON point in a “trial-and-error” manner. Each has its advantages and weaknesses. In this paper, we present a combined approach that takes the advantages of both ideas, i.e., it has both algebraic manipulations and a dynamic “search” process in a mixed way. It is shown that this approach is more efficient than others.
49|2-3|http://www.sciencedirect.com/science/journal/01641212/49/2-3|Software engineering education and training for the next millennium|
49|2-3||A real-world case study in information technology for undergraduate students|Real-world case studies are important to complement the academic skills and knowledge acquired by computer science students. In this paper we relate our experiences with a course specifically designed to address this issue. The problem to be addressed is the replacement of a Hospital Information System (HIS) in a large regional hospital. The case mimics as close as possible the project as it really took place.
49|2-3||Strategies for industrial relevance in software engineering education|This paper presents a collection of experiences related to success factors in graduate and postgraduate education. The experiences are mainly concerned with how to make the education relevant from an industrial viewpoint. This is emphasized as a key issue in software engineering education and research, as the main objective is to give the students a good basis for large-scale software development in an industrial environment. The presentation is divided into experiences at the graduate and postgraduate levels, respectively. For each level a number of strategies to achieve industrial relevance are presented. On the graduate level a course in large-scale software development is described to exemplify how industrial relevance can be achieved on the graduate level. The strategies on the postgraduate level have been successful, but it is concluded that more can be done regarding industrial collaboration in the planning and conduction of experiments and case studies. Another interesting strategy for the future is a special postgraduate program for people employed in industry.
49|2-3||Providing new graduate opportunities: experiences with a UK masterâs level computing conversion course|The forces that operate within the software industry are outlined and the case is made for postgraduate software engineering education that is flexible in terms of attendance patterns and responsive to the market in terms of curricula. The developments within a postgraduate “conversion” course at the University of Sunderland over a 9 year period with regard to both patterns of delivery/attendance and curricula are then described (including the origins of the course and the development of five different delivery modes). The profile of graduates at entry to the course is illustrated, and thumbnail sketches of several graduates from the course are drawn to reflect the diversity of their subsequent careers. The content of the original and revised versions of the course are outlined, and comparison is made of the approaches adopted towards the treatment of Software Engineering (SE) in the two versions. Finally, a critical appraisal is presented with regard to the changes introduced in the course.
49|2-3||Teaching software project management: a responseâinteraction approach|Southern Polytechnic State University has recently implemented a new Master of Science in Software Engineering degree, which includes a course in Software Project Management in its core requirements. This paper addresses an innovative approach to teaching this course through what is described as response–interaction. Also included are the results of the first offering of this course.
49|2-3||Integration of computer security into the software engineering and computer science programs|This paper presents a role for computer security education in a computer science curriculum and argues that it should become a standard course offering at both the undergraduate and graduate levels of instruction. Computer security instruction requires a fundamental computer science foundation and integrates nicely into the junior or senior year of study. Additionally, a typical computer security overview course tends to reinforce previously taught material – particularly in the areas of networks, operating systems, database, software engineering, computer hardware design/architecture, data communications, and artificial intelligence. It also introduces a wonderful forum from which ethical/societal issues can be discussed and debated.
49|2-3||Industry/university collaborations: different perspectives heighten mutual opportunities|In this paper, we present the results of a survey of formal industry/university collaborations. The purpose of these collaborations is to meet the software engineering education and training needs of adult learners through joint ventures such as graduate programs (degree and certificate) and professional development activities (customized classes, seminars, forums, and conferences). Members of the Software Engineering Institute (SEI) working group on software engineering education and training conducted the survey in 1997–1998. The working group drew on the extensive experience of industry and university collaboration participants to help answer practical questions about the benefits of collaboration, the collaboration process itself, successful collaboration administration and programming, and lessons learned. Survey results are being published as a service to the software engineering education and training community to assist organizations interested in forming a new collaboration or improving an existing collaboration.
49|2-3||Guidance for the development of software engineering education programs|In this paper, we discuss issues and ideas that can improve the undergraduate education of software engineers. We submit that a key impediment to the advancement of software engineering education is the lack of guidance and support for the development of new courses and curricula. We discuss the work and results of a project to create a set of Guidelines for Software Engineering Education. We outline the content of the Guidelines, describe how they relate to recent and current professional activities to improve the practice of software engineering, and discuss future plans for their development.
49|2-3||An information systems-centric curriculum, ISCC â99|This article describes a curriculum designed to prepare students to work with large systems, to take a systems view from the very beginning, and to develop collaborative and communications skills. The curriculum is a result of a mutually beneficial collaboration between university professors and representatives from business and industry. The curriculum builds an academically sound conceptual technical base for the student and focuses on the needs of the workplace. Student learning and faculty teaching paradigms were explored and were incorporated into the curriculum design. The resulting curriculum provides an educational experience that produces students prepared to meet the increasing needs of industry to create and use complex information systems and forms the basis for life-long learning.
49|2-3||Software developer perceptions about software project failure: a case study|Software development project failures have become commonplace. With almost daily frequency these failures are reported in newspapers, journal articles, or popular books. These failures are defined in terms of cost and schedule over-runs, project cancellations, and lost opportunities for the organizations that embark on the difficult journey of software development. Rarely do these accounts include perspectives from the software developers that worked on these projects. This case study provides an in-depth look at software development project failure through the eyes of the software developers. The researcher used structured interviews, project documentation reviews, and survey instruments to gather a rich description of a software development project failure. The results of the study identify a large gap between how a team of software developers defined project success and the popular definition of project success. This study also revealed that a team of software developers maintained a high-level of job satisfaction despite their failure to meet schedule and cost goals of the organization.
50|1|http://www.sciencedirect.com/science/journal/01641212/50/1|Y2K behind us: smooth sailing and blue skies?|
50|1||An encompassing life cycle centric survey of software inspection|This paper contributes an integrated survey of the work in the area of software inspection. It consists of two main sections. The first one introduces a detailed description of the core concepts and relationships that together define the field of software inspection. The second one elaborates a taxonomy that uses a generic development life-cycle to contextualize software inspection in detail.
50|1||Software management and cost estimating error|Software cost estimating is an important concern for software managers and other software professionals. The hypothesized model in this research suggests that an organization's use of an estimate influences its estimating practices which influence both the basis of the estimating process and the accuracy of the estimate. The model also suggests that the estimating basis directly influences the accuracy of the estimate. A study of business information systems managers and professionals at 112 different organizations refined the model.
50|1||An analysis of factors affecting software reliability|This paper presents the findings of empirical research from 13 companies participating in software development to identify the factors that may impact software reliability. Thirty-two potential factors involved in every stage of the software development process are defined. The study uses a survey instrument to analyze these factors and identify factors that have significant impact on software reliability. The survey focuses on the perspective of the primary participants, managers, system engineers, programmers, testers and other people involved in software research or development teams. Two techniques such as the relative weight method and analysis of variance technique (ANOVA) have been used to analyze all factors and rank them in terms of their impact on software reliability. The research findings have important implications for further research and the practice of software development. For researchers, it points to improvement schemes of existing reliability modeling and factors that may be further verified and extended in subsequent research. For practitioners, it provides a general guide to the important aspects to consider in the whole software development process.
50|1||Module interconnection features in object-oriented development tools|The black-box reuse of library classes in the construction of an object-oriented (OO) application is difficult: the principle of information hiding may be violated if classes must know their partners, and code must be typically rewritten. One of the possible ways to increase class decoupling and thus reuse is to employ interconnection languages to describe OO architectures. In this paper we present a decoupling paradigm for individual classes or class collections that facilitates reuse, introducing interconnection features at the design and programming levels. We give examples in a new, second generation OO development system, using asynchronous messages sent to generically identified receivers.
50|1||Reading between the lines: an examination of systems analysis and design texts|Systems analysis and design is a key component of most information systems academic programs. One of the primary sources of information provided in such courses is the textbook. The purpose of the study is three-fold: first, to determine on an individual text level, the amount of coverage of various tasks, activities and tools; second, to identify which texts capture the largest percentage of the topics being studied; and lastly, to compare the overall rankings of the tasks, activities and tools to those of educators and practitioners. This paper discusses the results of this content analysis, and their implications for IS educators.
50|1||On the concept of coupling, its modeling and measurement|Although measurement is of importance, the difficulty to develop sound measures should not be underestimated. If a quality we want to assess is ambiguous, it is unlikely to develop reliable measures. For the fundamental concept of coupling, we reveal that there is a big difference between the commonly perceived essence of coupling and its usual definition. We show also that the classification of five coupling levels ranging from data, stamp to content coupling is overly simplified and inconsistent.
50|1||Specifying and verifying real-time systems with timing uncertainty|Modeling timing behaviors of systems and verifying timing constraints against the model are major tasks in developing real-time systems. However, it is recognized to be extremely difficult to predict the timing behavior of systems precisely in the requirements phase. Timing uncertainty, if not considered properly in the modeling and verifying steps, may incur subtle, yet critical errors in final products. We propose a method of dealing with such timing uncertainty using an extended duration calculus formalism. The extended duration calculus, named fuzzy duration calculus, allows us to specify uncertain timing behavior of the system using the fuzzy theory. Semantics and proof system of the duration calculus are redefined in order to accommodate fuzzy concepts. Based on the semantics and proof system, we can verify timing constraints against the system model. A portion of nuclear power plant (NPP) control system is employed to demonstrate the feasibility and the effectiveness of our approach.
50|2|http://www.sciencedirect.com/science/journal/01641212/50/2|The CMMI: it's formidable|
50|2||ElGamal-like digital signature and multisignature schemes using self-certified public keys|In this paper, we first present a digital signature scheme using self-certified public key. Subsequently, we present a digital multisignature scheme extended from the proposed digital signature. Both the proposed schemes have the advantage that the authentication of the public key can be accomplished with the verification of the signature or multisignature. The security of both the proposed schemes is based on the one-way hash function, the factorization and the discrete logarithm assumptions. Moreover, the proposed multisignature scheme preserves the main merits inherent in most of the previously developed schemes. We also demonstrate that the proposed multisignature scheme can withstand the active attack that some malicious impostors try to universally forge an individual signature or a multisignature for a given message that is rejected to be signed by the other co-signers.
50|2||Robust transparent image watermarking system with spatial mechanisms|A very robust against JPEG compression, difficult for collusion attack, and transparent secret key digital watermarking system, which takes a spatial-domain approach, is presented in this paper. The unique watermark that consists of both the owner's logo and the user's licence number provides the ownership protection and traceability of illicit dissemination. The technique of block-oriented and modular-arithmetic–based watermark embedding and extraction achieves the robustness against image processing such as lossy compression, filtering, and cropping. The random perturbation of the pixels within the marked block as well as the unique and fused watermark information result in difference images with irregular-shape and uneven-luminance blobs, hence greatly increase the difficulty of collusion attack. Transparency is further fulfilled by means of embedding the information bits around but not on the exact edges of the features to avoid edge sharpening or smoothing, which effectively magnifies the contrast among the distributed images. The experimental results demonstrate that the proposed watermarking system is superior to Voyatazis and Pitas's scheme with respect to image processing, and is much more secure than the method of Cox et al.'s method.
50|2||Towards integration of use case modelling and usage-based testing|This paper focuses on usage modelling as a basis for both requirements engineering (RE) and testing, and investigates the possibility of integrating the two disciplines of use case modelling (UCM) and statistical usage testing (SUT). The paper investigates the conceptual framework for each discipline, and discusses how they can be integrated to form a seamless transition from requirements models to test models for reliability certification. Two approaches for such an integration are identified: integration by model transformation and integration by model extension. The integration approaches are illustrated through an example, and advantages as well as disadvantages of each approach are discussed. Based on the fact that the two disciplines have models with common information and similar structure, it is argued that an integration may result in coordination benefits and reduced costs. Several areas of further research are identified.
50|2||DHARMA: A tool for evaluating dynamic scheduling algorithms for real-time multiprocessor systems|A majority of today's real-time systems assume a priori knowledge of task characteristics and hence are based on static designs which contribute to their high cost and inflexibility. The next generation of hard real-time systems must be designed to be dynamic and flexible. This provides the motivation to study various dynamic scheduling proposals. In this paper, we evaluate new algorithms for scheduling and resource reclaiming in a dynamic multiprocessor system with fault-tolerant requirements. Resource reclaiming refers to the problem of utilizing resources left unused by a task when its actual computation time is less than its worst case computation time, or when a task is deleted in a fault-tolerant schedule. We also describe the design and implementation of a tool, called Dynamic (Heuristic) scheduling Algorithms for Real-time Multiprocessor systems (DHARMA), to study various dynamic scheduling algorithms, with or without fault-tolerance requirements, and associated resource reclaiming algorithms in a multiprocessor real-time system.
50|2||The effect of compression on performance in a demand paging operating system|As engineers increase microprocessor speed, many traditionally computer-bound tasks are being transformed to input/output (I/O) bound tasks. Where processor performance had once been the primary bottleneck, I/O performance is now the primary inhibitor to faster execution. As the performance gap widens between processor and I/O, it is becoming more important to improve the efficiency of I/O in order to improve overall system performance. In a demand paging operating system, secondary memory is accessed during program execution when a page fault occurs. To improve program execution, it is increasingly important to decrease the amount of time required to process a page fault. This paper describes a compression format which is suitable for both pages of code and pages of data. We find that when the OS/2 operating system is modified to use this compression format, the time saved due to reduced I/O offsets the additional time required to decompress the page.
50|3|http://www.sciencedirect.com/science/journal/01641212/50/3|The âmaintenance-firstâ software era|
50|3||Domain analysis for software reuse|A theory of domain knowledge is proposed that consists of `grounded domains' that model a set of cooperating objects that achieve a purpose. Grounded domains have spatial presence in the real world and contain agents that act on objects within a context of structures. More complex meta-domains use grounded domains as their subject matter and describe education, management, etc. The third component of the theory, generic tasks, describes problem solving activity such as diagnosis, searching, planning and scheduling. Generic tasks describe the behavioural components in both grounded and meta-domains. The reusable library of generic models is applied to the design of interactive systems by reusing the models as templates, and to reuse design knowledge in the form of associated design rationale. A process for recognising generic models is described with recognition heuristics structured in a walkthrough type of analysis for identifying key abstractions in new applications. The design process is illustrated with an information retrieval case study developed as a decision support system for emergency management, reusing information searching services. The discussion reviews the prospects for reusable patterns in interactive systems design, and similar approaches in software and knowledge engineering.
50|3||A problem-oriented and rule-based component repository|Repository-based component reuse is an important technique for raising the efficiency of software development. Traditional approaches focus only on repository organisation and retrieval techniques. This paper proposes a problem-oriented component reuse framework through incorporating a problem-solving mechanism with the traditional component repository. The framework improves the traditional approaches in two aspects. Firstly, the reuse is promoted from the component level to the problem-solving level. User is encouraged to concentrate on the problem description and solving problem through top-down refinement rather than plunge into the technique details at the beginning. Secondly, the environment evolution during running the candidate components for composing an application is simulated through case-based rule reasoning. The candidate components are dynamically checked before composing an application system. We have designed a model base system for the application in scientific computing field based on the framework. The application and analysis show the approach feasible.
50|3||An architecture for software that adapts to changes in requirements|The goal of the research presented in this paper was to study a new software paradigm – adaptive software – in which the structure of an adaptive program is patterned upon the structure of an adaptive controller. Towards this aim, we implemented a domain-specific (object/target recognition) program (A Reconfigurable Architecture for Adapting to Changes in the Requirements (RAACR)) that can adapt to changes in software requirements through the incorporation of feedback. RAACR is a hierarchy of domains (blackboards). Each domain includes multiple knowledge sources (KSs) and a domain scheduler (DS). In response to feedback, KSs change their processing parameters, while DSs change the scheduling policy of the KSs. A generic communication mechanism is implemented on the CORBA compliant SPRING operating system. The adaptability of the program is evaluated quantitatively using a requirements volatility measure and the probability of correct recognition.
50|3||The impact of software architecture reuse on development processes and standards|We have developed an architecture for distributed simulations with reuse in mind from the start. During the past six years, two teams on five projects at US Army Tank-Automotive and Armaments Command (TACOM) have used it. We found that in order to reuse the architecture, it was not only necessary to design it for reuse but also to develop and provide associated development processes and standards for reuse. Not only its functionality for the task to be performed, but also its resultant impact on the software development process determine acceptance of an architecture for reuse. Software reuse is similar to both software tool assessment/technology transfer and requirements analysis in that in all three cases one needs to assess not only product functionality but also its resultant impact on the way that the users of the product work. The DoD is now mandating the use of the High Level Architecture Standard (HLA) for distributed military simulations. Because of our experience in architecture reuse, we realize that HLA adoption will impact not only our product, but also the way we work. In addition to examining HLA functionality, we are assessing HLA's impact on our existing development processes and standards.
50|3||State restoration in Ada 95: a portable approach to supporting software fault tolerance|Studies indicate that techniques for tolerating hardware faults are so effective that software designerrors are the leading cause of all faults encountered. To handle these unanticipated software faults, two main approaches have been proposed: N-versionprogramming and recoveryblocks. Both are based on the concept of designdiversity: the assumption that different designs will exhibit different faults (if any) for the same inputs and will, therefore, provide alternatives for each other. Both approaches have advantages, but this paper focuses upon recovery blocks; specifically, the requirement to save and restore application state. Judicious saving of state has been described as “checkpointing” for over a decade. Using the object-oriented features of the revised Ada language (Ada 95) – a language widely used in this domain – we present three portable implementations of a checkpointing facility and discuss the trade-offs offered by each. Results of the implementation of these mechanisms are used to highlight both the strengths and weaknesses of some of the object-oriented features of Ada. We then show a reusable implementation of recovery blocks illustrating the checkpointing schemes. A performance analysis is made and measurements are presented in support of the analysis.
||||
volume|issue|url|title|abstract
51|1|http://www.sciencedirect.com/science/journal/01641212/51/1|The âsoftware-firstâ revolution in computer hardware design|
51|1||Cache affinity and resequencing in a shared-memory multiprocessing system|Most shared-memory multiprocessor systems have a per-processor memory cache that can improve average memory access time. A task executing on a processor with a cache, accumulates a considerable amount of context at that processor in the form of cache entries. Therefore, a task that restarts after I/O, will probably execute faster on the processor it last ran on, since some of its data may still be present in the cache. A task that is shifted to another processor will have little or no initial cache context, so it must renew its context through cache misses. This not only increases task execution time, but also degrades overall system performance due to an increase in bus traffic.
51|1||An empirical incremental approach to tool evaluation and improvement|Software inspection is well-known as an effective defect finding process. One avenue of inspection research concerns tool support, with the aim of further increasing its efficiency and effectiveness. This has resulted in the production of a number of prototype tools. In general, however, these tools are poorly evaluated and the performance gain which these tools may provide has not been quantified. This paper describes an incremental, cyclic approach to the development and evaluation of a tool to support software inspection. A version of the tool was developed then evaluated against traditional, paper-based inspection. The results were then used to guide development of the next version. This paper describes two of these cycles and the results of the corresponding evaluations. The approach was found to be a useful one, and is applicable to the development of many other tools, processes or procedures.
51|1||A new routing control technique using active temporal data management|Determining the optimal path from source to destination as well as switching packets along the path is one of the main roles of a router in the Internet. Although there are vigorous researches so far related to various kinds of routing protocol, it still needs more sophisticated control techniques to resolve a routing loop problem. In this paper, we propose a new routing control technique using active temporal data management. It uses historical routing information as well as active message processing for efficient management so that it not only minimizes the size and the frequency of messages exchanged among routers, but also assures fast convergence to prevent the message loop. Also in terms of active real-time routing function, it determines the order of messages that happen periodically or aperiodically by means of computing their deadline time described in messages. With a simple example using the suggested technique, we analyze a logical performance evaluation by comparing it with some popular conventional routing protocols such as RIP and IGRP. Finally, concluding remarks and future works are discussed.
51|1||Priority and deadline assignment to triggered transactions in distributed real-time active databases|A distributed real-time active database system (DRTADBS) reacts to the critical events that occurred in the external environment by triggering of transactions. In this paper, the priority and deadline assignment to triggered transactions under two coupling modes, the deferred and immediate, in a DRTADBS is discussed. Two new approaches, the data state dependent (DSD) and the transaction-data deadline (TDD), are proposed to assign criticality and deadlines to the triggered transactions, respectively. In the DSD approach, the criticality of a triggered transaction is defined according to the state of the temporal data object which is responsible for its triggering. The objective of the DSD approach is to increase the number of commit achieved by the triggered transactions especially the more critical ones. The performance of these two approaches under the two coupling modes has been investigated. The results show that the DSD approach is more effective under the immediate coupling mode than under the deferred coupling mode due to the late creation of the triggered transactions under the deferred coupling mode. The TDD approach can improve the system performance under both deferred and immediate coupling mode.
51|1||History-driven dynamic load balancing for recurring applications on networks of workstations|The existing dynamic approaches to load balancing on parallel or distributed computer systems are primarily based on the current system load. Often, variations in the future exertable load are neglected or assumed as fixed.
51|1||A soft computing approach for recognition of occluded shapes|An efficient pattern recognition system based on soft computing concepts has been developed. A new reliable genetic stereo vision algorithm is used in order to estimate depth of objects without using any point-to-point correspondence. Instead, correspondence of the contours as a whole is required. Invariant breakpoints are located on a shape contour using the colinearity principle. Thus, a localized representation of a shape contour including 3-D moments as well as a chain code can be obtained. This representation is invariant to rotation, translation, scale, and starting point. The system is provided with a neural network classifier and a dynamic alignment procedure at its output. Combining the robustness of neural network classifier with the genetic algorithm capability results in a reliable pattern recognition system which can tolerate high degrees of noise and occlusion levels. The performance of the system has been demonstrated using five different types of aircraft and the experimental results are reported.
51|2|http://www.sciencedirect.com/science/journal/01641212/51/2|Software requirements success predictors â behavioral factors beat technical ones|
51|2||Estimation support by lexical analysis of requirements documents|Estimation of the effort required for a software project is difficult. Various means are used, but most rely on some expert assessment of the individual requirements and their implications. A method of supporting this assessment for object-oriented developments is described. Lexical analysis of a draft requirements specification can be used to identify individual objects which will translate directly into the final implementation. These object counts can then be used to provide ‘first-cut’ effort estimates, using historical information from previous projects. Experiments were conducted on a problem implemented by student project teams. The results show that the untrained domain-independent automated noun and technical term finding programs used were no worse than the typical student group in deriving problem-space objects, and that these object counts provided a reasonable indicator to the effort required. Further work in this area is discussed.
51|2||Coupling and control flow measures in practice|
51|2||An empirical study of complexity metrics in Cobol programs|Complexity has been used as an important means for exploiting codes to predict or improve program quality and to measure the impact of maintenance costs. Many of the complexity metrics have been known to measure a similar construct of program codes. In revealing the underlying construct of 13 complexity metrics, this study divides the 368 Cobol programs into four classes with respect to the lines of code and then performs factor analysis on optimally scaled complexity metrics. In each factor, a representative metric is identified and then used to predict the remaining metrics. The results show that each class has a slightly different set of metrics in factors and that the representative metric can successfully estimate the remaining metrics in the same factor. The findings indicate high quality estimations in the 12 Halstead software-science metrics by utilizing a McCabe cyclomatic metric at the end of the design phase. In addition, the lines of code are nicely fitted by utilizing each complexity metric as an independent variable. The fitted lines are computed by two methods; least squares (LS) and relative least squares (RLS). The predictive quality of the LS and RLS is revealed to be dependent on the metrics.
51|2||Validating the ISO/IEC 15504 measures of software development process capability|ISO/IEC 15504 is an emerging international standard on software process assessment. It defines a number of software engineering processes, and a scale for measuring their capability. A basic premise of the measurement scale is that higher process capability is associated with better project performance (i.e., predictive validity). This paper describes an empirical study that evaluates the predictive validity of the capability measures of the ISO/IEC 15504 software development processes (i.e., develop software design, implement software design, and integrate and test). Assessments using ISO/IEC 15504 were conducted on projects world-wide over a period of two years. Performance measures on each project were also collected using questionnaires, such as the ability to meet budget commitments and staff productivity. The results provide evidence of predictive validity for the development process capability measures used in ISO/IEC 15504 for large organizations (defined as having more than 50 IT staff). Furthermore, it was found that the “Develop Software Design” process was associated with most project performance measures. For small organizations evidence of predictive validity was rather weak. This can be interpreted in a number of different ways: that the measures of capability are not suitable for small organizations, or that software development process capability has less effect on project performance for small organizations.
51|2||Improving the quality of the analysis phase|This paper discusses three techniques for improving the quality of the Systems Analysis phase. These techniques are simple, easy, and effective. However, they require a shift from parochial self-reliance to an opportunistic and open-minded use of outside resources.
51|3|http://www.sciencedirect.com/science/journal/01641212/51/3|Academics, and the scarlet letter `A'|
51|3||An impact factor model of Intranet adoption: an exploratory and empirical research|Intranet adoption is a topic of increasing importance to enterprises as well as researchers. This paper discusses the development of a theoretical model and testing of research hypotheses. The model captures the critical factors affecting the success of Intranet adoption. An exploratory survey and an empirical study were conducted. Empirical analysis indicates that the proposed model offers enterprises a model that can be used to plan, design, manage, and to evaluate Intranet adoption, and in turn promote the possibility of successful adoption. Moreover, the findings of the success factors on Intranet adoption are consistent with those in other studies.
51|3||Approaches for broadcasting temporal data in mobile computing systems|Rapid advances in mobile communication technology have spawned many new mobile applications. A key element in many of these systems is the need to distribute real-time information from a database server to mobile clients. While data broadcast has been shown to be an efficient data dissemination technique, many issues such as selection of broadcast data and caching strategies at mobile clients are still active research areas. In this paper, we consider an important characteristic of many mobile computing systems which has often been ignored in the design of broadcast algorithms: the fact that many data items are associated with temporal constraints on their validity. We introduce the notion of absolute validity interval (AVI) to capture the temporal constraints of the data items, formulate a temporal data model and examine both static and dynamic approaches to select data items based on their access frequencies as well as their AVI. The reason for considering the AVI of the data items in broadcast selection is to increase the client cache hit probability so that the access delay for a data item will be much reduced. Based on the results from extensive simulation experiments, it is concluded the AVI-based approaches, by improving cache hit probability, can significantly improve the mean response time and reduce the number of deadline missing requests.
51|3||An index replication scheme for wireless data broadcasting|In mobile distributed environment, data broadcasting has many applications because it has two desirable characteristics: energy efficiency and bandwidth efficiency. There have been some researches on the indexing mechanisms for wireless data broadcasting in the past. We first describe the problematic issue in the conventional index replication scheme, and then propose various index replication methods based on three criteria: accessibility, energy efficiency and adjacency. We evaluate the proposed scheme in analytic and experimental ways. We show that the proposed index replication scheme reduces the data access cost of mobile clients in an energy efficient manner. We also discuss the relaxation of the given criteria.
51|3||TVIS: an interactive multimedia communication engine and its applications|With the rapid progress in computer and communication technologies, it has been the era of interactive distributed applications, e.g., news-on-demand and distant learning. The provision of VCR-like interactive functions, e.g., reverse, skip, freeze–restart, and scale, makes multimedia applications more flexible and useful to users. However, since the media bases are located remotely in a distributed multimedia environment, many issues must be solved to provide smooth interactive multimedia presentations to users. Thus, to facilitate the development of distributed multimedia applications, an interactive communication engine that is capable of handling multimedia communication is required. In this paper, we adopt the master–medium-based control schemes to solve related issues that exist in interactive multimedia presentations. These control schemes can be adopted in the communication engine of a distributed multimedia application for processing reverse, skip, freeze–restart, and scale VCR-like interactive functions. Based on the synchronization control schemes, an interactive multimedia communication engine called the visual interactive system (TVIS) is developed on SUN SPARC workstations. TVIS can be used as the multimedia processing kernel of interactive distributed applications.
51|3||A multi-server video-on-demand system with arbitrary-rate playback support|In this paper, we propose a multi-server design for video-on-demand (VoD) systems that can provide different quality of service (QoS) guarantees by arbitrary-rate video playback. This design chooses a symmetric architecture that consists of a set of identically configured servers and supports arbitrary rates of video playback required by different clients in the following way. The playback requirements requested from all clients are first sent to the master server. If the master server decides to accept a new client, it distributes the task of the new client to all the servers, including itself. Through the pre-processing of the video streams, we can map the data of each frame of every video to a frame map. This frame map indicates whether a frame should be sent to the client at a given frame rate. In our multi-server design for a VoD system, using these frame maps and a suitable transmission schedule, a client can demand to play any video, at any playback rate and at any time. Furthermore, the VoD system provides a high degree of fault tolerance. This design not only can tolerate the failure of one or more servers, but also requires relatively small bandwidth capability or reservation for fault tolerance. In addition, this multi-server design can effectively overcome the hot spot problem of popular videos and keep the load balanced at the same time. Finally, we discuss how to generalize the proposed method for a large-scale VoD system with heterogeneous architecture.
51|3||IDRS: an interactive digital radio station over Internet|Many networking applications become feasible and required with the popular use of Internet. In this paper, we present an interactive digital radio station, which is called IDRS, that can be executed over Internet. Two modes that exist in IDRS are the DJ mode and the Call-In mode. In the DJ mode, music and/or DJ's speech are multicasted to audiences; in the Call-In mode, an audience is allowed to Call-In. In the Call-In mode, the audience speech, the DJ's speech, and/or the background music can be multicasted to audiences. Related technique issues and the corresponding system development are presented in detail in this paper.
51|3||Exploring the relationships between design measures and software quality in object-oriented systems|One goal of this paper is to empirically explore the relationships between existing object-oriented (OO) coupling, cohesion, and inheritance measures and the probability of fault detection in system classes during testing. In other words, we wish to better understand the relationship between existing design measurement in OO systems and the quality of the software developed. The second goal is to propose an investigation and analysis strategy to make these kind of studies more repeatable and comparable, a problem which is pervasive in the literature on quality measurement. Results show that many of the measures capture similar dimensions in the data set, thus reflecting the fact that many of them are based on similar principles and hypotheses. However, it is shown that by using a subset of measures, accurate models can be built to predict which classes most of the faults are likely to lie in. When predicting fault-prone classes, the best model shows a percentage of correct classifications higher than 80% and finds more than 90% of faulty classes. Besides the size of classes, the frequency of method invocations and the depth of inheritance hierarchies seem to be the main driving factors of fault-proneness.
51|3||Corrigendum to: An assessment of systems and software engineering scholars and institutions (1994â1998) [The Journal of Systems and Software 49 (1) (1999) 81â86]|
52|1|http://www.sciencedirect.com/science/journal/01641212/52/1|On design|
52|1||Software development risks to project effectiveness|Controlling the risks to a software development project is a critical issue examined by information system researchers. Many of the studies and methods to date examine the risks from the broad aspect of system success but attempt to promote activities related to a narrow domain of risks. Our intent is to examine the impact of the spectrum of risks on different aspects of system development. We report the results of a survey of 86 project managers that indicate common aspects of project effectiveness are generally under control, but are most effected by lack of expertise on the project team. Significant relationships also show that lack of clear role definition and conflicts on the team are also elevated risks. Other items are not as critical or limited to a much smaller aspect of effectiveness than overall success. This focusing in on the more important risk aspects will allow for more effective management of the project and a narrowing of techniques to mitigate the significant risks.
52|1||Breadth-first search oriented symbolic picture representation for spatial match retrieval|To provide sufficient information for percepting symbolic objects among pictures from pictorial information systems, an efficient spatial data structure is needed to identify objects and to specify the spatial relationships among objects in a picture. In this paper, a new symbolic picture representation is proposed through the traversal of a quadtree corresponding to a given picture using breadth-first search and thus is called BFS string. An iconic indexing scheme for retrieving symbolic pictures based on the proposed BFS strings is presented and the algorithms introduced in the bin-tree oriented iconic indexing scheme by Chang and Lee (1998b) [Journal of Data and Knowledge Engineering 26 (2)]. is applied to the scheme for manipulating spatial relationships among objects. Compared with the bin-tree oriented scheme in storage complexity, experimental results show that the proposed BFS string requires less storage space when the picture has a great number of objects.
52|1||An on-line scheduling policy for IRIS real-time composite tasks|In a new class of real-time tasks, called increasing reward with increasing service tasks (IRIS), the value of a task's computation increases as a function of the amount of execution time it has been able to accrue before its deadline expires. A composite task is a set of dependent component tasks. This paper presents an on-line scheduling policy for IRIS composite tasks. This policy aims to increase the total accrued reward of composite tasks within deadlines on a uniprocessor. If there is no enough time to schedule all the component tasks of a composite task, those component tasks having the little impact on the total reward of the composite task are not scheduled for execution by the proposed scheduling policy. A flexible task graph is also introduced to aid in determining the amount of service times to be allocated to the given composite tasks. The performance of the proposed scheduling policy is evaluated both analytically and by computer simulation.
52|1||Efficient run-time assurance in distributed systems through selection of executable assertions|Run-time assurance of a distributed system can be obtained by comparing, at run-time, the actual behavior of a program with the expected behavior described in the program's specification. Executable assertions, embedded into the program code, can determine when there are discrepancies between actual and expected behavior. There is no global monitoring scheme and error-detection will occur at the process level. We can assume that a non-faulty process will always perform correct computations. It can detect errors in other processes after receiving information from them and checking it against expected values using executable assertions. In order to efficiently check programs at run-time, we need to determine how many assertions need to be used, where they need to be located, and what they need to check to ensure that all occurring errors can be detected. This paper introduces temporal subsumption to remove, from a given set of assertions for a specific distributed program, the assertions which perform redundant checking. The remaining set of assertions is then the set necessary to provide run-time assurance. To subsume assertions, the flow graphs of the individual components of the distributed system are examined using a graph traversal algorithm. Temporal subsumption is a pre-processing step that creates a smaller set of assertions to be embedded into the program and to be checked at run-time. This makes error-detection at run-time less time-consuming and thus more efficient since redundant checking is avoided.
52|1||Data dependence analysis for array references|Data dependence analysis is essential in order to determine whether a loop can be vectorized or parallelized. It is used to investigate whether two memory references in a loop are in the same location. In this paper we present a new scheme of exact data dependence analysis for array subscript analysis in parallel programs, called interval reduction (IR) test. This method reduces the solution interval of each constraint variable in the dependence equation by repeated projection. Once the effective solution interval of any variable shrinks to empty, this linear diophantine equation has no integer solution and the memory accesses subject to this constraint are then independent. Otherwise, all the integer solutions can be found, and thus the dependence distance can be computed as well. Experimental results are also presented to show its effectiveness compared to the Power test and the Omega test.
52|1||Applying stack simulation for branch target buffers|Branch target buffer (BTB) is widely used in modern microprocessor designs to reduce the penalties caused by branches. To evaluate the performance of a BTB, trace-driven simulation is often used. However, as the trace of a typical program is very large, the simulation time is often too long. To reduce the simulation time, we developed a stack simulation technique for BTB to evaluate many sets of design parameters in one simulation pass. Due to the fact that the prediction information in the BTB does not have the inclusion property – a property which makes the stack simulation work, we propose a state vector method to enumerate the prediction information for different sets of BTB design parameters to mimic the inclusion property. Simulation results show that the state vector method greatly reduces the simulation time. The speedup of the stack simulation for BTB proposed in this paper over the traditional BTB simulation is 4.68 in terms of simulation time when 13 sets of BTB design parameters are simulated in one simulation pass.
52|1||To Kanji or not to Kanji: a cognitive perspective|The Kanji-writing people represent almost a quarter of the world's population and their use of computers is only beginning. For most of them, to move the computers from the back room to the desk top requires that interfaces not only be easy to use but also easy to learn. Unfortunately, the complexity of Kanji entry renders the adoption of computers in countries like Taiwan and China difficult, if not impossible. Unless text entry, the most basic issue of computer usage, is better understood and assisted by an easy-to-use Kanji entry method, the growth of the computer industry will be limited.
52|2-3|http://www.sciencedirect.com/science/journal/01641212/52/2-3|Evaluation and assessment in software engineering|
52|2-3||Investigating principles of stakeholder evaluation in a modern IS development approach|This paper investigates how stakeholder evaluation strategies may be embraced into a modern IS development approach – the Dynamic Systems Development Method (DSDM). DSDM is currently the de facto standard for rapid application development (RAD) in the UK. Theoretically, the participative and iterative nature of DSDM can facilitate effective IS evaluation strategies of an interpretive and informal nature. Using both the object system class framework [Hirschheim96], and the informal findings of a recent survey of DSDM-affiliated UK organisations, the authors produce results encouraging to those proponents of a more holistic approach to IS evaluation. The disparate nature of DSDM in practice also serves as a warning that the success of such evaluation strategies may depend heavily on how the organisation embraces specific IS development strategies.
52|2-3||Quantitative analysis of static models of processes|The upstream activities of software development projects are often viewed as both the most important, the least understood, and hence the most problematic. This is particularly noticeable in terms of satisfying customer requirements. Business process modelling is one solution that is being increasingly used in conjunction with traditional software development, often feeding in to requirements and analysis activities. In addition, research in Systems Engineering for Business Process Change,1 highlights the importance of modelling business processes in evolving and maintaining legacy systems that support those processes. However, the major use of business process modelling, is to attempt to restructure the business process, in order to improve some given aspect, e.g., cost or time. This restructuring may be seen either as separate activity or as a pre-cursor to the development of systems to support the new or improved process. The analysis of these business models is, therefore, vital to the improvement of the process and the development of supporting software systems. Supporting this analysis is the focus of this paper. Business processes are typically described with static (diagrammatic) models. This paper proposes a quantitative approach to aid analysis and comparison of these models. This is illustrated using the process-modelling notation, Role Activity Diagrams (RADs). We studied 10 prototyping processes across a number of organisations and found that roles of the same type exhibited similar levels of coupling across processes. Where roles did not adhere to tentative threshold values, further investigation revealed unusual circumstances or hidden behaviour. Notably, analysis of the prototyping roles (which exhibited the greatest variation in coupling), found that coupling was highly correlated with the size of the development team and the number of participants. This suggests that prototyping in large projects had a different process to that for small projects and required more mechanisms for communication. We conclude that counts (measures) may be useful in the analysis of static process models.
52|2-3||Evaluation of code review methods through interviews and experimentation|This paper presents the results of a study where the effects of introducing code reviews in an organisational unit have been evaluated. The study was performed in an ongoing commercial project, mainly through interviews with developers and an experiment where the effects of introducing code reviews were measured. Two different checklist-based review methods have been evaluated. The objectives of the study are to analyse the effects of introducing code reviews in the organisational unit and to compare the two methods. The results indicate that many of the faults that normally are found in later test phases or operation are instead found in code reviews, but no difference could be found between the two methods. The results of the study are considered positive and the organisational unit has continued to work with code reviews.
52|2-3||The role of comprehension in software inspection|In spite of code inspections having been demonstrated as an effective defect detection process, little work has been done to determine how this process best supports the object-oriented paradigm. In contrast, this paradigm (or at least its questionable manifestation in C++) is well supported by tools that purport to aid comprehension. These tools typically take the form of visualisation tools designed to assist in the maintenance process, and it is natural to consider that these tools (or adaptations thereof) might also support inspection. However, since these tools claim to aid comprehension, it is important to consider the role of comprehension in inspection. Or put simply, does comprehension matter, or are there simple techniques in existence which are similarly effective in detecting defects? This paper presents the issues associated with inspections (and the complications presented by the object-oriented paradigm) and comprehension, and presents the results of two experiments which considered the relationship between comprehension and inspection. The results indicate a relationship, but further work is needed to determine the precise nature of this relationship and how inspections might best be supported in the future.
52|2-3||A formative evaluation of information retrieval techniques applied to software catalogues|Software catalogues are crucial ingredients of any development process based on reuse. In fact, to be reused, software components must be first properly catalogued and then easily found and understood. In this paper we discuss how information retrieval (IR) techniques can be utilized to handle a catalogue derived from an existing software package and how their effectiveness can be assessed. An empirical evaluation of a prototype system handling an industrial-level software package is described and its outcomes are discussed. The conclusion of the formative evaluation is that such techniques are effective and that both expert and non-expert users find them useful and satisfying.
52|2-3||Robust estimations of fault content with captureârecapture and detection profile estimators|Inspections are widely used in the software engineering community as efficient contributors to reduced fault content and improved product understanding. In order to measure and control the effect and use of inspections, the fault content after an inspection must be estimated. The capture–recapture method, with its origin in biological sciences, is a promising approach for estimation of the remaining fault content in software artefacts. However, a number of empirical studies show that the estimates are neither accurate nor robust. In order to find robust estimates, i.e., estimates with small bias and variations, the adherence to the prerequisites for different estimation models is investigated. The basic hypothesis is that a model should provide better estimates the closer the actual sample distribution is to the model's theoretical distribution. Firstly, a distance measure is evaluated and secondly a Ï2-based procedure is applied. Thirdly, smoothing algorithms are tried out, e.g., mean and median values of the estimates from a number of estimation models. Based on two different inspection experiments, we conclude that it is not possible to show a correlation between adherence to the models’ theoretical distributions and the prediction capabilities of the models. This indicates that there are other factors that affect the estimation capabilities more than the prerequisites. Neither does the investigation point out any specific model to be superior. On the contrary, the Mh–JK model, which has been shown as the best alternative in a prior study, is inferior in this study. The most robust estimations are achieved by the smoothing algorithms.
52|2-3||An evaluation of the business object approach to software development|In this paper, we report the result of an evaluation of the use of business objects and business components for developing business application software. This evaluation was a replicated product case study in which a part of an existing product was re-implemented using an UML-based development process. In order to assess the impact of re-use, a second related product was implemented using the new technology. We found that producing software from scratch using UML was less productive during the development lifecycle, but productivity improved when substantial reuse (48%) was achieved. Time to market was not much affected by the new technology but was greatly improved when substantial reuse was achieved. Defect rates appeared substantially lower for the new technology irrespective of reuse levels. The technology also had other benefits including provision of documentation and less reliance on individual members of staff.
52|2-3||Coupling measures and change ripples in C++ application software|This paper describes an investigation into the effects of class couplings on changes made to a commercial C++ application over a period of  yr. The Chidamber and Kemerer CBO metric is used to measure class couplings within the application and its limitations are identified. Through an in-depth study of the ripple effects of changes to the source code, practical insight into the nature and extent of software couplings is provided.
52|2-3||Dependability certification of software components|Software components need a uniform approach for rating their quality. The need for this stems from a licensee's inaccessibility to the source code as well as other information concerning how thoroughly the component was validated. This paper proposes a `test quality rating' (TQR) metric that will act as a component's dependability `score'. We envision a process whereby a software publisher submits a component to an independent certification organization that would then calculate TQR for that component. It is preferable for component dependability validation to be performed by an independent organization. This is because even an honest dependability overestimation error on the part of the publisher could be grounds for severe legal penalties. This score would be displayed on any marketing materials or contracts which license that component. In our paper we provides results from applying the metric to a commercial financial application written in Java in order to demonstrate the effectiveness of the metric.
52|2-3||Experimental assessment of the effect of inheritance on the maintainability of object-oriented systems|In this paper, we describe an empirical investigation into the modifiability and understandability of object-oriented (OO) software. A controlled experiment was conducted to establish the effects of varying levels of inheritance on understandability and modifiability. The software used in this experiment consisted of a C++ system without any inheritance and a corresponding version containing three levels of inheritance, as well as a second larger C++ system without inheritance and a corresponding version with five levels of inheritance. For both of the systems, the application modelled a database for a University personnel system. A number of statistical hypotheses were tested. Results indicated that the systems without inheritance were easier to modify than the corresponding systems containing three or five levels of inheritance. Also, it was easier to understand the system without inheritance than a corresponding version containing three levels of inheritance. Results also indicated that larger systems are equally difficult to understand whether or not they contain inheritance. The results contained in this paper highlight the need for further empirical investigations in this area, particularly into the benefits of using inheritance.
53|1|http://www.sciencedirect.com/science/journal/01641212/53/1|Empirical studies of software development and evolution|
53|1||Towards a framework for empirical assessment of changeability decay|Evolutionary development allows early and frequent adaptations to new or changed requirements. However, such unanticipated changes may invalidate design documentation and cause structural degradations of the software, which in turn may accelerate changeabilitydecay. Our definition of changeability decay focuses on the increased effort required to implement changes. We have identified three approaches to the assessment of changeability decay: (1) structure measurement, (2) change complexity measurement, and (3) benchmarking. Our research aims to evaluate and compare these approaches in order to develop an empirical assessment framework. In this paper we propose a set of change complexity measures (2) and compare them with structural attribute measures (1) using detailed process and product data collected from a commercial object-oriented development project. The preliminary results indicate that the change complexity measures capture some dimensions of changeability decay not accounted for with structural attribute measures. However, the current findings also suggest that many aspects of changeability decay cannot be accounted for by the indirect measures utilized in approach (1) and (2). As an alternative approach, we therefore propose using benchmarks (3) where change effort can be measured more directly. A research methodology for the development of benchmarks and benchmarking procedures are described.
53|1||Use of friends in C++ software: an empirical investigation|Use of friends in C++ software is widely considered as poor programming practice. However, little empirical evidence exists to support this belief. In this paper, the results of a case-study based investigation into the use of friends in object-oriented software are described. Four C++ systems of varying sizes were analysed and data related to friends collected for each. Class metrics were collected from each of the systems. Five hypotheses were then investigated.
53|1||An investigation of machine learning based prediction systems|Traditionally, researchers have used either off-the-shelf models such as COCOMO, or developed local models using statistical techniques such as stepwise regression, to obtain software effort estimates. More recently, attention has turned to a variety of machine learning methods such as artificial neural networks (ANNs), case-based reasoning (CBR) and rule induction (RI). This paper outlines some comparative research into the use of these three machine learning methods to build software effort prediction systems. We briefly describe each method and then apply the techniques to a dataset of 81 software projects derived from a Canadian software house in the late 1980s. We compare the prediction systems in terms of three factors: accuracy, explanatory value and configurability. We show that ANN methods have superior accuracy and that RI methods are least accurate. However, this view is somewhat counteracted by problems with explanatory value and configurability. For example, we found that considerable effort was required to configure the ANN and that this compared very unfavourably with the other techniques, particularly CBR and least squares regression (LSR). We suggest that further work be carried out, both to further explore interaction between the end-user and the prediction system, and also to facilitate configuration, particularly of ANNs.
53|1||Empirical analysis in software process simulation modeling|Software process simulation modeling is increasingly being used to address a variety of issues from the strategic management of software development, to supporting process improvements, to software project management training. The scope of software process simulation applications ranges from narrow focused portions of the life cycle to longer-term product evolutionary models with broad organizational impacts. This paper discusses some of the important empirical issues that arise in software process simulation modeling. We first address issues concerning real-world data used to (1) establish input parameters to a software process simulation model, and (2) establish actual organizational results against which the model’s results (i.e., outputs) will be compared. On the input side, the challenges include small sample sizes, considerable variability and outliers, lack of desired data, loosely defined metrics, and so forth. On the output side, the paper addresses (1) verification and validation of the model, and (2) quantitative approaches to evaluating model outputs in support of managerial decision making including financial performance using Net Present Value (NPV), multi-criteria utility functions, and Data Envelopment Analysis (DEA). The paper focuses on the stochastic modeling using Monte Carlo simulation. The paper is grounded in the authors’ practical application experiences, and major points are illuminated by examples drawn from that field work.
53|1||Using evolution to evaluate reverse engineering technologies: mapping the process of software change|This paper describes a case study where versions of software are used to track actual changes made to software applications. The process of evolution within a number of software applications is described. The applications are developed using two different software development languages; C and COBOL. The evolution of the applications is tracked and language specific differences are highlighted. Graphical representations of the change process are then produced and the reasons for specific change features identified. The aim of the approach is to gain a more detailed understanding of how and where change processes take place. The consequence of identifiable sets of changes and their effect on the future maintainability of software is discussed. The overall aim of this study is to provide a characterisation of the change process that eventually can be used to evaluate the suitability of reverse engineering technologies in re-structuring legacy applications to a form more suited to successful evolution.
53|1||Priorities for the education and training of software engineers|We present the complete results of our 1998 survey of software practitioners. In this survey we asked over 200 software developers and managers from around the world what they thought about 75 educational topics. For each topic, we asked them how much they had learned about it in their formal education, how much they know about it now and how important the topic has been in their career. The objective of the survey was to provide data that can be used to improve the education and training of information technology workers. The results suggest that some widely taught topics perhaps should be taught less, while coverage of other topics should be increased.
53|1||Optimal binary vote assignment for replicated data|Data replication can be used to improve the availability of data in a distributed database system. In such a system, a mechanism is required to maintain the consistency of the replicated data. Weighted voting is a popular solution for maintaining the consistency. The performance of a voting system is determined by the vote assignment. It was shown in previous studies that finding the optimal solution among nonnegative integer vote assignments requires O(2N2) time. So it is not suitable to find the optimal integer vote assignment (OIVA) for large systems. In this paper, we propose the optimal binary vote assignment (OBVA) considering only binary vote assignments. It is shown that OBVA can achieve nearly the same availability of OIVA. On the other hand, OBVA requires only O(N2) time, which is preferred for large systems.
53|1||Branch grafting method for R-tree implementation|R-trees are an important spatial data structure for database storage. In this paper, we introduce a new method, called branch grafting method, for R-tree implementation. The major motivation of this proposed technique is to produce more desirable result and to reduce the number of nodes created in the R-tree. We present this method by comparing it with reinsertion method as used in R*-tree. Experiments have been carried out, and the strength of this proposed method is outlined.
53|2|http://www.sciencedirect.com/science/journal/01641212/53/2|The End of the âOutsourcing Eraâ|
53|2||Coping with ârequirements-uncertaintyâ: the theories-of-action of experienced IS/software project managers|The notion of ‘requirements-uncertainty’ has received a lot of attention in the Information Systems (IS) and Software Engineering literature. As the level of uncertainty of user-requirements increases, this literature advises project managers to move away from the traditional waterfall life-cycle model and towards more ‘experimental’ approaches, such as incremental-delivery and prototyping. But there is evidence from empirical research to show that this advice is not always followed. So, it seems that managing requirements-uncertainty may be a more complicated matter. In this paper, I identify the strategies that experienced IS project managers espouse for coping with requirements-uncertainty. I show that project managers espouse different strategies for coping with different aspects of requirements-uncertainty. I also show that project managers view prototyping and incremental development as ‘broad-spectrum’ strategies that are salient for coping with a wide range of project risk-drivers, including aspects of requirements-uncertainty. Finally, I conjecture that the notion of ‘requirements-uncertainty’ may have been over-abstracted in the literature.
53|2||A method and tool for assessing object-oriented projects and metrics management|The number of metrics and tools for the assessment/control of object-oriented project is increasing. In the last years, the effort spent in defining new metrics has not been followed by a comparable effort in establishing methods and procedures for their systematic application. To make the investment on project assessment effective, specific methods and tools for product and process control have to be applied and customized on the basis of specific needs. In this paper, the experience of the authors cumulated in interpreting assessment results and defining a tool and a method for the assessment of object-oriented systems is reported. The tool architecture and method for system assessment provide support for: (1) customizing the assessment process to satisfy company needs, project typology, product profile, etc.; (2) visualizing results in an understandable way; (3) suggesting actions for tackling with problems; (4) avoiding unuseful interventions and shortening the assessment analysis; (5) supporting metrics validation and tuning. The tool and method have been defined in years of work in identifying tool features and general guidelines to define a modus operandi with metrics, with a special care to detect analysis and design problems as soon as possible, and for effort estimation and prediction. In this line, a specific assessment tool has been built and used as a research prototype in several projects.
53|2||Estimating the extent of standards use: the case of ISO/IEC 15504|There has been a proliferation of software engineering standards in the last two decades. While the utility of standards in general is acknowledged, thus far little attempt has been made to evaluate the success of any of these standards. One suggested criterion of success is the extent of usage of a standard. In this paper we present a general method for estimating the extent to which a standard is used. The method uses a capture–recapture (CR) model that was originally proposed for estimating birth and death rates in human populations. We apply the method to estimate the number of software process assessments that were conducted world-wide between September 1996 and June 1998 using the emerging ISO/IEC 15504 International Standard. Our results indicate that 1264 assessments were performed with a 90% confidence interval of 916 and 1895. The method used here can be applied to estimate the extent of usage of other software engineering standards, and also of other software engineering technologies. Such estimates can benefit standards (or technology) developers, funding agencies, and researchers by focusing their efforts on the most widely used standards (or technologies).
53|2||An investigation of risk perception and risk propensity on the decision to continue a software development project|Many information system (IS) failures may result from the inadequate assessment of project risk. To help managers appraise project risk more accurately, IS researchers have developed a variety of risk assessment tools including checklists and surveys. Implicit in this line of research, however, is the assumption that the use of such devices will lead to more accurate risk perceptions that will, in turn, lead to more appropriate decisions regarding project initiation and continuation. Little is known, though, about the factors that influence risk perception or the interrelationships that exist among risk perception, risk propensity, and decisions about whether or not to continue a project. Without a better understanding of these relationships it is difficult to know whether the application of risk instruments will be an effective means for reducing the incidence of IS failure. This study presents the results of a laboratory experiment designed to: (1) examine the relative contribution of two factors that are believed to shape risk perception: probability that a loss will occur and the magnitude of the potential loss, and (2) explore the relative influence of risk perception and risk propensity on the decision of whether or not to continue a software development project. The results indicate that magnitude of potential loss is the more potent factor in shaping risk perception and that a significant relationship exists between risk perception and decision-making. The implications of these findings are discussed along with directions for future research.
53|2||A new approach to fault-tolerant scheduling using task duplication in multiprocessor systems|In this paper we propose a new approach to fault-tolerant scheduling of parallel programs in multiprocessor systems. It is well known that inter-processor communication makes serious effects on the performance of parallel processing, and that task duplication is an effective technique to reduce overheads of communication. Though it was originally developed only for improving performance, we propose the use of this technique also for achieving fault-tolerance. Based on this approach, we develop a scheduling algorithm for tolerating a single processor failure. This algorithm duplicates all tasks of a program and allocates them to processors so as to eliminate communication delays as much as possible. The experimental results show that the obtained schedules can achieve fault-tolerance at the cost of small degree of time redundancy.
53|2||TOFF-2: A high-performance fault-tolerant file service|
53|2||An experimental comparison of reading techniques for defect detection in UML design documents|The basic motivation for software inspections is to detect and remove defects before they propagate to subsequent development phases where their detection and removal become more expensive. To maximize this potential, the examination of the artefact under inspection must be as thorough and detailed as possible. This implies the need for systematic reading techniques that tell inspection participants what to look for and, more importantly, how to scrutinize a software document. Recent research efforts have investigated the benefits of scenario-based reading techniques for defect detection in functional requirements and functional code documents. A major finding has been that these techniques help inspection teams find more defects than existing state-of-the-art approaches, such as, ad hoc or checklist-based reading (CBR). In this paper, we describe and experimentally compare one scenario-based reading technique, namely perspective-based reading (PBR), for defect detection in object-oriented design documents using the notation of the unified modeling language (UML) to the more traditional CBR approach. The comparison was performed in a controlled experiment with 18 practitioners as subjects. Our results indicate that PBR teams discovered, on average, 58% of the defects and had an average cost per defect ratio of 56 min per defect. In this way, PBR is more effective than CBR (i.e., it resulted in inspection teams detecting, on average, 41% more unique defects than CBR). Moreover, the cost of defect detection using PBR is significantly lower than CBR (i.e., PBR exhibits, on average, a 58% cost per defect improvement over CBR). This study therefore provides evidence demonstrating the efficacy of PBR scenarios for defect detection in UML design documents. In addition, it demonstrates that a PBR inspection is a promising approach for improving the quality of models developed using the UML notation.
53|3|http://www.sciencedirect.com/science/journal/01641212/53/3|The missing piece of software development|
53|3||Extending the product family approach to support safe reuse|Upcoming spacecraft will reuse software components to the extent that some systems will form product families of similar or identical units (e.g., a fleet of spaceborne telescopes). Missions such as these must be demonstrably safe, but the consequences of broad reuse are hard to evaluate from a software safety perspective. This paper reports experience specifying an interferometer (telescope) subsystem as a product family and supplementing the specification with results from a failure analysis. Extensions to the product family approach, with lessons learned, are discussed in three areas: (1) integration of safety analysis with the product family approach; (2) modeling decisions that have safety implications (e.g., how to handle near-commonalities, establishing a hierarchy of variabilities, and specifying dependencies among options); and (3) use of the product family requirements for design evaluation of reusable components as well as a specific product.
53|3||Types of collaborative work in software engineering|This paper is an account on work distribution analyzed from the collaboration point of view. It presents a new classification of the collaborative work in software engineering project. Four types of collaborative work are defined derived from empirical measurements of activities. Mandatory collaborative works are formal scheduled meetings. Called collaborative work is defined when team members call a meeting to solve a problem, which is most often technical. Ad hoc collaborative work is defined when team members work on the same task at the same time and individual work occurs when a team member works on its own on a task related to the project. Data are extracted from the logbook filled out by four team members working on an industrial project that lasts 19 weeks. The characteristics of each type of collaborative activity are described and a quantitative breakdown of how people spend their time in collaboration within a single project is presented.
53|3||A hybrid approach to analyze empirical software engineering data and its application to predict module fault-proneness in maintenance|Knowledge discovery from software engineering measurement data is essential in deriving the right conclusions from experiments. Various data analysis techniques may provide data analysts with different and complementary insights into the studied phenomena. In this paper, two data analysis techniques – Rough Sets (RSs) and Logistic Regression (LR) are compared, from both the theoretical and the experimental point of view. In particular, the empirical study was performed as a part of the ESPRIT/ESSI project CEMP on a real-life maintenance project, the DATATRIEVE™ project carried out at Digital Engineering Italy. We have applied both techniques to the same data set. The goal of the experimental study was to predict module fault-proneness and to determine the major factors affecting software reliability in the application context. The results obtained with either analysis technique are discussed and compared. Then, a hybrid approach is built, by integrating different and complementary knowledge obtained from either approach on the fault-proneness of modules. This knowledge can be reused in the organizational framework of a company-wide experience factory.
53|3||Function point counting: one programâs experience|
53|3||System Software support for distributed real-time systems|In this paper, we consider a scalable distributed-memory architecture for which we propose a problem representation that assigns real-time tasks on the processing units of the architecture to maximize deadline compliance rate. Based on the selected problem representation, we derive an algorithm that dynamically schedules real-time tasks on the processors of the distributed architecture. The algorithm uses a formula to generate the adequate scheduling time so that deadline loss due to scheduling overhead is minimized while deadline compliance rate is being maximized. The technique we propose is proved to be correct in the sense that the delivered solutions are not obsolete, i.e. the assigned tasks to working processors are guaranteed to meet their deadlines once executed. The correctness criterion is obtained based on our technique to control the scheduling time.To evaluate the performance of the algorithms that we propose, we provide a number of experiments through a simulation study. We also propose an implementation of our algorithms in the context of scheduling real-time transactions on an Intel-Paragon distributed-memory multiprocessor. The results of the conducted experiments show interesting performance trade-offs among the candidate algorithms.
53|3||Risky business: what we have yet to learn about risk management|This paper examines the way in which software practitioners are taught to perform risk management, and compares it with risk management in other fields. We find that there are three major problems with risk management: false precision, bad science, and the confusion of facts with values. All of these problems can lead to bad decisions, all in the guise of more objective decision-making. But we can learn from these problems and improve the way we do risk management.
53|3||Testing software to detect and reduce risk|The risk of a piece of software is defined to be the expected cost due to operational failures of the software. Notions of the risk detected by a testing technique and the risk reduction due to a technique are introduced and are used to analytically compare the effectiveness of testing techniques. It is proved that if a certain relation holds between testing techniques A and B, then A is guaranteed to be at least as good as B at detecting and reducing risk, regardless of the particular faults in the program under test or their costs. These results can help practitioners choose an appropriate technique for testing software when risk reduction is the goal.
53|3||Risk-based testing:: Risk analysis fundamentals and metrics for software testing including a financial application case study|The idea of risk-based testing is to focus testing and spend more time on critical functions. By combining the focused process with metrics it is possible to manage the test process by intelligent assessment and to communicate the expected consequences of decisions taken. This paper discusses an approach to risk-based testing and how risk-based testing was carried out in a large project in a financial institution. The paper concludes with how practical risk-based testing experience should inform theory and provide advice on organizational requirements that are necessary to achieve success.
54|1|http://www.sciencedirect.com/science/journal/01641212/54/1|A letter from the frustrated author of a journal paper|
54|1||An information retrieval system based on a user profile|In this era of information explosion, providing the right information to the right person within reasonable time duration is a very important goal for today’s information retrieval (IR) systems. Due to the characteristics of retrieving methods, the conventional IR models often suffer from inaccurate and incomplete queries as well as inconsistent document relevance. Moreover, each user has his/her own interpretation of the semantic meaning of query terms during the retrieving process. Thus, high accuracy of retrieved information is somewhat hard to achieve. However, accuracy can be improved by designing an IR model that can adapt to the diverse needs of an individual and can perform a personalized search. In this paper, an adaptive information retrieval system embedded in an intelligent feedback tuning mechanism is proposed to capture the personal notions of query terms. This mechanism together with a correlation table is used as a user profile to ‘simulate’ the user’s notions of terms. This correlation table includes the degrees of semantic relevance (SR) and co-occurrence (CO) among terms. By using the personal profile during retrieval, the system can obtain a better match between the information needs and the retrieved results. It is also shown that the recall/precision rates for our system are approximately 91% and 81%, respectively, on average.
54|1||Similarity retrieval based on group bounding and angle sequence matching in shape database systems|In this paper, a new method for retrieving similar shapes from a shape database is proposed. The shapes in the database are indexed by their CPA-strings. When a query shape is submitted to the system, it is converted to a CPA-string from which both the lower and the upper bounds of the locations of the potentially matched shapes are computed. This will restrict the search space to a reasonable small proportion of the whole database. At the second stage, an angle sequence matching algorithm is invoked to compute the Weighted Levensthein Distances between the query CPA-string and the selected database CPA-strings. The shapes that have distances less than a given threshold are finally retrieved. Experimental results show that our approach is robust, accurate and efficient in terms of finding the desired shapes within a reasonable time, even if the shapes are rotated, scaled, and may have boundary noise.
54|1||An empirical analysis of debugging performance â differences between iterative and recursive constructs|An experiment involving more than 500 responses from over 250 subjects was conducted for the purpose of identifying differences in debugging performance among subjects who debugged iterative code and those who debugged recursive code. The subjects, classified as novices or experienced, based on programming experience, were asked to locate and fix a single logical bug in small segments of C code. Two different tasks – searching a linked list and copying a linked list, were used in the experiment. Statistical analyses yielded the following significant results: (i) 63.2% of all subjects located the bug in the recursive constructs compared to 41.5% for the iterative constructs (p<.00001); (ii) Grouped by task, and level of programmer experience, there were significantly more correct responses for the recursive code than for the iterative code (p<.02 in all four groupings); (iii) In terms of locating and fixing the bug, more responses were correct for the recursive version of the copy task than for the iterative version (p=.01875). Results of a t-test procedure showed no significant difference between the iterative and recursive constructs in terms of mean times needed for successful completion of the debugging tasks.
54|1||Applying meta-analytical procedures to software engineering experiments|Deriving reliable empirical results from a single experiment is an unlikely event. Hence to progress multiple experiments must be undertaken per hypothesis and the subsequent results effectively combined to produce a single reliable conclusion. Since results are quantitative in nature, a quantitative conclusion would be the optimal solution. Other disciplines use meta-analytic techniques to achieve this result. The treatise of this paper is: can meta-analysis be successfully applied to current software engineering experiments? The question is investigated by examining a series of experiments, which themselves to investigate — which defect-detection technique is best? Applying meta-analysis techniques to the software engineering data is relatively straightforward, but unfortunately the results are highly unstable, as the meta-analysis shows that the results are highly disparate and do not lead to a single reliable conclusion. The reason for this deficiency is the excessive variation within various components of the experiments. The paper outlines various ideas from other disciplines for controlling this variation and describes a number of recommendations for controlling and reporting empirical work to advance the discipline towards a position, where meta-analysis can be profitably employed.
54|1||Performance evaluation of transmission schemes for real-time traffic in a high-speed timed-token MAC network|
54|1||Perceptions of contribution in software teams|In this paper, we report results of a survey of graduate students who worked in teams on software projects. The survey revealed that there is a statistically significant gap between how team members perceived their own contribution towards the goals of the project, and how that contribution was perceived by their teammates. The paper discusses this gap and its implication for the functioning of the teams.
54|1||Non-blocking distributed transaction processing system|The prolific development of the wide variety of Internet applications forces a need for a software solution to enable access to multiple data sources while ensuring data integrity and consistency. Common Object Request Broker Architecture (CORBA) Services specified by the Object Management Group (OMG) includes a Transaction Service Specification. The Object Transaction Server (OTS) developed in this project is based on this specification. It acts as a robust system level tool to enable distributed applications to coordinate their operations into transactions. The OTS incorporates a three phase commit protocol to ensure atomicity of the transactions. This is a non-blocking protocol that ensures that operational sites participating in a transaction come to a common outcome based on the local data even in the event of (non-catastrophic) site failures.
54|1||An assessment of systems and software engineering scholars and institutions (1995â1999)|This paper presents the findings of a five-year study of the top scholars and institutions in the Systems and Software Engineering field, as measured by the quantity of papers published in the journals of the field. The top scholar is Richard Lai of La Trobe University in Australia, and the top institution is Carnegie Mellon University and its Software Engineering Institute. The paper lists the top 15 scholars and institutions.
54|2|http://www.sciencedirect.com/science/journal/01641212/54/2|Software maintenance, Y2K and other software non-crises|
54|2||Quantifying the closeness between program components and features|One of the most important steps towards effective software maintenance of a large complicated system is to understand how program features are spread over the entire system and their interactions with the program components. However, we must first be able to represent an abstract feature in terms of some concrete program components. In this paper, we use an execution slice-based technique to identify the basic blocks which are used to implement a program feature. Three metrics are then defined, based on this identification, to determine quantitatively, the disparity between a program component and a feature, the concentration of a feature in a program component, and the dedication of a program component to a feature. The computations of these metrics are automated by incorporating them in a tool (ÏSuds), which makes the use of our metrics immediately applicable in real-life contexts. We demonstrate the effectiveness of our technique by experimenting with a reliability and performance evaluator. Results of our study suggest that these metrics can provide an indication of the closeness between a feature and a program component which is very useful for software programmers and maintainers to better understand the system at hand.
54|2||Decomposing legacy programs: a first step towards migrating to clientâserver platforms|A solution to the problem of salvaging the past investments in centralised, mainframe-oriented software development, while keeping competitive in the dynamic business world, consists of migrating legacy systems towards more modern environments, in particular client–server platforms. However, a migration process entails costs and risks that depend on the characteristics of both the architecture of the source system and the target client–server platform.
54|2||Software evolution: code delta and code churn|As software modules evolve over time so do the measurements associated with them. Measuring a software system just once might give some idea of where the system is, but it gives no insight into where it has been or where it is going. In order to evaluate how a system has changed over successive iterations, it is necessary to establish a baseline. This baseline provides the ability to compare different versions of the system to determine how its complexity has changed. Two measurements based on this idea, code delta and code churn, can be used to assess the amount of change in the complexity of the system across successive software builds. The concepts of code delta and code churn are illustrated by measuring a real, industrial sized software system.
54|2||The application of subjective estimates of effectiveness to controlling software inspections|One of the recently proposed tools for controlling software inspections is capture–recapture models. These are models that can be used to estimate the number of remaining defects in a software document after an inspection. Based on this information one can decide whether to reinspect a document to ensure that it is below a prespecified defect density threshold, and that the inspection process itself has attained a minimal level of effectiveness. This line of work has also recently been extended with other techniques, such as the detection profile method (DPM). In this paper, we investigate an alternative approach: the use of subjective estimates of effectiveness by the inspectors for making the reinspection decision. We performed a study with 30 professional software engineers and found that the median relative error of the engineers’ subjective estimates of defect content to be zero, and that the reinspection decision based on that estimate is consistently more correct than the default decision of never reinspecting. This means that subjective estimates provide a good basis for ensuring product quality and inspection process effectiveness during software inspections. Since a subjective estimation procedure can be easily integrated into existing inspection processes, it represents a good starting point for practitioners before introducing more objective decision making criteria by means of capture–recapture models or the defect detection profile method.
54|2||An experimental investigation of the impact of individual, program, and organizational characteristics on software maintenance effort|Resources allocated to software maintenance constitute a major portion of the total lifecycle cost of a system and can effect the ability of an organization to react to dynamic environments. A major component of software maintenance resources is analyst and programmer labor. This paper is an experimental evaluation of how the Human Information Processing (HIP) model can serve as a framework for examining the interaction of an individual's information processing capability and characteristics of the maintenance task. Independent variables investigated include program size, control flow complexity, variable name mnemonicity, time pressure, level of semantic knowledge and some of their interactions on maintenance effort. Data collection was done using the Program Maintenance Performance Testing System (PROMPTS) designed especially for the experiment. The results indicate that a HIP perspective on software maintenance may contribute to a decrease in maintenance cost and increase the responsiveness of maintenance to changing organizational needs.
54|2||Locality metrics and program physical structures|Years of programming experience has convinced us that the physical structure of a program, such as the locations of the program's components, their calls, and the depth of nested calls, is important in determining how effective and efficient the program can be debugged and maintained. This paper introduces a new class of physical metrics, known as locality metric, that measures the relative positions of components in a program listing and reveals useful attributes that may affect programmer productivity. The placement of the components can be determined by a simple algorithm that is of polynomial time complexity. The paper compares the performance of the algorithm with that of an exhaustive search approach and also reports various characteristics of the locality metric based on the collected statistical data. The performance shows the feasibility of the algorithm and closeness of its output to the optimal result found by the exhaustive approach.
54|2||Discussion|
54|3|http://www.sciencedirect.com/science/journal/01641212/54/3|Y2K, and believing in software practice|
54|3||Selecting reusable stand-alone routines: a proposal and a case study|Almost daily engineers write, mainly from scratch, ad hoc programs suitable to solve very specific problems. In order to provide assistance to engineers willing to develop their programs through white-box reuse, it is mandatory to offer them repositories of software components populated with stand-alone routines selected according to software metrics able to measure the code complexity. Our selection approach is inspired to that previously adopted by Caldiera and Basili in a well-known paper, indeed, we use the same classical metrics, namely: volume, cyclomatic complexity and regularity. In essence, this paper represents both a refinement and a replication of the experiment performed by Caldiera and Basili, being the domain of the software investigated and the structure of the software itself the two major differences. In order to carry out the experiment, it was necessary to adapt the extremes of the values of the metrics mentioned above to our reference context which is very different from that Caldiera and Basili referred to. The adaptation was not trivial because in the case of routines being part of software libraries it is not possible to proceed as they did. In this paper, a general strategy suitable to “calibrate” their original reusability model is given. To validate the proposal, the strategy is instantiated to the domain of mathematical software. By referring to the calibrated reusability model, we carried out a large scale empirical investigation in order to select candidate reusable stand-alone (Fortran) routines from two commercial libraries belonging to the mathematical domain totally adding up to 2500 routines. The second part of the paper reports about such an experiment.
54|3||Software retrieval by samples using concept analysis|Finding and retrieving software components is one of the tasks of the building-block approach to software reuse. One interesting property of code components unlike other types of software artifacts is that they can be executed. The execution-based retrieval process tends to be too long to be incorporated in practice and faces the problem of non-termination and very long execution time. This paper describes a software component retrieval method using sample input–output behavior of the components (but without actual execution) based on concept analysis. The retrieval uses samples chosen by the developers of the components (rather than generated randomly or provided by the users). Based on the validity relation between components and samples, a concept lattice is constructed for the library by applying formal concept analysis. The user retrieves components by selecting valid samples incrementally for a desired component from a dynamically created menu of samples available in the library. Our method avoids the problems associated with actual execution-based retrieval such as non-termination and very long execution time, and also improves the retrieval time. Our approach can be directly applied to other levels of software components than code components as long as the components can be described in terms of some input–output relation.
54|3||Modularized design for wrappers/monitors in data warehouse systems|To simplify the task of constructing wrapper/monitor for the information sources in data warehouse systems, we provide a modularized design method to re-use the code. By substituting some parts of wrapper modules, we can re-use the wrapper on a different information source. For each information source, we also develop a toolkit to generate a corresponding monitor. By the method, we can reduce much effort to code the monitor component. We also develop a method to map the object-relational schema into relational one. The mapping method helps us make an uniform interface between wrapper and an integrator.
54|3||A multi-granularity locking-based concurrency control in object-oriented database systems|In this paper, a concurrency control scheme to increase concurrency among methods in object-oriented database systems (OODBs) is presented. The author is concerned with all types of access to an object: instance access and class definition access. For instance access, the proposed work has the following characteristics. First, construction of commutativity relation among methods can be automated. Second, it provides more concurrency by taking attributes as locking granularity. Third, deadlocks due to lock escalation can be reduced. Finally, concurrency is increased further with the use of run-time information. For class definition access, the proposed work allows class definition access methods to run concurrently by taking fine granularity. The proposed work also allows more parallelism between class definition access methods and instance access methods. For the performance evaluation of the proposed scheme, a simulation model is constructed. Using this model, the proposed technique is then compared with the two existing techniques. The performance studies show that the proposed scheme is superior to existing works.
54|3||An efficient data structure for dynamic memory management|Advances in software engineering, such as graphical user interface and object-oriented programming, have caused applications to become more memory intensive. The growing popularity of these applications increases the importance of high-performance dynamic memory management. This paper introduces a new data structure intended to increase the efficiency of implementations of common dynamic memory allocation and deallocation algorithms, including First Fit. The data structure, called the Common List, establishes a relationship between the allocated and free list. This strategy increases significantly the deallocation performance. Simulation results based on the traces from publicly available software applications are presented.
54|3||Direct execution simulation of load balancing algorithms with real workload distribution|This paper describes the model and implementation of a distributed direct execution simulation study of load balancing algorithms for a workstation-based locally distributed system. A general simulation system for load balancing algorithms is constructed on a local area network of DEC workstations, which directly executes the codes of the load balancing algorithms but simulates the underlying network and system environment. Using the simulation system, simulations with real workload distribution are conducted. Traces of user workstation activity collected in a university department environment are used in the simulation runs. We describe the methods used for distributed direct execution simulation of load balancing algorithms. The design of our simulations of several load balancing algorithms is presented and the simulation results are discussed.
54|3||Using viewpoints to derive object-oriented frameworks: a case study in the web-based education domain|This paper is an experience report that illustrates the applicability of a viewpoint-based design method for the Web-based education (WBE) domain. The method is a new approach for domain analysis that generates an object-oriented framework from a set of concrete applications. These applications are defined as viewpoints, since they provide different perspectives of the framework domain. Various existent WBE environments have been used as viewpoints in our case study. The design method has been successfully applied for these viewpoints generating the ALADIN framework. The analysed WBE environments are presented through object-oriented diagrams. The implementation and use of ALADIN is discussed to validate the results of the case study.
54|3||A practical run-time technique for exploiting loop-level parallelism|In order to exploit potential parallelism of loops, in which the array access patterns cannot be analyzed at compile-time, parallelizing compilers must rely on the run-time dependence analysis techniques. The LRPD test is noted in this field for its applicability. It assumes full parallelism and executes speculatively, then examines the correctness of parallel execution after loop termination. We extended the concept and developed a practical run-time technique, called the speculative parallelization with new technology (SPNT) test, to further exploit loop-level parallelism. Two main characteristics make the SPNT test distinguished. The first is improving the success rate of speculative parallelization by eliminating all cross-iteration data dependences except the cross-processor flow dependences. The second is reducing the failure penalty by aborting the speculative parallel execution immediately once a cross-processor flow dependence is detected during the execution. Our experimental results on shared-memory parallel machines HP SPP2000 and ALR QUAD6 prove the high effectiveness of the SPNT test.
55|1|http://www.sciencedirect.com/science/journal/01641212/55/1|Talk About a Software Crisis â Not!|
55|1||Kendra: adaptive Internet system|This paper describes an audio delivery system model called Kendra. We focus on Kendra’s adaptive delivery processes and the experimentation carried out to analyse it. The system trades off quality of sound against periods of silence; therefore, adaptation changes the format of the audio data to save bandwidth when bandwidth becomes more scarce. Typically adaptive systems are composed of a monitoring component and a switching component. The Kendra monitoring component has predictive capabilities, which advise the switching component to adapt either to a better quality or to a more bandwidth friendly format when it detects lower bandwidth. The prediction element can vary the amount of data it uses in its prediction, trading-off accuracy with the system’s impedance on the performance. Such systems exhibit many other tradeoffs that determine how sensitive and/or optimistic the system is to its environment, and this is explored in our experimentation. We conclude our work by discussing these network conditions and tradeoffs and how they affect the performance of the system.
55|1||Dynamic adaptation of sharing granularity in dsm systems|The trade-off between false sharing elimination and aggregation in distributed shared memory (dsm) systems has a major effect on their performance. Some studies in this area show that fine grain access is advantageous, while others advocate the use of large coherency units. One way to resolve the trade-off is to dynamically adapt the granularity to the application memory access pattern. In this paper, we propose a novel technique for implementing multiple sharing granularities over page-based dsms. We present protocols for efficient switching between small and large sharing units during runtime. We show that applications may benefit from adapting the memory sharing to the memory access pattern, using both coarse grain sharing and fine grain sharing interchangeably in different stages of the computation. Our experiments show a substantial improvement in the performance using adapted granularity level over using a fixed granularity level.
55|1||Hierarchical loop scheduling for clustered NUMA machines|Loop scheduling is an important issue in the development of high performance multiprocessors. As modern multiprocessors have high and non-uniform memory access (NUMA) costs, the communication costs dominate the execution of parallel programs. Previous affinity algorithms perform better than dynamic algorithms under non-clustered NUMA multiprocessors, but they suffer heavy overheads when migrating work load under clustered NUMA machines. In this paper, we propose a new loop scheduling policy, hierarchical policy, to improve various affinity scheduling algorithms (AFSs) for clustered NUMA machines. We cyclically distribute the iteration chunks to clusters. When imbalance occurs, the migration of iterations is carried on hierarchically. We use hierarchical policy to improve AFS and modified AFS (MAFS), and we call them Hierarchical AFS (HAFS) and Hierarchical MAFS (HMAFS), respectively. AFS uses a deterministic assignment policy to assign repeated executions of loop iteration to the same processor. MAFS modifies the migration policy of AFS, and reduces the number of synchronization operations. We confirm our idea by running many applications under a clustered NUMA simulator. Our experimental result shows that hierarchical policy reduces the inter-cluster remote memory accesses, decreases the locks to the queues, and effectively balances the work load. We also show that HMAFS is the best choice among these algorithms in most cases.
55|1||A frame of reference for the performance evaluation of asynchronous, distributed decision-making algorithms|The discipline of distributed algorithms aims at distributing a computational task among multiple, concurrent processors to achieve higher throughput, efficiency, performance and other advantages. Although parallel execution time, concurrency, scalability, isoefficiency function, and speed-up have been proposed in the literature as performance metrics, the use of speed-up and its variation, scaled speed-up, dominate the literature. Speed-up is defined as the ratio of the execution time of the best known serial algorithm to the execution time of the parallel algorithm on a given number of concurrent processors. This paper analyzes critically the role of speed-up from the perspective of an emerging class of distributed algorithms, termed asynchronous, distributed, decision-making (ADDM) algorithms. ADDM algorithms constitute the underlying control of many real-world systems, wherein the constituent sub-components are geographically dispersed, they interact between themselves asynchronously, and are permitted autonomy in local decision-making. The term real-world implies that such systems are subject to computer control and that they relate to everyday life and are beneficial to the society in the large. Analysis reveals several key limitations of speed-up from the perspective of describing the performance of ADDM algorithms. First, by definition, speed-up utilizes the execution time of a centralized system as the basis for comparison and is, therefore, only a relative measure. The centralized implementation is obviously neither optimal nor the absolute best. Second, for many systems that are amenable to ADDM algorithms, speed-up may not constitute a relevant performance metric. This paper adopts Ferrari's definition of performance and presents a frame of reference for the performance evaluation of ADDM algorithms. While novel decision criterion may need to be developed for each ADDM algorithm, given the diversity inherent in systems, the frame of reference implies that the ideal or absolute standard, relative to the criterion, must either be (i) derived automatically as the absolute best measure, utilizing the intrinsic definition of the problem or (ii) may consist of perfect decisions, determined by transcending the physical limitations of time. The absolute ideal is independent of the underlying computing infrastructure and, although unrealizable in the real world, it may serve as the absolute standard for comparing the effectiveness of the state-of-the-art distributed systems. Finally, this paper illustrates the determination of the absolute standards, corresponding to the designer's choice of the performance criteria, for a select few ADDM algorithms.
55|1||Conflict free transaction scheduling using serialization graph for real-time databases|A best effort approach to data scheduling, such as optimistic concurrency control in real-time database systems (RTDBS), imposes a heavy burden on the systems by restarting conflicting transactions. The restarted transactions themselves may miss their deadlines and the resources consumed by them may be wasted. Hence it can be better to schedule transactions such that only conflict free transactions can be executed concurrently at one time. This study explores this approach by making use of serialization graph testing. A serialization graph is used to enforce the serializability of transactions. Only transactions without data conflicts with the executing transactions will be allocated CPU. Consequently, conflict free concurrency among executing transactions can be achieved. All resources including CPU, I/O and data objects will not be wasted on restarted transactions. Therefore, the system can sustain a higher workload. We also devise a real-time serialization graph that considers the timing constraints of transactions. By using our protocols, only a limited amount of transaction delay overhead is observed. However, experimental results confirm that the overall performance of our protocols is better than the real-time optimistic concurrency control (OCC) protocol that is reported as one of the best performing data scheduling approaches in RTDBS.
55|1||Changing class behaviors at run-time in MRP systems|This paper presents an architecture that can be used to develop and maintain class behaviors in object-oriented material requirements planning (MRP) systems at run-time. The architecture provides this capability through a user-interface, and does not require knowledge of any programming language. The architecture is based on the concept of software reuse, it utilizes a library of fine-grained pre-compiled objects to develop and maintain class behaviors. The architecture is a dynamic-object application builder implemented on top of an object-oriented run-time environment.
55|1||Distinguishing sharing types to minimize communication in software distributed shared memory systems|Using thread migration to redistribute threads to processors is a common scheme for minimizing communication needed to maintain data consistency in software distributed shared memory (DSM) systems. In order to minimize data-consistency communication, the number of shared pages is used to identify the pair of threads that will cause the most communication. This pair of threads is then co-located on the same node. Thread pairs sharing a given page can be classified into thee types, i.e., read/read (r/r), read/write (r/w) and write/write (w/w). Based on memory-consistency protocol, these three types of sharing generate distinct amounts of data-consistency communication. Ignoring this factor will mispredict the amount of communication caused by cross-node sharing and generate wrong decisions in thread migration. This paper presents a new policy called distinguishing of types sharing (DOTS) for DSM systems. The basic concept of this policy is to classify sharing among threads as r/r, r/w or w/w, each with a different weight, and then evaluate communication cost in terms of these weights. Experiments show that considering sharing types is necessary for minimization of data-consistency communication in DSM. Using DOTS for thread mapping produces more communication reduction than considering only the number of shared pages.
55|1||Simulating multiple inheritance in Java|Multiple inheritance is a feature whose worth has been vigorously debated, and even now there is no agreement as to whether a language should include it. Java does not have multiple inheritance, but its designers claim that many of the benefits of multiple inheritance can be gained through the use of a new feature, the interface. In this paper we explore the claims of the Java designers, and show how the interface may be used to simulate multiple inheritance. We first discuss the benefits of inheritance and multiple inheritance, and then demonstrate how the need for them arises in program development, using an extended example based on a real application. We then present the technique for simulating multiple inheritance, showing how it allows the main benefits of multiple inheritance to be achieved in Java. Finally, we discuss the limitations of the approach. In particular, we show how the approach faces difficulties when used with class libraries, such as the Java Core API, and we suggest a convention for Java class library designers that will mitigate this problem.
55|2|http://www.sciencedirect.com/science/journal/01641212/55/2|A good-bye of sorts|
55|2||Heuristic search revisited|In this paper, we present general models for estimating time and space requirements of heuristic search algorithms. We also present an empirical study on a set of four well-known heuristic search algorithms. This study is useful in two ways. On one hand, it gives a general framework for comparing heuristic search algorithms from various respects including time, space, solution quality, and search effectiveness. On the other hand, it provides additional independent empirical results collected from different domains. These results can be used to strengthen other results obtained by other researchers in the area. Furthermore, the obtained results are machine independent in the sense that the CPU time is separated from the timing parameters. This enables us to give quantitative arguments on heuristic search algorithms for any architecture.
55|2||Data placement schemes in replicated mirrored disk systems|In the present paper, we study the most frequent data placement schemes, i.e., the organ-pipe and camel arrangements, in a mirrored disk system which supports cylinder replication at appropriate locations across the disk surface. Five schemes are proposed to identify the appropriate replica positions for the most frequently accessed cylinders. Three schemes, which are called Left, Right and Symmetric Replication Techniques, are based on an analytical approach. More specifically, given a single replica in the above strategies, estimates are derived for the expected seek distance decrease with respect to a non-replicated mirrored disk system. On the other hand, the remaining two schemes, which are called Positional and Frequency Replication Techniques, are based on heuristic approaches. We globally compare the performance of these five schemes as a function of the number of replicas. These results are also compared with the corresponding metrics of a conventional (with no replication) mirrored disk system.
55|2||A spatiotemporal database model and query language|Conventional spatial databases support efficiently a spatial management for objects. However, they manage only those that are valid at current time. Owing to this property, whenever a new value is inserted into databases, the old one should be deleted. So, it is very difficult for time varying spatial information to manage efficient historical information. To solve this problem, there is a rapid increase of interest for spatiotemporal databases. Spatiotemporal databases support historical information as well as spatial management at the same time, so that they can be used in various application areas such as geographic information system, urban plan system, and car navigation system, and so on. In this paper, we suggest a spatiotemporal data model that supports a bitemporal concept for spatial objects, and design a spatiotemporal database query language, entitled as STQL as well. When it is compared with the results of previous researches, it is the first spatiotemporal query language that supports temporal concept and spatial expression as well.
55|2||An open and safe nested transaction model: concurrency and recovery|In this paper, we present an open and safe nested transaction model. We discuss the concurrency control and recovery algorithms for our model. Our nested transaction model uses the notion of a recovery point subtransaction in the nested transaction tree. It incorporates a prewrite operation before each write operation to increase the potential concurrency. Our transaction model is termed “open and safe” as prewrites allow early reads (before writes are performed on disk) without cascading aborts. The systems restart and buffer management operations are also modeled as nested transactions to exploit possible concurrency during restart. The concurrency control algorithm proposed for database operations is also used to control concurrent recovery operations. We have given a snapshot of complete transaction processing, data structures involved and, building the restart state in case of crash recovery.
55|2||Newsmonger: a technique to improve the performance of atomic broadcast protocols|A new technique, called the newsmonger technique, for atomic broadcast protocols has been proposed. This technique can be incorporated in a large number of existing atomic broadcast protocols and it results in improving their performance. An extensive experimental evaluation of this technique is provided by incorporating it in sequencer-based and token-based atomic broadcast protocols. This evaluation has been done under several different operating conditions created by varying five system parameters: group size, mean interarrival time between update arrivals, communication failure probability, maximum silence period, and update arrival pattern. The evaluation shows that this techniques can decrease the average stability time of an atomic broadcast protocol by as much as 80%, without significantly affecting any other performance indices.
55|2||List ranking on processor arrays|List ranking finds for each cell in a linked list the number of cells that precede it in the list. This paper presents a work-efficient list-ranking algorithm for fine-grained processor arrays. This algorithm runs on an array of n/log2n processors with the expected run-time of O(log2n). As list ranking is highly communication intensive, the proposed algorithm is able to reduce communication cost among processors by assigning sublists, instead of arbitrary cells, of a linked list to each processor. The proposed algorithm is also capable of keeping all processors busy during the whole list-ranking process in order to utilize all processors efficiently.
55|2||Automated translation of JSD into CSP â a case study in methods integration|A software tool has been developed for translating the notation of the Jackson system development (JSD) method into the formalism of communicating sequential processes (CSP). The paper illustrates the translation using a small example and then introduces a systematic approach to automating the translation. The automation involves a formalized version of the JSD notation, called FJSD, and the definition of a translation function. The applicability and limitations of this approach to automated translation are considered in the wider context of methods integration.
55|2||Enhancing online catalog searches with an electronic referencer|Studies have shown that users have difficulty in using the many different search systems of library catalogs. This is because users do not possess the knowledge and skills that librarians have for performing searches on the catalogs. With the growth of the Internet, more libraries are offering their services online including the information in the catalog systems. In the library premises, users can seek the help of Librarians in carrying out their searches on the catalogs. However, such assistance is usually not available over the Internet for practical reasons. With an electronic referencer playing the role of a Librarian, users can get similar assistance and conduct more effective searches of the catalogs over the Internet. This paper describes the design and implementation of an electronic referencer known as the E-Referencer. It is a web-based interface developed to help users achieve more effective searches in online catalogs. It does so by capturing the expert knowledge of librarians (in the form of search strategies) into its knowledge base, and selecting and using them in helping users retrieve relevant records. The E-Referencer system is implemented using the JESS expert system shell and uses the Z39.50 protocol to access library databases on the Internet. The results of a test conducted to study the effectiveness of the system using a small sample of twelve queries are presented.
55|3|http://www.sciencedirect.com/science/journal/01641212/55/3|A simple micro-payment scheme|The micro-payment system is an important technique in electronic commerce. The high-value payment system is not suitable for a micro-payment system because the requirements are different. To meet sufficient security for all participants in electronic commerce, a micro-payment system makes it possible to make small payment through electronic communication networks. In this paper, we propose a simple and secure micro-payment system which can be used for purchasing information goods on the network. The new micro-payment system is developed using a tamper-resistant device (i.e., smart card), an efficient Message Authentication Code (MAC) technique, and the concept of overall network security. In our proposed scheme, we achieve the authentication of the legal users, protection of the integrity of transaction messages, and prevention of duplicate spending. This new micro-payment system is a simple, efficient and economical system.
55|3||A timed workflow process model|
55|3||Handling signature purposes in workflow systems|In paper-based workflow systems, signatures of individuals or groups of people have been used extensively for different purposes. Currently there are numerous studies on computerizing workflow systems. Also there are studies on implementing digital signatures in electronic media. But the diversified purposes of a signature in workflow makes a straightforward implementation of digital signature schemes inadequate. There are few studies on the implication of different signature purposes on electronic workflow systems. The purposes of signature are closely associated with the two modes of decision making, namely single and group. These two modes of decision making in turn lead to single and group signatures. This paper reports our comprehensive studies on signature purposes. These include the analysis of common signature purposes in workflow, classification of these purposes, classification of modes of decision making associated with these signature purposes, signing and validation requirements for handling these signature purposes, and finally an architecture to be incorporated in workflow engines for handling these signature purposes. In summary, this paper addresses a commonly neglected problem in information systems research: management control by signature in workflow.
55|3||Digital watermarking models for resolving rightful ownership and authenticating legitimate customer|Tampering resistance of a digital watermark scheme has recently drawn much attention of many researchers after various techniques against image manipulations have been presented and proved robust. In this paper, two digital watermark models for resolving rightful ownership are presented; the second one also authenticates the legitimate customer – the user such as a webmaster that obtains an image from an Internet site and wants to use it on his/her site. A scheme conformed to Model-II is presented with simulation results that demonstrate its robustness against JPEG compression, mean and median filtering, noise, cropping, and scaling besides tampering. A comparison to Scheme Lin-O also demonstrates that this proposed scheme is just as robust against image manipulations while requiring no original for watermark detection.
55|3||Anomalous intrusion detection system for hostile Java applets|An intrusion detection system (IDS) aims to increase the security of a computer system by dynamically monitoring various features and parameters of the system so as to be able to detect intrusions at the earliest possible. IDS's have been developed for privileged UNIX programs like sendmail, lpr, and login. The IDS that we have built is for applets. It serves as a protection against malicious applets and warns the user when such applets are downloaded. Our system monitors applets using system call traces from the Java runtime environment. Feature vectors created from the system call traces are used to train a machine learning algorithm. The rule-set produced by the algorithm can then be used to distinguish hostile applets from good applets.
55|3||A modified remote login authentication scheme based on geometric approach|In 1995, Wu proposed an efficient smart card-oriented remote login authentication scheme. The scheme allows a user to freely choose his password, and no verification tables are required. Hwang recently showed the insecurity in Wu's scheme; however, he did not propose his improvement. In this article, authors show a different approach to break the scheme, and propose their improvement. The modified scheme can withstand all possible attacks.
55|3||Optimizing storage utilization in R-tree dynamic index structure for spatial databases|Spatial databases have been increasingly and widely used in recent years. The R-tree proposed by Guttman is probably the most popular dynamic index structure for efficiently retrieving objects from a spatial database according to their spatial locations. However, experiments show that only about 70% storage utilization can be achieved in Guttman's R-tree and its variants. In this paper, we propose a compact R-tree structure which can achieve almost 100% storage utilization. Our experiments also show that the search performance of compact R-trees is very competitive as compared to Guttman's R-trees. In addition, the overhead cost of building a compact R-tree is much lower than that of a Guttman's R-tree because the frequency of node splitting is reduced significantly.
55|3||Comparing case-based reasoning classifiers for predicting high risk software components|Case-based reasoning (CBR) has been proposed for predicting the risk class of software components. Risky components can be defined as those that are fault-prone, or those that require a large amount of effort to maintain. Thus far evaluative studies of CBR classifiers have been promising, showing that their predictive performance is as good as or better than other types of classifiers. However, a CBR classifier can be instantiated in different ways by varying its parameters, and it is not clear which combination of parameters provides the best performance. In this paper we evaluate the performance of a CBR classifier with different parameters, namely: (a) different distance measures, (b) different standardization techniques, (c) use or non-use of weights, and (d) the number of nearest neighbors to use for the prediction. In total, we compared 30 different CBR classifiers. The study was conducted with a data set from a large real-time system, and the objective was to predict the fault-proneness of its components. Our results indicate that there is no difference in prediction performance when using any combination of parameters. Based on these results, we recommend using a simple CBR classifier with Euclidean distance, z-score standardization, no weighting scheme, and selecting the single nearest neighbor for prediction. The advantage of such a classifier is its intuitive appeal to nonspecialists, and the fact that it performs as well as more complex classifiers.
55|3||Delegated multisignature scheme with document decomposition|Consider the case that a large document including different subjects can be decomposed into a disjoint set of subdocuments and is to be delegated to a group of signatories with different knowledge domains for signing. Based on the intractability of the discrete logarithm problem, the authors propose a novel multisignature scheme, namely the delegated multisignature scheme in which each qualified signatory only signs a subset of the set of subdocuments according to the justification that his/her knowledge domain completely covers these delegated subdocuments' subjects. Meanwhile, any verifier can verify the multisignature of the document with only knowing the group's public key, i.e., the multisignature verification key, not necessarily to know the results regarding document decomposition and subdocument delegation. The proposed scheme has the merit that the size of the subdocuments delegated to the qualified signatories can be further reduced and as “balanced” as possible. Besides, the proposed scheme is more realistic to practical aspects for multisignature construction and requires smaller communication bandwidth as compared to conventional multisignature schemes.
56|1|http://www.sciencedirect.com/science/journal/01641212/56/1|A distributed EDI model|Electronic commerce (EC) has marked a new era for contemporary business. Although electronic data interchange (EDI) is a major component of EC, it is not widely adopted mainly due to high entrance cost and the lack of interoperability among information systems. To alleviate these problems, this paper proposes a distributed model so that EDI transactions can be directly generated from or transformed to either local or remote databases without going through expensive mapping and translation procedures. The most superior outcome of this model is that it combines the advantages of both application-to-application and database-to-database approaches and thus results in an integrated EDI model independent of databases structures, network locations, development languages, etc.
56|1||SMART mobile agent facility|With ever growing use of Internet for electronic commerce and data mining type applications there seems to be a need for new network computing paradigms that can overcome the barriers posed by network congestion and unreliability. Mobile agent programming is a paradigm that enables the programs to move from one host to another, do the processing locally and return results asynchronously. In this paper, we present the design and development of a mobile agent system that will provide a platform for developing mobile applications that are Mobile Agent Facility (MAF) specification compliant. We start by exploring mobile agent technology and establish its merits with respect to the client–server technology. Next, we introduce a concept called dynamic aggregation to improve the performance of mobile agent applications. We, then focus on the design and implementation issues of our system, Scalable, Mobile and Reliable Technology (SMART), which is based on the MAF specification.
56|1||A mechanism for view consistency in a data warehousing system|
56|1||Decentralized user group assignment in Windows NT|The notion of groups in Windows NT is much like that in other operating systems. Rather than set user and file rights individually for each and every user, the administrator can give rights to various groups, then place users within those groups. Each user within a group inherits the rights associated with that group. In this paper, we describe an experiment to extend the Windows NT group mechanism in two significant ways that are useful in managing group-based access control in large-scale systems. The goal of our experiment is to demonstrate how group hierarchies (where groups include other groups) and decentralized user-group assignment (where administrators are selectively delegated authority to assign certain users to certain groups) can be implemented by means of Microsoft remote procedure call (RPC) programs. In both respects the experimental goal is to implement previously published models (RBAC96 for group hierarchies and URA97 for decentralized user-group assignment). Our results indicate that Windows NT has adequate flexibility to accommodate sophisticated access control models to some extent.
56|1||Effectiveness of the FDDI-M protocol in supporting synchronous traffic|Timed token networks such as fiber distributed data interface (FDDI) networks have been widely deployed to support synchronous traffic. However the medium access control (MAC) protocol of FDDI allows transmission of synchronous messages up to at most one-half of the total bandwidth of the network. Shin and Zheng have proposed a modification to the FDDI MAC protocol, called FDDI-M, which can double a ring's ability in supporting synchronous traffic (K.G. Shin, G. Zheng, IEEE Transactions on Parallel and Distributed Systems 6 (1995) 1125–1131). It is widely known that the ability of timed token protocols such as FDDI to guarantee synchronous message deadlines is very dependent on the synchronous bandwidth allocation (SBA) schemes used, but the original paper does not address this issue. In this paper, we will compare the ability of FDDI-M to support synchronous traffic under different SBA schemes with that of FDDI. We use a new taxonomy of SBA schemes based on the strategy used to partition the synchronous bandwidth, and present an analytical study of the timing properties of the FDDI-M protocol using the worst case achievable utilization (WCAU) as the performance metric. The results show that while FDDI-M improves the WCAU values under one class of SBA schemes, its performance under the other category of SBA schemes is mixed. We also perform extensive simulation to study performance of FDDI-M for MPEG video traffic, and conclude FDDI-M does outperform FDDI significantly at heavy load. The effect of SBA schemes under overload conditions is also shown to be relatively minor, with the local SBA schemes actually performing better than the global schemes.
56|1||The prediction of faulty classes using object-oriented design metrics|Contemporary evidence suggests that most field faults in software applications are found in a small percentage of the software's components. This means that if these faulty software components can be detected early in the development project's life cycle, mitigating actions can be taken, such as a redesign. For object-oriented applications, prediction models using design metrics can be used to identify faulty classes early on. In this paper we report on a study that used object-oriented design metrics to construct such prediction models. The study used data collected from one version of a commercial Java application for constructing a prediction model. The model was then validated on a subsequent release of the same application. Our results indicate that the prediction model has a high accuracy. Furthermore, we found that an export coupling (EC) metric had the strongest association with fault-proneness, indicating a structural feature that may be symptomatic of a class with a high probability of latent faults.
56|1||Interfacing MATLAB with a parallel virtual processor for matrix algorithms|This paper describes the results of a project to interface MATLAB with a parallel virtual processor (PVP) that allows execution of matrix operations in MATLAB on a set of computers connected by a network. The software, a connection-oriented BSD socket-based client–server model, automatically partitions a MATLAB problem and delegates work to server processes running on separate remote machines. Experimental data on the matrix multiply operation shows that the speed improvement of the parallel implementation over the single-processor MATLAB algorithm depends on the size of the matrices, the number of processes, the speed of the processors, and the speed of the network connection. In particular, the advantage of doing matrix multiply in parallel increases as the size of the matrices increase. A speedup of 2.95 times was achieved in multiplying 2048 by 2048 square matrices using 15 workstations. The algorithm was also implemented on a network of four PC's, which was up to 2.5 times as fast as four workstations. The study also showed that there is an optimal number of processes for a particular problem, and so using more processes is not always faster.
56|1||A dynamic simulator of software processes to test process assumptions|Validation testing of software processes may provide both qualitative and quantitative suggestions to understand the ways to change the software process to improve its quality, or the ways to achieve specific organisation's needs. In many cases, however, such understanding has to be done without affecting the actual environment. To this purpose, this paper introduces the use of process simulators for validation testing. To be effective, however, simulators must combine the ability to sustain the complexity of the modelling problem with the so-called dynamic estimation capability, that is the capability of representing the dynamics of the simulated process. To achieve such objectives, the introduced simulator is based on the association of three conventional modelling methods (analytical, continuous and discrete-event) into a unique hybrid multi-level new model, called dynamic capability model (DCM). The paper applies DCM in the context of a waterfall-based software process to study the effects of three different quality assurance management policies on given process quality attributes, as effort, delivery time, productivity, rework percentage, and product quality. The verification of the simulator representativeness is also performed by reproducing empirically known facts in the process behaviour.
56|1||Notable design patterns for domain-specific languages|The realisation of domain-specific languages (dsls) differs in fundamental ways from that of traditional programming languages. We describe eight recurring patterns that we have identified as being used for dsl design and implementation. Existing languages can be extended, restricted, partially used, or become hosts for dsls. Simple dsls can be implemented by lexical processing. In addition, dsls can be used to create front-ends to existing systems or to express complicated data structures. Finally, dsls can be combined using process pipelines. The patterns described form a pattern language that can be used as a building block for a systematic view of the software development process involving dsls.
56|1||Dot-coms' coma|
56|2|http://www.sciencedirect.com/science/journal/01641212/56/2|Contents|
56|2||Employing multiple views to separate large-scale software systems|In order to achieve a well-separated system structure including hiding of information on a need-to-know basis it is highly desirable to provide multiple views of an interface for different subjects. In this context subjects may be structural parts of the system such as classes, or, preferably, architectural entities like clusters, subsystems or application areas. Establishing a view of an interface means that a particular subject may only use a selected subset of it, if at all. In this article several approaches cited in the literature are discussed in the light of the demands made by large-scale commercially installed software systems. It is shown that these approaches in large systems either lead to a considerable performance degradation or to a software structure which is somewhat difficult to survey and which does not possess satisfactory extensibility.
56|2||A methodology for building content-oriented hypermedia systems|Recently there have been many efforts to develop hypermedia systems. Hypermedia applications can be categorized into process-oriented and content-oriented. However, it is not a trivial task to build content-oriented hypermedia in a systematic fashion. Content-oriented hypermedia systems are developed primarily for the applications of marketing or information services. This paper proposes a methodology for building a content-oriented hypermedia system. Hypermedia contents are typically structured in the form of hierarchies. An index node can be used to gain access to this hierarchical structure. The proposed methodology adopts three types of nodes (content, index and function) and two navigation mechanisms (simple and operational). Furthermore, various design techniques are proposed for maintaining content-oriented hypermedia systems that have an evolutionary feature. To demonstrate its practical usefulness, the methodology is applied to developing a hypermedia system.
56|2||Synchronization and flow adaptation schemes for reliable multiple-stream transmission in multimedia presentations|In the practical networking environments, an orchestration of distributed multimedia presentation (DMP) with multiple media streams, e.g., WWW multimedia presentations over the Internet, is always disturbed due to the unpredictable end-to-end delay. Presentation adaptation associated with multimedia synchronization and flow control is a feasible solution to achieve a smooth multiple-stream DMP. In the paper, we propose the Pause-And-Run approach for k-stream (PARK) multimedia presentations over Internet to achieve reliable transmission of continuous media. Main characteristics of the PARK approach are as follows. (i) To achieve reliable transmission for continuous media, PARK adopts TCP instead of UDP. (ii) Since the slow-start scheme is embedded in TCP, a novel flow adaptation scheme is proposed to reduce the overhead of the network and end hosts. The server adapts its transmission rates to the buffer situation of the client and prevents the client's buffers from overflow and underflow as much as possible. (iii) With the provision of multiple-stream synchronization and the multi-level adaptation control, the client achieves smooth multimedia presentations and achieves graceful presentation degradation when the resources are insufficient. This paper shows the evaluation of applying the PARK approach over Internet. The evaluation results reveal a suitable buffering control policy for the audio and video media, respectively.
56|2||Spatial database with each picture self-contained multiscape and access control in a hierarchy|The applications of spatial database are more and more popular in the computer and network environments. It is the reason why information security is an important issue now in our world. The purpose of this paper is to integrate the spatial database processing technique and the access control in a hierarchy technique to enhance the security of spatial database. Combining the concepts of traditional cryptography and steganography, a new spatial database cryptosystem with double security mechanisms is proposed.
56|2||A fast content-based indexing and retrieval technique by the shape information in large image database|In this paper, we present an efficient content-based image retrieval (CBIR) system which employs the shape information of images to facilitate the retrieval process. For efficient feature extraction, we extract the shape feature of images automatically using edge detection and wavelet transform which is widely used in digital signal processing and image compression. To facilitate speedy retrieval, we also propose the Spherical Pyramid-Technique (SPY-TEC), a new indexing method for similarity search in high-dimensional data space. The SPY-TEC is based on a special space partitioning strategy, which is to divide the d-dimensional data space first into 2d spherical pyramids, and then cut the single spherical pyramid into several spherical slices. This partition provides a transformation of d-dimensional space into 1-dimensional space. Thus, we are able to use a B+-tree to manage the transformed 1-dimensional data. We show that our image indexing method supports faster retrieval than other multi-dimensional indexing methods such as the R*-tree through various experiments. Finally, we show the retrieval effectiveness of our CBIR system using the measure proposed by the QBIC (C. Faloutsos et al., 1994) system.
56|2||Database management systems: design considerations and attribute facilities|Database management systems (DBMSs) form the foundation of most information systems. This study asked database developers and administrators to identify the features that are most important to them. The study finds that the features desired by respondents relate to query performance – particularly on complex queries. Respondents are also concerned about data integrity controls – particularly support for triggers, deadlock resolution, and control over data transaction logs. The study also provides insight into how respondents use the DBMS. Transaction processing and application development were primary factors. Distributed databases, administration, and general features were less important.
56|2||Seeking consonance in information systems|Information system projects are notorious for their failure rate. We propose that much failure is due to a difference in expectations prior to the start of a new system development. Much of the difference in expectations may be in the use of metrics not fully understood by every stakeholder in a new system. Current theory and management practice suggests a better focus on building an understanding of the critical evaluators to develop a common understanding of expectations will improve success rates. Such activity requires broader viewpoints of success and the input of more stakeholders well before any project tasks are conducted. Four studies are summarized that highlight the importance of reaching a priori mutual agreement on the metrics and targets.
56|3|http://www.sciencedirect.com/science/journal/01641212/56/3|Job scheduling in heterogeneous distributed systems|This paper investigates scheduling policies in a heterogeneous distributed system, where half of the total processors have double the speed of the others. Processor performance is examined and compared under a variety of workloads. Two job classes are considered. Programs of the first class are dedicated to fast processors, while second class programs are generic in the sense that they can be allocated to any processor. It was our intention to find a policy that increases overall system throughput by increasing the throughput of the generic jobs without seriously degrading performance of the dedicated jobs. However, simulation results indicate that each scheduling policy considered has its merits and the best performer tended to depend on the degree of multiprogramming.
56|3||An optimal scheduling algorithm for minimizing the computing period of cyclic synchronous tasks on multiprocessors|We present an efficient optimal algorithm that schedules cyclic synchronous tasks into multiprocessors to minimize the computing period of iterative execution. Due to the rapid development of higher speed microprocessors and digital signal processors (DSPs), small-scale parallel embedded systems and on-chip parallel processors with a simple network structure became feasible for applications such as large-scale simulations and computation-intensive plant control systems that were previously executed by massively parallel computers. We consider cyclic synchronous tasks with communication overhead, which run on multiprocessors with a fully connected network. We suggest the computing period as the performance measure to maximize overall computation speed and the individual start policy that allows overlapping different iterations. The concepts and characteristics of the local period and the global period are also introduced. To solve the complex optimal scheduling problem in an efficient way, our algorithm uses a new spatial scheduling technique using the scheduling space which represents all possible start-time schedules in a multi-dimensional space. By using spatial searching and an enhanced branch-and-bound technique, the optimal schedule which minimizes the computing period can be found. The scheduling results for power plant simulation verify the practicality of our algorithm.
56|3||Autonomous agents for coordinated distributed parameterized heuristic routing in large dynamic communication networks|Parameterized heuristics offers an elegant and powerful theoretical framework for design and analysis of autonomous adaptive traffic management agents in communication networks. Routing of messages in such networks presents a real-time instance of a multi-criterion optimization problem in a dynamic and uncertain environment. This paper describes the analysis of the properties of heuristic routing agents through a simulation study within a large network with grid topology. A formal analysis of the underlying principles is presented through the incremental design of a set of autonomous agents that realize heuristic decision functions that can be used to guide messages along a near-optimal (e.g., minimum delay) path in a large network. This paper carefully derives the properties of such heuristics under a set of simplifying assumptions about the network topology and load dynamics and identify the conditions under which they are guaranteed to route messages along an optimal path, so as to avoid hotspots in the load landscape of the network. The paper concludes with a discussion of the relevance of the theoretical results to the design of intelligent autonomous adaptive communication networks and an outline of some directions of future research.
56|3||Proteus: an efficient runtime reconfigurable distributed shared memory system|This paper describes Proteus, a distributed shared memory (DSM) system which supports runtime node reconfiguration. Proteus allows users to change the node set during the execution of a DSM program. The capability of node addition allows users to further shorten the execution time of their DSM programs by dynamically adding newly available nodes to the system. Furthermore, competition for resources between system users and computer owners can be avoided by dynamically deleting nodes from the system. To make the system adapt to the node configuration efficiently, Proteus employs several techniques, including adaptive workload redistribution, affinity page movement, and forced update. Proteus supports both sequential consistency and release consistency. It provides an object-oriented parallel programming environment. This paper describes the design and implementation of node reconfiguration in Proteus, and presents the performance of the system. Experimental results indicate that Proteus can further improve the performance of the tested programs by taking advantage of node reconfiguration. Our results further demonstrate that the techniques employed in Proteus minimize communication and overhead.
56|3||Supporting parallel computing on a distributed object architecture|The availability of high-speed networks and increasingly powerful commodity microprocessors is making the usage of clusters, or networks, of computers an appealing platform for cost effective parallel computing. However, the ease of developing efficient high-performance parallel software to exploit these platforms presents a major challenge. Advances in distributed object software technology have made the management of distributed computing resources easier than before. This also brings many benefits for parallel computing. Firstly, distributed object technology facilitates the encapsulating of parallel computing resources into a uniform model despite their differences in implementations that are based on different languages executing on different platforms. Secondly, mature object-oriented analysis, design method, as well as component idea embodied in distributed object technology can enhance the reusability of parallel software. To support parallel computing in a distributed object-based computing platform, a uniform high performance distributed object architecture layer is necessary. In this paper, we propose a distributed object-based framework called DoHPC to support parallel computing on distributed object architectures. We present the use of dependence analysis technique to exploit intra-object parallelism and an interoperability model for supporting distributed parallel objects. Experimental results on a Fujitsu AP3000 workstation cluster consisting of a cluster of 32 UltraSPARC workstations show that the implementation of inter-object parallelism on a workstation cluster environment is efficient. With intra-object parallel computation speedup efficiency is greater than 90% and with overhead of less than 10% for large problem, and the interoperability model improves speedup by 20%.
56|3||ODCHP: a new effective mechanism to maximize parallelism of nested loops with non-uniform dependences|There are many methods for nested loop partitioning. However, most of them perform poorly when partitioning loops with non-uniform dependences. This paper proposes a generalized and optimized loop partitioning mechanism to exploit parallelism from nested loops with non-uniform dependences. Our approach, based on dependence convex theory, will divide the loop into variable size partitions. Furthermore, the proposed algorithm partitions a nested loop by using the copy-renaming and the optimized partitioning techniques to minimize the number of parallel regions of the iteration space. Consequently, it outperforms the previous partitioning mechanisms of nested loops with non-uniform dependences. Many optimization techniques are used to reduce the complexity of the algorithm. Compared with other popular techniques, our scheme shows a dramatic improvement in the preliminary performance results.
57|1|http://www.sciencedirect.com/science/journal/01641212/57/1|A note on the evolution of software engineering practices|
57|1||Management of process improvement by prescription|This paper examines the efficiency and effectiveness of a prescriptive systems development methodology in practice. The UK Government's mandatory structured systems analysis and design method (SSADM) was examined to determine its value to software projects. The evidence was collected from interviews with 17 project managers, discussions with participants on three large SSADM projects and from observing 90 end users in training. The generic conclusions are that prescriptive information systems methodologies are unlikely to cope well with strategic uncertainty, user communication or staff development. The recommendations are to focus more on soft organizational issues and to use approaches tailored to each project.
57|1||Experimental comparison of coarse-grained concepts in UML, OML, and TOS|Object-oriented approaches offer coarse-grained modeling concepts to structure complex application systems consisting of hundreds or even thousands of classes. This paper analyses such concepts by using a software engineering experiment. Two factors are controlled: Factor A consists of three object-oriented approaches (unified modeling language (UML), OPEN modeling language (OML), and taxonomic objectsystem (TOS)), and factor B consists of two application systems (a database-oriented application and a process-oriented application). The results show that the coarse-grained concepts of the object-oriented approaches OML and TOS are superior to those of UML when modeling a database-oriented application. From the results it can be concluded that the coarse-grained concepts of UML should be improved in further releases. The data were analysed by using advanced statistical methods.
57|1||A simple process for migrating server applications to SMP:s|
57|1||Incentive compatibility and systematic software reuse|
57|1||A simulation study on coordination strategies: decision cycle-time perspective|
57|1||Exploratory analysis of environmental factors for enhancing the software reliability assessment|Today software development is no longer an isolated work of a single programmer. Large systems are usually developed in a multi-language environment and run simultaneously on various platforms. Software development is a very complex process involving various factors. In this paper 32 environmental factors are defined and a survey was launched to investigate the impact of these factors on software reliability assessment. We extend our study by combining the original factors to reduce the dimension of the factor space; examine the impact of the environmental factors on the software reliability assessment improvement; investigate whether people have different opinions on ranking the environmental factors depending on their years of experience; and test the association between software reliability assessment improvement and participant's background information. Statistical analysis methodologies including factor analysis, linear regression, nested design, and chi-square test are utilized in this study.
57|1||Empirical comparison of regression test selection algorithms|In the maintenance phase, the regression test selection problem refers to selecting test cases from the initial suite of test cases used in the development phase. In this paper, we empirically compare five representative regression test selection algorithms, which include: Simulated Annealing, Reduction, Slicing, Dataflow, and Firewall algorithms. The comparison is based on eight quantitative and qualitative criteria. These criteria are: number of selected test cases, execution time, precision, inclusiveness, preprocessing requirements, type of maintenance, level of testing, and type of approach. The empirical results show that the five algorithms can be used for different requirements of regression testing. For example the Simulated Annealing algorithm can be used for emergency non-safety-critical maintenance situations with a large number of small modifications.
57|2|http://www.sciencedirect.com/science/journal/01641212/57/2|Contents|
57|2||A hybrid approach to OO development: the SUMMITrak project at TCI|Industry evidence suggests that many purebred object-oriented (OO) software development projects in the industry are failing to meet expectations and are being abandoned in favor of a “hybrid” approach, where OO tools, technologies, and methodologies are combined with those from the traditional structured system development paradigm. The value of this hybrid approach lies in that it can help organizations leverage their existing traditional system development resources (staff, skills, tools, and project management), while taking advantage of the architectural superiority of OO tools and techniques for timely and cost-effective implementation of large, complex projects. This paper describes the SUMMITrak project at TCI, where such a hybrid approach was deployed successfully to build an enterprise-scale billing system for the cable industry. Our study provides a “proof of concept” that a hybrid approach is not only feasible, but in fact, may be desirable for many enterprise-scale projects.
57|2||An industrial study of reuse, quality, and productivity|The relationship between amount of reuse, quality, and productivity was studied using four sets of C and C++ modules collected from industrial organizations. The data domains are: text retrieval, user interface, distributed repository, medical records. Reuse in this paper is ad hoc, black box, compositional code reuse. The data generally show that more reuse results in higher quality, but are ambiguous regarding the relationship between amount of reuse and productivity.
57|2||A study of the allocation behavior of C++ programs|The object-oriented programming (OOP) language systems tend to perform object creation and deletion prolifically. An empirical study has shown that C++ programs can have 10 times more memory allocation and deallocation than comparable C programs. However, the allocation behavior of C++ programs is rarely reported. This paper attempts to locate where the dynamic memory allocations are coming from and report an empirical study of the allocation behavior of C++ programs. Firstly, this paper summarizes the hypothesis of situations that invoke the dynamic memory management explicitly and implicitly. They are: constructors, copy constructors, overloading assignment operator=, type conversions and application-specific member functions. Secondly, the development of a source code level tracing tool is reported as a procedure to investigate the hypothesis. Most of the five C++ programs traced are real-world applications. Thirdly, allocation patterns, object size and age distribution are summarized. Among other things, we found that objects tend to have a very short life-span, and most of them are created through constructors and copy constructors. With these findings, we may improve the performance of dynamic memory management through, a profile-based strategy or reusing objects.
57|2||Grounding the OML metamodel in ontology|The paper analyses and evaluates the OPEN Modelling Language (OML) in terms of the Bunge–Wand–Weber (BWW) model of information systems in order to: (1) Define the semantics of each relevant OML construct in terms of the kind of problem-domain phenomena they are intended to represent. (2) Inform further improvement of OML and similar object-oriented (OO) modelling languages by identifying OML-constructs which are ontologically overloaded, redundant or excessive and by identifying construct deficits in OML. (3) Investigate the ontological assumptions underlying OO modelling further. (4) Identify the multiple roles played by OML-constructs, some of which must simultaneously (a) support representation of the problem domain, (b) support developers and other stakeholders in establishing requirements and creating a software artifact, (c) support representation of that software artifact and (d) form a well-defined, compact and tightly integrated modelling language. (5) Provide general guidelines for defining OO and other modelling constructs in terms of the kind of problem-domain phenomena they are intended to represent.
57|2||Non-linear array data dependence test|Data dependence analysis is the most essential process while parallelizing a sequential program. Most current data dependence tests cannot handle array subscripts that are non-linear expressions. In this paper, we present a new parallelization algorithm, called non-linear array subscripts (NLA) test, to deal with non-linear or complex array subscripts. In this scheme, the iterations subject to loop-carried dependence are scheduled into different wavefronts, while the iterations with no loop-carried dependence are assigned into the same wavefront. Based on the wavefront information, the original loop is then transformed into parallel code. Our experimental results on shared-memory parallel machines HP SPP2000 and ALR Quad6 prove the high effectiveness of the NLA test.
57|2||An approach to modeling and evaluation of functional and timing specifications of real-time systems|Real-time systems need to be correct with respect to both functional and timing behaviors. Specification and modeling methods for real-time systems must thus permit the evaluation of functional and timing properties. Conventional real-time schedulability analyzers and simulators are not based on a formal specification and a model can thus not be formally verified. Formal specifications, on the other hand, frequently ignore preemptive scheduling and resource access protocols and the results obtained are thus only of limited value for systems using state-of-the-art scheduling algorithms. This paper proposes a novel Petri net-based approach for constructing specifications that are both formally verifiable and operational. It presents a solution to the preemption problem and suggests a pragmatic generic real-time system model which can be easily transformed into models for formal verification of functionality and for scheduling simulation. The well-known mine drainage system case study is used to demonstrate the application of this approach.
57|3|http://www.sciencedirect.com/science/journal/01641212/57/3|References architectures for enterprise integration|The dynamic and competitive enterprise environment requires enterprises to ensure the highest profit from their resources, integrating them to work together in obtaining the enterprises objectives. The project of design and implementation of an Integrating Enterprise System, is an extremely complex project that involves different technological, human and organisational elements. For this purpose several different reference architectures (RA) have been proposed. However, this area of research is not yet totally satisfactory because these methods may still be improved. It is necessary to adapt the different techniques to the concrete needs of each type of enterprise activity. In addition, new methods enabling the integration of several enterprises (called virtual enterprises) must be developed and their use must be popularized through examples and application experiences. This paper shows the results of the research project in RA for enterprise integration of the IRIS group from the University Jaume I of Castellón. Mainly, it is a framework consisting of a step by step methodology, reference models and a set of supporting tools, which will allow the creation of an Integrated Enterprise. Some of the results obtained from the applications in different enterprises are also shown.
57|3||A conceptual foundation for component-based software deployment|We use the term component-based software deployment (CBSD) to refer to the process of deploying a software application in a component-based format. In this paper, we propose a formal conceptual framework for CBSD. This framework allows us to articulate various strategies for deploying component-based software. In addition, the framework permits us to express conditions under which various forms of CBSD are both successful (the deployed application works) and safe (no existing applications are damaged).
57|3||Using UML-F to enhance framework development: a case study in the local search heuristics domain|Currently frameworks are most commonly represented through design diagrams written in standard object-oriented analysis and design languages. However, these design notations do not provide elements for identifying framework variation points and how their instantiation should be performed. Therefore, in order to create framework instances, users have to rely on extra documentation that is not always available. This paper shows the benefits of an extension to UML that explicitly represents all the information required for framework development and instantiation. The approach is illustrated through a large real-world framework for local search heuristics for combinatorial optimization problems.
57|3||Experience with identifying and characterizing problem-prone modules in telecommunication software systems|This paper compares several risk identification techniques and applies tree-based defect models (TBDMs) to analyze project measurement and defect data for several large telecommunication software systems from Nortel Networks. The modeling results are used to identify problem-prone modules and provide constructive information to guide quality improvement activities aimed at these modules. The results are also compared with previous results using similar analysis techniques for both legacy and new software systems to help us generalize our results. Based on these results, we recommend an integrated approach so that problematic areas can be identified and characterized for focused defect removal and quality improvement.
57|3||A technique for function block counting|A systematic approach to the estimation of software size early in the development cycle can pay valuable dividends for project planning and management. This paper reports the successful application of a method which combined function block (FB) counting with structured analysis, used linear regression analysis to convert FB counts to function points (FPs), and employed statistical measures of variation to create probability ranges for the estimates. A simple regression model provided significant predictive capability when the model parameters were tuned for the development domain and specific project.
57|3||How good is the critical factor concept in estimating the average number of character comparisons per item in string sorting?|In this paper, we present a new approach for determining the cost of sorting a string item, in terms of the average number character comparisons. The approach is based on the critical factor (CF) concept which refers to the first character at which strings Si and Sj disagree. The research assumptions of the study were tested using samples from Arabic and English. The results indicate that the technique described, in this paper, to estimate the time cost of string sorting provides a measure that seems to offer figures that are closer to the real average cost than what is offered by the measures suggested before. The results also show some differences between the sorting costs of Arabic and English string objects.
57|3||Communication cost of cognitive co-operation for distributed team development|Current team development approaches only focus on work co-operation and resource sharing. Cognitive co-operation has not been paid enough attention, while it becomes more important than ever when the development team is geographically distributed. This paper identifies five types of Internet-based communication approaches for cognitive co-operation among distributed team members: the email-based approach; the blackboard-centred approach; the pure flow-based approach; the combination between the flow-based approach and the email-based approach; and the combination between the flow-based approach and the blackboard-centred approach. The communication costs of these approaches are defined and compared based on a set of assumptions. These comparisons provide the development teams or the designers of team wares the evidence and the method for choosing a suitable cognitive co-operation approach.
57|3||Contents Volume 57|
58|1|http://www.sciencedirect.com/science/journal/01641212/58/1|Minimizing the mean delay of quorum-based mutual exclusion schemes|Achieving mutual exclusion is one of the most fundamental problems in distributed computing. The use of coteries is a well-known approach to this problem. A coterie is a special set of pair-wise intersecting node groups called quorums. The communication delay incurred in a quorum-based mutual exclusion scheme depends critically on the coterie adopted, and thus it is important to find a coterie with small delay. Recently, two related measures called max-delay and mean-delay have been introduced. The former measure represents the largest delay among all nodes, while the latter is the arithmetic mean of the delays. In a previous paper, we have proposed a polynomial-time algorithm to find max-delay optimal coteries, but there has been no algorithm to find mean-delay optimal coteries. In this paper, the first algorithm that finds mean-delay optimal coteries in general topology networks is proposed. This algorithm employs the branch-and-bound method to effectively explore the search space. Although its running time can be exponential, it is shown that the algorithm is applicable to moderate-sized networks through experiments.
58|1||Hiding communication overheads in dynamic load balancing for multicomputers|We introduce new techniques for hiding communication overheads involved in dynamic load balancing for hypercubes, meshes and trees. The basic idea is either coalescing some phases of balancing to overlap the transfer of load units on different links or dividing each phase into steps to pipeline the transfer of load unit by unit for maximum utilization of links. Previous schemes perform transfer in a sequential way, leaving many links idle in some phases. The first technique (method1 technique in the paper) collects information about transfers on all the links in all phases and determines which can be overlapped with which. We construct new phases such that all the transfer actions that can be overlapped are combined into a phase. In some cases (especially severely uneven load distribution) the method1 is not so effective since processors should wait for incoming units before sending out. To remedy this, we introduce a new technique (method2 technique in the paper) in which each phase is split into steps and one unit is transferred in a step. Since each processor does not have to wait for all of the units to be sent to its neighbor before sending out, the method1 technique enhances the utilization of links. Our techniques are shown via simulation to result in 15–65% reduction (hiding) to known techniques, depending on the initial load distribution.
58|1||Fault-tolerant gamma interconnection network without backtracking|A no-backtracking gamma interconnection network (NBGIN) is a modified gamma interconnection network (GIN) that can tolerate one switch or link fault without backward packet transmission. Basically, this can be achieved by providing two alternative paths at all intermediate nodes during the progression of routing through carefully designed algorithm. In this work, a destination tag routing named as NB function is designed. With this NB function, a packet can find an alternate path as soon as it encounters a switch or link fault in NBGIN. The major advantage of NBGIN over gamma-induced networks is that there is no rerouting penalty under a fault. In addition, NBGIN processes the characteristics of one-fault tolerance, destination tag routing, and dynamic rerouting. Furthermore, the routing and rerouting path can be simply determined by the destination tag only instead of being computed with the routing tag.
58|1||A learning database system to observe malfunctions and to support network planning|This paper presents a learning database system that can accommodate malfunction observations. Consequently, such observations may be expressed in structured patterns to support network planing which is one of the important network management functions. The underlying system monitors the network protocol tables in order to discover interesting patterns. To achieve this purpose two learning techniques are used. The first technique is empirical and it focuses on data samples by selecting specific fields and subsets of records using structured query language (SQL). Then data abstraction is carried out and interesting characteristics are extracted. The second technique exploits an explanation_based learning (EBL) procedure to obtain operational rules. In this case the domain (network) knowledge is formally expressed and only one training example is analyzed in terms of this knowledge. Thus, the system is capable of discovering various operational patterns, provide sensible advices, and support the network planning activity.
58|1||On the neural network approach in software reliability modeling|Previous studies have shown that the neural network approach can be applied to identify defect-prone modules and predict the cumulative number of observed software failures. In this study we examine the effectiveness of the neural network approach in handling dynamic software reliability data overall and present several new findings. Specifically, we find 
58|1||Editor's Corner|
58|1||Proportional sampling strategy: a compendium and some insights|There have been numerous studies on the effectiveness of partition and random testing. In particular, the proportional sampling (PS) strategy has been proved, under certain conditions, to be the only form of partition testing that outperforms random testing regardless of where the failure-causing inputs are. This paper provides an integrated synthesis and overview of our recent studies on the PS strategy and its related work. Through this synthesis, we offer a perspective that properly interprets the results obtained so far, and present some of the interesting issues involved and new insights obtained during the course of this research.
58|2|http://www.sciencedirect.com/science/journal/01641212/58/2|A new encryption algorithm for image cryptosystems|There are two major differences of the characteristics of the text data and image data. One difference is that the size of image data is usually much larger than that of text data. The other is that plain data rarely permit loss when a compression technique is used, but image data do. In this paper, we design an efficient cryptosystem for images. Our method is based on vector quantization, which is one of the popular image compression techniques. Our method can achieve the following two goals. One goal is to design a high security image cryptosystem. The other goal is to reduce computational complexity of the encryption and decryption algorithms.
58|2||Specification and analysis of n-way key recovery system by Extended Cryptographic Timed Petri Net|
58|2||A proxy-based security architecture for Internet applications in an extranet environment|Current Internet communications security is typically provided by the integration of secure transport functionality into client and server software. Two problems arise with this approach: Firstly, the use of integrated security services requires modification to the existing Internet applications, requiring re-development and re-deployment projects. Secondly, high-level security services such as authorisation are not provided by secure transport protocols, requiring applications to rely on customised (and often insecure) mechanisms for the provision of such services. We propose a platform-independent system that uses proxy applications to provide both secure transport and authorisation services transparently to existing Internet applications. We demonstrate that our approach requires no modification to existing applications, and that our security services are based on existing and widely used technologies. We discuss the merits of our architecture in the context of the intended deployment environment: an Internet-based heterogeneous private network such as an extranet or Virtual Private Network (VPN). We show that our approach achieves its goals at the expense of introducing a minor degree of performance loss into overall client–server communications, yet we maintain that this performance loss is a minor expense in relation to the advantages of the system as a whole.
58|2||New nonrepudiable threshold proxy signature scheme with known signers|Based on Kim et al.'s threshold proxy signature scheme, Sun proposed an nonrepudiable threshold proxy signature scheme with known signers. In Sun's scheme, actual proxy signers cannot deny the signatures they have signed. However, his scheme is vulnerable against the conspiracy attack. Any t malicious proxy signers can collusively derive the secret keys of other proxy signers and impersonate some other proxy signers to generate proxy signatures. In this paper, we proposed a new nonrepudiable threshold proxy signature scheme that overcomes the above weakness. Furthermore, the proposed scheme is more efficient than Sun's in terms of computational complexities and communication costs.
58|2||A dominance relation enhanced branch-and-bound task allocation|
58|2||Design of a scalable multiprocessor architecture and its simulation|
58|2||Understanding complex, real-world systems through asynchronous, distributed decision-making algorithms|Traditionally, the underlying decision-making algorithms for most real-world systems have been centralized. The term, real-world, refers to systems under computer control that relate to everyday life, are beneficial to the society in the large, and are generally large-scale in scope. Examples include AT&T's dynamic non-hierarchical routing (DNHR) for routing telephone calls, the North American advanced train control system (ATCS) for routing railways, the Swiss banking system (SIC), and inventory management algorithms. While centralized algorithms are simple, easy to conceive and implement, they execute sequentially on uniprocessors and are slow. In addition, by their very nature, centralized algorithms are highly susceptible to natural and artificial disasters. Synchronous distributed algorithms constitute a performance improvement over centralized algorithms, and have been used in fault simulation within the discipline of computer-aided design of digital systems and in matrix manipulations. However, their performance is limited due to frequent inherent synchronizations. This paper critically examines the nature of large-scale, real-world systems and observes that, fundamentally, most complex systems are composed of entities – concurrent, independent, and self-contained units of decision-making, that interact with each other, asynchronously. This paper presents a new class of algorithms – asynchronous, distributed, decision-making (ADDM) algorithms, to constitute the underlying control of such systems. While ADDM algorithms are closely related to autonomous decentralized systems (ADS) in the principal elements, their characteristics and boundaries are defined rigorously. While ADDM algorithms are difficult to conceive, design, and implement, they constitute the natural and logical choice for systems control, and hold the promise of extracting the maximal parallelism inherent in these systems. In addition, in principle, true asynchronous systems can be described accurately only by asynchronous, distributed algorithms, never by synchronous distributed algorithms. This paper reasons the nature of most complex real-world systems from first principles and reasons for its increasing importance in the design of future, large-scale, systems. It then presents the underlying principle of ADDM algorithms, details their fundamental characteristics, enumerates a number of successful ADDM algorithms for problems from different disciplines, and briefly reviews the nature of three of them – (1) real-time, domestic payments processing system, (2) distributed scheduling in railway networks, and (3) distributed routing in ATM networks.
58|2||Controversy Corner|
58|2||Making inconsistency respectable in software development|The development of software systems inevitably involves the detection and handling of inconsistencies. These inconsistencies can arise in system requirements, design specifications and, quite often, in the descriptions that form the final implemented software product. A large proportion of software engineering research has been devoted to consistency maintenance, or geared towards eradicating inconsistencies as soon as they are detected. Software practitioners, on the other hand, live with inconsistency as a matter of course. Depending on the nature of an inconsistency, its causes and its impact, they sometimes choose to tolerate its presence, rather than resolve it immediately, if at all. This paper argues for “making inconsistency respectable” [A phrase first used by D. Gabbay and A. Hunter (in: Proceedings of Fundamentals of Artificial Intelligence Research'91, Springer, Berlin, p. 19; in: Symbolic and Quantitative Approaches to Reasoning and Uncertainty, Lecture Notes in Computer Science, Springer, Berlin, 1992, p. 129) to describe the same sentiments that motivated our work.] – sometimes avoided or ignored, but more often used as a focus for learning and as a trigger for further (constructive) development actions. The paper presents a characterization of inconsistency in software development and a framework for managing it in this context. It draws upon practical experiences of dealing with inconsistency in large-scale software development projects and relates some lessons learned from these experiences.
58|3|http://www.sciencedirect.com/science/journal/01641212/58/3|Case study of the evolution of routing algorithms in a network planning tool|Traffic routing is a key component in a network planning system. This paper concentrates on the routing algorithms and follows their evolution over multiple releases of a planning tool during a period of six years. The algorithms have grown from the initial stage of finding shortest paths with Dijkstra's algorithm to cover more complex routing tasks such as finding protected and unprotected routes and capacity limited routing. We present the algorithms and in particular emphasize the practical aspects: how previous algorithms were reused and what were the practical experiences of using the algorithms. A conclusion of the study is that algorithms should be considered with an engineering attitude. It is not enough to focus on selecting the most sophisticated state-of-the-art algorithm for a given problem. Evolution capability, potential for reuse, and the development cost over the system lifetime are equally important aspects.
58|3||The design and implementation of an active replication scheme for distributing services in a cluster of workstations|Replication is the key to providing high availability, fault tolerance, and enhanced performance in a cluster of workstations (COWs). However, building such a system remains as a difficult and challenging task, mainly due to the difficulty of maintaining data consistency among replicas and the lack of easy and efficient tools supporting the development procedure. In this paper we propose an active replication scheme in which data consistency can be maintained. Based on the active replication scheme, we present an object-oriented design pattern and a constructing tool to simplify the design and implementation of service replications in COWs.
58|3||Improving the performance of time-constrained workflow processing|Many workflow applications often have timing constraints such that each processing of a workflow needs to be finished within its deadline. There have been some works to improve the performance of time-constrained workflow processing such as predictive workflow scheduling. They, however, have not much considered appropriate analyses to determine the processing capacities for certain activities so that most workflow instances can satisfy the deadline. Our objective in this paper is to address a suitable scheme that can maximize the number of workflow instances satisfying the given deadline. We first present a method to find out a set of critical activities where a critical activity is the one whose delay of completion directly affects the overall processing time of a workflow. Since each critical activity must have a certain number of servers for the sufficient processing capacity, we then develop a method to determine the minimum number of servers (MNS) for the critical activity such that this activity should be finished without delay for a given input arrival rate. We show through performance experiments that our method can be effectively utilized in practice.
58|3||A priority-based resource allocation strategy in distributed computing networks|In a distributed computing network, various types of resources attached at nodes, such as disks, main memory, resident programs, and printers, could be shared among applications. To promote the resource utilization, an efficient resource allocation algorithm for application requests should be developed. In this paper, our system model considers the factors such as the computation power of nodes, the upper limit of shared programs, the transmission bandwidth of peripherals, the channel bandwidth, and the quality of service (QoS) requirement of applications. Based on these factors, a priority-based resource allocation strategy is proposed and applied to different environments, respectively. The simulation results show that our strategy has a near optimal solution.
58|3||A study of page replacement performance in garbage collection heap|Large number of page-faults can severely degrade the performance of any system. While much attention has been paid on the virtual memory space of an entire process, the paging behavior of one particular memory segment has not been reported. Unlike the static behavior in most of the memory segments, the heap segment has unique characteristics closely related to its memory management scheme. First, this paper presents a thorough study on the effect of heap page-faults on the performance of Kaffe JVM version 1.0.5 and the Kaffe Java virtual machine (JVM) with the modified buddy system. Second, a modified least recently used (mLRU) is proposed to improve the performance of the LRU page-replacement policy. Four different page-replacement policies, first-in-first-out (FIFO), Random, LRU and mLRU, are used to study the performance of each JVM. Third, the Java applications used in this study are SPECjvm98 benchmark suite. These programs represent real-world workload and are highly dynamic memory intensive. Fourth, an instrumented Kaffe JVM is used to generate memory traces. Then, a simulator is used to analyze and reconstruct the heap as the benchmark programs are executed. Finally, this study has shown that the modified buddy system has less page-fault occurrences in six out of eight applications. The improvement can be up to 74%. Moreover, the proposed mLRU can improve the performance up to 68.2%.
58|3||Joint scheduling of garbage collector and hard real-time tasks for embedded applications|Programs with complex data structures often require dynamic memory management based on automatic memory reclamation (garbage collection). A major problem in adopting garbage collection for embedded real-time systems is that it often causes unpredictable pauses and that, as a result of such delays, hard real-time tasks may miss their deadlines. In this paper, we propose a new real-time garbage collection technique for embedded applications. In our approach, the system jointly schedules garbage collector and hard real-time tasks using one of the aperiodic server approaches. Our study focuses on reducing memory requirements while guaranteeing the deadlines of hard real-time tasks. To achieve this objective, we model garbage collection requests as aperiodic hard real-time tasks, and schedule them using the sporadic server (SS). We also present an effective live-memory analysis to bound the worst-case garbage collection time. Performance analysis shows that the proposed approach considerably reduces the worst-case memory reservation compared with a background policy. The analytic results are verified by simulation based on trace-driven data.
58|3||The effects of design pattern application on metric scores|One method suggested for improving software quality has been that of collecting metric scores for a given design, and refactoring in response to what are deemed to be unsatisfactory metric values. More recently, the usage of design patterns has been recommended to promote adaptable designs, so reducing maintenance effort. These two approaches are therefore observed to effectively have the same general aim. The question then arises as to whether design metrics and design patterns are always compatible, and where this is not found to be the case whether the metric, the pattern or both are anomalous. Methods of analysis are presented which demonstrate the effects of applying various patterns on certain metric scores, the initial conclusion being that the two approaches are indeed mainly congruent.
58|3||An empirical study of XML/EDI|Electronic data interchange (EDI) plays an important role in contemporary electronic commerce. It is believed that Extensible Markup Language (XML)/EDI will be the next generation of EDI. In this paper, we thoroughly study and analyze the XML/EDI framework proposed by the XML/EDI Group, and design as well as implement an XML/EDI prototype for Taiwan's flower distribution channels. In accordance with the experiences obtained from the prototype development, we provide in depth analysis of the technology for XML developers and our observations of possible research opportunities in the applications of XML in electronic commerce. Also this paper proposes a general implementation procedure for the industries that are interested in the application of XML/EDI.
58|3||Contents volume 58|
59|1|http://www.sciencedirect.com/science/journal/01641212/59/1|Transitioning from Academia to Industrial Research|
59|1||Integrating scenario-based and measurement-based software product assessment|The software industry needs means to evaluate software products and compare development and implementation technologies in the context of actual projects. Solutions need to be cost-effective but also technically sound. This paper presents a methodology to combine two software product evaluation techniques: measurement of structural design properties, and evaluation of change scenarios. The goal is to use these two approaches together so that they can address each other's limitations. In a case study in the context of the European aerospace industry, this combined methodology was used to assess the impact of choice of programming language and distribution technology on the maintainability of resulting systems. It encompasses the comparison of C++ and Java, as well as distribution/communication technologies such as IPC via sockets, and CORBA implementation. Lessons learned in terms of benefits and limitations are presented. The study shows the usefulness of the approach presented but it is also clear that it needs to be used in combination with other means of evaluation and with a critical mind, as for any engineering solution.
59|1||An empirical evaluation of the ISO/IEC 15504 assessment model|The emerging international standard ISO/IEC 15504 (Software Process Assessment) includes an exemplar assessment model (known as Part 5). Thus far, the majority of users of ISO/IEC 15504 employ the exemplar model as the basis for their assessments. This paper describes an empirical evaluation of the exemplar model. Questionnaire data was collected from the lead assessors of 57 assessments world-wide. Our findings are encouraging for the developers and users of ISO/IEC 15504 in that they indicate that the current model can be used successfully in assessments. However, they also point out some weaknesses in the rating scheme that need to be rectified in future revisions of ISO/IEC 15504.
59|1||The relationship between ISO/IEC 15504 process capability levels, ISO 9001 certification and organization size: An empirical study|The gradual spread in the use of ISO 9001 and ISO/IEC 15504 (also known as software process improvement and capability determination (SPICE)) has raised questions such as “At what ISO/IEC 15504 capability level would one expect an ISO 9001 certified organization's processes to be?” and “Is there any significant difference between the ISO/IEC 15504 capability levels achieved by the processes of ISO 9001 certified organizations and those of non ISO 9001 certified organizations?”. This paper provides answers to those questions as well as to the following question “Is there any significant difference in the capability levels achieved by the ISO/IEC 15504 processes of organizations with a large information technology (IT) staff and those with a small IT staff?” In order to answer these questions, we analyzed a data set including 691 process instances (PIs) taken from 70 SPICE phase 2 trial assessments performed over the two years from September 1996 to June 1998. Results show that the ISO/IEC 15504 processes of the ISO 9001 certified organizations attained capability levels of around 1–2.3 in 15504 terms. Results also show differences between the capability levels achieved by ISO 9001 certified organizations and non ISO 9001 certified organizations, as well as between organizations with a large IT staff and those with a small IT staff.
59|1||An empirical study of certain object-oriented software metrics|This research focuses on analyzing certain software metrics in an object-oriented (OO) environment. The metrics collected and analyzed includes size, number of message (NOM) sends, reuse, inherited methods, and hierarchical nesting level. The site used is the factory systems department of a large manufacturing company. This department uses SmallTalk as the OO programming language to implement the OO design paradigm. Using automated tools developed in SmallTalk, these metrics were collected from three domain applications comprising 600 classes. Four propositions are empirically tested and the results provided in this study.
59|1||Using self-organizing maps to analyze object-oriented software measures|In this study, we present self-organizing maps and discuss their role in the analysis and visualization of software modules in the space of software measures. We reveal how self-organizing maps create a user-friendly and interactive visualization tool that helps user/software designer inspect various alternatives and get a thorough insight into the structure of the clusters of the software modules and the related metrics (software measures). We show how using self-organizing maps we can grow clusters in a dynamic fashion and thus explicitly capture relationships between the software measures and quantify these dependencies for larger and less homogeneous clusters of software modules. The experimental environment exploited in this study relies on software measures coming from 10 large public domain systems, 5 Java and 5 C++ systems.
59|1||Viewpoint representation validation: a case study on two metrics from the Chidamber and Kemerer suite|A metric definition follows from a comprehensive sequence of steps that takes into account all the requirements of the representational theory of measurement. The last step emphasises that numerical relations must preserve and are preserved by empirical relations or viewpoints. It is the preservation of viewpoints in numerical relations that we have tried to empirically validate for the metrics depth of inheritance tree (DIT) and number of children (NOC), proposed by Chidamber and Kemerer [S.R. Chidamber, C.F. Kemerer, IEEE Trans. Software Eng. 20 (6) (1994) 476–493], and which we also refer to as viewpoint representation validation. An analysis of the results indicates the possibility of the two metrics not preserving the viewpoints adequately.
59|1||The determinants of visibility of software engineering researchers|The objective of this paper is to examine the visibility of software engineers who are highly cited in the literature and to present an analysis of the predictors of this visibility. We selected 59 leading software engineering researchers (the subjects) from a much larger group of well-respected software engineers using three criteria: (1) frequency of co-citation for the period 1991–1997, (2) extent to which the research is representative within the field, and (3) adequate coverage of the software engineering subject field. The visibility of the subjects was determined by asking other software engineers to classify each of the subjects by research area. The percentage of respondents who were able to identify the subject by his/her research area was taken as a measure of that subject's visibility. A number of variables were used to explain visibility including the area of expertise, the breadth of the research, and the vintage and form of publication of the subjects' most cited work.
59|1||An assessment of Systems and Software Engineering scholars and institutions (1996â2000)|This paper presents the findings of a five-year study of the top scholars and institutions in the Systems and Software Engineering (SSE) field, as measured by the quantity of papers published in the journals of the field. The top scholar is Richard Lai of La Trobe University in Australia, and the top institution is Carnegie Mellon University (CMU) and its Software Engineering Institute. The paper lists the top 15 scholars and institutions.
59|2|http://www.sciencedirect.com/science/journal/01641212/59/2|Some informal thoughts about reviewing as a social process|
59|2||An integrated approach to distributed version management and role-based access control in computer supported collaborative writing|Tools to support computer supported collaborative writing (CSCWriting) allow multiple distributed users to collaborate over a wide area network on constructing a shared document. Prior research on computer supported collaborative work (CSCW) in general has predominantly focused on synchronous collaboration. Network latency becomes a bottleneck in maintaining shared artifacts during synchronous collaboration. Besides, to enable truly cooperative work asynchronous modes need to be supported as well, so that mobile users can switch between synchronous and asynchronous modes while they disconnect and reconnect to the network. These two considerations motivated the development of a distributed version control system for CSCWriting described in this paper. The most important contribution of our work is the proposal of an activity identification (AID) tag as the fundamental mechanism to support distributed management of multiple versions of a document. The AID tag facilitates the design and implementation of an integrated approach that includes differencing, merging and role-based access control at different levels of granularity, maintaining and visualizing the version structure, and group awareness of document status and operations. The AID tag leads to simple and effective differencing and merging schemes. Its unique address scheme eliminates the need for large storage capacity for version maintenance. Role-based access control can be implemented by associating the access right table and role assignment capabilities with the AID tag. Information for providing group awareness of the changing document is available from the AID tag. In addition, since the system maintains a user-browsable version structure of the evolving document that incorporates AID tag information, any user collaborating in the authoring of a document can easily visualize the historical evolution and current context of the document.
59|2||A framework for evaluation and prediction of software process improvement success|The literature shows that software process improvement (SPI) is a current popular approach to software quality and that many companies are undertaking formal or informal SPI programs. However, the anticipated improvements to software quality through SPI have not, as yet, been fully realised. Many companies are neither ready nor equipped to implement a successful SPI program: how do companies evaluate and validate the necessary organisational requirements for the establishment of a successful SPI program? This paper examines the outcomes of a UK study of a sample of SPI programs and compares these programs with an evaluation framework we have developed. The validated framework will help companies conduct a self-assessment of their readiness to undertake SPI.
59|2||Reschedulable-Group-SCAN scheme for mixed real-time/non-real-time disk scheduling in a multimedia system|Real-time disk scheduling is important for time-critical multimedia applications. Previous approaches, such as SCAN-earliest deadline first (EDF) or DM-SCAN, applied the SCAN scheme to reschedule service sequence of input tasks to reduce tasks' service time. However, they required the input to be in EDF order. In DM-SCAN, a deadline modification scheme was employed to obtain a pseudo-EDF sequence from non-EDF ordered input tasks. Because the modified deadlines were smaller than the original ones, number of tasks that could be rescheduled is decreased and thus data throughput is reduced. In this paper, we propose Reschedulable-Group-SCAN (RG-SCAN), a new real-time disk scheduling algorithm using the concept of Reschedulable-Group (R-Group). Differing from previous approaches, RG-SCAN has no limitation on the input task's sequence. In addition, by exploiting the service time's reduction after rescheduling in each R-Group, RG-SCAN has been extended to serve mixed real-time and non-real-time workloads. As shown in experimental results, our approach can support more tasks than DM-SCAN, both real-time and non-real-time. Additionally, our approach can provide larger data throughput and offer better response time to non-real-time tasks. For example, given 30 random-generated real-time tasks, the number of non-real-time tasks that can be supported by RG-SCAN is 1.3 times that supported by DM-SCAN. In addition, our data throughput is 1.1 times DM-SCAN's.
59|2||Object-oriented real-world modeling revisited|When applied to real-world problems, object-oriented modeling maps an entity in the real world to a class as it is. This seemingly natural “genuine” real-world modeling can be rightly applied to cases when the purpose of modeling is merely to represent a problem in a class diagram and thus to facilitate its understanding. Business process reengineering is a good example of this. Genuine real-world modeling can also be applied to the development of a program that simulates the real world on a computer. Contrary to these cases, however, “pseudo” real-world modeling has instead to be applied when a business assistance application is to be developed. It maps an entity whose information is dealt with by the business to be automated to a class that represents the information about the entity. These two modeling methods have to be appropriately applied according to the type of their target problems. This point, however, has not been sufficiently recognized. Many authors of the literature on object-oriented methodologies and techniques teach us “naive” real-world modeling, whose real nature is a mixture of genuine and pseudo real-world modeling methods. Naive analyzers who believe the literature are lured into severe modeling errors when they develop business assistance applications.
59|2||Illustrating the cognitive consequences of object-oriented systems development|The purported “naturalness” of object-oriented (OO) approaches refers to the assertion that an alignment exists among the innate cognitive activities of system designers and the characteristics of OO system development approaches. That is, designers can think about the objects that make up the system in the same way they think about objects in the world. This fit of cognitive activities and OO characteristics appeals to many system developers. Contrary to the claim of naturalness, many students and practitioners report difficulties learning and using OO techniques. To understand why OO approaches seem to be difficult to learn and use, we illustrate the consistency of common characteristics of OO techniques and the cognitive activities of systems designers. A group of graduate students identified 148 beliefs, facts, and ideas pertaining to OO complexity. Researchers classified each item in terms of the abstraction, communication, problem-orientation, and shared behavior characteristics common to OO approaches. Then classified each item in terms of the problem understanding, problem decomposition, and design solution cognitive activities. The consistency of the two classifications confirms that claims are supported by usage. However, it seems that methodology and project management are part of the task of developing information systems (ISs) that are not adequately addressed by OO approaches.
59|2||Object-oriented design patterns recovery|Object-Oriented (OO) design patterns are an emergent technology: they are reusable micro-architectures, high-level building blocks. A system which has been designed using well-known, documented and accepted design patterns is also likely to exhibit good properties such as modularity, separation of concerns and maintainability. While for forward engineering the benefits of using design patterns are clear, using reverse engineering technologies to discover instances of patterns in a software artifact (e.g., design or code) may help in several key areas, among which are program understanding, design-to-code traceability and quality assessment. This paper describes a conservative approach and experimental results, based on a multi-stage reduction strategy using OO software metrics and structural properties to extract structural design patterns from OO design or C++ code. To assess the effectiveness of the pattern recovery approach, a process and a portable tool suite written in Java, remotely accessible by means of any WEB browser, has been developed. The developed system and experimental results on 8 industrial software (design and code) and 200,000 lines of public domain C++ code are presented.
59|2||A comparative study of exception handling mechanisms for building dependable object-oriented software|Modern object-oriented systems have to cope with an increasing number of exceptional conditions and incorporate fault tolerance into systems' activities in order to meet dependability-related requirements. An exception handling mechanism is one of the most important schemes for detecting and recovering errors, and for structuring fault-tolerant activities in a system. The mechanisms that were ill designed can make an application unreliable and difficult to understand, maintain and reuse in the presence of faults. This paper surveys various exception mechanisms implemented in different object-oriented languages, evaluates and compares different designs. A taxonomy is developed to help address 10 basic technical aspects for a given exception handling proposal, including exception representation, external exceptions in signatures, separation between internal and external exceptions, attachment of handlers, handler binding, propagation of exceptions, continuation of the control flow, clean-up actions, reliability checks, and concurrent exception handling. Practical issues and difficulties are summarized, major trends in actual languages are identified, and directions for future work are suggested.
59|3|http://www.sciencedirect.com/science/journal/01641212/59/3|Software Process Simulation Modelling|
59|3||Hybrid simulation modelling of the software process|The paper proposes the combination of three traditional modelling methods (analytical, continuous and discrete-event), into a unique hybrid two-level modelling approach, to address software process simulation modelling issues. At the higher abstraction level, the process is modelled by a discrete-event queueing network, which represents the component activities, their interactions, and the exchanged artefacts. At the lower abstraction level, the analytical and continuous methods are used to describe the behaviour of the introduced activities.
59|3||Application of a hybrid process simulation model to a software development project|Simulation models of the software development process can be used to evaluate potential process changes. Careful evaluation should consider the change within the context of the project environment. While system dynamics models have been used to model the project environment, discrete event and state-based models are more useful when modeling process activities. Hybrid models of the software development process can examine questions that cannot be answered by either system dynamics models or discrete event models alone. In this paper, we present a detailed hybrid model of a software development process currently in use at a major industrial developer. We describe the model and show how the model was used to evaluate simultaneous changes to both the process and the project environment.
59|3||Stochastic simulation of risk factor potential effects for software development risk management|One of the proposed purposes for software process simulation is the management of software development risks, usually discussed within the category of project planning/management. However, modeling and simulation primarily for the purpose of software development risk management has been quite limited. This paper describes an approach to modeling risk factors and simulating their effects as a means of supporting certain software development risk management activities. The effects of six common and significant software development risk factors were studied. A base model was then produced for stochastically simulating the effects of the selected factors. This simulator is a tool designed specifically for the risk management activities of assessment, mitigation, contingency planning, and intervention.
59|3||Behavioral characterization: finding and using the influential factors in software process simulation models|Most software process simulation work has focused on the roles and uses of software process simulators, on the scope of models, and on simulation approaches. Consequently, the literature reflects a growing body of models that have recently been characterized by modeling purpose, scope, key result variables, and simulation method. While the software process simulation arena is maturing, little effort appears to have been given to statistical evaluation of model behavior through sensitivity analysis. Rather, most of software process simulation experimentation has examined selected factors for the sake of understanding their effects with regard to particular issues, such as the economics of quality assurance or the impact of inspections practice. In a broad sense, sensitivity analysis assesses the effect of each input on model outputs. Here, we discuss its use for behaviorally characterizing software process simulators. This paper discusses the benefits of using sensitivity analysis to characterize model behavior; the use of experimental design for this purpose; our procedure for using designed experiments to analyze deterministic simulation models; the application of this procedure to four published software process simulators; the results of our analysis; and the merits of this approach.
59|3||System dynamics modelling of software evolution processes for policy investigation: Approach and example|This paper describes one of the latest in a series of system dynamics models developed during the Feedback, Evolution And Software Technology (FEAST) investigation into software evolution processes. The intention of early models was to simulate real-world processes in order to increase understanding of such processes. The work resulted in a number of lessons learnt, in particular, with regard to the application of system dynamics to the simulation of key attributes of long-term software evolution. The work reported here combines elements of previous work and extends them by describing an approach to investigate the consequences on long-term evolution, of decisions made by the managers of these processes. The approach is illustrated by discussion of the impact of complexity control activity. This model of the impact on product and global process attributes of decisions regarding the fraction of work applied to progressive and to anti-regressive activities such as complexity control, for instance, exemplifies the results of the FEAST investigation.
59|3||A CBT module with integrated simulation component for software project management education and training|Due to increasing demand for software project managers in industry, efforts are needed to develop the management-related knowledge and skills of the current and future software workforce. In particular, university education needs to provide to their computer science and software engineering (SE) students not only technology-related skills but, in addition, a basic understanding of typical phenomena occurring in industrial (and academic) software projects. The objective of this paper is to present concepts of a computer-based training (CBT) module for student education in software project management. The single-learner CBT module can be run using standard web-browsers (e.g. Netscape). The simulation component of the CBT module is implemented using the system dynamics (SD) simulation modelling method. The paper presents the design of the simulation model and the training scenario offered by the existing CBT module prototype. Possibilities for empirical validation of the effectiveness of the CBT module in university education are described, results of a first controlled experiment are presented and discussed, and future extensions of the CBT module towards collaborative learning environments are suggested.
59|3||A simplified model of software project dynamics|The simulation of a dynamic model for software development projects (hereinafter SDPs) helps to investigate the impact of a technological change, of different management policies, and of maturity level of organisations over the whole project. In the beginning of the 1990s, with the appearance of the dynamic model for SDPs by Abdel-Hamid and Madnick [Software Project Dynamics: An Integrated Approach, Prentice-Hall, Englewood Cliffs, NJ, 1991], a significant advance took place in the field of project management. From this work, several dynamic models have been developed in order to simulate the behaviour of these kinds of projects. From the comparison made between one of the best known empirical estimation models and dynamic estimation models, we have analysed the existing problems in dynamic models in order to make dynamic estimations at the early stages of software projects, when little information is available. We present the results obtained from a Reduced Dynamic Model developed to estimate and analyse the behaviour of SDPs in the early phases, in which there is not much information regarding the project. The modelling approach followed to obtain this simplified model has been determined by the simplification of Abdel-Hamid and Madnick's model using the works of Eberlein [Syst. Dyn. Rev. 1(5) (1989) 51] about understanding and simplification of models.
59|3||System dynamics modelling and simulation of collaborative requirements engineering|
59|3||Exploring bottlenecks in market-driven requirements management processes with discrete event simulation|This paper presents a study where a market-driven requirements management process is simulated. In market-driven software development, generic software packages are released to a market with many customers. New requirements are continuously issued, and the objective of the requirements management process is to elicit, manage, and prioritize the requirements. In the presented study, a specific requirements management process is modelled using discrete event simulation, and the parameters of the model are estimated based on interviews with people from the specific organisation where the process is used. Based on the results from simulations, conditions that result in an overload situation are identified. Simulations are also used to find process change proposals that can result in a non-overloaded process. The risk of overload can be avoided if the capacity of the requirements management process is increased, or if the number of incoming requirements is decreased, for example, through early rejection of low-priority requirements.
59|3||Analysing a process landscape by simulation|One purpose of analysing process models is to identify possible process improvements. Process modelling serves as a precondition for process model analysis. In this paper, we discuss how to model a process landscape at different levels of abstraction in order to do some static analysis. A process landscape consists of a set of hierarchically structured models for (distributed) processes. First we introduce the “Process Landscaping” method, which allows us to simulate different aspects of a complex set of processes. Afterwards, we focus on several simulation requirements and parameters needed to check consistency conditions and to analyse communication aspects of a given software process landscape. An example process landscape describes the development of e-business applications taking place at different locations.
59|3||Modeling and simulating software acquisition process architectures|In this paper, we describe our efforts to support the modeling and simulation of processes associated with software system acquisition activities. Software acquisition is generally a multi-organization endeavor concerned with the funding, management, engineering, system integration, deployment and long-term support of large software systems. We first describe our approach supporting the modeling and simulation of software acquisition processes using a software process architecture (SPA). We then introduce how we support the distribution, concurrent execution and interoperation of multiple software process simulations using the high-level architecture (HLA) and run-time infrastructure (RTI) to address the complexity of software acquisition process architectures. To illustrate this, we provide examples from the design and prototyping of a Web-based environment that supports the modeling and simulation of acquisition process architectures. This environment thus serves as a new kind of software process test-bed that can demonstrate and support experiments incorporating multiple software process simulation systems that interoperate in a distributed and concurrent manner across a network.
59|3||A tool for evaluation of the software development process|As part of their effort to reduce development time and improve the quality of their products, software developers are looking at the software development process they use. They reason that a high quality process is likely to lead to a high quality product. However, few developers receive high ratings on the accepted scales when their process is evaluated. To achieve the highest ratings for their processes, developers need to measure and manage their process and to do this effectively, they need tool support. However, they also have significant investments in their existing processes and support systems that they cannot afford to abandon. This makes adopting the process model that is implicit in most process support tools difficult. In this paper, we describe our tool, RolEnact that attempts to address the requirements for a tool for the simulation and evaluation of software development processes and illustrate its use with an example.
59|3||Contents Volume 59|
60|1|http://www.sciencedirect.com/science/journal/01641212/60/1|A Final Good-bye|
60|1||Stepping up to the Plate|
60|1||On the independence of software inspectors|This paper investigates the proposition: are the results (defects found) of individual software inspectors during a software inspection process (preparation phase) statistically independent or dependent? The rationale for exploring this proposition is that many capture–recapture models assume statistical independence in their formulations. Recently these models have been applied to estimating the number of undiscovered defects undetected by an inspection process. Using large amounts of inspection data from a previous experiment, and subsequently simulating a capture–recapture time-series from this data, the paper explores this proposition. The correlation within the time-series is then estimated using non-parametric techniques. The paper concludes that, for the data used, no evidence exists to support the statement that the results from individual inspectors are dependent.
60|1||A matching-based algorithm for page access sequencing in join processing|One of the fundamental problems in relational database management is the handling of the join operation. Two of the problems are: (1) finding a page access sequence which requires the minimum number of buffer pages if there are no page reaccesses, and (2) finding a page access sequence which requires the minimum number of page reaccesses given a fixed buffer size. In general, a heuristic for the first problem can be converted to a corresponding heuristic to the second problem.
60|1||Streaming extensibility in the Modify-on-Access file system|This paper presents the Modify-on-Access (Mona) file system that provides extensibility through transformations applied to streams of data. Mona overcomes two limitations of prior extensible file systems. First, the Mona file system offers two levels of extensions (kernel and user) that share a common interface. It allows performance-critical operations to execute with modest overhead in the kernel and untrusted or more complex operations to safely execute in user space. Second, Mona enables fine-grained extensions which allow an application to customize the file system at runtime. This paper discusses the implementation of the Mona file system. Our implementation adds modest overhead of 0–3% () to file system operations. This overhead has even less effect on net system performance for several benchmarks. Moreover, this paper describes applications that achieve 4–5 times speedup using custom transformations. This paper also describes several transformations that increase functionality. Among these are the ftp transformation that allows a user to browse a remote file as though it were local and the command transformation which invokes an arbitrary executable (even a shell script) on a data stream.
60|1||Design and implementation of spatiotemporal database query processing system|Researches on spatial and temporal databases have been done independently. In terms of spatial databases, whenever a new object instance is inserted into a database, the old one should be deleted. It stands for the difficulty to manage efficiently the historical information about spatial object that has been changed with temporal evolution. In view of temporal databases, because it did not consider supporting spatial type and operation, it is extremely hard to manage directly spatial objects without any modification. Nevertheless, because these research domains are closely related, an integration field, i.e., spatiotemporal databases, has been launched. Spatiotemporal databases support historical information as well as spatial management for the object at the same time and can deal with geometries changing over time. They can be used in the various application areas such as geographic information system (GIS), urban plan system (UPS), and car navigation system (CNS), and so on.
60|1||On the application of formal description techniques to the design of interception systems for GSM mobile terminals|In this paper we show how a formal approach to design may speed up the development process for a new standard-based product: an interception device for GSM networks. We discuss the benefits, the limitations, and the lessons learnt using this approach, based on our own practical experience. A software package containing the results of this work and related documentation is freely available for academic institutions.
60|1||Formal modeling in a commercial setting: A case study|Formal modeling, particularly of software requirements, has long been advocated by the software engineering research community. Usually, such modeling is linked with formal verification and is confined to safety-critical projects where software correctness is the pivotal goal. In contrast, the software industry seeks practical techniques that can be seamlessly integrated into the existing processes and improve productivity; very high quality is often a desirable but not crucial objective. A number of modeling languages have been designed to address this issue, and some have industrial-strength tool support that can be used effectively in commercial settings. This paper describes a case study conducted in collaboration with Nortel Networks to demonstrate the economic feasibility and effectiveness of applying formal modeling techniques to telecommunication systems. A formal specification and description language (SDL) was chosen to model a multimedia-messaging system described by an 80-page natural-language specification. Our model was used to identify errors in the software requirements document and to derive test suites, shadowing the existing development process and keeping track of a variety of productivity data. Our results clearly show that it is possible to use formal modeling in the commercial setting effectively: we were able to locate a number of specification errors that were missed by several manual specification inspections, and used the model to derive the test cases that, by the time we left the project, doubled the number of errors discovered by Nortel testers. The formalization did not lengthen the overall time to market of this product.
60|1||The importance of ignorance in requirements engineering: An earlier sighting and a revisitation|
60|2|http://www.sciencedirect.com/science/journal/01641212/60/2|Guest editorial|
60|2||From program languages to software languages|In modern Software Engineering it is no longer sufficient to consider “programming” as the task of writing implementations in more or less suited programming languages and adding a more or less accurate documentation. Rather, the amalgamation of various aspects and views into a heterogeneous assembly of coordinated products is required. In this paper we present a vision of such a scenario. This vision is based on experiences with both academic style foundational research and industry-oriented applied research projects.
60|2||Designing reactive systems: integration of abstraction techniques into a synthesis procedure|This paper presents a new paradigm for designing reactive systems. It combines the use of formal methods widely recognized in software engineering and synthesis procedures developed within the framework of the Supervisory Control Theory for discrete event systems. It promotes design exploration by means of a synthesis approach with the sole aim of producing reliable reactive systems. The adoption of these particular synthesis procedures is, however, not sufficient to achieve this objective, because of scalability and computational complexity issues. To circumvent these difficulties, this paper suggests two extensions with respect to conventional synthesis procedures. The first one concerns the representation of reactive programs by attributed controllers. This requires that the process to be controlled must be described not only in terms of controllable active components but also in terms of uncontrollable passive components by using timed transition graphs and algebraic specifications, respectively. The second one involves abstraction and equational reasoning to take into account the use of strongly typed objects. This requires various kinds of transformation applied to the original problem specification as well as to intermediate solutions.
60|2||Formal design and development of a Corba-based application for cooperative HTML group editing support|This paper presents SEdit, a formally designed and developed application for collaborative HTML group editing support. The design model separates coordination from cooperation and communication, allowing different cooperation interfaces (ASCII support and HTML support) to be integrated with the same coordination system. The coordination protocol allows different users to simultaneously work on different parts of the shared document. This protocol has been synthesized from a formal description based on graph grammars.
60|2||A software environment task object-oriented design (ETOOD)|This paper is intended to present an approach to the construction of a task model of method, named task object-oriented design (TOOD), used for the development of an interactive system. This approach is based on a formal notation, giving quantitative results which may be checked by designers and which provide the possibility of performing mathematical verifications on the models. The modeling formalism is based on the joint use of the object approach and of high-level Petri nets. The concepts borrowed from the object approach make it possible to describe the static aspect of tasks and the Petri nets enable the description of dynamics and behavior. We also describe a software aid tool for the manipulation of these models, which allow the editing of a task model. In order to facilitate comprehension of the method, a simple example of procedure used in missile firing management will be given.
60|2||Implementation of distributed iterative algorithm for optimal control problems on several parallel architectures|We consider an optimal control problem without constraint. The solution of this problem leads to a large scale block linear system. We present a two-stage method which is well suited to an asynchronous implementation with flexible communication whereby every processor can have access at any time and without any fixed rule to the current value of the components of the iteration vector updated by any other processor. The implementation is carried out on three types of architecture: a super computer with distributed memory CRAY T3E, a shared memory symmetric multiprocessor (SMP) and a high speed network of SMP.
60|2||Galois connection, formal concepts and Galois lattice in real relations: application in a real classifier|In this paper, we introduce the notion of a real set as an extension of a crisp and a fuzzy set by using sequences of intervals as membership degrees, instead of a single value in [0,1]. We also propose, to extend the notion of Galois connection in a real binary relation as well as the notions of rectangular relation, formal concept and Galois lattice. We present finally a real classifier based on this mathematical foundation.
60|3|http://www.sciencedirect.com/science/journal/01641212/60/3|Automated discovery of concise predictive rules for intrusion detection|This paper details an essential component of a multi-agent distributed knowledge network system for intrusion detection. We describe a distributed intrusion detection architecture, complete with a data warehouse and mobile and stationary agents for distributed problem-solving to facilitate building, monitoring, and analyzing global, spatio-temporal views of intrusions on large distributed systems. An agent for the intrusion detection system, which uses a machine learning approach to automated discovery of concise rules from system call traces, is described.
60|3||A group signature scheme with strong separability|Group signatures, introduced by Chaum and van Heijst, allow members of a group to sign messages anonymously on behalf of the group. Only a designated group manager is able to identify the group member who issued a given signature. Many applications of group signatures, for example, electronic market, require that the group manager can be split into a membership manager and a revocation manager. The former is responsible for adding new members to the group. The latter is responsible for opening signatures. Previously proposed group signatures schemes can only achieve a weak form of separability. That is, the revocation manager and the membership manager must work in concert to reveal the identity of the signer. In this paper, we propose a group signature scheme with strong separability in which the revocation manager can work without the involvement of the membership manager.
60|3||Special section on Industrial information systems: progresses and perspectives in Pacific Rim|
60|3||Architectural design and evaluation of an efficient Web-crawling system|This paper presents an architectural design and evaluation result of an efficient Web-crawling system. The design involves a fully distributed architecture, a URL allocating algorithm, and a method to assure system scalability and dynamic reconfigurability. Simulation experiment shows that load balance, scalability and efficiency can be achieved in the system. Currently this distributed Web-crawling subsystem has been successfully integrated with WebGather, a well-known Chinese and English Web search engine, aimed at collecting all the Web pages in China and keeping pace with the rapid growth of Chinese Web information. In addition, we believe that the design can also be useful in other context such as digital library, etc.
60|3||A reference system for internet based inter-enterprise electronic commerce|As supply chain management and its global outsourcing are widely spread recently, many researches and development focus on integration of inter-enterprise information systems. The most important factors to achieve inter-enterprise integration are standardization and digitalization. The objectives of this work are to develop the reference model for internet based inter-enterprise electronic commerce and to implement a prototype system for its fundamental issues such as business process, information standards, and information system integration. The electronics assembly processes are chosen as the target industry. For the reference architecture of inter-enterprise electronic commerce, we also investigate intra-enterprise business processes and model them with IDEF0. The standard documents and its items are defined from analysis of traditional documents for outsourcing processes and EDI documents. For standard exchange method, web-based XML is adopted, and corresponding DTD and XSL for RFQ and Quotation are developed. In order to prove feasibility and effectiveness of the reference system a prototype system is introduced and implemented.
60|3||Integrity protection for Code-on-Demand mobile agents in e-commerce|The mobile agent paradigm has been proposed as a promising solution to facilitate distributed computing over open and heterogeneous networks. Mobility, autonomy, and intelligence are identified as key features of mobile agent systems and enabling characteristics for the next-generation smart electronic commerce on the Internet. However, security-related issues, especially integrity protection in mobile agent technology, still hinder the widespread use of software agents: from the agent's perspective, mobile agent integrity should be protected against attacks from malicious hosts and other agents. In this paper, we present Code-on-Demand (CoD) mobile agents and a corresponding agent integrity protection scheme. Compared to the traditional assumption that mobile agents consist of invariant code parts, we propose the use of dynamically upgradeable agent code, in which new agent function modules can be added and redundant ones can be deleted at runtime. This approach will reduce the weight of agent programs, equip mobile agents with more flexibility, enhance code privacy and help the recoverability of agents after attack. In order to meet the security challenges for agent integrity protection, we propose agent code change authorization protocols and a double integrity verification scheme. Finally, we discuss the Java implementation of CoD mobile agents and integrity protection.
60|3||A computerized causal forecasting system using genetic algorithms in supply chain management|Forecasting activities are widely performed in the various areas of supply chains for predicting important supply chain management (SCM) measurements such as demand volume in order management, product quality in manufacturing processes, capacity usage in production management, traffic costs in transportation management, and so on. This paper presents a computerized system for implementing the forecasting activities required in SCM. For building a generic forecasting model applicable to SCM, a linear causal forecasting model is proposed and its coefficients are efficiently determined using the proposed genetic algorithms (GA), canonical GA and guided GA (GGA). Compared to canonical GA, GGA adopts a fitness function with penalty operators and uses population diversity index (PDI) to overcome premature convergence of the algorithm. The results obtained from two case studies show that the proposed GGA provides the best forecasting accuracy and greatly outperforms the regression analysis and canonical GA methods. A computerized system was developed to implement the forecasting functions and is successfully running in real glass manufacturing lines.
60|3||Applying a mediator architecture employing XML to retailing inventory control|The concept of mediation has been successfully applied to the development of distributed information systems, where mediators form the middle tier between client applications and data resources, providing valuable abstraction and integration functions. Extensible MarkUp Language (XML) has been used to develop self-describing data models and common interfaces for mediators to be deployed in Web-based information systems. This makes such systems easier to develop, to maintain, and to evolve. This paper discusses the design of such an architecture and its application in retail inventory control in electronic commerce (e-commerce). The prototype system is implemented with Java Servlets and corba.
60|3||Introduction to an integrated methodology for development and implementation of enterprise information systems|An integrated methodology for successful development and implementation of enterprise information systems is developed. This paper describes the methodology and defines five components and one repository which can be customized with business scenarios and patterns according to various business environments. Five components consist of information strategy planning, economic justification and measurement, enterprise information system appraisal, package software evaluation, and unified modeling tools. They characterize the methodology through its entire road map. Also, case studies are provided to prove its practical values.
60|3||Contents of Volume 60|
||||
volume|issue|url|title|abstract
61|1|http://www.sciencedirect.com/science/journal/01641212/61/1|Strategies for resolving inter-class data conflicts in mixed real-time database systems|Although many efficient concurrency control protocols have been proposed for real-time database systems, they are mainly restricted for systems with a single type of real-time transactions or a mixed set of soft real-time and non-real-time transactions only. Their performance objective usually aims at the minimization of the number of missed deadlines of soft real-time transactions or to guarantee the deadline satisfaction of hard real-time transactions. So far, it is still lack of any good study on the design of concurrency control strategies for mixed real-time database systems (MRTDBS), which consist of both hard and soft real-time transactions, and together with non-real-time transactions. Due to the very different performance requirements of hard and soft real-time transactions, existing real-time concurrency control protocols may not be suitable to MRTDBS. In this paper, we propose strategies for resolving data conflicts between different types of transactions in an MRTDBS so that the performance requirements of each individual transaction type can be satisfied and, at the same time, the overall system performance can be improved. The performance of the proposed strategies is evaluated and compared with a real-time optimistic approach, which has been shown to give a better performance than the lock-based protocols for soft and firm real-time transactions.
61|1||Maintaining security and timeliness in real-time database system|Real-time database systems can have security constraints in addition to timing constraints. Such real-time systems are typically contained in environments that exhibit hierarchical propagation of information, where mandatory access control for security is required. Conventional multilevel secure (MLS) database models that implement mandatory access control are inadequate for time-critical applications and conventional real-time database models do not address security constraints. The objective of this work is to incorporate security constraints in real-time database systems in such a way that not only is security achieved, but achieving security does not degrade real-time performance significantly in terms of deadlines missed. We present two concurrency control algorithms for secure real-time databases: the Secure two-phase locking high priority (2PLHP) algorithm is based on a two-phase locking protocol and the Secure optimistic concurrency control (OPT) algorithm uses the properties of an optimistic concurrency protocol. We implement the two algorithms and study their performance using a real-time database system simulation model. Our study covers both soft and firm real-time databases. Results show that both the algorithms perform fairly well in terms of security and timeliness compared to non-secure algorithms. We show that achieving increased security does not necessarily mean an increased sacrifice in real-time performance.
61|1||The impact of component architectures on interoperability|Component interoperability has become an important concern as companies migrate legacy systems, integrate COTS products, and assemble modules from disparate sources into a single application. While middleware is available for this purpose, it often does not form a complete bridge between components and may be inflexible as the application evolves. What is needed is the explicit design information that will forecast a more accurate, evolvable, and less costly integration solution implementation.
61|1||Experiences with ALMA: Architecture-Level Modifiability Analysis|Modifiability is an important quality for software systems, because a large part of the costs associated with these systems is spent on modifications. The effort, and therefore cost, that is required for these modifications is largely determined by a system's software architecture. Analysis of software architectures is therefore an important technique to achieve modifiability and reduce maintenance costs. However, few techniques for software architecture analysis currently exist. Based on our experiences with software architecture analysis of modifiability, we have developed ALMA, an architecture-level modifiability analysis method consisting of five steps. In this paper we report on our experiences with ALMA. We illustrate our experiences with examples from two case studies of software architecture analysis of modifiability. These case studies concern a system for mobile positioning at Ericsson Software Technology AB and a system for freight handling at DFDS Fraktarna. Our experiences are related to each step of the analysis process. In addition, we made some observations on software architecture analysis of modifiability in general.
61|1||An estimation of the decision models of senior IS managers when evaluating the external quality of organizational software|The increased usage of software in large corporations, coupled with the explosion in software availability has made it important to evaluate software quality (SQ) from the point of view of its consumers. In this study, we focus on the external quality (quality from the point of view of the consumer) of off-the-shelf software used in large corporations. We refine external software quality into four factors. Next, we utilize conjoint analysis (CA) to study the relative values of these factors in the decision models of senior IS managers, when evaluating software for use by their organization. Our results indicate that software used in large corporations today has evolved, so that, contrary to earlier studies that indicate learnability and features as important, it is now the reliability of the software that is the primary factor in IS managers' decision models. The findings have implications for IS theory, and provide guidelines for resource allocation for software developers, IS managers, researchers in the area of software reliability and designers of IS curricula.
61|2|http://www.sciencedirect.com/science/journal/01641212/61/2|An XML approach for legacy code reuse|In this paper, we investigate a hybrid approach of wrapping and migration for the reuse of legacy applications through a case study. This approach is based on eXtensible Markup Language (XML), client/server architecture and the concepts used by Netron Fusion. To implement wrapping and migration of legacy applications, necessary information is achieved by gaining the understanding of the information flow and logic, and tracing users' interactions with systems, aiming at minimal modifications of the original applications. We present, through a case study, a way of applying this approach to an application.
61|2||SigDAQ: an enhanced XML query optimization technique|XML is an emerging standard for data representation and exchange on the Web. XML is represented as a tree and the query as a regular path expression (RPE). The query is evaluated by traversing each node of the tree. Several indexes are proposed for RPEs for fast retrieval. In some cases these indexes may not cover all possible paths because of storage requirements. In this paper, we propose a signature-based query optimization technique to minimize the number of nodes retrieved from the database when the indexes cannot be used. The signature is a hint attached to each node, and is used to prune unnecessary sub-trees as early as possible when traversing nodes. For this goal, we propose the SigDAQ which is a signature-based DOM (s-DOM) as a storage model and a signature-based query executor (s-NFA). Our experimental results show that the signature method outperforms the original.
61|2||Design erosion: problems and causes|Design erosion is a common problem in software engineering. We have found that invariably, no matter how ambitious the intentions of the designers were, software designs tend to erode over time to the point that redesigning from scratch becomes a viable alternative compared to prolonging the life of the existing design. In this paper, we illustrate how design erosion works by presenting the evolution of the design of a small software system. In our analysis of this example, we show how design decisions accumulate and become invalid because of new requirements. Also it is argued that even an optimal strategy for designing the system (i.e. no compromises with respect to e.g. cost are made) does not lead to an optimal design because of unforeseen requirement changes that invalidate design decisions that were once optimal.
61|2||Availability analysis and improvement of Active/Standby cluster systems using software rejuvenation|Cluster systems, using commercially available personal computers connected in a loosely coupled fashion can provide high levels of availability. To improve the availability of personal computer-based Active/Standby cluster systems, we have conducted a study of software rejuvenation that follows a proactive fault-tolerant approach to handle software-origin system failure. In this paper, we map software rejuvenation and switchover states with a semi-Markov process and get mathematical steady-state solutions of the chain. We calculate the availability and the downtime of Active/Standby cluster systems using the solutions and find that software rejuvenation can be used to improve the availability of Active/Standby cluster systems.
61|2||Visual requirement representation|Multimedia technology has played an important role in modern computing because it offers more natural and user-friendly interactions with an automated system. This is particularly true for systems utilizing graphical, icon or window-based input and output. Multimedia technology also facilitates “reuse” more naturally, since the basic components and functions of presentation and animation can be reused for several different animation scenarios. This is evidenced by the rapid prototyping capability of computer and video games where although the characters and story lines change, the basic animation remains constant. In this paper we utilize multimedia technology for eliciting requirements of software systems, particularly those systems that utilize windows- (or graphical)-based interactions with the user. Our methodology will implicitly emphasize reuse since in our approach reusable components include not only code and documents, but also voice narration, animation sequences and message mechanisms. We call such software components as multimedia reusable components (MRCs). Using MRCs, one can view software requirements instead of reading textual representation of the requirements.
61|2||Software requirements validation via task analysis|As a baseline for software development, a correct and complete requirements definition is one foundation of software quality. Previously, a novel approach to static testing of software requirements was proposed in which requirements definitions are tested on a set of task scenarios by examining software behaviour in each scenario described by an activity list. Such descriptions of software behaviour can be generated automatically from requirements models. This paper investigates various testing methods for selecting test scenarios. Data flow, state transition and entity testing methods are studied. A variety of test adequacy criteria and their combinations are formally defined and the subsume relations between the criteria are proved. Empirical studies of the testing methods and the construction of a prototype testing tool are reported.
61|3|http://www.sciencedirect.com/science/journal/01641212/61/3|Best practices in software engineering|
61|3||Classification and evaluation of defects in a project retrospective|There are three interdependent factors that drive our development processes: interval, quality and cost. As market pressures continue to demand new features ever more rapidly, the challenge is to meet those demands while increasing, or at least not sacrificing, quality. One advantage of defect prevention as an upstream quality improvement practice is the beneficial effect it can have on interval: higher quality early in the process results in fewer defects to be found and repaired in the later parts of the process, thus causing an indirect interval reduction.
61|3||COTS-based software development: Processes and open issues|The work described in this paper is an investigation of the COTS-based software development within a particular NASA environment, with an emphasis on the processes used. Fifteen projects using a COTS-based approach were studied and their actual process was documented. This process is evaluated to identify essential differences in comparison to traditional software development. The main differences, and the activities for which projects require more guidance, are requirements definition and COTS selection, high level design, integration and testing. Starting from these empirical observations, a new process and set of guidelines for COTS-based development are developed and briefly presented.
61|3||Challenges of component-based development|It is generally understood that building software systems with components has many advantages but the difficulties of this approach should not be ignored. System evolution, maintenance, migration and compatibilities are some of the challenges met with when developing a component-based software system. Since most systems evolve over time, components must be maintained or replaced. The evolution of requirements affects not only specific system functions and particular components but also component-based architecture on all levels. Increased complexity is a consequence of different components and systems having different life cycles. In component-based systems it is easier to replace part of system with a commercial component. This process is however not straightforward and different factors such as requirements management, marketing issues, etc., must be taken into consideration. In this paper we discuss the issues and challenges encountered when developing and using an evolving component-based software system. An industrial control system has been used as a case study.
61|3||Producing reliable software: an experiment|A customer of high assurance software recently sponsored a software engineering experiment in which a small real-time software system was developed concurrently by two popular software development methodologies. One company specialized in the state-of-the-practice waterfall method rated at a Capability Maturity Model Level 4. A second developer employed his mathematically based formal method with automatic code generation. As specified in separate contracts, C++ code plus development documentation and process and product metrics (errors) were to be delivered. Both companies were given identical functional specifications and agreed to a generous and equal cost, schedule, and explicit functional reliability objectives. At conclusion of the experiment an independent third party determined through extensive statistical testing that neither methodology was able to meet the user's reliability objectives within cost and schedule constraints. The metrics collected revealed the strengths and weaknesses of each methodology and why they were not able to reach customer reliability objectives. This paper will explore the specification for the system under development, the two development processes, the products and metrics captured during development, the analysis tools and testing techniques used by the third party, and the results of a reliability and process analysis.
61|3||An empirical study of industrial security-engineering practices|This paper presents lessons learned and observations noted about the state of security-engineering practices by three information security practitioners with different perspectives – two in industry and one in academia. All authors have more than 20-years experience in this field and two were former members of the US National Computer Security Center during the early days of the Trusted Computer System Evaluation Criteria and the strong promotion of trusted operating systems that accompanied the release of that document. In the last 20 years, it has been argued that security-engineering practices have not kept pace with the escalating threats to information systems. Much has occurred since that time – new security paradigms, failure of evaluated products to emerge into common use, new systemic threats, and an increased awareness of the risk faced by information systems. This paper presents an empirical view of lessons learned in security-engineering, experiences in applying the trade, and observations made about the successes and failures of security practices and technology. This work was sponsored in part by NSF Grant.
61|3||Interprocess communications in the AN/BSY-2 distributed computer system: a case study|This paper presents a case study of the design and implementation of the interprocess communications facility developed for the AN/BSY-2 distributed computer system, the computer system for the Seawolf submarine. The interprocess communications facility was identified as a critical design challenge for the AN/BSY-2 system, as the system incorporated new component and network technology along with new run time system services as well as application programs. The requirements specified for the interprocess communications included aggressive performance, as well as functional capabilities that had not been previously fielded. The AN/BSY-2 computer system is comprised of over 100 processors interconnected in multiple fault tolerant fiber optic rings. First, a description of the AN/BSY-2 distributed architecture is presented. The message passing semantics are then presented. A key feature of the IPC facility is its support for both synchronous and asynchronous communications based on logical addressing. Logical addressing within the AN/BSY-2 system supports point-to-point as well as group communications, and also supports the fault tolerant requirements of the system. The hardware developed to support fast real time messaging, and support fault tolerance is discussed. Finally, the low level semantics of a message transfer through the system is outlined.
61|3||Contents Volume 61|
62|1|http://www.sciencedirect.com/science/journal/01641212/62/1|The cost of errors in software development: evidence from industry|The search for and correction of errors in software are often time consuming and expensive components of the total cost of software development. The current research investigates to what extent these costs of software error detection and correction contribute to the total cost of software. We initiated the research reported here with the collection of a sample of transactions recording progress on one phase of development of a set of software programs. Each of these projects represented the completion of an identical phase of development (i.e., country localisation) for a different country. This enabled each project to be compared with the other, and provided an unusually high degree of control over the data collection and analysis in real-world empirical study. The research findings relied on programmers' self-assessment of the severity of errors discovered. It found that serious errors have less influence on total cost than errors that were classified as less serious but which occurred with more frequency once these less serious errors are actually resolved and corrected. The research suggests one explanation – that programmers have greater discretion in how and when to resolve these less severe errors. The data supports the hypothesis that errors generate significant software development costs if their resolution requires system redesign. Within the context of the research, it was concluded that uncorrected errors become exponentially more costly with each phase in which they are unresolved, which is consistent with earlier findings in the literature. The research also found that the number of days that a project is open is a log-linear predictor of the number of software errors that will be discovered, implying a bias in error discovery over time. This implies that testing results need to be interpreted in light of the length of testing, and that in practice, tests should take place both before and after systems release.
62|1||A testing approach for large system portfolios in industrial environments|The Year 2000 and the Euro conversion have shown to be a major challenge to IT departments of many organizations. In addition to the project management and software maintenance issues the right regression testing approach has turned out to be a critical factor to the success of these organizations.
62|1||A survey of communication protocol testing|Academic research has made significant advances in the generation of test sequences from formal specifications, and in the development of computer-aided test tools, with the aim of improving the effectiveness of protocol testing. However, this state-of-the-art research is not necessarily state-of-the-practice; these methods and tools are seldom used in the computer and communications industries. There is a big gap between testing practice and research results published in journals and reported at conference. To help the industrialization of academic techniques, empirical studies and a new orientation towards real issues need to be given higher priority by researchers, and real-life test coverage and field experience need to be more frequently reported [66]. This paper presents a literature survey of communication protocol testing. We have focused our survey on the following five areas: test sequence generation methods, test coverage, fault model and prediction, test tools, and experience reports.
62|1||Designing a resourceful fault-tolerance system|This paper examines the feasibility of creating a “resourceful” software fault-tolerance system. Current fault-tolerant methods typically replace a faulty module with a redundant backup version, making no attempt to assess and correct errors in the original module. Error-recovery options are therefore limited by the number of backup modules. In contrast, a resourceful system dynamically generates alternative error-correction strategies. Periodically, the system determines which of its pre-defined goals has not been met, then executes different strategies until its goals are achieved. We outline a resourceful fault-tolerance system that defines recovery goals and specifies separate detection and correction procedures for each goal. When errors are detected, various sequences of correction procedures are examined to identify ones that meet the recovery goals. Implementation issues such as specifying recovery goals, creating recovery options, and reducing runtime overhead are examined. We describe a strategy to increase the efficiency of our method by planning each recovery before implementing it, eliminating strategies expected to be unsuccessful, impractical, or cyclical.
62|1||Fundamental principles of software engineering â a journey|A set of fundamental principles can act as an enabler in the establishment of a discipline; however, software engineering still lacks a set of universally recognized fundamental principles. This article presents a progress report on an attempt to identify and develop a consensus on a set of candidate fundamental principles. A fundamental principle is less specific and more enduring than methodologies and techniques. It should be phrased to withstand the test of time. It should not contradict a more general engineering principle and should have some correspondence with “best practice”. It should be precise enough to be capable of support and contradiction and should not conceal a tradeoff. It should also relate to one or more computer science or engineering concepts. The proposed candidate set consists of fundamental principles which were identified through two workshops, two Delphi studies and a web-based survey.
62|2|http://www.sciencedirect.com/science/journal/01641212/62/2|Key success factors for implementing software process improvement: a maturity-based analysis|We report on a questionnaire survey of key success factors that impact software process improvement (SPI). We analysed responses to identify factors that have a major impact, or no impact, on implementing SPI. We found four factors (reviews, standards and procedures, training and mentoring, and experienced staff) that practitioners generally considered had a major impact on successfully implementing SPI, and a further four factors (internal leadership, inspections, executive support and internal process ownership) that the more mature companies considered had a major impact on successfully implementing SPI. We also identified two factors (estimating tools and reward schemes) that may not have an impact on SPI. We briefly discuss how these factors may be broadly understood in terms of process, people, skills and leadership. We discuss some of the implications of our findings for research and practice.
62|2||Motivators of Software Process Improvement: an analysis of practitioners' views|We present empirical findings from our study of Software Process Improvement (SPI) motivators in 13 UK software companies. Our analysis aims to provide SPI managers with some insight into designing appropriate SPI implementation strategies to maximise practitioner support for SPI.
62|2||Optimizing controllability of an interactive videoconferencing system with Web-based control interfaces|With the rapid advances in computing and communications technologies, interactive multimedia applications, such as videoconferencing, over wide-area networks are becoming feasible. We developed an ATM-based multipoint videoconferencing (AMV) system, which can support continuous presence with high-quality audio/video and provide Web-based control interfaces for conferees to conduct interactive videoconferences in wide-area networks. On the basis of a well-developed multimedia distribution platform, we developed an application development platform, on which we can prototype videoconferencing applications quickly. Through running a prototype AMV system, we obtained many valuable experiment results. This paper reports the practical experience and studies the controllability of the AMV system on basis of a proposed videoconference model. Since there is unpredictable delay in transmitting conferees' interaction requests, in general, we cannot simultaneously achieve high controllability and high interactivity. To gain more insight into this issue, we develop an approach which can dynamically adjust, according to network conditions, the interval for collecting interaction requests to achieve either high controllability or high interactivity. We conduct experiments to validate and evaluate the approach and, moreover, determine the proper parameter values that achieve optimum tradeoff between controllability and interactivity.
62|2||Experimental analysis of specification language diversity impact on NPP software diversity|In order to increase computer system reliability, software fault tolerance methods have been adopted to some safety critical systems including nuclear power plants (NPPs). Prevention of software common mode failures is a crucial problem in software fault tolerance, but an effective method to solve this problem has not yet been found. Our research, to find an effective method to prevent software common mode failure s experimentally examined the impact of specification language diversity on NPP software diversity. Three specification languages were used to compose three requirements specifications, and programmers made 12 product codes from the specifications. From the product codes analysis using fault diversity criteria, we concluded that a diverse specification language method would enhance program diversity through diversification of requirements specification imperfections.
62|2||Design of the Ajanta system for mobile agent programming|We describe the architecture and programming environment of Ajanta, a Java-based system for programming applications using mobile agents over the Internet. Agents are mobile objects which are hosted by servers on the network. Ajanta provides primitives for creating and dispatching agents, securely controlling agents at remote sites, and transferring agents from one server to another. For secure access to server resources by visiting agents, a proxy-based access control mechanism is used. The Ajanta design includes mechanisms to protect an agent's state and prevent misuse of its credentials. We describe the use of migration patterns for programming an agent's travel path. A pattern encapsulates the abstract notion of agent mobility. Pattern composition allows one to build complex travel plans using some basic migration patterns. Finally, we present agent-based distributed applications implemented using the Ajanta system to demonstrate Ajanta's functional capabilities. These include a distributed calendar management system, a middleware for sharing files over the Internet, an agent-based middleware for distributed collaborations, and an agent-based network monitoring system.
62|3|http://www.sciencedirect.com/science/journal/01641212/62/3|ViSta: a tool suite for the visualization of behavioral requirements|ViSta is a tool suite designed to support the requirements specification phase of reactive systems. It enables the user to prepare and analyze a diagrammatic description of requirements using the statechart notation. ViSta includes a template wizard, a graphical editor and a statechart visualization tool. The template wizard guides the user through the steps necessary for the extraction of relevant information from a textual description of requirements. This information is stored in a database that is used by the statechart visualization tool to automatically generate statechart layouts. The statechart visualization tool offers a framework that combines hierarchical drawing, labeling, and floorplanning techniques. Hence, the automatically produced drawings enjoy several important aesthetic properties: they emphasize the natural hierarchical decomposition of states into substates; they have a low number of edge crossings; they have a good aspect ratio, and require a small area. These aesthetic features are invaluable since they allow the user to shift focus from organizing the mental or physical structure of the requirements document to its analysis.
62|3||Heuristic approach for early separated filter and refinement strategy in spatial query optimization|Recently, we proposed an optimization strategy for spatial and non-spatial mixed queries. In the strategy, the filter step and the refinement step of a spatial operator are regarded as individual algebraic operators, and are early separated at the algebraic level by the query optimizer. By doing so, the optimizer using the strategy could generate more diverse and efficient plans than the traditional optimizer. We called this optimization strategy the Early Separated Filter And Refinement (ESFAR).
62|3||A placement strategy of multimedia objects in multimedia information systems|
62|3||A public verifiable copy protection technique for still images|It is well known that a digital signature scheme and digital timestamp are two well-defined security methods for copyright protection. However, these techniques are not suitable for directly dealing with digital images. For this reason, digital watermarking has received considerable attention for copyright protection. Unfortunately, there are still some challenges in the proposed watermarking schemes. Here, we will introduce cryptographic tools into watermarking, in such a way that both the advantages of cryptographic tools and digital watermarking are available. The publicly verifiable property and tolerant distortion property are therefore possible. Based on this idea, a lossless and robust copyright-protection scheme, distinct from conventional watermarking techniques, is proposed. Our scheme is strong enough to resist common image processing, geometric distortions and some intentional attacks. Some of the attacks are still challenges for the proposed watermarking methods. A series of experiments are conducted to prove the robustness property. The StirMark and unZign attacks are adopted as benchmark verification.
62|3||Convertible authenticated encryption scheme|The digital signature provides the functions of integration, authentication, and non-repudiation for the signing message. In some applications, however, the signature only needs to be verified by some specified recipients while keeping the message secret from the public. The authenticated encryption schemes can be used to achieve this purpose. To protect the recipient's benefit in the case of a later dispute, we should further enable the recipient to convert the signature into an ordinary one that can be verified by anyone. Recently, Araki et al. proposed a convertible limited verifier scheme to resolve the problem. Their scheme equips the recipient with the ability to convert the signature into an ordinary one. However, the conversion requires the cooperation of the signer. In the paper, we proposed a convertible authenticated encryption scheme that can easily produce the ordinary signature without the cooperation of the signer. Further, the proposed scheme is more efficient than Araki et al.'s in terms of the computation complexities and the communication costs.
62|3||Security of Lin's image watermarking system|It is shown that a watermarking system based on block-oriented and modular-arithmetic-based embedding and extraction techniques proposed by Lin is insecure. The embedded watermark can be effectively removed without knowing any secret parameters used in the embedding process and the distortion of the resulting image is perceptually invisible.
62|3||Contents Volume 62|
63|1|http://www.sciencedirect.com/science/journal/01641212/63/1|Editorial board|
63|1||A partitioning method for efficient system-level diagnosis|
63|1||Perception differences of software success: provider and user views of system metrics|The success of information systems (ISs) is normally defined as a composite of such performance measures as cost, time, and savings. With few systems being completed on time and within budget, application developers and users have moved to other perspectives that reflect value to organizations. Sometimes these other approaches include feedback from peers, subordinates and various other stakeholders instead of the traditional superior-subordinate performance evaluation models. Noticing the need to improve existing IS success measures for providers and users, common instruments were used to measure difference in perception over the distinct stakeholder groups of IS users and IS staff. Based on confirmatory factor analysis (CFA), the data support significant differences in perceptions between IS users and IS staff on the indicators of IS service, user satisfaction and IS staff job performance. The findings suggest efforts should be made to establish partnerships with all stakeholders to provide a more complete control process for IS development. This may include spelling out the requirements, metrics to be used, an the expectations of the deliverables and the current state of technology. Software metrics of success depend on where an individual is in the organization. Agreeing on the system success cannot occur until there is a mutual understanding of objectives and the purpose of the software.
63|1||Business process reengineering and workflow automation: a technology transfer experience|In the last few years many public and private organizations have been changing the way of thinking their business processes to improve the quality of delivered services while achieving better efficiency and efficacy. This paper presents results and lessons learned from an on-going technology-transfer research project aimed at introducing service and technology innovation within a peripheral public administration while transferring enabling workflow methodologies and technologies to local Small and Medium Enterprises (SMEs). We discuss a process reverse engineering approach and its application in the technology transfer project. We also discuss an approach for evaluation and assessment of workflow technology and present a prototype implementation for a selected process of the subject organization.
63|1||An abbreviated concept-based query language and its exploratory evaluation|Research on the use of conceptual information in database queries has primarily focused on semantic query optimization. Studies on the important aspects of conceptual query formulation are currently not as extensive. Only a relatively small number of works exist in this area. The existing concept-based query languages are similar in the sense that they require the user to specify the entire query path in formulating a query. In this study, we present the Conceptual Query Language (CQL), which does not require entire query paths to be specified but only their terminal points. CQL is an abbreviated concept-based query language that allows for the conceptual abstraction of database queries and exploits the rich semantics of semantic data models to ease and facilitate query formulation. CQL was developed with the aim of providing typical end-users like secretaries and administrators an easy-to-use database query interface for querying and report generation. A CQL prototype has been implemented and currently runs as a front-end to an underlying relational DBMS. A statistical experiment conducted to probe end-users' reaction to using CQL vis-à-vis SQL as a database query language indicates that end-users perform better with CQL and have a better perception of it than of SQL. This paper discusses the design of CQL, the strategies for CQL query processing, and the comparative study between CQL and SQL.
63|1||A capacity planning model of unreliable multimedia service systems|To analyze the performance of multimedia service systems, which have unreliable resources, and to estimate the capacity requirement of the systems, we have developed a capacity planning model using an open queueing network. By acquisition of utilization, queue length of the resources and packet delay, and reliability of the systems, we have derived the service capacity of the systems along with the arrival rates of clients and the failure rates of the resources. We have validated the proposed capacity planning model by comparison of analytic results with simulation output.
63|1||Call for papers|
63|2|http://www.sciencedirect.com/science/journal/01641212/63/2|Editorial board|
63|2||COPS: cooperative problem solving using DCOM|We present a framework that utilizes Distributed Component Object Model (DCOM) for distributed problem solving. The Cooperative Problem Solving (COPS) system provides for the coding and execution of a parallel algorithm by a number of networked computers running Microsoft Windows. We show the effectiveness of the COPS system by implementing parallel breadth-first search used in conjunction with a branch-and-bound algorithm for the Traveling Salesman Problem.
63|2||A simple and general approach to parallelize loops with arbitrary control flow and uniform data dependence distances|Loop distribution is applied to exploit the parallelism to loops. For loops with dependence cycle(s) involving all of the statements embedded in control flow, previous methods have significant restrictions. In this paper, an algorithm is proposed to exploit the parallelism for loops under arbitrary control flow and uniform dependence distances. Experiments with benchmark cited from Vector loops, Parallel loops and Livermore loops showed that between 170 subroutines and 24 kernels tested 11 subroutines and 2 kernels had their parallelism exploited by our proposed method.
63|2||A precise dependence analysis for multi-dimensional arrays under specific dependence direction|In process of automatic parallelizing/vectorizing constant-bound loops with multi-dimensional arrays under specific dependence direction, the Lambda test is claimed to be an efficient and precise data dependence analysis method that can check whether there exist generally inexact `real-valued' solutions to the derived dependence equations. In this paper, we propose a precise data dependence analysis method – the multi-dimensional direction vector I test. The multi-dimensional direction vector I test can be applied towards testing whether there exist generally accurate `integer-valued' solutions to the dependence equations derived from multi-dimensional arrays under specific dependence direction in constant-bound loops. Experiments with benchmark showed that the accuracy rate and the improvement rate for the proposed method are approximately 33.3% and 21.6%, respectively.
63|2||Posting file partitioning and parallel information retrieval|The rapid growth in Internet usages brings new challenges on designing a scalable information retrieval system. To reduce the response time of a query to a large database, we parallelize both CPU computation and disk access of Boolean query processing on a cluster of workstations. The key issue is to partition the inverted file such that, during parallel query processing, each workstation consults only its own locally resident data to complete its task. To achieve this goal, we treat the set of all postings referring to a document ID as an object to be allocated in the develop data placement problem. Following the partitioning by document ID principle, we develop posting file partitioning algorithms to transform a sequential information retrieval system to a parallel information retrieval system. The advantage is that a better speed-up can be achieved by deriving from the fast sequential approach – the compressed posting file. The partitioning schemes are designed to balance work-load of workstations in parallel query processing without increasing the average disk access time per posting. The experiment shows that almost linear speed-up can be achieved and the performance bottleneck in previous work, which parallelize only disk access, can be removed. This work shows that, by using parallel processing technique, it is feasible to build a scalable information retrieval system.
63|2||Treating uncertainty in distributed scheduling|In distributed systems the scheduler of an overloaded node may choose to transfer the execution of one or more tasks to other less busy nodes, in order to minimize their expected service times, or to increase the number of tasks that meet their deadlines, among other criteria. One solution makes use of Bayesian theory to infer the load state of the system and, based on this information, the scheduler of a busy node chooses an “appropriate” node to transfer a task too. The meaning of “appropriate” will be a function of the objectives established in the adopted location policy. In the Bayesian decision method, objectives are represented by a utility function. However, the development of a utility function can be a tricky and somewhat subjective task. In this paper, we describe a new approach that easily maps transfer objectives into useful mathematical expressions, representing location policy objectives by fuzzy sets. The proposed approach was successfully employed to add objectives to a Bayesian decision-based algorithm improving the number of tasks that are executed over time in a distributed real-time system.
63|2||Checkpointing MPI applications on symmetric multi-processor machines using SMPCkpt|Researchers from many different areas have requirements for computational power to solve their specific problems. Symmetric multi-processor (SMP) machines are also widely available and their processing capacity is in demand particularly for applications in areas such as virtual reality and multimedia. Checkpointing provides the backbone for rollback recovery (fault-tolerance), playback debugging, process migration and job swapping. Numerous checkpointing tools have been designed and implemented but few are based on SMP machines for MPI applications. This work designs, develops, and implements SMPCkpt, a checkpointing system for symmetric multi-processor environments. SMPCkpt supports a range of facilities, including transparent checkpointing, fault detection, and rollback recovery. Two coordinated checkpointing algorithms, barrier and non-barrier, are developed and implemented in SMPCkpt that can be used to reduce the execution down time in the presence of failures.
63|2||On the attractiveness of the star network|In defiance of the tremendous research the star network had attracted during the last decade, no parallel machine based on this network has yet been built. In this paper, we explore the possible reasons that made the star network not so viable for actual implementation. First we show that the major deficiencies related to poor scalability, cumbersome routing, and unsuitability for real applications make the construction of star-based parallel machines not so appealing. Furthermore, we show that each of the star network variants solves some of the deficiencies, but fails to solve others and introduces new concerns. We also show that the star network and its variants will find it difficult to compete with simpler topologies such as binary cubes, meshes, and k-ary n-cubes in terms of simplicity and suitability for real applications. The results in this paper can be used in two ways. In one hand, they can be used to discourage further research that might end up useless, and on the other hand they can be used as guidelines for useful research in the star interconnection network and its derivatives.
63|2||Call for papers|
63|3|http://www.sciencedirect.com/science/journal/01641212/63/3|Editorial board|
63|3||The reflective practitioner perspective in software engineering education|This paper focuses on the application of the reflective practitioner (RP) perspective to the profession of software engineering (SE). The RP perspective guides professional people to rethink their professional creations during and after the accomplishment of the creation process. Analysis of the field of SE supports the adoption of the RP perspective to SE in general and to SE education in particular. The RP perspective emphasizes the studio––the basic training method in architecture schools––as the educational environment for design studies. In such studios students develop projects with a close guidance of a tutor. Analysis of the kind of tasks that architecture students are working on and a comparison of these tasks to the problems that SE students are facing, suggest that the studio may be an appropriate teaching method in SE as well. The paper presents the main ideas of the RP perspective and examines its fitness to SE in general and to SE education in particular. The discussion is based on analysis of the RP perspective and of the SE profession, visits to architecture studios, and conversations with tutors in architecture studios and with computing science practitioners.
63|3||Analyzing software science data with partial repeatability|
63|3||DMMX: Dynamic memory management extensions|
63|3||Versioning concurrency control for hard real-time systems|The problems of hard real-time systems scheduling involve not only guaranteeing schedulability but also ensuring that shared data will not be corrupted. Maintaining shared data consistency has long been studied in database systems. In this paper, we discuss the key differences between database concurrency control and concurrency control for hard real-time systems and describe an approach to adapting advanced concurrency control techniques to systems requiring analytic worst-case latency guarantees. We describe an example concurrency control technique which avoids interference between queries and updaters. The versioning technique can be implemented with simple and predictable low-overhead algorithms and data structures and its application to a task set is driven by the schedulability analysis. Performance evaluation results via case studies and simulation experiments are presented that show that the versioning technique can improve pure locking protocols in a variety of settings. In particular, when tasks are computation-intensive, the improvement made by the versioning technique in reducing worst-case blocking and increasing schedulability is most significant.
63|3||Performance evaluation of linear hash structure model in a nested transaction environment|We design and implement a linear hash algorithm in nested transaction environment to handle large amount of data with increased concurrency. Nested transactions allow parallel execution of transactions, and handle transaction aborts, thus provides more concurrency and efficient recovery. We use object-oriented methodology in the implementation which helped in designing the programming components independently. In our model, buckets are modeled as objects and linear hash operations are modeled as methods. The papers contribution is novel in the sense that the system, to our knowledge, is the first to implement linear hashing in a nested transactions environment. We have build a system simulator to analyze the performance. A subtle benefit of the simulator is that it works as the real system with only minor changes.
63|3||FasTLInC: a constraint-based tracing approach|In an approach to software monitoring called Dynamic Monitoring with Integrity Constraints (DynaMICs), integrity constraints are used to monitor program behavior at runtime. The constraints capture domain knowledge, limitations imposed by the design, and assumptions made by programmers. This paper introduces Fast Tracing with Links using Integrity Constraints (FasTLInC), a component of DynaMICs, that manages integrity-constraint specifications, software artifacts, and program state information, permitting tracing of constraints and artifacts, specifically requirements and source code. Because DynaMICs verifies that a program behaves in accordance to constraints, the traceability provided by FasTLInC is significant since the monitor targets the detection of faults that result from ambiguity and changes in requirements, conflicts among requirements, and change in program use. The automated identification of bi-directional links between constraints and code eliminates the laborious task of managing links, which can be problematic because of the evolutionary nature of code.
63|3||Contents|
64|1|http://www.sciencedirect.com/science/journal/01641212/64/1|Editorial board|
64|1||Editorâs Corner|
64|1||Diagnosis of the significance of inconsistencies in object-oriented designs: a framework and its experimental evaluation|This paper presents: (a) a framework for assessing the significance of inconsistencies which arise in object-oriented design models that describe software systems from multiple perspectives, and (b) the findings of a series of experiments conducted to evaluate it. The framework allows the definition of significance criteria and measures the significance of inconsistencies as beliefs for the satisfiability of these criteria. The experiments conducted to evaluate it indicate that criteria definable in the framework have the power to create elaborate rankings of inconsistencies in models.
64|1||Identifying the difficulties of object-oriented development|Identifying the perceptions of developers that use object-oriented (OO) system development techniques is necessary to understand why they are described as difficult to learn and use. Professional developers with a broad range of experience were asked to share their perceptions of the issues that contribute to the difficulties of using OO techniques. The 67 developers primarily were from the tele-communications and systems consulting industries in a large metropolitan area. Seventeen small groups of developers with similar levels of experience completed a 2.5-h group cognitive mapping process using a group support system (GSS). Each GSS session consisted of activities to identify difficult issues, define categories that classify the issues by similarity, rate the importance of the categories for causing difficulties, and identify causal relationships among the categories to form a cognitive map that represents the group’s shared perceptions of the difficulties of using OO techniques. The 1279 issues identified were organized into 141 categories by the 17 groups. These 141 group-generated categories were merged to identify a set of 9 overall categories to allow comparisons of perceptions across groups and levels of experience. The results reveal a common “core” of difficult issues associated with using OO techniques that was shared by novices, intermediates, and experts. Overall category group cognitive maps reveal substantial differences in the causal relationships perceived by novices and experts.
64|1||Reengineering legacy systems for distributed environments|In the last decade, we have seen an increasing use of both the object-oriented paradigm and distributed systems. As a result, there is increasing interest in migrating and reengineering legacy systems to these new hardware technologies and software development paradigms. We define a reengineering environment that assists with the migration of legacy systems to distributed object environments. The reengineering environment includes the methodology and an integrated set of tools that support the implementation of the methodology. The methodology consists of multiple phases. First, we use reverse engineering techniques for program comprehension and design recovery. We then decompose the system into a hierarchy of subsystems by defining relationships between the entities of the underlying paradigm of the legacy system. The decomposition is driven by data mining, software metrics, and clustering techniques. Next, if the underlying paradigm of the legacy system is not object-based, we perform object-based adaptations on the subsystems. We then create components by wrapping objects and defining an interface. Finally, we allocate components to different sites by specifying the requirements of the system and characteristics of the network as an integer-programming model that minimizes the remote communication. We use middleware technologies for the implementation of the distributed object system.
64|1||An empirical study of maintenance and development estimation accuracy|We analyzed data from 145 maintenance and development projects managed by a single outsourcing company, including effort and duration estimates, effort and duration actuals, and function points counts. The estimates were made as part of the company’s standard project estimating process that involved producing two or more estimates for each project and selecting one estimate to be the basis of client-agreed budgets. We found that effort estimates chosen as a basis for project budgets were, in general, reasonably good, with 63% of the estimates being within 25% of the actual value, and an average absolute error of 0.26. These estimates were significantly better than regression estimates based on adjusted function points, although the function point models were based on a homogeneous subset of the full data set, and we allowed for the fact that the model parameters changed over time. Furthermore, there was little evidence that the accuracy of the selected estimates was due to their becoming the target values for the project managers.
64|1||An assessment of systems and software engineering scholars and institutions (1997â2001)|This paper presents the findings of a five-year study of the top scholars and institutions in the systems and software engineering field, as measured by the quantity of papers published in the journals of the field. The top scholar is Richard Lai of LaTrobe University in Australia, and the top institution is Carnegie Mellon University and its Software Engineering Institute. The paper lists the top 15 scholars and institutions.
64|2|http://www.sciencedirect.com/science/journal/01641212/64/2|Editorial board|
64|2||An ERP-client benefit-oriented maintenance taxonomy|The worldwide installed base of enterprise resource planning (ERP) systems has increased rapidly over the past 10 years now comprising tens of thousands of installations in large- and medium-sized organizations and millions of licensed users. Similar to traditional information systems (IS), ERP systems must be maintained and upgraded. It is therefore not surprising that ERP maintenance activities have become the largest budget provision in the IS departments of many ERP-using organizations. Yet, there has been limited study of ERP maintenance activities. Are they simply instances of traditional software maintenance activities to which traditional software maintenance research findings can be generalized? Or are they fundamentally different, such that new research, specific to ERP maintenance, is required to help alleviate the ERP maintenance burden? This paper reports a case study of a large organization that implemented ERP (an SAP system) more than three years ago. From the case study and data collected, we observe the following distinctions of ERP maintenance: (1) the ERP-using organization, in addition to addressing internally originated change-requests, also implements maintenance introduced by the vendor; (2) requests for user-support concerning the ERP system behavior, function and training constitute a main part of ERP maintenance activity; and (3) similar to the in-house software environment, enhancement is the major maintenance activity in the ERP environment, encompassing almost 64% of the total change-request effort. In light of these and other findings, we ultimately: (1) propose a clear and precise definition of ERP maintenance; (2) conclude that ERP maintenance cannot be sufficiently described by existing software maintenance taxonomies; and (3) propose a benefits-oriented taxonomy, that better represents ERP maintenance activities. Three salient dimensions (for characterizing requests) incorporated in the proposed ERP maintenance taxonomy are: (1) who is the maintenance source? (2) why is it important to service the request? and (3) what––whether there is any impact of implementing the request on the installed module(s)?
64|2||EIS data: findings from an evolutionary study|In such a competitive environment as the present one, it is no longer enough for managers to make the right choices, but they must also make and implement them as quickly as possible. For this reason, they need several tools to provide them with information. These tools are called executive information systems (EIS). However, an EIS is only as useful as the data it utilizes. Findings show that quantitative data is the most common type of information provided by the EIS. Operative databases in functional areas constitute the main source of internal data provided by EIS, closely followed by data warehouses. Soft data gives a great value to EIS completing usual data. Despite this fact, none of the EIS analyzed include soft data. The capability to provide access to reliable data from several sources (external and internal) is a major issue in EIS. The use of external data is increasing. The most outstanding sources are the services of financial institutions. Findings show that analyzed EIS are experiencing a transition from the first-generation EIS to second-generation EIS.
64|2||Introduction of accounting capabilities in future service architectures|This paper proposes enhancements to the accounting support capabilities of legacy, standardised service architectures. The TINA service architecture is used as a reference, even though our approach is applicable to other models as well. The key points in this paper are the following. First, the definition of minimal extensions to the standard service architecture components, so as to enable them to offer accounting information. Second, the definition of the interface among the extended service architecture components and the new components that will undertake the accounting functionality. Third, the definition of the functionality of the new components that will undertake the accounting functionality. New components are defined so as to minimally impact the specified service architecture.
64|2||Heterogeneous image database selection on the Web|Image databases on the Web have heterogeneous characteristics since they use different similarity measures and queries are processed depending on their own schemes. In the content-based image retrieval from distributed sites, it is crucial that the metaserver has the capability to find objects, similar to a given query object in terms of the global similarity measure, from different image databases with different local similarity measures. In this paper, we investigate the problem of finding databases, which contain more objects relevant to a given query than other databases, from many image databases dispersed on the Web. This problem is referred to as a database selection problem.
64|2||Detection and restoration of tampered JPEG compressed images|To protect the integrity of digital images, our scheme proposes a technique based on standard JPEG still image compression. The proposed scheme can detect tampered images and can also recover them. Images are compressed using the JPEG compression technique, and the proposed scheme also incorporates an edge detection technique. First, the edge detection technique identifies the edges of the image before image compression takes place. The obtained edge characteristic is then embedded into the image immediately after compression. If the image is tampered during transmission, the embedded edge characteristic can be used to detect the tampered areas, and these tampered areas can be reconstructed using the interpolation method and the embedded edges. Therefore, the proposed scheme allows recipients to know that the image has been tampered with during transmission.
64|2||Sharing multiple secrets in digital images|The share of multiple secrets among participants in secret transmission is a critical topic for a new digital image scheme. The new scheme is derived from the least significant bit substitution method and the visual cryptography method. Given some secrets and a set of cover images, the proposed scheme will convert the secrets into many bit planes and modify the cover images based on these bit planes. In our method, each participant has a unique modified cover image called a stego-image. A pair of participants can therefore reconstruct a unique secret without destroying of its secrecy. Experiments shown that the quality of all stego-images is visually acceptable. The proposed scheme also prevents anyone who possesses only one stego-image from gaining information about the secret and the other stego-image.
64|2||Linguistic kleptomania in computer science|
64|3|http://www.sciencedirect.com/science/journal/01641212/64/3|Editorial board|
64|3||An efficient broadcast data clustering method for multipoint queries in wireless information systems|Mobile computing has become a reality with the convergence of two technologies: powerful portable computers and wireless networks. The restrictions of wireless networks, such as bandwidth and energy limitations make data broadcasting an attractive data communication method. This paper addresses the clustering of wireless broadcast data for multipoint queries. By effective clustering of broadcast data, the mobile client can access the data on the air in short latency. In the paper, we define two affinity measures: data affinity and segment affinity. The data affinity is the degree that two data objects are accessed by queries, and the segment affinity is the degree that two sets of data (i.e., segments) are accessed by queries. Our method clusters data objects based on data and segment affinity measures. We show that the performance of our method is scarcely influenced by the growth of the number of queries.
64|3||A two phase optimization technique for XML queries with multiple regular path expressions|As XML (eXtensible Markup Language) has emerged as a standard for information exchange on the World Wide Web, it has gained attention in database communities to extract information from XML seen as a database model. XML queries are based on regular path queries, which find objects reachable by given regular expressions. To answer many kinds of user queries, it is necessary to evaluate queries that have multiple regular path expressions. However, previous work on subjects such as query rewriting and query optimization in the frame work of semistructured data has usually dealt with a single regular path expression. For queries that have multiple regular path expressions we suggest a two phase optimizing technique: query rewriting using views by finding the mappings from the view’s body to the query’s body and for rewritten queries, evaluating each query conjunct and combining them. We show that our rewriting algorithm is sound and our query evaluation technique is more efficient than that of previous work on optimizing semistructured queries.
64|3||XML query processing using document type definitions|As eXtensible Markup Language (XML) has become an emerging standard for information exchange on the World Wide Web, it has gained attention in database communities to extract information from XML seen as a database model. Data in XML can be mapped to a semistructured data model based on edge-labeled graph, and queries can be processed against it. Here, we propose new query optimization techniques using document type definitions which have the schema information about XML data. Our techniques reduce the large search space significantly while at the same time requiring less memory compared to the traditional index techniques. Also, as they preserve source database’s structure, they can process many kinds of complex queries. We implemented our techniques and provided preliminary performance results.
64|3||An automatic load/extract scheme for XML documents through object-relational repositories|Extensible markup language (XML), a simplified version of standard generalized markup language (SGML), is designed to enable electronic text interchange in the Internet. XML documents have a rigorously described structure that may be analyzed by computers and easily understood by humans. Most current approaches store XML documents in file systems or in relational database systems. However, the nature and the design of file system or relational database schema may cause limitations on fitting with XML document structure. In this paper, we present an automatic load/extract scheme to store and retrieve XML documents through object-relational databases. We propose an architecture, called XML meta-generator (XMG), which, after reading a specific document type definition (DTD), automatically generates the corresponding object-relational database schema (OR-Schema), a DI-Decomposer and a DI-Reconstructor, which are explained as follows:
64|3||An effective query pruning technique for multiple regular path expressions|Regular path expressions are essential for formulating queries over the semistructured data without specifying the exact structure. The query pruning is an important optimization technique to avoid useless traversals in evaluating regular path expressions. While the previous query pruning optimizes a single regular path expression well, it often fails to fully optimize multiple regular path expressions. Nevertheless, multiple regular path expressions are very frequently used in nontrivial queries, and so an effective optimization technique for them is required. In this paper, we present a new technique called the two-phase query pruning that consists of the preprocessing phase and the pruning phase. Our two-phase query pruning is effective in optimizing multiple regular path expressions, and is more scalable and efficient than the combination of the previous query pruning and post-processing in that it never deals with exponentially many combinations of sub-results produced from all the regular path expressions.
64|3||Estimating internal memory fragmentation for Java programs|Dynamic memory management has been an important part of a large class of computer programs and with the recent popularity of object oriented programming languages, more specifically Java, high performance dynamic memory management algorithms continue to be of great importance. In this paper, an analysis of Java programs, provided by the SPECjvm98 benchmark suite, and their behavior, as this relates to fragmentation, is performed. Based on this analysis, a new model is proposed which allows the estimation of the total internal fragmentation that Java systems will incur prior to the programs execution. The proposed model can also accommodate any variation of segregated lists implementation. A comparison with a previously introduced fragmentation model is performed as well as a comparison with actual fragmentation values that were extracted from SPECjvm98. Finally the idea of a test-bed application that will use the proposed model to provide to programmers/developers the ability to know, prior to a programs execution, the fragmentation and memory utilization of their programs, is also introduced. With this application at hand developers as well as designers of applications could better assess the stability, efficiency as well reliability of their applications at compile time.
64|3||Software project management auditsââupdate and experience report|This paper describes software project management audits as they have been defined in the literature. It outlines the motivation for conducting such audits routinely, which differs markedly from earlier recommendations in the literature and outlines how an audit can be conducted efficiently, from both the points of view of the software project manager and the auditing team, using procedures that have not been previously published. Finally, it provides some data on concerns that were found during several audits in which the author has participated and recommends, based on the data, what issues project managers and project management auditors should focus on when planning projects and conducting audits in the future.
64|3||Contents|
65|1|http://www.sciencedirect.com/science/journal/01641212/65/1|Editorial board|
65|1||Practical assessment of the models for identification of defect-prone classes in object-oriented commercial systems using design metrics|The goal of this paper is to investigate and assess the ability of explanatory models based on design metrics to describe and predict defect counts in an object-oriented software system. Specifically, we empirically evaluate the influence of design decisions to defect behavior of the classes in two products from the commercial software domain. Information provided by these models can help in resource allocation and serve as a base for assessment and future improvements.
65|1||Evaluating defect estimation models with major defects|
65|1||From diagnosis to diagnosability: axiomatization, measurement and application|Classical views on testing and their associated testing models are not dealing with the question of fault repairing but only focus on fault detection. Diagnosis consists of determining the nature of a detected fault, of locating it and hopefully repairing it. Correlatively, the only standardized quality factors implied in the detection/repair aspects of software engineering are testability and maintainability: those quality factors are misleading since they do not pinpoint this question of the location/repairing effort, that can be identified under the concept of diagnosability. This paper is thus concerned with diagnosability, its definition and the axiomatization of its expected behavior. The paper aims at:
65|1||LMR, DTA: adaptive communication algorithms for asynchronous real-time distributed systems using token-ring networks|We present two adaptive communication algorithms called LMR and DTA, for asynchronous real-time distributed systems that use IEEE 802.5 token-ring networks and FDDI networks, respectively. The objective of the algorithms is to minimize end-to-end missed-deadline ratio of application tasks. While LMR adapts application tasks to workload fluctuations by reprioritizing trans-node application messages, DTA performs adaptation by dynamically changing token holding times of host machines, respectively. We study the performance of the algorithms through benchmark-driven experimental studies. The performance of the algorithms is compared with an adaptive resource allocation algorithm that performs adaptation by dynamically replicating application processes for load sharing. The experimental results indicate that LMR and DTA outperform the process replication algorithm for load patterns that cause communication latencies to grow faster than execution latencies. Furthermore, we observe that LMR and DTA perform as “good” as the process replication algorithm for load patterns that cause execution latencies to grow faster than communication latencies.
65|1||Software architecture supporting integrated real-time systems|To achieve reliability, reusability, and cost reduction, a significant trend in building large complex real-time systems is to integrate separate application modules of different criticalities in a common hardware platform. An essential requirement of integrated real-time systems is to guarantee spatial and temporal partitioning among applications in order to ensure an exclusive access of physical and temporal resources to the applications. In this paper we propose software architecture, implemented as SPIRIT-Î¼Kernel, for strongly partitioned integrated real-time systems. The SPIRIT-Î¼Kernel has been designed and implemented based on a two-level hierarchical scheduling methodology such that the real-time constraints of each application can be guaranteed. To demonstrate the feasibility of the SPIRIT-Î¼Kernel, we have ported two real-time operating systems (RTOS), WindRiver’s VxWorks and Cygnus’s eCos, on the top of the microkernel. Thus, different RTOS can be applied in various partitions to provide required features for each application. Based on the measured performance results, the SPIRIT-Î¼Kernel architecture is practical and appealing due to its low overheads of kernel services and the support for dependable integration of real-time applications via scheduling algorithm.
65|2|http://www.sciencedirect.com/science/journal/01641212/65/2|Editorial board|
65|2||Assessing the maintenance processes of a software organization: an empirical analysis of a large industrial project|The use of statistical process control methods can determine the process capability of sustaining stable levels of variability, so that processes will yield predictable results. This enables to prepare achievable plans, meet cost estimates and scheduling commitments, and deliver required product functionality and quality with acceptable and reasonable reliability. We present initial results of applying statistical analysis methods to the maintenance processes of a software organization rated at the CMM level 3 that is currently planning the assessment to move to the CMM level 4. In particular, we present results from an empirical study conducted on the massive adaptive maintenance process of the organization. We analyzed the correlation between the maintenance size and productivity metrics. The resulting models allow to estimate the costs of a project conducted according to the adopted maintenance processes. Model performances on future observations were assessed by means of a cross validation which guarantees a nearly unbiased estimate of the prediction error. Data about the single phases of the process were also available, thus allowing to analyze the distribution of the effort among the phases and the causes of variations.
65|2||A comparison of methods for locating features in legacy software|Software engineers frequently need to locate the code that implements a specific feature of a program in order to fix a problem or add an enhancement. Several methods have recently been proposed to aid in feature location, notably the software reconnaissance method, which uses dynamic analysis of traces of execution, and the dependency graph method which involves static tracing of calling and data flow relationships in the program’s dependency graph. Most studies performed so far on these methods have used relatively modern C code. However there is a large body of existing legacy software in Fortran and similar languages which is often much more poorly structured. This paper describes a case study to locate two features in a sample of poorly structured legacy Fortran code. Both methods were applied to locate the features, along with the well known “grep” text search method for comparison. Both the software reconnaissance and dependency graph methods located both features, although some difficulties were encountered and adaptations were needed due to the very tangled nature of the code. The “grep” search method worked well for one of the two features, but was ineffective for the second, more complex case.
65|2||A controlled experiment on inheritance depth as a cost factor for code maintenance|In two controlled experiments we compare the performance on code maintenance tasks for three equivalent programs with 0, 3, and 5 levels of inheritance. For the given tasks, which focus on understanding effort more than change effort, programs with less inheritance were faster to maintain. Daly et al. previously reported similar experiments on the same question with quite different results. They found that the 5-level program tended to be harder to maintain than the 0-level program, while the 3-level program was significantly easier to maintain than the 0-level program. We describe the design and setup of our experiment, the differences to the previous ones, and the results obtained. Ours and the previous experiments are different in several ways: We used a longer and more complex program, made an inheritance diagram available to the subjects, and added a second kind of maintenance task. When taken together, the previous results plus ours suggest that there is no such thing as usefulness or harmfulness of a certain inheritance depth as such. Code maintenance effort is hardly correlated with inheritance depth, but rather depends on other factors (partly related to inheritance depth). Using statistical modeling, we identify the number of relevant methods to be one such factor. We use it to build an explanation model of average code maintenance effort that is much more powerful than a model relying on inheritance depth.
65|2||An empirical investigation of an object-oriented design heuristic for maintainability|This empirical study has two goals. First, to investigate the impact of a design heuristic on the maintainability of object-oriented designs, namely the ‘god class’ problem. In other words, we wish to better understand to what extent a specific design heuristic contributes to the quality of designs developed. The second goal is to investigate the relationship between that OO design heuristic and metrics. Namely, are we able to capture a specific design heuristic by applying a suitable subset of design metrics? The results of this study show that: (a) the investigated design heuristic significantly affects the performance of the participants; (b) it also affects the evolution of design structures; and (c) there is a considerable relationship between that design heuristic and metrics so that it could be feasible to conduct an assessment by using appropriate metrics.
65|2||How well can we predict changes at architecture design time?|Two years ago, we analyzed the architecture of Sagitta 2000/SD, a large business information system being developed on behalf of Dutch Customs. We were in particular interested in assessing the capabilities of the system to accommodate future complex changes. We asked stakeholders to bring forward possible changes to the system, and next investigated how these changes would affect the software architecture. Since then, the system has been implemented and used, and actual modifications have been proposed and realized. We studied all 117 change requests submitted since our initial analysis. The present paper addresses how well we have been able to predict complex changes during our initial analysis, and how and to what extent the process to elicit and assess the impact of such changes might be improved. This study suggests that architecture analysis can be improved if we explicitly challenge the initial requirements. The study also hints at some fundamental limitations of this type of analysis: (1) fundamental modifiability-related decisions need not be visible in the documentation available, (2) the actual evolution of a system remains, to a large extent, unpredictable and (3) some changes concern complex components, and this complexity might not be known at the architecture level, and/or be unavoidable.
65|2||Operational anomalies as a cause of safety-critical requirements evolution|This paper reports the results of a small study of requirements changes to the onboard software of seven spacecraft subsequent to launch. Only those requirement changes that resulted from operational (i.e., post-launch) anomalies were of interest here, since the goal was to better understand the relationship between critical anomalies during operations and how safety-critical requirements evolve. The results of the study were surprising in that anomaly-driven requirements changes during operations were rarely due to previous requirements having been incorrect. Instead, changes involved new requirements either (1) for the software to handle rare but high-consequence events or (2) for the software itself to compensate for hardware failures or limitations. The prevalence of new requirements as a result of post-launch anomalies suggests a need for increased requirements-engineering support of maintenance activities in these systems. The results also confirm both the difficulty and the benefits of pursuing requirements completeness, especially in terms of fault tolerance, during development of critical systems.
65|2||The future of programming languages: evidence to support a midwest university information systems curriculum|The university curriculum for an Information Systems program has become a challenge for colleges and universities. This paper determined the status of programming languages used by business and industry for application development in a regional area located in the Midwest. The paper reflected the percentage of what languages were being used, the increase, decrease, or non-usage of languages, ranked the current and future use of languages, and identified business and industry recommendations for curriculum content in a university program.
65|3|http://www.sciencedirect.com/science/journal/01641212/65/3|Editorial board|
65|3||Guest Editorial|
65|3||Deadlock-free software architectures for COM/DCOM Applications|Many software projects are based on the integration of independently designed software components that are acquired on the market rather than developed within the project itself. Sometimes interoperability and composition mechanisms provided by component based integration frameworks cannot solve the problem of binary component integration in an automatic way. In this paper we present a technique to allow connectors synthesis for deadlock-free component based architectures [IEEE Proceedings of the 16th ASE, 2001] in the context of COM/DCOM applications. This work also provides guidelines to implement an automatic tool that derives the implementation of routing deadlock-free policies within the connector from the dynamic behavior specification of the COM components. Deadlock is then prevented by inserting the synthesized connector within the system via COM composition mechanisms while letting the system COM servers unmodified. We present a successful application of this technique on the (COM version of the) problem known as “The dining philosophers”.
65|3||Enabling predictable assembly|Demands for increased functionality, better quality, and faster time-to-market in software products continue to increase. Component-based development is the software industry’s response to these demands. The industry has developed technologies such as EJB and CORBA to assemble components that are created in isolation. Component technologies available today allow designers to plug components together, but do little to allow the developer to reason about how well they will play together. Predictable assembly focuses on issues related to assembling component-based systems that predictably meet their quality attribute requirements. This paper introduces prediction-enabled component technology (PECT) as a means of packaging predictable assembly as a deployable product. A PECT is the integration of a component technology with one or more analysis technologies. Analysis technologies support prediction of assembly properties and also identify required component properties and their certifiable descriptions. This report describes the major structures of a PECT. It then discusses the means of validating the predictive powers of a PECT, which provides measurably bounded trust in design-time predictions. Last, it demonstrates the above concepts in an illustrative model problem: predicting average end-to-end latency of a ‘soft’ real time application built from off-the-shelf software components.
65|3||Runtime verification of .NET contracts|We propose a method for implementing behavioral interface specifications on the .NET platform. Our interface specifications are expressed as executable model programs. Model programs can be run either as stand-alone simulations or used as contracts to check the conformance of an implementation class to its specification. We focus on the latter, which we call runtime verification.
65|3||Revealing component properties through architectural styles|An underlying assumption in even using the phrase “component certification and system prediction” is that an understanding of individual components’ properties will lead to an understanding of a system’s properties by some form of compositional reasoning. Unfortunately, standard analytical composition techniques suffer from two problems: (1) they require that the internal structure of components be revealed in order to reason about them and (2) they deal clumsily with properties that require analysis of patterns of interaction. Here, based on the observation that a formal software architecture description itself is a constructive composition mechanism, I illustrate how the use of software architecture styles can sometimes alleviate the first problem and solve the latter.
65|3||Trustworthy componentsââcompositionality and prediction|This article defines key requirements for an architecture-based approach to trustworthy components. We then provide a brief overview of our architecture definition language RADL with a focus on compositionality and extra-functional properties.
65|3||Towards a composition model problem based on IEC61850|In order to derive real-world model problems for the breath of research challenges in the area of predictable assembly of certifiable software components, this paper introduces the substation automation domain, as a representative for the wide spectrum of data acquisition and process control systems. Special emphasis in the paper is put on those aspects of the application area, which make its employment as a model problem provider attractive. First, the compositions are required to meet several well defined quality requirements besides functionality and must therefore allow for the prediction of these qualities before the final system is assembled and is undergoing a potentially costly factory and/or onsite acceptance test. Second, the upcoming IEC61850 standard defines agreed upon domain models and quality attribute requirements on system operations. In order to make this problem domain attractive to research in the area of compositional reasoning of component assemblies, this paper provides the needed step of relating the domain models and the domain’s quality attribute requirements to software components and assembly properties.
65|3||Contents Volume 65|
66|1|http://www.sciencedirect.com/science/journal/01641212/66/1|Editorial board|
66|1||Is software work routinized?: Some empirical observations from Indian software industry|This paper tries to find out whether software work is routinized. Six major hypotheses about routinization of software work were derived from the available literature and verified empirically. Data was collected from two software firms in Bangalore (India) using a semi-structured interview schedule and the participatory observation method. Findings of the study are discussed under six broad categories: chameleonic division of work, team work, symmetric information, level playing field, barrierless career and distributed control. The study did not find support for any of the hypotheses and thus the study rejects the routinization thesis. Some possible reasons for why software work is hard to routinize are also given.
66|1||A quantitative and qualitative analysis of factors affecting software processes|Despite the growing body of research on software process improvement (SPI), there is still a great deal of variability in the success of SPI programmes. In this paper, we explore 26 factors that potentially affect SPI. We also consider the research strategies used to study these factors. We have used a multi-strategy approach for this study: first, by combining qualitative and quantitative analysis within case studies; second, by comparing our case study results with the results of a previously conducted survey study. Seven factors relevant to SPI (i.e. executive support, experienced staff, internal process ownership, metrics, procedures, reviews, and training) were identified by the case studies and the survey study. Two factors (reward schemes and estimating tools) were found, by both the case studies and the survey study, not to be relevant to SPI. Three additional factors (people, problems and change) were identified by the case studies. The frequency with which people, problems and change are discussed by practitioners suggests that these three factors may be pervasive in SPI, in a way that the other factors are not. These factors, however, require further investigation.
66|1||De-motivators for software process improvement: an analysis of practitionersâ views|We present a study of software practitioners’ de-motivators for software process improvement (SPI). The aim of this study is to understand the nature of the issues that de-motivate software practitioners for SPI, so that SPI managers can better manage these de-motivators. This study compares what the SPI literature reports as the factors that hinder SPI success with software practitioners’ perception of the factors that de-motivate them. Focus groups are used to elicit the perceptions of over 200 software practitioners. Our findings show that software practitioners confirm what the literature reports as the major issues that de-motivate them for SPI. These issues are related to resistance to change, lack of evidence, imposed SPI initiatives, resource constraints and commercial pressures. Our findings also show that there are differences in de-motivators for SPI across staff groups and that these differences are related to the role that software practitioners have in software development generally. We offer these findings as insight to aid SPI managers to design more targeted SPI strategies.
66|1||Generating test cases from class vectors|Specifications are the primary source for obtaining test cases in software testing. Specification based testing is becoming more and more important when black box components and COTS are widely used in software development. An important issue in system testing is to identify all the legitimate input. One of these systematic approaches is deriving test cases from classification tree. This approach partitions the input domain into classifications which are further partitioned into classes. Test cases are combinations of classes. Relations between classification and classes are identified and are used to construct the classification tree. From the classification tree, combination table is constructed. Test cases are derived from the combination table. However, human decisions are required in determining whether test cases derived from the combination table are legitimate. This problem is incurred by the limitation of the expressive power of classification trees which cannot express the relations among classes precisely. We propose an enhancement by expressing the relations among classes and the relations among classifications directly in vectors. We call this new approach Class Vectors. This paper presents the class vector approach with formal definitions of basic concepts and entities. We find that the expressive power of class vectors is higher than classification trees, that is, this approach can express the information given in the specification in a better way and derive all legitimate test cases with minimal human decisions. Furthermore, a method of generating legitimate test cases using class vectors is described. This method is derived from the formal semantics of class vector, hence it is theoretical sound. Finally, we discussed that the proposed method requires the least amount of human decisions, can be highly automatic and has good usability.
66|1||Optimal testing-resource allocation with genetic algorithm for modular software systems|In software testing, an important issue is to allocate the limited testing resources to achieve maximum reliability. There are numerous publications on this issue, but the models are usually developed under the assumption of simple series or parallel modules. For complex system configuration, the optimization problem becomes difficult to solve. In this paper, we present a genetic algorithm for testing-resource allocation problems that can be used when the software systems structure is complex, and also when there are multiple objectives. We consider both system reliability and testing cost in the testing-resource allocation problems. The approach is easily implemented. Some numerical examples are shown to illustrate the applicability of the approach.
66|1||Detecting associative shift faults in predicate testing|Fault-based predicate testing is a promising strategy for generating effective software test data. The test target is faults that may appear in predicates found in the specification or implementation of a computer program. A hierarchy of fault classes has been recently established, identifying associative shift faults (ASFs) as one of the strongest fault classes in predicate testing. The upto date proposed approaches resolve adequately the issue of test generation for weaker fault classes but do not guarantee the detection of all ASFs. In this paper we define a suitable fault model to represent this type of faults and propose a heuristic test strategy, while trying to keep low the number of required tests by fault simulation. Empirical results of the algorithm application on the TCAS-II expression suite are encouraging and suggest that it is easy to detect a significant subset of ASFs by considering only few additional tests.
66|1||Quality assurance under the open source development model|The open source development model has defied traditional software development practices by generating widely accepted products (e.g., Linux, Apache, Perl) while following unconventional principles such as the distribution of free source code and massive user participation. Those achievements have initiated and supported many declarations about the potential of the open source model to accelerate the development of reliable software. However, the pronouncements in favor or against this model have been usually argumentative, lacking of empirical evidence to support either position. Our work uses a survey to overcome those limitations. The study explores how software quality assurance is performed under the open source model, how it differs from more traditional software development models, and whether some of those differences could translate into practical advantages given the right circumstances. The findings indicate that open source has certainly introduced a new dimension in large-scale distributed software development. However, we also discovered that the potential of open source might not be exploitable under all scenarios. Furthermore, we found that many of the open source quality assurance activities are still evolving.
66|1||Open source softwareââan evaluation|The success of Linux and Apache has strengthened the opinion that the open source paradigm is one of the most promising strategies to enhance the maturity, quality, and efficiency of software development activities. This observation, however, has not been discussed in much detail and critically addressed by the software engineering community. Most of the claims associated with open source appear to be weakly motivated and articulated.
66|2|http://www.sciencedirect.com/science/journal/01641212/66/2|Editorial board|
66|2||Combining techniques to optimize effort predictions in software project management|This paper tackles two questions related to software effort prediction. First, is it valuable to combine prediction techniques? Second, if so, how? Many commentators have suggested the use of more than one technique in order to support effort prediction, but to date there has been little or no empirical investigation to support this recommendation. Our analysis of effort data from a medical records information system reveals that there is little, or even negative, covariance between the accuracy of our three chosen prediction techniques, namely, expert judgment, least squares regression and case-based reasoning. This indicates that when one technique predicts poorly, one or both of the others tends to perform significantly better. This is a particularly striking result given the relative homogeneity of our data set. Consequently, searching for the single “best” technique, at least in this case, leads to a sub-optimal prediction strategy. The challenge then becomes one of identifying a means of determining a priori which prediction technique to use. Unfortunately, despite using a range of techniques including rule induction, we were unable to identify any simple mechanism for doing so. Nevertheless, we believe this remains an important research goal.
66|2||Queueing network analysis: concepts, terminology, and methods|Queueing network analysis can be a valuable tool to analyze network models. However, the vast number and diverse nature of the tools available to analyze a problem can often leave the uninitiated frustrated or bewildered––awash in concepts, terminology, and methods not encountered elsewhere. As a primer for queueing network analysis, this paper emphasizes essential concepts and terminology. Selection of analytical methods based on the type of queueing network is discussed. Analytical methods are demonstrated using numerous examples and references for further study into advanced analysis are included throughout.
66|2||Dynamic class-based queue management for scalable media servers|Real-time media servers are becoming increasingly important due to the rapid transition of the Internet from text- and graphics-based applications to multimedia-driven environments. In order to meet these ever increasing demands, real-time media servers are responsible for supporting a large number of clients with a heterogeneous mix of quality of service (QoS) requirements. In this paper, we propose a dynamic class-based queue management scheme that effectively captures the trade-off between scalability and QoS granularity in a media server. We examine the adaptiveness of the scheme and its integration with the existing schedulers. Finally, we evaluate the effectiveness of the proposed scheme through extensive simulation studies.
66|2||3-Disjoint gamma interconnection networks|In this paper, we propose a new multistage interconnection network, called 3-disjoint gamma interconnection network (3DGIN). The 3DGIN is a modified gamma interconnection network that provides 3-disjoint paths to tolerate two switch or link faults between any source and destination pairs. The 3DGIN has lower hardware cost than GIN; furthermore, the routing and rerouting tags to generate 3-disjoint paths can be obtained in O(logN) time. To show the advantage features of 3DGIN, we also make a comparison between the gamma-related networks, the GIN, enhanced IADM, and 3DGIN.
66|2||A stochastic software reliability model with imperfect-debugging and change-point|
66|2||API documentation with executable examples|The rise of component-based software development has created an urgent need for effective application program interface (API) documentation. Experience has shown that it is hard to create precise and readable documentation. Prose documentation can provide a good overview but lacks precision. Formal methods offer precision but the resulting documentation is expensive to develop. Worse, few developers have the skill or inclination to read formal documentation.
66|2||Variable-size data item placement for load and storage balancing|The rapid growth of Internet brings the need for a low cost high performance file system. Two objectives are to be pursued in building such a large scale storage system on multiple disks: load balancing and storage minimization. We investigate the optimization problem of placing variable-size data items onto multiple disks with replication to achieve the two objectives. An approximate algorithm, called LSB_Placement, is proposed for the optimization problem. The algorithm performs bin packing along with MMPacking to obtain a load balanced placement with near-optimal storage balancing. The key issue in deriving the algorithm is to find the optimal bin capacity for the bin packing to reduce storage cost. We derive the optimal bin capacity and prove that LSB_Placement algorithm is asymptotically 1-optimal on storage balancing. That is, when the problem size exceeds certain threshold, the algorithm generates a load balanced placement in which the data sizes allocated on disks are almost balanced. We demonstrate that, for various Web applications, a load balanced placement can be generated with disk capacity not exceeding 10% more than the balanced storage space. This shows that the LSB_Placement algorithm is useful in constructing a low cost and high performance storage system.
66|2||Design and implementation of an Internet-based medical image viewing system|Electronic medical image viewers combined with picture archival and communication systems have become a common part of how hospitals manage their images. Typically, these electronic viewers are platform-dependent and are designed to make use of images stored on a workstation or local area network. However, the rise of the Internet and the ubiquity of the Web browser open up a wide range of new possibilities for distributing and accessing these images. The Internet allows us to pull information distributed across many geographically separated data sources, while the Web browser provides a common environment from where programs can be launched. This document details the design of an Internet-based, platform-independent medical image viewing system that harnesses the potential of these technologies.
66|3|http://www.sciencedirect.com/science/journal/01641212/66/3|Editorial board|
66|3||Software architecture â Engineering quality attributes|
66|3||Linking usability to software architecture patterns through general scenarios|Usability is an important quality attribute to be considered during software architecture design. Up to this point, usability has been served only by separating a system’s user interface from its functionality to support iterative design. However, this has the effect of pushing revisions to achieve usability toward the end of the software development life cycle. Many usability benefits link directly to a variety of architectural tactics in addition to separation of the user interface and these benefits can be discovered early in the life cycle. For each of 27 scenarios, we identified potential usability benefits a user could realize and an architectural pattern that supports achievement of those benefits. We organized the scenarios into an emergent hierarchy of potential benefits to the user and into an emergent hierarchy of architectural tactics used in the supporting patterns. The range of architectural tactics identified in this hierarchy demonstrates that separation is far from the only architectural tactic necessary to support usability. We present techniques that permit important usability issues to be addressed proactively at architecture design time instead of retroactively after user testing.
66|3||From problem to solution with quality attributes and design aspects|It is commonly accepted that quality attributes shape the architecture of a system. There are several means via which the architecture can support certain quality attributes. For example, to deal with reliability the system can be decomposed into a number of fault containment units, thus avoiding fault propagation. In addition to structural issues of architecture, qualities also influence architectural rules and guidelines, such as coding standards. In this paper we will focus on design aspects as a means of supporting quality attributes. An example of a design aspect is error handling functionality, which supports reliability. Quality attributes play a role in the problem domain; design aspects are elements in the solution domain.
66|3||Patterns and performance of distributed real-time and embedded publisher/subscriber architectures|This paper makes four contributions to the design and evaluation of publisher/subscriber architectures for distributed real-time and embedded (DRE) applications. First, it illustrates how a flexible publisher/subscriber architecture can be implemented using standard CORBA middleware. Second, it shows how to extend the standard CORBA publisher/subscriber architecture so it is suitable for DRE applications that require low latency and jitter, periodic rate-based event processing, and event filtering and correlation. Third, it explains how to address key performance-related design challenges faced when implementing a publisher/subscriber architecture suitable for DRE applications. Finally, the paper presents benchmarks that empirically demonstrate the predictability, latency, and utilization of a widely used real-time CORBA publisher/subscriber architecture. Our results demonstrate that it is possible to strike an effective balance between architectural flexibility and real-time quality of service for important classes of DRE applications.
66|3||Quality-driven software re-engineering|Software re-engineering consists of a set of activities intended to restructure a legacy system to a new target system that conforms with hard and soft quality constraints (or non-functional requirements, NFR). This paper presents a framework that allows specific NFR such as performance and maintainability to guide the re-engineering process. Such requirements for the migrant system are modeled using soft-goal interdependency graphs and are associated with specific software transformations. Finally, an evaluation procedure at each transformation step determines whether specific qualities for the new migrant system can be achieved.
66|3||Reliability prediction for component-based software architectures|One of the motivations for specifying software architectures explicitly is the use of high level structural design information for improved control and prediction of software system quality attributes. In this paper, we present an approach for determining the reliability of component-based software architectures.
66|3||Analysis of a software product line architecture: an experience report|This paper describes experiences with the architectural specification and tool-assisted architectural analysis of a mission-critical, high-performance software product line. The approach used defines a “good” product line architecture in terms of those quality attributes required by the particular product line under development. Architectures are analyzed against several criteria by both manual and tool-supported methods. The approach described in this paper provides a structured analysis of an existing product line architecture using (1) architecture recovery and specification, (2) architecture evaluation, and (3) model checking of behavior to determine the level of robustness and fault tolerance at the architectural level that are required for all systems in the product line. Results of an application to a software product line of spaceborne telescopes are used to explain the approach and describe lessons learned.
66|3||Quality-driven software architecture composition|This paper discusses an approach for the top–down composition of software architectures. First, an architecture is derived that addresses functional requirements only. This architecture contains a number of variability points which are next filled in to address quality concerns. The quality requirements and associated architectural solution fragments are captured in a so-called feature-solution (FS) graph. The solution fragments captured in this graph are used to iteratively compose an architecture driven by quality requirements. Our versatile composition technique allows for pre- and post-refinements, refinements that involve multiple variability points, and functionality extensions. In addition, the usage of the FS-graph supports aspect-oriented programming at the architecture level.
66|3||Contents Volume 66|
67|1|http://www.sciencedirect.com/science/journal/01641212/67/1|Editorial board|
67|1||Towards a systematic approach to the capture of patterns within a business domain|Although the reuse of patterns within business domains has many potential benefits, a lack of a situational method for sorting through the immensity of domain knowledge towards the capture of patterns may constitute a handicap. Realizing the difficulty associated with pattern development due to its empirical and knowledge-intensive nature, we propose a method to aid in the process of capturing and reusing patterns in a business domain. In this paper, we describe the first stage of the method dedicated to the capture of patterns. Our approach to pattern development is based on domain analysis principles and is process-oriented, so as to ensure a progressive and increasing understanding of the business domain and the awareness of new opportunities for improving business. We illustrate the application of the pattern development approach within the clothing manufacturing domain in the context of a business process improvement project and report the experiences obtained.
67|1||OS Portal: an economic approach for making an embedded kernel extensible|With the rapid development of embedded system techniques and Internet technologies, network-enabled embedded devices have grown in their popularity. One critical design trend of such devices is that they are shifting from static and fixed-function systems to more dynamic and extensible ones, which are capable of running various kinds of applications. To support the diversity of the applications, kernels on these devices must be extensible. However, making embedded kernels extensible is challenging due to the shortage of resources on these devices.
67|1||A dynamic protocol conformance test method|
67|1||Improving system performance in contiguous processor allocation for mesh-connected parallel systems|Fragmentation is the main performance bottleneck of large, multiuser parallel computer systems. Current contiguous processor allocation techniques for mesh-connected parallel systems are restricted to rectangular submesh allocation strategies causing significant fragmentation problems. This paper presents an L-shaped submesh allocation (LSSA) strategy, which lifts the restriction on the rectangular shape formed by allocated processors in order to address the problem of fragmentation. LSSA can manipulate the shape of the required submesh to fit into the fragmented mesh system. As other strategies, LSSA first tries to allocate the conventional rectangular submeshes. If it fails, LSSA further tries to allocate more flexible L-shaped submeshes instead of signaling the allocation failure. Thus, LSSA accommodates incoming jobs faster than other strategies and results in the reduction of job response time. Extensive simulations show that LSSA performs more efficiently than other strategies in terms of the external fragmentation, the job response time and the system utilization.
67|2|http://www.sciencedirect.com/science/journal/01641212/67/2|Editorial board|
67|2||CASE tool evaluation: experiences from an empirical study|While research activity in software engineering often results in the development of software tools and solutions that are intended to demonstrate the feasibility of an idea or concept, any resulting conclusions about the degree of success attained are rarely substantiated through the use of supporting experimental evidence. As part of the development of a prototype computer assisted software engineering (CASE) tool intended to support opportunistic software design practices, we sought to evaluate the use of the tool by both experienced and inexperienced software engineers. This work involved performing a review of suitable techniques, and then designing and performing a set of experimental studies to obtain data which could be used to assess how well the CASE tool met its design goals. We provide an assessment of how effective the chosen evaluation process was, and conclude by identifying the need for an ‘evaluation framework’ to help with guiding such studies.
67|2||A morphology-driven string matching approach to Arabic text searching|In this paper, a morphological matching approach based on the lexical structure of Arabic words is introduced. Given a root R, the principle underlying morphological matching is to search for R in a text T to find all its occurrences. An algorithm is presented and its empirical and running time performance is analyzed. The results of searching experiments show that morphological matching provides high precision and recall ratios. But, since morphological searching is probabilistic, the results of experiments involved a number of errors caused by weak verbs. The analysis of running time performance indicates that morphological searching can be as efficient as pattern matching, if the root is entered by the user.
67|2||Threshold signature scheme using self-certified public keys|
67|2||Controlling access in large partially ordered hierarchies using cryptographic keys|The problem of access control in a hierarchy is present in many application areas. Since computing resources have grown tremendously, access control is more frequently required in areas such as computer networks, database management systems, and operating systems. Many schemes based on cryptography have been proposed to solve this problem. However, previous schemes need large values associated with each security class. In this paper, we propose a new scheme to solve this problem achieving the following two goals. One is that the number of keys is reduced without affecting the security of the system. The other goal is that when a security class is added to the system, we need only update a few keys of the related security classes with simple operations.
67|2||Lightweight agents for intrusion detection|We have designed and implemented an intrusion detection system (IDS) prototype based on mobile agents. Our agents travel between monitored systems in a network of distributed systems, obtain information from data cleaning agents, classify and correlate information, and report the information to a user interface and database via mediators.
67|2||A hybrid authentication protocol for large mobile network|As the rapid development of wireless LAN and mobile network layer protocol Mobile-IP, a mobile user is allowed to access the service at the visited domain after he has been authenticated. The designing criteria of the inter-domain authentication protocols include: the scalability, the communication efficiency and the computational efficiency, and the robustness of security. In this article, we first show the weakness of some existing protocols against the session key compromise, and then propose a new and efficient inter-domain authentication protocol. Based on public key, challenge–response and hash chaining, this new approach simultaneously achieves several practical merits: (1) good scalability, (2) low communication cost and low computational cost, and (3) resistance to the session key compromise attack.
67|2||Detecting and restoring the tampered images based on iteration-free fractal compression|In current research, fractal image compression scheme has been used to compress images. In this paper, we apply this technique to refine characteristic values of a specific image and embed the characteristic values into the least significant bits of pixels in the image. Once the picture is damaged by an intruder, the system can detect the tampered blocks and restore them using characteristic values without requiring the original image. From experimental results, we see that the proposed scheme can precisely achieve the detection of a damaged image and improve the performance of image restoration.
67|3|http://www.sciencedirect.com/science/journal/01641212/67/3|Editorial board|
67|3||ARDIN extension for virtual enterprise integration|Virtual enterprise integration is the task of improving the performance of a temporary alliance of globally distributed independent enterprises that participate in the different phases of the life cycle of a product or service by efficiently managing the interactions among the participants. This is a very complex task that involves different approaches regarding technology, management and cultural elements. There are different proposals for enterprise integration (usually called Reference Architectures) that have been very useful in applications for a single-enterprise. However, they need to be adapted to support the new requirements that appear in virtual enterprise integration. This paper shows the modifications applied to ARDIN (Spanish acronym of Reference Architecture for INtegrated Development) to help in the design and management of an efficient and flexible virtual enterprise. The modifications are synthesized in a methodology, a set of reference models of best business practices, and in the design of a technological infrastructure.
67|3||An empirical comparison and characterization of high defect and high complexity modules|We analyzed a large set of complexity metrics and defect data collected from six large-scale software products, two from IBM and four from Nortel Networks, to compare and characterize the similarities and differences between the high defect (HD) and high complexity modules. We observed that the most complex modules often have an acceptable quality and HD modules are not typically the most complex ones. This observation was statistically validated through hypothesis testing. Our analyses also indicated that the clusters of modules with the highest defects are usually those whose complexity rankings are slightly below the most complex ones. These results should help us better understand the complexity behavior of HD modules and guide future software development and research efforts.
67|3||GM-WTA: An efficient workflow task allocation method in a distributed execution environment|A workflow is a collection of units of work called workflow tasks which cooperatively realize a business objective by utilizing system resources such as databases. The roles of workflow tasks are performed in the order driven by a computer representation of the workflow logic. During completing each task’s role, the remote control transfers and the remote resource accesses may often occur in a distributed workflow system. Hence, the efficient distribution of workflow components, especially workflow tasks, is so effective as to improve the performance of workflow processing. If we can place adjacent workflow tasks as close as possible and locate workflow tasks near to the required resources, we can significantly reduce the overhead of workflow processing.
67|3||Mining association rules on significant rare data using relative support|Recently, data mining, a technique to analyze the stored data in large databases to discover potential information and knowledge, has been a popular topic in database research. In this paper, we study the techniques discovering the association rules which are one of these data mining techniques. And we propose a technique discovering the association rules for significant rare data that appear infrequently in the database but are highly associated with specific data. Furthermore, considering these significant rare data, we evaluate the performance of the proposed algorithm by comparing it with other existing algorithms for discovering the association rules.
67|3||Web-application centric object prefetching|The popularity of the Internet has produced an increased demand on performance and functionality in the form of Web-applications. A Web-application is a collection of logically related hyperlink Web pages that provide a specific service to Web clients. The combination of a hyperlink predictor in conjunction with Web-applications has the potential to reduce the loading time of Web-application pages. We propose a Web-application centric object prefetcher that augments the customary demand-fetching mechanism founded in most Web browsers utilized for object caching. This technique is complimentary to the operations of Web browser and proxy server caching. It predicts Web-application users’ click actions in order to launch requests for future Web page objects, such that they are available at the client’s system when they are needed. We have conducted trace-driven simulations that indicate the possible performance gains associated with Web-application centric prefetching, when combining demand fetching and prefetching within a Web-application’s bounded hyperlink domain.
67|3||An inexact model matching approach and its applications|Matching between two model specifications is the key step of the repository-based model reuse. From the approximate specification point of view, this paper presents a quantified inexact matching theory for flexible model retrieval from large-scale model repositories. The theory specifies a model repository as two levels: a model level based on a multi-valued model specialization relationship and a fundamental function level based on a function specialization relationship. The matching degree between two models depends on their matching functions. The matching degree between two functions on a function specialization graph depends on the function-distance between them. A set of model specialization rules enables a new matching to be derived from the existing matchings. Embedded in an SQL-like command, the theory has been applied to a large-scale mathematical software model repository system. Users can use the command to retrieve the required models with an inexact query condition. Applications show that the approach is useful and tractable.
67|3||A method of formal requirement analysis for NPP I&C systems based on UML modeling with software cost reduction|
67|3||Contents Volume 67|
68|1|http://www.sciencedirect.com/science/journal/01641212/68/1|Editorial board|
68|1||Information systems project management: an agency theory interpretation|The failure rate of information systems development projects is high. Agency theory offers a potential explanation for it. Structured interviews with 12 IS project managers about their experiences managing IS development projects show how it can be used to understand IS development project outcomes. Managers can use the results of the interviews to improve their own IS project management. Researchers can use them to examine agency theory with a larger number of project managers.
68|1||SmartTutor: An intelligent tutoring system in web-based adult education|This paper describes the design of SmartTutor, an intelligent tutoring system implemented for distance learning in Hong Kong. Many projects and researches on online distance learning have been carried out in these few years. Most of them emphasized on the application of multimedia elements, but did not pay much attention on two crucial elements: personalization and intelligent tutoring, which are important for life-long/adult education. School of Professional and Continuing Education, The University of Hong Kong (HKU SPACE) has developed SmartTutor to address these two issues. This project can be treated as a case study of combining Internet technology, education research, and artificial intelligence. The SmartTutor has been integrated into the SPACE Online Universal Learning platform (an online learning platform), which provides support to courses offered by HKU SPACE. The effectiveness of SmartTutor has been evaluated and the results are very positive.
68|1||Performance analysis of five interprocess communication mechanisms across UNIX operating systems|The availability of a variety of UNIX implementations necessitates the evaluation of their performance with respect to specific application characteristics. This paper concentrates on the performance evaluation of five different Interprocess Communication (IPC) mechanisms––pipes, FIFOs, messages, shared memory (with semaphores), and UNIX domain sockets. Benchmark programs were created for each mechanism, simulating a simple Producer/Consumer message transfer problem. Results were obtained for six UNIX releases––Linux 2.2.5-15, Linux 2.2.17, Linux 2.4.0-test9, RTLinux v2.3, FreeBSD 4.1, and FreeBSD 4.2. The UNIX source code was rigorously examined, and additional tests were created to determine implementation differences that would explain performance variations. Identified causes for performance differences include memory allocation schemes, transfer buffer sizes, data transfer mechanisms, locking mechanism implementations, and underlying code complexity. For all IPC mechanisms with the exception of shared memory, Linux 2.2.5-15 exhibited the best performance, followed by the remaining Linux kernels, with the FreeBSD releases finishing last. When compared against each other, pipes outperformed the other mechanisms. The developed benchmarks and ancillary tests attempt to isolate each aspect of the tested IPC mechanisms to facilitate a full understanding of IPC at the source code level.
68|1||Amorphous program slicing|Traditional, syntax-preserving program slicing simplifies a program by deleting components (e.g., statements and predicates) that do not affect a computation of interest. Amorphous slicing removes the limitation to component deletion as the only means of simplification, while retaining the semantic property that a slice preserves the selected behaviour of interest from the original program. This leads to slices which are often considerably smaller than their syntax-preserving counterparts.
68|1||Designing electronic reference documentation for software component libraries|Contemporary software development is based on global sharing of software component libraries. As a result, programmers spend much time reading reference documentation rather than writing code, making library reference documentation a central programming tool. Traditionally, reference documentation is designed for textbooks even though it may be distributed online. However, the computer provides new dimensions of change, evolution, and adaptation that can be utilized to support efficiency and quality in software development. What is difficult to determine is how the electronic text dimensions best can be utilized in library reference documentation.
68|1||An assessment of systems and software engineering scholars and institutions (1998â2002)|This paper presents the findings of a five-year study of the top scholars and institutions in the Systems and Software Engineering field, as measured by the quantity of papers published in the journals of the field. The top scholar is Khaled El Emam of the Canadian National Research Council, and the top institution is Carnegie Mellon University and its Software Engineering Institute.
68|2|http://www.sciencedirect.com/science/journal/01641212/68/2|Editorial board|
68|2||Techniques for efficiently allocating persistent storage|Efficient disk storage is a crucial component for many applications. The commonly used method of storing data on disk using file systems or databases incurs significant overhead which can be a problem for applications which need to frequently access and update a large number of objects. This paper presents efficient algorithms for managing persistent storage which usually only require a single seek for allocations and deallocations and allow the state of the system to be fully recoverable in the event of a failure. We have developed a portable implementation of our algorithms in Java. Results in this paper demonstrate the superiority of our approach over file systems and databases for Web-related workloads. Our system has been a crucial component for persistently storing data at a number of highly accessed Web sites. We describe our experiences from a large real deployment of our system.
68|2||Usability-based caching of query results in OLAP systems|In this paper we propose a new cache management scheme for online analytical processing (OLAP) systems based on the usability of query results in rewriting and processing other queries. For effective admission and replacement of OLAP query results, we consider the benefit of query results not only for recently issued queries but for the expected future queries of a current query. We exploit semantic relationships between successive queries in an OLAP session, which are derived from the interactive and navigational nature of OLAP query workloads, in order to classify and predict subsequent future queries. We present a method for estimating the usability of query results for the representative future queries using a probability model for them. Experimental evaluation shows that our caching scheme using the past and future usability of query results can reduce the cost of processing OLAP query workloads effectively only with a small cache size and outperforms the previous caching strategies for OLAP systems.
68|2||On avoiding remote blocking via real-time concurrency control protocols|Locking protocols for hard-real-time systems have not generalized well from uniprocessors to multiprocessors. Bounding and reducing so-called “remote blocking” is widely recognized as an important problem for hard-real-time computing. We describe a combination of locking and versioning protocols together with a “chopping” analysis to shorten critical sections. Selective application of chopping and versioning reduces remote blocking and relaxes constraints imposed by pure locking protocols for multiprocessors. Using the same design-time information required for schedulability analysis in pure locking protocols, the integrated locking and versioning protocol can be implemented using only simple data structures with tunable bounded overheads and worst-case access times. Performance evaluation results via case studies and simulation experiments are presented that show that the protocol can improve pure locking protocols in a variety of settings. Experiments show that in some cases the number of tasks that can be guaranteed schedulability by our integrated protocol is 2.5 times more than the number of tasks that can be guaranteed schedulability by pure locking protocols.
68|2||Concurrency control in real-time broadcast environments|Owing to the unique characteristics of real-time broadcast environments, serializability is too strong as a correctness criterion and not suitable for mobile real-time transactions. Considering that relaxing serializability such as epsilon and similarity serializability may sacrifice database consistency to some extent, we propose using a correctness criterion called weak serializability. In this paper, we formally define weak serializability at first. After the necessary and sufficient conditions for weak serializability are shown, corresponding concurrency control protocol based on this criterion is outlined for real-time broadcast environments. Finally, in a series of simulation studies, experimental results show that the proposed protocol helps more mobile real-time transactions to meet their deadlines and improves response time while database consistency is maintained.
68|2||High performance distributed real-time commit protocol|In a distributed real-time database system, the only way to ensure transaction atomicity is to investigate and develop a real-time atomic commit protocol. This paper presents the model of distributed real-time transaction and analyses all kinds of dependencies because of data conflicts access. Based on this model, we propose an optimistic real-time commit protocol, double space commit (2SC), which is specifically designed for the high-performance distributed real-time transaction. 2SC allows a non-healthy transaction to lend its held data to the transactions in its commit dependency set. When the prepared transaction aborts, only the transactions in its abort dependency set are aborted while the transactions in its commit dependency set will execute as normal. The two properties of 2SC can reduce the data inaccessibility and the priority inversion that is inherent in distributed real-time commit processing. Extensive simulation experiments have been performed to compare the performance of the 2SC with that of other protocols such as the base protocol, the permits reading of modified prepared-data for timeliness [IEEE Transactions on Parallel and Distributed Systems 11 (2) (2000) 160–181] and the deadline-driven conflict resolution [The Computer Journal 42 (8) (1999) 674–692]. The simulation results show that 2SC has the best performance. Furthermore, it is easy to incorporate in current concurrency control protocols.
68|2||Modeling and verification of a class of real-time systems by the use of High Level Petri Nets|Homogeneous, shared memory multiprocessors that incorporate real-time operating systems constitute in many corporations the basic platforms for developing applications of plant monitoring and automation. In this work, a template model based on the High Level Petri Net (HLPN) formalism is proposed for this class of computers. Mapping functional and timing requirements of the application software to states of this model and searching for their existence in the reachability tree of the net can verify the satisfaction of these requirements. A state searching algorithm has been developed for the case of a shared memory multiprocessor in which there is a bus-based interconnection network supporting a single communication channel for the exchange of data among the CPUs, the common memory and the computer interfaces. This algorithm groups the infinite number of states of the HLPN to a number of finite regions, identifies the region, which the desired state belongs to, and checks for the existence of a path from the initial state that leads to this region. In order to demonstrate the use of the template in the modeling and verification of timing and functional specifications of a system of the considered class, the implementation of the automation functions of a chemical reactor by a VME-bus based multiprocessor with two CPUs and running under the control of the OS-9 operating system was studied. In this study the template model was used to create a specific for this application HLPN model. The response times of two automation functions were predicted by the use of this model and compared with those derived from the operating characteristics of the reactor.
68|3|http://www.sciencedirect.com/science/journal/01641212/68/3|Editorial board|
68|3||Best papers on Software Engineering from the SEKEâ01 Conference|
68|3||Handling variant requirements in domain modeling|Domain models describe common and variant requirements for a family of similar systems. Although most of the notations, such as UML, are meant for modeling a single system, they can be extended to model variants. We have done that and applied such extended notations in our projects. We soon found that our models with variants were becoming overly complicated, undermining the major role of domain analysis which is understanding. One variant was often reflected in many models and any given model was affected by many variants. The number of possible variant combinations was growing rapidly and mutual dependencies among variants even further complicated the domain model. We realized that our purely descriptive domain model was only useful for small examples but it did not scale up. In this paper, we describe a modeling method and a Flexible Variant Configuration tool (FVC for short) that alleviate the above mentioned problems. In our approach, we start by modeling so-called domain defaults, i.e., requirements that characterize a typical system in a domain. Then, we describe variants as deltas in respect to domain defaults. The FVC interprets variants to produce customized domain model views for a system that meets specific requirements. We implemented the above concepts using commercial tools Netron Fusion™ and Rational Rose™. In the paper, we illustrate our domain modeling method and tool with examples from the Facility Reservation System domain.
68|3||A conceptual model completely independent of the implementation paradigm|Several authors have pointed out that current conceptual models have two main shortcomings. First, they are clearly oriented to a specific development paradigm (structured, objects, etc.). Second, once the conceptual models have been obtained, it is really difficult to switch to another development paradigm, because the model orientation to a specific development approach. This fact induces problems during development, since practitioners are encouraged to think in terms of a solution before the problem at hand is well understood, thus anticipating perhaps bad design decisions.
68|3||Bridging models across the software lifecycle|Numerous notations, methodologies, and tools exist to support software system modeling. While individual models help to clarify certain system aspects, the large number and heterogeneity of models may ultimately hamper the ability of stakeholders to communicate about a system. A major reason for this is the discontinuity of information across different models. In this paper, we present an approach for dealing with that discontinuity. We introduce a set of “connectors” to bridge models, both within and across the “upstream” activities in the software development lifecycle (specifically, requirements, architecture, and design). While the details of these connectors are dependent upon the source and destination models, they share a number of underlying characteristics. These characteristics can be used as a starting point in providing a general understanding of software model connectors. We illustrate our approach by applying it to a system we have designed and implemented in collaboration with a third-party organization.
68|3||Evaluating dynamic correctness properties of domain reference architectures|
68|3||Application of an evaluation framework for analyzing the architecture tradeoff analysis methodSM|Evaluation is a critical analytical process in all disciplines and fields and therefore also in software engineering. For developing and analyzing an evaluation method a framework of six basic components (target, evaluation criteria, yardstick, data-gathering techniques, synthesis techniques, and evaluation process) can be applied. This framework was developed based on the analysis of theoretical and methodological evaluation concepts applied in software and non-software disciplines. In particular, in this paper we present the application of the framework for analyzing the architecture tradeoff analysis methodSM (ATAMSM), developed by the Software Engineering Institute (SEI). The results of the matching of the framework with the ATAM definition facilitate the identification of each evaluation component and stress some key aspects, such as the relevant role of stakeholders and the significance of attribute-based architectural styles in an ATAM evaluation.
68|3||Temporal logic properties of Java objects|Applying finite-state verification techniques to software systems looks attractive because they are capable of detecting very subtle defects in the logic design of these systems. Nevertheless, the integration of existing formal verification tools within programming environments is not yet easy, mainly because of the semantic gap between widely used programming languages and the languages used to describe system requirements. In this paper, we propose a formal requirement specification notation based on linear temporal logic, with regard to object oriented program elements, such as classes and interfaces. The specification is inherently object oriented and is meant for the verification of concurrent and distributed software systems.
68|3||Software effort estimation by analogy and âregression toward the meanâ|Estimation by analogy is, simplified, the process of finding one or more projects that are similar to the one to be estimated and then derive the estimate from the values of these projects. If the selected projects have an unusual high or low productivity, then we should adjust the estimates toward productivity values of more average projects. The size of the adjustments depends on the expected accuracy of the estimation model. This paper evaluates one adjustment approach, based on the findings made by Sir Francis Galton in the late 1800s regarding the statistical phenomenon “regression toward the mean” (RTM). We evaluate this approach on several data sets and find indications that it improves the estimation accuracy. Surprisingly, current analogy based effort estimation models do not, as far as we know, include adjustments related to extreme analogues and inaccurate estimation models. An analysis of several industrial software development and maintenance projects indicates that the effort estimates provided by software professionals, i.e., expert estimates, to some extent are RTM-adjusted. A student experiment confirms this finding, but also indicates a rather large variance in how well the need for RTM-adjustments is understood among software developers.
68|3||Contents Volume 68|
69|1-2|http://www.sciencedirect.com/science/journal/01641212/69/1-2|Editorial board|
69|1-2||Service provider oriented management systems over open cellular network infrastructures|An interesting concept for the open cellular communications world of the near future is to enable service providers (SPs) to dynamically find, and co-operate with, the best cellular network providers (NPs), i.e., those offering desired quality levels, in the most cost-efficient manner, at a given service area region and time zone. This concept calls for an evolution of legacy management paradigms. In this direction, this paper presents parts of a Service Management System (SMS) that adopts the perspective of an SP (SP-SMS). The paper provides elements of the system design (functionality layers, components in each layer, component distribution pattern and high level functionality) and a more detailed look at the functionality of some of the SP-SMS components. The SP-SMS will be decomposed in three layers, namely session configuration, local planning, and global planning. A component type will be introduced in each layer. The role of the different components in the SP-SMS layers will be described. Emphasis will be given to the description of the functionality of the components in the local planning layer. Specifically, the problem addressed at that layer enables an SP to allocate its service demand, at the best possible (desired) quality levels, and at the most cost-efficient cellular NPs, within a service area region and time-zone. Finally, results showing merits from the introduction of an SP-SMS are presented, and concluding remarks are made.
69|1-2||An adaptive flow control with the re-transmission policy over the serverâproxyâclient networking environment|The current server–client 2-stage-based multimedia transmission configuration is to be changed to a server–proxy–client 3-stage-based one when the broadband Internet becomes true. The traffic flow control scheme becomes different for the coming 3-stage-based multimedia transmission. In this paper, we propose a 3-stage traffic flow control scheme for transmitting multimedia data over the server–proxy–client transmission architecture. Based on the layered video streaming technique, the proposed traffic flow control scheme adjusts the presentation quality according to the currently available bandwidth, i.e., dropping/adding some video layers when the networking situation gets congested/smooth. Additionally, the proposed traffic flow control scheme provides layered video stream buffering control to smoothen video playout and adopts a re-transmission technique to reduce the loss possibility of some important video frames. With the proposed traffic flow control scheme, the coming 3-stage multimedia presentation systems can have a better optimization of bandwidth utilization and presentation quality over a broadband Internet environment.
69|1-2||Parameter driven synthetic web database generation|To support intelligent data analysis on the web information, a data warehousing system called WHOWEDA (WareHouse Of WEb DAta) has been proposed. Unlike other relational data warehouses, WHOWEDA incorporates a Web Data Model that describes the web objects and their relationships as they are maintained within a data warehouse. A set of web operations has also been developed to manipulate the warehoused web information. In order to measure the performance of WHOWEDA and other similar systems that store and manipulate web information, a synthetic web database generator called WEDAGEN (WEb DAtabase GENerator) has been developed. It has the capability of generating collections of web objects of different sizes and complexities determined by a set of user-specified parameters. This paper presents the issues in the design and implementation of WEDAGEN. It also gives a detailed description of its system components and the strategy to generate synthetic web databases. A formal analysis of the generated web database and an empirical assessment of WEDAGEN has been reported.
69|1-2||Efficient trip generation with a rule modeling system for crew scheduling problems|Trip generation is the most time consuming phase in the solution process of crew scheduling problems faced by large transportation companies such as airlines and railways. A large number of trips must be constructed while satisfying a complex set of regulations. In this paper, we present an efficient trip generation method that utilizes originally a rule modeling system in order to reduce the corresponding search space. Special pruning rules are defined using a high-level rule language, which also supports the modeling of the business regulations required in the scheduling process. In addition, the legality checking mechanism involved has been tuned to perform efficiently in order to cope with the vast amount of the legality checks required by the trip generator. The algorithms are tested as a module for a crew scheduling application satisfying the tight response time requirements of a production system. We present experimental results based on problems provided by a major European airline that validate the usefulness and applicability of our work.
69|1-2||Viewpoints of DSP software and service architectures|The software architecture of a future mobile telecommunication system consists of three main parts: system infrastructure services, middleware services and application services. Infrastructure services provide access technologies and networking services for the middleware services that again provide richer capabilities for wireless applications through mobile Internet. Architecture describes the organization of software systems, components, their internal relationships and connections to the environment. Reusing architectural structures benefits companies, because the architecture is a pivotal part of any system, and a costly one to construct. Architecture is documented and reused through architectural views that describe identified stakeholders and concerns, e.g. the purpose of a system, and the feasibility of constructing, deploying, evolving and maintaining it. Views conform to special viewpoints defined for the domain. This paper describes the viewpoints selected for developing the architecture of middleware services and digital signal processing software and provides a general framework for comparing viewpoints. Comparison and analysis of the defined viewpoints show that domain and system size are the dominant issues to be considered when architectural viewpoints are being selected.
69|1-2||Content-aware cooperative caching for cluster-based web servers|Most traditional cooperative caching schemes were developed for network file systems but were not designed for cluster-based web servers with content-based request distribution. Hence, although the schemes are applied to the web servers, their performance is not good due to high disk accesses and large block access latency. This paper proposes and evaluates a new cooperative caching scheme suitable to file systems in web servers. It uses a cache replacement policy, called duplicate first copy replacement, to avoid caching unnecessary data produced during serving requests and to minimize disk accesses. In addition it reduces block access latency required to fetch a file-block in the cooperative cache. A simulation shows that our cooperating caching decreases the disk access ratio by 29%, and reduces block access latency by about 26% when compared to the existing cooperative algorithms.
69|1-2||Designing a high-performance database engine for the âDb4XMLâ native XML database system|eXtensible Markup Language (XML) is fast becoming the common electronic data interchange language between applications. In this paper, we describe a database engine called ‘Db4XML’, which provides storage for XML documents in native format. Db4XML is a high performance, main memory resident database engine. Db4XML is being used as a testbed for comparing various query evaluation techniques. The use of wild card (*, ?, ‘//’, etc.) in the path expressions of a query allows users to query documents whose structural information is not available. This paper lists different techniques that can be used to evaluate generalized path expressions (GPE) and presents a performance comparison of the same. A preliminary performance study of the effect of using concurrency control techniques on the various query evaluation techniques is also performed. This paper briefly discusses a suitable recovery technique for the database engine.
69|1-2||Minimum distance queries for time series data|In this paper, we propose an indexing scheme for time sequences which supports the minimum distance of arbitrary Lp norms as a similarity measurement. In many applications where the shape of the time sequence is a major consideration, the minimum distance is a more suitable similarity measurement than the simple Lp norm. To support minimum distance queries, most of the previous work has the preprocessing step for vertical shifting which normalizes each sequence by its mean. The vertical shifting, however, has the additional overhead to get the mean of a sequence and to subtract it from each element of the sequence. The proposed method can match time series of similar shape without vertical shifting and guarantees no false dismissals. In addition, the proposed method needs only one index structure to support minimum distance queries in any arbitrary Lp norm. The experiments are performed on real data (stock price movement) to verify the performance of the proposed method.
69|1-2||New methods for redistributing slack time in real-time systems: applications and comparative evaluations|This paper addresses the problem of scheduling hard and non-hard real-time sets of tasks that share the processor. The notions of singularity and k-schedulability are introduced and methods based on them are proposed. The execution of hard tasks is postponed in such a way that hard deadlines are not missed but slack time is advanced to execute non-hard tasks. In a first application, two singularity methods are used to schedule mixed systems with hard deterministic sets and stochastic non-hard sets. They are compared to methods proposed by other authors (servers, slack stealing), background and M/M/1. The metric is the average response time in servicing non-hard tasks and the proposed methods show a good relative performance. In a second application, the previous methods, combined with two heuristics, are used for the on-line scheduling of real-time mandatory/reward-based optional systems with or without depreciation of the reward with time. The objective is to meet the mandatory time-constraints and maximize the reward accrued over the hyperperiod. To the best of the authors’ knowledge, these are the only on-line methods proposed to address the problem and outperform Best Incremental Return, often used as a yardstick.
69|1-2||Architecture-level modifiability analysis (ALMA)|Several studies have shown that 50–70% of the total lifecycle cost for a software system is spent on evolving the system. Organizations aim to reduce the cost of these adaptations, by addressing modifiability during the system’s development. The software architecture plays an important role in achieving this, but few methods for architecture-level modifiability analysis exist. Independently, the authors have been working on scenario-based software architecture analysis methods that focus exclusively on modifiability. Combining these methods led to architecture-level modifiability analysis (ALMA), a unified architecture-level analysis method that focuses on modifiability, distinguishes multiple analysis goals, has explicit assumptions and provides repeatable techniques for performing the steps. ALMA consists of five main steps, i.e. goal selection, software architecture description, change scenario elicitation, change scenario evaluation and interpretation. The method has been validated through its application in several cases, including software architectures at Ericsson Software Technology, DFDS Fraktarna, Althin Medical, the Dutch Department of Defense and the Dutch Tax and Customs Administration.
69|1-2||EMBOT: An enhanced motion-based object tracker|The problem of tracking moving parts of image is a significant for many practical applications. There are good reasons to track a wide variety of objects, including airplanes, missiles, vehicles, people, animals, and microorganisms. There are several approaches that can be used to find out an optimal solution for this problem. Here, an enhanced motion-based tracking system is presented. The enhancements basically are:
69|1-2||Location management in cellular mobile computing systems with dynamic hierarchical location databases|An important issue in the design of a mobile computing system is how to manage the location information of mobile clients. In the existing commercial cellular mobile computing systems, a two-tier architecture is adopted. However, the two-tier architecture is not scalable. In the literatures, a hierarchical database structure is proposed in which the location information of mobile clients within a cell is managed by the location database responsible for the cell. The location databases of different cells are organized into a tree-like structure to facilitate the search of mobile clients. Although this architecture can distribute the updates and the searching workload amongst the location databases in the system, location update overheads can be very expensive when the mobility of clients is high. In this paper, we study the issues on how to generate location updates under the distance-based method for systems using hierarchical location databases. A cost-based method is proposed for calculating the optimal distance threshold with the objective to minimize the total location management cost. Furthermore, under the existing hierarchical location database scheme, the tree structure of the location databases is static. It cannot adapt to the changes in mobility patterns of mobile clients. This will affect the total location management cost in the system. In the second part of the paper, we present a reorganization strategy to restructure the hierarchical tree of location databases according to the mobility patterns of the clients with the objective to minimize the location management cost. Extensive simulation experiments have been performed to investigate the reorganization strategy when our location update generation method is applied.
69|1-2||Real-time broadcast algorithm for mobile computing|Broadcasting mechanisms have been widely used to transfer information to a large number of clients. Information is transferred by broadcast servers (satellites or base stations) downstream with a wide bandwidth. Most of the broadcast schemes try to minimize the average “access time”. This paper presents a real-time broadcast algorithm that transfers many information items including one with timing constraint. Our real-time broadcast algorithm attempts to meet the deadline for real-time information as well as to minimize the average access time for non-real-time information. Simulation results show that our algorithm can reduce the average access time for the non-real-time information while meeting the deadline for the real-time information.
69|1-2||Efficient validation of mobile transactions in wireless environments|In broadcast environments, the limited bandwidth of the upstream communication channel from the mobile clients to the server bars the application of conventional concurrency control protocols. In this paper, we propose a new variant of the optimistic concurrency control (OCC) protocol that is suitable for broadcast environments. At the server, forward validation of a transaction is done against currently running transactions, including mobile transactions and server transactions. At the mobile clients, partial backward validation of a transaction is done against committed transactions at the beginning of every broadcast cycle. Upon completion of execution, read-only mobile transactions can be validated and committed locally and update mobile transactions are sent to the server for final validation. These update transactions have a better chance of commitment because they have gone through the partial backward validation. In addition to the nice properties of conventional OCC protocols, this protocol provides autonomy between the mobile clients and the server with minimum upstream communication, which is a desirable feature to the scalability of applications running in broadcast environments. This protocol is able to process both update transactions and read-only transactions at the mobile clients at low space and processing overheads.
69|1-2||Brute force web search for wireless devices using mobile agents|Web based search engines have been with us for a long time now. They proved to be an irreplaceable tool for researchers and Internet users all over the world. The exponential growth of the Internet has disclosed great challenges to these engines, as it is hard to maintain an accurate database of numerous web pages over time. This problem becomes wearisome, as it is often necessary to browse through several results before locating a web page that matches the given query. As today mobile devices are able to connect to the Internet through high-cost low-bandwidth wireless networks, this tactic can become very expensive. Motivated by these issues, we designed and implemented SearchSweep, a mobile agent based client-server system that uses existing search engine systems on the web to locate and download web pages. A refinement system on the server makes this solution ideal for mobile users, or users with limited bandwidth. The structure of SearchSweep platform is presented and the use of mobile agents on wireless devices is proposed as a way of attacking their limitations.
69|3|http://www.sciencedirect.com/science/journal/01641212/69/3|Editorial board|
69|3||Ubiquitous computing|
69|3||Roam, a seamless application framework|One of the biggest challenges in future application development is device heterogeneity. In the future, we expect to see a rich variety of computing devices that can run applications. These devices have different capabilities in processors, memory, networking, screen sizes, input methods, and software libraries. We also expect that future users are likely to own many types of devices. Depending on users’ changing situations and environments, they may choose to switch from one type of device to another that brings the best combination of application functionality and device mobility (size, weight, etc.). Based on this scenario, we have designed and implemented a seamless application framework called the Roam system that can both assist developers to build multi-platform applications that can run on heterogeneous devices and allow a user to move/migrate a running application among heterogeneous devices in an effortless manner. The Roam system is based on partitioning of an application into components and it automatically selects the most appropriate adaptation strategy at the component level for a target platform. To evaluate our system, we have created several multi-platform Roam applications including a Chess game, a Connect4 game, and a shopping aid application. We also provide measurements on application performance and describe our experience with application development in the Roam system. Our experience shows that it is relatively easy to port existing applications to the Roam system and runtime application migration latency is within a few seconds and acceptable to most non-real-time applications.
69|3||Tooling and system support for authoring multi-device applications|This paper presents a development model, tooling environment, and system support for building and deploying applications targeted to run on multiple heterogeneous end-user devices. Our approach is based on a device-independent application model and consists of three elements: (1) an automated process of specialization, by which device-specific versions of the application are generated, (2) support for hand-customization of generated applications, a process we call tweaking, both within our workbench and using external editors, and (3) a designer-in-the-loop process of generalization, by which a generic model is inferred from concrete interface artifacts such as HTML pages. We argue that this approach is cost-effective and results in usable applications that run on a variety of devices.
69|3||Extending tuplespaces for coordination in interactive workspaces|The current interest in programming models and software infrastructures to support ubiquitous and environmental computing is heightened by the falling cost of hardware and the ubiquity of local-area wireless networking technologies. Interactive workspaces are technologically augmented team-project rooms that represent a specific sub-domain of ubiquitous computing. We argue both from related work and from our own experience with a prototype that the tuplespace model of communication forms the best basis for a coordination infrastructure for such workspaces. This paper presents the usage and characteristics expected of interactive workspaces, from which we derive a set of key system properties for any coordination infrastructure in an interactive workspace. We show that the design aspects of tuplespaces, augmented with some new extensions, yield a system model, which we call the Event Heap, that satisfies all of the desired properties. We also briefly discuss why other coordination models fall short of the desired properties, and describe our experience using our implementation of the Event Heap model. The paper focuses on a justification of the use of tuplespaces in interactive workspaces, and does not provide a detailed discussion of the Event Heap implementation or our more general experience with interactive workspaces, each of which is treated in detail elsewhere.
69|3||The BEACH application model and software framework for synchronous collaboration in ubiquitous computing environments|In this paper, a conceptual model for synchronous applications in ubiquitous computing environments is proposed. To test its applicability, it was used to structure the architecture of the BEACH software framework that is the basis for the software infrastructure of i-LAND (the ubiquitous computing environment at FhG-IPSI). The BEACH framework provides the functionality for synchronous cooperation and interaction with roomware components, i.e. room elements with integrated information technology. To show how the BEACH model and framework can be applied, the design of a sample application is explained. Also, the BEACH model is positioned against related work. In conclusion, we provide our experiences with the current implementation.
69|3||Contents Volume 69|
70|1-2|http://www.sciencedirect.com/science/journal/01641212/70/1-2|Editorial board|
70|1-2||Editorâs corner|
70|1-2||Software project control centers: concepts and approaches|On-line interpretation and visualization of project data are gaining increasing importance on the long road towards predictable and controllable software project execution. In the context of software development, only few techniques exist for supporting these tasks. This is caused particularly by the often insufficient use of engineering principles in the software development domain. Beyond that, interpretation and visualization techniques from other domains (such as business or production processes) are not directly applicable to software processes because of the specific characteristics of software development. A software project control center (SPCC) is a means for collecting, interpreting, and visualizing measurement data in order to provide purpose- and role-oriented information to all involved parties (e.g., project manager, quality assurer) during the execution of a project. This article presents a reference model for concepts and definitions around SPCCs. Based on this reference model, a characterization and classification of essential approaches contributing to this field is given. Finally, an outline for future research is derived from identified deficiencies of existing approaches.
70|1-2||Supporting risks in software project management|Complex software development is a risky job. The number of unsuccessful projects surpasses the number of successful developments, particularly when large projects are analyzed. This paper describes an approach to develop, retrieve, and reuse management knowledge and experience concerned with software development risks. Scenarios are used to model risk impact and resolution strategies efficacy within risk archetypes. A risk archetype is an information structure that holds knowledge about software development risks. A risk management process organizes the use of risk archetypes within an application development effort. The process resembles a reuse process framework, where two sub-processes are respectively responsible for identifying and reusing risk information. Simulating the impact of the expected risks can support some of the decisions throughout the software development process. The contribution of this paper is to show how risk archetypes and scenario models can represent reusable project management knowledge. An observational analysis of applying such an approach in an industrial environment and a feasibility study are also described.
70|1-2||A review of studies on expert estimation of software development effort|This paper provides an extensive review of studies related to expert estimation of software development effort. The main goal and contribution of the review is to support the research on expert estimation, e.g., to ease other researcher’s search for relevant expert estimation studies. In addition, we provide software practitioners with useful estimation guidelines, based on the research-based knowledge of expert estimation processes. The review results suggest that expert estimation is the most frequently applied estimation strategy for software projects, that there is no substantial evidence in favour of use of estimation models, and that there are situations where we can expect expert estimates to be more accurate than formal estimation models. The following 12 expert estimation “best practice” guidelines are evaluated through the review: (1) evaluate estimation accuracy, but avoid high evaluation pressure; (2) avoid conflicting estimation goals; (3) ask the estimators to justify and criticize their estimates; (4) avoid irrelevant and unreliable estimation information; (5) use documented data from previous development tasks; (6) find estimation experts with relevant domain background and good estimation records; (7) Estimate top-down and bottom-up, independently of each other; (8) use estimation checklists; (9) combine estimates from different experts and estimation strategies; (10) assess the uncertainty of the estimate; (11) provide feedback on estimation accuracy and development task relations; and, (12) provide estimation training opportunities. We found supporting evidence for all 12 estimation principles, and provide suggestions on how to implement them in software organizations.
70|1-2||Architecting for usability: a survey|Over the years the software engineering community has increasingly realized the important role software architecture plays in fulfilling the quality requirements of a system. The quality attributes of a software system are, to a large extent determined by the system’s software architecture. In recent years, the software engineering community has developed various tools and techniques that allow for design for quality attributes, such as performance or maintainability, at the software architecture level. We believe this design approach can be applied not only to “traditional” quality attributes such as performance or maintainability but also to usability. This survey explores the feasibility of such a design approach. Current practice is surveyed from the perspective of a software architect. Are there any design methods that allow for design for usability at the architectural level? Are there any evaluation tools that allow assessment of architectures for their support of usability? What is usability? A framework is presented which visualizes these three research questions. Usability should drive design at all stages, but current usability engineering practice fails to fully achieve this goal. Our survey shows that there are no design techniques or assessment tools that allow for design for usability at the architectural level.
70|1-2||Better sure than safe? Over-confidence in judgement based software development effort prediction intervals|The uncertainty of a software development effort estimate can be indicated through a prediction interval (PI), i.e., the estimated minimum and maximum effort corresponding to a specific confidence level. For example, a project manager may be “90% confident” or believe that is it “very likely” that the effort required to complete a project will be between 8000 and 12,000 work-hours. This paper describes results from four studies (Studies A–D) on human judgement (expert) based PIs of software development effort. Study A examines the accuracy of the PIs in real software projects. The results suggest that the PIs were generally much too narrow to reflect the chosen level of confidence, i.e., that there was a strong over-confidence. Studies B–D try to understand the reasons for the observed over-confidence. Study B examines the possibility that the over-confidence is related to type of experience or estimation process. Study C examines the possibility that the concept of confidence level is difficult to interpret for software estimators. Finally, Study D examines the possibility that there are unfortunate feedback mechanisms that reward over-confidence.
70|1-2||Heterogeneous formal specification based on Object-Z and statecharts: semantics and verification|This work presents a specification language, called OZS, based on two formalisms: Object-Z and the statecharts. Such a specification style facilitates the modeling of systems with both reactive and functional aspects. The accent is placed on OZS semantics so as to give formal foundations to verification and simulation of OZS models. Every OZS model has a transition system as its semantic interpretation. Untimed and timed versions of the OZS semantics are presented. Both transition system models of an OZS class can be used for verification purposes by model checking. In this work, a real-word example is treated and the resulting specification is model-checked by using the Stanford Temporal Prover environment from Stanford.
70|1-2||Flexible retrieval of Web Services|An important issue arising from Web Service applications is how to conveniently, accurately and efficiently retrieve services from large-scale and expanding service repositories. This paper proposes a flexible Web Service retrieval approach, which solves this issue by means of an orthogonal service space and establishing the multi-valued specialization relationships between services. The similarity degree between services is measured based on the specialization relationship between operations defined in services. An SQL-like flexible query language is used to support the flexible retrieval of services. The related programming environment and graphical user operation interface of the language have been implemented in the Service Grid environment. Compared with the current UDDI-based service retrieval approach, the proposed approach has the advantages of convenience, accuracy and efficiency.
70|1-2||Timeboxing: a process model for iterative software development|In today’s business where speed is of essence, an iterative development approach that allows the functionality to be delivered in parts has become a necessity and an effective way to manage risks. In this paper we propose the timeboxing model for iterative software development in which each iteration is done in a time box of fixed duration, and the functionality to be built is adjusted to fit the time box. By dividing the time box into stages, pipelining concepts are employed to have multiple time boxes executing concurrently, leading to a reduction in the delivery time for product releases. We illustrate the use of this process model through an example of a commercial project that was successfully executed using the proposed model.
70|1-2||Formally based modeling and inheritance of behaviour in object-oriented systems|Despite the popularity of (graphical) notations for the specification of object behaviour, there is no common understanding of what exactly constitutes the life cycle of an object. Consequently, different frameworks for the object-oriented modeling of systems allow for the specification of different kinds of behaviour. Unfortunately, the semantics of languages used in this area is often not clearly stated. In addition to the problems arising from this lack of formality, inheritance of behaviour is usually not covered by commonly used object-oriented languages. Therefore, flawless systems are difficult to build, because unpleasant surprises most easily occur if an object of a subclass is used in the context of its superclass.
70|1-2||Defect evolution in a product line environment|One mechanism used for monitoring the development of the Space Shuttle flight control software, in order to minimize any risks to the missions, is the independent verification and validation (IV&V) process. Using data provided by both the Shuttle software developer and the IV&V contractor, in this paper we describe the overall IV&V process as used on the Space Shuttle program and provide an analysis of the use of metrics to document and control this process over multiple releases of this software. Our findings reaffirm the value of IV&V, show the impact of IV&V on multiple releases of a large complex software system, and indicate that some of the traditional measures of defect detection and repair are not applicable in a multiple-release environment such as this one.
70|1-2||Uncertainty profile and software project performance: A cross-national comparison|Many software projects are inevitably associated with various types and degrees of uncertainty. It is not uncommon to see software project spiral out of control with escalated resource requirements. Thus, risk management techniques are critical issues to information system researchers. Previous empirical studies of US software firms support the adoption of development standardization and user requirement analysis techniques in risk-based software project management. Using data collected from software projects developed in Korea during 1999–2000, we conduct a comparative study to determine how risk management strategies impact software product and process performance in countries with dissimilar IT capabilities. In addition, we offer an alternative conceptualization of residual performance risk. We show that the use of residual performance risk as an intervening variable is inappropriate in IT developing countries like Korea where the role of late stage risk control remedies are critical. A revised model is proposed that generates more reliable empirical implications for Korean software projects.
70|1-2||Research in computer science: an empirical study|In this paper, we examine the state of computer science (CS) research from the point of view of the following research questions:
70|1-2||Agent framework to support the computational grid|This paper presents an agent-based computational grid (ACG), which applies the concept of computational grid to agents. The ACG system is to implement a uniform higher-level management of the computing resources and services on the Grid, and provide users with a consistent and transparent interface for accessing such services. All entities in the Grid environment including computing resources and services can be represented as agents. Each entity is registered with a grid service manager. Service requestor agent locates a specific grid service by submitting requests to the grid service manager with descriptions of required services. XML is used to describe both grid service descriptions and service requestor agent’s queries. An ACG grid service can be a service agent that provides the actual grid service to the other grid member. In this paper, firstly, the conceptual model about ACG grid is described, and then the design and implementation are given. Finally, some conclusions are given.
70|1-2||Incremental specification with SCTL/MUS-T: a case study|The past decade witnessed a great advance in the field of timed formal methods for the specification and analysis of real-time and safety-critical systems. In this context, timed automata and real-time temporal logics provide a simple, and yet general, way to model and specify the behavior of these systems. At the same time, iterative and incremental development has been massively adopted in professional practice. In order to get closer to this current trend, timed formal methods should be adapted to such lifecycle structures, getting over their traditional role of verifying that a model meets a set of fixed requirements. In the pursuit of this ultimate aim, we propose SCTL/MUS-T, a timed methodology in which many-valuedness let deal with both the uncertainty and the disagreement which are pervasive and desirable in an iterative and incremental process. To illustrate the main ideas behind SCTL/MUS-T methodology this paper focuses on the specification, synthesis and verification of the well known steam-boiler case study.
70|1-2||An experimental evaluation of weak-branch criterion for class testing|In this paper an experiment has been presented in which we have formulated and tested various hypotheses on the fault detecting ability of the weak branch criterion ]J. Syst. Software 23 (1993) 95]. Further, sequences that satisfied the weak branch criterion were prefixed to another sequence so as to satisfy the strong branch criterion (ibid), and also to a sequence that was generated using the category partitioning method [Commun. ACM 31 (1988) 676]. The outcome of this addition has also been evaluated and compared. In particular, we have tried to test the possible influence of the following on the fault detecting ability: length of test-sequences, nature of the fault and class features. The experiment was carried out on eight classes that represented some of the important data structures used in programming and which were implemented in C++.
70|3|http://www.sciencedirect.com/science/journal/01641212/70/3|Editorial board|
70|3||Rapid system prototyping|
70|3||Object-based hardware/software component interconnection model for interface design in system-on-a-chip circuits|The design of system-on-a-chip (SoC) circuits requires the integration of complex hardware/software components that are customized to efficiently execute a specific application. Nowadays, these components include many different embedded processors executing concurrent software tasks. In this paper, we present an object-based component interconnection model to represent both hardware and software components within a system architecture in a very high level of abstraction. This model is used in a design flow for automatic generation of hardware/software interfaces for SoC circuits. Design tools for automatic generation of embedded operating systems, hardware interfaces and associated device drivers are presented and evaluated using the results obtained with a VDSL application.
70|3||Rapid design exploration of safety-critical distributed automotive applications via virtual integration platforms|Modern automotive applications such as Drive-by-Wire are implemented over distributed architectures where electronic control units (ECU’s) communicate via broadcast buses. In this paper, we present the concept of virtual integration platform for automotive applications. The platform provides the basis for early analysis and validation of distributed applications, therefore enhancing the current model based development process techniques that are applied to one ECU at a time. The virtual prototype includes both functional and performance (time) models of the application software, scheduling policies, and the bus communication protocols. As a result, since design errors can be found earlier in the design process before the different sub-systems are integrated in the car, savings in both production and development costs can be achieved. The virtual integration platform concept is supported by an integrated IP-based tool environment for authoring, integration, and validation. First, a model of the distributed application is built by composing models of HW and SW components. The models can be either authored or imported from different tools. Functional simulation of the overall distributed control algorithm can be carried out first. Then, the mapping phase can take place: sub-functions of the control algorithm are mapped to architectural resources (CPUs), and zero-time communication links between the sub-functions are mapped to bus protocol delay models. Changing mappings, parameters such as task priorities, and bus schedule enables the exploration of alternative implementations. The validation is carried out by simulating the resulted virtual prototype of the distributed control algorithm running on the ECU network. The design environment shortens design turn-around time by supporting (semi)-automatic configuration of the architecture model (e.g. frame packaging, redundancy level, communication matrix, bus and RTOS scheduling, etc.).
70|3||Rapid prototyping of real-time control laws for complex mechatronic systems: a case study|Rapid prototyping of complex systems embedded in even more complex environments raises the need for a layered design approach. Our example is a mechatronic design taken from the automotive industry and illustrates the rapid-prototyping procedure of real-time-critical control laws. The approach is based on an object-oriented structuring allowing not only central control units but also distributed control units as needed by today’s designs. The implementation of control laws is a hardware-in-the-loop simulation, refined in steps and reducing the simulation part at every one of these. On the lower level, common platforms, such as FPGAs, microcontrollers or specialized platforms, can be instantiated.
70|3||Distributed prototyping from validated specifications|We present vpl2cxx, a translator that automatically generates efficient, fully distributed C++ code from high-level system models specified in the mathematically well-founded VPL design language. As the Concurrency Workbench of the New Century (CWB-NC) verification tool includes a front-end for VPL, designers may use the full range of automatic verification and simulation checks provided by this tool on their VPL system designs before invoking the translator, thereby generating distributed prototypes from validated specifications. Besides being fully distributed, the code generated by vpl2cxx is highly readable and portable to a host of execution environments and real-time operating systems (RTOSes). This is achieved by encapsulating all generated code dealing with low-level interprocess communication issues in a library for synchronous communication, which in turn is built upon the adaptive communication environment (ACE) client-server network programming interface. Finally, example applications show that the performance of the generated code is very good, especially for prototyping purposes. We discuss two such examples, including the RETHER real-time Ethernet protocol for voice and video applications.
70|3||FPGA based hardware acceleration for elliptic curve public key cryptosystems|This paper addresses public key cryptosystems based on elliptic curves, which are aimed to high-performance digital signature schemes. Elliptic curve algorithms are characterized by the fact that one can work with considerably shorter keys compared to the RSA approach at the same level of security. A general and highly efficient method for mapping the most time-critical operations to a configurable co-processor is proposed. By means of real-time measurements the resulting performance values are compared to previously published state of the art hardware implementations.
70|3||Model based testing in incremental system development|The spiraling nature of evolutionary software development processes produces executable parts of the system at the end of each loop. It is argued that these parts should consist not only of programming language code, but of executable graphical system models. As a main benefit of the use of more abstract, yet formal, modeling languages, a method for model based test sequence generation for reactive systems on the grounds of Constraint Logic Programming as well as its implementation in the CASE tool AutoFocus is presented.
70|3||Contents Volume 70|
||||
volume|issue|url|title|abstract
71|1-2|http://www.sciencedirect.com/science/journal/01641212/71/1-2|Editorial board|
71|1-2||Advanced obfuscation techniques for Java bytecode|There exist several obfuscation tools for preventing Java bytecode from being decompiled. Most of these tools simply scramble the names of the identifiers stored in a bytecode by substituting the identifiers with meaningless names. However, the scrambling technique cannot deter a determined cracker very long. We propose several advanced obfuscation techniques that make Java bytecode impossible to recompile or make the decompiled program difficult to understand and to recompile. The crux of our approach is to over use an identifier. That is, an identifier can denote several entities, such as types, fields, and methods, simultaneously. An additional benefit is that the size of the bytecode is reduced because fewer and shorter identifier names are used. Furthermore, we also propose several techniques to intentionally introduce syntactic and semantic errors into the decompiled program while preserving the original behaviors of the bytecode. Thus, the decompiled program would have to be debugged manually. Although our basic approach is to scramble the identifiers in Java bytecode, the scrambled bytecode produced with our techniques is much harder to crack than that produced with other identifier scrambling techniques. Furthermore, the run-time efficiency of the obfuscated bytecode is also improved because the size of the bytecode becomes smaller after obfuscation.
71|1-2||Formally analyzing software architectural specifications using SAM|In the past decade, software architecture has emerged as a major research area in software engineering. Many architecture description languages have been proposed and some analysis techniques have also been explored. In this paper, we present a graphical formal software architecture description model called software architecture model (SAM). SAM is a general software architecture development framework based on two complementary formalisms––Petri nets and temporal logic. Petri nets are used to visualize the structure and model the behavior of software architectures while temporal logic is used to specify the required properties of software architectures. These two formal methods are nicely integrated through the SAM software architecture framework. Furthermore, SAM provides the flexibility to choose different compatible Petri net and temporal logic models according to the nature of system under study. Most importantly, SAM supports formal analysis of software architecture properties in a variety of well-established techniques––simulation, reachability analysis, model checking, and interactive proving. In this paper, we show how to formally analyze SAM software architecture specifications using two well-known techniques––symbolic model checking with tool Symbolic Model Verifier, and theorem proving with tool STeP.
71|1-2||Systems analyst activities and skills in the new millennium|The nature of systems development has continued to undergo change as new technologies emerge and impact the environment in which systems must function. A nationwide survey of systems analysts was conducted to assess what tasks are most important, what skills are most important in completing the tasks, and how the tasks and skills needed may have changed over the last decade. The most important tasks were those associated with traditional systems development, namely defining system scope, objectives, system requirements, as well as assessing the impact of systems and evaluating their performance. Analytical skills were considered the most important skills overall, followed by technical and communication skills. The least important skills overall were interpersonal skills. When compared to results from a similar survey conducted in the early 1990s, the data from the present study revealed that while the most important tasks remained relatively unchanged, a number of shifts occurred in other aspects of the systems analyst’s work that reflect changes in the technological environment.
71|1-2||Anomaly-free component adaptation with class overriding|Software components can be implemented and distributed as collections of classes, then adapted to the needs of specific applications by means of subclassing. Unfortunately, subclassing in collections of related classes may require re-implementation of otherwise valid classes just because they utilize outdated parent classes, a phenomenon that is referred to as the subclassing anomaly. The subclassing anomaly is a serious problem since it can void the benefits of component-based programming altogether. We propose a code adaptation language mechanism called class overriding that is intended to overcome the subclassing anomaly. Class overriding does not create new and isolated derived classes as subclassing does, but rather extends and updates existing classes across collections of related classes. If adopted in new languages for component-based programming, or in existing compiled languages such as C# and Java, class overriding can help maintain the integrity of evolving collections of related classes and thus enhance software component adaptability.
71|1-2||Design and implementation of the just-in-time retrieving policy for schedule-based distributed multimedia presentations|In order to provide the smooth playback of a distributed multimedia presentation, the object-retrieving engine for the player must fetch each object before its playback time. In this paper, a smart object-retrieving engine is proposed, which adopts a retrieving policy named the just-in-time policy. The policy expects the retrieval process of an object to finish right before the playback time of the object to achieve a better buffer utilization and network bandwidth efficiency. The proposed object-retrieving engine focuses on SMIL1.0-based multimedia presentations. By converting the synchronization relationship of objects in the SMIL1.0 document to Real-Time Synchronization Model, which simplifies the handling of the synchronization relationship, and considering the end-to-end bandwidth as well as the user interactions, the object-retrieving engine determines the object request time for each object. The engine issues the request to fetch each object for the ongoing presentation at the object request time, and provides the player with proper media objects. The feasibility and better performance of the proposed just-in-time retrieving policy had been proved by performance measurements of system implementation.
71|1-2||Image retrieval system based on color-complexity and color-spatial features|This paper proposes a color-complexity image feature which can effectively describe the color variation of the pixels in an image. This paper also presents a color-spatial feature to state the pixel color distributions on different locations in an image. Because these two features are highly complementary, this paper integrates them to provide an image retrieval system. Experimental results show that the recognition ability of the system can be drastically enhanced after integrating these two image features.
71|1-2||Improving workload balance and code optimization on processor-in-memory systems|Processor-in-memory (PIM) architectures have recently been proposed, with the objective of reducing the performance gap between processor and memory. An earlier study of Huang and Chu [Proceedings of 2nd Workshop on Intelligent Memory Systems, Cambridge, MA, 2000] designed a statement-based parallelizing system, SAGE, to exploit the potential benefits of PIM. This study extends this system to achieve better performance. Several comprehensive optimization approaches, including self-patch weight evaluation, loop splitting for PIM, intelligent memory operation (IMOP) recognition, and tiling for PIM, are devised to produce execution schedules with improved load balance. Experimental results confirm the effectiveness of the proposed method.
71|1-2||Case study: an infrastructure for C/ATLAS environments with object-oriented design and XML representation|
71|1-2||Analysing failure behaviours in component interaction|In order to facilitate the process of safety analysis of an evolving software system, this paper presents an architectural approach that enhances the safety analysis by providing appropriate abstractions for modelling and analysing interactions between components, since faulty interactions are the usual cause of accidents. For that, instead of considering components as the locus of change, the proposed approach assumes that components remain unchanged while their interactions (captured by connectors) adapt to the different changes made in the system. The behavioural description of connectors is provided in terms of extended time automata, and the safety analysis is performed using model checking, which verifies whether safe behaviour is maintained when interactions between components change. The feasibility of the approach is demonstrated in terms of a case study that deals with the safety procedures associated with the launching of a sounding rocket.
71|1-2||Dual link fault diagnosis agreement|Previously, most Byzantine Agreement protocols can reach an agreement by the way of fault masking. Few of them can detect and locate the faulty components. And on the other hand, the most fault diagnosis algorithms can detect and locate faulty components but few of them can make all fault free processors reach an agreement. This study analyzes the messages received at the period of reaching agreement, and then, to detect and to locate the faulty links of the network. Finally, the proposed protocol can further make all fault free processors agree on the common failure report of the synchronous fully connected network. The symptoms of the faults include the malicious fault and the dormant fault.
71|1-2||NT-SwiFT: software implemented fault tolerance on Windows NT|Today, there are increasing demands to make application software more tolerant to failures. Fault-tolerant applications detect and recover from failures that are not handled by the application’s underlying hardware or operating system. In recent years, an increasing number of highly available applications are being implemented on Windows NT. However, the current version of Windows (NT4.0, 2000) and its utilities, such as Microsoft Cluster Server (MSCS), do not provide some facilities (such as transparent checkpointing, and message logging) that are needed to implement fault-tolerant applications. In this paper, we describe a set of reusable software components collectively named software implemented fault tolerance (NT-SwiFT) that facilitates building fault-tolerant and highly available applications on Windows NT, 2000. NT-SwiFT provides components for automatic error detection and recovery, checkpointing, event logging and replay, and communication error recovery, and incremental data replication. Using NT-SwiFT, we conducted fault injection experiments on three commercial server applications––Apache web server, Microsoft IIS web server, and Microsoft SQL––to study the failure coverage and the overhead of NT-SwiFT components. Preliminary results show that NT-SwiFT can detect and recover more application failures than MSCS does in all three applications.
71|1-2||Embedding role-based access control model in object-oriented systems to protect privacy|The role-based access control (RBAC) approach has been recognized as useful in information security and many RBAC models have been proposed. Current RBAC researches focus on developing new models or enhancing existing models. In our research, we developed an RBAC model that can be embedded in object-oriented systems to control information flows (i.e. to protect privacy) within the systems. This paper proposes the model. The model, which is named OORBAC, is an extension of RBAC96. OORBAC offers the following features: (a) precisely control information flows among objects, (b) control method invocation through argument sensitivity, (c) allow purpose-oriented method invocation and prevent leakage within an object, (d) precisely control write access, and (e) avoid Trojan horses. We implemented a prototype for OORBAC using JAVA as the target language. The implementation resulted in a language named OORBACL, which can be used to implement secure applications. We evaluated OORBAC using experiments. The evaluation results are also shown in this paper.
71|1-2||Hyppocrates: a new proactive password checker|In this paper, we propose a new proactive password checker, a program which prevents the choice of easy-to-guess passwords. The checker uses a decision tree, constructed applying the minimum description length principle and a pessimistic pruning technique. Experimental results show a substantial improvement in performance of this checker compared to previous proposals. Moreover, the whole software package we provide has a user-friendly interface, enabling the system administrator to configure an ad hoc password proactive checker, in order to satisfy certain policy requirements.
71|1-2||A component based methodology for Web application development|Today’s enterprises increasingly rely on the Web to support their operations, global business alliances, and integration of their business processes with those of their suppliers, partners, and customers. In order to stay competitive, businesses must also respond to changing business and competitive environments in near real-time. This requires that the architecture and design of business systems and applications should allow quick reconfiguration as well as collaboration among distributed software components. Existing software design methodologies often lack the Web focus and explicit support for componentization. This paper presents a methodology for requirements analysis and high-level design, specifically for component-based Web applications.
71|1-2||DIPS: an efficient pointer swizzling strategy for incremental uncaching environments|Pointer swizzling improves the performance of OODBMSs by reducing the number of table lookups. However, the object replacement incurs the unswizzling overhead. In this paper, we propose a new pointer swizzling strategy, the dynamic indirect pointer swizzling (DIPS). DIPS dynamically applies pointer swizzling techniques in order to reduce the overhead of unswizzling. DIPS uses the temporal locality information which is gathered by the object buffer manager. The information is used to select the object to whose pointers the pointer swizzling techniques are applied and to dynamically bind the pointer swizzling techniques using the virtual function mechanism. We show the efficiency of the proposed strategy through experiments over various object buffer sizes and workloads.
71|3|http://www.sciencedirect.com/science/journal/01641212/71/3|Editorial board|
71|3||Computer systems|
71|3||Designing embedded systems using patterns: A case study|If software for embedded processors is based on a time-triggered architecture, using co-operative task scheduling, the resulting system can have very predictable behaviour. Such a system characteristic is highly desirable in many applications, including (but not restricted to) those with safety-related or safety-critical functions. In practice, a time-triggered, co-operatively scheduled (TTCS) architecture is less widely employed than might be expected, not least because care must be taken during the design and implementation of such systems if the theoretically predicted behaviour is to be obtained. In this paper, we argue that the use of appropriate ‘design patterns’ can greatly simplify the process of creating TTCS systems. We briefly explain the origins of design patterns. We then illustrate how an appropriate set of patterns can be used to facilitate the development of a non-trivial embedded system.
71|3||Agent-based distance vector routing: a resource efficient and scalable approach to routing in large communication networks|In spite of the ever-increasing availability of computation and communication resources in modern networks, the overhead associated with network management protocols, such as traffic control and routing, continues to be an important aspect in the design of new methodologies. Resource efficiency of such protocols has become even more prominent with the recent developments of wireless and ad hoc networks, which are marked by much more severe resource constraints in terms of bandwidth, memory, and computational capabilities. This paper presents an agent-based approach to distance vector routing (ADVR) that addresses these resource constraints. ADVR is a resource efficient implementation of distance vector routing that is fault tolerant and scales well for large networks. ADVR draws upon some basic biologically inspired principles to facilitate coordination among the mobile agents that implement the routing task. Specifically, simulated pheromones are used to control the movement of agents within the network and to dynamically adjust the number of agents in the population. The behavior of ADVR is analyzed and compared to that of traditional distance vector routing.
71|3||Dynamic adaptation of application aspects|In today’s fast changing environments, adaptability has become an important feature in modern computing systems, programming languages and software engineering methods. Different approaches and techniques are used to achieve the development of adaptable systems. Following the principle of separation of concerns, aspect-oriented programming (AOP) distinguishes application functional code from specific concerns that cut across the system, creating the final application by weaving the program’s main code and its specific aspects. In many cases, dynamic application adaptation is needed, but few existing AOP tools offer it in a limited way. Moreover, these tools use a fixed programming language: aspects cannot be implemented regardless of its programming language.
71|3||Genetic-algorithm-based real-time task scheduling with multiple goals|This paper presents and evaluates a new method for real-time task scheduling in multiprocessor systems. Its objectives are to minimize the number of processors required and the total tardiness of tasks. The minimization is carried out through a multiobjective genetic algorithm (GA), because the problem has non-commensurable and competing objectives to be optimized. The experimental results showed that when compared to five methods used previously, such as list-scheduling algorithms and a specific GA, the performance of our algorithm was comparable or better for 178 out of 180 randomly generated task graphs. Also shown is the impact of the sparsity of a task graph on the performance of our algorithm.
71|3||Analysis of true fully adaptive routing with software-based deadlock recovery|Several analytical models of fully adaptive routing (AR) in wormhole-routed networks have recently been reported in the literature. All these models, however, have been discussed for routing algorithms with deadlock avoidance. Recent studies have revealed that deadlocks are quite rare in the network, especially when enough routing freedom is provided. Thus the hardware resources, e.g. virtual channels, dedicated for deadlock avoidance are not utilised most of the time. This consideration has motivated researchers to introduce fully adaptive routing algorithms with deadlock recovery. This paper describes a new analytical model of a true fully AR algorithm with software-based deadlock recovery, proposed in Martinez et al. (Software-based deadlock recovery techniques for true fully AR in wormhole networks, Proc. Int. Conf. Parallel Processing (ICPP’97), 1997, p. 182) for k-ary n-cubes. The proposed model uses the results from queueing systems with impatient customers to capture the effects of the timeout mechanism used in this routing algorithm for deadlock detection. Results obtained through simulation experiments confirm that the model predicts message latency with a good degree of accuracy under different working conditions.
71|3||Adaptable architecture generation for embedded systems|Architecture generation is the first step in the design of software systems. Many of the qualities that the final software system possesses are usually decided at the architecture development stage itself. Thus, if the final system should be usable, testable, secure, high performance, mobile and adaptable, then these qualities or non-functional requirements (NFRs) should be engineered into the architecture itself. In particular, recently adaptability is emerging as an important attribute required by almost all software systems. Briefly, adaptability is the ability of a software system to accommodate changes in its environment. Embedded systems are usually constrained both in hardware and software. Current adaptable architecture development methods for embedded systems are usually manual and ad hoc––there are almost no comprehensive systematic approaches to consider NFRs at the architecture development stage. While there are several examples of approaches to generate architectures based on functional requirements, we believe that there are very few techniques that consider NFRs such as adaptability during the process of architecture generation. In this paper we present an automated design method that helps develop adaptable architectures for embedded systems by developing a tool called Software Architecture Adaptability Assistant (SA3). SA3 helps the developer during the process of software architecture development by selecting the architectural constituents such as components, connections, patterns, constraints, styles, and rationales that best fit the adaptability requirements for the architecture. The developer can then complete the architecture from the constituents chosen by the tool. SA3 uses the knowledge base properties of the NFR Framework in order to help automatically generate the architectural constituents. The NFR Framework provides a systematic method to consider NFRs, in particular their synergies and conflicts. We validated the architectures generated by SA3 in a type of embedded system called the vocabulary evolution system (VES) by implementing the codes from the generated architectures in the VES and confirming that the resulting system satisfied the requirements for adaptability. A VES responds to external commands through an interface such as ethernet and the vocabulary of these commands changes with time. The validation process also led to the discovery of some of the shortcomings of our automated design method.
71|3||Contents Volume 71|
72|1|http://www.sciencedirect.com/science/journal/01641212/72/1|Editorial board|
72|1||QoS management specification support for multimedia middleware|Middleware technologies are now widely used in order to provide support for the interaction of systems relying on different hardware and operating systems. At present middleware platforms, however, do not provide enough support for both the configuration and reconfiguration of quality of service (QoS) management aspects of real-time applications such as distributed multimedia systems. That is, current middleware only provides support for the low-level specification of QoS properties. This paper presents an architecture description language (ADL) called Xelha for the high-level specification of QoS management in multimedia middleware whereas lower-level aspects can be tuned by using an aspect-oriented suite of languages referred to as resource configuration description language (RCDL). Tool support is also provided for the interpretation of the Xelha and RCDL languages.
72|1||Fair blind threshold signatures in wallet with observers|In this paper, we propose efficient fair blind (t,n) threshold signature schemes in wallet with observers. By these schemes, any t out of n signers in a group can represent the group to sign fair blind threshold signatures, which can be used in anonymous e-cash systems. Since blind signature schemes provide perfect unlinkability, such e-cash systems can be misused by criminals, e.g. to safely obtain a ransom or to launder money. Our schemes allow the judge (or the judges) to deliver information allowing anyone of the t signers to link his view of the protocol and the message–signature pair.
72|1||Traps in Java|Though the Java programming language was designed with extreme care, there are still a few ambiguities and irregularities left in the language. The ambiguities are those issues that are not defined clearly in the Java language specification. The different results produced by different compilers on several example programs justify our observations. The irregularities are issues that often confuse programmers. It is hard for a confused programmer to write robust programs. Furthermore, a few issues of the Java language are left intentionally open to the compiler writers. Their effects on Java programs are discussed. The problems of ambiguity, irregularity, and dependence on implementations frequently trap an incautious Java programmer. Some suggestions and solutions for the problems are provided in this paper.
72|1||Extraction of Java program fingerprints for software authorship identification|Computer programs belong to the authors who design, write, and test them. Authorship identification is concerned with determining the likelihood of a particular author having written some piece(s) of code, usually based on other code samples from the same programmer. Java is a popular object-oriented computer programming language. Programming fingerprints attempt to characterize the features that are unique to each programmer. In this study, we investigated the extraction of a set of software metrics of a given Java source code––by a program written in Visual C++––that could be used as a fingerprint to identify the author of the Java code. The contributions of the selected metrics to authorship identification were measured by a statistical process, namely canonical discriminant analysis, using the statistical software package SAS. Out of the 56 extracted metrics, 48 metrics were identified as being contributive to authorship identification. The authorship of 62.6–67.2% of the Java programs considered could be correctly identified with the extracted metrics. The identification rate could be as high as 85.8%, with derived canonical variates. Moreover, layout metrics played a more important role in authorship identification than the other metrics.
72|1||An uncaught exception analysis for Java|Current JDK Java compiler relies on programmer's declarations (by throws clauses) for checking against uncaught exceptions of the input program. It is not elaborate enough to remove programmer's unnecessary handlers nor suggest to programmers for specialized handlings (when programmer's declarations are too broad). We propose a static analysis of Java programs that estimates their uncaught exceptions independently of the programmer's declarations. This analysis is designed and implemented based on set-based framework. Its cost-effectiveness is suggested by sparsely analyzing the program at method level (hence reducing the number of unknowns in the flow equations). We have shown that our interprocedural exception analysis is more precise than JDK-style intraprocedural analysis, and also that our analysis can effectively detect uncaught exceptions for realistic Java programs.
72|1||Resource space model, its design method and applications|A resource space model (RSM) is a model for specifying, sharing and managing versatile Web resources with a universal resource view. A normal resource space is a semantic coordinate system with independent coordinates and mutual-orthogonal axes. This paper first introduces the main viewpoint and basic content of the RSM, and then proposes a four-step method for designing the logical-level resource spaces: resource analysis, top-down resource partition, design two-dimensional resource spaces, and join between resource spaces. Design strategies and tools include: reference model, analogy and abstraction strategy, resource dictionary, independency checking tool, and orthogonality checking tool. The study on using the RSM to manage relational tables shows that the RSM is also suitable for managing structured resources. Applications show that the RSM together with the proposed development method is an applicable solution to realize normal and effective management of versatile web resources. Comparisons show the differences between the proposed model and the relational data model.
72|1||A unified approach to a fair document exchange system|This paper aims to propose a solution to unify different approaches to fair document exchange for the development of a flexible, general and secure fair document exchange system capable of guaranteeing strong fairness. The solution caters for a document exchange among any number of parties under various situations. At the heart of this unified solution are methods for a verifiable and recoverable encryption of a symmetric (or conventional) key for document encryption and decryption in a simple and efficient manner. The verifiability means that any party can verify the correctness of a key encrypted without actually viewing the key, and the recoverability implies that the party can be assured that a designated party or a group of designated parties can decrypt the encrypted key to recover the original key. These methods are essential for the unified solution to achieve strong fairness. The main novel contribution of the work presented in this paper is that the proposed unified solution represents the first effort on providing an efficient and unified approach to achieving strong fairness in document exchange under various situations.
72|1||Password-based user authentication and key distribution protocols for clientâserver applications|Up to now, most of the literature on three-party authentication and key distribution protocols has focused on the environment in which two users (clients) establish a session key through an authentication server. In this paper, we discuss another environment in which a user (client) requests service from an application server through an authentication server. We propose two secure and efficient authentication protocols (KTAP: key transfer authentication protocol and KAAP: key agreement authentication protocol) to fit this environment. These two proposed protocols can be efficiently applied to various communication systems in distributed computing environments since they provide security, efficiency, and reliability.
72|2|http://www.sciencedirect.com/science/journal/01641212/72/2|Editorial board|
72|2||Rule-based generation of requirements traceability relations|The support for traceability between requirement specifications has been recognised as an important task in the development life cycle of software systems. In this paper, we present a rule-based approach to support the automatic generation of traceability relations between documents which specify requirement statements and use cases (expressed in structured forms of natural language), and analysis object models for software systems. The generation of such relations is based on traceability rules of two different types. More specifically, we use requirement-to-object-model rules to trace the requirements and use case specification documents to an analysis object model, and inter-requirements traceability rules to trace requirement and use case specification documents to each other. By deploying such rules, our approach can generate four different types of traceability relations. To implement and demonstrate our approach, we have implemented a traceability prototype system. This system assumes requirement and use case specification documents and analysis object models represented in XML. It also uses traceability rules which are represented in an XML-based rule mark-up language that we have developed for this purpose. This XML-based representation framework makes it easier to deploy our prototype in settings characterised by the use of heterogeneous software engineering and requirements management tools. The developed prototype has been used in a series of experiments that we have conducted to evaluate our approach. The results of these experiments have provided encouraging initial evidence about the plausibility of our approach and are discussed in the paper.
72|2||A controlled experiment investigation of an object-oriented design heuristic for maintainability|The study presented in this paper is a controlled experiment, aiming at investigating the impact of a design heuristic, dealing with the `god class' problem, on the maintainability of object-oriented designs. In other words, we wish to better understand to what extent a specific design heuristic contributes to the quality of designs developed. The experiment has been conducted using undergraduate students as subjects, performing on two system designs using the Coad & Yourdon method. The results of this study provide evidence that the investigated design heuristic: (a) affects the evolution of design structures; and (b) considerably affects the way participants apply the inheritance mechanism.
72|2||Domain-oriented software development environment|During software development, one of the most critical activities for software engineers is the correct description and identification of product requirements. This activity involves understanding the problem, which is essential to allow one to define a solution for it. To do this, it is important not only to understand the tasks routinely performed which are part of the problem, but more importantly to understand the domain in which the system will take place. Believing that the use of domain knowledge during software development can be very useful to support software development activities, we define the concept of “Domain-Oriented Software Development Environment” (DOSDE). This kind of environment readies knowledge about a specific domain in a symbolic representation (a domain ontology). It also considers a library of potential tasks from the domain to support problem understanding. This paper presents the main concepts of DOSDE, its features, examples of implementation, and how one can use its embedded knowledge, on a domain and its tasks, to assist in the software development activities.
72|2||A development environment for customer-oriented Internet business: eBizBench|An important challenge in the age of Internet business is the proper alignment of customers’ needs with the business system. In order to improve an Internet business system according to customers’ needs continuously, reuse of design results is of particular importance. This paper proposes a development environment, called eBizBench, which provides support for this alignment and reusability. The eBizBench includes seven functions: (1) project management, (2) customer analysis, (3) value analysis, (4) web design, (5) implementation design, (6) repository management, and (7) repository. The eBizBench can help develop and maintain Internet business systems in a systematic fashion. The repository is useful for the conversion of design results among development environments. A real-life web site is illustrated to demonstrate the usefulness of the eBizBench.
72|2||Information theory-based software metrics and obfuscation|A new approach to software metrics using concepts from information theory, data compression, complexity theory and analogies with real physical systems is described. A novel application of software obfuscation allows an existing software package to be analysed in terms of the effects of perturbations caused by the obfuscator. Parallels are drawn between the results of the software analysis and the behaviour of physical systems as described by classical thermodynamics.
72|2||Virtual reality systems estimation vs. traditional systems estimation|This paper examines the problems of applying traditional function points count rules to virtual reality systems (VRS). From the analysis of the differences between traditional and VRS systems, a set of deficiencies in the IFPUG 4.1 function points count method was detected. Due to the increasing importance of these kinds of applications, it is necessary to study how traditional function points count rules can be adapted to estimate VRS. In this paper, we are going to focus on the possibility of estimating function points accurately using a proposed guideline which was successfully applied to estimate two VRS.
72|2||Architecture modeling and evaluation for design of agent-based system|As the Internet computing environment is expanded, service requests from users are increasing and systems become large and complex. To provide users with services efficiently, system must react flexibly to changing requests in distributed and mobile environments. Recently, software agent technology has been recognized as a solution technology to cope with this situation. Many researches are primarily related to system implementation or application by an ad hoc engineering approach. However, researches on the design of agent-based systems are not enough. This paper deals with architectural design of agent-based system. In detail, this paper identifies the possible architectural models of agent-based system, evaluates the identified architectural models using metrics and compares the architectural models based on evaluation results. Also, this paper applies the evaluation approach to real architectures of agent-based system. The proposed performance analysis activity can be included in agent-based systems engineering activity.
72|2||ISO quality standards for measuring architectures|The main concern of this paper is measuring the quality of the architectural design. The goal of this work is to use the architectural design process proposed in the unified process framework, adapting and detailing it to include the quality requirements specification at architectural level. There is general agreement on the fact that in modern applications the selection of the architecture must be addressed early in the development process, to mitigate risks. Moreover, the integration of enterprise applications is a component-based development requiring quality values associated to the services offered by the components. The services depend mostly on the architecture. In consequence, methods arise for guiding the selection or for constructing software architectures. Our approach allows associating the quality requirements (nonfunctional properties) for the architecture expressed using the ISO 9126-1 standard quality model, with the use cases, to facilitate the selection of the “key” use cases. Measures for the architecture's quality characteristics are specified in details, precising attributes, units, numerical systems and scale types. A case study of a real-time application for monitoring stock exchanges illustrates our approach. We hope that our results will be particularly useful for practitioners, such as software architects, analysts and designers.
72|2||A flexible method for maintaining software metrics data: a universal metrics repository|A neglected aspect of software measurement programs is what will be done with the metrics once they are collected. Often databases of metrics information tend to be developed as an afterthought, with little, if any concessions to future data needs, or long-term, sustaining metrics collection efforts. A metric repository should facilitate an on-going metrics collection effort, as well as serving as the “corporate memory” of past projects, their histories and experiences. In order to address these issues, we describe a transformational view of software development that treats the software development process as a series of artifact transformations. Each transformation has inputs and produces outputs. The use of this approach supports a very flexible software engineering metrics repository.
72|2||Modeling change requests due to faults in a large-scale telecommunication system|It is widely known that a small number of modules in any system are likely to contain the majority of faults. Early identification and consequent attention to such modules may mitigate or prevent many defects. The objective of this study is to use product metrics to build a prediction model of the number of change requests (CRs) that are likely to occur in individual modules during testing. The study first empirically validates eight product metrics, while considering the confounding effects of code size (lines of code). Next, a prediction model of CR outcomes is developed with the validated metrics by utilizing a negative binomial regression that allows over-dispersion. In total, 816 modules written in the Chill programming language were analyzed in a large-scale telecommunication system. There is a positive association between the number of CRs and four product metrics (number of unique operators, unique operands, signals, and library calls) after considering the confounding effect of code size. A prediction model that includes only code size and the number of unique operands provides the best empirical fit.
72|2||Captureârecapture in software inspections after 10 years researchââtheory, evaluation and application|Software inspection is a method to detect faults in the early phases of the software life cycle. In order to estimate the number of faults not found, capture–recapture was introduced for software inspections in 1992 to estimate remaining faults after an inspection. Since then, several papers have been written in the area, concerning the basic theory, evaluation of models and application of the method. This paper summarizes the work made in capture–recapture for software inspections during these years. Furthermore, and more importantly, the contribution of the papers are classified as theory, evaluation or application, in order to analyse the performed research as well as to highlight the areas of research that need further work. It is concluded that (1) most of the basic theory is investigated within biostatistics, (2) most software engineering research is performed on evaluation, a majority ending up in recommendation of the Mh–JK model, and (3) there is a need for application experiences. In order to support the application, an inspection process is presented with decision points based on capture–recapture estimates.
72|2||A case study of a reusable component collection in the information retrieval domain|This paper reports on practical issues in the development, distribution, use, and evolution of a reusable component collection in the domain of information retrieval.
72|2||Changing perceptions of CASE technology|The level to which CASE technology has been successfully deployed in IS and software development organisations has been at best variable. Much has been written about an apparent mismatch between user expectations of the technology and the products which are developed for the growing marketplace. In this paper we explore how this tension has developed over time, with the aim of identifying and characterising the major factors contributing to it. We identify three primary themes: volatility and plurality in the marketplace; the close relationship between tools and development methods; and the context sensitivity of feature assessment. By exploring the tension and developing these themes we hope to further the debate on how to improve evaluation of CASE prior to adoption.
72|2||Prototyping an integrated information gathering system on CORBA|The sheer volume of information and variety of sources from which it may be retrieved on the Web make searching the sources a difficult task. Usually, meta-search engines can be used only to search Web pages or documents; other major sources such as data bases, library corpuses and the so-called Web data bases are not involved. Faced with these restrictions, an effective retrieval technology for a much wider range of sources becomes increasingly important. In our previous work, we proposed an Integrated Retrieval (IIR), which is based on Common Object Request Broker Architecture, to spare clients the trouble of complicated semantics when federating multiple sources. In this paper, we present an IIR-based prototype for integrated information gathering system. It offers a unified interface for querying heterogeneous interfaces or protocols of sources and uses SQL compatible query language for heterogeneous backend targets. We use it to link two general search engines (Yahoo and AltaVista), a science paper explorer (IEEE), and two library corpus explorers. We also perform preliminary measurements to assess the potential of the system. The results shown that the overhead spent on each source as the system queries them is within reason, that is, that using IIR to construct an integrated gathering system incurs low overhead.
72|3|http://www.sciencedirect.com/science/journal/01641212/72/3|Editorial board|
72|3||Code compression by register operand dependency|This paper proposes a dictionary-based code compression technique that maps the source register operands to the nearest occurrence of a destination register in the predecessor instructions. The key idea is that most destination registers have a great possibility to be used as source registers in the following instructions. The dependent registers can be removed from the dictionary if this information can be specified otherwise. Such destination–source relationships are so common that making use of them can result in much better code compression. After removing the dependent register operands, the original dictionary size can be reduced significantly. As a result, the compression ratio can benefit from: (a) the reduction of dictionary size due to the removal of dependent registers, and (b) the reduction of program encoding due to the reduced number of dictionary entries.
72|3||A page-coherent, causally consistent protocol for distributed shared memory|Distributed Shared Memory (DSM) provides a virtual address space that is shared among processors in a distributed system. It allows application programmers to elude message passing and use the more familiar shared-memory paradigm. To increase efficiency, DSM implementations replicate memory pages, introducing the problem of consistency. As fewer restrictions are imposed to the replicas, more efficient implementations are possible, but application programming becomes more difficult. Causal consistency is a model that offers a balance, by allowing efficient implementations without significantly increasing programming difficulty. This work presents a page-coherent, causally consistent DSM protocol. This protocol requires that each time a node sends the contents of a page p, it piggybacks a list of all pages the receiver must invalidate. This list includes all pages written by operations unknown to the receiver, and that potentially causally precede the most recent write operation over p known by the sender. The protocol’s main feature is less communication overhead than other causally consistent protocols in the literature. This behavior was confirmed running test applications on top of a simple simulator.
72|3||Adaptive message scheduling for supporting causal ordering in wide-area group communications|Most of existing causal ordering protocols employ the passive message ordering (PMO) approach, which passively re-orders the disturbed messages at destinations. Since the PMO approach cannot prevent disturbance of message ordering, seriously disturbed messages can overrun the receiving buffer of these protocols and cause damaging message loss. This paper proposes the adaptive message scheduling (AMS) approach, which introduces scheduling latency for messages at sources to alleviate the disturbance of message ordering at destinations. We develop a theoretical base for calculating the value of scheduling latency. The scheduling latency can adapt to varying channel latency and is related to the message ordering relation and the message lifetime. We then describe a new protocol that employs the AMS approach. On basis of a proposed model for evaluating group communication protocols, we conduct experiments and simulations to validate the AMS approach. The simulation results show that the AMS approach can effectively improve the ordering latency and reduce the receiving buffer size. A small penalty of employing the AMS approach is a larger loss ratio if strict real-time delivery of messages is to be guaranteed.
72|3||A new Petri net modeling technique for the performance analysis of discrete event dynamic systems|An interesting modeling problem is the need to model one or more of the system modules without exposition to the other system modules. This modeling problem arises due to our interest in these modules or incomplete knowledge, or inherent complexity, of the rest of the system modules. Whenever the performance measures (one or more) of the desired modules are available through previous performance studies, data sheets, or previous experimental works, the required performance measures for the whole system can be predicted from our proposed modeling technique. The incomplete knowledge problem of the dynamic behavior of some system modules has been studied by control theory. In the control area, such systems are known as partially observed discrete event dynamic systems, or POS systems. To the best of our knowledge, the performance evaluation of the POS system has not been addressed by the Petri net theory yet.
72|3||A new disk-based technique for solving the largeness problem of stochastic modeling formalisms|Stochastic modeling formalisms such as stochastic Petri nets, generalized stochastic Petri nets, and stochastic reward nets can be used to model and evaluate the dynamic behavior of realistic computer systems. Once we translate the stochastic system model to the underlying corresponding Markov Chain (MC), the developed MC grows wildly to several hundred thousands states. This problem is known as the largeness problem. To tolerate the largeness problem of Markov models, several iterative and direct methods have been proposed in the literature. Although the iterative methods provide a feasible solution for most realistic systems, a major problem appears when these methods fail to reach a solution. Unfortunately, the direct method represents an undesirable numerical technique for tolerating large matrices due to the fill-in problem. In order to solve such problem, in this paper, we develop a disk-based segmentation (DBS) technique based on modifying the Gauss Elimination (GE) technique. The proposed technique has the capability of solving the consequences of the fill-in problem without making assumptions about the underlying structure of the Markov processes of the developed model. The DBS technique splits the matrix into a number of vertical segments and uses the hard disk to store these segments. Using the DBS technique, we can greatly reduce the memory required as compared to that of the GE technique. To minimize the increase in the solution time due to the disk accessing processes, the DBS utilizes a clever management technique for such processes. The effectiveness of the DBS technique has been demonstrated by applying it to a realistic model for the Kanban manufacturing system.
72|3||Implementing innovative services supporting user and terminal mobility: the SCARAB architecture|The paper shows how secure telecommunication services supporting user and terminal mobility and service personalization can be designed using the SCARAB architecture, developed within the EU ACTS SCARAB project. The most important aspects of this architecture are the combined use of smart cards and mobile code technologies to solve the mobility and personalization issues, and the ability to support different types of terminals, ranging from powerful PC terminals to mobile personal digital assistants. These features are illustrated through the design of a demonstrative service.
72|3||A software/hardware cooperated stack operations folding model for Java processors|Java has become the most important language in the Internet area, but the execution performance of Java processors is severely limited by the true data dependency inherited from the stack architecture defined by Sun's Java Virtual Machine. A sequential hardware-based folding algorithm––POC folding model was proposed in the earlier research to eliminate up to 80.1% of stack push and pop bytecodes. The remaining stack push and pop bytecodes cannot be folded due to the sequential checking characteristic of the POC folding model. In this paper, a new software/hardware cooperated folding algorithm––T-POC (Tagged-POC) folding model is proposed to enhance the folding ability of the POC-based Java processors to fold the remaining stack push and pop bytecodes. While executing the bytecodes, bytecode grouping and rescheduling are done by a T-POC bytecode rescheduler to generate the new binary class images in memory. With the cooperation of the hardware-based POC folding model, higher execution performance can be achieved by executing the newly generated class images. Statistical data show that 94.8% of stack push and pop bytecodes can be folded, and the overall execution speedups of 2-, 3-, and 4-foldable strategies are 1.72, 1.73 and 1.74, respectively, as compared to a single-pipelined stack machine without folding.
72|3||The design and implementation of a runtime system for graph-oriented parallel and distributed programming|Graph has been widely used in modeling, specification, and design of parallel and distributed systems. Many parallel and distributed programs can be expressed as a collection of parallel functional modules whose relationships can be defined by a graph. Often, the basic functions of communication and coordination of the parallel modules are expressed in terms of the underlying graph. Furthermore, parallel/distributed graph algorithms are used to realize various control functions. To facilitate the implementation of these algorithms, it is desirable to have an integrated approach that provides direct support for efficient operations on graphs. We have proposed a graph-oriented programming model, called GOP, which aims at providing high-level abstractions for configuring and programming cooperative parallel processes. GOP enables the programmer to configure the logical structure of a distributed program by using a logical graph and to write the program using communications and synchronization primitives based on the logical structure. In this paper, we describe the design and implementation of a portable run-time system for the GOP framework. The runtime system provides an interface with a library of programming primitives to the low-level facilities required to support graph-oriented communications and synchronization. The implementation is on top of the parallel virtual machine in a local area network of Sun workstations. We focus our discussion on the following four aspects: the software architecture, including the structure of runtime system and interfaces between user programs and the runtime kernel; graph representation; implementation of graph operations; and performance of the run-time in terms of the implementation of graph-oriented communications.
72|3||A reactive system architecture for building fault-tolerant distributed applications|Most fault-tolerant application programs cannot cope with constant changes in their environments and user requirements because they embed policies and mechanisms together so that if the policies or mechanisms are changed the whole programs have to be changed as well. This paper presents a reactive system approach to overcoming this limitation. The reactive system concepts are an attractive paradigm for system design, development and maintenance because they separate policies from mechanisms. In the paper we propose a generic reactive system architecture and use group communication primitives to model it. We then implement it as a generic package which can be applied in any distributed applications. The system performance shows that it can be used in a distributed environment effectively.
72|3||Analyzing reconfigurable algorithms for managing replicated data|We analyze reconfigurable algorithms for managing replicated data to determine how often one should detect and react to failure conditions so that reorganization operations can be performed at the appropriate time to improve the availability of replicated data. We use dynamic voting as a case study to reveal design trade-offs for designing such reconfigurable algorithms and illustrate how often failure detection and reconfiguration activities should be performed so as to maximize data availability. Stochastic Petri nets are used as a tool to facilitate our analysis.
72|3||A novel algorithm for multimedia multicast routing in a large scale network|A new efficient multiple-searching genetic algorithm is presented for constructing minimum-cost multicast trees. In order to have computers successfully become consumer electronics for multimedia multicast communication applications, we propose a novel multimedia multicast routing approach for the large scale network in this paper. According to our simulation results, our algorithm is robust and can obtain optimal solution of a given problem with probability for both the sparse and dense networks. It outperforms some existing promising genetic algorithms.
72|3||The design and analysis of a quantitative simulator for dynamic memory management|The use of object-oriented programming in software development allows software systems to be more robust and more maintainable. At the same time, the development time and expense are also reduced. To achieve these benefits, object-oriented applications use dynamic memory management (DMM) to create generic objects that can be reused. Consequently, these applications are often highly dynamic memory intensive. For the last three decades, several DMM schemes have been proposed. Such schemes include first fit, best fit, segregated fit, and buddy systems. Because the performance (e.g., speed, memory utilization, etc.) of each scheme differs, it becomes a difficult choice in selecting the most suitable approach for an application and what parameters (e.g., block size, etc.) should be adopted.
72|3||A client-based logging technique using backward analysis of log in client/server environment|The existing recovery technique using the logging technique in client/server database systems only administers the log as a whole in a server. This recovery technique contains the logging record transmission cost on transactions that are potentially executed in each client and increases network traffic. In this paper, a logging technique for redo-only log is suggested. The new technique removes redundant before-images and supports client-based logging to eliminate the transmission cost of the logging record. Also, in the case of a client crash, redo recovery through a backward client analysis log is performed in a self-recovering way. In the case of a server crash, the after-images of pages which need recovery through simultaneous backward analysis log are only transmitted and redo recovery is accomplished through the received after-image and backward analysis log. We also select a comparison model to estimate the performance of the proposed recovery technique. Finally, in this paper, the superiority of the suggested technique was proved by comparing and analyzing redo time and recovery time of various techniques under changing number of clients and updated operations.
72|3||Contents Volume 72|
73|1|http://www.sciencedirect.com/science/journal/01641212/73/1|Editorial board|
73|1||Performance modeling and analysis of computer systems and networks|
73|1||Broadcasting schemes for hypercubes with background traffic|Optimal broadcasting schemes for interconnection networks (INs) are most essential for the efficient interprocess communication amongst parallel computers. In this paper two novel broadcasting schemes are proposed for hypercube computers with bursty background traffic and a single-port mode of message passing communication. The schemes utilize a maximum entropy (ME) based queue-by-queue decomposition algorithm for arbitrary queueing network models (QNMs) [D.D. Kouvatsos, I. Awan, Perform. Eval. 51 (2003) 191] and are based on binomial trees and graph theoretic concepts. It is shown that the overall cost of the one-to-all broadcasting scheme is given by max{Ï1,Ï2,…,Ï2n/2}, where Ïi, i=1,2,…,2n/2 is the total weight at each leaf node of the binomial tree and n is the degree of the hypercube. Moreover, the upper bound of the total cost of the neighbourhood broadcasting scheme is determined by ∑i=1Fmax{Ïi}, where F is an upper bound of the number of steps and is equal to 1.33⌈log2(n−1)⌉+1. Evidence based on empirical studies indicates the suitability of the schemes for achieving optimal broadcasting costs.
73|1||Design and analysis of a replicated elusive server scheme for mitigating denial of service attacks|The paper proposes a scheme, referred to as proactive server roaming, to mitigate the effects of denial of service (DoS) attacks. The scheme is based on the concept of “replicated elusive service”, which through server roaming, causes the service to physically migrate from one physical location to another. Furthermore, the proactiveness of the scheme makes it difficult for attackers to guess when or where servers roam. The combined effect of elusive service replication and proactive roaming makes the scheme resilient to DoS attacks, thereby ensuring a high-level of quality of service. The paper describes the basic components of the scheme and discusses a simulation study to assess the performance of the scheme for different types of DoS attacks. The details of the NS2-based design and implementation of the server roaming strategy to mitigate the DoS attacks are provided, along with a thorough discussion and analysis of the simulation results.
73|1||File distribution using a peer-to-peer networkââa simulation study|
73|1||How accurate should early design stage power/performance tools be? A case study with statistical simulation|To cope with the widening design gap, the ever increasing impact of technology, reflected in increased interconnect delay and power consumption, and the time-consuming simulations needed to define the architecture of a microprocessor, computer engineers need techniques to explore the design space efficiently in an early design stage. These techniques should be able to identify a region of interest with desirable characteristics in terms of performance, power consumption and cycle time. In addition, they should be fast since the design space is huge and the design time is limited. In this paper, we study how accurate early design stage techniques should be to make correct design decisions. In this analysis we focus on relative accuracy which is more important than absolute accuracy at the earliest stages of the design flow. As a case study we demonstrate that statistical simulation is capable of making viable microprocessor design decisions efficiently in early stages of a microprocessor design while considering performance, power consumption and cycle time.
73|1||Approximating the connectivity between nodes when simulating large-scale mobile ad hoc radio networks|Simulation is a widely used technique in the design and evaluation of mobile ad hoc networks. However, time and space constraints can often limit the extent of the size, complexity and detail of the networks that can be simulated. Approximations in the system model can possibly alleviate this problem, but we need to be careful about how much accuracy is compromised when employing them.
73|1||Proactive QoS negotiation in asynchronous real-time distributed systems|We present a fast, proactive, quality of service (QoS) negotiation algorithm called Best Effort Negotiation (or BEN), for asynchronous real-time distributed systems. BEN considers an application model where trans-node application timeliness and fault-tolerance requirements are expressed using benefit functions, and anticipated workload and system failure rates during future time intervals are expressed using adaptation functions and reliability functions, respectively. Furthermore, BEN considers an adaptation model where subtasks of application tasks are replicated at run-time for tolerating failures as well as for sharing workload increases. Given such models, the objective of the algorithm is to maximize the sum of aggregate real-time and fault-tolerance benefits during the time window of adaptation functions. Since determining the optimal solution is computationally intractable, BEN heuristically computes sub-optimal resource allocations in polynomial-time. To determine how well BEN performs, we describe another algorithm called HLC, that is inspired by the well-known Hill Climbing heuristic. We show that HLC is significantly slower than BEN. However, our experimental studies reveal that the performance of BEN, in general, is as good as that of HLC.
73|1||Feasibility analysis of hard real-time periodic tasks|The problem of determining the feasibility of hard real-time periodic tasks is known to be co-NP-complete when there exists a task with relative deadline shorter than its period. Thus “Processor Demand Approach (PDA)” for synchronous task sets has been considered as a practical tool to solve the feasibility problem. PDA determines the feasibility of a task set by checking whether there exists a task whose deadline is missed by a certain time which is computed based on the characteristic of the task set. In this paper, we present a new method for feasibility test by combining PDA with the analysis methods for aperiodic scheduling. It is shown that the number of tests required of the new method to determine the feasibility is never greater than the smallest number of tests the existing algorithm requires. Although our method has pseudo-polynomial time complexity, experimental results show that the new method requires significantly less computation to determine feasibility.
73|1||A cost model for spatio-temporal queries using the TPR-tree|A query optimizer requires cost models to calculate the costs of various access plans for a query. An effective method to estimate the number of disk (or page) accesses for spatio-temporal queries has not yet been proposed. The TPR-tree is an efficient index that supports spatio-temporal queries for moving objects. Existing cost models for the spatial index such as the R-tree do not accurately estimate the number of disk accesses for spatio-temporal queries using the TPR-tree, because they do not consider the future locations of moving objects, which change continuously as time passes.
73|1||An efficient query optimization strategy for spatio-temporal queries in video databases|The interest for multimedia database management systems has grown rapidly due to the need for the storage of huge volumes of multimedia data in computer systems. An important building block of a multimedia database system is the query processor, and a query optimizer embedded to the query processor is needed to answer user queries efficiently. Query optimization problem has been widely studied for conventional database systems; however it is a new research area for multimedia database systems. Due to the differences in query processing strategies, query optimization techniques used in multimedia database systems are different from those used in traditional databases. In this paper, a query optimization strategy is proposed for processing spatio-temporal queries in video database systems. The proposed strategy includes reordering algorithms to be applied on query execution tree. The performance results obtained by testing the reordering algorithms on different query sets are also presented.
73|1||ACODF: a novel data clustering approach for data mining in large databases|In this paper, we present an efficient clustering approach for large databases. Our simulation results indicate that the proposed novel clustering method (called ant colony optimization with different favor algorithm) performs better than the fast self-organizing map (SOM) combines K-means approach (FSOM+K-means) and genetic K-means algorithm (GKA). In addition, in all the cases we studied, our method produces much smaller errors than both the FSOM+K-means approach and GKA.
73|1||On accessing data in high-dimensional spaces: A comparative study of three space partitioning strategies|While experience shows that contemporary multi-dimensional access methods perform poorly in high-dimensional spaces, little is known about the underlying causes of this important problem. One of the factors that has a profound effect on the performance of a multi-dimensional structure in high-dimensional situations is its space partitioning strategy. This paper investigates the partitioning strategies of KDB-trees, the Pyramid Technique, and a new point access method called the Îs Technique. The paper reveals important dimensionality problems associated with these strategies and shows how each strategy affects the retrieval performance across a range of spaces with varying dimensionalities. The Pyramid Technique, which is frequently regarded as the state-of-the-art access method for high-dimensional data, suffers from numerous problems that become particularly severe with highly skewed data in heavily sparse spaces. Although the partitioning strategy of KDB-trees incurs several problems in high-dimensional spaces, it exhibits a remarkable adaptability to the changing data distributions. However, the experimental evidence gathered on both simulated and real data sets shows that the Îs Technique generally outperforms the other two schemes in high-dimensional spaces, usually by a significant margin.
73|1||Supporting metasearch with XSL|Metasearch engines offer better coverage and are more fault-tolerant and expandable than single search engines. A metasearch engine is required to post queries with and obtain retrieval results from several other Internet search engines. In this paper, we describe the use of the extensible style language (XSL) to support metasearches. We show how XSL can transform a query, expressed in XML, into different forms for different search engines. We show how the retrieval results could be transformed into a standard format so that the metasearch engine can interpret the retrieved data, filtering the irrelevant information (e.g. advertisement). The proposed structure treats the metasearch engine and the individual search engines as separate modules with a clearly defined communication structure through XSL. Thus, the system is more extensible than coding the structure and syntactic transformation processes. It allows other new search engines to be included just through plug-and-play, requiring only that the new transformation of XML for this search engine be included in the XSL.
73|1||A top-down approach for density-based clustering using multidimensional indexes|Clustering on large databases has been studied actively as an increasing number of applications involve huge amount of data. In this paper, we propose an efficient top-down approach for density-based clustering, which is based on the density information stored in index nodes of a multidimensional index. We first provide a formal definition of the cluster based on the concept of region contrast partition. Based on this notion, we propose a novel top-down clustering algorithm, which improves the efficiency through branch-and-bound pruning. For this pruning, we present a technique for determining the bounds based on sparse and dense internal regions and formally prove the correctness of the bounds. Experimental results show that the proposed method reduces the elapsed time by up to 96 times compared with that of BIRCH, which is a well-known clustering method. The results also show that the performance improvement becomes more marked as the size of the database increases.
73|2|http://www.sciencedirect.com/science/journal/01641212/73/2|Editorial board|
73|2||Applications of statistics in software engineering|
73|2||Statistical significance testingââa panacea for software technology experiments?|Empirical software engineering has a long history of utilizing statistical significance testing, and in many ways, it has become the backbone of the topic. What is less obvious is how much consideration has been given to its adoption. Statistical significance testing was initially designed for testing hypotheses in a very different area, and hence the question must be asked: does it transfer into empirical software engineering research? This paper attempts to address this question. The paper finds that this transference is far from straightforward, resulting in several problems in its deployment within the area. Principally problems exist in: formulating hypotheses, the calculation of the probability values and its associated cut-off value, and the construction of the sample and its distribution. Hence, the paper concludes that the topic should explore other avenues of analysis, in an attempt to establish which analysis approaches are preferable under which conditions, when conducting empirical software engineering studies.
73|2||BBN-based software project risk management|This paper presents a scheme to incorporate BBNs (Bayesian belief networks) in software project risk management. A theoretical model is defined to provide insights into risk management. Based on these insights, we have developed a BBN-based procedure using a feedback loop to predict potential risks, identify sources of risks, and advise dynamic resource adjustment. This approach facilitates the visibility and repeatability of the decision-making process of risk management. Both analytical and simulated cases are reported.
73|2||Using multiple adaptive regression splines to support decision making in code inspections|Inspections have been shown to be an effective means of detecting defects early on in the software development life cycle. However, they are not always successful or beneficial as they are affected by a number of technical and managerial factors. To make inspections successful, one important aspect is to understand what are the factors that affect inspection effectiveness (the rate of detected defects) in a given environment, based on project data. In this paper we collected data from over 230 code inspections and performed a multivariate statistical analysis in order to look at how management factors, such as the effort assigned and the inspection rate, affect inspection effectiveness. Because the functional form of effectiveness models is a priori unknown, we use a novel exploratory analysis technique: multiple adaptive regression splines (MARS). We compare the MARS model with more classical regression models and show how it can help understand the complex trends and interactions in the data, without requiring the analyst to rely on strong assumptions. Results are reported and discussed in light of existing studies.
73|2||Computing system reliability using Markov chain usage models|Markov chains have been used successfully to model system use, generate tests, and compute statistics about anticipated system use in the field. Several reliability models are in use for Markov chain-based testing, but each has certain limitations. A Bayesian reliability model that is gaining support in field use is presented here.
73|2||Applications of clustering techniques to software partitioning, recovery and restructuring|The artifacts constituting a software system are sometimes unnecessarily coupled with one another or may drift over time. As a result, support of software partitioning, recovery, and restructuring is often necessary. This paper presents studies on applying the numerical taxonomy clustering technique to software applications. The objective is to facilitate those activities just mentioned and to improve design, evaluation and evolution. Numerical taxonomy is mathematically simple and yet it is a useful mechanism for component clustering and software partitioning. The technique can be applied at various levels of abstraction or to different software life-cycle phases. We have applied the technique to: (1) software partitioning at the software architecture design phase; (2) grouping of components based on the source code to recover the software architecture in the reverse engineering process; (3) restructuring of a software to support evolution in the maintenance stage; and (4) improving cohesion and reducing coupling for source code. In this paper, we provide an introduction to the numerical taxonomy, discuss our experiences in applying the approach to various areas, and relate the technique to the context of similar work.
73|2||Assessing the cost-effectiveness of software reuse: A model for planned reuse|Information systems development is typically acknowledged as an expensive and lengthy process, often producing code that is of uneven quality and difficult to maintain. Software reuse has been advocated as a means of revolutionizing this process. The claimed benefits from software reuse are reduction in development cost and time, improvement in software quality, increase in programmer productivity, and improvement in maintainability. Software reuse entails undeniable costs of creating, populating, and maintaining a library of reusable components. There is anecdotal evidence to suggest that some organizations benefit from reuse. However, many software developers practicing reuse claim these benefits without formal demonstration thereof. There is little research to suggest when the benefits are expected and to what extent they will be realized. For example, does a larger library of reusable components lead to increased savings? What is the impact of component size on the effectiveness of reuse? This research seeks to address some of these questions. It represents the first step in a series wherein the effects of software reuse on overall development effort and costs are modeled with a view to understanding when it is most effective.
73|2||Applying sampling to improve software inspections|The main objective of software inspections is to find faults in software documents. The benefits of inspections are reported from researchers as well as software organizations. However, inspections are time consuming and the resources may not be sufficient to inspect all documents. Sampling of documents in inspections provides a systematic solution to select what to be inspected in the case resources are not sufficient to inspect everything. The method presented in this paper uses sampling, inspection and resource scheduling to increase the efficiency of an inspection session. A pre-inspection phase is used in order to determine which documents need most inspection time, i.e. which documents contain most faults. Then, the main inspection is focused on these documents. We describe the sampling method and provide empirical evidence, which indicates that the method is appropriate to use. A Monte Carlo simulation is used to evaluate the proposed method and a case study using industrial data is used to validate the simulation model. Furthermore, we discuss the results and important future research in the area of sampling of software inspections.
73|2||Resource constraints analysis of workflow specifications|A workflow specification is a formal description of business processes in the real world. Its correctness is critical to the workflow execution and hence the realisation of business objectives. In addition to structural and temporal constraints, resource constraints are also implied in workflow specifications. Therefore, they should be analysed to ensure that the workflow specification is resource consistent at build-time. In this paper, we first identify the problem of resource constraints in a workflow specification. Then we propose an innovative approach with corresponding algorithms to the checking of resource consistency for a workflow specification. Furthermore, we extend our analysis work to timed workflow specifications, where time information is taken into consideration for the checking of the resource consistency of a workflow specification. The work reported in this paper provides a theoretical foundation for workflow modeling and analysis in workflow management.
73|2||Multi-devices âMultipleâ user interfaces: development models and research opportunities|Today, Internet-based appliances can allow a user to interact with the server-side services and information using different kinds of computing platforms including traditional office desktops, palmtops, as well as a large variety of wireless devices including mobile telephones, Personal Digital Assistants, and Pocket Computers. This technological context imposes new challenges in user interface software engineering, as it must run on different computing platforms accommodating the capabilities of various devices and the different contexts of use. Challenges are triggered also because of the universal access requirements for a diversity of users. The existing approaches of designing a single user interface using one computing platform do not adequately address the challenges of diversity, cross-platform consistency, universal accessibility and integration. Therefore, there is an urgent need for a new integrative framework for modeling, designing and evaluating multi-device user interfaces for the emerging generation of interactive systems. This paper begins by describing a set of constraints and characteristics intrinsic to multi-device user interfaces, and then by examining the impacts of these constraints on the specification, design and validation processes. Then, it discusses the research opportunities in important topics relevant to multi-device user interface development, including task and model-based, pattern-driven and device-independent development. We will highlight how research in these topics can contribute to the emergence of an integrative framework for Multiple-User Interface design and validation.
73|2||A simple and powerful type system for programming languages|A type system serves to guarantee that programs mix data of distinct types only as allowed by programmer declared rules. For example, a function declaration corresponds to one of these rules, providing a way to mix data of distinct types, possibly producing data of another type as a result. A type system also discovers the type of each (sub)expression throughout a program.
73|2||Knowledge centered assessment pattern: an effective tool for assessing safety concerns in software architecture|In software-based systems, the notion of software failure is magnified if the software in question is a component of a safety critical system. Hence, to ensure a required level of safety, the product must undergo expensive rigorous testing and verification/validation activities. To minimize the cost of quality (COQ) associated with the development of safety critical systems, it becomes imperative that the assessment of intermediate artifacts (e.g., requirement, design documents or models) is done efficiently and effectively to maximize early defect detection and/or defect prevention. However, as a human-centered process, the assessment of software architecture for safety critical systems relies heavily on the experience and knowledge of the assessment team to ensure that the proposed architecture is consistent with the software functional and safety requirements.
73|2||Selecting components in large COTS repositories|The growing availability of COTS (commercial-off-the-shelf) components in the software market has concretized the possibility of building whole systems based on components. In this multitude, a recurrent problem is the location and selection of the components that best fit the requirements. Commercial repositories that offer search mechanisms have reduced these difficulties: system integrators can rely on a wider variety of components and can focus better on the composition of systems. The size of the repository can be an initial obstacle but iterative approaches allow integrators to familiarize with the repository's structure and to formulate effective queries. This paper discusses the search techniques in CLARiFi, a component broker project that supports integrators in the selection of components for systems.
73|2||A framework instantiation approach based on the Features Model|The extreme competitiveness of the contemporary economy generates a huge demand for cheaper, efficient and reliable software products, which often are developed under great pressures of time and budget. These premises suggest that software development must take place in an environment where proven solutions can be modified, combined and adapted to be used in the development of new products. Therefore, the use of object-oriented frameworks, or frameworks for short, seems to be one of the most promising techniques for code and design reuse. Nevertheless, one of the obstacles that has to be removed before the widespread use of frameworks is the great amount of time for study necessary to become proficient in the use of a specific framework. This situation occurs due to the inherent complexity of the design of the frameworks, which are conceived to fulfill the requirements of an entire application family.
73|2||Quantitative evaluation of safety critical software testability based on fault tree analysis and entropy|One of the definitions for testability is the probability whether tests will detect a fault, given that a fault in the program exists. The testability can be estimated from the probability of each statement fault leading to output failure. The probability of the test detecting a fault depends on the probability of individual statement faults appearing as an output failure when a fault exists at a statement. The testability measure of the software has been introduced based on output failure probability and the entropy of the importance of basic statements to the output failure from the software fault tree analysis. The output failure probability and the importance of statements are calculated from software fault tree analysis. The suggested testability measure has been applied to the two modules of the safety system in a nuclear power plant. The proposed testability measure can be used for the selection of output variables or to determine the modules that are more vulnerable to undetected faults.
73|2||Task-directed software inspection|Software inspection is recognized as an effective verification technique. Despite this fact, the use of inspection is surprisingly low. This paper describes a new inspection technique, called task-directed inspection (TDI), and a light-weight process, that were used to introduce inspection in a particular industrial environment. This environment had no history of inspections, was resistant to the idea of inspection, but had a situation where confidence in a safety-related legacy suite of software had to be increased. The characteristics of TDI are explored. They give rise to a variety of approaches that may encourage more widespread use of inspections. This paper examines the industrial exercise as a case study, with the intent that it be useful in other situations that share characteristics with the situation described.
73|3|http://www.sciencedirect.com/science/journal/01641212/73/3|Editorial board|
73|3||Solving the invalid signer-verified signature problem and comments on XiaâYou group signature|In general, a signature that has been verified by the signers should be valid when the verifiers verify it later. However, a signer-verified signature would become invalid in Wang et al.'s generalized threshold signature and authenticated encryption scheme and Hsu et al.'s revisions. This problem has not been reported before, and it is called the invalid signer-verified signature problem. This paper discusses the problem and proposes the solution. By the way, we also point out the forgery attack on Xia–You recently published group signature scheme.
73|3||Role-based authorizations for workflow systems in support of task-based separation of duty|Role-based authorizations for assigning tasks of workflows to roles/users are crucial to security management in workflow management systems. The authorizations must enforce separation of duty (SoD) constraints to prevent fraud and errors. This work analyzes and defines several duty-conflict relationships among tasks, and designs authorization rules to enforce SoD constraints based on the analysis. A novel authorization model that incorporates authorization rules is then proposed to support the planning of assigning tasks to roles/users, and the run-time activation of tasks. Different from existing work, the proposed authorization model considers the AND/XOR split structures of workflows and execution dependency among tasks to enforce separation of duties in assigning tasks to roles/users. A prototype system is developed to realize the effectiveness of the proposed authorization model.
73|3||Fuzzy resource space model and platform|The resource space model (RSM) is a model for organizing versatile resources in normal forms and providing uniform resource management operations. In applications, we find three important factors that influence the effectiveness of organizing and operating resources: natural semantics of resources, resource providers' beliefs, and resource users' beliefs. The relationship between the three factors influences the effectiveness of resource management operations. This paper proposes a fuzzy resource space model (FRSM), which consists of a fuzzy resource space and a fuzzy operation language expressing the resource providers' beliefs and the resource users' beliefs. By properly dealing with the two kinds of beliefs, the FRSM improves the RSM in effectively managing resources. The proposed model has been implemented in the Knowledge Grid platform VEGA-KG.
73|3||A new design of efficient partially blind signature scheme|Recently, the techniques of the partially blind signatures played an important role in many electronic commerce applications. Therefore, to guarantee the quality of the growing popular communication services, it is urgent to construct low-computation partially blind signature scheme for both parties of the signer and the requester to obtain a signature. So far there is no efficient and secure partially blind signature scheme for the signer and a signature requester. Based on the discrete logarithm and the Chinese Remainder Theorem, we propose an efficient and secure partially blind signature scheme for both the signer and the requester to obtain a signature. Moreover, the proposed scheme is easier than the existing partially blind signature schemes in the blinding procedure. Hence, our proposed scheme is suitable for the limited computation capacities of requesters such as smart cards or mobile units. Similarly, our scheme can let the signer provide more efficient services to the signature requester. So, it is very useful for many applications.
73|3||Secret image sharing with steganography and authentication|A novel approach to secret image sharing based on a (k,n)-threshold scheme with the additional capabilities of steganography and authentication is proposed. A secret image is first processed into n shares which are then hidden in n user-selected camouflage images. It is suggested to select these camouflage images to contain well-known contents, like famous character images, well-known scene pictures, etc., to increase the steganographic effect for the security protection purpose. Furthermore, an image watermarking technique is employed to embed fragile watermark signals into the camouflage images by the use of parity-bit checking, thus providing the capability of authenticating the fidelity of each processed camouflage image, called a stego-image. During the secret image recovery process, each stego-image brought by a participant is first verified for its fidelity by checking the consistency of the parity conditions found in the image pixels. This helps to prevent the participant from incidental or intentional provision of a false or tampered stego-image. The recovery process is stopped if any abnormal stego-image is found. Otherwise, the secret image is recovered from k or more authenticated stego-images. Some effective techniques for handling large images as well as for enhancing security protection are employed, including pixelwise processing of the secret image in secret sharing, use of parts of camouflage images as share components, adoption of prime-number modular arithmetic, truncation of large image pixel values, randomization of parity check policies, etc. Consequently, the proposed scheme as a whole offers a high secure and effective mechanism for secret image sharing that is not found in existing secret image sharing methods. Good experimental results proving the feasibility of the proposed approach are also included.
73|3||A practical experience in workspace separation for developing multiple storefronts on customized commerce engines|In this paper, we describe our experience in separating workspaces, using the IBM VisualAge for Java development tool, for multiple web storefronts development for several major IBM public and private commerce sites. There was a need to create multiple workspaces since different storefronts may be developed at the same time. In addition, we wanted to keep the completed software packages for compatibility checking. Instead of tuning the current program setting to a desired mixed-version environment for each project, we generate a dedicated workspace by applying dependency analysis. New icons are created to conveniently invoke the development tool under different software packages and workspaces. We also present a systematic approach to facilitate the generation of a plan with instruction steps to produce new workspaces for their dedicated and required resources.
73|3||Providing flexible access control to an information flow control model|Protecting privacy within an application is essential. Many information flow control models have been developed for that protection. We developed an information flow control model based on role-based access control (RBAC) for object-oriented systems, which is called OORBAC (object-oriented role-based access control). According to the experiences of using OORBAC, we found that a model allowing every secure information flow and blocking every non-secure flow is too restricted. We propose that the following flexible access control features should be offered: (a) non-secure but harmless information flows should be allowed and (b) secure but harmful information flows should be blocked. According to our survey, no existing model offers the above control. We thus revised OORBAC to offer the control. The revised OORBAC have been implemented and evaluated. This paper presents flexible access control in the revised OORBAC and the evaluation result.
73|3||Secure one snapshot protocol for concurrency control in real-time stock trading systems|To prevent any data from being accessed by unauthorized users, it is necessary for stock trading systems (STS) to use multilevel secure database management systems in controlling concurrent executions among multiple transactions. In STS, analytical transactions as well as mission critical transactions are executed concurrently, which makes it difficult to use traditional secure real-time transaction management schemes for STS environment. In this paper, we propose the read-down relationship-based secure one snapshot protocol (SOS) that is devised for the secure real-time transaction management in STS. By maintaining an additional one snapshot as well as working database, SOS blocks covert-channels without causing the priority inversion phenomenon. We introduce the process of SOS protocol with some examples, present the proofs of devised protocol, and then evaluate the performance gains by means of simulation method.
73|3||Retrieve images by understanding semantic links and clustering image fragments|The main obstacle to realize real semantic-based image retrieval is that semantic description of versatile images is difficult. The basic ideas of this paper are that the semantics of an object can be refined through top–down orthogonal semantic classification and that the semantics of an object can be reflected by the semantics of relevant objects and the semantic relationships between them. To reflect the semantic relationships between images, we propose a set of primitive semantic links as the enhancement of the hyperlinks connecting Web pages. The semantic link network is the natural extension of the hyperlink network so it can inherit the existing theory and method on hyperlink network. Based on the single semantic image established upon the orthogonal semantic space and the semantic link space, the proposed image retrieval approach enables users to obtain the semantic clustering of relevant images rather than a list of isolated images as the output of the current search engine and to browse images along semantic paths with the support of semantic link reasoning. Semantic matching and reasoning for realizing intelligent semantic image retrieval is based on graph operation and matrix operation.
73|3||A simulated annealing approach for multimedia data placement|Multimedia applications are characterized by their strong timing requirements and constraints and thus multimedia data storage is a critical issue in the overall system's performance and functionality. This paper describes multimedia data representation models that effectively guide data placement towards the improvement of the Quality of Presentation for the considered multimedia applications. The performance of both constructive placement and iterative improvement placement algorithms is evaluated and discussed. Emphasis is given on placement schemes which are based on the simulated annealing optimization algorithm. A placement policy, based on a self-improving version of the simulated annealing (SISA) algorithm is applied and evaluated. Performance of the placement policies is experimentally evaluated on a simulated tertiary storage subsystem. As proven by the experimentation, the proposed approach shows considerable gain in terms of seek and service times. The improvements of the proposed SISA approach are in the range of 40% when compared to random placement and at the range of 15–35% when compared to the typical simulated annealing algorithm, depending a lot on the initial configuration and the neighborhood search.
73|3||Temporal moving pattern mining for location-based service|The primary objective of location-based service (LBS) which is generally described as a mobile information service is to provide useful location aware information, at a minimum cost and resources, to its users. This functionality can be implemented through data mining techniques. However, since the conventional studies on data mining do not consider spatial and temporal aspects of data simultaneously, these techniques have limited application in studying the moving objects of LBS with respect to the spatial attributes that is changing over time. Defining individual users of LBS as moving objects, this paper proposes a new data mining technique and algorithms for identifying temporal patterns from series of locations of moving objects that have temporal and spatial dimensions. For this purpose, we use the spatial operation to generalize a location of moving point, applying time constraints between locations of moving objects to make valid moving sequences. Through the experiments, we show that our technique generates temporal patterns found in frequent moving sequences in efficient. Finally, the spatio-temporal technique proposed in this work is an innovative approach in providing knowledge applicable to improving the quality of LBS.
73|3||Performance evaluation of a database of repetitive elements in complete genomes|The analysis of repetitive elements reveals repetitive elements in our genome may have been very important in the evolutionary genomics. In this work, we propose approaches to improve the performance of the repetitive elements in a database, namely Repetitive Sequence Database (RSDB).1 There are hundreds of millions of repetitive elements in RSDB. Performance evaluation and tuning are critical for us to manage such a large database. Not only does this study have a lot of performance improvements on centralized RSDB, but also this study provides a comparison of performance between centralized RSDB and distributed RSDB. From the experimental results, we find the improvements proposed speed up our RSDB very much.
73|3||On the efficiency of nonrepudiable threshold proxy signature scheme with known signers|In the (t,n) proxy signature scheme, the signature signed by the original signer can be signed by t or more proxy signers out of a proxy group of n proxy signers. Recently, Hsu et al. proposed a nonrepudiable threshold proxy signature scheme with known signers. In this article, we shall propose an improvement of Hsu et al.'s scheme that is more efficient in terms of computational complexity and communication cost.
73|3||A video caching policy for providing differentiated service grades and maximizing system revenue in hierarchical video servers|A video server normally targets at providing abundant bandwidth access and massive storage in supporting large-scale video archival applications. Its performance is sensitive to the deployment of the stored contents. In this paper, we propose a video caching policy for a video server, based on the knowledge of video profiles, namely: access rate, video size and bandwidth, tolerable rejection probability, and rental price. We consider the video server as having a hierarchical architecture which consists of a set of high-speed disk drives located in the front end for caching a subset of videos, and another set of high-capacity tertiary devices located in the back end for archiving the entire video collection. The front-end disks particularly, are organized together by employing a proposed data striping scheme, termed the adaptive striping (AS), which is flexible on heterogeneous disk integration. The proposed policy determines what video set should be cached, and how to arrange them in the front-end disks with two objectives in mind: (1) offering differentiated service grades conforming to the video profiles as well as (2) maximizing the overall system revenue. We simulate the system with various configurations, and the results affirm our effective approach.
73|3||Modeling and implementation of digital rights|The widespread use of the Internet raises issues regarding intellectual property. After content is downloaded, no further protection is provided on that content. DRM (Digital Rights Management) technologies were developed to ensure the protection of digital information. In this paper, we explore the issues associated with content distribution and digital rights management. We develop a modified Imprimatur model based on a multi-level content distribution model and, based on those two models, we present the formal model of a new DRM system and identify the properties that a DRM system should have. We design a prototype DRM system that distributes due royalties among participants, enforces content usage rules, and distributes content keys securely in the content distribution channel.
73|3||An adaptable vertical partitioning method in distributed systems|Vertical partitioning is a process of generating the fragments, each of which is composed of attributes with high affinity. The concept of vertical partitioning has been applied to many research areas, especially databases and distributed systems, in order to improve the performance of query execution and system throughput. However, most previous approaches have focused their attention on generating an optimal partitioning without regard to the number of fragments finally generated, which is called best-fit vertical partitioning in this paper. On the other hand, there are some cases that a certain number of fragments are required to be generated by vertical partitioning, called n-way vertical partitioning in this paper. The n-way vertical partitioning problem has not fully investigated.
73|3||Contents Volume 73|
74|1|http://www.sciencedirect.com/science/journal/01641212/74/1|Editorial board|
74|1||Contents|
74|1||Automated Component-Based Software Engineering|
74|1||Deployed software component testing using dynamic validation agents|Software component run-time characteristics are highly dependent on their actual deployment situation. Validating that software components meet required functional and non-functional properties is time consuming and for some properties quite challenging. We describe the use of “validation agents” to automate the testing of deployed software components to verify that component functional and non-functional properties are met. Our validation agents utilise “component aspects” that describe functional and non-functional cross-cutting concerns impacting software components. Aspect information is queried by our validation agents and these construct and run automated tests on the deployed software components. The agents then determine if the deployed components meet their aspect-described requirements. Some agents deploy an existing performance test-bed generation tool to run realistic loading tests on these components. We describe the motivation for our work, how component aspects are designed and encoded, our automated agent-based testing process, the architecture and implementation or our validation agents, and our experiences using them.
74|1||Towards automatic monitoring of component-based software systems|The quality of software components is very important for the overall service quality of the component-based software systems. Several factors make exhaustive testing of components very difficult. Furthermore, the behavioral correctness of each independently produced component does not guarantee the behavioral correctness of the composed software system. Experience shows that there are faults in components which elude the testing effort and do not surface until the system is operating. In this paper, a specification-based software monitor is presented which can be used for detecting certain kinds of errors and failures of a component as well as the whole system while the system is operating. The behavior of each component is assumed to be specified in a formalism based on communicating finite state machines with addressing variables, and inter-component communications are achieved via asynchronous message passing. The monitor passively observes the external input/output and receives partial state information of the target system or component. These are used to interpret the specification. The approach is compositional as it achieves global monitoring by analyzing the behavior of the components of a system individually, and then combining the results obtained from the independent component analyses. The paper describes the architecture and operations of the monitor and includes illustrative examples. Techniques for dealing with non-determinism and concurrency issues in monitoring a concurrent component-based system are also discussed.
74|1||A data-centric approach to composing embedded, real-time software components|Software for embedded systems must cope with a variety of stringent constraints, such as real-time requirements, small memory footprints, and low power consumption. It is usually implemented using low-level programming languages, and as a result has not benefitted from component-based software development techniques. This paper describes a data-centric component model for embedded devices that (i) minimizes the number of concurrent tasks needed to implement the system, (ii) allows one to verify whether components meet their deadlines by applying rate monotonic analysis, and (iii) can generate and verify schedules using constraint logic programming. This model forms the foundation for a suite of tools for specifying, composing, verifying and deploying embedded software components developed in the context of the Pecos project.
74|1||Performance prediction of component-based applications|One of the major problems in building large-scale enterprise systems is anticipating the performance of the eventual solution before it has been built. The fundamental software engineering problem becomes more difficult when the systems are built on component technology. This paper investigates the feasibility of providing a practical solution to this problem. An empirical approach is proposed to determine the performance characteristics of component-based applications by benchmarking and profiling. Based on observation, a model is constructed to act as a performance predictor for a class of applications based on the specific component technology. The performance model derived from empirical measures is necessary to make the problem tractable and the results relevant. A case study applies the performance model to an application prototype implemented by two component infrastructures: CORBA and J2EE.
74|1||A formal approach to component adaptation|Component adaptation is widely recognised to be one of the crucial problems in Component-Based Software Engineering (CBSE). We present a formal methodology for adapting components with mismatching interaction behaviour. The three main ingredients of the methodology are: (1) the inclusion of behaviour specifications in component interfaces, (2) a simple, high-level notation for expressing adaptor specifications, and (3) a fully automated procedure to derive concrete adaptors from given high-level specifications.
74|1||âComputer, please, tell me what I have to doâ¦â: an approach to agent-aided application composition|The process of starting to use any reuse technology is usually one of the most frustrating factors for novice users. For this reason, tools able to reduce the learning curve are valuable to augment the potential of the technology to rapidly build new applications. In this work, we present Hint, an environment for assisting the instantiation of Java applications based on software agents technology. Hint is built around a software agent that has the knowledge about how to use a reusable asset and, using this knowledge, is able to propose a sequence of programming activities that should be carried out in order to implement a new application satisfying the functionality the user wants to implement. The most relevant contribution of this work is the use of planning techniques to guide the execution of instantiation activities for a given technology.
74|1||Automated support for service-based software development and integration|A service-based development paradigm is one in which components are viewed as services. In this model, services interact and can be providers or consumers of data and behavior. Applications in this paradigm dynamically integrate services at runtime-based on available resources. This paper describes an architecture-based approach for the creation of services and their subsequent integration with service-requesting client applications.
74|1||A formal software requirements specification method for digital nuclear plant protection systems|This article describes NuSCR, a formal software requirements specification method for digital plant protection system in nuclear power plants. NuSCR improves the readability and specifiability by providing graphical or tabular notations depending on the type of operations. NuSCR specifications can be formally analyzed for completeness, consistency, and against the properties specified in temporal logic. We introduce the syntax and semantics of NuSCR and demonstrate the effectiveness of the approach using reactor protection system, digital protection system being developed in Korea, as a case study.
74|1||Some successful approaches to software reliability modeling in industry|Over the past three years, we have been actively engaged in both software reliability growth modeling and architecture-based software reliability modeling for projects at Lucent Technologies. Our goal has been to include software into the overall reliability evaluation of a product design using either or both of these two fundamentally different approaches. During the course of our application efforts to real projects, we have identified practical difficulties with each approach. The application of software reliability growth models, for example, is plagued by widespread use of ad hoc test environments, and the use of architecture-based software reliability models is plagued by a large number of unknown parameters. In this paper, we discuss our methods for overcoming these and other practical difficulties. In particular, we show how calibration factors can be defined and used to adjust for the mismatch between the test and operational profiles of the software. We also present two useful ways to do sensitivity analyses that help alleviate the problem of so many uncertainties in the architecture-based modeling approach. We illustrate our methods with case studies, and offer comments on further work that is required to more satisfactorily bridge the gap between theory and applications in this research area.
74|1||Software requirement understanding using Pathfinder networks: discovering and evaluating mental models|Understanding and communicating user requirements in a software requirement analysis effort is very important. Misunderstandings of user requirements between software developers and users, will cause problems in terms of satisfying user needs, defects, cost and schedule during the software development process. This paper proposes a new technique that has the ability to represent the mental models of the user and developer communities as network representations using Pathfinder networks. Graphs (mental models) are generated for each of the user and developer groups and compared for similarities/dissimilarities using a graph similarity metric. This paper overviews how this technique is used to categorize requirements and to identify ambiguous and duplicate requirements. We also propose to extend this technique to enhance communication and reduce misunderstanding surrounding the user requirements during the requirement analysis phase.
74|2|http://www.sciencedirect.com/science/journal/01641212/74/2|Editorial board|
74|2||Contents|
74|2||The new context for software engineering education and training|
74|2||Developing and using a web-based project process throughout the software engineering curriculum|In order to facilitate the study and use of software process, which is essential to the education of future software professionals, a standard and tailorable project process has been developed over the last five years at Texas Tech University for use in both undergraduate and graduate curricula, with a total of 12 courses involved. The process is entirely web-based, and includes a complete set of HTML document templates in order to facilitate the creation of project artifacts which are posted to the course web page. This method enhances communication between team members, including distance education students, and between the project team and client. The project process has received positive feedback from all stakeholders involved.
74|2||eXtreme Programmingââhelpful or harmful in educating undergraduates?|Criticism is sometimes leveled at the academic Software Engineering community on the basis that current educational practices are too document-centric. Both students and practitioners have suggested that one of the popular, lighter-weight, agile methods would be a better choice. This paper examines the educational goals for undergraduate Software Engineering education and considers how they might be met by the practices of eXtreme Programming. Our judgment is that education about some agile practices could be beneficial for small-scale development. However, as it stands now, eXtreme Programming as a package does not lend itself for use in educating about large-scale system development in tertiary education.
74|2||Teaching extreme programming to large groups of students|We find the extreme programming methodology highly suitable for introducing undergraduate students to software engineering. To be able to apply this methodology at a reasonable teaching cost for large student groups, we have developed two courses that work in tandem: a team programming course taken by more than 100 students, and a coaching course taken by around 25 students. In this paper we describe our view of how extreme programming fits into the software engineering curriculum, our approach to teaching it, and our experiences, based on two years of running these courses. Particularly important aspects of our set up include team coaching (by older students), fixed working hours, and colocation during development. Our experiences so far are very positive, and we see that students get a good basic understanding of the important concepts in software engineering, rooted in their own practical experience.
74|2||Integrating formalism into undergraduate software engineering|This paper describes an approach and rational for using logic and formal methods in undergraduate software engineering education. Formal methods and logic provide a mathematical basis for modeling software analogous to the role of continuous mathematics in traditional engineering disciplines. Traditional software engineering techniques provide means for modeling software development processes and structuring specifications. Neither formal methods nor traditional approaches subsume the other, but are complimentary in software engineering education and practice. The course described here was a part of the standard Computer Engineering curriculum at The University of Cincinnati from 1993 through 1999. This paper reports on the course and observations over six years of teaching the course to undergraduate and graduate students.
74|2||A maturity model for the implementation of software process improvement: an empirical study|Different advances have been made in the development of software process improvement (SPI) standards and models, e.g. capability maturity model (CMM), more recently CMMI, and ISO's SPICE. However, these advances have not been matched by equal advances in the adoption of these standards and models in software development which has resulted in limited success for many SPI efforts. The current problem with SPI is not a lack of standard or model, but rather a lack of an effective strategy to successfully implement these standards or models. The importance of SPI implementation demands that it be recognised as a complex process in its own right and that organizations should determine their SPI implementation maturity through an organized set of activities. In the literature, much attention has been paid to “what activities to implement” instead of “how to implement” these activities. We believe that identification of only “what” activities to implement is not sufficient and that knowledge of “how” to implement is also required for successful implementation of SPI programmes.
74|2||Product derivation in software product families: a case study|From our experience with several organizations that employ software product families, we have learned that, contrary to popular belief, deriving individual products from shared software assets is a time-consuming and expensive activity. In this paper we therefore present a study that investigated the source of those problems. We provide the reader with a framework of terminology and concepts regarding product derivation. In addition, we present several problems and issues we identified during a case study at two large industrial organizations that are relevant to other, for example, comparable or less mature organizations.
74|2||Modification of standard Function Point complexity weights system|Function Point (FP) is a software size measure, which includes the standard FP and many different models derived from it. The standard FP method created by Albrecht in 1979 is currently known as the International FP User group (IFPUG) version, which consists of three main parts: The first part is five components, and the second is the complexity weights that include three levels of complexity; simple, average, and complex. The third part is the general system characteristics of software projects, which consists of 14 technical complexity factors. Although, FP was widely used as a software size measure, but it still suffers from many weaknesses. One of which is the subjectivity in the weights system. In this paper a new FP weights system was established using Artificial Neural Networks. This method is a modification of the complexity weights of FP measure (IFPUG version). The final results were very accurate and much suitable when they were applied on real data sets of software projects.
74|2||Characterizing a data model for software measurement|In order to develop or acquire a software product with appropriate quality, it is widely accepted that quality must be identified, planned, measured and controlled during the development process using quality measures based on a quality model. However, few practitioners in the software industry would call measurement a success story. This weakness arises, on one hand because the people involved are not always aware of the importance of collecting measures. The policy of the management board must make sure that people are sufficiently motivated and that data is actually collected in the specified way. On the other hand, software measures have been often poorly defined in industry. When software measurement definitions are incomplete and/or poorly documented, it is easy to collect invalid or incomparable measures from different data collectors. Thus, the primary issue is not only whether a definition for a measure is theoretically correct, but that everyone understands what the measured values represent. Then, the values can be collected consistently and other people, different from the collectors, can interpret the results correctly and apply them to reach valid conclusions. The objective of this paper is to present a data MOdel for Software MEasurement (MOSME) to explicitly define software measures, providing the elements required to describe a consistent measurement process. MOSME can be used for defining and modeling data sets of software products involving several software projects. The inspiration of this work comes from the SQUID (Software QUality In the Development process) approach, which combines many results from previous research on software quality and the European Commission funded projects SQUAD and CLARiFi. The application of MOSME is illustrated with a case study. We believe that a conceptual model of fully defined meaningful measures will help both the management board to give support to the data collection policy and the practitioner to avoid ambiguity in the definitions of the data measures.
74|2||Erratum to âAn empirical study of maintenance and development estimation accuracyâ [The Journal of Systems and Software 64 (2002) 57â77]|
74|3|http://www.sciencedirect.com/science/journal/01641212/74/3|Editorial board|
74|3||ECEM: an event correlation based event manager for an I/O-intensive application|In Internet servers that run on general purpose operating systems, network subsystems and disk subsystems cooperate with each other for user requests. Many studies have focused on optimizing the data movement across the subsystems to reduce data copying overhead among kernel buffers, a network send buffer and a disk buffer. When data are moved across the subsystems, events such as read requests and write requests for data movement are also delivered across the subsystems by the servers and the operating system. However, there have been fewer studies on the optimization of event delivery across the subsystems. In conventional operating systems, an event from a disk subsystem is delivered to a network subsystem regardless of the status of the network subsystem. If the network subsystem is not ready for data sending, the execution of the server will be blocked, which causes scheduling and context switching overheads. This non-contiguous execution will incur deficiencies such as avoidable process blocking, context switching, cache pollution and long response time. To alleviate the deficiencies, we have developed inter-subsystem event delivery mechanisms that define event dependencies among the subsystems involved. We define an event correlation based on the happened-before relation. We propose deferred event delivery (DED) and disk-to-network splicing (DNS) to suppress scheduling and context switching during I/O request processing. We performed experiments on Linux 2.4 and the experimental results show that the number of context switching is reduced by up to 20% and server data transmit rate is improved by 4.0–8.1%.
74|3||Measurement-based end to end latency performance prediction for SLA verification|End-to-end delay is one of the important metrics used to define perceived quality of service. Measurement is a fundamental tool of network management to assess the performance of the network. The conventional approach to measure the performance metrics is on an intrusive basis that may cause extra-burden to the network. In contrast to this, our scheme can be considered as non-intrusive. The main idea relies on the knowledge of the queuing behaviour. The queue length is non-intrusively monitored, and then we capture the parameters of the queue state distribution of every queue along the path in order to deduce the end-to-end delay performance.
74|3||Simulating autonomous agents in augmented reality|In many critical applications such as airport operations (for capacity planning), military simulations (for tactical training and planning), and medical simulations (for the planning of medical treatment and surgical operations), it is very useful to conduct simulations within physically accurate and visually realistic settings that are represented by real video imaging sequences. Furthermore, it is important that the simulated entities conduct autonomous actions which are realistic and which follow plans of action or intelligent behavior in reaction to current situations. We describe the research we have conducted to incorporate synthetic objects in a visually realistic manner in video sequences representing a real scene. We also discuss how the synthetic objects can be designed to conduct intelligent behavior within an augmented reality setting. The paper discusses both the computer vision aspects that we have addressed and solved, and the issues related to the insertion of intelligent autonomous objects within an augmented reality simulation.
74|3||An empirical study of system design instability metric and design evolution in an agile software process|Software project tracking and project plan adjustment are two important software engineering activities. The class growth shows the design evolution of the software. The System Design Instability (SDI) metric indicates the progress of an object-oriented (OO) project once the project is set in motion. The SDI metric provides information on project evolution to project managers for possible adjustment to the project plan. The objectives of this paper are to test if the System Design Instability metric can be used to estimate and re-plan software projects in an XP-like agile process and study system design evolution in the Agile software process. We present an empirical study of the class growth and the SDI metric in two OO systems, developed using an agile process similar to Extreme Programming (XP). We analyzed the system evolutionary data collected on a daily basis from the two systems. We concluded that: the systems' class growth follows observable trends, the SDI metric can indicate project progress with certain trends, and the SDI metric is correlated with XP activities. In both of the analyzed systems, we observed two consistent jumps in the SDI metric values in early and late development phases. Part of the results agrees with a previous empirical study in another environment.
74|3||Software failure prediction based on a Markov Bayesian network model|Due to the complexity of software products and development processes, software reliability models need to possess the ability of dealing with multiple parameters. Also in order to adapt to the continually refreshed data, they should provide flexibility in model construction in terms of information updating. Existing software reliability models are not flexible in this context. The main reason for this is that there are many static assumptions associated with the models. Bayesian network is a powerful tool for solving this problem, as it exhibits strong ability to adapt in problems involving complex variant factors. In this paper, a software prediction model based on Markov Bayesian networks is developed, and a method to solve the network model is proposed. The use of our model is illustrated with an example.
74|3||Static and dynamic distance metrics for feature-based code analysis|In this paper we present metrics to determine the distance between the features of a software system. Such a measurement can elucidate how features of the system being examined are close to each other. We first use an execution slice-based technique to identify a set of code (basic blocks in our case) that is used to implement each feature. Then, depending on whether the execution frequency of each block is considered during the construction of such sets of code, a static as well as a dynamic distance is computed for each pair of features. These two types of distance differ in that the former computes the distance between two features only by how these features are implemented in the system, while the latter also takes into account how each feature is executed based on a user's operational profile. In other words, the static distance quantitatively gives the closeness of two features from the system implementation point of view, whereas the dynamic distance presents such closeness from the users' execution point of view. To illustrate the use of our metrics, we report a case study on a Symbolic Hierarchical Automated Reliability and Performance Evaluator (SHARPE). The results of our study suggest that the distance metrics discussed in this paper can provide a good measurement, in a quantitative way, of how close two program features are. Such information can also serve as a good start to understanding how a modification made to one feature is likely to affect other features.
74|3||On the security of some proxy blind signature schemes|A proxy blind signature scheme is a digital signature scheme which combines the properties of proxy signature and blind signature schemes. Recently, Tan et al. proposed two proxy blind signature schemes based on DLP and ECDLP respectively. Later, compared with Tan et al.'s scheme, Lal and Awasthi further proposed a more efficient proxy blind signature scheme. In this paper, we show that both Tan et al.'s schemes do not satisfy the unforgeability and unlinkability properties. Moreover, we also point out that Lal and Awasthi's scheme does not possess the unlinkability property either.
74|3||Improving the performance of client Web object retrieval|The growth of the Internet has generated Web pages that are rich in media and that incur significant rendering latency when accessed through slow communication channels. The technique of Web-object prefetching can potentially expedite the presentation of Web pages by utilizing the current Web page's view time to acquire the Web objects of likely future Web pages. The performance of the Web object prefetcher is contingent on the predictability of future Web pages and quickly determining which Web objects to prefetch during the limited view time interval of the current Web page. The proposed Markov–Knapsack method uses an approach that combines a Multi-Markov Web-application centric prefetch model with a Knapsack Web object selector to enhance Web page rendering performance. The Markov Web page model ascertains the most likely next Web page set based on the current Web page and the Web object Knapsack selector determines the premium Web objects to request from these Web pages. The results presented in the paper show that the proposed methods can be effective in improving a Web browser cache-hit percentage while significantly lowering Web page rendering latency.
74|3||Adaptive index management for future location-based queries|Many location-based applications have arisen in various areas including mobile communications, traffic control and military command and control (C2) systems. And one of the important research issue in these areas is tracking and managing moving objects through spatiotemporal indexing for the efficient location-based services. However, managing exact geometric location information is difficult to be achieved due to continual changes of moving objects.
74|3||An investigation of software engineering curricula|We adapted a survey instrument developed by Timothy Lethbridge to assess the extent to which the education delivered by four UK universities matches the requirements of the software industry. We propose a survey methodology that we believe addresses the research question more appropriately than the one used by Lethbridge. In particular, we suggest that restricting the scope of the survey to address the question of whether the curricula for a specific university addressed the needs of its own students, allowed us to identify an appropriate target population. However, our own survey suffered from several problems. In particular the questions used in the survey are not ideal, and the response rate was poor.
74|3||Contents Volume 74|
75|1-2|http://www.sciencedirect.com/science/journal/01641212/75/1-2|Contents|
75|1-2||Editorial|
75|1-2||An experimental card game for teaching software engineering processes|The typical software engineering course consists of lectures in which concepts and theories are conveyed, along with a small “toy” software engineering project which attempts to give students the opportunity to put this knowledge into practice. Although both of these components are essential, neither one provides students with adequate practical knowledge regarding the process of software engineering. Namely, lectures allow only passive learning and projects are so constrained by the time and scope requirements of the academic environment that they cannot be large enough to exhibit many of the phenomena occurring in real-world software engineering processes. To address this problem, we have developed Problems and Programmers, an educational card game that simulates the software engineering process and is designed to teach those process issues that are not sufficiently highlighted by lectures and projects. We describe how the game is designed, the mechanics of its game play, and the results of an experiment we conducted involving students playing the game.
75|1-2||Engineering-based processes and agile methodologies for software development: a comparative case study|The emergence of various software development methodologies raises the need to evaluate and compare their efficiencies. One way of performing such a comparison is to have different teams apply different process models in the implementation of multiple versions of common specifications. This study defines a new cognitive activity classification scheme which has been used to record the effort expended by six student teams producing parallel implementations of the same software requirements specifications. Three of the teams used a process based on the Unified Process for Education (UPEDU), a teaching-oriented process derived from the Rational Unified Process. The other three teams used a process built around the principles of the Extreme Programming (XP) methodology. Important variations in effort at the cognitive activity level between teams shows that the classification could scarcely be used without categorization at a higher level. However, the relative importance of a category of activities aimed at defining “active” behaviour was shown to be almost constant for all teams involved, possibly showing a fundamental behaviour pattern. As secondary observations, aggregate variations by process model tend to be small and limited to a few activities, and coding-related activities dominate the effort distribution for all the teams.
75|1-2||An industry/university collaboration to upgrade software engineering knowledge and skills in industry|This paper describes an ongoing collaboration between Boeing Australia Limited and the University of Queensland to develop and deliver an introductory course on software engineering. The aims of the course are to provide a common understanding of the nature of software engineering for all Boeing Australia's engineering staff, and to ensure they understand the practices used throughout the company. The course is designed so that it can be presented to people with varying backgrounds, such as recent software engineering graduates, systems engineers, quality assurance personnel, etc. The paper describes the structure and content of the course, and the evaluation techniques used to collect feedback from the participants and the corresponding results. The immediate feedback on the course indicates that it has been well received by the participants, but also indicates a need for more advanced courses in specific areas. The long-term feedback from participants is less positive, and the long-term feedback from the managers of the course participants indicates a need to expand on the coverage of the Boeing-specific processes and methods.
75|1-2||The role of modelling in the software engineering curriculum|This paper argues that the concept of modelling, and particularly the modelling of software system structures, is not being given sufficient attention within current sources that describe aspects of the software engineering curriculum. The paper describes the scope of modelling as a general concept, and explains the role that the modelling of software system structures plays within it. It discusses the treatment of this role within the various sources, and compares this both with the experience of the role that such modelling plays in the undergraduate curriculum at Sheffield University, and with the practice in other branches of engineering. The idea is examined that modelling should be treated as a recurring concept within the curriculum, and it is shown that this gives rise to a matrix structure for the software engineering curriculum. The paper discusses how such a structure can be mapped into a conventional hierarchical curriculum model, and the relationships that need to be made explicit in doing so. It describes the practical implications of these results for the structures of degree programmes in software engineering.
75|1-2||The SEI curriculum modules and their influence: Norm Gibbs' legacy to software engineering education|The Software Engineering Institute (SEI) at Carnegie Mellon University started its first contract with a carte blanche opportunity and generous funding to improve the state of software engineering education. Norm Gibbs, the first Director of Education at the SEI guided efforts in this area. One of his innovations, discussed here, were the “curriculum modules” encapsulating software engineering knowledge. We describe the scope and form of the curriculum modules, together with our personal experiences of developing the prototype modules. We conclude with an informal assessment of how well the original set of SEI curriculum modules match current ideas, both about software engineering education and also about the activities and practices that make up software engineering as a discipline.
75|1-2||Secure key agreement protocols for three-party against guessing attacks|Key exchange protocol is important for sending secret messages using the session key between two parties. In order to reach the objective, the premise is to generate a session key securely. Encryption key exchange was first proposed to generate a session key with a weak authenticated password against guessing attacks. Next, another authenticated key exchange protocols for three-party, two clients who request the session key and one server who authenticates the user's identity and assist in generating a session key, were proposed. In this paper, we focus on the three-party authenticated key exchange protocol. In addition to analyzing and improving a password-based authenticated key exchange protocol, a new verified-based protocol is also proposed.
75|1-2||A practical pattern recovery approach based on both structural and behavioral analysis|While the merit of using design patterns is clear for forward engineering, we could also benefit from design pattern recovery in program understanding and reverse engineering. In this paper, we present a practical approach to enlarge the recoverable scope and improve precision ratio of pattern recovery. To specify both structural aspect and behavioral aspect of design patterns, we introduce traditional predicate logic combined with Allen's interval-based temporal logic as our theory foundation. The formal specifications could be conveniently converted into Prolog representations to support pattern recovery. To illustrate how to specify and recover design patterns in our approach, we take one example for each category of design patterns. Moreover, we give a taxonomy of design patterns based on the analysis in our approach to show its applicable scope. To validate our approach, we have developed a tool named PRAssistor and analyzed two well-known open source frameworks. The experiment results show that most of the patterns addressed in our taxonomy have been recovered. Besides larger recoverable scope, the recovery precision of our approach is much higher than others. Furthermore, we consider that our approach and tool could be promisingly extended to support “Debug at Design Level” and “Pattern-Driven Refactoring”.
75|1-2||Cryptanalysis of XiaâYou group signature scheme|Group signature is a variant of digital signatures, which allows members of a group to sign messages anonymously on behalf of the group. The application of group signatures includes e-voting, e-bidding and e-cash. Recently, Xia and You proposed an identity-based group signature scheme with strong separability. In this paper, however, we find that there are several problems in the Xia–You group signature scheme. This scheme is vulnerable to forgery attacks. Any adversary could easily forge a valid group signature for any message without the knowledge of the secret values of the legal members of the group. Furthermore, the Xia–You group signature scheme does not satisfy nearly all other security requirements of group signatures. The signatures are too long to be computed, stored and transmitted.
75|1-2||Performance study on implementation of VCR functionality in staggered broadcast video-on-demand systems|Data sharing is a feasible solution to support a large-scale video-on-demand (VoD) system, however, it is still an open issue to provide a full set of continuous interactive functions in such an environment that a single channel is used to serve a group of customers. In this paper, we first investigate the performance of client buffer management and its hybrid use with contingency channels for providing VCR functionality in a staggered broadcast VoD system. Results show that directly combining the two methods may not gain any advantages since the resources are not fully utilized. To tackle this problem, a greedy algorithm is proposed to efficiently manage the contingency channels such that the system performance can be significantly improved by exploiting the property of the staggered broadcast scheme and the use of multiple loaders in the receiver. Simulation results show that the proposed greedy system can efficiently provide a full set of VCR functions with satisfied user performance in a broadcast VoD system.
75|1-2||Preventing information leakage within workflows that execute among competing organizations|This paper proposes a model for access control within workflows. It is based on access control lists (ACLs) and is named WfACL (ACL-based access control model for workflows). WfACL prevents information leakage within workflows that may execute among competing organizations. Its objective is threefold. First, it prevents an organization that executes a workflow from leaking its information to other organizations. Second, it prevents information leakage among competing organizations. Third, it prevents information leakage within an organization. In addition to achieving the objective, WfACL offers the following features: (a) managing dynamic role association change, (b) managing dynamic role change, (c) avoiding indirect information leakage, (d) detailing the control granularity to roles, and (e) controlling both read and write access. We embedded WfACL in a rule-based workflow language WfACLL and implemented a prototype environment WfACLE. We evaluated WfACL using WfACLL and WfACLE. The evaluation result is also shown in this paper.
75|1-2||Secure agent computation: X.509 Proxy Certificates in a multi-lingual agent framework|Mobile agent technology presents an attractive alternative to the client–server paradigm for several network and real-time applications. However, for most applications, the lack of a viable agent security model has limited the adoption of the agent paradigm. This paper describes how the security infrastructure for computational Grids using X.509 Proxy Certificates can be extended to facilitate security for mobile agents. Proxy Certificates serve as credentials for Grid applications, and their primary purpose is the temporary delegation of authority. We are exploiting the similarities between Grid applications and mobile agent applications, and motivate the use of Proxy Certificates as credentials for mobile agents. Further, we propose extensions for Proxy Certificates to facilitate the characteristics of mobile agent applications, and present mechanisms that achieve agent-to-host authentication, restriction of agent privileges, and secure delegation of authority during spawning of new agents.
75|1-2||Wireless protocol testing and validation supported by formal methods. A hands-on report|We apply formal testing and validation techniques and tools to analyse a configuration protocol for a Bluetooth Location Network (BLN). This network is composed by static Bluetooth nodes that establish a spontaneous network at system initialization. Once configured, BLN provides location services for location-aware or context-driven wireless environments, such as m-commerce networks or e-museums. BLN configuration was initially defined in natural language, and had passed some initial tests and simulation-based analysis. Formal methods have provided deeper understanding and discovered unexpected errors that may arise in some failure scenarios.
75|1-2||A two-level scheduling method: an effective parallelizing technique for uniform nested loops on a DSP multiprocessor|A digital signal processor (DSP), which is a special-purpose microprocessor, is designed to achieve higher performance on DSP applications. Because most DSP applications contain many nested loops and permit a very high degree of parallelism, the DSP multiprocessor has a suitable architecture to execute these applications. Unfortunately, conventional scheduling methods used on DSP multiprocessors allocate only one operation to each DSP every time unit, even if the DSP includes several function units that can operate in parallel. Obviously they cannot achieve full function unit utilization. Hence, in this paper, we propose a two-level scheduling method (TSM) to overcome this common failing. TSM contains two approaches, which integrates unimodular transformations, loop tiling technique, and conventional methods used on single DSP. Besides introducing algorithm, we also use an analytic module to analyze its preliminary performance. Based on our analyses the TSM can achieve shorter execution time and more scalable speedup results. In addition, the TSM causes less memory access and synchronization overheads, which are usually negligible in the DSP multiprocessor architecture.
75|1-2||Migratable sockets in cluster computing|Optimal utilization of cluster computing is partly dependent upon pre-emptive process migration. However, this migration involves a host of issues, one of them being the transfer of system-dependent resources. We focus on the overhead incurred by migrated processes using sockets. We then describe a solution that we devised and implemented to avoid this overhead through the use of `migratable sockets'. Our studies show that the use of `migratable sockets' considerably improves the execution time of a process using sockets as compared to the execution time of a process that uses standard sockets and thus bears the communication overhead.
75|1-2||An agent-based inter-application information flow control model|Access control in an application prevents information leakage in the application. The prevention can be achieved by controlling information flows. Many information flow control models have been developed. Since applications may cooperate, controlling information flows among cooperating applications is necessary. Our survey reveals that no existing model offers the control. We thus developed a model to control information flows among cooperating object-oriented applications. In designing the model, we require that cooperating applications communicate with one another through JAVA RMI (remote method invocation). Our model is based on the following considerations: when a RMI occurs, the security level of an argument should not be higher than that of the parameter receiving the argument's value, and the security level of a variable receiving a method return value should not be lower than that of the method return value. The model is agent-based. Moreover, different applications can embed different information flow control models. With this, the proposed model coordinates heterogeneous information flow control models. This paper presents our inter-application information flow control model.
75|1-2||Cryptanalysis of HwangâYang scheme for controlling access in large partially ordered hierarchies|Recently, Hwang and Yang [J. Syst. Software 67 (2003) 99] proposed a cryptographic key assignment scheme for access control in large partially ordered hierarchies. In this paper, we show that their scheme is insecure against the collusion attack whereby some security classes conspire to derive the secret keys of other leaf security classes, which is not allowed according to the scheme.
75|1-2||Enhancing Service Location Protocol for efficiency, scalability and advanced discovery|This paper presents three new mechanisms for the Service Location Protocol (SLP): mesh enhancement, preference filters and global attributes. The mesh enhancement simplifies Service Agent (SA) registrations and improves consistency among Directory Agents (DAs) by defining an interaction scheme for DAs and supporting automatic registration distribution among peer DAs. Preference filters facilitate processing of search results (e.g., finding the best match) in SLP servers (DAs and SAs) to reduce the amount of data transferred to the client for saving network bandwidth. Global attributes allow using a single query to search services across multiple types. These mechanisms can improve SLP efficiency and scalability and support advanced discovery such as discovering multi-access-point services and multi-function devices. We expect that these techniques can also be applied to other service discovery systems.
75|1-2||Revocation of privacy-enhanced public-key certificates|This paper presents a novel protocol for the revocation of privacy-enhanced/anonymous public-key certificates in relation to a protocol for anonymous public-key certificate issuing published previously. Not only can this certificate revocation protocol revoke an anonymous public-key certificate upon a request from its holder, but also automatically revoke any certificate issued directly or indirectly based on the certificate revoked, in an anonymous and accountable manner. In case the private key associated with an anonymous public-key certificate is suspected of having been compromised, the certificate holder can operate the protocol to easily revoke the compromised certificate together with its related ones so as to stop them being abused. The protocol is also assessed with regard to requirements such as accountability and anonymity.
75|1-2||J2EE support for wireless services|We explore by design and implementation the applicability of J2EE (Java 2 Platform, Enterprise Edition) architecture in supporting wireless services as well as web-based services. This has been demonstrated by implementing a wireless prescription system, which provides wireless services to physicians, and web-based services to pharmacies and administrators. The goal of the wireless prescription system is to improve the efficiency of the healthcare system by reducing the overall time and cost used to create documents and retrieve information. We also present a performance evaluation of the implemented wireless prescription system using latency as the performance metric. In particular, we investigated the overheads associated with different components of the underlying J2EE architecture, which includes the Session Bean, Entity Bean, and the database. We found that the latency overheads introduced by these components account for 3.8–4.3%, 3.9–4.4%, and 91.8% of the total delay experienced inside the J2EE architecture respectively.
75|1-2||A generic anti-spyware solution by access control list at kernel level|Spyware refers to programs that steal the user information stored in the user’s computer and transmit this information via the Internet to a designated home server without the user being aware of this transmission. Existing anti-spyware solutions are not generic and flexible. These solutions either check for the existence of known spyware or try to block the transmission of the private information at the packet level. In this paper, we propose a more generic and flexible anti-spyware solution by utilizing an access control list in kernel mode of the operating system. The major difference between our approach and the existing approaches is that instead of asking a guard to look for the theft (spyware) or control the exit of the computer (and hence giving the spyware enough time to hide the information to be transmitted), we put a guard besides the treasure (the private information) and carefully control the access to it in the kernel mode. We also show the details of an implementation that realizes our proposed solution.
75|3|http://www.sciencedirect.com/science/journal/01641212/75/3|Editorial board|
75|3||Adaptive multimedia computing|
75|3||Streaming video delivery over Internet with adaptive end-to-end QoS|Many multimedia applications rely on video streaming techniques. However, video delivery over a Best Effort network such as today's Internet is a challenge. Traffic load in the network changes dynamically and in an unpredictable way causing the resource availability to vary. Providing an application level end-to-end quality of service in such an environment requires network-awareness and ability to adapt. We address the issue of mapping between application-level quality of service for streaming video and network-level quality of service. We show that continuous playback requires a limit on the delay jitter. We tackle the problem of providing end-to-end video quality given that the network does not guarantee limited delay variability. Our approach is unique in a way we do not model network as a black box but investigate what information about the network status is necessary for an application to make adaptation decisions. We rely on simple multi-level ECN-based mechanism to obtain a feedback from the network as well as on end-point observations to determine available bandwidth. Such an approach allows to obtain better user-perceived video quality by providing additional information to properly interpret the arrival rate observed at the end-point. We propose a 3-rate adaptation mechanism for video streaming to illustrate the philosophy of adaptivity based on network awareness, where the network awareness is not limited to observing network reaction to a set of stimuli.
75|3||Adaptive video transcoding and streaming over wireless channels|In this work, we investigate the problem of bit rate adaptation transcoding for transmitting pre-encoded VBR video over burst-error wireless channels, i.e., channels such that errors tend to occur in clusters during fading periods. In particular, we consider a scenario consisting of packet-based transmission with Automatic Repeat ReQuest (ARQ) error control and a feedback channel. With the acknowledgements received through the feedback channel and a statistical channel model, we have an estimate of the current channel state, and effective channel bandwidth. In this paper, we analyze the constraints of buffer and end-to-end delay, and derive the conditions that the transcoder buffers have to meet for preventing the end decoder buffer from underflowing and overflowing. Furthermore, we also investigate the source characteristics and scene changes of the pre-encoded video stream. Based on the channel constraints and source video characteristics, we propose an adaptive bit rate adaptation algorithm for transcoding and transmitting pre-encoded VBR video stream over wireless channel. Our experimental results demonstrate that, by reusing the source characteristics and scene change information, transcoding high quality video can produce better video picture quality than that produced by directly encoding the uncompressed video at the same low bit rate. Moreover, by controlling the frame bit budget according to the channel conditions and buffer occupancy, the initial startup delay of streaming pre-encoded video can be significantly reduced.
75|3||An adaptive rate-control streaming mechanism with optimal buffer utilization|In this paper, an end-to-end real-time adaptive protocol for multimedia transmission is presented. The bandwidth is dynamically allocated according to the network status, and the client buffer occupancy and playback requirement. The transmission rate is determined by the quadratic probing algorithm that can obtain the maximal utilization of the client buffer and minimal occupation of the network bandwidth. It is also coupled with a congestion control mechanism that can effectively decrease the packet loss rate during network congestion. We investigate the performance of our quadratic probing algorithm in different congestion levels under both the local area net (LAN) and Internet environments. Performance analysis reveals that our approach is more robust in avoiding overflows and underflows in different network congestion levels, and adapting to the changing network delays. Comparisons are made with the fixed rate approach and the rate by playback requirement approach. The experimental results show that our proposed real-time protocol with the rate adjusting quadratic probing algorithm is efficient in utilizing the network resources and decreasing the packet loss ratios.
75|3||ATF: an Adaptive Three-layer Framework for inter-stream synchronization of SMIL multimedia presentations|In this paper, we propose an Adaptive Three-layer Framework (ATF) to guarantee the inter-stream synchronization, which is called as the APplication Quality of Service (APQoS) in this paper, of World Wide Web Consortium (W3C) Synchronized Multimedia Integration Language (SMIL) presentation. This ATF framework, which is composed of the Framework Control Layer (FCL), the Quality Guarantee Layer (QGL) and the Media Control Layer (MCL), functions as the middleware between underlying best-effort network protocol stacks and the SMIL client/server applications. The FCL at the SMIL client periodically monitors underlying network bandwidth and delay for the QGL to verify whether the total bandwidth requirements of all concurrent objects to meet the APQoS of the SMIL presentation can be satisfied with the available bandwidth, which is the sum of the monitored bandwidths of all concurrent objects. The QGL executes the novel Intra-SMIL Bandwidth Reallocation (ISBR) scheme to dynamically adjust bandwidth allocations among concurrent SMIL objects, according to their Media Importance (MI) values calculated by the MCL. If the available bandwidth is not enough, the ISBR scheme gracefully degrades the layer-encoded objects that have the smallest MI values and only allocate the necessary bandwidth for their base layer (BL) streams instead. Oppositely, it aggressively upgrades the degraded layer-encoded objects that have the largest MI values and allocate the necessary bandwidth for both the BL and enhancement layer (EL) streams. These bandwidths are then feedback to the remote media servers to modify the transmission rates accordingly. With the quantitative analysis for the SMIL presentation quality, this ATF framework achieves the APQoS with the best SMIL playback quality.
75|3||An agent based adaptive bandwidth allocation scheme for multimedia applications|Bandwidth allocation for multimedia applications in case of network congestion and failure poses technical challenges due to bursty and delay sensitive nature of the applications. The growth of multimedia services on Internet and the development of agent technology have made us to investigate new techniques for resolving the bandwidth issues in multimedia communications. Agent technology is emerging as a flexible promising solution for network resource management and QoS (Quality of Service) control in a distributed environment. In this paper, we propose an adaptive bandwidth allocation scheme for multimedia applications by deploying the static and mobile agents. It is a run-time allocation scheme that functions at the network nodes. This technique adaptively finds an alternate patchup route for every congested/failed link and reallocates the bandwidth for the affected multimedia applications. The designed method has been tested (analytical and simulation) with various network sizes and conditions. The results are presented to assess the performance and effectiveness of the approach. This work also demonstrates some of the benefits of the agent based schemes in providing flexibility, adaptability, software reusability, and maintainability.
75|3||Scheduling legacy multimedia applications|Millions of applications have been developed on conventional time-sharing systems. We call those applications legacy applications. Many of them, typically multimedia applications, have Quality of Service (QoS) demands, which are not supported in time-sharing systems. Although many scheduling algorithms and schedulers have been proposed to schedule multimedia applications, it is not feasible to rebuild millions of legacy multimedia applications in a completely new programming model. Moreover, the execution pattern of multimedia applications is difficult to predict.
75|3||Contents Volume 75|
76|1|http://www.sciencedirect.com/science/journal/01641212/76/1|Editorial board|
76|1||Contents|
76|1||Editorial|
76|1||Reliability assessment and sensitivity analysis of software reliability growth modeling based on software module structure|
76|1||Smart debugging software architectural design in SDL|Statistical data show that it is much cheaper to fix software bugs at the early design stage than the late stage of the development process where the final system has already been implemented and integrated together. The use of slicing and execution histories as an aid in software debugging is well established for programming languages like C and C++; however, it is rarely applied in the field of software specification for designs. We propose a solution by applying the technology at source code level to debugging software designs represented in a high-level specification and description language such as SDL. More specifically, we extend execution slice-based heuristics from source code-based debugging to the software design specification level. Suspicious locations in an SDL specification are prioritized based on their likelihood of containing faults. Locations with a higher priority should be examined first rather than those with a lower priority as the former are more suspicious than the latter, i.e., more likely to contain the faults. A debugging tool, SmartDSDL, with user-friendly interfaces was developed to support our method. An experiment is reported to demonstrate the feasibility of using our method to effectively debug an architectural design.
76|1||A middleware service for secure group communication in mobile ad hoc networks|Secure group communication (SGC) is required in many applications using mobile ad hoc networks (MANETs). Due to the mobility and limited resources of mobile devices used by group members, secure groups in a MANET may be required to be set up and maintained dynamically, and applications requiring SGC may request to change the secure groups according to network and application status. In this paper, a secure group model is presented to describe common characteristics of secure groups based on hierarchical authentication relations among the members in each secure group. Based on this model, a middleware service for SGC is developed to set up and maintain secure groups automatically and support development and runtime of applications using SGC in MANETs. This middleware service is implemented in a context-sensitive middleware RCSM.
76|1||Facilitating secure ad hoc service discovery in public environments|
76|1||Checking of models built using a graphically based formal modelling language|
76|1||Effects of introducing survival behaviours into automated negotiators specified in an environmental and behavioural framework|
76|1||Evolving car designs using model-based automated safety analysis and optimisation techniques|Development processes in the automotive industry need to evolve to address increasing demands for integration of car functions over common networked infrastructures. New processes must address cost and safety concerns and maximize the potential for automation to address the problem of increasing technological complexity. In this paper, we propose a design process in which techniques for semi-automatic safety and reliability analysis of systems models are combined with multi-objective optimisation techniques to assist the gradual development of designs that can meet reliability and safety requirements and maximise profit within pragmatic development cost constraints. The proposed process relies on tools to automate some aspects of the design that we believe could be automated and thus simplified without loss of the creative input brought in the process by designers.
76|1||An assessment of systems and software engineering scholars and institutions (1999â2003)|This paper presents the findings of a five-year study of the top scholars and institutions in the Systems and Software Engineering field, as measured by the quantity of papers published in the journals of the field. The top scholar is Khaled El Emam of the Canadian National Research Council, and the top institution is Carnegie Mellon University and its Software Engineering Institute.
76|2|http://www.sciencedirect.com/science/journal/01641212/76/2|Impartial evaluation in software reliability practice|In the past decade, the size scale of software systems and their technical complexity has become much more complicated. Accordingly, quality assessment of software applications has been intensively investigated lately. Among popular software quality metrics, software reliability has been proven to be one of the most useful indices in evaluating software applications. In the literature, statistical usage testing has been widely shown to be effective in estimating software reliability. Essentially, it first transfers the practical operations of a software system into a usage model, which then forms a basis for performing statistical testing and analyzing software reliability. This research investigates the extension of statistical usage testing by considering any prior information or prejudgment on software quality before performing a validation test, and propose the derivation of impartial reliability evaluation that is fair for both the software producer and consumer (end user). A numerical demonstration of validating the correctness of hyper links on a web site via the proposed computation is illustrated and discussed. The suggested mechanism with some prior information will converge much more quickly than other similar reliability models. In addition, the proposed framework also provides the flexibility of taking the practical prejudgment into account.
76|2||Resource-oriented software quality classification models|Developing high-quality software within the allotted time and budget is a key element for a productive and successful software project. Software quality classification models that provide a risk-based quality estimation, such as fault-prone (fp) and not fault-prone (nfp), have proven their usefulness as software quality assurance techniques. However, their usefulness is largely dependent on the availability of resources for deploying quality improvements to modules predicted as fp. Since every project has its own special needs and specifications, we feel a classification modeling approach based on resource availability is greatly warranted.
76|2||Metadata-driven design of integrated environments for software performance validation|Lifecycle validation of the performance of software products (i.e., the prediction of the product ability to satisfy the user performance requirements) encompasses the production of performance models from CASE documents.
76|2||Application of neural networks for software quality prediction using object-oriented metrics|This paper presents the application of neural networks in software quality estimation using object-oriented metrics. In this paper, two kinds of investigation are performed. The first on predicting the number of defects in a class and the second on predicting the number of lines changed per class. Two neural network models are used, they are Ward neural network and General Regression neural network (GRNN). Object-oriented design metrics concerning inheritance related measures, complexity measures, cohesion measures, coupling measures and memory allocation measures are used as the independent variables. GRNN network model is found to predict more accurately than Ward network model.
76|2||A comparative study of SOAP and DCOM|Simple Object Access Protocol (SOAP) is a recent technology, which aims at replacing traditional methods of remote communications, such as RPC-based Distributed Component Object Model (DCOM). Being designed with a goal of increasing interoperability among wide range of programs and environment, SOAP allows applications written in different languages and deployed on different platforms to communicate with each other over the network. While SOAP offers obvious benefits in the world of interoperability, it comes at a price of performance degradation and additional development efforts required for implementation of features missing from SOAP, such as security and state management. This paper reports the outcome of a comparative study of SOAP and DCOM in terms of features, development effort, and application performance. The results indicate that SOAP performance on a homogeneous platform is consistently worse than that of DCOM. Depending on the operation and the amount of data transferred, SOAP performance degradation can range from minor (e.g. two or three times slower than DCOM) to major (e.g. twenty or more times slower).
76|2||Design and implementation of an extended relationship semantics in an ODMG-compliant OODBMS|Relationships, in addition to entities, are important in real-world database modeling. In particular, many object oriented database applications including CAD/CAM, CASE and multi-media need to model various and complex relationships, especially the `part–whole' relationship. Without the built-in relationship supports from DBMSs, there is a huge overhead in managing relationships from application development to maintenance, since the relationships should be hard-coded within the application program itself.
76|2||Performance analysis of software reliability growth models with testing-effort and change-point|In this paper, a scheme for constructing software reliability growth model based on Non-Homogeneous Poisson Process is proposed. The main focus is to provide a method for software reliability modeling, which considers both testing-effort and change-point. In the vast literature, most researchers assume a constant detection rate per fault in deriving their software reliability models. They suppose that all faults have equal probability of being detected during the software testing process, and the rate remains constant over the intervals between fault occurrences. In reality, the fault detection rate strongly depends on the skill of test teams, program size, and software testability. Therefore, it may not be smooth and can be changed. On the other hand, sometimes we have to detect more additional faults in order to reach the desired reliability objective during testing. It is advisable for project managers to purchase new automated test tool, technology or additional manpower. These approaches can provide a conspicuous improvement in software testing and productivity. In this case, the fault detection rate will be changed during the software development process. Therefore, here we incorporate both generalized logistic testing-effort function and change-point parameter into software reliability modeling. New theorems are proposed and software testing data collected from real application are utilized to illustrate the proposed model. Experimental results show that the proposed framework to incorporate both testing-effort and change-point for SRGM has a fairly accurate prediction capability.
76|2||The task-dependent nature of the maintenance of object-oriented programs|In this paper we present the results of three experiments we have conducted to test the maintainability of object-oriented software products. Previous research in the field has yielded contradictory results as to whether or not object orientation promotes maintainability. Our experiments were conducted to test our hypothesis that the type of maintenance task influences whether or not the presence of an inheritance hierarchy aids maintainability. We constructed two equivalent C++ programs. One version incorporated an inheritance hierarchy; the other was flat. We used a between-subjects design in each experiment. In each experiment, the maintenance task was the same for all the subjects. Essentially the same two versions of the program were used in all three experiments but with different subjects and a different maintenance task. The results of the first experiment support the hypothesis that at least one type of maintenance task exists where an inheritance-based implementation is more maintainable than a flat version, and conversely for the second experiment. The results of the third experiment support the hypothesis that at least one type of maintenance task exists where there is no significant difference between an inheritance-based implementation and a flat version. We conclude that, when maintaining object-oriented software products, the nature of the maintenance task itself determines whether or not the presence of an inheritance hierarchy promotes maintainability.
76|3|http://www.sciencedirect.com/science/journal/01641212/76/3|DPE/PAC: decentralized process engine with product access control|This paper proposes a process engine called DPE/PAC (decentralized process engine with product access control). It can be embedded in a PSEE (process-centered software engineering environment) to decentralize the PSEE. In a decentralized PSEE, every site can enact process programs and therefore the workload of the PSEE’s sites is balanced (i.e., no site will become a bottleneck). Moreover, when a site is down, other sites can enact process programs. Therefore, a decentralized PSEE overcomes the following drawbacks of a traditional client/server PSEEs: (a) the server may become a bottleneck and (b) when the server is down, the entire process should be suspended. In addition to decentralizing PSEEs, DPE/PAC offers an additional function, which is ensuring secure product access (including the enforcement of separation-of-duty constraints). The function is essential for a software process that develops sensitive software systems.
76|3||Workflow analysis for web publishing using a stage-activity process model|Web publishing has been the core business of ICP (Internet Content Provider) companies. It is also a major functional component in other Internet industry sectors. However, although there have been on-going developments in this area, up to our knowledge, very little work has been done on web publishing process models and methodologies. In this paper we describe a general process model for web publishing based on workflow analysis. Using a case study approach, the existing web publishing methods used by major Hong Kong ICP companies have been evaluated. Based on the evaluation results and our observations, the issues and problems of the existing methods are identified. A general process model, called the stage-activity model, is proposed which facilitates the development of a framework for automating the web publishing process with the incorporation of workflow management. The results of this research contribute to a clear understanding of what challenges ICPs are currently facing and a solution for building an automating web publishing system. The framework can also be used to evaluate and select the web publishing tools in the market.
76|3||Multi-disk scheduling for time-constrained requests in RAID-0 devices|In this paper, we study the scheduling problem of real-time disk requests in multi-disk systems, such as RAID-0. We first propose a multi-disk scheduling algorithm, called Least-Remaining-Request-Size-First (LRSF), to improve soft real-time performance of I/O systems. LRSF may be integrated with different real-time/non-real-time single-disk scheduling algorithms, such as SATF and SSEDV, adopted by the disks in a multi-disk system. We then extend LRSF by considering the serving requests on-the-way (OTW) to the target request to minimize the starvation problem for requests that need to retrieve a large amount of data. The pre-fetching issue in RAID-0 is also studied to further improve the I/O performance. The performance of the proposed algorithm and schemes is investigated and compared with other disk scheduling algorithms through a series of experiments using both randomly generated workload and realistic workload.
76|3||Using an expert panel to validate a requirements process improvement model|In this paper we present components of a newly developed software process improvement model that aims to represent key practices in requirements engineering (RE). Our model is developed in response to practitioner needs highlighted in our empirical work with UK software development companies. We have now reached the stage in model development where we need some independent feedback as to how well our model meets our objectives. We perform this validation through involving a group of software process improvement and RE experts in examining our RE model components and completing a detailed questionnaire. A major part of this paper is devoted to explaining our validation methodology. There is very little in the literature that directly relates to how process models have been validated, therefore providing this transparency will benefit both the research community and practitioners. The validation methodology and the model itself contribute towards a better understanding of modelling RE processes.
76|3||Genetic granular classifiers in modeling software quality|Hyperbox classifiers are one of the most appealing and intuitively transparent classification schemes. As the name itself stipulates, these classifiers are based on a collection of hyperboxes––generic and highly interpretable geometric descriptors of data belonging to a given class. The hyperboxes translate into conditional statements (rules) of the form “if feature1 is in [a, b] and feature2 is in [d, f] and … and featuren is in [w, z] then class Ï” where the intervals ([a, b], … , [w, z]) are the respective edges of the hyperbox. The proposed design process of hyperboxes comprises of two main phases. In the first phase, a collection of “seeds” of the hyperboxes is formed through data clustering (realized by means of the Fuzzy C-Means algorithm, FCM). In the second phase, the hyperboxes are “grown” (expanded) by applying mechanisms of genetic optimization (and genetic algorithm, in particular). We reveal how the underlying geometry of the hyperboxes supports an immediate interpretation of software data concerning software maintenance and dealing with rules describing a number of changes made to software modules and their linkages with various software measures (such as size of code, McCabe cyclomatic complexity, number of comments, number of characters, etc.).
76|3||A crisscross checking technique for tamper detection in halftone images|As the race for digital technological advancements accelerates, foremost is the troubling concern for better protection for digital data. Unequivocally, detecting for tamper in data poses a great challenge to information security and image processing. In this paper, a crisscross checking technique is proposed for tamper detection in halftone images. First, a host image is divided into N × N blocks. The individual blocks will be scrambled with a random number into code-strings which will be concatenated into rows and columns code-streams. Finally, these code-streams will be applied to the MD5 and RSA algorithms to generate encrypted signed messages that will be hidden in the blocks. For the detection process, the row and column blocks will be checked for these signed messages which will be extracted, decrypted and compared to the rows and columns blocks of the image for dissimilar bits as sign of tamper. The tampered row and column blocks will be crisscrossed and the intersecting blocks will be identified as tampered areas.
76|3||Implementation of wireless network environments supporting inter access point protocol and dual packet filtering|With advances in wireless technologies, future internetworks will include large numbers of mobile hosts roaming across wireless cells. The goal of this paper is to provide a wireless network communication environment, in which mobile hosts can roam among various access points (wireless cells) and across different subnets. For this purpose, we develop methods to implement and integrate the communication mechanisms of Mobile IP and Inter Access Point Protocol (IAPP). For efficient transmission, we also devise a dual packet filtering technique to filter out unnecessary packet transmission, thereby reducing traffic on wireless networks. Such filtering is especially important because of the limited bandwidth of wireless links. Our work can serve as the reference once IAPP has been approved as standard.
76|3||Specifying process and measuring progress in terms of information state|This paper suggests an approach to specifying process in such a way that progress can be measured in terms of achievement rather than effort expended. The outcome of each process activity is specified in terms of the status of information items and the relationships between them. Pre-requisites to this approach are the presence of an overall information model, and the ability to manage this model and its instantiation in an information repository. A description is given of how the approach has been implemented for a requirements management process using the DOORS requirements management tool. The work was carried out as part of the EC funded Framework project, SUCSEDE.
76|3||Cryptanalysis of HuangâChang partially blind signature scheme|Partially blind signature is an extension of blind signature that allows a signer to sign a partially blinded message that include pre-agreed information such as expiry date or collateral conditions in the resulting signatures. It is more useful than blind signature in many applications, such as electronic cash (e-cash) system. Recently, Huang and Chang proposed a new design of efficient partially blind signature scheme. However, in this paper, we show that Huang–Chang partially blind signature scheme is not secure, this scheme does not meet the basic property of a partially blind signature, partial blindness: a malicious user (requester) can prepare a special public information a′ and then remove the original information a from the signer’s signature to obtain a signature with this special information.
76|3||A new multi-secret images sharing scheme using Largrangeâs interpolation|Secret sharing is to send shares of a secret to several participants, and the hidden secret can be decrypted only by gathering the distributed shares. This paper proposes an efficient secret sharing scheme using Largrange’s interpolation for generalized access structures. Furthermore, the generated shared data for each qualified set is 1/(r − 1) smaller than the original secret image if the corresponding qualified set has r participants. In our sharing process, a sharing circle is constructed first and the shared data is generated from the secret images according to this sharing circle. The properties of the sharing circle not only reduce the size of generated data between two qualified sets but also maintain the security. Thus the actual ratio of the shared data to the original secret images is reduced further. The proposed scheme offers a more efficient and effective way to share multiple secrets.
76|3||Contents Volume 76|
77|1|http://www.sciencedirect.com/science/journal/01641212/77/1|Parallel and distributed real-time systems|
77|1||Robust scheduling in team-robotics|
77|1||Reliable event-triggered systems for mechatronic applications|Mechatronic systems most often require hard real-time behaviour of the controlling system. The standard solution for this kind of application is based on the time-triggered approach, and for certain circumstances the schedulability is provable. In contrast, this paper introduces an approach using some hardware enhancements that allow first to substitute the time-triggered system by an event-triggered system but conserving the reliability, second to enhance the event-triggered system by a two-level reaction system while conserving the hard real-time capabilities and third to combine tasks to improve even the worst-case behaviour. This results in a hard-time-but-weak-logic reaction system when computing time is tide but maintains full processing capabilities and therefore exact reaction values for all reactions whenever possible. This meets the goal of creating an event-triggered and reliable system approach. Combining two or more events to more than one combination will improve the theoretical schedulability of the system too, especially in the case when configurable computing elements are used.
77|1||COSMIC: A real-time event-based middleware for the CAN-bus|The paper describes the event model and the architecture of the COSMIC (COoperating SMart devICes) middleware. Based on the assumption of tiny smart sensors and actuators, COSMIC supports a distributed system of cooperating autonomous devices. COSMIC considers quality of service requirements in the event model and provides an application interface which allows to express the respective temporal and reliability attributes on a high, application related abstraction level. According to the need in most real-time systems, COSMIC supports event channels with different timeliness and reliability classes. Hard real-time event channels are considered to meet all temporal requirements under the specified fault assumptions. The resource requirements for this type of channel are statically assigned by an appropriate reservation scheme. Soft real-time event channels are scheduled by their deadlines, but they are not guaranteed under transient overload conditions. Non-real-time event channels are used for events without any specified timeliness requirements in a best-effort manner. The paper finally presents the layered COSMIC architecture to map the different channel classes to the CAN-Bus.
77|1||Design and performance of a CAN-based connection-oriented protocol for Real-Time CORBA|The Real-Time CORBA and minimumCORBA specifications are important steps towards defining standard-based middleware which can satisfy real-time requirements in an embedded system. To reach a broad acceptance in the real-time and embedded community, these specifications have to facilitate the utilization of traditional real-time networks for embedded systems. The Controller Area Network (CAN) is one of the most important networks in the field of real-time embedded systems. Consequently, this paper presents a CAN-based connection-oriented communication model and its integration into Real-Time CORBA. In order to make efficient use of the advantages of CAN, we present a new inter-ORB protocol, which uses smaller message headers for CAN and maps the CAN priorities to bands of CORBA priorities. We also present design and implementation details and evaluate the performance of the new inter-ORB protocol.
77|1||Towards modeling and evaluation of ETCS real-time communication and operation|The future European Train Control System (ETCS) will be based on mobile communication and overcome fixed blocks. It is introduced in order to increase track utilization and interoperability throughout Europe while reducing trackside equipment cost. Data processing on board the train and in radio block centers as well as the radio communication link are crucial factors for the safe and efficient operation. Their real-time behavior under inevitable link failures needs to be modeled and evaluated. The paper presents a stochastic Petri net model of communication failure and recover behavior. A second model for the exchange of location and movement authority data packets between trains and radio block centers is presented and analyzed. Performance evaluation of the model shows the significant impact of packet delays and losses on the reliable operation of high-speed trains.
77|1||MIP formulation for robust resource allocation in dynamic real-time systems|Real-time systems usually operate in an environment that changes continuously. These changes cause the performance of the system to vary during run time. An allocation of resources in this environment must be robust. Using the amount of load variation that the allocation can accommodate as a measure of robustness, we develop a mathematical formulation for the problem of robust resource allocation. Due to the complexity of the models used to represent the problem, the formulation is non-linear. We propose a linearization technique based on variable substitution to reduce the mathematical formulation to a mixed integer programming formulation, called SMIP. Compared with existing techniques, the search space of SMIP is not restricted. Thus, if a feasible allocation exists, SMIP will always produce an optimal allocation.
77|1||Fair scheduling of dynamic task systems on multiprocessors|In dynamic real-time task systems, tasks that are subject to deadlines are allowed to join and leave the system. In previous work, Stoica et al. and Baruah et al. presented conditions under which such joins and leaves may occur in fair-scheduled uniprocessor systems without causing missed deadlines. In this paper, we extend their work by considering fair-scheduled multiprocessors. We show that their conditions are sufficient on M processors, under any deadline-based Pfair scheduling algorithm, if the utilization of every subset of M − 1 tasks is at most one. Further, for the general case in which task utilizations are not restricted in this way, we derive sufficient join/leave conditions for the PD2 Pfair algorithm. We also show that, in general, these conditions cannot be improved upon without causing missed deadlines.
77|2|http://www.sciencedirect.com/science/journal/01641212/77/2|Improved standard FPA methodâresolving problems with upper boundaries in the rating complexity process|The standard Function Point Analysis (FPA) method maps complexity to an ordinal scale such that function types (functions) labelled “high” complexity may have a very different underlying complexity. This deficiency is the subject of this article. An improved FPA method is presented that breaks down a very complex function into an equivalent set of functions with normal complexity. The improved FPA method is not a new method, it is an improvement of the standard FPA method. In the process of defining an improved FPA method, a model was built, which helped us find the connection between the standard FPA method and the Common Software Measurement International Consortium—Full Function Point (COSMIC-FFP) and Mark II Function Point Analysis (MKII FPA) methods. These methods were used to define the improved FPA method algorithm, because they show a bigger functional size for very complex functions. In the end, an evaluation of the improved FPA method was made with the results of the ISBSG Repository Data Disk.
77|2||An empirical comparison of the dynamic modeling in OML and UML|This paper presents an empirical research for evaluating the semantic comprehension of two standard languages, UML (Unified Modeling Language) versus OML (OPEN Modeling Language), from the perspective of the dynamic modeling. We carried out two controlled experiments using a 2 × 2 crossover design, where the metrics studied were the comprehension time and the total score. We examined the OML and UML interaction diagrams and the statecharts of each language corresponding to the design of a real-time embedded system. The results obtained reveal that the specification of the dynamic behavior using OML is faster to comprehend and easier to interpret than using the UML language, regardless of the dynamic diagram type.
77|2||Bayesian network based software reliability prediction with an operational profile|This paper uses a Bayesian network to model software reliability prediction with an operational profile. Due to the complexity of software products and development processes, software reliability models need to possess the ability to deal with multiple parameters. A Bayesian network exhibits a strong ability to adapt to problems involving complex variant factors. A special kind of Bayesian network named a Markov Bayesian network has been applied successfully into modeling software reliability prediction. However, the existing research did not pay enough attention to the fact that the failure characteristics of many software systems often depend on the specific operation performed. In this paper, an extended Markov Bayesian network is developed to model software reliability prediction with an operational profile. The extended Markov Bayesian network proposed in the paper is focused on discrete-time failure data. Methods to solve the network are proposed, and an example is used to illustrate the utilization of the model.
77|2||A family of experiments to validate metrics for software process models|Process modelling is a key activity of software process management and it is the starting point for enacting, evaluating and improving software processes. The current competitive marketplace calls for the continuous improvement of processes and therefore, it is fundamental to have software process models with a high maintainability. In this paper we introduce a set of metrics for software process models and discuss how these can be used as maintainability indicators. In particular, we report the results of a family of experiments that assess relationships between the structural properties, as measured by the defined metrics, of the process models and their maintainability.
77|2||An empirical investigation of the impact of the object-oriented paradigm on the maintainability of real-world mission-critical software|Empirical evidence for the maintainability of object-oriented systems is far from conclusive, partly due to the lack of representativeness of the subjects and systems used in earlier studies. We empirically examined this issue for mission-critical software that was currently operational and maintained by software professionals. Two functionally equivalent versions of a credit approval system were maintained, one object oriented (OO) and the other non-object oriented (NOO). We found that the OO group took less time to maintain a greater number of software artifacts than its NOO counterpart. This difference held for all phases of the software development life cycle. This result was due to the usefulness of UML for impact analysis of the OO version, which contributed to effective comprehension and communication. Insufficient design specifications for the NOO version led to ambiguity and costly defects in transferring design solutions to development. Also, the encapsulation of the OO version appeared to reduce mental loads for maintenance tasks and resulted in code reuse. On the other hand, the number of files to be managed increased and, thus, dependency management was required for the OO version. Furthermore, despite much tuning, the OO version ran slower than its NOO counterpart. More field studies on software professionals are needed to compare contextual factors such as methods, processes, and maintenance tools.
77|2||Cost-reliability-optimal release policy for software reliability models incorporating improvements in testing efficiency|Over the past 30 years, many software reliability growth models (SRGMs) have been proposed for estimation of reliability growth of products during software development processes. One of the most important applications of SRGMs is to determine the software release time. Most software developers and managers always want to know the date on which the desired reliability goal will be met. In this paper, we first review a SRGM with generalized logistic testing-effort function and the proposed generalized logistic testing-effort function can be used to describe the actual consumption of resources during the software development process. Secondly, if software developers want to detect more faults in practice, it is advisable to introduce new test techniques, tools, or consultants, etc. Consequently, here we propose a software cost model that can be used to formulate realistic total software cost projects and discuss the optimal release policy based on cost and reliability considering testing effort and efficiency. Some theorems and several numerical illustrations are also presented. Based on the proposed models and methods, we can specifically address the problem of how to decide when to stop testing and when to release software for use.
77|2||Investigating Web size metrics for early Web cost estimation|This paper’s aim is to bring light to this issue by identifying size metrics and cost drivers for early Web cost estimation based on current practices of several Web Companies worldwide. This is achieved using two surveys and a case study. The first survey (S1) used a search engine to obtain Web project quote forms employed by Web companies worldwide to provide initial quotes on Web development projects. The 133 Web project quote forms gathered data on size metrics, cost factors, contingency and possibly profit metrics. These metrics were organised into categories and ranked. Results indicated that the two most common size metrics used for Web cost estimation were “total number of Web pages” (70%) and “which features/functionality to be provided by the application” (66%).
77|2||On-line prediction of software reliability using an evolutionary connectionist model|An on-line adaptive software reliability prediction model using evolutionary connectionist approach based on multiple-delayed-input single-output architecture is proposed. Based on the currently available software failure time data, genetic algorithm is used to globally optimize the number of the delayed input neurons and the number of neurons in the hidden layer of the neural network architecture. Bayesian regularization is applied to our network training scheme to improve the generalization capability. The corresponding optimized neural network architecture is iteratively and dynamically reconfigured in real-time as new actual failure time data arrives. The performance of our proposed approach has been tested using four real-time control and flight dynamic application data sets. Numerical results show that our proposed approach is robust across different software projects, and has a better performance with respect to next-step-predictability compared to existing neural network model for failure time prediction.
77|2||Hybrid approaches to product recommendation based on customer lifetime value and purchase preferences|Recommending products to attract customers and meet their needs is important in fiercely competitive environments. Recommender systems have emerged in e-commerce applications to support the recommendation of products. Recently, a weighted RFM-based method (WRFM-based method) has been proposed to provide recommendations based on customer lifetime value, including Recency, Frequency and Monetary. Preference-based collaborative filtering (CF) typically makes recommendations based on the similarities of customer preferences. This study proposes two hybrid methods that exploit the merits of the WRFM-based method and the preference-based CF method to improve the quality of recommendations. Experiments are conducted to evaluate the quality of recommendations provided by the proposed methods, using a data set concerning the hardware retail marketing. The experimental results indicate that the proposed hybrid methods outperform the WRFM-based method and the preference-based CF method.
77|2||Model driven generation and testing of object-relational mappings|Object-oriented software development as well as relational data storage are leading standards in their respective areas. The persistent storage of objects in relational tables is therefore a topic of major interest. To do so efficiently, a plethora of problems has to be overcome due to the impedance mismatch between the object-oriented and relational paradigms. Nowadays, dedicated object-relational middlewares are frequently used to decouple relational databases from object-oriented applications. Even if this approach shields the developer from the majority of run-time related aspects, the manual mapping of incrementally evolving complex object models to relational tables still remains as an inherently difficult and error-prone task. Therefore, this article focuses on automation support for the model driven generation and testing of object-relational mappings.
77|3|http://www.sciencedirect.com/science/journal/01641212/77/3|Software reverse engineering|
77|3||Spectral and meta-heuristic algorithms for software clustering|When large software systems are reverse engineered, one of the views that is produced is the system decomposition hierarchy. This hierarchy shows the system’s subsystems, the contents of the subsystems (i.e., modules or other subsystems), and so on. Software clustering tools create the system decomposition automatically or semi-automatically with the aid of the software engineer.
77|3||A language-independent software renovation framework|One of the undesired effects of software evolution is the proliferation of unused components, which are not used by any application. As a consequence, the size of binaries and libraries tends to grow and system maintainability tends to decrease. At the same time, a major trend of today’s software market is the porting of applications on hand-held devices or, in general, on devices which have a limited amount of available resources. Refactoring and, in particular, the miniaturization of libraries and applications are therefore necessary.
77|3||ConSUS: a light-weight program conditioner|Program conditioning consists of identifying and removing a set of statements which cannot be executed when a condition of interest holds at some point in a program. It has been applied to problems in maintenance, testing, re-use and re-engineering. All current approaches to program conditioning rely upon both symbolic execution and reasoning about symbolic predicates. The reasoning can be performed by a ‘heavy duty’ theorem prover but this may impose unrealistic performance constraints.
77|3||Static object trace extraction for programs with pointers|A trace is a record of the execution of a computer program, showing the sequence of operations executed. Dynamic traces are obtained by executing the program and depend upon the input. Static traces, on the other hand, describe potential sequences of operations extracted statically from the source code. Static traces offer the advantage that they do not depend upon input data.
77|3||Discovering thread interactions in a concurrent system|Understanding the behavior of a system is a central reverse engineering task, and is crucial for being able to modify, maintain, and improve the system. An often difficult aspect of some system behaviors is concurrency, in particular identifying those areas that exhibit mutual exclusion and those that exhibit synchronization. In this paper we present a technique that builds on our previous work in behavior discovery to find the points in the system that demonstrate mutually exclusive and synchronized behavior. Finding these points in the behavior of the system is an important aid in reverse engineering a complete and correct model of the system.
77|3||Contents Volume 77|
78|1|http://www.sciencedirect.com/science/journal/01641212/78/1|A novel image watermarking scheme based on support vector regression|In this paper, a novel support vector regression based color image watermarking scheme is proposed. Using the information provided by the reference positions, the support vector regression can be trained at the embedding procedure, and the watermark is adaptively embedded into the blue channel of the host image by considering the human visual system. Thanks to the good learning ability of support vector machine, the watermark can be correctly extracted under several different attacks. Experimental results show that the proposed scheme outperform the Kutter’s method and Yu’s method against different attacks including noise addition, shearing, luminance and contrast enhancement, distortion, etc. Especially when the watermarked image is enhanced in luminance and contrast at rate 70%, our method can extract the watermark with few bit errors.
78|1||Relevance feedback using adaptive clustering for image similarity retrieval|Research has been devoted in recent years to relevance feedback as an effective solution to improve performance of image similarity search. However, few methods using the relevance feedback are currently available to perform relatively complex queries on large image databases. In the case of complex image queries, images with relevant concepts are often scattered across several visual regions in the feature space. This leads to adapting multiple regions to represent a query in the feature space. Therefore, it is necessary to handle disjunctive queries in the feature space.
78|1||OPERA: An open-source extensible router architecture for adding new network services and protocols|In this paper, we present the design and implementation of a programmable and extensible router architecture. The proposed architecture not only provides the conventional packet forward/routing functions, but also the flexibility to integrate additional services (or extension) into a router. These extensions are dynamically loadable modules so one can easily deploy new services, such as reliability and security enhancement, onto the router in a dynamic and incremental fashion. To avoid new extensions that may monopolize system resource and degrade the performance of normal packet forwarding/routing function, we propose a novel CPU resource reservation scheme which facilitates the efficient use of resources and increases the stability of extension execution. To illustrate the “extensibility” and “effectiveness” of the proposed architecture, we present the results of a new service, namely, how to perform “Distributed Denial-of-Service (DDoS) attack traceback”. In particular, we illustrate the deployment of the probabilistic marking in performing IP traceback. Note that this approach requires the collaboration of routers so that effective traceback can be performed. Currently, the programmable router platform is released as an open source1 and we believe the system provides an ideal platform for researchers to experiment and to validate new services and protocols.
78|1||Lightweight multigranularity locking for transaction management in XML database systems|As eXtensible Markup Language (XML) provides a capability for describing data structures, and can accommodate many kinds of semistructured data. The semistructured data format is flexible for changing data structures through insertion and deletion of data elements in mission-critical applications. In the case of concurrently changing such a data format, this flexibility could be endangered by a phantom problem which might lead to inconsistent information flow. For the purpose of developing a concurrency control scheme without the phantom phenomenon, we propose a lightweight multigranularity locking (LWMGL) scheme that is a hybrid mechanism of Tree-based Locking and Multigranularity Locking. The goal of this scheme is to realize locking at the level of precise elements in an XML database while preventing the phantom problems. Since these precise locks could considerably reduce the number of pseudo-conflicts that are regarded as unnecessary locks, they provide high concurrency compared with other concurrency control schemes. In order to realize the LWMGL scheme we also devised a new data model of XML indexed element tables (XIETs) for transferring diverse XML documents. This data model does not only can preserve the XML tree structure in application levels, but also enables execution of the structural change operations as well as the data access operations in parallel.
78|1||A least upper bound on the fault tolerance of real-time systems|This paper presents a method to deal with the reexecution of tasks in a hard real-time system subject to temporary faults. The set of tasks follows the Liu and Layland model: they are periodic, independent and preemptible. Time is considered to be slotted. The system is said to be k-schedulable if it is schedulable in spite of the fact that in the interval between its release and its deadline, every task admits that k slots are devoted to uses other than its first execution. In this case, the k slots are used to reexecute tasks subject to temporary faults. Since the value of k can be easily determined, a least upper bound on all the possible combinations of faults that the system can tolerate while meeting the hard time-constraints, follows immediately. The method is bandwidth preserving and the expression of the bound is a diophantic inequality relating k, the execution time and the period of each task. The method is compared to methods proposed by other authors to solve the same problem and it is evaluated through extensive simulations performed on random generated sets of tasks.
78|1||Remarks on WuâHsuâs threshold signature scheme using self-certified public keys|Wu and Hsu proposed a (t, n) threshold signature scheme using self-certified public keys in order to integrate the properties of self-certified public key schemes and threshold signature schemes. Even though their scheme is more efficient when compared to previous works based on the certificate-based public key systems, we find some design defects of their scheme. In this paper, by identifying some concrete instances and analyses we will show that their scheme is not as secure as they claimed.
78|1||Overcoming the obfuscation of Java programs by identifier renaming|Decompilation is the process of translating object code to source code and is usually the first step towards the reverse-engineering of an application. Many obfuscation techniques and tools have been developed, with the aim of modifying a program, such that its functionalities are preserved, while its understandability is compromised for a human reader or the decompilation is made unsuccessful. Some approaches rely on malicious identifiers renaming, i.e., on the modification of the program identifiers in order to introduce confusion and possibly prevent the decompilation of the code.
78|1||DDH-based group key agreement in a mobile environment|A group key agreement protocol is designed to efficiently implement secure multicast channels for a group of parties communicating over an untrusted, open network by allowing them to agree on a common secret key. In the past decade many problems related to group key agreement have been tackled and solved (diminished if not solved), and recently some constant-round protocols have been proven secure in concrete, realistic setting. However, all forward-secure protocols so far are still too expensive for small mobile devices. In this paper we propose a new constant-round protocol well suited for a mobile environment and prove its security under the decisional Diffie–Hellman assumption. The protocol meets simplicity, efficiency, and all the desired security properties.
78|1||An information flow control model for C applications based on access control lists|Access control within an application during its execution prevents information leakage. The prevention can be achieved through information flow control. Many information flow control models were developed, which may be based on discretionary access control (DAC), mandatory access control (MAC), label-based approach, and role-based access control (RBAC). Most existing models are for object-oriented systems. Since the procedural C language is still in use heavily, offering a model to control information flows for C applications should be fruitful. Although we identified information flow control models that can be applied to procedural languages, they do not offer the features we need. We thus developed a model to control information flows for C applications. Our model is based on access control lists (ACLs) and named CACL. It offers the following features: (a) controlling both read and write access, (b) preventing indirect information leakage, (c) detailing the control granularity to variables, (d) avoiding improper function call, (e) controlling function call through argument sensitivity, and (f) preventing change of an application when the access rights of the application’s real world users change. This paper presents CACL.
78|2|http://www.sciencedirect.com/science/journal/01641212/78/2|Automatic detection and correction of programming faults for software applications|Software reliability is an important feature of a good software implementation. However some faults which cause software unreliability are not detected during the development stages, and these faults create unexpected problems for users whenever they arise. At present most of the current techniques detect faults while a software is running. These techniques interrupt the software process when a fault occurs, and require some forms of restart.
78|2||Bi-directional safety analysis of product lines|As product-line engineering becomes more widespread, more safety-critical software product lines are being built. This paper describes a structured method for performing safety analysis on a software product line, building on standard product-line assets: product-line requirements, architecture, and scenarios. The safety-analysis method is bi-directional in that it combines a forward analysis (from failure modes to effects) with a backward analysis (from hazards to contributing causes). Safety-analysis results are converted to XML files to allow automated consistency checking between the forward and backward analysis results and to support reuse of the safety-analysis results throughout the product line. The paper demonstrates and evaluates the method on a safety-critical product-line subsystem, the Door Control System. Results show that the bi-directional safety-analysis method found both missing and incorrect software safety requirements. Some of the new safety requirements affected all the systems in the product line while others affected only some of the systems in the product line. The results demonstrate that the proposed method can handle the challenges to safety analysis posed by variations within a product line.
78|2||Software complexity and its impacts in embedded intelligent real-time systems|Applications of intelligent software systems are proliferating. As these systems proliferate, understanding and measuring their complexity becomes vital, especially in safety-critical environments. This paper proposes a model assessing the impacts of complexity for a particular type of intelligent software system, embedded intelligent real-time systems (EIRTS), and answers two research questions. (1) How should the complexity of embedded intelligent real-time systems be measured?, and (2) What are the impacts of differing levels of EIRTS complexity on software, operator and system performance when EIRTS are deployed in a safety-critical large-scale system? The model is tested and operationalized using an operational EIRTS in a safety-critical environment. The results suggest that users significantly prefer simple decision support and user interfaces, even when sophisticated user interfaces and complex decision support capabilities have been embedded in the system. These results have interesting implications for operators using complex EIRTS in safety-critical settings.
78|2||Efficient online computation of statement coverage|Evaluation of statement coverage is the problem of identifying the statements of a program that execute in one or more runs of a program. The traditional approach for statement coverage tools is to use static code instrumentation. In this paper we present a new approach to dynamically insert and remove instrumentation code to reduce the runtime overhead of statement coverage measurement. We also explore the use of dominator tree information to reduce the number of instrumentation points needed. Our experiments show that our approach reduces runtime overhead by 38–90% compared with purecov, a commercial statement coverage tool. Our tool is fully automated and available for download from the Internet.
78|2||Two controlled experiments concerning the comparison of pair programming to peer review|This paper reports on two controlled experiments comparing pair programming with single developers who are assisted by an additional anonymous peer code review phase. The experiments were conducted in the summer semester 2002 and 2003 at the University of Karlsruhe with 38 computer science students. Instead of comparing pair programming to solo programming this study aims at finding a technique by which a single developer produces similar program quality as programmer pairs do but with moderate cost.
78|2||FNDS: a dialogue-based system for accessing digested financial news|Electronic financial news available on the Internet contains a wealth of information useful for business decision-making. However, as this information is both qualitative and existent in huge volumes, it is very inefficient to digest manually. This paper presents a prototype system called FNDS, which automatically digests financial news by extracting important information from the articles and using this information to fill in pre-defined templates. A unique feature of FNDS is that users can access the extracted information through an interactive dialogue-based interface. This has the advantage that if users do not know exactly what information is required, the system will provide feedback to help them to formulate the information requirement incrementally.
78|2||What do software practitioners really think about project success: an exploratory study|Understanding what software practitioners value and how they define project success has implications for both practitioner motivation and software development productivity. We conducted a survey to discover some of the components of project outcome (in terms of personal/professional aspects as well as the project as a whole) that practitioners consider important in defining project success. We also investigated some of those components that practitioners perceived were important contributors to success through their impact on the development process. Sixty-six practitioners participated in our study. They considered software projects to be successful if they provide them with intrinsic, internally motivating work in developing software systems that both meet customer/user needs and are easy to use.
78|2||A framework for assisting the design of effective software process improvement implementation strategies|A number of advances have been made in the development of software process improvement (SPI) standards and models, e.g. Capability Maturity Model (CMM), more recently CMMI, and ISO’s SPICE. However, these advances have not been matched by equal advances in the adoption of these standards and models in software development which hasresulted in limited success for many SPI efforts. The current problem with SPI is not a lack of standards or models, but rather a lack of an effective strategy to successfully implement these standards or models.
78|3|http://www.sciencedirect.com/science/journal/01641212/78/3|Announcement: New Section on Software Architecture|
78|3||Pinned demand paging based on the access frequency of video files in video servers|In this paper, we present an ameliorative demand-paging algorithm called PDPAF (i.e., pinned demand paging based on the access frequency of video files), to efficiently utilize the limited buffer space in a VOD (video-on-demand) server. It excludes the limitation of the disk bandwidth, and raises the hit ratio of video pages in the buffer, thereby increasing the total number of concurrent clients. Furthermore, we also propose an admission control algorithm to decide whether a new request can be admitted. Finally, we conduct extensive experiments to compare PDPAF with other algorithms on the average waiting time and the maximal number of concurrent requests, and the simulation results validate the superiority of our approach.
78|3||Distributed multiple selection algorithm for peer-to-peer systems|
78|3||Worst case constant time priority queue|
78|3||Adaptive row major order: a new space filling curve for efficient spatial join processing in the transform space|A transform-space index indexes spatial objects represented as points in the transform space. An advantage of a transform-space index is that optimization of spatial join algorithms using these indexes can be more formal. The authors earlier proposed the Transform-Based Spatial Join algorithm that joins two transform-space indexes. It renders global optimization easy with little overhead by utilizing the characteristics of the transform space. In particular, it allows us to globally determine the order of accessing disk pages, which makes a significant impact on the performance of joins. For this purpose, we use various space filling curves. In this paper, we propose a new space filling curve called the adaptive row major order (ARM order). The ARM order adaptively controls the order of accessing pages and significantly reduces the one-pass buffer size (the minimum buffer size required for guaranteeing one disk access per page) and the number of disk accesses for a given buffer size. Through analysis and experiments, we verify excellence of the ARM order when used with the Transform-Based Spatial Join. The Transform-Based Spatial Join with the ARM order always outperforms those with other conventional space filling curves in terms of both measures used: the one-pass buffer size and the number of disk accesses. Specifically, it reduces the one-pass buffer size by up to 25.9 times and the number of disk accesses by up to 2.11 times. We conclude that we achieve these results mainly due to global optimization of the order of accessing disk pages using an adaptive space filling curve.
78|3||HANet: a framework toward ultimately reliable network services|High availability is becoming an essential part of network services because even a little downtime may lead to a great loss of money. According to previous research, network failure is one of the major causes of system unavailability. In this paper, we propose a framework called HANet for building highly available network services. The main goal of HANet is to allow a server to continue providing services when all its network interfaces to the outside world (i.e., public interfaces) have failed. This is achieved by two techniques. First, a network interface can be backed up not only by other public network interfaces, but also by other inter-server I/O communication interfaces (i.e., private interfaces) such as Ethernet, USB, RS232, etc. Therefore, IP packets can still be transmitted and received via these I/O links, even when all of the public network interfaces have failed. Second, HANet allows a server to take over the packet transmission job of another network-failed server.
78|3||Contents Volume 78|
79|1|http://www.sciencedirect.com/science/journal/01641212/79/1|Towards the automatic generation of mobile agents for distributed intrusion detection system|The Mobile Agent Intrusion Detection System (MAIDS) is an agent based distributed Intrusion Detection System (IDS). A disciplined requirement engineering process is developed to build MAIDS. The starting point is a high level description of intrusions expressed as Software Fault Trees (SFTs). Then the SFTs are translated to Colored Petri Nets (CPNs) that specify the IDS design. Subsequently, the CPNs are implemented as software intrusion detection agents in the MAIDS agent system. By using SFT and CPN as the theoretical underpinnings, the design and implementation of MAIDS can be verified and the design and implementation errors can be substantially reduced.
79|1||The knowledge management efficacy of matching information systems development methodologies with application characteristicsâan experimental study|
79|1||REDUP: a packet loss recovery scheme for real-time audio streaming over wireless IP networks|
79|1||FMF: Query adaptive melody retrieval system|Recent progress of computer and network technologies makes it possible to store and retrieve a large volume of multimedia data in many applications. In such applications, efficient indexing scheme is very important for multimedia retrieval. Depending on the media type, multimedia data shows distinct characteristics and requires different approach to handle. In this paper, we propose a fast melody finder (FMF) that can retrieve melodies fast from audio database based on frequently queried tunes. Those tunes are collected from user queries and incrementally updated into index. Considering empirical user request pattern for multimedia data, those tunes will cover significant portion of user requests. FMF represents all the acoustic and common music notational inputs using a well-known string format such as UDR and LSR and uses string matching techniques to find query results. We implemented a prototype system and report on its performance through various experiments.
79|1||An approach to feature location in distributed systems|
79|1||Designing state-based systems with entity-life modeling|This article introduces the entity-life modeling (ELM) design approach for multithread software. The article focuses on problems that can be described by state machines with associated activities. We compare ELM with a traditional design approach based on a dataflow model. Using a cruise controller for a car as an example, we show that entity-life modeling is a more direct and less ceremonious approach that produces a much simpler architecture.
79|1||Mining association rules with multi-dimensional constraints|To improve the effectiveness and efficiency of mining tasks, constraint-based mining enables users to concentrate on mining their interested association rules instead of the complete set of association rules. Previously proposed methods are mainly contributed to handling a single constraint and only consider the constraints which are characterized by a single attribute value. In this paper, we propose an approach to mine association rules with multiple constraints constructed by multi-dimensional attribute values. Our proposed approach basically consists of three phases. First, we collect the frequent items and prune infrequent items according to the Apriori property. Second, we exploit the properties of the given constraints to prune search space or save constraint checking in the conditional databases. Third, for each itemset possible to satisfy the constraint, we generate its conditional database and perform the three phases in the conditional database recursively. Our proposed algorithms can exploit the properties of constraints to prune search space or save constraint checking. Therefore, our proposed algorithm is more efficient than the revised FP-growth and FIC algorithms.
79|1||Efficient index caching for data dissemination in mobile computing environments|Due to the limited power supply of mobile devices, much research has been done on reducing the power consumption of mobile devices in mobile computing environments. Since supporting indices on the broadcast data items can effectively reduce the power consumption of the mobile clients, most of the existing research on data broadcasting has focused on designing efficient indexing schemes. In this paper, we propose to cache indices on the mobile clients and to use cached indices to facilitate the access of the broadcast data items. To manage the cache usage of the mobile clients, we propose two cache management policies. The lower-level-index-first policy caches the index nodes that are at the lower level of the index tree, while the cut-plane-first policy caches the index nodes that belong to a cut-plane of the index tree. Through experiments, we compare the performance of the two proposed policies with some existing policies in terms of tuning time and access time. The experiments show that index caching significantly reduces the tuning time of a mobile client without increasing its access time. In terms of tuning time, the experiments show that, when the access pattern of a mobile client is not skew, the cut-plane-first policy outperforms the lower-level-index first policy, LRU and LRFU. On the contrary, when the mobile client has a small cache and its access pattern is skew, the lower-level-index-first policy outperforms the cut-plane-first policy, LRU and LRFU. In terms of access time, the lower-level-index first policy outperforms the cut-plane-first policy, LRU and LRFU.
79|1||A practical framework for eliciting and modeling system dependability requirements: Experience from the NASA high dependability computing project|The dependability of a system is contextually subjective and reflects the particular stakeholder’s needs. In different circumstances, the focus will be on different system properties, e.g., availability, real-time response, ability to avoid catastrophic failures, and prevention of deliberate intrusions, as well as different levels of adherence to such properties. Close involvement from stakeholders is thus crucial during the elicitation and definition of dependability requirements. In this paper, we suggest a practical framework for eliciting and modeling dependability requirements devised to support and improve stakeholders’ participation. The framework is designed around a basic modeling language that analysts and stakeholders can adopt as a common tool for discussing dependability, and adapt for precise (possibly measurable) requirements. An air traffic control system, adopted as testbed within the NASA High Dependability Computing Project, is used as a case study.
79|1||A method for defining IEEE Std 1471 viewpoints|With the growing impact of information technology the proper understanding of IT-architecture designs is becoming ever more important. Much debate has been going on about how to describe them. In 2000, the IEEE Std 1471 proposed a model of an architecture description and its context.
79|1||Architecture-based software reliability modeling|
79|10|http://www.sciencedirect.com/science/journal/01641212/79/10|Architecting dependable systems|
79|10||Using temporal logic to specify adaptive program semantics|Computer software must dynamically adapt to changing conditions. In order to fully realize the benefit of dynamic adaptation, it must be performed correctly. The correctness of adaptation cannot be properly addressed without precisely specifying the requirements for adaptation. This paper introduces an approach to formally specifying adaptation requirements in temporal logic. We introduce A-LTL, an adaptation-based extension to linear temporal logic, and use this logic to specify three commonly used adaptation semantics. Composition techniques are developed and applied to A-LTL to construct the specification of an adaptive program. We introduce adaptation semantics graphs to visually represent the adaptation semantics, which can also be used to automatically generate specification for adaptive programs.
79|10||An architectural pattern for non-functional dependability requirements|
79|10||Software architecture-based regression testing|Software architectures are becoming centric to the development of quality software systems, being the first concrete model of the software system and the base to guide the implementation of software systems. When architecting dependable systems, in addition to improving system dependability by means of construction (fault-tolerant and redundant mechanisms, for instance), it is also important to evaluate, and thereby confirm, system dependability. There are many different approaches for evaluating system dependability, and testing has been always an important one, being fault removal one of the means to achieve dependable systems.
79|10||Specification of exception flow in software architectures|
79|10||Comparative evaluation of dependability characteristics for peer-to-peer architectural styles by simulation|An important concern for the successful deployment of a dependable system is its quality of service (QoS), which is significantly influenced by its architectural style. We propose the comparative evaluation of architectural styles by simulation. Our approach integrates architectural styles and concrete architectures to enable early design-space exploration in order to predict the QoS of peer-to-peer systems. We illustrate the approach via two case studies where availability of resources and performance of peer-to-peer search methods are evaluated. Based on our experience with these simulation environments, we sketch tool support for simulating architectural changes at runtime.
79|10||Verification of rectangular hybrid automata models|Formal analysis of hybrid systems is concerned with verifying whether a hybrid system satisfies a desired specification, like avoiding an unsafe region of the state space. For safety, economical and performance reasons, the satisfaction of critical design specifications of hybrid systems must be verified before embarking into their expensive construction and operation. Verification approaches based on system modelling with rectangular hybrid automata can lead to decidable solutions that find an algorithm which searches for the existence of states in the model which can be associated with the design specifications. This search is usually computation time intensive and after the elapse of a significant portion of time the user is unable to judge whether the state of interest is not reachable or the algorithm requires more time to terminate. In this work an algorithm, which alleviates the drawback of state space search, is proposed. Analytic approximations are used to express the time period the automaton stays at each one of its vertices in terms of the number of automaton iterations required to reach a desired state. Solving these relationships, one can find out whether the state of the model is reachable after a finite number of iterations. A case study is also presented that demonstrates the application of the algorithm to the verification of the operating specifications of a chemical reaction process that is controlled by a computer.
79|10||Automatic code generation from high-level Petri-Nets for model driven systems engineering|One of the main concepts of the model driven architecture framework proposed by the OMG is the transformation of platform independent models into either platform dependent ones or into implementations. The latter needs generators which automatically translate models into code of a chosen target programming language. Nowadays most models in systems engineering are created with the UML as standardized set of notations. However, the UML is still largely undefined from a semantical point of view. Automatic code generation from these models is thus only possible if further semantic definitions are provided. Since Petri-Nets are frequently utilized for these purposes, the automatic generation of code from Petri-Nets is an important topic.
79|10||Structural translation from Time Petri Nets to Timed Automata|In this paper, we consider Time Petri Nets (TPN) where time is associated with transitions. We give a formal semantics for TPNs in terms of Timed Transition Systems. Then, we propose a translation from TPNs to Timed Automata (TA) that preserves the behavioral semantics (timed bisimilarity) of the TPNs. For the theory of TPNs this result is twofold: (i) reachability problems and more generally TCTL model-checking are decidable for bounded TPNs; (ii) allowing strict time constraints on transitions for TPNs preserves the results described in (i). The practical applications of the translation are: (i) one can specify a system using both TPNs and Timed Automata and a precise semantics is given to the composition; (ii) one can use existing tools for analyzing timed automata (like Kronos, Uppaal or Cmc) to analyze TPNs. In this paper we describe the new feature of the tool Romeo that implements our translation of TPNs in the Uppaal input format. We also report on experiments carried out on various examples and compare the result of our method to state-of-the-art tool for analyzing TPNs.
79|10||Semantic component networking: Toward the synergy of static reuse and dynamic clustering of resources in the knowledge grid|
79|11|http://www.sciencedirect.com/science/journal/01641212/79/11|Introduction to the special section on software cybernetics|
79|11||A control-theoretic approach to the management of the software system test phase|A quantitative, adaptive process control technique is described using an industrially validated model of the software system test phase (STP) as the concrete target to be controlled. The technique combines the use of parameter correction and Model Predictive Control to overcome the problems induced by modeling errors, parameter estimation errors, and limits on the resources available for productivity improvement.
79|11||Requirement process establishment and improvement from the viewpoint of cybernetics|As a branch of engineering cybernetics, automatic control theory has been extensively applied to improve products, increase productivity and rationalize management. This paper adapts the principles of automatic control theory to the field of software process improvement. In particular, the work described uses control theory to define a requirement engineering (RE) process control system, its dynamic and steady-state performance, and the steps in designing, analyzing and improving such a system. The work has highlighted the need for process activities relating to measuring elements, including those in feedback compensation and organizational support. The results of this research can be used to guide the establishment and improvement of RE processes, compare different requirement process solutions quantitatively, develop methods for evaluating benefits from process improvements, and structure the application of knowledge about RE.
79|11||Developing adaptive systems with synchronized architectures|
79|11||Monitoring techniques for an online neuro-adaptive controller|
79|11||Software project managers and project success: An exploratory study|Traditionally, a project should deliver agreed upon functionality on time and within budget. This study examines the mindset of software development project managers in regard to how they ‘define’ a successful project in order to arrive at a richer perspective of ‘success’ from the perspective of project managers. Senior management and members of the development team can gain a better understanding of the perspective of project managers regarding some of the aspects of their work and the project as a whole. Such understanding can facilitate better communication and cooperation among these groups. We investigated components of the developed system (project) in terms of some of the aspects of the delivered system (outcome/project) in order to place traditional measures of success in context with other organizational/managerial measures that have been suggested in the literature. We also investigated specific work-related items. These items have potential implications for the intrinsic motivation of the project manager. The consensus among the project managers who participated in our study indicated that delivering a system that meets customer/user requirements and works as intended through work that provides a sense quality and personal achievement are important aspects that lead to a project being considered a success.
79|11||An intelligent early warning system for software quality improvement and project management|There are three major problems with software projects: over budget, behind schedule, and poor quality. It is often too late to correct these problems by the time they are detected in failed software projects. In this paper, we discuss design, implementation, and evaluation of an experimental intelligent software early warning system based on fuzzy logic using an integrated set of software metrics. It consists of the following components: software metrics database, risk knowledge base, intelligent risk assessment, and risk tracing. It helps to assess risks associated with the three problems from perspectives of product, process, and organization in the early phases of the software development process. It is capable of aggregating various partial risk assessment results into an overall risk indicator even if they may be conflicting. In addition, it can be used to analyze a risk by identifying its root causes through its risk tracing utility.
79|11||A combined specification language and development framework for agent-based application engineering|
79|11||Software reliability growth model with change-point and environmental function|This paper presents a SRGM (software reliability growth model) with both change-point and environmental function based on NHPP (non-homogeneous Poisson process). Although a few research projects have been devoted to the change-point problems of SRGMs, consideration of the variation of environmental factors in the existing models during testing time is limited. The proposed model is one of a few NHPP models, which takes environmental factors as a function of testing time. FDR (fault detection rate) is usually used to measure the effectiveness of fault detection by test techniques and test cases. A FDR function after the change-point of the testing is proposed, which is computed from both environmental factors and FDR before the change-point of the testing. A NHPP SRGM with both change-point and environmental function called CE-SRGM is built which integrates the FDR before the change-point of the testing and the proposed FDR function after the change-point of the testing. CE-SRGM is evaluated using two sets of software failure data. The experimental results show that the predictive power of CE-SRGM is better than those of other SRGMs.
79|11||Identifying use cases in source code|Understanding the behavior of a software system is an important problem in software maintenance. As use cases have been accepted as an effective means for describing behavioral requirements for a software system, it should be helpful for maintainers to acquire the use case model from source code. In this paper, we propose a novel approach to identifying use cases in source code. The central idea of our approach is based on the observation that branch statements are a primary mechanism to separate one use case from another in source code. Following this idea, we design a static representation of software systems through incorporating branch information into the traditional call graph, which is named the Branch-Reserving Call Graph (BRCG). To effectively use this representation for use case identification, branches that do not serve as the separations of use cases should be pruned off in the BRCG. In this paper, we also provide a metric-based heuristic to automate this pruning. From the pruned BRCG, use cases, which may just undergo some modifications from human experts, can be generated through traversing the pruned BRCG. Overall, our method can effectively reduce the intensive human involvement in use case identification. We have also performed two case studies for this method on two shareware systems. The results from the case studies can confirm the effectiveness of our approach.
79|11||Shortcut method for reliability comparisons in RAID|Given that the reliability of each disk in a disk array during its useful lifetime is given as r = 1 − Ïµ with Ïµ âª 1, we show that the reliability of a RAID disk array tolerating all possible n − 1 disk failures can be specified as R ≈ 1 − anÏµn, where an is the smallest nonzero coefficient in the corresponding asymptotic expansion, e.g., for n-way replication R = 1 − Ïµn. We compare the reliability of several mirrored disk organizations, which provide tradeoffs between reliability and load balancedness (after disk failure) by comparing their a2 values, which can be obtained via a partial reliability analysis taking into account a few disk failures. We next use asymptotic expansions to compare the reliability of hierarchical RAID disk arrays, which combine replication and rotated parity disk arrays (RAID5 and RAID6). Finally, we argue that the mean time to data loss in systems with repair is related to the reliability without repair. As part of this discussion we show how to estimate the mean time to data loss in RAID5 and RAID6 disk arrays without resorting to transient analysis.
79|11||Packaging experiences for improving testing technique selection|One of the major problems within the software testing area is how to get a suitable set of cases to test a software system. A good set of test cases should assure maximum effectiveness with as few cases as possible. There are now numerous testing techniques available for generating test cases. However, many are never used, while just a few are used over and over again. Testers use little (if any) information about the available techniques, their usefulness and, generally, how suited they are to the project at hand, upon which to base their decision on which testing techniques to use. Using a characterisation schema is one solution for improving testing techniques selection. The schema helps to choose the best-suited techniques for a given project based on relevant information for the purpose of selection, assuring that testers’ selections are systematic. However, a characterisation schema is only part of the solution. We have found that a critical aspect for making a good selection is the availability of the necessary information and the sources of information that have to be consulted to access this information. Any organisation wishing to use characterisation schemas to select SE techniques needs to first address the issue of packaging the information that the schema contains.
79|11||A semantic-based P2P resource organization model R-Chord|
79|11||Automated software clustering: An insight using cluster labels|Clustering techniques have shown promising results for the architecture recovery and re-modularization of legacy software systems. Clusters that are obtained as a result of the clustering process may not be easy to interpret until they are assigned appropriate labels. Automatic labeling of clusters reduces the time required to understand them and can also be used to evaluate the effectiveness of the clustering process, if the assigned labels are meaningful and convey the purpose of each cluster effectively. In this paper, we present a labeling scheme based on identifiers of an entity. As the clustering process proceeds, keywords within identifiers are ranked using two ranking schemes: frequency and inverse frequency. We present experimental results to demonstrate the effectiveness of our labeling approach. A comparison between the ranking schemes reveals the inverse frequency scheme to form more meaningful labels, especially for small clusters. A comparison of clustering results of the complete and weighted combined algorithms based on labels of the clusters produced by them during clustering shows that the latter produces a more understandable cluster hierarchy with easily identifiable software sub-systems.
79|11||Priority assessment of software process requirements from multiple perspectives|The improvement of software processes involves the collection of software process requirements from various groups of stakeholders – each having their own perception of the process. Often stakeholders specify similar requirements in different ways. This results in the fact that different perceptions are related to each other. An effective technique is needed to incorporate the relationships between various stakeholders’ process requirements. This paper presents a Correlation-Based Priority Assessment framework (CBPA), which prioritizes software process requirements gathered from multiple stakeholders by incorporating inter-perspective relationships of requirements. This research uses a relationship matrix to analyze the impacts between requirements and to facilitate the integration. Higher stakeholder satisfaction is achieved by increasing the priorities of the software process requirements whose satisfaction can increase the satisfaction of other requirements from multiple perspectives.
79|12|http://www.sciencedirect.com/science/journal/01641212/79/12|Distributed dynamic slicing of Java programs|
79|12||Generalized comparison of graph-based ranking algorithms for publications and authors|Citation analysis helps in evaluating the impact of scientific collections (journals and conferences), publications and scholar authors. In this paper we examine known algorithms that are currently used for Link Analysis Ranking, and present their weaknesses over specific examples. We also introduce new alternative methods specifically designed for citation graphs. We use the SCEAS system as a base platform to introduce these new methods and perform a generalized comparison of all methods. We also introduce an aggregate function for the generation of author ranking based on publication ranking. Finally, we try to evaluate the rank results based on the prizes of ‘VLDB 10 Year Award’, ‘SIGMOD Test of Time Award’ and ‘SIGMOD E.F.Codd Innovations Award’.
79|12||Modelling and simulation of off-chip communication architectures for high-speed packet processors|In this work, we propose a visual, custom-designed, event-driven interconnect simulation framework to evaluate the performance of off-chip multi-processor/memory communications architectures for line cards. The simulator uses the state-of-the-art software design techniques to provide the user with a flexible, robust and comprehensive tool that can evaluate k-ary n-cube based network topologies under non-uniform traffic patterns. The simulator provides full control over essential network parameters and flow control mechanisms such as virtual channels and sub-channeling. We compare three low-dimensional k-ary n-cube based interconnects that can fit into the physical limitations on line cards, where each one of these interconnects has multiple processor-memory configurations. Performance results show that k-ary n-cube architectures perform better than existing interconnects, and they can sustain current line rates and higher. In addition, we provide performance tradeoffs between multiple flow control mechanisms and performance metrics such as throughput, routing accuracy, failure rate and interconnect utilization.
79|12||Semantic prefetching objects of slower web site pages|The structure of most web sites consists of a composition of web pages that require varying amounts of time to render. Typically, web pages with large amount content (text/images/code) require more time to render than web pages with a smaller aggregate content size. Most users expect this discrepancy among web page rendering times. In this paper, we will describe a semantic link prefetching technique that uses object bundling to expedite the rendering time of slower web pages, at the cost of extending the rendering speed of faster web pages within an established threshold value limit. Study results show that our approach enhances the overall rendering time of slower web pages with imperceptible time extension to other, faster rendering web pages.
79|12||Component adaptation for event-based application integration using active rules|
79|12||Computing frequent itemsets in parallel using partial support trees|A key process in association rules mining, which has attracted a lot of interest during the last decade, is the discovery of frequent sets of items in a database of transactions. A number of sequential algorithms have been proposed that accomplish this task. On the other hand, only few parallel algorithms have appeared in the literature. In this paper, we study the parallelization of the partial-support-tree approach Goulbourne et al. (2000). Numerical results show that this method is generally competitive, while it is particularly adequate for certain types of datasets.
79|12||A faster exact schedulability analysis for fixed-priority scheduling|Real-time scheduling for task sets has been studied, and the corresponding schedulability analysis has been developed. Due to the considerable overheads required to precisely analyze the schedulability of a task set (referred to as exact schedulability analysis), the trade-off between precision and efficiency is widely studied. Many efficient but imprecise (i.e., sufficient but not necessary) analyses are discussed in the literature. However, how to precisely and efficiently analyze the schedulability of task sets remains an important issue. The Audsley’s Algorithm was shown to be effective in exact schedulability analysis for task sets under rate-monotonic scheduling (one of the optimal fixed-priority scheduling algorithms). This paper focuses on reducing the runtime overhead of the Audsley’s Algorithm. By properly partitioning a task set into two subsets and differently treating these two subsets during each iteration, the number of iterations required for analyzing the schedulability of the task set can be significantly reduced. The capability of the proposed algorithm was evaluated and compared to related works, which revealed up to a 55.5% saving in the runtime overhead for the Audsley’s Algorithm when the system was under a heavy load.
79|12||A difference expansion oriented data hiding scheme for restoring the original host images|
79|12||An efficient subscription routing algorithm for scalable XML-based publish/subscribe systems|XML-based publish/subscribe systems have gained popularity as one of the newly emerging communication paradigms. As the number of clients has increased, the concept of the scalability has become the most important factor in the distributed information processing. In this paper, we propose an efficient subscription routing algorithm as a part of efforts to achieve scalability, with subscriptions especially expressed in XPath patterns. We first discuss that the homomorphism relationship among XPath patterns can be utilized for routing XPath patterns. Then, we develop a lattice data structure called the partially ordered set of XPath patterns which is abbreviated as POX and its corresponding algorithm for efficiently maintaining homomorphism relationships. Finally, we present performance evaluation results which validate our algorithm with respect to system performance and scalability.
79|12||Using playing cards to estimate interference in frequency-hopping spread spectrum radio networks|Many mobile ad hoc networks, wireless LANs, cellular systems, and military radio networks use frequency-hopping spread spectrum (FHSS) to provide multiple access to the channel as well as for anti-jamming, interference suppression, and mitigating other channel effects. Estimates of interference between two networks using the same set of frequencies is often assumed to be 1/N where N is the total number of available hopping frequencies. However, this simple model does not incorporate the partial spectral or partial temporal overlap that may occur between the hopping sequences or account for the fact that this overlap may be “stable” which will result in a significantly larger (or smaller) probability of interference.
79|12||Erratum to âOn the security of the Yen-Guoâs domino signal encryption algorithm (DSEA)â [The Journal of Systems and Software 79 (2006) 253â258]|
79|12||Understanding the past, improving the present, and mapping out the future of software architecture|
79|12||A survey of architecture design rationale|Many claims have been made about the consequences of not documenting design rationale. The general perception is that designers and architects usually do not fully understand the critical role of systematic use and capture of design rationale. However, there is to date little empirical evidence available on what design rationale mean to practitioners, how valuable they consider it, and how they use and document it during the design process. This paper reports a survey of practitioners to probe their perception of the value of design rationale and how they use and document the background knowledge related to their design decisions. Based on 81 valid responses, this study has discovered that practitioners recognize the importance of documenting design rationale and frequently use them to reason about their design choices. However, they have indicated barriers to the use and documentation of design rationale. Based on the findings, we conclude that further research is needed to develop methodology and tool support for design rationale capture and usage. Furthermore, we put forward some specific research questions about design rationale that could be further investigated to benefit industry practice.
79|2|http://www.sciencedirect.com/science/journal/01641212/79/2|An efficient and secure protocol for sensor network time synchronization|The emerging field of wireless sensor networks offers countless possibilities for achieving large scale monitoring in a distributed environment. These networks of resource constrained nodes require time synchronization for various distributed operations, but traditional protocols have significant overhead and rapidly deplete battery power. This paper addresses the challenges of sensor network engineering by proposing an efficient and secure time synchronization protocol named Tempest. The protocol reduces overhead and conserves power by using passive participation. It allows a node to infer the canonical time by simply overhearing the communication of its neighbors. It also authenticates protocol messages, and uses cross-layer control to manipulate counters in an encryption module to prevent attacks. Its implementation uses only minimal processing and negligible state, while an emphasis on reuse and modularity reduces code size. The protocol is implemented on embedded sensor node hardware, and is shown to substantially reduce overhead while maintaining the synchronization accuracy of recent related work. This reduction in overhead saves valuable energy, extending the lifetime of each sensor node and the lifetime of the sensor network itself.
79|2||Hotswapping Linux kernel modules|Contemporary operating system kernels are able to improve their functionality by installing kernel extensions at runtime. However, when an existing kernel extension needs to be upgraded, it must be completely removed before the new kernel extension is installed. Consequently, the new kernel extension needs to be run from the beginning, which also influences the applications using this kernel extension.
79|2||Analysis of Sun et al.âs linkability attack on some proxy blind signature schemes|The proxy blind signature scheme allows the designated proxy signer using the proxy secret key to generate a blind signature on behalf of the original signer. Tan et al. presented the DLP-based and ECDLP based blind signature schemes. Awasthi and Lal proposed a improved DLP-based scheme later. Recently, Sun et al. presented linkability attack on Tan et al.’s and Awasthi–Lal’s proxy blind signature schemes respectively. In this paper, we show that Sun et al.’s attack is failed and these schemes are still satisfy the unlinkability property.
79|2||Efficient evaluation of linear path expressions on large-scale heterogeneous XML documents using information retrieval techniques|We propose XIR-Linear, a method for efficiently evaluating linear path expressions (LPEs) on large-scale heterogeneous XML documents using information retrieval (IR) techniques. LPEs are the primary form of XPath queries, and their evaluation techniques have been researched actively. XPath queries in their general form are partial match queries, and these queries are particularly useful for searching documents of heterogeneous schemas. Thus, XIR-Linear is geared for partial match queries expressed as LPEs. XIR-Linear has its basis on existing methods using relational tables (e.g., XRel, XParent), and drastically improves their efficiency using the inverted index technique. Specifically, it indexes the labels in label paths (i.e., sequences of node labels) like keywords in texts, and finds the label paths matching the LPE far more efficiently than string match used in the existing methods. We demonstrate the efficiency of XIR-Linear by comparing it with XRel and XParent using XML documents crawled from the Internet. The results show that XIR-Linear outperforms XRel and XParent by an order of magnitude with the performance gap widening as database size grows.
79|2||Shape-based retrieval in time-series databases|The shape-based retrieval is defined as the operation that searches for the (sub)sequences whose shapes are similar to that of a query sequence regardless of their actual element values. In this paper, we propose a similarity model suitable for shape-based retrieval and present an indexing method for supporting the similarity model. The proposed similarity model enables to retrieve similar shapes accurately by providing the combination of multiple shape-preserving transformations such as normalization, moving average, and time warping. Our indexing method stores every distinct subsequence concisely into the disk-based suffix tree for efficient and adaptive query processing. We allow the user to dynamically choose a similarity model suitable for a given application. More specifically, we allow the user to determine the parameter p of the distance function Lp when submitting a query. The result of extensive experiments revealed that our approach not only successfully finds the subsequences whose shapes are similar to a query shape but also significantly outperforms the sequential scan method.
79|2||Odyssey-Search: A multi-agent system for component information search and retrieval|
79|2||Shortening retrieval sequences in browsing-based component retrieval using information entropy|Reuse repositories are an essential element in component-based software development (CBSD). Querying-based retrieval and browsing-based retrieval are two main retrieval mechanisms provided in real world reuse repositories, especially web-based repositories. Although browsing-based retrieval is superior to querying-based retrieval in some aspects, the tedious retrieval process is its main drawback, because the browsing-based component retrieval usually involves long retrieval sequences. In this paper, we propose a novel approach to shorten the retrieval sequences in browsing-based component retrieval using information entropy. The basic idea of our approach is to build a navigation model by ranking the features into a tree structure using the components’ indexing information. According to our experimental results on real data, our approach can effectively shorten the average length of retrieval sequences.
79|2||Polyhedral space generation and memory estimation from interface and memory models of real-time video systems|
79|2||Assessment of eco-security in the Knowledge Grid e-science environment|Ecological security is an important basis of the entire human security system, the cornerstone for human survival. Knowing the status of ecological security is crucial for making decisions to avoid ecological disaster. Existing research, both the basic research on ecological security mechanism and information service systems, is still primitive in their abilities to resolve eco-security problems. This paper investigates the eco-security impact factors and identifies eco-security types. Taking urban eco-security as an example, we develop an assessment method including the indicator system and assessment model, and construct an integrated assessment framework based on data integrity, security assessment and security management with the support of the Knowledge Grid e-science environment. The proposed assessment framework enables decision makers to better know the status of eco-security in making policies for achieving sustainability.
79|2||On the security of the YenâGuoâs domino signal encryption algorithm (DSEA)|Recently, a new domino signal encryption algorithm (DSEA) was proposed for digital signal transmission, especially for digital images and videos. This paper analyzes the security of DSEA, and points out the following weaknesses: (1) its security against the brute-force attack was overestimated; (2) it is not sufficiently secure against ciphertext-only attacks, and only one ciphertext is enough to get some information about the plaintext and to break the value of a sub-key; (3) it is insecure against known/chosen-plaintext attacks, in the sense that the secret key can be recovered from a number of continuous bytes of only one known/chosen plaintext and the corresponding ciphertext. Experimental results are given to show the performance of the proposed attacks, and some countermeasures are discussed to improve DSEA.
79|2||A scheduling framework for enterprise services|Application-level scheduling is a common process in the enterprise domain, but in order to be productive it should be time-effective, interoperable and reliable. In this paper we present the design and implementation aspects of a modular scheduling solution, which aims to fulfill the above-mentioned requirements. The proposed solution constitutes an integral part of an existing service provisioning platform targeted to the location based services (LBS) domain; however application to other service-oriented architectures is also possible. A careful design process is followed in order to identify the scheduler’s functional subsystems and the relationships between them. The implementation is based on the J2EE framework, thus guaranteeing independence from underlying technologies and applicability in enterprise environments. The high quality and scalability of the scheduler is also validated through an intensive performance analysis.
79|2||Comparing requirements analysis methods for developing reusable component libraries|Two approaches to requirements modelling are compared—the Domain Theory [Sutcliffe, A.G., 2002. The Domain Theory: Patterns for Knowledge and Software Reuse. Lawrence Erlbaum Associates, Mahwah, NJ.] and Problem Frames [Jackson, M., 2001. Problem Frames: Analysing and Structuring Software Development Problems, Pearson Education, Harlow.]—as a means of domain analysis for creating a reusable library of software components for constructing telemedicine applications. Experience of applying each approach as a domain analysis method to specify abstract components (object system models and Problem Frames) is reported. The two approaches produced detailed specifications although at different levels of abstraction: problems frames were better for monitoring, updating and data integrity requirements whereas the Domain Theory proved more useful for task support and user interface requirements. The lessons learned from using model-based approaches to requirements specification, and their merits for creating consistent specifications for reuse libraries, are discussed.
79|2||Call for Papers|
79|3|http://www.sciencedirect.com/science/journal/01641212/79/3|Software field failure rate prediction before software deployment|For both in-house development and outsourcing development environments, knowing the field failure rate of an integrated software system prior to field deployment provides guidance for better decision-makings in balancing reliability, time-to-market and development cost. This paper demonstrates a field failure rate prediction methodology that starts with analyzing system test data and field data (of previous releases or products) using software reliability growth models (SRGMs). A typical issue associated with predicting field failure rate based on test data is that potentially the test environment might not match exactly up the field environment. We discuss how to address the mismatch of the operational profiles of the test and filed environments. Two other practical issues in predicting field failure rates include that fault removals in the field are usually non-instantaneous and fixes of certain faults reported in the field can be deferred. Non-instantaneous fault removal and fault fix deferral becomes more realistic as the current software development environment shifts to a new trend of leveraging third-party, off-the-shelf, and semi-custom hardware and software and having the suppliers focus on development of highest-value applications and system integration. In such an environment, removing a fault might require a longer time and fix deferrals of certain faults becomes more possible in particular for the faults whose fixes will result in changes to other software components. In this paper, we illustrate how to incorporate these issues into field failure rate prediction. Confidence intervals of the predicted failure rate are also included to account for variations in the parameter estimation. Sensitivity analyses are conducted to estimate the uncertainties in the field failure rate prediction.
79|3||Towards human-centred design: Two case studies|Currently much system development is done using a technology-centred approach: automating the functions the technology is able to perform. Human-centred design including a cognitive work analysis seems a promising alternative for systems combining skilled humans and automated support. Carefully selected information technology can support this innovative system development approach.
79|3||Technology-driven business evolution|Innovating the business processes and supporting software systems of an enterprise requires their preliminary analysis and assessment. In particular, data concerning the performance and costs of activities and processes must be gathered in order to identify candidates for innovation. A critical point is finding a suitable presentation means for the gathered data in order to effectively support decision makers.
79|3||Automated impact analysis of UML models|The use of Unified Modeling Language (UML) analysis/design models on large projects leads to a large number of interdependent UML diagrams. As software systems evolve, UML diagrams undergo changes that address error corrections and changed requirements. Those changes can in turn lead to subsequent changes to other elements in the UML diagrams. Impact analysis is defined as the process of identifying the potential consequences (side-effects) of a change, and estimating what needs to be modified to accomplish that change. In this article, we propose a UML model-based approach to impact analysis that can be applied before implementation of changes, thus allowing early decision-making and change planning. We first verify that the UML diagrams in a design model are consistent. Then the changes between two different versions of UML models are automatically identified according to a change taxonomy. Next, model elements which are directly or indirectly impacted by the changes (i.e., may undergo changes) are determined using formally defined impact analysis rules (defined with the Object Constraint Language). A measure of distance between a changed element and potentially impacted elements is also proposed to prioritize the results of impact analysis according to their likelihood of occurrence. We also present a prototype tool that provides automated support for our impact analysis strategy, and two case studies that validate both the methodology and the tool. Empirical results confirm that distance helps determine the likelihood of change in the code.
79|3||An empirical study of process-related attributes in segmented software cost-estimation relationships|Parametric software effort estimation models consisting on a single mathematical relationship suffer from poor adjustment and predictive characteristics in cases in which the historical database considered contains data coming from projects of a heterogeneous nature. The segmentation of the input domain according to clusters obtained from the database of historical projects serves as a tool for more realistic models that use several local estimation relationships. Nonetheless, it may be hypothesized that using clustering algorithms without previous consideration of the influence of well-known project attributes misses the opportunity to obtain more realistic segments. In this paper, we describe the results of an empirical study using the ISBSG-8 database and the EM clustering algorithm that studies the influence of the consideration of two process-related attributes as drivers of the clustering process: the use of engineering methodologies and the use of CASE tools. The results provide evidence that such consideration conditions significantly the final model obtained, even though the resulting predictive quality is of a similar magnitude.
79|3||Validating strategic alignment of organizational IT requirements using goal modeling and problem diagrams|Ensuring that organizational IT is in alignment with and provides support for an organization’s business strategy is critical to business success. We present an integrated approach to requirements engineering for organizational IT. To help validate IT-business strategy alignment, we propose a single model according to Jackson’s problem diagram framework to encompass both business strategy and system requirements. We use an organizational strategy analysis technique to deconstruct business strategy. Strategy is then modeled using a goal-oriented requirements engineering notation; a framework for modeling an organization’s business strategy proposed by the Business Rules Group is used to construct the goal model. We use Jackson’s context diagrams to represent both business and IT domain context. Our approach is illustrated via application to an exemplar, constructed from a variety of sources in the literature describing Seven–Eleven Japan.
79|3||An XML environment for scenario based requirements engineering|The pervasive use of scenarios in the development of computer systems and software has motivated the need of formalisms and tools for the description and manipulation of scenarios. To this aim, we present SME (Scenario Model Environment), an XML based tool for scenario-based requirements engineering. The tool enables to exploit the emerging XML technologies in order to offer powerful ways to create, maintain, distribute and use scenarios. We have widely experimented SME through several case studies, ranging from small examples, like a library system, to complex industrial applications.
79|3||Categorical missing data imputation for software cost estimation by multinomial logistic regression|A common problem in software cost estimation is the manipulation of incomplete or missing data in databases used for the development of prediction models. In such cases, the most popular and simple method of handling missing data is to ignore either the projects or the attributes with missing observations. This technique causes the loss of valuable information and therefore may lead to inaccurate cost estimation models. On the other hand, there are various imputation methods used to estimate the missing values in a data set. These methods are applied mainly on numerical data and produce continuous estimates. However, it is well known that the majority of the cost data sets contain software projects with mostly categorical attributes with many missing values. It is therefore reasonable to use some estimating method producing categorical rather than continuous values. The purpose of this paper is to investigate the possibility of using such a method for estimating categorical missing values in software cost databases. Specifically, the method known as multinomial logistic regression (MLR) is suggested for imputation and is applied on projects of the ISBSG multi-organizational software database. Comparisons of MLR with other techniques for handling missing data, such as listwise deletion (LD), mean imputation (MI), expectation maximization (EM) and regression imputation (RI) under different patterns and percentages of missing data, show the high efficiency of the proposed method.
79|3||Investigating the impact of usability on software architecture through scenarios: A case study on Web systems|Usability has primarily been served by separating the user interface from the remainder of the application. However, several researchers have recently determined that there is a direct relationship between architectural decisions and usability requirements. This leads us to conclude that more attention should be devoted to usability-driven architectural analysis methods. We present a case study, which involves adapting an existing software architecture analysis method (SAAM) for the purpose of deriving the interdependencies between architectural characteristics and usability requirements. More specifically, we investigate the impact on the architecture of implementing usability requirement changes. Potential design solutions that accommodate the corresponding usability mechanisms into the Web software architecture are presented, along with the rationale for applying them and the process by which they are obtained. We conclude by recommending how usability issues can be dealt with proactively during the design of the architecture, and explain the need to integrate those usability requirements into a software engineering process.
79|3||Measuring the usability of software components|The last decade marked the first real attempt to turn software development into engineering through the concepts of Component-Based Software Development (CBSD) and Commercial Off-The-Shelf (COTS) components, with the goal of creating high-quality parts that could be joined together to form a functioning system. One of the most critical processes in CBSD is the selection of a set of software components from in-house or external repositories that fulfil some architectural and user-defined requirements. However, there is a lack of quality models and metrics that can help evaluate the quality characteristics of software components during this selection process. This paper presents a set of measures to assess the Usability of software components, and describes the method followed to obtain and validate them. Such a method can be re-used as a pattern for defining and validating measures for further quality characteristics.
79|3||Call for Papers|
79|4|http://www.sciencedirect.com/science/journal/01641212/79/4|Adaptive schemes for location update generation in execution location-dependent continuous queries|An important feature that is expected to be owned by today’s mobile computing systems is the ability of processing location-dependent continuous queries on moving objects. The result of a location-dependent query depends on the current location of the mobile client which has generated the query as well as the locations of the moving objects on which the query has been issued. When a location-dependent query is specified to be continuous, the result of the query can continuously change. In order to provide accurate and timely query results to a client, the location of the client as well as the locations of moving objects in the system has to be closely monitored. Most of the location generation methods proposed in the literature aim to optimize utilization of the limited wireless bandwidth. The issues of correctness and timeliness of query results reported to clients have been largely ignored. In this paper, we propose an adaptive monitoring method (AMM) and a deadline-driven method (DDM) for managing the locations of moving objects. The aim of our methods is to generate location updates with the consideration of maintaining the correctness of query evaluation results without increasing location update workload. Extensive simulation experiments have been conducted to investigate the performance of the proposed methods as compared to a well-known location update generation method, the plain dead-reckoning (pdr).
79|4||A method for analysing multimedia protocol performance|Performance analysis derived from a formal specification of a multimedia protocol provides important information on the performance characteristics of a multimedia system before implementation. The major problem of existing performance methodology is the lack of feasibility in time-dependent and functional behaviours, as well as statistical traffic characteristics for distributed multimedia systems. The internationally standardised Formal Description Technique, Estelle, does not have enough expressive power to specify the time-dependent performance characteristics of a multimedia system. To address this limitation of Estelle, we have developed a variant of Estelle, called Time-Estelle. In this paper, we describe a method for analysing the performance of a multimedia protocol specified in Time-Estelle. To aid the analysis, we introduce a traffic model called the Time-interval Performance Indices in order to capture the burstiness and time correlations of realistic multimedia traffic. Thus, performance parameters of multimedia systems, such as the long-term average rate and short-term peak rate allocation, and exploitation of statistical resources utilisation, can be analysed. To demonstrate the soundness of our method, we have applied it to the Resource ReSerVation Protocol and some results are included in this paper.
79|4||An object-oriented cryptosystem based on two-level reconfigurable computing architecture|
79|4||Efficient and adaptive discovery techniques of Web Services handling large data sets|Attempts have been made concerning the search and finding of a Web Service based on keywords and descriptions. However, no work has been done concerning the efficient selection of the appropriate Web Service instance in terms of quality and performance factors at the moment of the Web Service consumption attempt. Such factors may include execution time and response time. The proposed approach adaptively selects the most efficient WS among possible different alternatives with real-time, optimized and countable factors-parameters. Implementation issues and case study experiments are presented along with the corresponding results. Additionally, an optimal selection algorithm for series of Web Services requests is proposed. Finally, conclusions and future steps are discussed.
79|4||Cryptanalysis of a hybrid authentication protocol for large mobile networks|In this paper we analyse a hybrid authentication protocol due to Chien and Jan, designed for use in large mobile networks. The proposed protocol consists of two sub-protocols, namely the intra-domain authentication protocol and the inter-domain authentication protocol, which are used depending on whether the user and the request service are located in the same domain. We show that both sub-protocols suffer from a number of security vulnerabilities.
79|4||Improved self-certified group-oriented cryptosystem without a combiner|
79|4||Managing role relationships in an information flow control model|
79|4||Declarative programming of integrated peer-to-peer and Web based systems: the case of Prolog|Web and peer-to-peer systems have emerged as popular areas in distributed computing, and their integrated usage permits the benefits of both to be exploited. While much work in these areas have utilized the imperative programming paradigm, the need for declarative programming paradigms is increasingly being recognized, not only for the often cited advantages such as a higher level of abstraction and specialized language features, but also to tackle the querying and manipulation of knowledge and reasoning with semantics that will be the mainstay of the proposed next generation of the Web and peer-to-peer computing. This paper presents an approach towards integrative use of the Web and peer-to-peer systems within a declarative programming paradigm. We contend that logic programming can be useful in peer-to-peer computing, especially for querying and representing knowledge shared over peer networks, and for scripting applications that involve sophisticated search behaviour over peer networks. As an example of peer-to-peer querying expressed in a logic programming language, we propose a simple extension of Prolog, which we call LogicPeer, to enable goal evaluation over peers in a peer network. Using LogicPeer, we outline how a peer-to-peer version of a Yahoo-like system can be built and queried, and several other applications that involve decentralized knowledge sharing. We then show how LogicPeer can be used with LogicWeb, a Prolog extension to access Web pages, thereby integrating peer-to-peer querying and Web querying in a common declarative framework.
79|4||Patterns of conflict among software components|Integrating a system of disparate components to form a single application is still a daunting, high risk task, especially for components with heterogeneous communication expectations. It benefits integration to know explicitly the interoperability conflicts that can arise based on the current application design and the components being considered. However, there is no consistent representation of identified conflicts that also defines strategies to resolve them. Instead, developers use prior experience which may have consequential gaps. In this paper, we formulate a common representation for six major interoperability conflicts that arise through discrepancies or direct mismatches among architectural properties of interacting components. We use an Extender-Translator-Controller (ETC) classification scheme to describe the conflict resolution strategies. Detailing these associations as patterns provides insight into formulating an overall integration architecture design reflecting the solution strategy for developers to codify with respect to all components in the application. This approach moves conflict detection and resolution toward automation, immediately reducing the risk associated with the development of component based integrated systems.
79|4||Recovering architectural assumptions|During the creation of a software architecture, the architects and stakeholders take a lot of decisions. Many of these decisions can be directly related to functional or quality requirements. Some design decisions, though, are more or less arbitrarily made on the fly because of personal experience, domain knowledge, budget constraints, available expertise, and the like. These decisions, as well as the reasons for those decisions, are often not explicit upfront. They are implicit, and usually remain undocumented. We call them assumptions. There is no accepted way to document assumptions, and the relation between the software architecture and these assumptions easily gets lost, becomes hidden in the girders of the architecture. They are rediscovered at a later stage, when the software evolves and assumptions become invalid or new assumptions contradict earlier ones. In this paper, we develop a method to recover such assumptions from an existing software product. We illustrate the method by applying it to a commercial software product, and show how the results can help assess the evolutionary capabilities of its architecture.
79|5|http://www.sciencedirect.com/science/journal/01641212/79/5|Editorial|
79|5||A comparison of MC/DC, MUMCUT and several other coverage criteria for logical decisions|Many testing criteria, including condition coverage and decision coverage, are inadequate for software characterised by complex logical decisions, such as those in safety-critical software. In the past decade, more sophisticated testing criteria have been advocated. In particular, compliance of the MC/DC criterion has been mandated in the commercial aviation industry for the approval of airborne software. Recently, the MUMCUT criterion has been proposed as it guarantees the detection of certain faults in logical decisions in disjunctive normal form in which no variable is redundant. This paper compares MC/DC, MUMCUT and several other related coverage criteria for logical decisions by both formal and empirical analysis, focusing on the fault-detecting ability of test sets satisfying these testing criteria. Our results show that MC/DC test sets are effective, but they may still miss some faults that can almost always be detected by test sets satisfying the MUMCUT criterion.
79|5||On the statistical properties of testing effectiveness measures|We examine the statistical variability of three commonly used software testing effectiveness measures—the E-measure (expected number of failures detected), P-measure (probability of detecting at least one failure), and F-measure (number of tests required to detect the first failure). We show that for random testing with replacement, the F-measure will be distributed according to the geometric distribution. A simulation study examines the distribution of two adaptive random testing methods, to investigate how closely their sampling distributions approximate the geometric distribution. One key observation is that in the worst case scenario, the sampling distribution of adaptive random testing is very similar to that of random testing. The E-measure and P-measure have a normal sampling distribution, but high variability, meaning that large sample sizes are required to obtain results with satisfactorily narrow confidence intervals. We illustrate this with a simulation study for the P-measure. Our results have reinforced, from a perspective other than empirical analysis, that adaptive random testing is a more effective alternative to random testing, with reference to the F-measure. We consider the implications of our findings for previous studies conducted in the area, and make recommendations to future studies.
79|5||Automatic goal-oriented classification of failure behaviors for testing XML-based multimedia software applications: An experimental case study|When testing multimedia software applications, we need to overcome important issues such as the forbidding size of the input domains, great difficulties in repeating non-deterministic test outcomes, and the test oracle problem. A statistical testing methodology is proposed. It applies pattern classification techniques enhanced with the notion of test dimensions. Test dimensions are orthogonal properties of associated test cases. Temporal properties are being studied in the experimentation in this paper. For each test dimension, a pattern classifier is trained on the normal and abnormal behaviors. A type of failure is said to be classified if it is recognized by the classifier. Test cases can then be analyzed by the failure pattern recognizers. Experiments show that some test dimensions are more effective than others in failure identification and classification.
79|5||ASM-based design of data warehouses and on-line analytical processing systems|On-line analytical processing (OLAP) systems deal with analytical tasks in businesses. As these tasks do not depend on the latest updates by transactions, it is assumed that the data used in OLAP systems are kept in a data warehouse, which separates the input from operational databases from the outputs to dialogue interfaces for OLAP. Data Warehouses and OLAP systems are a promising area for the application of Abstract State Machines (ASMs). In this article a layered ground model specification for data warehouses and OLAP system is presented that is based explicitly on the fundamental idea of separating input from operational databases from output to OLAP systems. On this basis we start defining formal refinement rules for such systems. As these refinement rules are formally correct they enable a formal method for the high-quality design of data warehouses and OLAP systems that can be applied without knowing mathematical details of the ASM formalism. Furthermore, we discuss pragmatic guidelines for the application of such rules.
79|5||Semantic errors in SQL queries: A quite complete list|We investigate classes of SQL queries which are syntactically correct, but certainly not intended, no matter for which task the query was written. For instance, queries that are contradictory, i.e. always return the empty set, are obviously not intended. However, current database management systems (DBMS) execute such queries without any warning. In this paper, we give an extensive list of conditions that are strong indications of semantic errors. Of course, questions like the satisfiability are in general undecidable, but a significant subset of SQL queries can actually be checked. We believe that future DBMS will perform such checks and that the generated warnings will help to develop application programs with fewer bugs in less time.
79|5||Yet shorter warmup by combining no-state-loss and MRRL for sampled LRU cache simulation|Sampling is a well known technique for speeding up time-consuming architectural simulations. An important issue with sampling is the hardware state at the beginning of each sampling unit. This paper presents a highly accurate and highly efficient warmup method for sampled cache simulation by combining ‘no-state-loss (NSL)’ and ‘memory reference reuse latency (MRRL)’. Our combined warmup scheme MRRL–NSL achieves the same accuracy for sampled LRU cache simulation as MRRL with a two orders of magnitude shorter warmup. Compared to NSL, MRRL–NSL has a factor 2–6 shorter warmup while inducing a small absolute miss rate error of 0.1%.
79|5||Optimal resource allocation for cost and reliability of modular software systems in the testing phase|Reliability is one of the most important quality attributes of commercial software since it quantifies software failures during the development process. In order to increase the reliability, we should have a comprehensive test plan that ensures all requirements are included and tested. In practice, software testing must be completed within a limited time and project managers should know how to allocate the specified testing-resources among all the modules. In this paper, we present an optimal resource allocation problem in modular software systems during testing phase. The main purpose is to minimize the cost of software development when the fixed amount of testing-effort and a desired reliability objective are given. An elaborated optimization algorithm based on the Lagrange multiplier method is proposed and numerical examples are illustrated. Moreover, sensitivity analysis is also conducted. We analyze the sensitivity of parameters of proposed software reliability growth models and show the results in detail. The experimental results greatly help us to identify the contributions of each selected parameter and its weight. The proposed algorithm and method can facilitate the allocation of limited testing-resource efficiently and thus the desired reliability objective during software module testing can be better achieved.
79|5||Effective fair allocation using smart market label auction with CSLF and CR-CSFQ|Core-stateless mechanisms achieve better scalability by reducing the complexity of fair queuing, which usually needs to maintain states, manage buffers, and perform flow scheduling on a per-flow basis. In this paper, we propose two core-stateless fair bandwidth allocation schemes. Both schemes do not need to maintain per-flow state. Packet is labeled using smart market model according to the characteristics of the flow to which the packet belongs. No matter TCP or UDP flows can get their fair share rate by the proposed schemes. The first scheme is called core-stateless labeling fairness (CSLF). In CSLF scheme, packets are labeled only once at the entrance of the network and estimators of number of active flows based on Bloom filter are employed at the core routers. The estimation can be used to provide a fair rate to perform auction. Packets of a flow whose rate exceeds the estimated fair share rate are dropped at a congested router. The second scheme, called congestion-responsive-CSFQ (CR-CSFQ), is one extension from CSFQ. Congestion-responsive flows should get a different treatment from non-responsive flows at each core router. Fairness can be achieved by one extra smart market label in each packet. Through simulations, CSLF and CR-CSFQ are shown to achieve fair allocation effectively.
79|5||Adaptive data dissemination schemes for location-aware mobile services|Broadcasting is the natural method of propagating information in wireless links, which guarantees scalability in the case of bulk data transfers. It is particularly attractive for resource limited mobile clients in asymmetric communications. To facilitate power saving via wireless data broadcast, index information is typically broadcast along with the data. By first accessing the broadcast index, the mobile client is able to predict the arrival time of the desired data. However, it suffers from the drawback that the client has to wait and tune for an index segment, in order to conserve battery power consumption. In location-aware mobile services (LAMSs), it is important to reduce the query response time, since a late query response may contain out-of-date information. This paper proposes a new broadcast-based spatial query processing method, called BBS designed to support NN query processing. In the BBS, broadcasted data objects are sorted sequentially based on their locations, and the server broadcasts the location dependent data along with an index segment. In this method, since the data objects broadcasted by the server are sequentially ordered based on their location, it is not necessary for the client to wait for an index segment, if it has already identified the desired data items before the associated index segment has arrived. The performance of this scheme is investigated in relation to various environmental variables, such as the distributions of the data objects, the average speed of the clients and the size of the service area.
79|5||Comparison of performance of Web services, WS-Security, RMI, and RMIâSSL|This article analyses two most commonly used distributed models in Java: Web services and RMI (Remote Method Invocation). The paper focuses on regular (unsecured) as well as on secured variants, WS-Security and RMI–SSL. The most important functional differences are identified and the performance on two operating systems (Windows and Linux) is compared. Sources of performance differences related to the architecture and implementation are identified. The overheads related to the usage of security and the influences of JCE (Java Cryptography Extension) security providers on the performance of secured remote invocations are identified. Finally, the impact of distributed models on design and implementation of distributed applications is identified and guidelines for improving distributed application performance in design and implementation stage are provided. The paper contributes to the understanding of functional and performance related differences between Web services and RMI and their secure variants, WS-Security and RMI–SSL.
79|5||An agent based synchronization scheme for multimedia applications|
79|5||Performance evaluation of peer-to-peer Web caching systems|Peer-to-peer Web caching has attracted a great attention from the research community recently, and is one of the potential peer-to-peer applications. In this paper, we systematically examine the three orthogonal dimensions to design a peer-to-peer Web caching system, including the caching algorithm, the document lookup algorithm, and the peer granularity. In addition to the traditional URL-based caching algorithm, we also evaluate the content-based caching algorithm for both dynamic and static Web content. Four different document lookup algorithms are compared and evaluated in the context of four different peer granularities, i.e., host level, organization level, building level, and centralized. A detailed simulation, using the traces collected at a medium size education institution, is conducted for the purpose of performance evaluation. Finally, several implications derived from this analysis are also discussed.
79|5||Empirical assessment of using stereotypes to improve comprehension of UML models: A set of experiments|Stereotypes were introduced into the Unified Modeling Language to provide means of customizing this general purpose modeling language for its usage in specific application domains. The primary role of stereotypes is to brand an existing model element with specific semantics, but stereotypes can also be used to provide means of a secondary classification of modeling elements. This paper elaborates on the influence of stereotypes on the comprehension of models. The paper describes a set of controlled experiments performed in academia and industry which were aimed at evaluating the role of stereotypes in improving comprehension of UML models. The results of the experiments show that stereotypes play a significant role in the comprehension of models and the improvement achieved both by students and industry professionals.
79|5||Book review: H. Zhuge, The Knowledge Grid, World Scientific Publishing Co., Singapore, 2004|
79|6|http://www.sciencedirect.com/science/journal/01641212/79/6|Software reliability forecasting by support vector machines with simulated annealing algorithms|Support vector machines (SVMs) have been successfully employed to solve non-linear regression and time series problems. However, SVMs have rarely been applied to forecasting software reliability. This investigation elucidates the feasibility of the use of SVMs to forecast software reliability. Simulated annealing algorithms (SA) are used to select the parameters of an SVM model. Numerical examples taken from the existing literature are used to demonstrate the performance of software reliability forecasting. The experimental results reveal that the SVM model with simulated annealing algorithms (SVMSA) results in better predictions than the other methods. Hence, the proposed model is a valid and promising alternative for forecasting software reliability.
79|6||Constraint based structural testing criteria|
79|6||A petri-net-based synthesis methodology for use-case-driven system design|
79|6||An efficient interprocedural dynamic slicing method|
79|6||Maintainability of the kernels of open-source operating systems: A comparison of Linux with FreeBSD, NetBSD, and OpenBSD|We compared and contrasted the maintainability of four open-source operating systems: Linux, FreeBSD, NetBSD, and OpenBSD. We used our categorization of common coupling in kernel-based software to highlight future maintenance problems. An unsafe definition is a definition of a global variable that can affect a kernel module if that definition is changed. For each operating system we determined a number of measures, including the number of global variables, the number of instances of global variables in the kernel and overall, as well as the number of unsafe definitions in the kernel and overall. We also computed the value of each our measures per kernel KLOC and per KLOC overall. For every measure and every ratio, Linux compared unfavorably with FreeBSD, NetBSD, and OpenBSD. Accordingly, we are concerned about the future maintainability of Linux.
79|6||An assessment of systems and software engineering scholars and institutions (2000â2004)|This paper presents the findings of a five-year study of the top scholars and institutions in the Systems and Software Engineering field, as measured by the quantity of papers published in the journals of the field in 2000–2004. The top scholar is Hai Zhuge of the Chinese Academy of Sciences, and the top institution is Korea Advanced Institute of Science and Technology. This paper is part of an ongoing study, conducted annually, that identifies the top 15 scholars and institutions in the most recent five-year period.
79|6||Automatic generation of test cases from Boolean specifications using the MUMCUT strategy|A recent theoretical study has proved that the MUMCUT testing strategy (1) guarantees to detect seven types of fault in Boolean specifications in irredundant disjunctive normal form, and (2) requires only a subset of the test sets that satisfy the previously proposed MAX-A and MAX-B strategies, which can detect the same types of fault. This paper complements previous work by investigating various methods for the automatic generation of test cases to satisfy the MUMCUT strategy. We evaluate these methods by using several sets of Boolean expressions, including those derived from real airborne software systems. Our results indicate that the greedy CUN and UCN methods are clearly better than others in consistently producing significantly smaller test sets, whose sizes exhibit linear correlation with the length of the Boolean expressions in irredundant disjunctive normal form. This study provides empirical evidences that the MUMCUT strategy is indeed cost-effective for detecting the faults considered in this paper.
79|6||A field study of the Wheelâa usability engineering process model|Interactive system developers are increasingly including usability engineering as an integral part of interactive system development. With recognition of the importance of usability come attempts to structure this new aspect of overall system development, leading to a variety of processes and methodologies. Unfortunately, these processes often lack flexibility, customizability, completeness, and breadth of coverage. This paper describes our development of a meta process or process model that we call the Wheel. This innovative approach to creating and tailoring usability engineering processes addresses these shortcomings, and describes an evaluation of its application in a real-world commercial development environment.
79|6||QoS analysis for component-based embedded software: Model and methodology|Component-based development (CBD) techniques have been widely used to enhance the productivity and reduce the cost for software systems development. However, applying CBD techniques to embedded software development faces additional challenges. For embedded systems, it is crucial to consider the quality of service (QoS) attributes, such as timeliness, memory limitations, output precision, and battery constraints. Frequently, multiple components implementing the same functionality with different QoS properties (measurements in terms of QoS attributes) can be used to compose a system. Also, software components may have parameters that can be configured to satisfy different QoS requirements. Composition analysis, which is used to determine the most suitable component selections and parameter settings to best satisfy the system QoS requirement, is very important in embedded software development process. In this paper, we present a model and the methodologies to facilitate composition analysis. We define QoS requirements as constraints and objectives. Composition analysis is performed based on the QoS properties and requirements to find solutions (component selections and parameter settings) that can optimize the QoS objectives while satisfying the QoS constraints. We use a multi-objective concept to model the composition analysis problem and use an evolutionary algorithm to determine the Pareto-optimal solutions efficiently.
79|6||Feature analysis for architectural evaluation methods|
79|7|http://www.sciencedirect.com/science/journal/01641212/79/7|Guest Editorial|
79|7||Effective program debugging based on execution slices and inter-block data dependency|Localizing a fault in program debugging is a complex and time-consuming process. In this paper we present a novel approach using execution slices and inter-block data dependency to effectively identify the locations of program faults. An execution slice with respect to a given test case is the set of code executed by this test, and two blocks are data dependent if one block contains a definition that is used by another block or vice versa. Not only can our approach reduce the search domain for program debugging, but also prioritize suspicious locations in the reduced domain based on their likelihood of containing faults. More specifically, we use multiple execution slices to prioritize the likelihood of a piece of code containing a specific fault. In addition, the likelihood also depends on whether this piece of code is data dependent on other suspicious code. A debugging tool, DESiD, was developed to support our method. A case study that shows the effectiveness of our method in locating faults on an application developed for the European Space Agency is also reported.
79|7||Assigning tasks in a 24-h software development model|With the advent of globalization and the Internet, the concept of global software development is gaining ground. The global development model opens up the possibility of 24-h software development by effectively utilizing the time zone differences. To harness the potential of the 24-h software development model for reducing the overall development time, a key issue is the allocation of project tasks to the resources in the distributed team. In this paper, we examine this issue of task allocation in order to minimize the completion time of a project. We discuss a model for distributed team across time zones and propose a task allocation algorithm for the same. We apply the approach on tasks of a few synthetic projects and two real projects and show that there is a potential to reduce the project duration as well as improve the resource utilization through 24-h development.
79|7||An empirical study of groupware support for distributed software architecture evaluation process|Software architecture evaluation is an effective means of addressing quality related issues early in the software development lifecycle. Scenario-based approaches to evaluate architecture usually involve a large number of stakeholders, who need to be collocated for face-to-face evaluation meetings. Collocating a large number of stakeholders is an expensive and time-consuming exercise, which may prove to be a hurdle in the wide-spread adoption of disciplined architectural evaluation practices. Drawing upon the successful introduction of groupware applications to support geographically distributed teams in software inspection, and requirements engineering disciplines, we propose the concept of distributed architectural evaluation using Internet-based collaborative technologies. This paper presents a pilot study used to assess the viability of a larger experiment intended to investigate the feasibility of groupware support for distributed software architecture evaluation. In addition, the results of the pilot study provide some preliminary findings on the viability of groupware-supported software architectural evaluation process.
79|7||Goal and scenario based domain requirements analysis environment|Identifying and representing domain requirements among products in a product family are crucial activities for a successful software reuse. The domain requirements should be not only identified based on the business goal, which drives marketing plan, product plan, and differences among products, but also represented as familiar notations in order to support developing a particular product in the product family. Thus, our proposal is to identify the domain requirements through goals and scenarios, and represent them as variable use cases for a product family. Especially, for identification of the domain requirements, we propose four abstraction levels of requirements in a product family, and goal and scenario modeling. For representation of them, variable use case model is suggested, and also the use case transfer rules are proposed so as to bridge the gap between the identification and representation activity. The paper illustrates the application of the approach within a supporting tool using the HIS (Home Integration System) example.
79|7||MUDABlue: An automatic categorization system for Open Source repositories|Open Source communities typically use a software repository to archive various software projects with their source code, mailing list discussions, documentation, bug reports, and so forth. For example, SourceForge currently hosts over seventy thousand Open Source software systems. Because of the size of the rich information content, such repositories offer numerous opportunities for sharing information among projects. For example, one would like to know a set of projects that are related or similar to each other, so that the project groups can collaborate and share their work. With thousands of projects in typical repositories, however, manually locating related projects can be difficult. Hence, we propose MUDABlue, a tool that automatically categorizes software systems. MUDABlue has three major aspects: (1) it relies on no other information than the source code, (2) it determines category sets automatically, and (3) it allows a software system to be a member of multiple categories. MUDABlue has a Web interface to visualize determined categories, which eases browsing a software repository. We show the effectiveness of MUDABlue’s categorization capability by comparing its generated categories with that of some other existing research tools.
79|7||A unified model for the implementation of both ISO 9001:2000 and CMMI by ISO-certified organizations|ISO 9001 is a standard for quality management systems and CMMI is a model for process improvement. If an ISO-certified organization wishes to improve its processes continuously, implementing CMMI would be a good choice, as it provides more detailed practices for process improvement than the ISO standards. However, there are two issues that need to be resolved when an ISO-certified organization implements CMMI. First, it is not easy to identify any reusable parts of the ISO standards, and it would be advantageous to be able to reuse selected portions of the ISO standards during CMMI adoption in order to use existing resources to their best advantage. Second, it is difficult for an ISO-certified organization to implement CMMI in a straightforward, easy manner because of the differences in the language, structure, and details of the two sets of documents. In this paper, we present our unified model for ISO 9001:2000 and CMMI that resolves these two issues. Our model would be an extremely useful tool for ISO-certified organizations that plan to implement CMMI.
79|7||Accessing embedded program in untestable mobile environment: Experience of a trustworthiness approach|
79|7||Automatic generation of document semantics for the e-science Knowledge Grid|
79|7||An empirical study of XML data management in business information systems|Due the popularity of XML, an increasingly large amount of business transactions encoded in XML have been exchanged on-line. Currently there are two approaches to process and manage these XML data. One is to store them in relational databases, and the other is to store them in recently-developed native XML databases. There is no conclusion yet as to which approach suits better for contemporary business information systems. Also, the effectiveness of native XML databases used in daily operational systems has not been completely investigated. Therefore, in this paper, we provide: (1) a complete and systematic survey of the current development and challenges of processing XML data in relational and native XML databases, (2) a useful benchmark for IT practitioners who need to process XML data effectively, (3) experimental results and detailed analysis which reveal several interesting tips that can be helpful to XML document designers, and (4) a conclusion, based on the findings of using native XML databases in EDI processes, that it is practical to use native XML databases for daily operations although our experimental results showed that relational database systems outperform native XML databases in processing XML data.
79|7||Results from introducing component-level test automation and Test-Driven Development|For many software development organizations it is of crucial importance to reduce development costs while still maintaining high product quality. Since testing commonly constitutes a significant part of the development time, one way to increase efficiency is to find more faults early when they are cheaper to pinpoint and remove. This paper presents empirical results from introducing a concept for early fault detection. That is, an alternative approach to Test-Driven Development which was applied on a component level instead of on a class/method level. The selected method for evaluating the result of introducing the concept was based on an existing method for fault-based process assessment and was proven practically useful for evaluating fault reducing improvements. The evaluation was made on two industrial projects and on different features within a project that only implemented the concept partly. The evaluation result demonstrated improvements regarding decreased fault rates and Return On Investment (ROI), e.g. the total project cost became about 5–6% less already in the first two studied projects.
79|7||Methodology for customer relationship management|Customer relationship management (CRM) is a customer-focused business strategy that dynamically integrates sales, marketing and customer care service in order to create and add value for the company and its customers.
79|7||Prototyping mediators to project performance: Learning and interaction|
79|8|http://www.sciencedirect.com/science/journal/01641212/79/8|Location-aware multimedia proxy handoff over the IPv6 mobile network environment|In a server-proxy-client 3-tier networking architecture that is executed in the mobile network, proxies should be dynamically assigned to serve mobile hosts according to geographical dependency and the network situation. The goal of proxy handoff is to allow a mobile host still can receive packets from the corresponding server while its serving proxy is switched from the current one to another one. Once proxy handoff occurs, a proper proxy should be selected based on the load balance concern and the network situation concern. In this paper, a 3-tier Multimedia Mobile Transmission Platform, called MMTP, is proposed to solve the proxy handoff problem in the IPv6-based mobile network. A proxy handoff scheme based on the application-layer anycasting technique is proposed in MMTP. A proper proxy is selected from a number of candidate proxies by using the application-layer anycast. An experimental environment based on MMTP is also built to analyze the performance metrics of MMTP, including load balance among proxies and handoff latency.
79|8||A tunable hybrid memory allocator|Dynamic memory management can make up to 60% of total program execution time. Object oriented languages such as C++ can use 20 times more memory than procedural languages like C. Bad memory management causes severe waste of memory, several times that actually needed, in programs. It can also cause degradation in performance. Many widely used allocators waste memory and/or CPU time. Since computer memory is an expensive and limited resource its efficient utilization is necessary. There cannot exist a memory allocator that will deliver best performance and least memory consumption for all programs and therefore easily tunable allocators are required. General purpose allocators that come with operating systems give less than optimal performance or memory consumption. An allocator with a few tunable parameters can be tailored to a program’s needs for optimal performance and memory consumption. Our tunable hybrid allocator design shows 11–54% better performance and nearly equal memory consumption when compared to the well known Doug Lea allocator in seven benchmark programs.
79|8||On the efficiency and performance evaluation of the bandwidth clustering scheme for adaptive and reliable resource allocation|A variety of disciplines have recently advocated the use of self-adaptive and auto-configuration methods, including biodynamics, cybernetics and computer modeling. Of these methods, one which exhibits numerous powerful features that are desirable in communication systems is adaptive swarm based intelligence. Swarm-based self-configuration does not require the need of external help, supervision or control. The stochastic nature of random events adversely affects the complexity of optimization tasks. This work proposes a bandwidth clustering scheme suited for this network resource allocation problem. Bandwidth clustering is used in a swarm-based active network environment where active packets continuously communicate with active nodes by using the Split Agent Routing Technique (SART). This mechanism enables the adaptation of the system to new conditions (bandwidth reservation/capacity allocation), as well as the passing of additional information to neighboring nodes in which the information is held in transmitted packets. Paths are clustered with respect to different levels of bandwidth in order to enable capacity allocation and bandwidth reservation on demand, for any requested traffic. The performance, reliability and adaptivity degree of the proposed scheme is thoroughly examined for different traffic measures, as well as the corresponding QoS offered (in terms of the end-to-end delay, available bandwidth and probability of packet loss). This scheme offers a decentralized and non-path-oriented way to efficiently increase the overall network utilization, enabling an equal share of network resources at the same time.
79|8||On past-time indexing of moving objects|
79|8||The implementation of an extensible system for comparison and visualization of class ordering methodologies|In this paper we present the design and implementation of a system that exploits well-known design patterns to facilitate construction of an extensible system for comparison and visualization of ordering methodologies for class-based testing of C++ applications. Using our implementation, we present a comparative study and evaluation of two advanced ordering methodologies: the edge based approach by Briand et al., and the Class Ordering System (COS) introduced in this paper. We compare two variations of the approach by Briand and three variations of the COS system and draw conclusions about the number of edges removed, and therefore the number of stubs that must be constructed, using each approach. We also compare the run-time efficiency of each approach and raise some interesting questions about edge type considerations for removal in the presence of cycles in the ORD. Using the design patterns together with the dot tool from the Graphviz package, we incorporate visualization of the ORD and the edge removals into our system. We present details and graphical visualization of the edge removal process.
79|8||Timeslot-sharing algorithm with a dynamic grouping for WDM broadcast-and-select star networks|
79|8||Reversible index-domain information hiding scheme based on side-match vector quantization|Information hiding has become an interesting topic that receives more and more attention. Recently, many hiding techniques were proposed to directly conceal secret information on an image. However, for convenience and efficiency, images are usually stored and compressed by lossy or lossless compression mechanisms in indices format. The hidden information might be erased or cancelled when the stego image is lossy compressed. Hence, this paper proposes an information hiding scheme based on side-match vector quantization (SMVQ), which conceals the secret information on the indices of the SMVQ compressed images. The proposed scheme not only can embed information in the indices of the compressed image with low image distortion, but also can recover the original indices to reconstruct the SMVQ compressed image. As the experimental results indicated, the proposed scheme indeed outperforms other schemes in terms of image quality, hiding capacity, and compression rate.
79|8||An image size unconstrained ownership identification scheme for gray-level and color ownership statements based on sampling methods|
79|8||Formal specification applied to multiuser distributed services: Experiences in collaborative t-learning|The development of multiuser and distributed software systems faces the difficulty to program the applications correctly, in a way that guarantees the desired interaction among the users. Motivated by experiences with collaborative t-learning services (i.e. multiuser educational services over Interactive TV), this paper presents a solution to that problem, based on supplementing visual development with formal specification techniques. As a contribution to the development of interactive systems, a software process is introduced that helps defining the separate and the conjoint behavior of different users, incrementally and using highly-accessible formalisms.
79|8||Error resilient locally adaptive data compression|
79|8||An efficient key-management scheme for hierarchical access control based on elliptic curve cryptosystem|The elliptic curve cryptosystem is considered to be the strongest public-key cryptosystem known today and is preferred over the RSA cryptosystem because the key length for secure RSA has increased over recent years, and this has put a heavier processing load on its applications. An efficient key management and derivation scheme based on the elliptic curve cryptosystem is proposed in this paper to solve the hierarchical access control problem. Each class in the hierarchy is allowed to select its own secret key. The problem of efficiently adding or deleting classes can be solved without the necessity of regenerating keys for all the users in the hierarchy, as was the case in previous schemes. The scheme is shown much more efficiently and flexibly than the schemes proposed previously.
79|8||An efficient free-list submesh allocation scheme for two-dimensional mesh-connected multicomputers|This paper presents an efficient free-list submesh allocation scheme for two-dimensional mesh-connected systems. The scheme maintains an unordered list of free submeshes. For allocation, it selects the first free submesh that has at least the same size as the request, and when the selected submesh is larger than the request the part actually allocated is one that has the largest number of busy neighbors and mesh boundary processors. Whereas the best previously proposed schemes have time complexities that are either quadratic or cubic in the number of free or allocated submeshes, the time complexity of the proposed scheme is linear in the number of free submeshes. To evaluate the effectiveness of the scheme, its system performance in terms of parameters such as the average job turnaround time was compared to that of promising previously proposed schemes. Simulation results show that the scheme performs at least as well as these schemes, yet it has the lowest time complexity.
79|8||AVDL: A highly adaptable architecture view description language|Architectural views are rapidly gaining a momentum as a vehicle to document and analyze software architectures. Despite their popularity, there is no dedicated language flexible enough to support the specifications of an unbound variety of views including those preexisting and needing to be newly created on demand. In this paper, we propose a novel view description language intended for specifying any arbitrary views, using a uniform set of conventions for constructing views and how to use them. The highly adaptable nature of the new language results from its built-in mechanisms to define different types of views in a systematic and repeatable manner.
79|8||The essential components of software architecture design and analysis|Architecture analysis and design methods such as ATAM, QAW, ADD and CBAM have enjoyed modest success and are being adopted by many companies as part of their standard software development processes. They are used in the lifecycle, as a means of understanding business goals and stakeholders concerns, mapping these onto an architectural representation, and assessing the risks associated with this mapping. These methods have evolved a set of shared component techniques. In this paper we show how these techniques can be combined in countless ways to create needs-specific methods in an agile way. We demonstrate the generality of these techniques by describing a new architecture improvement method called APTIA (Analytic Principles and Tools for the Improvement of Architectures). APTIA almost entirely reuses pre-existing techniques but in a new combination, with new goals and results. We exemplify APTIA’s use in improving the architecture of a commercial information system.
79|9|http://www.sciencedirect.com/science/journal/01641212/79/9|Selected papers from the fourth Source Code Analysis and Manipulation (SCAM 2004) Workshop|
79|9||An empirical study into class testability|In this paper we investigate factors of the testability of object-oriented software systems. The starting point is given by a study of the literature to obtain both an initial model of testability and existing object-oriented metrics related to testability. Subsequently, these metrics are evaluated by means of five case studies of commercial and open source Java systems for which JUnit test cases exist. The goal of this paper is to identify and evaluate a set of metrics that can be used to assess the testability of the classes of a Java system.
79|9||Beyond source code: The importance of other artifacts in software development (a case study)|Current software systems contain increasingly more elements that have not usually been considered in software engineering research and studies. Source artifacts, understood as the source components needed to obtain a binary, ready to use version of a program, comprise in many systems more than just the elements written in a programming language (source code). Especially when we move apart from systems-programming and enter the realm of end-user applications, we find files for documentation, interface specifications, internationalization and localization modules and multimedia data. All of them are source artifacts in the sense that developers work directly with them, and that applications are built automatically using them as input. This paper discusses the differences and relationships between source code (usually written in a programming language) and these other files, by analyzing the KDE software versioning repository (with about 6,800,000 commits and 450,000 files). A comprehensive study of those files, and their evolution in time, is performed, looking for patterns and trying to infer from them the related behaviors of developers with different profiles, from where we conclude that studying those ‘other’ source artifacts can provide a great deal of insight on a software system.
79|9||Effects of context on program slicing|Whether context-sensitive program analysis is more effective than context-insensitive analysis is an ongoing discussion. There is evidence that context-sensitivity matters in complex analyses like pointer analysis or program slicing. Empirical data shows that context-sensitive program slicing is more precise and under some circumstances even faster than context-insensitive program slicing. This article will add to the discussion by examining if the context itself matters, i.e. if a given context leads to more precise slices for that context. Based on some experiments, we will show that this is strongly dependent on the structure of the programs.
79|9||Program restructuring using clustering techniques|Program restructuring is a key method for improving the quality of ill-structured programs, thereby increasing the understandability and reducing the maintenance cost. It is a challenging task and a great deal of research is still ongoing. This paper presents an approach to program restructuring inside of a function based on clustering techniques with cohesion as the major concern. Clustering has been widely used to group related entities together. The approach focuses on automated support for identifying ill-structured or low-cohesive functions and providing heuristic advice in both the development and evolution phases. A new similarity measure is defined and studied intensively specifically from the function perspective. A comparative study on three different hierarchical agglomerative clustering algorithms is also conducted. The best algorithm is applied to restructuring of functions of a real industrial system. The empirical observations show that the heuristic advice provided by the approach can help software designers make better decision of why and how to restructure a program. Specific source code level software metrics are presented to demonstrate the value of the approach.
79|9||Efficient reversal of the intraprocedural flow of control in adjoint computations|
79|9||How agile are industrial software development practices?|Representatives from the agile development movement claim that agile ways of developing software are more fitting to what is actually needed in industrial software development. If this is so, successful industrial software development should already exhibit agile characteristics. This article therefore aims to examine whether that is the case. It presents an analysis of interviews with software developers from five different companies. We asked about concrete projects, both about the project models and the methods used, but also about the real situation in their daily work. Based on the interviews, we describe and then analyze their development practices. The analysis shows that the software providers we interviewed have more agile practices than they might themselves be aware of. However, plans and more formal development models also are well established. The conclusions answer the question posed in the title: It all depends! It depends on which of the different principles you take to judge agility. And it depends on the characteristics not only of the company but also of the individual project.
79|9||An integration of fault detection and correction processes in software reliability analysis|Software reliability is defined as the probability of failure-free software operation for a specified period of time in a specified environment and is widely recognized as one of the most significant aspects of software quality. Over the past 30 years, many software reliability growth models (SRGMs) have been proposed and they can greatly help us to estimate some important measures such as the mean time to failure, the number of remaining faults, defect levels, and the failure intensity, etc. Besides, SRGMs can also help to determine person power needed to support the desired reliability requirements. However, from our studies, most of SRGMs only focus on describing the behavior of fault detection process and assume that faults are fixed immediately upon detection. In fact, this assumption may not be realistic. Thus, in this paper, we will propose a general framework for modeling the software fault detection and correction processes. We will also show that the proposed approaches cover a number of well-known SRGMs. Two numerical examples based on two real software failure data sets are presented and discussed in detail.
79|9||Automatic generation of assumptions for modular verification of software specifications|Model checking is a powerful automated technique mainly used for the verification of properties of reactive systems. In practice, model checkers are limited due to the state explosion problem. Modular verification based on the assume-guarantee paradigm mitigates this problem using a “divide and conquer” technique. Unfortunately, this approach is not automated, for the reason that the user must specify the environment model. In this paper, a novel technique is presented for automatically generating component assumptions based on the behaviour of the environment (the remainder of components of the systems). In the first phase, the environment of the component is computed using state space exploration techniques, and then the assumptions are generated as association rules of the component environment interface. This approach presents a number of advantages. Firstly, user assistance to specify assumptions is not necessary and assumption discharge is avoided. Secondly, the component assumptions are more restrictive and real, and therefore reduce the resources needed by the model checker. The technique is applied to the specification of a steam boiler system.
79|9||A formal representation of functional size measurement methods|Estimating software size is a difficult task that requires a methodological approach. Many different methods that exist today use distinct abstractions to depict a software system. The gap between abstractions becomes even greater with object-oriented artifacts developed in unified modeling language (UML). In this paper, a formal foundation for the representation of functional size measurement (FSM) methods is presented. The generalized abstraction of the software system (GASS) is then used to formalize different functional measurement methods, namely the FPA, MK II FPA and COSMIC-FFP. The same model is also used for object-oriented projects where UML artifacts are mapped into the GASS form. The algorithms in symbolic code for those UML diagrams that are crucial for size estimation are also given. The mappings defined in this paper enable diverse FSM methods to be supported in estimation tools, the automation of counting steps and a higher-level of independence from the FSM method, since the software abstraction is written in a generalized form. Both improvements are crucial for the practical use of FSM methods.
80|1|http://www.sciencedirect.com/science/journal/01641212/80/1|Modeling the evolution of operating systems: An empirical study|In this paper, we report on an empirical experiment where we observe, record and analyze the evolution of selected operating systems over the past decades, and derive a statistical model that captures relevant evolutionary laws. This model is derived by quantifying relevant attributes of operating systems, including intrinsic technical factors and time-dependent environmental factors. We use this model to understand past evolution and learn to predict future evolution, not only of individual operating systems, but also of operating systems features. We combine the insights gained from this study with insights gained from other similar empirical experiments to attempt to derive evolutionary laws for software technology trends.
80|1||Evaluating software project portfolio risks|As in any other business, software development organizations try to maximize their profits and minimize their risks. The risks represent uncertain events and conditions that may prevent enterprises from attaining their goals, turning risk management into a major concern, not only for project managers but also for executive officers involved with strategic objectives. In this sense, economical concepts can greatly support Software Engineers in the effort to better quantify the uncertainties of either a single project or even a project portfolio.
80|1||The maintenance and evolution of resource-constrained embedded systems created using design patterns|Most previous work on pattern-based software development has focused on the process of system creation rather than on the post-creation project phases (such as maintenance and evolution). In the study reported in this paper, we present the results from a short series of empirical studies in which we examined techniques for exchanging patterns used in an embedded design after the project had been completed. When exchanging patterns at this time, our aim was to identify the implementation of the pattern of interest in the system code and then substitute a suitable version of the replacement pattern. Findings are presented both from two small test projects, and from a more realistic case study. The results obtained suggest that this approach has considerable potential.
80|1||An empirical analysis of risk components and performance on software projects|Risk management and performance enhancement have always been the focus of software project management studies. The present paper shows the findings from an empirical study based on 115 software projects on analyzing the probability of occurrence and impact of the six dimensions comprising 27 software risks on project performance. The MANOVA analysis revealed that the probability of occurrence and composite impact have significant differences on six risk dimensions. Moreover, it indicated that no association between the probability of occurrence and composite impact among the six risk dimensions exists and hence, it is a crucial consideration for project managers when deciding the suitable risk management strategy. A pattern analysis of risks across high, medium, and low-performance software projects also showed that (1) the “requirement” risk dimension is the primary area among the six risk dimensions regardless of whether the project performance belongs to high, medium, or low; (2) for medium-performance software projects, project managers, aside from giving importance to “requirement risk”, must also continually monitor and control the “planning and control” and the “project complexity” risks so that the project performance can be improved; and, (3) improper management of the “team”, “requirement”, and “planning and control” risks are the primary factors contributing to a low-performance project.
80|1||A new imputation method for small software project data sets|Effort prediction is a very important issue for software project management. Historical project data sets are frequently used to support such prediction. But missing data are often contained in these data sets and this makes prediction more difficult. One common practice is to ignore the cases with missing data, but this makes the originally small software project database even smaller and can further decrease the accuracy of prediction. The alternative is missing data imputation. There are many imputation methods. Software data sets are frequently characterised by their small size but unfortunately sophisticated imputation methods prefer larger data sets. For this reason we explore using simple methods to impute missing data in small project effort data sets. We propose a class mean imputation (CMI) method based on the k-NN hot deck imputation method (MINI) to impute both continuous and nominal missing data in small data sets. We use an incremental approach to increase the variance of population. To evaluate MINI (and k-NN and CMI methods as benchmarks) we use data sets with 50 cases and 100 cases sampled from a larger industrial data set with 10%, 15%, 20% and 30% missing data percentages respectively. We also simulate Missing Completely at Random (MCAR) and Missing at Random (MAR) missingness mechanisms. The results suggest that the MINI method outperforms both CMI and the k-NN methods. We conclude that this new imputation technique can be used to impute missing values in small data sets.
80|1||Identifying and characterizing change-prone classes in two large-scale open-source products|Developing and maintaining open-source software has become an important source of profit for many companies. Change-prone classes in open-source products increase project costs by requiring developers to spend effort and time. Identifying and characterizing change-prone classes can enable developers to focus timely preventive actions, for example, peer-reviews and inspections, on the classes with similar characteristics in the future releases or products. In this study, we collected a set of static metrics and change data at class level from two open-source projects, KOffice and Mozilla. Using these data, we first tested and validated Pareto’s Law which implies that a great majority (around 80%) of change is rooted in a small proportion (around 20%) of classes. Then, we identified and characterized the change-prone classes in the two products by producing tree-based models. In addition, using tree-based models, we suggested a prioritization strategy to use project resources for focused preventive actions in an efficient manner. Our empirical results showed that this strategy was effective for prioritization purposes. This study should provide useful guidance to practitioners involved in development and maintenance of large-scale open-source products.
80|1||Experimental use of code delta, code churn, and rate of change to understand software product line evolution|This research is a longitudinal study of change processes. It links changes in the product line architecture of a large telecommunications equipment supplier with the company’s customers, inner context, and eight line card products over six-year period. There are three important time related constructs in this study: the time it takes to develop a new product line release; the frequency in which a metric is collected; and the frequency at which financial results and metrics related to the customer layer are collected and made available. Data collection has been organized by product release. The original goal of this research is to study the economic impact of market reposition on the product line and identify metrics that can be used to records changes in product line. We later look at the product line evolution vis-à-vis the changes in the products that form the product line. Our results show that there is no relationship between the size of the code added to the product line and the number of designers required to develop and test it; and there is a positive relationship between designer turnover and impact of change.
80|1||Interprocedural side-effect analysis for incomplete object-oriented software modules|We introduce a new approach to computing interprocedural modification side effects for part of an object-oriented program (e.g., components, libraries or client modules). Our approach consists of first performing a whole-program points-to analysis to such an incomplete program and then applying a so-called mutability analysis (MA) to determine which objects in the program are mutable by unknown code and which references and call sites in the program are complete (since their points-to sets and target methods are statically resolvable). Based on these results, we present a new MA-based interprocedural side-effect analysis for computing the modification side-effects for an incomplete program. Our experimental results show that our mutability analysis enables a variety of pure methods to be detected, yielding the purity information useful in program understanding and debugging. In addition, our MA-based side-effect analysis enables more redundant loads to be removed than a recent TBAA-based PRE algorithm guided by type-based alias analysis (TBAA). Our approach is simple since it is flow-insensitive and achieves these improvements at small costs.
80|1||A general model of software architecture design derived from five industrial approaches|We compare five industrial software architecture design methods and we extract from their commonalities a general software architecture design approach. Using this general approach, we compare across the five methods the artifacts and activities they use or recommend, and we pinpoint similarities and differences. Once we get beyond the great variance in terminology and description, we find that the five approaches have a lot in common and match more or less the “ideal” pattern we introduced. From the ideal pattern we derive an evaluation grid that can be used for further method comparisons.
80|1||Using Bayesian belief networks for change impact analysis in architecture design|Research into design rationale in the past has focused on argumentation-based design deliberations. These approaches cannot be used to support change impact analysis effectively because the dependency between design elements and decisions are not well represented and cannot be quantified. Without such knowledge, designers and architects cannot easily assess how changing requirements and design decisions may affect the system. In this article, we introduce the Architecture Rationale and Element Linkage (AREL) model to represent the causal relationships between architecture design elements and decisions. We apply Bayesian Belief Networks (BBN) to AREL, to capture the probabilistic causal relationships between design elements and decisions. We employ three different BBN-based reasoning methods to analyse design change impact: predictive reasoning, diagnostic reasoning and combined reasoning. We illustrate the application of the BBN modelling and change impact analysis methods by using a partial design of a real-world cheque image processing system. To support its implementation, we have developed a practical, integrated tool set for the architects to use.
80|10|http://www.sciencedirect.com/science/journal/01641212/80/10|Methodology of security engineering for industrial security management systems|
80|10||IT compliance of industrial information systems: Technology management and industrial engineering perspective|IT compliance is one of the hottest issues in IT and technology management fields. The purpose of this paper is to provide a common framework for IT compliance. First, a review on the compliance age is provided. Second, the characteristics of business records communicated via industrial information systems are described shortly. Finally, an IT compliance framework is suggested. The framework of this paper is not proven by industrial studies because the spread of IT compliance and the industrial response to it are still under progress. Future works should prove the practical value of this framework and should include technical studies.
80|10||Archetypal behavior in computer security|The purpose of this study is to understand observed behavior and to diagnose and find solutions to issues encountered in organizational computer security using a systemic approach, namely system archetypes. In this paper we show the feasibility of archetypes application and the benefits of simulation. We developed a model and simulation of some aspects of security based on system dynamics principles. The system dynamics simulation model can be used in support of decision-making, training, and teaching regarding the mitigation of computer security risks. In this paper, we combine two archetypes and show the computer security relevance of such combinations. Presented are instances of the archetypes “Escalation”, in which an organization must continuously increase its efforts to counter additional attacker effort; and “Limits to Growth”, in which the gains from an organization’s security efforts plateau or decline due to its limited capacity for security-related tasks. We describe a scenario where these archetypes (individually and combined) can help in diagnosis and understanding, and present simulation of “what-if” scenarios suggesting how an organization might remedy these problems and maximize its gains from security efforts.
80|10||Managing information security in a business network of machinery maintenance services business â Enterprise architecture as a coordination tool|Today, technologies enable easy access to information across organizational boundaries, also to systems of partners in business networks. This raises, however, several complex research questions on privacy, information security and trust. The study reported here provides motivation and a roadmap for approaching integrated security management solutions in a business network of partners with heterogeneous information and communication technologies (ICT): Systems, platforms, infrastructures as well as security policies. Enterprise architecture (EA) is proposed as a means for comprehensive and coordinated planning and management of corporate ICT and the security infrastructure. The EA approach is proposed as a pre-requisite for transparent and secure inter-organizational information exchange and business process support crossing corporate boundaries. This study provides an example of security architecture planning based on EA, which aligns the development of technological solutions with the business goals. The EA approach combines the planning of business and ICT developments. The alignment provides arguments for cohesive identity and access management (IAM) in a business network. A case study with Metso Paper, Inc., the leading manufacturer of paper machinery and related services, exemplifies the EA-based security architecture planning and specification.
80|10||A software-based trust framework for distributed industrial management systems|One of the major problems in industrial security management is that most organizations or enterprises do not provide adequate guidelines or well-defined policy with respect to trust management, and trust is still an afterthought in most security engineering projects. With the increase of handheld devices, managers of business organizations tend to use handheld devices to access the information systems. However, the connection or access to an information system requires appropriate level of trust. In this paper, we present a flexible, manageable, and configurable software-based trust framework for the handheld devices of mangers to access distributed information systems. The presented framework minimizes the effects of malicious recommendations related to the trust from other devices or infrastructures. The framework allows managers to customize trust-related settings depending on network environments in an effort to create a more secure and functional network. To cope with the organizational structure of a large enterprise, within this framework, handheld devices of managers are broken down into different categories based upon available resources and desired security functionalities. The framework is implemented and applied to build a number of trust sensitive applications such as health care.
80|10||Common defects in information security management system of Korean companies|To reduce the possible trials and errors while promoting the establishment and certification of the information security management system (ISMS) by enterprises is the purpose of this paper. To satisfy this purpose, this study presents the defects by item found during the certification process of the ISMS of a number of enterprises by government certification agency in Korea. As a result, by analyzing the derived defects, this paper has outlined the issues to be attended to among enterprises at each stage of the establishment of the ISMS. Furthermore, this study presents a reference model for conducting a self assessment, so that companies may be able to self verify the completeness of their establishment of the ISMS. The case study is also provided to prove the practical value of this study.
80|10||Active and passive techniques for group size estimation in large-scale and dynamic distributed systems|This paper presents two solutions to a distributed statistic collection problem, called Group Size Estimation. These algorithms are intended for large-scale and dynamic distributed systems such as Grids, peer-to-peer overlays, etc. Each algorithm estimates (both in a one-shot and continuous manner) the number of non-faulty processes present in the global group. The first active scheme samples receipt times of gossip messages, while the second passive scheme calculates the density of process identifiers when hashed to a real interval. Our analysis, trace-driven simulation and deployment on a 33-node Linux cluster study and compare the latencies, scalability, and accuracy of these schemes.
80|10||Collocation optimizations in an aspect-oriented middleware system|In distributed object-oriented systems, there are situations where client and server objects are deployed in the same address space. In such scenarios, it is possible to dispatch remote calls without having to transverse the infrastructure provided by the underlying communication middleware system and thus without incurring the overhead of using a networking loopback interface. Such optimizations are called collocation optimizations. In this paper we describe an implementation of collocation that is centered on aspect-oriented programming abstractions. This implementation provides high degrees of modularization, configurability and adaptability than current object-oriented support to collocation. The paper also presents results about the performance gains derived from the optimization proposed.
80|10||A hierarchical key management scheme for secure group communications in mobile ad hoc networks|A mobile ad hoc network (MANET) is a kind of wireless communication infrastructure that does not have base stations or routers. Each node acts as a router and is responsible for dynamically discovering other nodes it can directly communicate with. However, when a message without encryption is sent out through a general tunnel, it may be maliciously attacked. In this paper, we propose a hierarchical key management scheme (HKMS) for secure group communications in MANETs. For the sake of security, we encrypt a packet twice. Due to the frequent changes of the topology of a MANET, we also discuss group maintenance in this paper. Finally, we conducted the security and performance analysis to compare the proposed scheme with Tseng et al.’s [Tseng, Y.-M., Yang, C.-C., Liao, D.-R., 2007. A secure group communication protocol for ad hoc wireless networks. In: Advances in Wireless Ad Hoc and Sensor Networks and Mobile Computing. Book Series Signal and Communication Technology. Springer] and Steiner et al.’s [Steiner, M., Tsudik, G., Waidner, M., 1998. CLIQUES: a new approach to group key agreement. In: Proceedings of the 18th IEEE International Conference on Distributed Computing System. Amsterdam, Netherlands, pp. 380–387] schemes.
80|10||Energy efficient strategies for object tracking in sensor networks: A data mining approach|In recent years, a number of studies have been done on object tracking sensor networks (OTSNs) due to the wide applications. One important research issue in OTSNs is the energy saving strategy in considering the limited power of sensor nodes. The past studies on energy saving in OTSNs considered the object’s movement behavior as randomness. In some real applications, however, the object movement behavior is often based on certain underlying events instead of randomness completely. In this paper, we propose a novel data mining algorithm named TMP-Mine with a special data structure named TMP-Tree for efficiently discovering the temporal movement patterns of objects in sensor networks. To our best knowledge, this is the first work on mining the movement patterns associated with time intervals in OTSNs. Moreover, we propose novel location prediction strategies that utilize the discovered temporal movement patterns so as to reduce the prediction errors for energy savings. Through empirical evaluation on various simulation conditions and real dataset, TMP-Mine and the proposed prediction strategies are shown to deliver excellent performance in terms of scalability, accuracy and energy efficiency.
80|10||Building intrusion pattern miner for Snort network intrusion detection system|In this paper, we enhance the functionalities of Snort network-based intrusion detection system to automatically generate patterns of misuse from attack data, and the ability of detecting sequential intrusion behaviors. To that, we implement an intrusion pattern discovery module which applies data mining technique to extract single intrusion patterns and sequential intrusion patterns from a collection of attack packets, and then converts the patterns to Snort detection rules for on-line intrusion detection. In order to detect sequential intrusion behavior, the Snort detection engine is accompanied with our intrusion behavior detection engine. Intrusion behavior detection engine will create an alert when a series of incoming packets match the signatures representing sequential intrusion scenarios.
80|10||Discrete-time performance analysis of a congestion control mechanism based on RED under multi-class bursty and correlated traffic|Internet traffic congestion control using queue thresholds is a well known and effective mechanism. This motivates the stochastic analysis of a discrete-time queueing systems for the performance evaluation of the active queue management (AQM) based congestion control mechanism called Random Early Detection (RED) with bursty and correlated traffic using a two-state Markov-Modulated Bernoulli arrival process (MMBP-2) as the traffic source. A two-dimensional discrete-time Markov chain is introduced to model the RED mechanism for two traffic classes where each dimension corresponds to a traffic class with its own parameters. This mechanism takes into account the reduction of incoming traffic arrival rate due to packets dropped probabilistically with the drop probability increasing linearly with system contents. The stochastic analysis of the queue considered could be of interest for the performance evaluation of the RED mechanism for the multi-class traffic with short range dependent (SRD) traffic characteristics. The performance metrics including mean system occupancy, mean packet delay, packet loss probability and system throughput are computed from the analytical model for a dropping policy which is a function of the thresholds and maximum drop probability. Typical numerical results are included to illustrate the credibility of the proposed mechanism in the context of external bursty and correlated traffic. These results clearly demonstrate how different threshold settings can provide different trade-offs between loss probability and delay to suit different service requirements. The effects on various performance measures of changes in the input parameters and of burstiness and correlations exhibited by the arrival process are also presented. The model would be applicable to high-speed networks which use slotted protocols.
80|10||SQUIRE: Sequential pattern mining with quantities|Discovering sequential patterns is an important problem for many applications. Existing algorithms find qualitative sequential patterns in the sense that only items are included in the patterns. However, for many applications, such as business and scientific applications, quantitative attributes are often recorded in the data, which are ignored by existing algorithms. Quantity information included in the mined sequential patterns can provide useful insight to the users.
80|10||Efficiency study of the information flow mechanism enabling interworking of heterogeneous wireless systems|The efficient co-operation of heterogeneous wireless technologies is directly relevant to the 4G wireless systems. Key building block of such an environment is the signaling protocol that allows the bidirectional flow of information between network and user devices. This provides the motivation behind this paper that initially presents key aspects of such a signalling protocol in a ‘Beyond 3G system’ (B3G) or otherwise known as a composite radio network. On the basis of the described architecture of the B3G system, simple simulation models for capturing the dynamics of the message exchanges are discussed. The simulation study allows the derivation of both qualitative and quantitative results, that provide insight on key characteristics of the composite environment.
80|11|http://www.sciencedirect.com/science/journal/01641212/80/11|Composing pattern-based components and verifying correctness|Designing large software systems out of reusable components has become increasingly popular. Although liberal composition of reusable components saves time and expense, many experiments indicate that people will pay for this (liberal composition) sooner or later, sometimes paying even a higher price than the savings obtained from reusing components. Thus, we advocate that more rigorous analysis methods to check the correctness of component composition would allow combination problems to be detected early in the development process so that people can save the considerable effort of fixing errors downstream. In this paper we describe a rigorous method for component composition that can be used to solve combination and integration problems at the (architectural) design phase of the software development lifecycle. In addition, we introduce the notion of composition pattern in order to promote the reuse of composition solutions to solve routine component composition problems. Once a composition pattern is proven correct, its instances can be used in a particular application without further proof. In this way, our proposed method involves reusing compositions as well as reusing components. We illustrate our approach through an example related to the composition of design patterns as design components. Structural and behavioral correctness proofs about the composition of design patterns are provided. Case studies are also presented to show the applications of the composition patterns.
80|11||Inconsistency of expert judgment-based estimates of software development effort|Expert judgment-based effort estimation of software development work is partly based on non-mechanical and unconscious processes. For this reason, a certain degree of intra-person inconsistency is expected, i.e., the same information presented to the same individual at different occasions sometimes lead to different effort estimates. In this paper, we report from an experiment where seven experienced software professionals estimated the same sixty software development tasks over a period of three months. Six of the sixty tasks were estimated twice. We found a high degree of inconsistency in the software professionals’ effort estimates. The mean difference of the effort estimates of the same task by the same estimator was as much as 71%. The correlation between the corresponding estimates was 0.7. Highly inconsistent effort estimates will, on average, be inaccurate and difficult to learn from. It is consequently important to focus estimation process improvement on consistency issues and thereby contribute to reduced budget-overruns, improved time-to-market, and better quality software.
80|11||Modelling software development methodologies: A conceptual foundation|Current modelling approaches often purport to be based on a strong theoretical underpinning but, in fact, contain many ill-defined concepts or even contradictions leading to potential misinterpretation. Although much modelling in object-oriented contexts is focussed on the use of the Unified Modelling Language (UML), this paper presents a technology-agnostic approach that analyses the basic concepts of structural models and modelling in software engineering, using an approach based on representation theory. We examine the different kinds of interpretive mappings (either isotypical, prototypical or metatypical) that are required in order to trace model entities back to the SUS (subject under study) entities that they represent. The difference between forward- and backward-looking models is also explained, as are issues relating to the appropriate definition of modelling languages in general based on representation theory. The need for product and process integration in methodologies is then addressed, leading to the conclusion that a mesh of verbal plus nominal nodes is necessary in any methodology metamodel. Finally, the need for a common, cross-cutting modelling infrastructure is established, and a solution proposed in the form of an ontologically universal modelling language, OOLang. Examples of the application of these theoretical analyses to the suite of OMG products (particularly SPEM, UML and MOF) are given throughout, with the hope that awareness of the importance of a better modelling infrastructure can be developed.
80|11||A component composition model providing dynamic, flexible, and hierarchical composition of components for supporting software evolution|Component composition is one of the practical and effective approaches for supporting software evolution. However, existing component composition techniques need to be complemented by advanced features which address various sophisticated composition issues. In this paper, we introduce a set of features that supports and manages dynamic as well as flexible composition of components in a controlled way. We also propose a component composition model that supports these features. The proposed model enables dynamic, flexible, and hierarchical composition of components by providing and manipulating dedicated composition information, which in turn increases reusability of components and capabilities for supporting software evolution. To show the benefits of our model concretely, we provide a Hotel Reservation System case study. The experimental results show that our model supports software evolution effectively and provides efficient and modular structures, refactoring, and collaboration-level extensions as well.
80|11||Exploiting agents for modelling and simulation of coverage control protocols in large sensor networks|A sensor network is composed of low-cost, low-power nodes densely deployable over a (possibly in-hospitable) territory in order to monitor the state of the environment, e.g. temperature, sound, radiation and so forth. Sensors have the ability to self-organize into an interconnected network and to cooperate for collecting, aggregating and disseminating information to end users. Major challenges in dealing with sensor networks are the strong limitations imposed by finite onboard power capacity. This paper proposes a lightweight actor infrastructure that is well-suited to modelling and simulation of complex sensor networks and, more in general, of multi-agent systems. This infrastructure is exploited for designing and implementing an efficient actor-based distributed simulation model for studying specific aspects of large wireless sensor networks. The paper proposes and compares the performances of two protocols for the coverage control problem that achieve their objective as an emergent property. In particular, one of the two protocols adopts a novel approach based on an evolutionary game. Distributed simulation of the achieved actor-based models is characterized by good execution performances witnessed by reported experimental results.
80|11||Virtual knowledge service marketâFor effective knowledge flow within knowledge grid|A knowledge service consists of systematic knowledge and the mechanism of using knowledge to perform a task. The supply of knowledge services forms a knowledge service layer over the knowledge flow network formed by free knowledge sharing. To stimulate the supply of knowledge services, this paper proposes a virtual knowledge service market by establishing reward and reputation mechanisms. Simulations demonstrate that a team with the market mechanism performs better than those without it. The virtual knowledge service market provides an experimental platform for exploring the rules of knowledge service such as the impact of individual behavior on states of individual and team as well as the change of the states.
80|11||Performance evaluation of UML design with Stochastic Well-formed Nets|The paper presents a method to compute performance metrics (response time, sojourn time, throughput) on Unified Modeling Language design. The method starts with UML design annotated according to the UML Profile for Schedulability, Performance and Time. The UML design is transformed into a performance model where to compute the referred metrics. Being the performance model a Stochastic Well-formed Net, the method is enabled to analyze systems where the object identities are relevant as well as those where they are not. A complete case study reveals how to apply the method and its usefulness.
80|11||TFRP: An efficient microaggregation algorithm for statistical disclosure control|Recently, the issue of statistic disclosure control (SDC) has attracted much attention. SDC is a very important part of data security dealing with the protection of databases. Microaggregation for SDC techniques is widely used to protect confidentiality in statistical databases released for public use. The basic problem of microaggregation is that similar records are clustered into groups, and each group contains at least k records to prevent disclosure of individual information, where k is a pre-defined security threshold. For a certain k, an optimal multivariable microaggregation has the lowest information loss. The minimum information loss is an NP-hard problem. Existing fixed-size techniques can obtain a low information loss with O(n2) or O(n3/k) time complexity. To improve the execution time and lower information loss, this study proposes the Two Fixed Reference Points (TFRP) method, a two-phase algorithm for microaggregation. In the first phase, TFRP employs the pre-computing and median-of-medians techniques to efficiently shorten its running time to O(n2/k). To decrease information loss in the second phase, TFRP generates variable-size groups by removing the lower homogenous groups. Experimental results reveal that the proposed method is significantly faster than the Diameter and the Centroid methods. Running on several test datasets, TFRP also significantly reduces information loss, particularly in sparse datasets with a large k.
80|11||Safety analysis of software product lines using state-based modeling|The difficulty of managing variations and their potential interactions across an entire product line currently hinders safety analysis in safety-critical, software product lines. The work described here contributes to a solution by integrating product-line safety analysis with model-based development. This approach provides a structured way to construct state-based models of a product line having significant, safety-related variations and to systematically explore the relationships between behavioral variations and potential hazardous states through scenario-guided executions of the state model over the variations. The paper uses a product line of safety-critical medical devices to demonstrate and evaluate the technique and results.
80|11||Six years of evaluating software architectures in student projects|Software architecture evaluations are an important decision support tool when developing software systems. It is thus important that they are conducted professionally and that the results are of high quality. In order to improve the quality, it is necessary for the participants to gain experience in conducting software architecture evaluations. In this article we present guidelines based on six years of experience in software architecture evaluations. Although we primarily focus on our experiences on software architecture evaluation in student projects, we have also applied the same method in industry with similar experiences.
80|11||RDL: A language for framework instantiation representation|
80|11||Open standards, open formats, and open source|The paper proposes some comments and reflections on the notion of “openness” and on how it relates to three important topics: open standards, open formats, and open source. Often, these terms are considered equivalent and/or mutually implicated: “open source is the only way to enforce and exploit open standards”. This position is misleading, as it increases the confusion about this complex and extremely critical topic.
80|12|http://www.sciencedirect.com/science/journal/01641212/80/12|Special issue: International Conference on Pervasive Services (ICPS 2006)|
80|12||COCOA: COnversation-based service COmposition in pervAsive computing environments with QoS support|Pervasive computing environments are populated with networked services, i.e., autonomous software entities, providing a number of functionalities. One of the most challenging objectives to be achieved within these environments is to assist users in realizing tasks that integrate on the fly functionalities of the networked services opportunely according to the current pervasive environment. Towards this purpose, we present COCOA, a solution for COnversation-based service COmposition in pervAsive computing environments with QoS support. COCOA provides COCOA-L, an OWL-S based language for the semantic, QoS-aware specification of services and tasks, which further allows the specification of services and tasks conversations. Moreover, COCOA provides two mechanisms: COCOA-SD for the QoS-aware semantic service discovery and COCOA-CI for the QoS-aware integration of service conversations towards the realization of the user task’s conversation. The distinctive feature of COCOA is the ability of integrating on the fly the conversations of networked services to realize the conversation of the user task, by further meeting the QoS requirements of user tasks. Thereby, COCOA allows the dynamic realization of user tasks according to the specifics of the pervasive computing environment in terms of available services and by enforcing valid service consumption.
80|12||The DYNAMOS approach to support context-aware service provisioning in mobile environments|To efficiently make use of information and services available in ubiquitous environments, mobile users need novel means for locating relevant content, where relevance has a user-specific definition. In the DYNAMOS project, we have investigated a hybrid approach that enhances context-aware service provisioning with peer-to-peer social functionalities. We have designed and implemented a system platform and application prototype running on smart phones to support this novel conception of service provisioning. To assess the feasibility of our approach in a real-world scenario, we conducted field trials in which the research subject was a community of recreational boaters.
80|12||A comprehensive approach to model and use context for adapting applications in pervasive environments|With an increasing diversity of pervasive computing devices integrated in our surroundings and an increasing mobility of users, it will be important for computer systems and applications to be context-aware. Lots of works have already been done in this direction on how to capture context data and how to carry it to the application. Among the remaining challenges are to create the intelligence to analyze the context information and deduce the meaning out of it, and to integrate it into adaptable applications. Our work focuses on these challenges by defining generic context storage and processing model and by studying its impact on the application core. We propose a reusable context ontology model that is based on two levels: a generic level and a domain specific level. We propose a generic adaptation framework to guarantee adaptation of applications to the context in a pervasive computing environment. We also introduce a comprehensive adaptation approach that involves content adaptation and presentation adaptation inline with the adaptation of the core services of applications. Our case study shows that the context model and the application adaptation strategies provide promising service architecture.
80|12||Situational computing: An innovative architecture with imprecise reasoning|Situation awareness is introduced as a more holistic variant of context awareness where situations are regarded as logically aggregated pieces of context. Situational computing can be viewed as the pervasive computing paradigm that deals with situational context representation and reasoning. One important problem that arises in such paradigm is the imperfect observations (e.g., sensor readings) that lead to the estimation of the current user situation. Hence, the knowledge upon which the context/situation aware paradigm is built is rather vague. To deal with this shortcoming, we propose the use of Fuzzy Logic theory with the purpose of determining (inferring) and reasoning about the current situation of the involved user. We elaborate on the architectural model that enables the system to assume actions autonomously according to previous user reactions and current situation. The captured, imperfect contextual information is matched against pre-developed situation ontologies in order to approximately infer the current user context. Finally, we present a series of experimental results that provide evidence on the flexible, efficient nature of the proposed situational computing.
80|12||Granular best match algorithm for context-aware computing systems|In order to be context-aware, a system or application should adapt its behaviour according to current context, acquired by various context provision mechanisms. After acquiring current context, this information should be matched against the previously defined context sets. In this paper, a granular best match algorithm dealing with the subjective, fuzzy, multi-granular and multi-dimensional characteristics of contextual information is introduced. The CAPRA – Context-Aware Personal Reminder Agent tool is used to show the applicability of the new context matching algorithm. The obtained outputs showed that proposed algorithm produces the results which are more sensitive to the user’s intention, and more adaptive to the aforementioned characteristics of the contextual information than the traditional exact match method.
80|12||A service provisioning system for distributed personalization with private data protection|Personalized services can provide significant user benefits since they adapt their behavior to better support the user. Personalized services use a variety of data related to the user to decide their behavior. Thus personalized service needs a provisioning system that can collect the data that impacts service behavior and allows selection of the most appropriate service. However, in the coming ubiquitous environment, some data necessary for determining service behavior might be unavailable due to two possible reasons. One is that the data does not exit. The other is that the data exists but cannot be accessed. For example, users do not want to disclose their personal information, and service providers do not also want to expose data related to their knowhow in services. This paper describes a new service provisioning system for distributed personalization with private data protection. Specifically, the system selects applicable services by assessing how well each candidate service behaves when some data is missing. It then executes those selected services while hiding the users’ and providers’ private data in a distributed manner. We first summarize the requirements for a personalized service system, and introduce our fundamental policies for the system. The two main components of our system are then described in detail. One component is a service assessment mechanism that can judge if a service can work without data that can be used for adaptation. The second component is a service execution mechanism that can utilize private data while still ensuring privacy. This component divides service logic and executes divided logic where necessary data is available. The paper finally describes our prototype implementation and its performance evaluation results.
80|12||Enabling run-time composition and support for heterogeneous pervasive multi-agent systems|User needs-driven and computer-supported development of pervasive heterogeneous and dynamic multi-agent systems remains a great challenge for agent research community. This paper presents an innovative approach to composing, validating and supporting multi-agent systems at run-time. Multi-agent systems (MASs) can and should be assembled quasi-automatically and dynamically based on high-level user specifications which are transformed into a shared and common goal–mission. Dynamically generating agents could also be supported as a pervasive service. Heterogeneity of MASs refers to diverse functionality and constituency of the system which include mobile as well as host associated software agents. This paper proposes and demonstrates on-demand and just-in-time agent composition approach which is combined with run-time support for MASs. Run-time support is based on mission cost-efficiency and shared objectives which enable termination, generation, injection and replacement of software agents as the mission evolves at run-time. We present the formal underpinning of our approach and describe the prototype tool – called eHermes, which has been implemented using available agent platforms. Analysis and results of evaluating eHermes are presented and discussed.
80|12||Round-Eye: A system for tracking nearest surrounders in moving object environments|This paper presents “Round-Eye”, a system for tracking nearest surrounding objects (or nearest surrounders) in moving object environments. This system provides a platform for surveillance applications. The core part of this system is continuous nearest surrounder (NS) query that maintains views of the nearest objects at distinct angles from query points. This query differs from conventional spatial queries such as range queries and nearest neighbor queries as NS query considers both distance and angular aspects of objects with respect to a query point at the same time. In our system framework, a centralized server is dedicated (1) to collect location updates of both objects and queries, (2) to determine which NS queries are invalidated in presence of object/query location changes and corresponding result changes if any, and (3) to refresh the affected query answers. To enhance the system performance in terms of processing time and network bandwidth consumption, we propose various techniques, namely, safe region, partial query reevaluation, and incremental query result update. Through simulations, we evaluate our system with the proposed techniques over a wide range of settings.
80|2|http://www.sciencedirect.com/science/journal/01641212/80/2|Providing fault-tolerant authentication and authorization in wireless mobile IP networks|In wireless Mobile IP systems, the authentications and authorizations are performed by AAA (Authentication, Authorization, and Accounting) servers. An AAA server associates with a mobility agent to form an administrative domain. If an AAA server fails, all the mobile nodes (MNs) within the corresponding domain (failure-effected MNs) are unable to execute data services since their authentications and authorizations cannot be performed by the faulty AAA server. To tolerate the failure of the AAA server, this paper presents an efficient fault-tolerant approach. Once a failure is detected in an AAA server of an administrative domain, the proposed approach utilizes the AAA servers in other administrative domains to virtually generate a backup AAA server. To further reduce the fault-tolerant cost, the proposed approach additionally uses two techniques: preservation and tracking to assist the generation of the backup AAA server. Due to introducing these two techniques, the proposed approach does not need to retrieve the AAA records of failure-effected MNs while performing fault tolerance. Finally, we use M/G/c/c queuing model to analyze the effectiveness of the proposed approach over previous approaches. The analytical results are also validated by simulations.
80|2||ID-based restrictive partially blind signatures and applications|Restrictive blind signatures allow a recipient to receive a blind signature on a message not known to the signer but the choice of message is restricted and must conform to certain rules. Partially blind signatures allow a signer to explicitly include necessary information (expiration date, collateral conditions, or whatever) in the resulting signatures under some agreement with receiver. Restrictive partially blind signatures incorporate the advantages of these two blind signatures. The existing restrictive partially blind signature scheme was constructed under certificate-based (CA-based) public key systems. In this paper we follow Brand’s construction to propose the first identity-based (ID-based) restrictive blind signature scheme from bilinear pairings. Furthermore, we first propose an ID-based restrictive partially blind signature scheme, which is provably secure in the random oracle model. As an application, we use the proposed signature scheme to build an untraceable off-line electronic cash system followed the Brand’s construction.
80|2||Improvement of Yang et al.âs threshold proxy signature scheme|Since the first (t, n) threshold proxy signature scheme was proposed, the threshold proxy signature has enjoyed a considerable amount of interest from the cryptographic research community. In 2001, Hsu et al. proposed a non-repudiable threshold proxy signature scheme with known signer, but the efficiency is rather low and a system authority (SA) is also required in this scheme. Recently, to overcome these shortcomings, Yang et al. proposed an improvement of Hsu et al.’s scheme that is very efficient and without employing a SA. However, in this paper, we shows that Yang et al.’s scheme is not secure against the warrant attack. That is, the adversary can replace the warrant of the proxy signature. To resist this attack, based on Yang et al.’s scheme, we propose a new and more efficient scheme without a secure channel.
80|2||Worm-IT â A wormhole-based intrusion-tolerant group communication system|This paper presents Worm-IT, a new intrusion-tolerant group communication system with a membership service and a view-synchronous atomic multicast primitive. The system is intrusion-tolerant in the sense that it behaves correctly even if some nodes are corrupted and become malicious. It is based on a novel approach that enhances the environment with a special secure distributed component used by the protocols to execute securely a few crucial operations. Using this approach, we manage to bring together two important features: Worm-IT tolerates the maximum number of malicious members possible; it does not have to detect the failure of primary-members, a problem in previous intrusion-tolerant group communication systems.
80|2||GSR: A global seek-optimizing real-time disk-scheduling algorithm|Earliest-deadline-first (EDF) is good for scheduling real-time tasks in order to meet timing constraint. However, it is not good enough for scheduling real-time disk tasks to achieve high disk throughput. In contrast, although SCAN can maximize disk throughput, its schedule results may violate real-time requirements. Thus, during the past few years, various approaches were proposed to combine EDF and SCAN (e.g., SCAN-EDF and RG-SCAN) to resolve the real-time disk-scheduling problem. However, in previous schemes, real-time tasks can only be rescheduled by SCAN within a local group. Such restriction limited the obtained data throughput. In this paper, we proposed a new globally rescheduling scheme for real-time disk scheduling. First, we formulate the relations between the EDF schedule and the SCAN schedule of input tasks as EDF-to-SCAN mapping (ESM). Then, on the basis of ESM, we propose a new real-time disk-scheduling algorithm: globally seek-optimizing rescheduling (GSR) scheme. Different from previous approaches, a task in GSR may be rescheduled to anywhere in the input schedule to optimize data throughput. Owing to such a globally rescheduling characteristic, GSR obtains a higher disk throughput than previous approaches. Furthermore, we also extend the GSR to serve fairly non-real-time tasks. Experiments show that given 15 real-time tasks, our data throughput is 1.1 times that of RG-SCAN. In addition, in a mixed workload, compared with RG-SCAN, our GSR achieves over 7% improvement in data throughput and 33% improvement in average response time.
80|2||New results on non-perfect sharing of multiple secrets|A non-perfect secret sharing scheme is a method to distribute a secret among a set of participants, in such a way that some qualified subsets of participants, pooling together their information, can reconstruct the secret, whereas, other subsets of participants may have some information about the secret. In this journal, Feng et al. (2005) [A new multi-secret image sharing scheme using Lagrange’s interpolation. The Journal of Systems and Software, 76 (3), 327–339] recently considered the situation in which there are many secrets to be shared among a set of participants, in such a way that each qualified subset of participant can reconstruct a different secret. They proposed a polynomial-based construction using as a main tool a particular sequence of participants, called a sharing-circle, and having the feature that the smaller the length of the sharing-circle, the smaller the size of the shares distributed to participants. They proposed a simple recursive algorithm to find a minimal length sharing-circle. Since their algorithm is exponential-time, they left the task of finding a better one as an open problem.
80|2||Timed verification of the reliable adaptive multicast protocol|The uses of timed parameters in formalisms are important for providing realistic descriptions of distributed multimedia systems. We have developed Time-Estelle, an extended Estelle which is capable of doing so. Correct operations of this type of systems have stringent requirements for synchronisation of different entities or media data residing in a number of nodes possibly located very remotely from each other. Verification of formal specifications for such systems with time taken into consideration has been a subject of research. We have developed a method of verifying Time-Estelle specifications; it involves translating Time-Estelle specifications to Communicating Time Petri Nets which can then be verified by using the automated tool ORIS, with the dynamic behaviours of Estelle modules all supported. Using this verification method, this paper describes a timed verification of the Reliable Adaptive Multicast Protocol formally specified in Time-Estelle, and presents the results of the verification. Its contribution is that it represents a success in the use of a method in verifying a real-life protocol with timed properties specified formally.
80|2||A case study in re-engineering to enforce architectural control flow and data sharing|
80|2||MDABench: Customized benchmark generation using MDA|This paper describes an approach for generating customized benchmark suites from a software architecture description following a Model Driven Architecture (MDA) approach. The benchmark generation and performance data capture tool implementation (MDABench) is based on widely used open source MDA frameworks. The benchmark application is modeled in UML and generated by taking advantage of the existing community-maintained code generation “cartridges” so that current component technology can be exploited. We have also tailored the UML 2.0 Testing Profile so architects can model the performance testing and data collection architecture in a standards compatible way. We then extended the MDA framework to generate a load testing suite and automatic performance measurement infrastructure. This greatly reduces the effort and expertise needed for benchmarking with complex component and Web service technologies while being fully MDA standard compatible. The approach complements current model-based performance prediction and analysis methods by generating the benchmark application from the same application architecture that the performance models are derived from. We illustrate the approach using two case studies based on Enterprise JavaBean component technology and Web services.
80|2||Call for Papers:âSoftware Process and Product Measurementâ|
80|3|http://www.sciencedirect.com/science/journal/01641212/80/3|REBNITAâ05â1st International Workshop on Requirements Engineering for Business Need and IT Alignment|
80|3||Managing requirements for a US$1bn IT-based business transformation: New approaches and challenges|Managing uncertainty and complexity in IT based business transformation presents new requirements engineering (RE) challenges where requirements are not known up-front and business outcomes have to be delivered over time in dynamic alignment with market developments. Responding to these challenges necessitates new RE techniques that go beyond the traditional goals of completeness, correctness and consistency and focuses instead on business needs and IT alignment. Using the case of a major IT based business transformation in a large retail bank, this paper outlines new approaches being applied in practice and identifies areas for further RE research.
80|3||Eliciting Web application requirements â an industrial case study|A small variety of methods and techniques are presented in the literature as solutions to manage requirements elicitation for Web applications. However, the existing state of the art is lacking research regarding practical functioning solutions that would match Web application characteristics. The main concern for this paper is how requirements for Web applications can be elicited. The Viewpoint-Oriented Requirements Definition method (VORD) is chosen for eliciting and formulating Web application requirements in an industrial case study. VORD is helpful because it allows structuring of requirements around viewpoints and formulating very detailed requirements specifications. Requirements were understandable to the client with minimal explanation but failed to capture the business vision, strategy, and daily business operations, and could not anticipate the changes in the business process as a consequence of introducing the Web application within the organisation. The paper concludes by a discussion of how to adapt and extend VORD to suit Web applications.
80|3||FBCM: Strategy modeling method for the validation of software requirements|The Balanced Scorecard has attracted great attention as a modeling technique for enterprise management strategy. However, the strategy model is inadequate for examining the validity of software requirements, because methods to evaluate strategy models have yet to be developed. Therefore, this paper proposes a fact based collaboration modeling methodology (FBCM). Based on the results of field observations and data of business processes, it is possible to develop enterprise strategy models from the viewpoints of collaboration between organizations. This paper describes the basic concepts and procedures of the methodology. The methodology has been evaluated via a case study of developing an SCM strategy of a Japanese automobile enterprise. The research project was conducted for seven months to develop a strategy for a complete car logistic process in which five different departments of the company are involved. The results show the effectiveness of the proposed methodology.
80|3||Requirements change: Fears dictate the must haves; desires the wonât haves|We attempt to contribute to a general theory of requirements change from a goal-oriented and viewpoints-driven angle. To practitioners, this knowledge is relevant to anticipate changes in certain types of requirements, which may shorten the project’s timeline, reduce costs, and increase product quality. Initially, we followed the common assumptions that what should be on a system is demanded by goals to achieve and what should not be on a system is demanded by goal states to avoid. However, requirements engineering of a diversity of systems (capacity and warehouse management, COTS PCs, and a Braille mouse) revealed that must requirements are predicted by goals to avoid (!) and won’t requirements by goals to approach (!). Expectations about the positive or negative impact (valence) of requirements on goals played a moderating role. We unfold the gradual discovery of this “goals-to-requirements chiasm” (CHI-effect or Ï-effect), claiming that variability in agreement to positive or negative requirements is predicted by goals of opposite polarity. We found that whether the Ï-effect occurred or not, depended on the alignment of stakeholder viewpoints on goals and requirements. Comments from practitioners are included.
80|3||SEAL: A secure communication library for building dynamic group key agreement applications|We present the SEcure communicAtion Library (SEAL) [source can be downloaded from: http://www.cse.cuhk.edu.hk/~cslui/ANSRlab/software/SEAL/], a Linux-based C language application programming interface (API) library that implements secure group key agreement algorithms that allow a communication group to periodically renew a common secret group key for secure and private communication. The group key agreement protocols satisfy several important characteristics: distributed property (i.e., no centralized key server is needed), collaborative property (i.e., every group member contributes to the group key), and dynamic property (i.e., group members can join or leave the group without impairing the efficiency of the group key generation). Using SEAL, we developed a testing tool termed Gauger to evaluate the performance of the group key agreement algorithms in both wired and wireless LANs according to different levels of membership dynamics. We show that our implementation achieves robustness when there are group members leaving the communication group in the middle of a rekeying operation. We also developed a secure chat-room application termed Chatter to illustrate the usage of SEAL. Our SEAL implementation demonstrates the effectiveness of group key agreement in real network settings.
80|3||Opportunistic prioritised clustering framework for improving OODBMS performance|In object oriented database management systems, clustering has proven to be one of the most effective performance enhancement techniques. Existing clustering algorithms are mainly static, that is re-clustering the object base when the database is off-line. However, this type of re-clustering cannot be used when 24-h database access is required. In such situations dynamic clustering is necessary, since it can re-cluster the object base while the database is in operation. We find that most existing dynamic clustering algorithms do not address the following important points: the use of opportunism to impose the smallest I/O footprint for re-organisation; the re-use of prior research on static clustering algorithms; and the prioritisation of re-clustering so that the worst clustered pages are re-clustered first. Our main achievement in this paper is to create the Opportunistic Prioritised Clustering Framework (OPCF). The framework allows any static clustering algorithm to be made dynamic. Most importantly it allows the created algorithm to have the properties of I/O opportunism and clustering prioritisation which are missing in most existing dynamic clustering algorithms. We have used OPCF to make the static clustering algorithms “Graph Partitioning” and “Probability Ranking Principle” into dynamic algorithms. In our simulation study we found these algorithms outperformed two existing highly competitive dynamic algorithms in a variety of situations.
80|3||Self-certified signature scheme from pairings|To overcome key escrow problems and secure channel problems that seem to be inherent to identity-based cryptography, here we propose a self-certified signature scheme (SCS) from pairings on elliptic curves. We also give a security model, and further provide a security proof in random oracle model. The scheme incorporates the advantages of self-certified public keys and pairings. Besides, it can also provide an implicit as well as mandatory verification of public keys.
80|3||The design and evaluation of path matching schemes on compressed control flow traces|A control flow trace captures the complete sequence of dynamically executed basic blocks and function calls. It is usually of very large size and therefore commonly stored in compressed format. On the other hand, control flow traces are frequently queried to assist program analysis and optimization, e.g. finding frequently executed subpaths that may be optimized. In this paper, we identify path interruption and path context problems in querying an intraprocedural path over control flow traces. While algorithms that perform pattern matching on compressed strings have been proposed, solving new challenges requires the extension of traditional algorithms. We design and evaluate four path matching schemes including those that match in the compressed data directly and those that match after decompression. In addition, simple indices are also designed to improve matching performance. Our experimental results show that these schemes are practical and can be adapted to environments with different hardware settings and path matching requests.
80|3||An effective and efficient code generation algorithm for uniform loops on non-orthogonal DSP architecture|To meet ever-increasing demands for higher performance and lower power consumption, many high-end digital signal processors (DSPs) commonly employ non-orthogonal architecture. This architecture typically is characterized by irregular data paths, heterogeneous registers, and multiple memory banks. Moreover, sufficient compiler support is obviously important to harvest its benefits. However, usual compilation techniques do not adapt well to non-orthogonal architectures and the compiler design becomes much more difficult due to the complexity of these architectures. The entire code generation process for non-orthogonal architecture must include several phases. In this paper, we extend our previous study to propose a code generation algorithm Rotation Scheduling with Spill Codes Avoiding (RSSA), which is suitable for various DSPs with similar architectural features. As well as introducing detailed principles and algorithms of RSSA, we select several DSP applications and evaluate it under Motorola DSP56000 architectures. The evaluation results clearly demonstrate the effectiveness of RSSA, which can obtain scheduling results with minimum length and fewer spill codes compared to related work. In addition, in order to study the influence of different number of resources on the scheduling result, we also define a hypothetical machine model to represent a scalable non-orthogonal DSP architecture. After evaluating RSSA on various target architectures, we find that adding additional accumulators is the most efficient way to reduce spill codes. Meanwhile, for instruction-level parallelism exploration, numbers of data ALUs and accumulators have to be concurrently increased. Furthermore, based on our analysis, RSSA is not only effective but also quite efficient compared to related studies.
80|3||A novel data hiding scheme for color images using a BSP tree|In this paper, we propose a novel data hiding technique for color images using a BSP (Binary Space Partitioning) tree. First, we treat the RGB values at each pixel as a three-dimensional (3D) virtual point in the XYZ coordinates and a bounding volume is employed to enclose them. Using predefined termination criteria, we construct a BSP tree by recursively decomposing the bounding volume into voxels containing one or several 3D virtual points. The voxels are then further categorized into eight subspaces, each of which is numbered and represented as three-digit binary characters. In the embedding process, we first traverse the BSP tree, locating a leaf voxel; then we embed every three bits of the secret message into the points inside the leaf voxel. This is realized by translating a point’s current position to the corresponding numbered subspace. Finally, we transform the data-embedded 3D points to the stego color image. Our technique is a blind extraction scheme, where embedded messages can be extracted without the aid of the original cover image. It achieves high data capacity, equivalent to at least three times the number of pixels in the cover image. The stego image causes insignificant visual distortion under this high data capacity embedding scheme. In addition, we can take advantage of the properties of tree data structure to improve the security of the embedding process, making it difficult to extract the secret message without the secret key provided. Finally, when we adaptively modify the thresholds used to construct the BSP tree, our technique can be robust against attacks including image cropping, pixel value perturbation, and pixel reordering. But, the scheme is not robust against image compression, blurring, scaling, sharpening, and rotation attacks.
80|4|http://www.sciencedirect.com/science/journal/01641212/80/4|Editorial|
80|4||Ensuring system performance for cluster and single server systems|A new approach that is useful in identifying and eliminating performance degradation occurring in aging software is proposed. A customer-affecting metric is used to initiate the restoration of such a system to full capacity. A case study is described in which, by simulating an industrial software system, we are able to show that by monitoring a customer-affecting metric and frequently comparing its degradation to the performance objective, we can ensure system stability at a very low cost.
80|4||Model-based system reconfiguration for dynamic performance management|Recently, growing attention focused on run-time management of Quality of Service (QoS) of complex software systems. In this context, system reconfiguration is considered a useful technique to manage QoS. Several reconfiguration approaches to performance management exist that help systems to maintain performance requirements at run time. However, many of them use prefixed strategies that are in general coded in the application or in the reconfiguration framework.
80|4||Interaction tree algorithms to extract effective architecture and layered performance models from traces|Models of software architecture and software performance both depend on identifying and describing the interactions between the components, during typical responses. This work identifies the components and interactions that are active during a tracing experiment, hence the name “effective architecture” and also derives layered performance models. The System Architecture and Model Extraction Technique (SAMEtech) described here overcomes a weakness of previous work with “angio traces” in two ways. It only requires standard trace formats (rather than a custom format which captures causality) and it uses a simpler algorithm which scales up linearly for very large traces. It accepts some limitations: components must not have internal parallelism with forking and joining of the flow of execution. SAMEtech uses pattern matching based on “interaction trees” for detecting various types of interactions (asynchronous, blocking synchronous, nested synchronous, and forwarding). With this information it builds architecture and performance models.
80|4||Quantifying software performance, reliability and security: An architecture-based approach|With component-based systems becoming popular and handling diverse and critical applications, the need for their thorough evaluation has become very important. In this paper we propose an architecture-based unified hierarchical model for software performance, reliability, security and cache behavior prediction. We employ discrete time Markov chains (DTMCs) to model software systems and provide expressions for predicting the overall behavior of the system based on its architecture as well as the characteristics of individual components. This approach also facilitates the identification of various bottlenecks. We illustrate its use through some case studies and also provide expressions to perform sensitivity analysis.
80|4||Efficient performance models for layered server systems with replicated servers and parallel behaviour|Capacity planning for large computer systems may require very large performance models, which are difficult or slow to solve. Layered queueing models solved by mean value analysis can be scaled to dozens of servers and hundreds of service classes, with large class populations, but this may not be enough. A common feature of planning models for large systems is structural repetition expressed through replicated subsystems, which can provide both scalability and reliability, and this replication can be exploited to scale the solution technique. A model has recently been described for symmetrically replicated layered servers, and their integration into the system, with a mean-value solution approximation. However, parallelism is often combined with replication; high-availability systems use parallel data-update operations on redundant replicas, to enhance reliability, and grid systems use parallel computations for scalability. This work extends the replicated layered server model to systems with parallel execution paths. Different servers may be replicated to different degrees, with different relationships between them. The solution time is insensitive to the number of replicas of each replicated server, so systems with thousands or even millions of servers can be modelled efficiently.
80|4||Filling the gap between design and performance/reliability models of component-based systems: A model-driven approach|To facilitate the use of non-functional analysis results in the selection and assembly of components for component-based systems, automatic prediction tools should be devised, to predict some overall quality attribute of the application without requiring extensive knowledge of analysis methodologies to the application designer. To achieve this goal, a key idea is to define a model transformation that takes as input some “design-oriented” model of the component assembly and produces as a result an “analysis-oriented” model that lends itself to the application of some analysis methodology. However, to actually devise such a transformation, we must face both the heterogeneous design level notations for component-based systems, and the variety of non-functional attributes and related analysis methodologies we could be interested in. To tackle these problems, we define a model-driven transformation framework, centered around a kernel language whose aim is to capture the relevant information for the analysis of non-functional attributes of component-based systems, with a focus on performance and reliability. Using this kernel language as a bridge between design-oriented and analysis-oriented notations we reduce the burden of defining a variety of direct transformations from the former to the latter to the less complex problem of defining transformations to/from the kernel language. The proposed kernel language is defined within the MOF (Meta-Object Facility) framework, to allow the exploitation of MOF-based model transformation facilities.
80|4||Defect prevention in software processes: An action-based approach|In addition to degrading the quality of software products, software defects also require additional efforts in rewriting software and jeopardize the success of software projects. Software defects should be prevented to reduce the variance of projects and increase the stability of the software process. Factors causing defects vary according to the different attributes of a project, including the experience of the developers, the product complexity, the development tools and the schedule. The most significant challenge for a project manager is to identify actions that may incur defects before the action is performed. Actions performed in different projects may yield different results, which are hard to predict in advance. To alleviate this problem, this study proposes an Action-Based Defect Prevention (ABDP) approach, which applies the classification and Feature Subset Selection (FSS) technologies to project data during execution.
80|4||Lessons from applying the systematic literature review process within the software engineering domain|A consequence of the growing number of empirical studies in software engineering is the need to adopt systematic approaches to assessing and aggregating research outcomes in order to provide a balanced and objective summary of research evidence for a particular topic. The paper reports experiences with applying one such approach, the practice of systematic literature review, to the published studies relevant to topics within the software engineering domain. The systematic literature review process is summarised, a number of reviews being undertaken by the authors and others are described and some lessons about the applicability of this practice to software engineering are extracted.
80|4||The WhenâWhoâHow analysis of defects for improving the quality control process|Most large software products have elaborate quality control processes involving many tasks performed by different groups using a variety of techniques. The defects found are generally recorded in a database which is used for tracking and prioritizing defects. However, this defect data also provides a wealth of information which can be analyzed for improving the process. In this paper, we describe the when–who–how approach for analyzing defect data to gain a better understanding of the quality control process and identify improvement opportunities. At the component level, the analysis provides the capability to assess strength of dependency between components, and new ways to study correlation between early and late defects. We also discuss the experience of applying this approach to defect data from an earlier version of Windows, and the improvement opportunities it revealed.
80|4||Modeling and analysis of software aging and software failure|Many studies reported that system suffered from outages more due to software faults than hardware faults. Recently, the phenomenon of “software aging”, which was caused by aging-related faults, is observed in many software systems. Software aging, characterized by progressive performance degradation, is mainly caused by exhaustion of the operating system resources, such as memory leaking, unreleased-file locks, data corruption, etc. This paper mainly focuses on the modeling and analysis of software aging and software failure. A stochastic time series decomposition algorithm based on robust locally weighted regression (Loess) is presented to separate the exhaustion of system resource from the resource usage, from which aging trend is estimated. Then the model of software aging and software failure process is constructed. Experiments on a practical server system verify the effectiveness of the algorithm presented in this paper, and the two-stage failure process is also confirmed for the first time in the history of research on software aging. The conclusions drawn from this paper will greatly benefit the application of software rejuvenation technique, that is, it makes it easy to determine when to perform software rejuvenation, which is a key issue in implementation of software rejuvenation. The results for the server system employing different rejuvenation policies show that software performance can be effectively improved.
80|4||Software development risk and project performance measurement: Evidence in Korea|As more US companies outsource their software projects overseas, they find that it is more challenging to control software development risk in countries with dissimilar IT capabilities. Using data collected from software projects developed in Korea, we investigate the impacts of specific risk management strategies and residual performance risk on objective performance measures such as cost and schedule overrun. Our results indicate that, unlike subjective performance measures, our objective cost and schedule overrun performance measures are positively associated with residual performance risk in Korea. We also investigate the impact of two alternative conceptualization of software development risk on both objective performance and subjective performance. Finally, we discuss relevant policy implications for software development and outsourcing.
80|4||Neural-network-based approaches for software reliability estimation using dynamic weighted combinational models|Software reliability is the probability of failure-free software operation for a specified period of time in a specified environment. During the last three decades, many software reliability growth models (SRGMs) have been proposed and analyzed for measuring software reliability growth. SRGMs are mathematical models that represent software failures as a random process and can be used to evaluate development status during testing. However, most of SRGMs depend on some assumptions or distributions. In this paper, we propose an artificial neural-network-based approach for software reliability estimation and modeling. We first explain the neural networks from the mathematical viewpoints of software reliability modeling. We will show how to apply neural network to predict software reliability by designing different elements of neural networks. Furthermore, we will use the neural network approach to build a dynamic weighted combinational model (DWCM). The applicability of proposed model is demonstrated through real software failure data sets. The results obtained from the experiments show that the proposed model has a fairly accurate prediction capability.
80|4||Software quality and IS project performance improvements from software development process maturity and IS implementation strategies|The capability maturity model (CMM) is part of several software process improvement (SPI), six sigma, and total quality management (TQM) initiatives in organizations. SPI and continuous quality improvements are associated with better return on investment (ROI) for organizations. The purpose of this empirical research is to study the impact of the CMM on certain critical factors in information systems implementation strategy, software quality and software project performance. Our findings are that CMM levels do associate with IS implementation strategies and higher CMM levels relate to higher software quality and project performance. We also conclude that information systems (IS) implementation strategies have a significant impact on software quality and project performance. While certain IS implementation strategies – executive commitment and prototyping – have a significant impact on both software quality and project performance, training had a significant effect only on software quality and simplicity has a significant effect only on project performance.
80|4||The adjusted analogy-based software effort estimation based on similarity distances|Analogy-based estimation is a widely adopted problem solving method that has been evaluated and confirmed in software effort or cost estimation domains. The similarity measures between pairs of projects play a critical role in the analogy-based software effort estimation models. Such a model calculates a distance between the software project being estimated and each of the historical software projects, and then retrieves the most similar project for generating an effort estimate. Although there exist numerous analogy-based software effort estimation models in literature, little theoretical or experimental works have been reported on the method of deriving an effort estimate from the adjustment of the reused effort based on the similarity distance. The present paper investigates the effect on the improvement of estimation accuracy in analogy-based estimations when the genetic algorithm method is adopted to adjust reused effort based on the similarity distances between pairs of projects. The empirical results show that applying a suitable linear model to adjust the analogy-based estimations is a feasible approach to improving the accuracy of software effort estimates. It also demonstrates that the proposed model is comparable with those obtained when using other effort estimation methods.
80|5|http://www.sciencedirect.com/science/journal/01641212/80/5|Guest Editorial|
80|5||Component-based hardware/software co-verification for building trustworthy embedded systems|We present a novel component-based approach to hardware/software co-verification of embedded systems using model checking. Embedded systems are pervasive and often mission-critical, therefore, they must be highly trustworthy. Trustworthy embedded systems require extensive verification. The close interactions between hardware and software of embedded systems demand co-verification. Due to their diverse applications and often strict physical constraints, embedded systems are increasingly component-based and include only the necessary components for their missions. In our approach, a component model for embedded systems which unifies the concepts of hardware IPs (i.e., hardware components) and software components is defined. Hardware and software components are verified as they are developed bottom-up. Whole systems are co-verified as they are developed top-down. Interactions of bottom-up and top-down verification are exploited to reduce verification complexity by facilitating compositional reasoning and verification reuse. Case studies on a suite of networked sensors have shown that our approach facilitates major verification reuse and leads to order-of-magnitude reduction on verification complexity.
80|5||The SAVE approach to component-based development of vehicular systems|The component-based strategy aims at managing complexity, shortening time-to-market, and reducing maintenance requirements by building systems with existing components. The full potential of this strategy has not yet been demonstrated for embedded software, mainly because of specific requirements in the domain, e.g., those related to timing, dependability, and resource consumption.
80|5||The design and performance of component middleware for QoS-enabled deployment and configuration of DRE systems|Quality of Service (QoS)-enabled component middleware can help reduce the complexity of deploying and configuring QoS aspects, such as priorities and rates of invocation. Few empirical studies have been conducted, however, to guide developers of distributed real-time and embedded (DRE) systems in choosing among alternative designs and performance optimizations. Moreover, few empirical studies have been conducted to examine the performance and flexibility trade-offs between standards-based and domain-specific DRE middleware solutions.
80|5||Early quality prediction of component-based systems â A generic framework|Component-based software engineering is currently an emerging technology used to develop complex embedded systems. These embedded systems need to fulfil requirements regarding quality attributes such as safety, reliability, availability, maintainability, performance, security and temporal correctness. Since quality problems should be identified and tackled early in the development process, there is a rising need to predict and evaluate these properties in the architecture design phase. This paper describes a generic framework for predicting quality properties based on component-based architectures, which is derived from a comprehensive study of recent architecture evaluation methods. This generic framework defines common aspects between the different evaluation methods and enables the improvement of evaluation methods for specific quality properties, by transferring knowledge from one quality domain to the other. Thus, this paper can help to create better evaluation methods in the future.
80|5||CAmkES: A component model for secure microkernel-based embedded systems|Component-based software engineering promises to provide structure and reusability to embedded-systems software. At the same time, microkernel-based operating systems are being used to increase the reliability and trustworthiness of embedded systems. Since the microkernel approach to designing systems is partially based on the componentisation of system services, component-based software engineering is a particularly attractive approach to developing microkernel-based systems. While a number of widely used component architectures already exist, they are generally targeted at enterprise computing rather than embedded systems. Due to the unique characteristics of embedded systems, a component architecture for embedded systems must have low overhead, be able to address relevant non-functional issues, and be flexible to accommodate application specific requirements. In this paper we introduce a component architecture aimed at the development of microkernel-based embedded systems. The key characteristics of the architecture are that it has a minimal, low-overhead, core but is highly modular and therefore flexible and extensible. We have implemented a prototype of this architecture and confirm that it has very low overhead and is suitable for implementing both system-level and application level services.
80|5||Intrusion detection aware component-based systems: A specification-based framework|Component-Based Software Engineering (CBSE) increases the reusability of software and hence decreases software development time and cost. Unfortunately, developing components for maximum reusability and acquiring third party components invite many security related concerns. The security related issues are more crucial for embedded and real-time systems. Currently, many approaches are proposed to aid the development and evaluation of secure components. However, it is well known among practitioners that, like any other software entities, components cannot be completely secure. This fact leads us to incorporate intrusion detection facilities to equip components with mechanisms to discover intrusions against components. In this paper, we present a framework for developing components with intrusion detection capabilities. This framework uses UMLintr, a UML profile for intrusion specifications. The profile allows developers to specify intrusion scenarios using UML diagrams. Specifying intrusion scenarios using the same language that is used for specifying software behavior eliminates the need for separate languages for describing intrusions. Other software specification languages can be easily adopted into this framework. The outcome of this framework are components equipped with intrusion detectors. Based on UMLintr, a prototype is built and used to generate signatures for some intrusions included in the benchmark DARPA attack datasets. Furthermore, we describe an Intrusion Detection System (IDS) which uses these signatures to detect component intrusions.
80|5||A skewed distributed indexing for skewed access patterns on the wireless broadcast|Data broadcast is an efficient way to disseminate information to a large number of mobile clients in the wireless environment. Adding an index data organization to the broadcast file can save client power consumption with little increase in client waiting time. The existing index technologies only consider equal access probabilities of data items. However, in real-life applications, some data items may be more popular than others; that is, access patterns of clients are skewed. In this paper, we propose a skewed distributed indexing, SDI, which considers the access probabilities of data items and the replication of index nodes. The proposed algorithm traverses an index tree to determine whether an index node should be replicated by considering the access probability of its child node. In our experimental results, we have shown that our proposed algorithm outperforms the variant-fanout index tree and the distributed indexing.
80|5||Task allocation for maximizing reliability of a distributed system using hybrid particle swarm optimization|In a distributed computing system, a number of program modules may need to be allocated to different processors such that the reliability of executing successfully these modules is maximized and the constraints with limited resources are satisfied. The problem of finding an optimal task allocation with maximum system reliability has been shown to be NP-hard; thus, existing approaches to finding exact solutions are limited to the use in problems of small size. This paper presents a hybrid particle swarm optimization (HPSO) algorithm for finding the near-optimal task allocation within reasonable time. The experimental results show that the HPSO is robust against different problem size, task interaction density, and network topology. The proposed method is also more effective and efficient than a genetic algorithm for the test-cases studied. The convergence and the worst-case characteristics of the HPSO are addressed using both theoretical and empirical analysis.
80|5||Serfs: Dynamically-bound parameterized components|Parameterization is an effective technique for decoupling design decisions in software. Several languages such as C++ and Ada (and Java and C# more recently) offer language constructs for building parameterized software. Using template or generic constructs, one can postpone committing to specific design choices until the software system is ready for deployment. However, in cases where such choices are influenced by the execution environment, deployment time may not be late enough. Moreover, in the context of software systems that have to satisfy high availability constraints, or are long-running, changes in design choices may be warranted even after deployment. In this paper, we present a design pattern-based methodology for building parameterized components that support dynamic binding of parameters. Moreover, the methodology also supports dynamic re-binding of parameters in the event that such online change is required.
80|5||Taking advantages of a disadvantage: Digital forensics and steganography using document metadata|All the information contained in a plain-text document are visible to everybody. On the other hand, compound documents using opaque formats, like Microsoft Compound Document File Format, may contain undisclosed data such as authors name, organizational information of users involved, previously deleted text, machine related information, and much more. Those information could be exploited by third party for illegal purposes. Computer users are unaware of the problem and, even though the Internet offers several tools to clean hidden data from documents, they are not widespread. Furthermore, there is only one paper about this problem in scientific literature, but there is no detailed analysis.
80|5||Distributed service provision using open APIs-based middleware: âOSA/Parlay vs. JAINâ performance evaluation study|Demand for value added services creates significant revenue opportunities for operators that decide to open their networks to external service providers. This process is practically feasible only if standardized interfaces to core network operations are utilized. This way the cost and complexity of the development and integration process is reduced, as well as the service administration overhead, which is distributed among service providers and does not burden the network operator. Most existing solutions for standardized access to core network operations follow the middleware paradigm that encapsulates network operations and provides open interfaces to outside service providers. The main concern about this approach has always been the performance impact of the additional architectural layers on top of the core network architecture. Different middleware technologies such as distributed object technologies and messaging middleware have been proposed and applied, all with varying performance results. The scope of this paper is to investigate and compare the performance impact inflicted by two such solutions encapsulating network call control functions. The first one is an OSA/Parlay based call control implementation with CORBA and RMI serving as underlying distributed object technologies and the second one is a JAIN call control implementation, again with an RMI underlying messaging framework. All performance measurements are taken in the context of a real-time “call forwarding” service translated from a corresponding VoIP SIP-based implementation.
80|5||Exploiting idle cycles to execute data mining applications on clusters of PCs|In this paper we present and evaluate Inhambu, a distributed object-oriented system that supports the execution of data mining applications on clusters of PCs and workstations. This system provides a resource management layer, built on the top of Java/RMI, that supports the execution of the data mining tool called Weka. We evaluate the performance of Inhambu by means of several experiments in homogeneous, heterogeneous and non-dedicated clusters. The obtained results are compared with those achieved by a similar system named Weka-Parallel. Inhambu outperforms its counterpart for coarse grain applications, mainly for heterogeneous and non-dedicated clusters. Also, our system provides additional advantages such as application checkpointing, support for dynamic aggregation of hosts to the cluster, automatic restarting of failed tasks, and a more effective usage of the cluster. Therefore, Inhambu is a promising tool for efficiently executing real-world data mining applications. The software is delivered at the project’s web site available at http://incubadora.fapesp.br/projects/inhambu/.
80|5||Security problems with improper implementations of improved FEA-M|This paper reports security problems with improper implementations of an improved version of FEA-M (fast encryption algorithm for multimedia). It is found that an implementation-dependent differential chosen-plaintext attack or its chosen-ciphertext counterpart can reveal the secret key of the cryptosystem, if the involved (pseudo-)random process can be tampered (for example, through a public time service). The implementation-dependent differential attack is very efficient in complexity and needs only O(n2) chosen plaintext or ciphertext bits. In addition, this paper also points out a minor security problem with the selection of the session key. In real implementations of the cryptosystem, these security problems should be carefully avoided, or the cryptosystem has to be further enhanced to work under such weak implementations.
80|6|http://www.sciencedirect.com/science/journal/01641212/80/6|Web error classification and analysis for reliability improvement|In this paper, we adapt an existing defect classification and analysis framework, orthogonal defect classification (ODC), to analyze web errors and identify problematic areas for focused reliability improvement. Based on information extracted from existing web server logs, web errors are classified according to their response code, file type, referrer type, agent type, and observation time. We also introduce an analysis procedure to identify high-risk/high-leverage sub-classes of problems and consolidate analysis results to recommend appropriate followup actions. Results applying our approach to the www.seas.smu.edu and www.kde.org web sites are included to demonstrate its applicability and effectiveness.
80|6||Assessing the validity of one-part software reliability models using likelihood ratio and early detection tests|To achieve higher accuracy in software failure rate estimates in a field environment, software reliability models that were fit from failure data collected during the test interval are often refit using the combined test interval and field interval data. The validity of this analysis depends on the test and field environments being compatible with respect to the manner in which the software is used. In this paper, we formulate the hypothesis of compatible test and field environments in terms of a statistical hypothesis and develop an appropriate test procedure. The test procedure is illustrated by applying it to a real-life software project.
80|6||Modeling software testing costs and risks using fuzzy logic paradigm|The overall lifecycle cost associated with product failures exceeds 10% of yearly corporations’ turnover. A major factor contributing to this loss is ineffective performance of software and systems Verification, Validation and Testing (VVT). Given these realities, we proposed a set of quantitative probabilistic models for estimating costs and risks stemming from carrying out any given VVT strategy [Engel, A., Barad, M., 2003. A methodology for modeling VVT risks and costs. Systems Engineering Journal 6 (3), 135–151, Wiley InterScience, Online ISSN: 1520-6858, Print ISSN: 1098-1241]. We also demonstrated that quality costs in software-intensive projects are likely to consume as much as 60% of the development budget. Finally, we showed that project cost and duration could be reduced by optimizing the VVT strategy, yielding about 10–15% reduction in development costs and project schedule [Engel, A., Shachar, S., 2006. Measuring and optimizing systems’ quality costs and project duration. Systems Engineering Journal 9 (3), 259–280].
80|6||Institutionalization of software product line: An empirical investigation of key organizational factors|
80|6||The impacts of software product management|The success of any product depends on the skills and competence of the product manager. This article evaluates the relevance of good product management on the success of a product. The empirical study is supported by data from 178 industry projects from telecommunication industry over a period of three years throughout which the product management role and competency was defined, deployed and improved. The behaviors and project performance before and after strengthening the product management discipline are compared. We found that with increasing institutionalization of a consistent and empowered product management role, the success rate of projects in terms of schedule predictability, quality and project duration improves. To allow better transfer of achieved results to other settings, the article provides concrete guidelines about key success factors for good product management.
80|6||Control and data flow structural testing criteria for aspect-oriented programs|Although it is claimed that, among other features, aspect-oriented programming (AOP) increases understandability and eases the maintenance burden, this technology cannot provide correctness by itself, and thus it also requires the use of systematic verification, validation and testing (VV&T) approaches. With the purpose of producing high quality software, many approaches to apply structural testing criteria for the unit testing of procedural and object-oriented (OO) programs have been proposed. Nevertheless, until now, few works have addressed the application of such criteria to test aspect-oriented programs. In this paper we define a family of control flow and data flow based testing criteria for aspect-oriented programs inspired by the implementation strategy adopted by AspectJ – an aspect-oriented extension of the Java language – and extending a previous work proposed for Java programs. We propose the derivation of a control and data flow model for aspect-oriented programs based upon the static analysis of the object code (the Java bytecode) resulted from the compilation/weaving process. Using this model, called aspect-oriented def-use graph (AODUAODU), traditional and also aspect-oriented testing criteria are defined (called Control and Data Flow Structural Testing Criteria for Aspect-Oriented Programs – CDSTC-AOP). The main idea is that composition of aspect-oriented programs leads to new crosscutting interfaces in several modules of the system, which must be considered for coverage during structural testing. The implementation of a prototype tool – the JaBUTi/AJ tool – to support the proposed criteria and model is presented along with an example. Also, theoretical and practical questions regarding the CDSTC-AOP criteria are discussed.
80|6||An exploratory study of why organizations do not adopt CMMI|This paper explores why organizations do not adopt CMMI (Capability Maturity Model Integration), by analysing two months of sales data collected by an Australian company selling CMMI appraisal and improvement services. The most frequent reasons given by organizations were: the organization was small; the services were too costly, the organization had no time, and the organization was using another SPI approach. Overall, we found small organizations not adopting CMMI tend to say that adopting it would be infeasible, but do not say it would be unbeneficial. We comment on the significance of our findings and research method for SPI research.
80|6||Understanding failure response in service discovery systems|Service discovery systems enable distributed components to find each other without prior arrangement, to express capabilities and needs, to aggregate into useful compositions, and to detect and adapt to changes. First-generation discovery systems can be categorized based on one of three underlying architectures and on choice of behaviors for discovery, monitoring, and recovery. This paper reports a series of investigations into the robustness of designs that underlie selected service discovery systems. The paper presents a set of experimental methods for analysis of robustness in discovery systems under increasing failure intensity. These methods yield quantitative measures for effectiveness, responsiveness, and efficiency. Using these methods, we characterize robustness of alternate service discovery architectures and discuss benefits and costs of various system configurations. Overall, we find that first-generation service discovery systems can be robust under difficult failure environments. This work contributes to better understanding of failure behavior in existing discovery systems, allowing potential users to configure deployments to obtain the best achievable robustness at the least available cost. The work also contributes to design improvements for next-generation service discovery systems.
80|6||A rationale-based architecture model for design traceability and reasoning|Large systems often have a long life-span and comprise many intricately related elements. The verification and maintenance of these systems require a good understanding of their architecture design. Design rationale can support such understanding but it is often undocumented or unstructured. The absence of design rationale makes it much more difficult to detect inconsistencies, omissions and conflicts in an architecture design. We address these issues by introducing a rationale-based architecture model that incorporates design rationale, design objects and their relationships. This model provides reasoning support to explain why design objects exist and what assumptions and constraints they depend on. Based on this model, we apply traceability techniques for change impact analysis and root-cause analysis, thereby allowing software architects to better understand and reason about an architecture design. In order to align closely with industry practices, we choose to represent the rationale-based architecture model in UML. We have implemented a tool-set to support the capture and the automated tracing of the model. As a case study, we apply this approach to an real-world electronic payment system.
80|7|http://www.sciencedirect.com/science/journal/01641212/80/7|Guest Editorâs introduction|
80|7||FC-ORB: A robust distributed real-time embedded middleware with end-to-end utilization control|A key challenge for distributed real-time and embedded (DRE) middleware is maintaining both system reliability and desired real-time performance in unpredictable environments where system workload and resources may fluctuate significantly. This paper presents FC-ORB, a real-time Object Request Broker (ORB) middleware that employs end-to-end utilization control to handle fluctuations in application workload and system resources. The contributions of this paper are three-fold. First, we present a novel utilization control service that enforces desired CPU utilization bounds on multiple processors by adapting the rates of end-to-end tasks within user-specified ranges. Second, we describe a set of middleware-level mechanisms designed to support end-to-end tasks and distributed multi-processor utilization control in a real-time ORB. Finally, we present extensive experimental results on a Linux testbed. Our results demonstrate that our middleware can maintain desired utilizations in face of uncertainties and variations in task execution times, resource contentions from external workloads, and permanent processor failure. FC-ORB demonstrates that the integration of utilization control, end-to-end scheduling, and fault-tolerance mechanisms in DRE middleware is a promising approach for enhancing the robustness of DRE applications in unpredictable environments.
80|7||Hierarchical resource allocation for robust in-home video streaming|High quality video streaming puts high demands on network and processor resources. The bandwidth of the communication medium and the timely arrival of the frames necessitate a tight resource allocation. Given the dynamic environment where videos are started and stopped and electro-magnetic perturbations affect the bandwidth of the wireless medium, a framework is needed that reacts timely to the changes in network load and network operational conditions. This paper describes a hierarchical framework, which can handle the dynamic network resource allocation in a timely manner.
80|7||Resource management for real-time tasks in mobile robotics|
80|7||Adaptive network QoS in layer-3/layer-2 networks as a middleware service for mission-critical applications|We present adaptive network Quality of Service (QoS) technology that provides delay bounds and capacity guarantees for traffic belonging to mission-critical tasks. Our technology uses a Bandwidth Broker to provide admission control and leverages the differentiated aggregated traffic treatment provided by today’s high-end COTS layer-3/2 switches. The technology adapts to changes in network resources, work load and mission requirements, using two components that are a particular focus of this paper: Fault Monitor and Performance Monitor. Our technology is being developed and applied in a CORBA-based multi-layer resource management framework.
80|7||A multi-layered resource management framework for dynamic resource management in enterprise DRE systems|Enterprise distributed real-time and embedded (DRE) systems can benefit from dynamic management of computing and networking resources to optimize and reconfigure system resources at runtime in response to changing mission needs and/or other situations, such as failures or system overload. This paper provides two contributions to the study of dynamic resource management (DRM) for enterprise DRE systems. First, we describe a standards-based multi-layered resource management (ARMS MLRM) architecture that provides DRM capabilities to enterprise DRE systems. Second, we show the results of experiments evaluating our ARMS MLRM architecture in the context of a representative enterprise DRE system for shipboard computing.
80|7||Feedback control-based dynamic resource management in distributed real-time systems|The resource management in distributed real-time systems becomes increasingly unpredictable with the proliferation of data-driven applications. Therefore, it is inefficient to allocate the resources statically to handle a set of highly dynamic tasks whose resource requirements (e.g., execution time) are unknown a prior. In this paper, we build a distributed real-time system based on the control theory, focusing on the computational resource management. Specifically, this work makes three important contributions. First, it allows the designer to specify the desired temporal behavior of system adaptation, such as the speed of convergence. This is in contrast to previous literature, specifying only steady-state metrics, e.g. the deadline miss ratio. Second, unlike QoS optimization approaches, our solution meets performance guarantees with no accurate knowledge of task execution parameters – a key advantage in a poorly modeled environment. Last, in contrast to ad hoc algorithms based on intuition and testing, we rigorously prove that our approach not only has excellent steady state behavior, but also meets stability, overshoot, and settling time requirements.
80|7||Characterizing robustness in dynamic real-time systems|The problem of robust task allocation is motivated by the need to deploy real-time systems in dynamic operational environments. Existing robust allocation approaches employ coarse robustness metrics, which can result in poor allocations. This paper proposes a metric that accurately characterizes a system’s robustness within feasible allocation regions. An allocation algorithm is provided to find allocations that are both feasible and robust; the robustness as measured by the metric is shown to have theoretical bounds. Experiments demonstrate that the algorithm produces good and scalable performance compared with several heuristic algorithms.
80|7||Verification of instrumentation techniques for resource management of real-time systems|Dynamic resource management is an effective way to ensure that real-time systems work correctly in unpredictable environments. For mission-critical systems, dynamic resource managers (RMs) must be verified. Every RM depends on instruments to report the state of the system and its resources, so verified instrumentation is foundational to the larger goal of verified RMs.
80|7||Identity based proxy multi-signature|
80|7||Resource allocation in network processors for network intrusion prevention systems|
80|7||Application-specific garbage collection|Prior work, including our own, shows that application performance in garbage collected languages is highly dependent upon the application behavior and on underlying resource availability. We show that given a wide range of diverse garbage collection (GC) algorithms, no single system performs best across programs and heap sizes.
80|7||Novel image copy detection with rotating tolerance|In 2003, Kim applied the discrete cosine transform (DCT) technique to propose a content-based image copy detection method. He successfully detected the copies both with and without modifications, and his method is the first to detect the copies with water coloring and twirling modifications. However, Kim’s method can only detect copies having a 180° rotation. When copies have a 90° or 270° rotation, Kim’s method fails to discover them. Also, his method cannot deal with copies having only minor rotations of 1° or 5°, and so on. To conquer this weakness, we propose an elliptical track division strategy to extract two kinds of features in the design of our proposed methods. Furthermore, we propose a dynamic center point detection mechanism for the elliptical track of each image to deal with the shifting process. The experimental results confirm that both proposed methods can successfully capture the features of an image even if it is shifted, cropped or rotated to any degree. In addition, our hybrid method can further provide accurate detection performance for a variety of manipulations.
80|7||Improvements of image sharing with steganography and authentication|Recently, Lin and Tsai proposed an image secret sharing scheme with steganography and authentication to prevent participants from the incidental or intentional provision of a false stego-image (an image containing the hidden secret image). However, dishonest participants can easily manipulate the stego-image for successful authentication but cannot recover the secret image, i.e., compromise the steganography. In this paper, we present a scheme to improve authentication ability that prevents dishonest participants from cheating. The proposed scheme also defines the arrangement of embedded bits to improve the quality of stego-image. Furthermore, by means of the Galois Field GF(28), we improve the scheme to a lossless version without additional pixels.
80|7||Efficient self-tuning spin-locks using competitive analysis|Reactive spin-lock algorithms that can automatically adapt to contention variation on the lock have received great attention in the field of multiprocessor synchronization since they can help applications achieve good performance in all possible contention conditions. However, in existing reactive spin-locks the reaction relies on (i) some fixed experimentally tuned thresholds, which may get frequently outdated in dynamic environments like multiprogramming/multiprocessor systems, or (ii) known probability distributions of inputs.
80|7||A communication-efficient and fault-tolerant conference-key agreement protocol with forward secrecy|
80|7||A context-aware cache structure for mobile computing environments|This paper proposes a cache management method that maintains a mobile terminal’s cache content by prefetching data items with maximum benefit and evicting cache data entries with minimum benefit. The data item benefit is evaluated based on the user’s query context which is defined as a set of constraints (predicates) that define both the movement pattern and the information context requested by the mobile user. A context-aware cache is formed and maintained using a set of neighboring locations (called the prime list) that are restricted by the validity of the data fetched from the server. Simulation results show that the proposed strategy, using different levels of granularity, can greatly improve system performance in terms of the cache hit ratio.
80|7||An empirical study of the bad smells and class error probability in the post-release object-oriented system evolution|Bad smells are used as a means to identify problematic classes in object-oriented systems for refactoring. The belief that the bad smells are linked with problematic classes is largely based on previous metric research results. Although there is a plethora of empirical studies linking software metrics to errors and error proneness of classes in object-oriented systems, the link between the bad smells and class error probability in the evolution of object-oriented systems after the systems are released has not been explored. There has been no empirical evidence linking the bad smells with class error probability so far. This paper presents the results from an empirical study that investigated the relationship between the bad smells and class error probability in three error-severity levels in an industrial-strength open source system. Our research, which was conducted in the context of the post-release system evolution process, showed that some bad smells were positively associated with the class error probability in the three error-severity levels. This finding supports the use of bad smells as a systematic method to identify and refactor problematic classes in this specific context.
80|7||Logic synthesis for PAL-based CPLD-s based on two-stage decomposition|A PAL-based (PAL – Programmable Array Logic) logic block is the core of a great majority of contemporary CPLD (Complex Programmable Logic Device) circuits. The purpose of the paper is to present a novel method of two-stage decomposition dedicated for PAL-based CPLD-s. The key point of the algorithm lies in sequential search for a decomposition providing feasibility of implementation of the free block in one PAL-based logic block containing a limited number of product terms. The proposed method is an alternative to the classical approach, based on two-level minimisation of separate single-output functions. An original method of determining the row multiplicity of the partition matrix is presented. For this purpose a new concept of graph is proposed – the Row Incompatibility and Complement Graph. An appropriate algorithm of the Row Incompatibility and Complement Graph colouring is presented. On the basis of row multiplicity evaluated for individual partitionings, the partitioning which provides minimisation of the bound block is chosen. Results of the experiments, which are also presented, prove that the proposed method leads to significant reduction of chip area in relation to the classical approach, especially for CPLD structures, that consist of PAL-based blocks containing 2i (a power of 2) product terms. The proposed method was also compared with decomposition algorithms presented in another works. The results lead to a conclusion, that the proposed two-stage PAL decomposition is especially attractive with respect to the number of logic levels obtained.
80|7||Synchronization modeling and its application for SMIL2.0 presentations|
80|7||A framework for the static verification of api calls|A number of tools can statically check program code to identify commonly encountered bug patterns. At the same time, programs are increasingly relying on external apis for performing the bulk of their work: the bug-prone program logic is being fleshed-out, and many errors involve tricky subroutine calls to the constantly growing set of external libraries. Extending the static analysis tools to cover the available apis is an approach that replicates scarce human effort across different tools and does not scale. Instead, we propose moving the static api call verification code into the api implementation, and distributing the verification code together with the library proper. We have designed a framework for providing static verification code together with Java classes, and have extended the FindBugs static analysis tool to check the corresponding method invocations. To validate our approach we wrote verification tests for 100 different methods, and ran FindBugs on 6.9 million method invocations on what amounts to about 13 million lines of production-quality code. In the set of 55 thousand method invocations that could potentially be statically verified our approach identified 800 probable errors.
80|7||An evaluation of the middlewareâs impact on the performance of object oriented distributed systems|In this paper, we present a performance analysis of the response time of a client–server e-banking application running over three different enterprise middleware platforms, namely HTTPServlets, RMI and Web services with JAX-RPC. We conducted performance testing with the purpose to reveal the specific characteristics of each middleware technology and the impact that they infer on the distributed application’s performance. A server node running the three J2EE platforms was benchmarked over a wide array of intranet usage patterns. A statistical analysis of the collected data led into conclusions regarding the benefits of each middleware technology. The simulation framework can be further extended to become a testing tool able to differentiate on various service demand classes as an input in distributed applications so as to offer first-cut validated results concerning the systems’ performance.
80|8|http://www.sciencedirect.com/science/journal/01641212/80/8|Reflections on the influences of the COCOMO, spiral and the Win-Win models on software project and risk management|
80|8||Impact and contributions of MBASE on software engineering graduate courses|As the founding Director of the Center for Software Engineering, Professor Barry Boehm developed courses that have greatly impacted the education of software engineering students. Through the use of the MBASE framework and complementary tools, students have been able to obtain real-life software development experience without leaving campus. Project team clients and the universities have also benefited. This paper provides evidence on the impact of Dr. Boehm’s frameworks on courses at two universities, and identifies major contributions to software engineering education and practice.
80|8||Making every student a winner: The WinWin approach in software engineering education|This paper shows how Theory-W and the WinWin requirements negotiation approach are used in software engineering education at several universities in the US, Europe, and Asia. We briefly describe Theory-W, the WinWin negotiation model, available processes, and tool support. We then discuss how students can benefit from WinWin in their software engineering education. We explore different options for teaching the approach and present concrete examples and experiences from the different universities.
80|8||The influence of COCOMO on software engineering education and training|As the discipline of software engineering has matured, COCOMO (constructive cost model) has evolved, both in response to and as a leading indicator of changes in software engineering methods and techniques. This paper traces the evolution of the COCOMO cost estimation models as they have evolved from 1981 to 2005. In particular, COCOMO 81, Ada COCOMO, and COCOMO II are presented. COCOMO has been, and continues to be a vehicle for introducing and illustrating software engineering methods and techniques. Emphasis is placed on the role COCOMO models have played, and continue to play, in software engineering education and training.
80|8||Reflections on 10 years of sponsored senior design projects: Students winâclients win!|
80|8||Experience teaching Barry Boehmâs techniques in industrial and academic settings|This paper discusses the author’s twenty five years of experience teaching Dr. Boehm’s techniques in software estimating, software risk management, and other aspects of software project management. This experience has occurred in both industrial and academic settings. Dr. Boehm’s techniques have proven to be robust, well suited for teaching purposes, straightforward to implement, and practical enough that many students reported successful application in the workplace.
80|8||Leadership by example: A perspective on the influence of Barry Boehm|Over the course of the past 10 years of working with Dr. Boehm on various projects has both influenced the software engineering program at Mississippi State University as well as provided growth opportunities and expansion of the MSU ABET accredited software engineering undergraduate degree program. Looking back over the key interactions with him, it is apparent that he leads (and influences) by his example, his work ethic, and his intellect in the software engineering field. This paper provides insights into his specific influences though collaborative work with another university.
80|8||Statistical models vs. expert estimation for fault prediction in modified code â an industrial case study|Statistical fault prediction models and expert estimations are two popular methods for deciding where to focus the fault detection efforts when the fault detection budget is limited. In this paper, we present a study in which we empirically compare the accuracy of fault prediction offered by statistical prediction models with the accuracy of expert estimations. The study is performed in an industrial setting. We invited eleven experts that are involved in the development of two large telecommunication systems. Our statistical prediction models are built on historical data describing one release of one of those systems. We compare the performance of these statistical fault prediction models with the performance of our experts when predicting faults in the latest releases of both systems. We show that the statistical methods clearly outperform the expert estimations. As the main reason for the superiority of the statistical models we see their ability to cope with large datasets. This makes it possible for statistical models to perform reliable predictions for all components in the system. This also enables prediction at a more fine-grain level, e.g., at the class instead of at the component level. We show that such a prediction is better both from the theoretical and from the practical perspective.
80|8||Fine-grain analysis of common coupling and its application to a Linux case study|Common coupling (sharing global variables across modules) is widely accepted as a measure of software quality and maintainability; a low level of common coupling is necessary (but not sufficient) to ensure maintainability. But when the global variables in question are large multi-field data structures, one must decide whether to consider such data structures as single units, or examine each of their fields individually. We explore this issue by re-analyzing a case study based on the Linux operating system. We determine the common coupling at the level of granularity of the component fields of large, complex data structures, rather than at the level of the data structures themselves, as in previous work. We claim that this is the appropriate level of analysis based on how such data structures are used in practice, and also that such a study is required due to concern that coarse-grained analysis leads to false coupling. We find that, for this case study, the granularity does not have a decisive effect on the results. In particular, our results for coupling based on individual fields are similar in spirit to the results reported previously (by others) based on using complete data structures. In both cases, the coupling indicates that the system kernel is vulnerable to modifications in peripheral modules of the system.
80|8||Verification method of dataflow algorithms in high-level synthesis|This paper presents a formal verification algorithm using the Petri Net theory to detect design errors for high-level synthesis of dataflow algorithms. Typically, given a dataflow algorithm and a set of architectural constraints, the high-level synthesis performs algorithmic transformation and produces the optimal scheduling. How to verify the correctness of high-level synthesis becomes a key issue before mapping the synthesis results onto a silicon. Many tools exist for RTL (Register Transfer Level) design, but few for high-level synthesis. Instead of applying Boolean algebra, this paper adopts the Petri Net theory to verify the correctness of the synthesis result, because the Petri Net model has the nature of dataflow algorithms. Herein, we propose three approaches to realize the Petri Net based formal verification algorithm and conclude the best one who outperforms the others in terms of processing speed and resource usage.
80|8||An automated approach to specification animation for validation|Formal specification has been increasingly adopted for the development of software systems of the highest integrity. However, the readability of specifications for large-scale and complex systems can be so poor that even the developers may not easily understand whether their specifications define the “intended behaviors”. In this paper, we describe a software tool that supports the animation of specifications by simulating their functional scenarios using the Message Sequence Chart (MSC). The tool extracts automatically functional scenarios from a specification and generates a message sequence chart for each of them for a syntactic level analysis. The tool can also execute a functional scenario with test cases for a semantic level analysis if all the processes involved in the scenario are defined using explicit specifications. With the tool support the animation of a specification can be carried out incrementally to assist its user to review the adequacy of the specification. We present a case study applying the tool to animate a formal specification for a library system and evaluate its result.
80|8||Practical experience of eliciting classes from use case descriptions|In moving from requirements analysis to design, use cases are often recommended as the starting point for the derivation of classes. However, exactly how classes are to be found within the use case is not entirely obvious. Typical approaches suggest a simple noun/verb search or brainstorming. Recent work is moving towards an interrogation of the use case diagram as a means of validation and of the description (and scenario) to elicit objects in the problem domain. This paper presents a set of Elicitation Questions that enables the interrogation of descriptions from the perspectives of specification, software architecture and design. This qualitative ‘interrogation’ teases out design issues. The Elicitation Questions were trialled through application to a real industrial project at a financial services company. Feedback from practitioners shows that the Elicitation Questions are important in raising design and testing issues from the use case descriptions but the organisational culture in how software is developed would impact its uptake.
80|8||Communication support for systems engineering â process modelling and animation with APRIL|The most important task in the early stages of systems engineering is the building of models which capture the relevant knowledge of a given application domain. A working communication with domain experts who possess this knowledge is crucial, since misunderstandings almost always lead to expensive system redesigns in later development stages. In this context, especially the modelling of systems behaviour is a challenging problem. While formally based languages in this area are often too difficult to understand for domain experts, more informal languages frequently lack animation support for dynamic process visualizations. Out of this, an easy to understand and semi-formal visual modelling language which allows for process animations is needed in order to improve communication in systems engineering. If the use of such a language leads to an earlier identification of conceptualization flaws, the overall costs of systems development may be significantly reduced.
80|8||Evaluating performances of pair designing in industry|
80|8||Adaptive software testing with fixed-memory feedback|Adaptive software testing is the counterpart of adaptive control in software testing. It means that software testing strategy should be adjusted on-line by using the testing data collected during software testing as our understanding of the software under test is improved. In this paper we propose a new strategy of adaptive software testing in the context of software cybernetics. This new strategy employs fixed-memory feedback for on-line parameter estimations and is intended to circumvent the drawbacks of the assumption that all remaining defects are equally detectable at constant rate and to reduce the underlying computational complexity of on-line parameter estimations. A comprehensive case study with the Space program demonstrates that the new adaptive testing strategy can really work in practice and may noticeably outperform the purely-random testing strategy and the random-partition testing strategy (or collectively, the random testing strategies) in terms of the number of tests used to detect and remove a given number of defects in a single process of software testing and the corresponding standard deviation. In addition, the case study shows that the input domain of the software under test should be partitioned non-evenly for the adaptive testing strategy.
80|8||Predicting object-oriented software maintainability using multivariate adaptive regression splines|Accurate software metrics-based maintainability prediction can not only enable developers to better identify the determinants of software quality and thus help them improve design or coding, it can also provide managers with useful information to help them plan the use of valuable resources. In this paper, we employ a novel exploratory modeling technique, multiple adaptive regression splines (MARS), to build software maintainability prediction models using the metric data collected from two different object-oriented systems. The prediction accuracy of the MARS models are evaluated and compared using multivariate linear regression models, artificial neural network models, regression tree models, and support vector models. The results suggest that for one system MARS can predict maintainability more accurately than the other four typical modeling techniques, and that for the other system MARS is as accurate as the best modeling technique.
80|8||The design and implementation of an application program interface for securing XML documents|The encryption and signature standards proposed by W3C specifying the format for encrypted XML documents are important advances towards XML security [Eastlake, Donald, Reagle, Joseph, Imamura, Takeshi, Dillaway, Blair, Simon, Ed, 2002. XML Encryption Syntax and Processing. W3C Recommendation 10 December 2002 <http://www.w3.org/TR/xmlenc-core/>, Eastlake, Donald, Reagle, Joseph, Solo, David, Bartel, Mark, Boyer, John, Fox, Barb, LaMacchia, Brian, Simon, Ed, 2002. XML-Signature Syntax and Processing W3C Recommendation, 12 February 2002. <http://www.w3.org/TR/xmldsig-core/>]. Related works include the proposal of a specification language that allows a programmer to describe the security details of XML documents [Hwang, Gwan-Hwan, Chang, Tao-Ku, 2004. An operational model and language support for securing XML documents. Computers & Security 23(6), 498–529, Hwang, Gwan-Hwan, Chang, Tao-Ku, 2001. Document security language (DSL) and an efficient automatic securing tool for XML documents. International Conference on Internet Computing, Las Vegas, Nevada, USA, 24–28 June 2001, pp. 393–399]. Despite the success of these works, we consider them to be insufficient from the viewpoint of software engineering. In this paper, we employ some real examples to demonstrate that it is necessary to design an appropriate API for the securing system of subtree encryption for XML documents. The goal is to increase productivity and reduce the cost of maintaining this kind of software, for which we propose a document security language (DSL) API. We describe the implementation of the DSL API, and use experimental results to demonstrate its practicality.
80|8||Towards efficient web engineering approaches through flexible process models|After more than a decade of web developments and some deafening fiascos, it has become clear that it is not possible to face the development of large scale web systems without following a systematic and well-defined process to guarantee quality, measurability, maintainability and reusability. There are a number of hypermedia/web engineering methods that provide mechanisms to specify the product requirements, including those concerning structure, navigation, interaction, presentation and access. But apart from product requirements there are also process requirements which in the web arena are constantly changing. Hence, to be put in practice without disturbing the project goals nor compromising its success, methods have to rely on empirical and flexible process models that can be easily adapted to fit process requirements. Little attention has been paid to the process model in most hypermedia/web methods which usually apply a classical iterative process based on the use of prototypes that are tested with users. In this paper we describe how we did apply the ADM web engineering method following a flexible star life cycle in the context of a specific web project highlighting the main benefits of this approach. In particular we will describe the different cycles applied in the ARCE project, illustrating the application of a usability engineering life cycle in a real case.
80|8||Pounamu: A meta-tool for exploratory domain-specific visual language tool development|Domain-specific visual language tools have become important in many domains of software engineering and end user development. However building such tools is very challenging with a need for multiple views of information and multi-user support, the ability for users to change tool diagram and meta-model specifications while in use, and a need for an open architecture for tool integration. We describe Pounamu, a meta-tool for realising such visual design environments. We describe the motivation for Pounamu, its architecture and implementation and illustrate examples of domain-specific visual language tools that we have developed with Pounamu.
80|8||Model-based user interface engineering with design patterns|The main idea surrounding model-based UI (User Interface) development is to identify useful abstractions that highlight the core aspects and properties of an interactive system and its design. These abstractions are instantiated and iteratively transformed at different level to create a concrete user interface. However, certain limitations prevent UI developers from adopting model-based approaches for UI engineering. One such limitation is the lack of reusability of best design practices and knowledge within such approaches. With a view to fostering reuse in the instantiation and transformation of models, we introduce patterns as building blocks, which can be first used to construct different models and then instantiated into concrete UI artefacts. In particular, we will demonstrate how different kinds of patterns can be used as modules for establishing task, dialog, presentation and layout models. Starting from an outline of the general process of pattern application, an interface for combining patterns and a possible formalization are suggested. The Task Pattern Wizard, an XML/XUL-based tool for selecting, adapting and applying patterns to task models, will be presented. In addition, an extended example will illustrate the intimate complicity of several patterns and the proposed model-driven approach.
80|9|http://www.sciencedirect.com/science/journal/01641212/80/9|Introduction to special section on Evaluation and Assessment in Software Engineering EASE06|
80|9||Experiences using systematic review guidelines|Systematic review is a method to identify, assess and analyse published primary studies to investigate research questions. We critique recently published guidelines for performing systematic reviews on software engineering, and comment on systematic review generally with respect to our experience conducting one. Overall we recommend the guidelines. We recommend researchers clearly and narrowly define research questions to reduce overall effort, and to improve selection and data extraction. We suggest that “complementary” research questions can help clarify the main questions and define selection criteria. We show our project timeline, and discuss possibilities for automating and increasing the acceptance of systematic review.
80|9||Establishing and maintaining trust in software outsourcing relationships: An empirical investigation|Our research objective is to understand software outsourcing practitioners’ perceptions of the role of trust in managing client–vendor relationships and the factors that are critical to trust in off-shore software outsourcing relationships. Participants were 12 Vietnamese software development practitioners developing software for Far Eastern, European, and American clients. They identified that cultural understanding, creditability, capabilities, and personal visits are important factors in gaining the initial trust of a client, while cultural understanding, communication strategies, contract conformance, and timely delivery are vital factors in maintaining that trust. We contrast Vietnamese and Indian practitioners’ views on factors affecting trust relationships.
80|9||Ranking reusability of software components using coupling metrics|This paper provides an account of new static measures of coupling developed to assess the reusability of Java components retrieved from the internet by a search engine. These measures differ from the majority of established metrics in three respects: they take account of indirect coupling, they reflect the degree to which two classes are coupled, and they take account of the functional complexity of classes. An empirical comparison of the new measures with six established coupling metrics is described. The new measures are shown to be consistently superior at ranking components according to their reusability.
80|9||Do programmer pairs make different mistakes than solo programmers?|Objective: Comparison of program defects caused by programmer pairs and solo developers.
80|9||Characteristics of software engineers with optimistic predictions|
80|9||SPICE in retrospect: Developing a standard for process assessment|The SPICE Project was established in 1993 to support the development, validation and transition into use of an International Standard for software process assessment. Its efforts have resulted in the publication of a five-part Standard for Process Assessment, ISO/IEC 15504. This paper reviews the evolution of the Standard, and reflects on the parallel achievements of the SPICE Project and the standardisation effort in advancing the state of the art in process assessment and improvement.
80|9||A three-tier knowledge management scheme for software engineering support and innovation|To ensure smooth and successful transition of software innovations to enterprise systems, it is critical to maintain proper levels of knowledge about the system configuration, the operational environment, and the technology in both existing and new systems. We present a three-tier knowledge management scheme through a systematic planning of actions spanning the transition processes in levels from conceptual exploration to prototype development, experimentation, and product evaluation. The three-tier scheme is an integrated effort for bridging the development and operation communities, maintaining stability to the operational performance, and adapting swiftly to software technology innovations. The scheme combines experiences of academic researches and industrial practitioners to provide necessary technical expertise and qualifications for knowledge management in software engineering support (SES) processes.
80|9||Analysing the impact of usability on software design|This paper analyses what implications usability has for software development, paying special attention to the impact of this quality attribute on design. In this context, the aim is twofold. On the one hand, we intend to empirically corroborate that software design and usability are really related. This would mean that this, like other quality attributes, would need to be dealt with no later than at design time to develop usable software at a reasonable cost. On the other hand, we present a possible quantification, calculated from a number of real applications, of the effect of incorporating certain usability features at design time.
80|9||Empirical study of the effects of open source adoption on software development economics|In this paper, we present the results of empirical study of the effects of open source software (OSS) components reuse on software development economics. Specifically, we examined three economic factors – cost, productivity, and quality. This study started with an extensive literature review followed by an exploratory study conducted through interviews with 18 senior project/quality managers, and senior software developers. Then, the result of the literature review and the exploratory study was used to formulate research model, hypotheses, and survey questionnaire. Software intensive companies from Canada and the US were targeted for this study. The period of study was between September 2004 and March 2006. Our findings show that there are strong significant statistical correlations between the factors of OSS components reuse and software development economics. The conclusion from this study shows that software organizations can achieve some economic gains in terms of software development productivity and product quality if they implement OSS components reuse adoption in a systematic way. A big lesson learned in this study is that OSS components are of highest quality and that open source community is not setting a bad example (contrary to some opinion) so far as ‘good practices’ are concerned.
80|9||A software fault tree key node metric|Analysis of software fault trees exposes failure events that can impact safety within safety-critical software product lines. This paper presents a software fault tree key node safety metric for measuring software safety within product lines. Fault tree structures impacting the metric’s composition are provided, and the mathematical basis for the metric is defined. The metric is applied to an embedded control system as well as to a series of experiments expected to either improve or degrade system safety. The effectiveness of the metric is analyzed, and lessons learned during the application of the metric are discussed.
80|9||Comprehension strategies and difficulties in maintaining object-oriented systems: An explorative study|Program comprehension is a major time-consuming activity in software maintenance. Understanding the underlying mechanisms of program comprehension is therefore necessary for improving software maintenance. It has been argued that acquiring knowledge of how a program works before modifying it (the systematic strategy) is unrealistic in larger programs. The goal of the experiment presented in this paper is to explore this claim. The experiment examines strategies for program comprehension and cognitive difficulties of developers who maintain an unfamiliar object-oriented system. The subjects were 38 students in their third or fourth year of study in computer science. They used a professional Java tool to perform several maintenance tasks on a medium-size Java application system in a 6-h long experiment. The results showed that the subjects who applied the systematic strategy were more likely to produce correct solutions. Two major groups of difficulties were related to the comprehension of the application structure, namely to the understanding of GUI implementation and OO comprehension and programming. Acquisition of strategic knowledge might improve program comprehension in software maintenance.
80|9||Efficient approach for restructuring multiple inheritance hierarchies|This paper discusses the restructuring of inheritance hierarchies of classes and introduces a method of restructuring multiple hierarchies of class inheritance, which removes duplicated methods and creates inheritance hierarchies without overridden methods while preserving the behavior of objects. This paper formulates a restructuring problem for 0–1 integer programming and presents a network-based solution method, which uses a distance parameter between every pair of characteristics for similarity metric. This paper presents basic theorems for clustering characteristics and defining of inheritance hierarchy. We create inheritance hierarchies based on the rules for a definition of class relationship. The method is analyzed and compared with the existing method.
80|9||A new research agenda for tool integration|This article highlights tool integration within software engineering environments. Tool integration concerns the techniques used to form coalitions of tools that provide an environment supporting some, or all, activities within a software engineering process. These techniques have been used to create environments that attempt to address aspects of software development, with varying success. This article provides a timely analysis and review of many of the significant projects in the field and, combined with evidence collected from industry, concludes by proposing an empirical manifesto for future research, where we see the need for work to justify tool integration efforts in terms of relevant socio-economic indicators.
80|9||Comments on âA Semantic Web Primerâ, Grigoris Antoniou, Frank Van Harmelen. The MIT Press, Cambridge, Massachusetts, London, England (2004)|
||||
volume|issue|url|title|abstract
81|1|http://www.sciencedirect.com/science/journal/01641212/81/1|Software performance tuning of software product family architectures: Two case studies in the real-time embedded systems domain|Software performance is an important non-functional quality attribute and software performance evaluation is an essential activity in the software development process. Especially in embedded real-time systems, software design and evaluation are driven by the needs to optimize the limited resources, to respect time deadlines and, at the same time, to produce the best experience for end-users. Software product family architectures add additional requirements to the evaluation process. In this case, the evaluation includes the analysis of the optimizations and tradeoffs for the whole products in the family. Performance evaluation of software product family architectures requires knowledge and a clear understanding of different domains: software architecture assessments, software performance and software product family architecture. We have used a scenario-driven approach to evaluate performance and dynamic memory management efficiency in one Nokia software product family architecture. In this paper we present two case studies. Furthermore, we discuss the implications and tradeoffs of software performance against evolvability and maintenability in software product family architectures.
81|1||Description templates for agent-oriented patterns|A number of agent-oriented patterns have been proposed to share agent-oriented software engineering experiences. However, most of the existing descriptions of these patterns are incomprehensive or inconsistent with descriptions of similar patterns by other authors. We seek to improve the communication and comprehension of agent-oriented patterns by improving their descriptions. This paper presents a pattern template structure for defining agent-oriented pattern templates. We used the template structure to define eight agent-oriented pattern templates. We describe the elements of these eight templates. We illustrate the expressiveness of these templates by using three of them to describe three existing agent-oriented patterns.
81|1||DRAMA: A framework for domain requirements analysis and modeling architectures in software product lines|One of the benefits of software product line approach is to improve time-to-market. The changes in market needs cause software requirements to be flexible in product lines. Whenever software requirements are changed, software architecture should be evolved to correspond with them. Therefore, domain architecture should be designed based on domain requirements. It is essential that there is traceability between requirements and architecture, and that the structure of architecture is derived from quality requirements. The purpose of this paper is to provide a framework for modeling domain architecture based on domain requirements within product lines. In particular, we focus on the traceable relationship between requirements and architectural structures. Our framework consists of processes, methods, and a supporting tool. It uses four basic concepts, namely, goal based domain requirements analysis, Analytical Hierarchy Process, Matrix technique, and architecture styles. Our approach is illustrated using HIS (Home Integration System) product line. Finally, industrial examples are used to validate DRAMA.
81|1||Dynamic interval-based labeling scheme for efficient XML query and update processing|XML data can be represented by a tree or graph structure and XML query processing requires the information of structural relationships among nodes. The basic structural relationships are parent–child and ancestor–descendant, and finding all occurrences of these basic structural relationships in an XML data is clearly a core operation in XML query processing. Several node labeling schemes have been suggested to support the determination of ancestor–descendant or parent–child structural relationships simply by comparing the labels of nodes. However, the previous node labeling schemes have some disadvantages, such as a large number of nodes that need to be relabeled in the case of an insertion of XML data, huge space requirements for node labels, and inefficient processing of structural joins. In this paper, we propose the nested tree structure that eliminates the disadvantages and takes advantage of the previous node labeling schemes. The nested tree structure makes it possible to use the dynamic interval-based labeling scheme, which supports XML data updates with almost no node relabeling as well as efficient structural join processing. Experimental results show that our approach is efficient in handling updates with the interval-based labeling scheme and also significantly improves the performance of the structural join processing compared with recent methods.
81|1||Utilizing venation features for efficient leaf image retrieval|Most Content-Based Image Retrieval systems use image features such as textures, colors, and shapes. However, in the case of a leaf image, it is not appropriate to rely on color or texture features only as such features are very similar in most leaves. In this paper, we propose a new and effective leaf image retrieval scheme. In this scheme, we first analyze leaf venation which we use for leaf categorization. We then extract and utilize leaf shape features to find similar leaves from the already categorized group in a leaf database. The venation of a leaf corresponds to the blood vessels in organisms. Leaf venations are represented using points selected by a curvature scale scope corner detection method on the venation image. The selected points are then categorized by calculating the density of feature points using a non-parametric estimation density. We show this technique’s effectiveness by performing several experiments on a prototype system.
81|1||DR-TCP: Downloadable and reconfigurable TCP|Advances in communication technology allow a variety of new network environments and services available very rapidly. Appearance of various network environments tends to enable a user with a mobile terminal to access among different network simultaneously. However, since new network environment affects performance of communication protocols, terminal systems should provide adaptation schemes for the protocols in order to keep the quality of network performance high. A possible solution is to make the protocol reconfigurable to be adapted to current network environment. Unfortunately, because most existing network systems are implemented monolithically, they cannot support protocol reconfiguration dynamically at runtime.
81|1||Under storage constraints of epidemic backup node selection using HyMIS architecture for data replication in mobile peer-to-peer networks|The attainment of high reliability and availability is very difficult to be achieved in very complex wireless infrastructureless networks. Reliability concept describes essentially the transmission characteristics of infrastructureless networks, such as packet loss probability, packet duplication, data misinsertion, and corruption of packets. Some other metrics nowadays, aggregate and contribute to the aggravation of end to end reliability. In this work a new scheme for end to end reliable file/resource sharing is studied, among mobile peer-to-peer users. The proposed scheme uses the Hybrid Mobile Infostation System (HyMIS) to maintain and enhance the reliability of file/resource sharing process among wireless devices. Under various storage constraints the epidemic backup node selection is adopted, merging the advantages of epidemic file dissemination through purely mobile Infostations, using the HyMIS architecture. Examination through simulation is performed, taking into account many newly introduced storage metrics, for the performance evaluation of the proposed scheme. These storage metrics are tuned into certain bounded values to enable high packet delivery ratio. Results show that this scheme under certain storage requirements offer a reliable and robust solution for sharing resources of any capacity in dynamic mobile peer-to-peer wireless environments.
81|1||Bidder-anonymous English auction scheme with privacy and public verifiability|This work studies the English auction protocol, which comprises three interactive parties—the Registration Manager, the Auction Manager and the Bidder. The registration manager confirms and authenticates the identities of bidders; the auction manager issues the bidding rights and maintains order in holding the auction. The proposed scheme provides the following security features—anonymity, traceability, no framing, unforgeability, non-repudiation, fairness, public verifiability, non-linkability among various auction rounds, linkability within a single auction round, bidding efficiency, single registration, and easy revocation. The scheme developed herein can effectively reduce the load on the registration and auction managers by requiring the end server to derive the key. It also eliminates the need for bidders to download the auction key and the auction certificate. Hence, the time complexity of processing data is clearly reduced and the best interests of the bidders can be achieved. Accordingly, the scheme is consistent with the actual practice of online transactions.
81|1||A parametrized algorithm that implements sequential, causal, and cache memory consistencies|In this paper, we present an algorithm that can be used to implement sequential, causal, or cache consistency in distributed shared memory (DSM) systems. For this purpose it includes a parameter that allows us to choose the consistency model to be implemented. If all processes run the algorithm with the same value in this parameter, the corresponding consistency is achieved. (Additionally, the algorithm tolerates that processes use certain combination of parameter values.) This characteristic allows a concrete consistency model to be chosen, but implements it with the more efficient algorithm in each case (depending of the requirements of the applications). Additionally, as far as we know, this is the first algorithm proposed that implements cache coherence.
81|1||Solving a real-time allocation problem with constraint programming|In this paper, we present an original approach (CPRTA for “Constraint Programming for solving Real-Time Allocation”) based on constraint programming to solve a static allocation problem of hard real-time tasks. This problem consists in assigning periodic tasks to distributed processors in the context of fixed priority preemptive scheduling. CPRTA is built on dynamic constraint programming together with a learning method to find a feasible processor allocation under constraints. Two efficient new approaches are proposed and validated with experimental results. Moreover, CPRTA exhibits very interesting properties. It is complete (if a problem has no solution, the algorithm is able to prove it); it is non-parametric (it does not require specific tuning) thus allowing a large diversity of models to be easily considered. Finally, thanks to its capacity to explain failures, it offers attractive perspectives for guiding the architectural design process.
81|1||A high quality steganographic method with pixel-value differencing and modulus function|In this paper, we shall propose a new image steganographic technique capable of producing a secret-embedded image that is totally indistinguishable from the original image by the human eye. In addition, our new method avoids the falling-off-boundary problem by using pixel-value differencing and the modulus function. First, we derive a difference value from two consecutive pixels by utilizing the pixel-value differencing technique (PVD). The hiding capacity of the two consecutive pixels depends on the difference value. In other words, the smoother area is, the less secret data can be hidden; on the contrary, the more edges an area has, the more secret data can be embedded. This way, the stego-image quality degradation is more imperceptible to the human eye. Second, the remainder of the two consecutive pixels can be computed by using the modulus operation, and then secret data can be embedded into the two pixels by modifying their remainder. In our scheme, there is an optimal approach to alter the remainder so as to greatly reduce the image distortion caused by the hiding of the secret data. The values of the two consecutive pixels are scarcely changed after the embedding of the secret message by the proposed optimal alteration algorithm. Experimental results have also demonstrated that the proposed scheme is secure against the RS detection attack.
81|10|http://www.sciencedirect.com/science/journal/01641212/81/10|Message from the guest editors|
81|10||An agent-based metric for quality of services over wireless networks|In a wireless LAN environment, clients tend to associate with the nearest access point (AP) which usually provides the strongest signal. However, this does not guarantee that users will receive the best quality of service (QoS) if the population sharing the network capacity were not considered. In other words, within the same access point, the more the population is, the less bandwidth each user will share, and the worse the quality of service will be. In this paper, we proposed an anticipative agent assistance (AAA) which is an agent-based metric for evaluating and managing the resource information of the wireless access points, computing the potential AP list, and providing clients with resource information of APs. We also propose a novel QoS feedback mechanism which allows users to promptly adjust the service quality with AAA according to the throughput and delay requirements. We evaluate the performance of our proposed method using the ns-2 simulator. Numerical results show that AAA achieves: (1) reduce the transmission delay, (2) increase the throughput, (3) improve the network utilization, (4) accommodate more users to access the networks, and (5) achieve load-balancing. Our metric is implementation feasible in various IEEE WLAN environments.
81|10||CVM â A communication virtual machine|The convergence of data, voice, and multimedia communication over digital networks, coupled with continuous improvement in network capacity and reliability has resulted in a proliferation of communication technologies. Unfortunately, despite these new developments, there is no easy way to build new application-specific communication services. The stovepipe approach used today for building new communication services results in rigid technology, limited utility, lengthy and costly development cycle, and difficulty in integration. In this paper, we introduce communication virtual machine (CVM) that supports rapid conception, specification, and automatic realization of new application-specific communication services through a user-centric, model-driven approach. We present the concept, architecture, modeling language, prototypical design, and implementation of CVM in the context of a healthcare application.
81|10||Specification, decomposition and agent synthesis for situation-aware service-based systems|Service-based systems are distributed computing systems with the major advantage of enabling rapid composition of distributed applications, such as collaborative research and development, e-business, health care, military applications and homeland security, regardless of the programming languages and platforms used in developing and running various components of the applications. In dynamic service-oriented computing environment, situation awareness (SAW) is needed for system monitoring, adaptive service coordination and flexible security policy enforcement. To greatly reduce the development effort of SAW capability in service-based systems and effectively support runtime system adaptation, it is necessary to automate the development of reusable and autonomous software components, called SAW agents, for situation-aware service-based systems. In this paper, a logic-based approach to declaratively specifying SAW requirements, decomposing SAW specifications for efficient distributed situation analysis, and automated synthesis of SAW agents is presented. This approach is based on AS3 calculus and logic, and our declarative model for SAW. Evaluation results of our approach are also presented.
81|10||A backtracking search tool for constructing combinatorial test suites|Combinatorial testing is an important testing method. It requires the test cases to cover various combinations of parameters of the system under test. The test generation problem for combinatorial testing can be modeled as constructing a matrix which has certain properties. This paper first discusses two combinatorial testing criteria: covering array and orthogonal array, and then proposes a backtracking search algorithm to construct matrices satisfying them. Several search heuristics and symmetry breaking techniques are used to reduce the search time. This paper also introduces some techniques to generate large covering array instances from smaller ones. All the techniques have been implemented in a tool called EXACT (EXhaustive seArch of Combinatorial Test suites). A new optimal covering array is found by this tool.
81|10||Software engineering article types: An analysis of the literature|The software engineering (SE) community has recently recognized that the field lacks well-established research paradigms and clear guidance on how to write good research reports. With no comprehensive guide to the different article types in the field, article writing and reviewing heavily depends on the expertise and the understanding of the individual SE actors.
81|10||XML security â A comparative literature review|Since the turn of the millenium, working groups of the W3C have been concentrating on the development of XML-based security standards, which are paraphrased as XML security. XML security consists of three recommendations: XML (digital) signature, XML encryption and XML key management specification (XKMS), all of them published by the W3C.
81|10||Facilitating software extension with design patterns and Aspect-Oriented Programming|Software products, especially large applications, need to continuously evolve, in order to adapt to the changing environment and updated requirements. With both the producer and the customer unwilling to replace the existing application with a completely new one, adoption of design constructs and techniques which facilitate the application extension is a major design issue. In the current work we investigate the behavior of an object-oriented software application at a specific extension scenario, following three implementation alternatives with regards to a certain design problem relevant to the extension. The first alternative follows a simplistic solution, the second makes use of a design pattern and the third applies Aspect-Oriented Programming techniques to implement the same pattern. An assessment of the three alternatives is attempted, both on a qualitative and a quantitative level, by identifying the additional design implications needed to perform the extension and evaluating the effect of the extension on several quality attributes of the application.
81|10||Early quality monitoring in the development of real-time reactive systems|The increasing trend toward complex software systems has highlighted the need to incorporate quality requirements earlier in the development cycle. We propose a new methodology for monitoring quality in the earliest phases of real-time reactive system (RTRS) development. The targeted quality characteristics are functional complexity, performance, reliability, architectural complexity, maintainability, and test coverage. All these characteristics should be continuously monitored throughout the RTRS development cycle, to provide decision support and detect the first signs of low or decreasing quality as the system design evolves. The ultimate goal of this methodology is to assist developers in dealing with complex user requirements and ensure that the formal development process yields a high-quality application. Each aspect of quality monitoring is formalized mathematically and illustrated using a train–gate–controller case study.
81|10||A framework for QoS-aware binding and re-binding of composite web services|QoS-aware dynamic binding of composite services provides the capability of binding each service invocation in a composition to a service chosen among a set of functionally equivalent ones to achieve a QoS goal, for example minimizing the response time while limiting the price under a maximum value.
81|10||An incremental analysis for resource conflicts to workflow specifications|Workflow management technology helps modulizing and controlling complex business processes within an enterprise. Generally speaking, a workflow management system (WfMS) is composed of two primary components, a design environment and a run-time system. Structural, timing and resource verifications of a workflow specification are required to assure the correctness of the specified system. In this paper, an incremental methodology is constructed to analyze resource consistency and temporal constraints after each edit unit defined on a workflow specification. The methodology introduces several algorithms for general and temporal analyses. The output returned right away can improve the judgment and thus the speed and quality on designing.
81|10||Evolution support mechanisms for software product line process|Software product family process evolution needs specific support for incremental change. Product line process evolution involves in addition to identifying new requirements the building of a meta-process describing the migration from the old process to the new one. This paper presents basic mechanisms to support software product line process evolution. These mechanisms share four strategies – change identification, change impact, change propagation, and change validation. It also examines three kinds of evolution processes – architecture, product line, and product. In addition, change management mechanisms are identified. Specifically we propose support mechanisms for static local entity evolution and complex entity evolution including transient evolution process. An evolution model prototype based on dependency relationships structure of the various product line artifacts is developed.
81|11|http://www.sciencedirect.com/science/journal/01641212/81/11|Systematic Component-Oriented development with Axiomatic Design|A form of design guidance is offered through a process that applies Axiomatic Design Theory to Component-Orientation. Axiomatic Design has been proposed by Do and Suh for object-oriented software development. Our approach leverages divide-and-conquer and find-integrate techniques that support service-based development as an alternative to developing code from scratch. Using this process, missing or conflicting components can be identified, and missing components can be defined. The effectiveness of our proposed system is demonstrated through an example based on High Level Architecture (HLA) simulations. Our Component-Oriented approach utilizing axiomatic design theory has been adapted to HLA Federation Development and Execution Process (FEDEP). As one of the Component-Oriented approaches proposed, FEDEP is able to obtain interoperability and reusability of available components, namely federates.
81|11||An e-contracting reference architecture|Business-to-business e-contracting aims at automating the contracting process between companies. It improves the efficiency of the contracting process and enables the introduction of new business models that can be supported by companies. For the development of an e-contracting system, an architecture is required that describes the system components and the communication channels between them. This paper presents a reference architecture for the development of e-contracting systems. The architecture is designed on the basis of a requirement analysis of e-contracting systems. Established architectural principles are used in its design. The architecture can serve as a foundation in the analysis and design of concrete architectures of e-contracting systems. Furthermore, it can be used as a standardization model that facilitates system integration and communication of ideas. Its value for both software architects and business professionals makes it an important tool in the analysis and implementation of e-contracting systems.
81|11||Reconciling usability and interactive system architecture using patterns|Traditional interactive system architectures such as MVC [Goldberg, A., 1984. Smaltalk-80: The Interactive Programming Environment, Addison-Wesley Publ.] and PAC [Coutaz, J., 1987. PAC, an implementation model for dialog design. In: Interact’87, Sttutgart, September 1987, pp. 431–436; Coutaz, J., 1990. Architecture models for interactive software: faillures and trends. In: Cockton, G. (Ed.), Engineering for Human–Computer Interaction, Elsevier Science Publ., pp. 137–153.] decompose the system into subsystems that are relatively independent, thereby allowing the design work to be partitioned between the user interfaces and underlying functionalities. Such architectures extend the independence assumption to usability, approaching the design of the user interface as a subsystem that can designed and tested independently from the underlying functionality. This Cartesian dichotomy can be fallacious, as functionalities buried in the application’s logic can sometimes affect the usability of the system. Our investigations model the relationships between internal software attributes and externally visible usability factors. We propose a pattern-based approach for dealing with these relationships. We conclude by discussing how these patterns can lead to a methodological framework for improving interactive system architectures, and how these patterns can support the integration of usability in the software design process.
81|11||Software development cost estimation using wavelet neural networks|Software development has become an essential investment for many organizations. Software engineering practitioners have become more and more concerned about accurately predicting the cost and quality of software product under development. Accurate estimates are desired but no model has proved to be successful at effectively and consistently predicting software development cost. In this paper, we propose the use of wavelet neural network (WNN) to forecast the software development effort. We used two types of WNN with Morlet function and Gaussian function as transfer function and also proposed threshold acceptance training algorithm for wavelet neural network (TAWNN). The effectiveness of the WNN variants is compared with other techniques such as multilayer perceptron (MLP), radial basis function network (RBFN), multiple linear regression (MLR), dynamic evolving neuro-fuzzy inference system (DENFIS) and support vector machine (SVM) in terms of the error measure which is mean magnitude relative error (MMRE) obtained on Canadian financial (CF) dataset and IBM data processing services (IBMDPS) dataset. Based on the experiments conducted, it is observed that the WNN-Morlet for CF dataset and WNN-Gaussian for IBMDPS outperformed all the other techniques. Also, TAWNN outperformed all other techniques except WNN.
81|11||The effectiveness of software metrics in identifying error-prone classes in post-release software evolution process|Many empirical studies have found that software metrics can predict class error proneness and the prediction can be used to accurately group error-prone classes. Recent empirical studies have used open source systems. These studies, however, focused on the relationship between software metrics and class error proneness during the development phase of software projects. Whether software metrics can still predict class error proneness in a system’s post-release evolution is still a question to be answered. This study examined three releases of the Eclipse project and found that although some metrics can still predict class error proneness in three error-severity categories, the accuracy of the prediction decreased from release to release. Furthermore, we found that the prediction cannot be used to build a metrics model to identify error-prone classes with acceptable accuracy. These findings suggest that as a system evolves, the use of some commonly used metrics to identify which classes are more prone to errors becomes increasingly difficult and we should seek alternative methods (to the metric-prediction models) to locate error-prone classes if we want high accuracy.
81|11||Automatic, evolutionary test data generation for dynamic software testing|This paper proposes a dynamic test data generation framework based on genetic algorithms. The framework houses a Program Analyser and a Test Case Generator, which intercommunicate to automatically generate test cases. The Program Analyser extracts statements and variables, isolates code paths and creates control flow graphs. The Test Case Generator utilises two optimisation algorithms, the Batch-Optimistic (BO) and the Close-Up (CU), and produces a near to optimum set of test cases with respect to the edge/condition coverage criterion. The efficacy of the proposed approach is assessed on a number of programs and the empirical results indicate that its performance is significantly better compared to existing dynamic test data generation methods.
81|11||A framework to support the evaluation, adoption and improvement of agile methods in practice|Agile methods are often seen as providing ways to avoid overheads typically perceived as being imposed by traditional software development environments. However, few organizations are psychologically or technically able to take on an agile approach rapidly and effectively. Here, we describe a number of approaches to assist in such a transition. The Agile Software Solution Framework (ASSF) provides an overall context for the exploration of agile methods, knowledge and governance and contains an Agile Toolkit for quantifying part of the agile process. These link to the business aspects of software development so that the business value and agile process are well aligned. Finally, we describe how these theories are applied in practice with two industry case studies using the Agile Adoption and Improvement Model (AAIM).
81|11||A pilot study to compare programming effort for two parallel programming models|Writing software for the current generation of parallel systems requires significant programmer effort, and the community is seeking alternatives that reduce effort while still achieving good performance.
81|11||Benchmarking temporal database models with interval-based and temporal element-based timestamping|Starting from mid 1980s, there has been a debate about what data model is most appropriate for temporal databases. A fundamental choice one has to make is whether to use intervals of time or temporal elements to timestamp objects and events with the periods of validity. The advantage of using interval timestamps is that Start and End columns can be added to relations for treating them within the framework of classical databases, leading to quick implementation. Temporal elements are finite unions of intervals. The advantage of temporal elements is that timestamps become implicitly associated with values, tuples, and relations. Furthermore, since temporal elements, by design, are closed under set theoretical operations such as union, intersection and complementation, they lead to query languages that are natural. Here, we investigate the ease of use as well as system performance for the two approaches to help settle the debate.
81|11||Segmented software cost estimation models based on fuzzy clustering|Parametric software cost estimation models are based on mathematical relations, obtained from the study of historical software projects databases, that intend to be useful to estimate the effort and time required to develop a software product. Those databases often integrate data coming from projects of a heterogeneous nature. This entails that it is difficult to obtain a reasonably reliable single parametric model for the range of diverging project sizes and characteristics. A solution proposed elsewhere for that problem was the use of segmented models in which several models combined into a single one contribute to the estimates depending on the concrete characteristic of the inputs. However, a second problem arises with the use of segmented models, since the belonging of concrete projects to segments or clusters is subject to a degree of fuzziness, i.e. a given project can be considered to belong to several segments with different degrees.
81|11||Analysis of ID-based restrictive partially blind signatures and applications|Partially blind signatures and restrictive blind signatures are two important techniques in electronic cash systems and voting systems. Restrictive partially blind signatures incorporate the advantages of these two blind signatures. Recently, Chen–Zhang–Liu first proposed an ID-based restrictive partially blind signature from bilinear pairings (Chen, X.F., Zhang, F.G., Liu, S.L., 2007. ID-based restrictive partially blind signatures and applications. The Journal of Systems and Software 80 (2), 164–171). However, in this paper, we show that Chen–Zhang–Liu’s scheme has a security weakness. Their scheme does not satisfy the property of restrictiveness as they claimed, and an account-holder cannot be revealed when double-spending happens.
81|11||The influence of organizational culture on the adoption of extreme programming|The adoption of extreme programming (XP) method requires a very peculiar cultural context in software development companies. However, stakeholders do not always consider this matter and tend to stand to technical requirements of the method. Hence this paper aims at identifying aspects of organizational culture that may influence favorably or unfavorably the use of XP. In order to identify those aspects, this study analyzes dimensions of organizational culture under the perspective of practices and values of XP. This paper is based on the review of the literature of the area and empirical observations carried out with six software companies. This study does not intend to develop a tool for measurement of XP’s compatibility with the organizational culture of each company. It intends to provide parameters (favorable and unfavorable aspects) for previous consideration of the convenience of XP implementation.
81|11||A model for software rework reduction through a combination of anomaly metrics|Analysis of anomalies reported during testing of a project can tell a lot about how well the processes and products work. Still, organizations rarely use anomaly reports for more than progress tracking although projects commonly spend a significant part of the development time on finding and correcting faults. This paper presents an anomaly metrics model that organizations can use for identifying improvements in the development process, i.e. to reduce the cost and lead-time spent on rework-related activities and to improve the quality of the delivered product. The model is the result of a four year research project performed at Ericsson.
81|11||Real-time scheduling of batch systems using Petri nets and linear logic|This paper presents an approach to model, design and verify scenarios of real-time systems used in the scheduling and global coordination of batch systems. The initial requirements of a system specified with sequence diagrams are translated into a single p-time Petri net model representing the global behavior of the system. For the Petri net fragments involved in conflicts, symbolic production and consumption dates assigned to tokens are calculated based on the sequent calculus of linear logic. These dates are then used for off-line conflict resolution within a token player algorithm used for scenario verification of real-time specifications and which can be seen as a simulation tool for UML interaction diagrams.
81|11||Avoiding semantic and temporal gaps in developing software intensive systems|Development of software intensive systems (systems) in practice involves a series of self-contained phases for the lifecycle of a system. Semantic and temporal gaps, which occur among phases and among developer disciplines within and across phases, hinder the ongoing development of a system because of the interdependencies among phases and among disciplines. Such gaps are magnified among systems that are developed at different times by different development teams, which may limit reuse of artifacts of systems development and interoperability among the systems. This article discusses such gaps and a systems development process for avoiding them.
81|11||Multi-layer survivable routing mechanism in GMPLS based optical networks|The structure of IP/MPLS over optical networks has become the trend for new generation backbone transport networks since the technology of general multi-protocol label switching (GMPLS) can realize the seamless converage between IP and optical networks. At the same time, due to the high-speed transmission rate of each wavelength in optical networks, survivability is a very important issue and has been studied all through these years. Therefore, in new generation GMPLS based networks, it is very necessary to investigate the multi-layer survivable algorithms. In this paper, we comprehensively review the existing survivable algorithms in multi-layer networks and analyze the shortages of current researches. Based on previous studies, we prospect challenges and propose new solutions of designing efficient survivable algorithms to well guide the future work of researchers in multi-layer networks.
81|11||Application of redundant computation in program debugging|Programmers spend most of their time and resources in localizing program defects. On the other hand, they commit many errors by manipulating dynamic data improperly, which may produce dynamic memory problems, such as dangling pointer, memory leaks, and inaccessible objects. Dangling pointers can occur when a function returns a pointer to an automatic variable, or when trying to access a deleted object. Inaccessible objects occur when a pointer is assigned to point to another object, leaving the original object inaccessible, either by using the new operator or regular assignment operator. Memory leaks occur when a dynamic data is allocated but never de-allocated. The existence of such dynamic memory problems causes the programs to behave incorrectly. Improper usage of dynamic data is a common defect that is easy to commit, but is difficult to diagnose and discover. In this paper, we propose a dynamic approach that detects different types of program defects including those that occur as a result of misusing the dynamic data in computer programs. Our approach uses the notion of redundant computation to identify the suspicious locations in the program that may contain defects. Redundant computation is an execution of a program statement(s) that does not contribute to the program output. The notion of redundant computation is introduced as a potential indicator of defects in programs. We investigate the application of redundant computation in debugging programs. The detection of redundant computation indicates deficiency that may represent a bug in the program. The results of the experiment show that, the redundant computation detection can help the debuggers to localize the source(s) of the program defects.
81|11||Bytecode fault injection for Java software|Developers using third party software components need to test them to satisfy quality requirements. In the past, researchers have proposed fault injection testing approaches in which the component state is perturbed and the resulting effects on the rest of the system are observed. Non-availability of source code in third-party components makes it harder to perform source code level fault injection. Even if Java decompilers are used, they do not work well with obfuscated bytecode. We propose a technique that injects faults in Java software by manipulating the bytecode. Existing test suites are assessed according to their ability to detect the injected faults and improved accordingly. We present a case study using an open source Java component that demonstrates the feasibility and effectiveness of our approach. We also evaluate the usability of our approach on obfuscated bytecode.
81|11||Design and implementation of an efficient web cluster with content-based request distribution and file caching|We have implemented an efficient and scalable web cluster named LVS-CAD/FC (i.e. LVS with Content-Aware Dispatching and File Caching). In LVS-CAD/FC, a kernel-level one-way content-aware web switch based on TCP Rebuilding is implemented to examine and distribute the HTTP requests from clients to web servers, and the fast Multiple TCP Rebuilding is implemented to efficiently support persistent connection. Besides, a file-based web cache stores a small set of the most frequently accessed web files in server RAM to reduce disk I/Os and a light-weight redirect method is developed to efficiently redirect requests to this cache. In this paper, we have further proposed new policies related to content-based workload-aware request distribution, in which the web switch considers the content of requests and workload characterization in request dispatching. In particular, web files with more access frequencies would be duplicated in more servers’ file-based caches, such that hot web files can be served by more servers. Our goals are to improve cluster performance by obtaining better memory utilization and increasing the cache hit rates while achieving load balancing among servers. Experimental results of practical implementation on Linux show that LVS-CAD/FC is efficient and scales well. Besides, LVS-CAD/FC with the proposed policies can achieve 66.89% better performance than the Linux Virtual Server with a content-blind web switch.
81|11||Local variable access behavior of a hardware-translation based Java virtual machine|Hardware bytecode translation is a technique to improve the performance of the Java virtual machine (JVM), especially on the portable devices for which the overhead of dynamic compilation is significant. However, since the translation is done on a single bytecode basis, a naive implementation of the JVM generates frequent memory accesses for local variables which can be not only a performance bottleneck but also an obstacle for instruction folding. A solution to this problem is to add a small register file to the data path of the microprocessor which is dedicated for storing local variables. However, the effectiveness of such a local variable register file depends on the size and the local variable access behavior of the applications.
81|11||The Java 5 generics compromise orthogonality to keep compatibility|In response to a long-lasting anticipation by the Java community, version 1.5 of the Java 2 platform – referred to as Java 5 – introduced generic types and methods to the Java language. The Java 5 generics are a significant enhancement to the language expressivity because they allow straightforward composition of new generic classes from existing ones while reducing the need for a plethora of type casts. While the Java 5 generics are expressive, the chosen implementation method, type erasure, has triggered undesirable orthogonality violations. This paper identifies six cases of orthogonality violations in the Java 5 generics and demonstrates how these violations are mandated by the use of type erasure. The paper also compares the Java 5 cases of orthogonality violations to compatible cases in C# 2 and NextGen 2 and analyzes the tradeoffs in the three approaches. The conclusion is that Java 5 users face new challenges: a number of generic type expressions are forbidden, while others that are allowed are left unchecked by the compiler.
81|11||Quality-of-service oriented web service composition algorithm and planning architecture|In the next few decades, it is expected that web services will proliferate, many web services will offer the same services, and the clients will demand more value added and informative services rather than those offered by single, isolated web services. As the result, the problem of synthesizing web services of high quality will be raised as a prominent issue. The clients will face the trouble of choosing or creating composition plans, among numerous possible plans, that satisfy their quality-of-service (QoS) requirements. Typical QoS properties associated with a web service are the execution cost and time, availability, successful execution rate, reputation, and usage frequency. In engineering perspective, generating the composition plan that fulfills a client’s QoS requirement is a time-consuming optimization problem. To resolve the problem in a timely manner, we propose a constraint satisfaction based web service composition algorithm that combines tabu search and simulated annealing meta-heuristics. As an implementation framework of the algorithm, we suggest a QoS-oriented web service composition planning architecture. The architecture maintains expert made composition schemas in a service category and assists the client as pure user to choose the one he/she wants to use. The main modules of the architecture are composition broker and execution plan optimizer. With the aid of the UDDI server, the composition broker discovers candidate outsourced web services for each atomic process of the selected schema and gathers QoS information on the web services. After that, the execution plan optimizer runs the web service composition algorithm in order to generate a QoS-oriented composition plan. The performance of the algorithm was tested in a simulated environment.
81|11||An efficient nonuniform index in the wireless broadcast environments|Data broadcast is an efficient dissemination method to deliver information to mobile clients through the wireless channel. It allows a huge number of the mobile clients simultaneously access data in the wireless environments. In real-life applications, more popular data may be frequently accessed by clients than less popular ones. Under such scenarios, Acharya et al.’s Broadcast Disks algorithm (BD) allocates more popular data appeared more times in a broadcast period than less popular ones, i.e., the nonuniform broadcast, and provides a good performance on reducing client waiting time. However, mobile devices should constantly tune in to the wireless broadcast channel to examine data, consuming a lot of energy. Using index technologies on the broadcast file can reduce a lot of energy consumption of the mobile devices without significantly increasing client waiting time. In this paper, we propose an efficient nonuniform index called the skewed index, SI, over BD. The proposed algorithm builds an index tree according to skewed access patterns of clients, and allocates index nodes for the popular data more times than those for the less popular ones in a broadcast cycle. From our experimental study, we have shown that our proposed algorithm outperforms the flexible index and the flexible distributed index.
81|12|http://www.sciencedirect.com/science/journal/01641212/81/12|Introduction to the Special Issue|
81|12||Using planning poker for combining expert estimates in software projects|When producing estimates in software projects, expert opinions are frequently combined. However, it is poorly understood whether, when, and how to combine expert estimates. In order to study the effects of a combination technique called planning poker, the technique was introduced in a software project for half of the tasks. The tasks estimated with planning poker provided: (1) group consensus estimates that were less optimistic than the statistical combination (mean) of individual estimates for the same tasks, and (2) group consensus estimates that were more accurate than the statistical combination of individual estimates for the same tasks. For tasks in the same project, individual experts who estimated a set of control tasks achieved estimation accuracy similar to that achieved by estimators who estimated tasks using planning poker. Moreover, for both planning poker and the control group, measures of the median estimation bias indicated that both groups had unbiased estimates, because the typical estimated task was perfectly on target. A code analysis revealed that for tasks estimated with planning poker, more effort was expended due to the complexity of the changes to be made, possibly caused by the information provided in group discussions.
81|12||Risk and risk management in software projects: A reassessment|Controlling risk in software projects is considered to be a major contributor to project success. This paper reconsiders the status of risk and risk management in the literature and practice. The analysis is supported by a study of risk practices in government agencies in an Australian State, contributing to a gap in research in the public sector. It is found that risk is narrowly conceived in research, and risk management is under-performed in practice. The findings challenge some conventional conceptions of risk management and project management. For example, it was found that software projects do not conform to a uniform structure, as assumed in much of the literature. This introduces variations in the risk and project management challenges they face. Findings also suggest that formal project management is neither necessary nor sufficient for project success. It is concluded that risk management research lags the needs of practice, and risk management as practiced lags the prescriptions of research. Implications and directions for future research and practice are discussed.
81|12||The architecture of an event correlation service for adaptive middleware-based applications|Loosely coupled component communication driven by events is a key mechanism for building middleware-based applications that must achieve reliable qualities of service in an adaptive manner. In such a system, events that encapsulate state snapshots of a running system are generated by monitoring components. Hence, an event correlation service is necessary for correlating monitored events from multiple sources. The requirements for the event correlation raise two challenges: to seamlessly integrate event correlation services with other services and applications; and to provide reliable event management with minimal delay. This paper describes our experience in the design and implementation of an event correlation service. The design encompasses an event correlator and an event proxy that are integrated with an architecture for adaptive middleware components. The implementation utilizes the common-based event (CBE) specification and stateful Web service technologies to support the deployment of the event correlation service in a distributed architecture. We evaluate the performance of the overall solution in a test bed and present the results in terms of the trade-off between the flexibility and the performance overhead of the architecture.
81|12||Distributing test cases more evenly in adaptive random testing|Adaptive random testing (ART) has recently been proposed to enhance the failure-detection capability of random testing. In ART, test cases are not only randomly generated, but also evenly spread over the input domain. Various ART algorithms have been developed to evenly spread test cases in different ways. Previous studies have shown that some ART algorithms prefer to select test cases from the edge part of the input domain rather than from the centre part, that is, inputs do not have equal chance to be selected as test cases. Since we do not know where the failure-causing inputs are prior to testing, it is not desirable for inputs to have different chances of being selected as test cases. Therefore, in this paper, we investigate how to enhance some ART algorithms by offsetting the edge preference, and propose a new family of ART algorithms. A series of simulations have been conducted and it is shown that these new algorithms not only select test cases more evenly, but also have better failure detection capabilities.
81|12||Timed Behavior Trees for Failure Mode and Effects Analysis of time-critical systems|Behavior Trees are a graphical notation used for formalising functional requirements, and have been successfully applied to several industrial case studies. However, the standard notation does not support the concept of time, and consequently its application is limited to non-real-time systems. To overcome this limitation we extend the notation to timed Behavior Trees. We provide an operational semantics which is based on timed automata, and thus serves as a formal basis for the translation of timed Behavior Trees into the input notation of the timed model checker UPPAAL. System-level timing properties of a Behavior Tree model can then be automatically verified using UPPAAL. Based on the notational extensions with model checking support, we introduce timed Failure Mode and Effects Analysis, a process for identifying cause-consequence relationships between component failures and system hazards in real-time safety critical systems.
81|12||Using social networking and semantic web technology in software engineering â Use cases, patterns, and a case study|We present an approach that uses social networking and semantic web technology to share knowledge within the software engineering community. We propose to use existing Web 2.0 services such as social bookmarking and blogs as the infrastructure to share knowledge artefacts. Due to the openness of these services, it is crucial to provide support to establish the trustworthiness of knowledge artefacts. The solution proposed is designed following the original semantic web stack architecture and uses existing and emerging W3C semantic web standards such as the Resource Description Framework (RDF), the Web Ontology Language (OWL), and the Semantic Web Rule Language (SWRL). The WebOfPatterns project is presented as a case study, which includes an Eclipse plug-in that can be used to discover design pattern definitions in social networks, to define and publish patterns, to rate patterns, to establish the trustworthiness of patterns found, and finally to scan Java projects for instances of discovered patterns. We also propose the ContributedProperties design pattern that can be used to bridge the conceptual gap between RDF resources and objects in an object-oriented programming language.
81|12||A component- and push-based architectural style for ajax applications|A new breed of web application, dubbed ajax, is emerging in response to a limited degree of interactivity in large-grain stateless Web interactions. At the heart of this new approach lies a single page interaction model that facilitates rich interactivity. Also push-based solutions from the distributed systems are being adopted on the web for ajax applications. The field is, however, characterized by the lack of a coherent and precisely described set of architectural concepts. As a consequence, it is rather difficult to understand, assess, and compare the existing approaches. We have studied and experimented with several ajax frameworks trying to understand their architectural properties. In this paper, we summarize four of these frameworks and examine their properties and introduce the spiar architectural style which captures the essence of ajax applications. We describe the guiding software engineering principles and the constraints chosen to induce the desired properties. The style emphasizes user interface component development, intermediary delta-communication between client/server components, and push-based event notification of state changes through the components, to improve a number of properties such as user interactivity, user-perceived latency, data coherence, and ease of development. In addition, we use the concepts and principles to discuss various open issues in ajax frameworks and application development.
81|12||Synthesis of decentralized and concurrent adaptors for correctly assembling distributed component-based systems|Building a distributed system from third-party components introduces a set of problems, mainly related to compatibility and communication. Our existing approach to solve such problems is to build a centralized adaptor which restricts the system’s behavior to exhibit only deadlock-free and desired interactions. However, in a distributed environment such an approach is not always suitable. In this paper, we show how to automatically generate a distributed adaptor for a set of black-box components. First, by taking into account a specification of the interaction behavior of each component, we synthesize a behavioral model for a centralized glue adaptor. Second, from the synthesized adaptor model and a specification of the desired behavior that must be enforced, we generate one local adaptor for each component. The local adaptors cooperatively behave as the centralized one restricted with respect to the specified desired interactions.
81|12||An architectural approach to the correct and automatic assembly of evolving component-based systems|Software components are specified, designed and implemented with the intention to be reused, and they are assembled in various contexts in order to produce a multitude of software systems. However, in the practice of software development, this ideal scenario is often unrealistic. This is mainly due to the lack of an automatic and efficient support to predict properties of the assembly code by only assuming a limited knowledge of the properties of single components. Moreover, to make effective the component-based vision, the assembly code should evolve when things change, i.e., the properties guaranteed by the assembly, before a change occurs, must hold also after the change. Glue code synthesis approaches technically permit one to construct an assembly of components that guarantees specific properties but, practically, they may suffer from the state-space explosion phenomenon.
81|12||Execution trace analysis through massive sequence and circular bundle views|An important part of many software maintenance tasks is to gain a sufficient level of understanding of the system at hand. The use of dynamic information to aid in this software understanding process is a common practice nowadays. A major issue in this context is scalability: due to the vast amounts of information, it is a very difficult task to successfully navigate through the dynamic data contained in execution traces without getting lost.
81|12||Analyzing clusters of class characteristics in OO applications|The transition from Java 1.4 to Java 1.5 has provided the programmer with more flexibility due to the inclusion of several new language constructs, such as parameterized types. This transition is expected to increase the number of class clusters exhibiting different combinations of class characteristics. In this paper we investigate how the number and distribution of clusters are expected to change during this transition. We present the results of an empirical study were we analyzed applications written in both Java 1.4 and 1.5. In addition, we show how the variability of the combinations of class characteristics may affect the testing of class members.
81|12||An empirical study of the relationship between the concepts expressed in source code and dependence|Programs express domain-level concepts in their source code. It might be expected that such concepts would have a degree of semantic cohesion. This cohesion ought to manifest itself in the dependence between statements all of which contribute to the computation of the same concept. This paper addresses a set of research questions that capture this informal observation. It presents the results of experiments on 10 programs that explore the relationship between domain-level concepts and dependence in source code. The results show that code associated with concepts has a greater degree of coherence, with tighter dependence. This finding has positive implications for the analysis of concepts as it provides an approach to decompose a program into smaller executable units, each of which captures the behaviour of the program with respect to a domain-level concept.
81|12||OCL2Trigger: Deriving active mechanisms for relational databases using Model-Driven Architecture|Transforming integrity constraints into active rules or triggers for verifying database consistency produces a serious and complex problem related to real time behaviour that must be considered for any implementation. Our main contribution to this work is to provide a complete approach for deriving the active mechanisms for Relational Databases from the specification of the integrity constraints by using OCL. This approach is designed in accordance with the MDA approach which consists of transforming the specified OCL clauses into a class diagram into SQL:2003 standard triggers, then transforming the standard triggers into target DBMS triggers. We believe that developing triggers and plugging them into a given model is insufficient because the behaviour of such triggers is invisible to the developers, and therefore not controllable. For this reason, a DBMS trigger verification model is used in our approach, in order to ensure the termination of trigger execution. Our approach is implemented as an add-in tool in Rational Rose called OCL2Trigger.
81|12||A self-stabilizing autonomic recoverer for eventual Byzantine software|We suggest modeling software package flaws (bugs) by assuming eventual Byzantine behavior of the package. We assume that if a program is started in a predefined initial state, it will exhibit legal behavior for a period of time but will eventually become Byzantine. We assume that this behavior pattern can be attributed to the fact that the manufacturer had performed sufficient package tests for limited time scenarios. Restarts are useful for recovering such systems. We suggest a general, yet practical, framework and paradigm for the monitoring and restarting of systems where the framework and paradigm are based on a theoretical foundation. An autonomic recoverer that monitors and initiates system recovery is proposed. It is designed to handle a task, given specific task requirements in the form of predicates and actions. A directed acyclic graph subsystem hierarchical structure is used by a consistency monitoring procedure for achieving a gracious recovery. The existence and correct functionality of the autonomic recovery is guaranteed by the use of a self-stabilizing kernel resident (anchor) process. The autonomic recoverer uses a new scheme for liveness assurance via on-line monitoring that complements known schemes for on-line safety assurance.
81|12||Security analysis of the full-round DDO-64 block cipher|DDO-64 is a 64-bit Feistel-like block cipher based on data-dependent operations (DDOs). It is composed of 8 rounds and uses a 128-bit key. There are two versions of DDO-64, named DDO-64V1 and DDO-64V2, according to the key schedule. They were designed under an attempt for improving the security and performance of DDP-based ciphers. In this paper, however, we show that like most of the existing DDP-based ciphers, DDO-64V1 and DDO-64V2 are also vulnerable to related-key attacks. The attack on DDO-64V1 requires 235.5 related-key chosen plaintexts and 263.5 encryptions while the attack on DDO-64V2 only needs 8 related-key chosen plaintexts and 231 encryptions; our attacks are both mainly due to their simple key schedules and structural weaknesses. These works are the first known cryptanalytic results on DDO-64V1 and DDO-64V2 so far.
81|12||Characterization of the evolution of a news Web site|The Web has become a ubiquitous tool for distributing knowledge and information and for conducting businesses. To exploit the huge potential of the Web as a global information repository, it is necessary to understand its dynamics. These issues are particularly important for news Web sites as they are expected to provide fresh information on current world events to a potentially large user population. This paper presents an experimental study aimed at characterizing and modeling the evolution of a news Web site. We focused on the MSNBC Web site as it is a good representative of its category in terms of structure, news coverage and popularity. Specifically, we analyzed how often and to what extent the content of this site changed and we identified models describing its dynamics. The study has shown that the rate of page creations and updates was characterized by some well defined patterns that varied as a function of time of day and day of week. On the contrary, the content of individual pages changed to a different extent. Most updates involved a very small fraction of their content, whereas very few were more extensive and spread over the whole page. By taking into accounts all these aspects, we derived analytical models able to accurately capture and reproduce the evolution of the news Web site.
81|12||Data access in distributed simulations of multi-agent systems|Distributed simulation has emerged as an important instrument for studying large-scale complex systems. Such systems inherently consist of a large number of components, which operate in a large shared state space interacting with it in highly dynamic and unpredictable ways. Optimising access to the shared state space is crucial for achieving efficient simulation executions. Data accesses may take two forms: locating data according to a set of attribute value ranges (range query) or locating a particular state variable from the given identifier (ID query and update). This paper proposes two alternative routing approaches, namely the address-based approach, which locates data according to their address information, and the range-based approach, whose operation is based on looking up attribute value range information along the paths to the destinations. The two algorithms are discussed and analysed in the context of PDES-MAS, a framework for the distributed simulation of multi-agent systems, which uses a hierarchical infrastructure to manage the shared state space. The paper introduces a generic meta-simulation framework which is used to perform a quantitative comparative analysis of the proposed algorithms under various circumstances.
81|12||Can k-NN imputation improve the performance of C4.5 with small software project data sets? A comparative evaluation|Missing data is a widespread problem that can affect the ability to use data to construct effective prediction systems. We investigate a common machine learning technique that can tolerate missing values, namely C4.5, to predict cost using six real world software project databases. We analyze the predictive performance after using the k-NN missing data imputation technique to see if it is better to tolerate missing data or to try to impute missing values and then apply the C4.5 algorithm. For the investigation, we simulated three missingness mechanisms, three missing data patterns, and five missing data percentages. We found that the k-NN imputation can improve the prediction accuracy of C4.5. At the same time, both C4.5 and k-NN are little affected by the missingness mechanism, but that the missing data pattern and the missing data percentage have a strong negative impact upon prediction (or imputation) accuracy particularly if the missing data percentage exceeds 40%.
81|12||Exploiting synergies between semantic reasoning and personalization strategies in intelligent recommender systems: A case study|Current recommender systems attempt to identify appealing items for a user by applying syntactic matching techniques, which suffer from significant limitations that reduce the quality of the offered suggestions. To overcome this drawback, we have developed a domain-independent personalization strategy that borrows reasoning techniques from the Semantic Web, elaborating recommendations based on the semantic relationships inferred between the user’s preferences and the available items. Our reasoning-based approach improves the quality of the suggestions offered by the current personalization approaches, and greatly reduces their most severe limitations. To validate these claims, we have carried out a case study in the Digital TV field, in which our strategy selects TV programs interesting for the viewers from among the myriad of contents available in the digital streams. Our experimental evaluation compares the traditional approaches with our proposal in terms of both the number of TV programs suggested, and the users’ perception of the recommendations. Finally, we discuss concerns related to computational feasibility and scalability of our approach.
81|12||The mechanisms of project management of software development|The changing environments of software development such as component-based, distributed and outsourced software development require matching changes by project managers to monitor, control and coordinate their projects. While the objectives of project management may be well established, the mechanisms with which those objectives are achieved are less well known. An empirical study was undertaken to investigate which mechanisms were used by practising project managers to monitor, control and coordinate software development projects.
81|12||A receiver-centric rate control scheme for layered video streams in the Internet|We present a new end-to-end protocol, namely Dynamic Video Rate Control (DVRC), which operates on top of UDP and enables the adaptive delivery of layered video streams over the Internet. The protocol optimizes the performance on video delivery with concern to friendliness with interfering traffic. DVRC enables a closed-loop control between server and client, where the receiver detects the state of congestion, determines the proper transmission rate, and eventually opts for the optimal number of layers that should be delivered according to this rate. The protocol relies on a hybrid Additive Increase Additive Decrease (AIAD)/Additive Increase Multiplicative Decrease (AIMD) algorithm (namely AIAMD) that manages to differentiate congestive and non-congestive loss by utilizing history in its control rules. AIAMD combines the most desirable features of AIAD and AIMD, reacting gently to random loss and more aggressively to congestion and adapting effectively to the dynamics of the network. Therefore, DVRC enables the desired smoothness for video streaming applications and at the same time avoids significant damage during congestion. Exploring DVRC’s potential through extensive simulations, we identify notable gains in terms of bandwidth utilization and smooth video delivery. Furthermore, our results indicate that the protocol allocates a well-balanced amount of network resources maintaining friendliness with corporate TCP connections.
81|12||What do software architects really do?|To be successful, a software architect—or a software architecture team, collectively—must strike a delicate balance between an external focus—both outwards: Listening to customers, users, watching technology, developing a long-term vision, and inwards: driving the development teams—and an internal, reflective focus: spending time to make the right design choices, validating them, and documenting them. Teams that stray too far away from this metastable equilibrium fall into some traps that we describe as antipatterns of software architecture teams.
81|2|http://www.sciencedirect.com/science/journal/01641212/81/2|Editorial|
81|2||Traffic-aware stress testing of distributed real-time systems based on UML models using genetic algorithms|This paper presents a model-driven, stress test methodology aimed at increasing chances of discovering faults related to network traffic in distributed real-time systems (DRTS). The technique uses the UML 2.0 model of the distributed system under test, augmented with timing information, and is based on an analysis of the control flow in sequence diagrams. It yields stress test requirements that are made of specific control flow paths along with time values indicating when to trigger them. The technique considers different types of arrival patterns (e.g., periodic) for real-time events (common to DRTSs), and generates test requirements which comply with such timing constraints. Though different variants of our stress testing technique already exist (that stress different aspects of a distributed system), they share a large amount of common concepts and we therefore focus here on one variant that is designed to stress test the system at a time instant when data traffic on a network is maximal. Our technique uses genetic algorithms to find test requirements which lead to maximum possible traffic-aware stress in a system under test. Using a real-world DRTS specification, we design and implement a prototype DRTS and describe, for that particular system, how the stress test cases are derived and executed using our methodology. The stress test results indicate that the technique is significantly more effective at detecting network traffic-related faults when compared to test cases based on an operational profile.
81|2||Applying machine learning to software fault-proneness prediction|The importance of software testing to quality assurance cannot be overemphasized. The estimation of a module’s fault-proneness is important for minimizing cost and improving the effectiveness of the software testing process. Unfortunately, no general technique for estimating software fault-proneness is available. The observed correlation between some software metrics and fault-proneness has resulted in a variety of predictive models based on multiple metrics. Much work has concentrated on how to select the software metrics that are most likely to indicate fault-proneness. In this paper, we propose the use of machine learning for this purpose. Specifically, given historical data on software metric values and number of reported errors, an Artificial Neural Network (ANN) is trained. Then, in order to determine the importance of each software metric in predicting fault-proneness, a sensitivity analysis is performed on the trained ANN. The software metrics that are deemed to be the most critical are then used as the basis of an ANN-based predictive model of a continuous measure of fault-proneness. We also view fault-proneness prediction as a binary classification task (i.e., a module can either contain errors or be error-free) and use Support Vector Machines (SVM) as a state-of-the-art classification method. We perform a comparative experimental study of the effectiveness of ANNs and SVMs on a data set obtained from NASA’s Metrics Data Program data repository.
81|2||Experiments with test case prioritization using relevant slices|Software testing and retesting occurs continuously during the software development lifecycle to detect errors as early as possible and to gain confidence that changes to software do not introduce defects. Once developed, test suites are reused and updated frequently, and their sizes grow as software evolves. Due to time and resource constraints, an important goal during regression testing of software is to prioritize the execution of test cases in a suite so as to improve the chances of increasing the rate of fault detection. Prior techniques for test case prioritization are based on the total number of coverage requirements exercised by the test cases. In this paper, we present a new approach to prioritize test cases that takes into account the coverage requirements present in the relevant slices of the outputs of test cases. We have implemented three different heuristics based on our relevant slicing based approach to prioritize test cases and conducted experiments to compare the effectiveness of our techniques with those of the traditional techniques that only account for the total requirement coverage. Our detailed experimental study and results provide interesting insights into the effectiveness of using relevant slices for test case prioritization in terms of ability to achieve high rate of fault detection.
81|2||Testing input validation in Web applications through automated model recovery|Input validation is essential and critical in Web applications. It is the enforcement of constraints that any input must satisfy before it is accepted to raise external effects. We have discovered some empirical properties for characterizing input validation in Web applications. In this paper, we propose an approach for automated recovery of input validation model from program source code. The model recovered is represented in a variant of control flow graph, called validation flow graph, which shows essential input validation features implemented in programs. Based on the model, we then formulate two coverage criteria for testing input validation. The two criteria can be used to guide the structural testing of input validation in Web applications. We have evaluated the proposed approach through case studies and experiments.
81|2||A relation-based method combining functional and structural testing for test case generation|Specification-based (or functional) testing enables us to detect errors in the implementation of functions defined in specifications, but since specifications are often incomplete in practice for some reasons (e.g., lack of ideas, no time to write), it is unlikely to be sufficient for testing all parts of corresponding programs. On the other hand, implementation-based (or structural) testing focuses on the examination of program structures, which allows us to test all parts of the programs, but may not be effective to show whether the programs properly implement the corresponding specifications. To perform a comprehensive testing of a program in practice, it is important to adopt both specification-based and implementation-based testing. In this paper we describe a relation-based test method that combines the specification-based and the implementation-based testing approaches. We establish a set of relations for test case generation, illustrate how the method is used with an example, and investigate the effectiveness and weakness of the method through an experiment on testing a software tool system.
81|2||An empirical, path-oriented approach to software analysis and testing|Error flow analysis and testing techniques focus on the introduction of errors through code faults into data states of an executing program, and their subsequent cancellation or propagation to output. The goals and limitations of several error flow techniques are discussed, including mutation analysis, fault-based testing, PIE analysis, and dynamic impact analysis. The attributes desired of a good error flow technique are proposed, and a model called dynamic error flow analysis (DEFA) is described that embodies many of these attributes. A testing strategy is proposed that uses DEFA information to select an optimal set of test paths and to quantify the results of successful testing. An experiment is presented that illustrates this testing strategy. In this experiment, the proposed testing strategy outperforms mutation testing in catching arbitrary data state errors.
81|2||A search-based framework for automatic testing of MATLAB/Simulink models|Search-based test-data generation has proved successful for code-level testing but almost no search-based work has been carried out at higher levels of abstraction. In this paper the application of such approaches at the higher levels of abstraction offered by MATLAB/Simulink models is investigated and a wide-ranging framework for test-data generation and management is presented. Model-level analogues of code-level structural coverage criteria are presented and search-based approaches to achieving them are described. The paper also describes the first search-based approach to the generation of mutant-killing test data, addressing a fundamental limitation of mutation testing. Some problems remain whatever the level of abstraction considered. In particular, complexity introduced by the presence of persistent state when generating test sequences is as much a challenge at the Simulink model level as it has been found to be at the code level. The framework addresses this problem. Finally, a flexible approach to test sub-set extraction is presented, allowing testing resources to be deployed effectively and efficiently.
81|2||Automated generation of test suites from formal specifications of real-time reactive systems|Real-time reactive systems are among the most difficult systems to test because of their size and complex time-dependent functionality. The number of test experiments for such systems is very large, if not infinite. Often such systems arise in safety-critical contexts. Hence, such systems require a rigorous analysis and thorough testing before they are deployed. This paper addresses test case generation methods and a metric-based test case selection algorithm for sufficient testing of real-time reactive systems. The methods are rigorous, and based on the formal specifications of the system and its fault models. The test generation and execution of algorithms are implemented in TROMLAB, a formal framework for developing real-time reactive systems. The methods are applied to the formal specification of the Train–Gate–Controller (TGC) example, a bench-mark case study in the real-time systems community. A brief description of the experimental results obtained on the case study is given.
81|3|http://www.sciencedirect.com/science/journal/01641212/81/3|Editorial â Outgoing Editor-in-Chief|
81|3||Editorial â Taking over|
81|3||Editorial|
81|3||Process pipeline scheduling|Two processes, p and q, may be scheduled in pipeline when q may start when p starts, and q may process data items from p, one-by-one, without waiting for p to write the complete set of data items. This paper explores how process pipeline scheduling may become a viable strategy for executing workflows. The paper first details a workflow model that captures the characteristics of the application programs that pipeline scheduling requires. It proceeds by showing that the process pipeline scheduling problem is NP-Complete. Then, it describes a specific algorithm that pipelines as many processes as possible, within the bounds of the storage space available, based on a greedy process scheduling heuristics that has acceptable performance. Finally, the paper presents a detailed example that illustrates how the algorithm schedules processes.
81|3||An adaptive in-network aggregation operator for query processing in wireless sensor networks|A wireless sensor network (WSN) is composed of tens or hundreds of spatially distributed autonomous nodes, called sensors. Sensors are devices used to collect data from the environment related to the detection or measurement of physical phenomena. In fact, a WSN consists of groups of sensors where each group is responsible for providing information about one or more physical phenomena (e.g., group for collecting temperature data). Sensors are limited in power, computational capacity, and memory. Therefore, a query engine and query operators for processing queries in WSNs should be able to handle resource limitations such as memory and battery life. Adaptability has been explored as an alternative approach when dealing with these conditions. Adaptive query operators (algorithms) can adjust their behavior in response to specific events that take place during data processing. In this paper, we propose an adaptive in-network aggregation operator for query processing in sensor nodes of a WSN, called ADAGA (ADaptive AGgregation Algorithm for sensor networks). The ADAGA adapts its behavior according to memory and energy usage by dynamically adjusting data-collection and data-sending time intervals. ADAGA can correctly aggregate data in WSNs with packet replication. Moreover, ADAGA is able to predict non-performed detection values by analyzing collected values. Thus, ADAGA is able to produce results as close as possible to real results (obtained when no resource constraint is faced). The results obtained through experiments prove the efficiency of ADAGA.
81|3||Accelerating k-medoid-based algorithms through metric access methods|Scalable data mining algorithms have become crucial to efficiently support KDD processes on large databases. In this paper, we address the task of scaling up k-medoid-based algorithms through the utilization of metric access methods, allowing clustering algorithms to be executed by database management systems in a fraction of the time usually required by the traditional approaches. We also present an optimization strategy that can be applied as an additional step of the proposed algorithm in order to achieve better clustering solutions. Experimental results based on several datasets, including synthetic and real ones, show that the proposed algorithm can reduce the number of distance calculations by a factor of more than three thousand times when compared to existing algorithms, while producing clusters of equivalent quality.
81|3||An investigation of artificial neural networks based prediction systems in software project management|A critical issue in software project management is the accurate estimation of size, effort, resources, cost, and time spent in the development process. Underestimates may lead to time pressures that may compromise full functional development and the software testing process. Likewise, overestimates can result in noncompetitive budgets. In this paper, artificial neural network and stepwise regression based predictive models are investigated, aiming at offering alternative methods for those who do not believe in estimation models. The results presented in this paper compare the performance of both methods and indicate that these techniques are competitive with the APF, SLIM, and COCOMO methods.
81|3||Using ontologies and Web services for content adaptation in Ubiquitous Computing|The diversity of small mobile devices and networks enabling users to access the Internet expands every day. In this highly dynamic environment of Ubiquitous Computing, current programming paradigms do not offer the flexibility needed for software reuse. To improve this flexibility, this paper proposes the use of ontologies and Web services, within a framework of components for the content adaptation domain, to facilitate the development of software based on reuse. A case study illustrates the use of the proposed solution.
81|3||XMobile: A MB-UID environment for semi-automatic generation of adaptive applications for mobile devices|Ubiquitous Computing promises seamless access to information anytime, anywhere with different and heterogeneous devices. This kind of environment imposes new challenges to software development. For example, information and user interface should be adapted according to contextual characteristics such as user, environment, and access device. In case of device adaptation, the development challenge is related to the heterogeneity of the devices, which requires software engineers to create different versions for each type of device and every platform. This paper proposes a MB-UID (model-based user interface development) approach for semi-automatic generation of adaptive applications for mobile devices. An environment, called XMobile, offers a device-independent user interface framework and a code generation tool for providing fast development of multi-platform and adaptive applications according to device and platform features. A case study is also presented to illustrate how the environment can be used for constructing an application for heterogeneous devices with different network connectivity modes.
81|3||Software Engineering Using RATionale|Many decisions have to be made when developing a software system and a successful outcome depends on how well thought out these decisions were. One way that the decisions made, and alternatives considered, can be captured is in the rationale for the system. The rationale goes beyond standard documentation by capturing the developers’ intent and all alternatives considered rather than only those selected. While the potential usefulness of this information is seldom questioned, it typically is not captured in practice. We feel that the key to motivating capture is to provide compelling uses and tool support integrated with the development environment. Here we describe the Software Engineering Using RATionale system which inferences over the rationale to evaluate decision alternatives and perform impact assessment when requirements, development criteria, and assumptions change.
81|3||Proactive and reactive multi-dimensional histogram maintenance for selectivity estimation|Many state-of-the-art selectivity estimation methods use query feedback to maintain histogram buckets, thereby using the limited memory efficiently. However, they are “reactive” in nature, that is, they update the histogram based on queries that have come to the system in the past for evaluation. In some applications, future occurrences of certain queries may be predicted and a “proactive” approach can bring much needed performance gain, especially when combined with the reactive approach. For these applications, this paper provides a method that builds customized proactive histograms based on query prediction and mergers them into reactive histograms when the predicted future arrives. Thus, the method is called the proactive and reactive histogram (PRHist). Two factors affect the usefulness of the proactive histograms and are dealt with during the merge process: the first is the predictability of queries and the second is the extent of data updates. PRHist adjusts itself to be more reactive or more proactive depending on these two factors. Through extensive experiments using both real and synthetic data and query sets, this paper shows that in most cases, PRHist outperforms STHoles, the state-of-the-art reactive method, even when only a small portion of the queries are predictable and a significant portion of data is updated.
81|3||Understanding knowledge sharing activities in free/open source software projects: An empirical study|Free/Open Source Software (F/OSS) projects are people-oriented and knowledge intensive software development environments. Many researchers focused on mailing lists to study coding activities of software developers. How expert software developers interact with each other and with non-developers in the use of community products have received little attention. This paper discusses the altruistic sharing of knowledge between knowledge providers and knowledge seekers in the Developer and User mailing lists of the Debian project. We analyze the posting and replying activities of the participants by counting the number of email messages they posted to the lists and the number of replies they made to questions others posted. We found out that participants interact and share their knowledge a lot, their positing activity is fairly highly correlated with their replying activity, the characteristics of posting and replying activities are different for different kinds of lists, and the knowledge sharing activity of self-organizing Free/Open Source communities could best be explained in terms of what we called “Fractal Cubic Distribution” rather than the power-law distribution mostly reported in the literature. The paper also proposes what could be researched in knowledge sharing activities in F/OSS projects mailing list and for what purpose. The research findings add to our understanding of knowledge sharing activities in F/OSS projects.
81|3||Examining the significance of high-level programming features in source code author classification|The use of Source Code Author Profiles (SCAP) represents a new, highly accurate approach to source code authorship identification that is, unlike previous methods, language independent. While accuracy is clearly a crucial requirement of any author identification method, in cases of litigation regarding authorship, plagiarism, and so on, there is also a need to know why it is claimed that a piece of code is written by a particular author. What is it about that piece of code that suggests a particular author? What features in the code make one author more likely than another? In this study, we describe a means of identifying the high-level features that contribute to source code authorship identification using as a tool the SCAP method. A variety of features are considered for Java and Common Lisp and the importance of each feature in determining authorship is measured through a sequence of experiments in which we remove one feature at a time. The results show that, for these programs, comments, layout features and package-related naming influence classification accuracy whereas user-defined naming, an obvious programmer related feature, does not appear to influence accuracy. A comparison is also made between the relative feature contributions in programs written in the two languages.
81|4|http://www.sciencedirect.com/science/journal/01641212/81/4|Guest Editorâs Introduction: 10th Conference on Software Maintenance and Reengineering|
81|4||A wrapping approach for migrating legacy system interactive functionalities to Service Oriented Architectures|Software systems modernisation using Service Oriented Architectures (SOAs) and Web Services represents a valuable option for extending the lifetime of mission-critical legacy systems.
81|4||Dynamic object process graphs|A trace is a record of the execution of a computer program, showing the sequence of operations executed. A trace may be obtained through static or dynamic analysis. An object trace contains only those operations that relate to a particular object.
81|4||Search-based refactoring for software maintenance|The high cost of software maintenance could be reduced by automatically improving the design of object-oriented programs without altering their behaviour. We have constructed a software tool capable of refactoring object-oriented programs to conform more closely to a given design quality model, by formulating the task as a search problem in the space of alternative designs. This novel approach is validated by two case studies, where programs are automatically refactored to increase flexibility, reusability and understandability as defined by a contemporary quality model. Both local and simulated annealing searches were found to be effective in this task.
81|4||Model-driven migration of supervisory machine control architectures|Supervisory machine control is the high-level control in advanced manufacturing machines that is responsible for the coordination of manufacturing activities. Traditionally, the design of such control systems is based on finite state machines. An alternative, more flexible approach is based on task-resource models. This paper describes an approach for the migration of supervisory machine control architectures towards this alternative approach. We propose a generic migration approach based on model transformations that includes normalisation of legacy architectures before their actual transformation. To this end, we identify a number of key concerns for supervisory machine control and a corresponding normalised design idiom. As such, our migration approach constitutes a series of model transformations, for which we define transformation rules. We illustrate the applicability of this model-driven approach by migrating (part of) the supervisory control architecture of an advanced manufacturing machine: a wafer scanner developed by ASML. This migration, towards a product-line architecture, includes a change in architectural paradigm from finite state machines to task-resource systems.
81|4||Documenting after the fact: Recovering architectural design decisions|Software architecture documentation helps people in understanding the software architecture of a system. In practice, software architectures are often documented after the fact, i.e. they are maintained or created after most of the design decisions have been made and implemented. To keep the architecture documentation up-to-date an architect needs to recover and describe these decisions.
81|4||Software architecture reliability analysis using failure scenarios|With the increasing size and complexity of software in embedded systems, software has now become a primary threat for the reliability. Several mature conventional reliability engineering techniques exist in literature but traditionally these have primarily addressed failures in hardware components and usually assume the availability of a running system. Software architecture analysis methods aim to analyze the quality of software-intensive system early at the software architecture design level and before a system is implemented. We propose a Software Architecture Reliability Analysis Approach (SARAH) that benefits from mature reliability engineering techniques and scenario-based software architecture analysis to provide an early software reliability analysis at the architecture design level. SARAH defines the notion of failure scenario model that is based on the Failure Modes and Effects Analysis method (FMEA) in the reliability engineering domain. The failure scenario model is applied to represent so-called failure scenarios that are utilized to derive fault tree sets (FTS). Fault tree sets are utilized to provide a severity analysis for the overall software architecture and the individual architectural elements. Despite conventional reliability analysis techniques which prioritize failures based on criteria such as safety concerns, in SARAH failure scenarios are prioritized based on severity from the end-user perspective. SARAH results in a failure analysis report that can be utilized to identify architectural tactics for improving the reliability of the software architecture. The approach is illustrated using an industrial case for analyzing reliability of the software architecture of the next release of a Digital TV.
81|4||Software reliability prediction by soft computing techniques|In this paper, ensemble models are developed to accurately forecast software reliability. Various statistical (multiple linear regression and multivariate adaptive regression splines) and intelligent techniques (backpropagation trained neural network, dynamic evolving neuro–fuzzy inference system and TreeNet) constitute the ensembles presented. Three linear ensembles and one non-linear ensemble are designed and tested. Based on the experiments performed on the software reliability data obtained from literature, it is observed that the non-linear ensemble outperformed all the other ensembles and also the constituent statistical and intelligent techniques.
81|4||Industrial validation of COVAMOF|COVAMOF is a variability management framework for product families that was developed to reduce the number of iterations required during product derivation and to reduce the dependency on experts. In this paper, we present the results of an experiment with COVAMOF in industry. The results show that with COVAMOF, engineers that are not involved in the product family were now capable of deriving the products in 100% of the cases, compared to 29% of the cases without COVAMOF. For experts, the use of COVAMOF reduced the number of iterations by 42%, and the total derivation time by 38%.
81|5|http://www.sciencedirect.com/science/journal/01641212/81/5|Special Issue on Software Process and Product Measurement|
81|5||Measuring where it matters: Determining starting points for metrics collection|Defining useful metrics to measure the goals of a software organisation is difficult. Defining useful metrics to measure the causes of the (failure) to fulfil those organisational goals is even more difficult, as the diversity of potential causes makes their measurement illusive. In this article, we describe a method to select useful software metrics based on findings from qualitative research. In a case study, we apply this method to a previously conducted study of project post-mortem reviews to assess the validity of our prior claims. For this we collected data on 109 new software projects in the organisation in which we conducted the previous case study.
81|5||Comparing cost prediction models by resampling techniques|The accurate software cost prediction is a research topic that has attracted much of the interest of the software engineering community during the latest decades. A large part of the research efforts involves the development of statistical models based on historical data. Since there are a lot of models that can be fitted to certain data, a crucial issue is the selection of the most efficient prediction model. Most often this selection is based on comparisons of various accuracy measures that are functions of the model’s relative errors. However, the usual practice is to consider as the most accurate prediction model the one providing the best accuracy measure without testing if this superiority is in fact statistically significant. This policy can lead to unstable and erroneous conclusions since a small change in the data is able to turn over the best model selection. On the other hand, the accuracy measures used in practice are statistics with unknown probability distributions, making the testing of any hypothesis, by the traditional parametric methods, problematic. In this paper, the use of statistical simulation tools is proposed in order to test the significance of the difference between the accuracy of two prediction methods: regression and estimation by analogy. The statistical simulation procedures involve permutation tests and bootstrap techniques for the construction of confidence intervals for the difference of measures. Four known datasets are used for experimentation in order to validate the results and make comparisons between the simulation methods and the traditional parametric and non-parametric procedures.
81|5||A framework for the design and verification of software measurement methods|
81|5||Predicting defect-prone software modules using support vector machines|Effective prediction of defect-prone software modules can enable software developers to focus quality assurance activities and allocate effort and resources more efficiently. Support vector machines (SVM) have been successfully applied for solving both classification and regression problems in many applications. This paper evaluates the capability of SVM in predicting defect-prone software modules and compares its prediction performance against eight statistical and machine learning models in the context of four NASA datasets. The results indicate that the prediction performance of SVM is generally better than, or at least, is competitive against the compared models.
81|5||On the conversion between IFPUG and COSMIC software functional size units: A theoretical and empirical study|Since 1984 the International Function Point Users Group (IFPUG) has produced and maintained a set of standards and technical documents about a functional size measurement methods, known as IFPUG, based on Albrecht function points. On the other hand, in 1998, the Common Software Measurement International Consortium (COSMIC) proposed an improved measurement method known as full function points (FFP). Both the IFPUG and the COSMIC methods both measure functional size of software, but produce different results. In this paper, we propose a model to convert functional size measures obtained with the IFPUG method to the corresponding COSMIC measures. We also present the validation of the model using 33 software projects measured with both methods. This approach may be beneficial to companies using both methods or migrating to COSMIC such that past data in IFPUG can be considered for future estimates using COSMIC and as a validation procedure.
81|5||Cross-company vs. single-company web effort models using the Tukutuku database: An extended study|
81|5||A comprehensive empirical evaluation of missing value imputation in noisy software measurement data|The handling of missing values is a topic of growing interest in the software quality modeling domain. Data values may be absent from a dataset for numerous reasons, for example, the inability to measure certain attributes. As software engineering datasets are sometimes small in size, discarding observations (or program modules) with incomplete data is usually not desirable. Deleting data from a dataset can result in a significant loss of potentially valuable information. This is especially true when the missing data is located in an attribute that measures the quality of the program module, such as the number of faults observed in the program module during testing and after release. We present a comprehensive experimental analysis of five commonly used imputation techniques. This work also considers three different mechanisms governing the distribution of missing values in a dataset, and examines the impact of noise on the imputation process. To our knowledge, this is the first study to thoroughly evaluate the relationship between data quality and imputation. Further, our work is unique in that it employs a software engineering expert to oversee the evaluation of all of the procedures and to ensure that the results are not inadvertently influenced by poor quality data. Based on a comprehensive set of carefully controlled experiments, we conclude that Bayesian multiple imputation and regression imputation are the most effective techniques, while mean imputation performs extremely poorly. Although a preliminary evaluation has been conducted using Bayesian multiple imputation in the empirical software engineering domain, this is the first work to provide a thorough and detailed analysis of this technique. Our studies also demonstrate conclusively that the presence of noisy data has a dramatic impact on the effectiveness of imputation techniques.
81|5||UM-RTCOM: An analyzable component model for real-time distributed systems|Component-based development is a key technology in the development of software for modern real-time systems. However, standard component models and tools are not suitable for this type of system, since they do not explicitly address real time, memory or cost constraints. This paper presents a new predictable component model for real-time systems (UM-RTCOM) together with a set of tools to support it. The environment allows new components to be developed which can then be assembled to build complete applications, including hardware interaction. The model includes support for real-time analysis at the component and application level. The analysis is achieved by combining component meta-information in the form of an abstract behaviour model and a method to measure worst-case execution times in the final platform. Additionally, we propose an implementation model based on RT-CORBA where the developer uses the UM-RTCOM components and a set of tools to map these elements to elements of the desired platform. In order to apply our proposals, we have used the model and tools in real applications specifically in the context of nuclear power plant simulators.
81|5||Integrating a software architecture-centric method into object-oriented analysis and design|The choice of methodology for the development of the architecture for software systems has a direct effect on the suitability of that architecture. If the development process is driven by the user’s functional requirements, we would expect the architecture to appropriately reflect those requirements. We would also expect other aspects not captured in the functional specification to be absent from the architecture. The same phenomenon is true in development approaches that stress the importance of systemic quality attributes or other non-functional requirements; those requirements are prominent in the resulting architecture, while other requirement types not stressed by the approach are absent. In other words, the final architecture reflects the focus of the development approach. An ideal approach, therefore, is one that incorporates all goals, expectations, and requirements: both business and technical. To accomplish this we have incorporated, into a single architectural development process, generalized Object-Oriented Analysis and Design (OOAD) methodologies with the software architecture-centric method, the Quality Attribute Workshop (QAW) and Attribute Driven Design (ADD). OOAD, while relatively intuitive, focuses heavily on functional requirements and has the benefit of semantic closeness to the problem domain making it an intuitive process with comprehendible results. Architecture-centric approaches, on the other hand, provide explicit and methodical guidance to an architect in creating systems with desirable qualities and goals. They provide minimal guidance in determining fine-grained architecture, however. The integrated approach described in this paper maximizes the benefits of the respective processes while eliminating their flaws and was applied in a eight university, global development research project with great success. A case study from that experiment is included here to demonstrate the method.
81|5||Teaching disciplined software development|Discipline is an essential prerequisite for the development of large and complex software-intensive systems. However, discipline is also important on the level of individual development activities. A major challenge for teaching disciplined software development is to enable students to experience the benefits of discipline and to overcome the gap between real professional scenarios and scenarios used in software engineering university courses. Students often do not have the chance to internalize what disciplined software development means at both the individual and collaborative level. Therefore, students often feel overwhelmed by the complexity of disciplined development and later on tend to avoid applying the underlying principles. The Personal Software Process (PSP) and the Team Software Process (TSP) are tools designed to help software engineers control, manage, and improve the way they work at both the individual and collaborative level. Both tools have been considered effective means for introducing discipline into the conscience of professional developers. In this paper, we address the meaning of disciplined software development, its benefits, and the challenges of teaching it. We present a quantitative study that demonstrates the benefits of disciplined software development on the individual level and provides further experience and recommendations with PSP and TSP as teaching tools.
81|5||Extracting entity-relationship diagram from a table-based legacy database|Current database reverse engineering researches presume that the information regarding semantics of attributes, primary keys, and foreign keys in database tables is complete. However, this may not be the case. In a recent DBRE effort to derive a data model from a table-based database system, we find the data content of many attributes are not related to their names at all. In this paper, we present a process that extracts an extended entity-relationship diagram from a table-based database with little descriptions for the fields in its tables and no description for keys. The primary inputs of our approach are system display forms, table schema and data instance. We utilize screen displays to construct form instances. Secondly, code analysis and data analysis involving comparisons of fields and decomposition of fields are applied to extract attribute semantics from forms and table schemas, followed by the determination of primary keys, foreign keys and constraints of the database system. In the final step of conceptualization, with the processes of table mergence and relationship identification, an extended ER diagram is successfully extracted in a case study.
81|5||Investigating software process in practice: A grounded theory perspective|This paper presents the results of a study of how software process and software process improvement (SPI) is applied in actual practice in the software industry using the indigenous Irish software product industry as a test-bed. The study used the grounded theory methodology to produce a theory, grounded in the field data, that explains how software processes are formed and evolve and when and why SPI is undertaken. Our research found that SPI programmes are implemented reactively and many software managers are reluctant to implement SPI best practice models because of the associated costs.
81|5||EASY: Efficient semAntic Service discoverY in pervasive computing environments with QoS and context support|Pervasive computing environments are populated with networked software and hardware resources providing various functionalities that are abstracted, thanks to the Service Oriented Architecture paradigm, as services. Within these environments, service discovery enabled by service discovery protocols (SDPs) is a critical functionality for establishing ad hoc associations between service providers and service requesters. Furthermore, the dynamics, the openness and the user-centric vision aimed at by the pervasive computing paradigm call for solutions that enable rich, semantic, context- and QoS-aware service discovery. Although the semantic Web paradigm envisions to achieve such support, current solutions are hardly deployable in the pervasive environment due to the costly underlying semantic reasoning with ontologies. In this article, we present EASY to support efficient, semantic, context- and QoS-aware service discovery on top of existing SDPs. EASY provides EASY-L, a language for semantic specification of functional and non-functional service properties, as well as EASY-M, a corresponding set of conformance relations. Furthermore, EASY provides solutions to efficiently assess conformance between service capabilities. These solutions are based on an efficient encoding technique, as well as on an efficient organization of service repositories (caches), which enables both fast service advertising and discovery. Experimental results show that the deployment of EASY on top of an existing SDP, namely Ariadne, enhancing it only with slight changes to EASY-Ariadne, enables rich semantic, context- and QoS-aware service discovery, which furthermore performs better than the classical, rigid, syntactic matching, and improves the scalability of Ariadne.
81|5||A Web services-based framework for building componentized digital libraries|We present a new Web services-based framework for building componentized digital libraries (DLs). We particularly demonstrate how traditional RDBMS technology can be easily deployed to support several common digital library services. Configuration and customization of the framework to build specialized systems is supported by a wizard-like tool which is based on a generic metamodel for DLs. Such a tool implements a workflow process that segments the DL design tasks into well-defined steps and drives the designer along these steps. Both the framework and the configuration tool are evaluated in terms of several performance and usability criteria. Our experimental evaluation demonstrates the feasibility and superior performance of our framework, as well as the effectiveness of the wizard tool for setting up DLs.
81|5||Mining software repositories for comprehensible software fault prediction models|Software managers are routinely confronted with software projects that contain errors or inconsistencies and exceed budget and time limits. By mining software repositories with comprehensible data mining techniques, predictive models can be induced that offer software managers the insights they need to tackle these quality and budgeting problems in an efficient way. This paper deals with the role that the Ant Colony Optimization (ACO)-based classification technique AntMiner+ can play as a comprehensible data mining technique to predict erroneous software modules. In an empirical comparison on three real-world public datasets, the rule-based models produced by AntMiner+ are shown to achieve a predictive accuracy that is competitive to that of the models induced by several other included classification techniques, such as C4.5, logistic regression and support vector machines. In addition, we will argue that the intuitiveness and comprehensibility of the AntMiner+ models can be considered superior to the latter models.
81|6|http://www.sciencedirect.com/science/journal/01641212/81/6|Agile Product Line Engineering - List of reviewers|
81|6||Editorial|
81|6||Process fusion: An industrial case study on agile software product line engineering|This paper presents a case study of a software product company that has successfully integrated practices from software product line engineering and agile software development. We show how practices from the two fields support the company’s strategic and tactical ambitions, respectively. We also discuss how the company integrates strategic, tactical and operational processes to optimize collaboration and consequently improve its ability to meet market needs, opportunities and challenges. The findings from this study are relevant to software product companies seeking ways to balance agility and product management. The findings also contribute to research on industrializing software engineering.
81|6||A product-line architecture for web service-based visual composition of web applications|A web service-based web application (WSbWA) is a collection of web services or reusable proven software parts that can be discovered and invoked using standard Internet protocols. The use of these web services in the development process of WSbWAs can help overcome many problems of software use, deployment and evolution. Although the cost-effective software engineering of WSbWAs is potentially a very rewarding area, not much work has been done to accomplish short time to market conditions by viewing and dealing with WSbWAs as software products that can be derived from a common infrastructure and assets with a captured specific abstraction in the domain. Both Product Line Engineering (PLE) and Agile Methods (AMs), albeit with different philosophies, are software engineering approaches that can significantly shorten the time to market and increase the quality of products. Using the PLE approach we built, at the domain engineering level, a WSbWA-specific lightweight product-line architecture and combined it, at the application engineering level, with an Agile Method that uses a domain-specific visual language with direct manipulation and extraction capabilities of web services to perform customization and calibration of a product or WSBWA for a specific customer. To assess the effectiveness of our approach we designed and implemented a tool that we used to investigate the return on investment of the activities related to PLE and AMs. Details of our proposed approach, the related tool developed, and the experimental study performed are presented in this article together with a discussion of planned directions of future work.
81|6||Agile product line planning: A collaborative approach and a case study|Agile methods and product line engineering (PLE) have both proven successful in increasing customer satisfaction and decreasing time to market under certain conditions. Key characteristics of agile methods are lean and highly iterative development with a strong emphasis on stakeholder involvement. PLE leverages reuse through systematic approaches such as variability modeling or product derivation. Integrating agile approaches with product line engineering is an interesting proposition which – not surprisingly – entails several challenges: Product lines (PL) rely on complex plans and models to ensure their long-term evolution while agile methods emphasize simplicity and short-term value-creation for customers. When incorporating agility in product line engineering, it is thus essential to define carefully how agile principles can support particular PLE processes. For instance, the processes of defining and setting up a product line (domain engineering) and deriving products (application engineering) differ significantly in practices and focus with implications on the suitability of agile principles. This paper presents practical experiences of adopting agile principles in product line planning (a domain engineering activity). ThinkLets, i.e., collaborative practices from the area of collaboration engineering, are the building blocks of the presented approach as they codify agile principles such as stakeholder involvement, rapid feedback, or value-based prioritization. We discuss how our approach balances agility and the intrinsic needs of product line planning. A case study carried out with an industrial partner indicates that the approach is practicable, usable, and useful.
81|6||Automated error analysis for the agilization of feature modeling|Software Product Lines (SPL) and agile methods share the common goal of rapidly developing high-quality software. Although they follow different approaches to achieve it, some synergies can be found between them by (i) applying agile techniques to SPL activities so SPL development becomes more agile; and (ii) tailoring agile methodologies to support the development of SPL. Both options require an intensive use of feature models, which are usually strongly affected by changes on requirements. Changing large-scale feature models as a consequence of changes on requirements is a well-known error-prone activity. Since one of the objectives of agile methods is a rapid response to changes in requirements, it is essential an automated error analysis support in order to make SPL development more agile and to produce error-free feature models.
81|6||What do software practitioners really think about project success: A cross-cultural comparison|Due to the increasing globalization of software development we are interested to discover if there exist significant cultural differences in practitioners’ definition of a successful software project. This study presents the results of a survey in which Chilean software practitioners’ perceptions of project success are compared with previous research with US practitioners. Responses from both groups of practitioners indicate that there is a relationship between team-work and success; our results also indicate that there are similar perceptions related to the importance of job satisfaction and project success. However, Chilean responses suggest that if a practitioner is allowed too much freedom within the work environment, job stress results; this in turn is reflected in increasing demands for both job satisfaction and good environmental conditions. This may indicate the potential for the attribution of failure to conditions outside the team, thus preventing a search for problematic team issues and technical problems. In contrast, the data suggests peer control inside the US teams indicating a less stressful environment.
81|6||The influence of checklists and roles on software practitioner risk perception and decision-making|This paper investigates: (1) the influence of risk checklists on software practitioner risk perception and decision-making, and (2) the influence of role (inside project manager vs. outside consultant) on software practitioner risk perception and decision-making. Evidence on these points is presented based on a role playing experiment conducted with 128 software practitioners. Results show that (1) the risk checklist helped subjects identify more risks than they would identify without the aid of a checklist, whereas (2) the role assigned to subjects did not seem to affect either their risk perception or behavior. Moreover, the use of a risk checklist shaped subjects’ perceptions of which risks were salient in the scenario, whereas the number of risks identified did not affect their decision-making. Interestingly, subjects using a checklist were able to identify more seeded risks in the scenario, but they also identified more unseeded risks. Implications for research and practice are discussed.
81|6||Setting checkpoints in legacy code to improve fault-tolerance|In this paper we describe the results of a study of the insertion of checkpoints within a legacy software system in the aerospace domain. The purpose of the checkpoints was to improve program fault-tolerance during program execution by rolling back system control to a saved state from which program execution can continue. The study used novice programmers for the determination of where the checkpoints were to be added. The focus was on the programmer’s understanding of the code, since this affected how the checkpoints were placed. The results should provide guidance to those interested in improving the fault-tolerance of legacy software systems, especially those written in older, nearly obsolescent programming languages.
81|6||An XML-based methodology for parametric temporal database model implementation|Parametric data model is one of dimensional data models. It defines attributes as functions, modeling a real world object into a single tuple in a database. Such one-to-one correspondence between an object in the real world and a tuple provides various advantages in modeling dimensional data, avoiding self-joins which frequently appear in temporal data models which fragment an object into multiple tuples. Despite its modeling advantages, it is impractical to implement the parametric data model on top of conventional database systems because of the data model’s variable attribute sizes. However, such implementation challenge can be resolved by XML because XML is flexible for data boundaries. In this paper, we present an XML-based implementation methodology for the parametric temporal data model. In our implementation, we develop our own XML storage called CanStoreX (Canonical Storage for XML) and build the temporal database system on top of the CanStoreX.
81|6||Predictive accuracy comparison of fuzzy models for software development effort of small programs|Regression analysis to generate predictive equations for software development effort estimation has recently been complemented by analyses using less common methods such as fuzzy logic models. On the other hand, unless engineers have the capabilities provided by personal training, they cannot properly support their teams or consistently and reliably produce quality products. In this paper, an investigation aimed to compare personal Fuzzy Logic Models (FLM) with a Linear Regression Model (LRM) is presented. The evaluation criteria were based mainly upon the magnitude of error relative to the estimate (MER) as well as to the mean of MER (MMER). One hundred five small programs were developed by thirty programmers. From these programs, three FLM were generated to estimate the effort in the development of twenty programs by seven programmers. Both the verification and validation of the models were made. Results show a slightly better predictive accuracy amongst FLM and LRM for estimating the development effort at personal level when small programs are developed.
81|6||A survey study of critical success factors in agile software projects|While software is so important for all facets of the modern world, software development itself is not a perfect process. Agile software engineering methods have recently emerged as a new and different way of developing software as compared to the traditional methodologies. However, their success has mostly been anecdotal, and research in this subject is still scant in the academic circles. This research study was a survey study on the critical success factors of Agile software development projects using quantitative approach.
81|6||A practitionerâs guide to light weight software process assessment and improvement planning|Software process improvement (SPI) is challenging, particularly for small and medium sized enterprises. Most existing SPI frameworks are either too expensive to deploy, or do not take an organizations’ specific needs into consideration. There is a need for light weight SPI frameworks that enable practitioners to base improvement efforts on the issues that are the most critical for the specific organization.
81|6||Software reuse: The Brazilian industry scenario|This paper aims at identifying some of the key factors in adopting an organization-wide software reuse program. The factors are derived from practical experience reported by industry professionals, through a survey involving 57 Brazilian small, medium and large software organizations. Some of them produce software with commonality between applications, and have mature processes, while others successfully achieved reuse through isolated, ad hoc efforts. The paper compiles the answers from the survey participants, showing which factors were more associated with reuse success. Based on this relationship, a guide is presented, pointing out which factors should be more strongly considered by small, medium and large organizations attempting to establish a reuse program.
81|6||A strategic analysis for successful open source software utilization based on a structural equation model|Commercial software companies face many challenges when competing in today’s fast moving and competitive industry environment. Recently, the use of open source software (OSS) has been proposed as a possible way to address those challenges. OSS provides many benefits, including high-quality software and substantial profits. Nevertheless, OSS has not been effectively utilized in real business. The purpose of this paper is to find what affects the utilization of OSS. For this study, we propose a structural equation model (SEM) to analyze the relationships between the quality factors based on ISO/IEC 9126 and OSS utilization. In addition, we suggest an open source software utilization index (OSSUI) based on the proposed SEM. The results provide us with the controllable feedback information to improve user (programmer) satisfaction during OSS utilization.
81|6||Enhancing and measuring the predictive capabilities of testing-effort dependent software reliability models|Software testing is necessary to accomplish highly reliable software systems. If the project manager can conduct well-planned testing activities, the consumption of related testing-resources will be cost-effective. Over the past 30 years, many software reliability growth models (SRGMs) have been proposed to estimate the reliability growth of software, and they are mostly applicable to the late stages of testing in software development. Thus far, it appears that most SRGMs do not take possible changes of testing-effort consumption rates into consideration. However, in some cases, the policies of testing-resource allocation could be changed or adjusted. Thus, in this paper, we will incorporate the important concept of multiple change-points into Weibull-type testing-effort functions. The applicability and performance of the proposed models are demonstrated through two real data sets. Experimental results show that the proposed models give a fairly accurate prediction capability. Finally, based on the proposed SRGM, constructive rules are developed for determining optimal software release times.
81|6||Active ERP implementation management: A Real Options perspective|Although enterprise resources planning (ERP) implementation has been one of the most significant challenges of the last decade, it comes with a surprisingly high failure rate due to its high risk nature. The risks of ERP implementation, which involve both technical and social uncertainties, must to be effectively managed. Traditional ERP practices address the implementation of ERP as a static process. Such practices focus on structure, not on ERP as something that will meet the needs of a changing organization. As a result, many relevant uncertainties that cannot be predefined are not accommodated, and cause the implementation fail in the form of project delay and cost overruns. The objective of this paper is to propose an active ERP implementation management perspective to manage ERP risks based on the Real Options (RO) theory, which addresses uncertainties over time, resolves uncertainties in changing environments that cannot be predefined. By actively managing ERP implementation, managers can improve their flexibility, take appropriate action to respond to the often-changing ERP environment, and achieve a more successful ERP implementation.
81|6||An analysis of research topics in software engineering â 2006|This paper is the first in a new annual series whose goal is to answer the following question: what are the active research focuses within the field of software engineering? We considered 7 top journals and 7 top international conferences in software engineering and examined all the 691 papers published in these journals or presented at these conferences in 2006. Consequently, we have a number of findings.
81|6||An assessment of systems and software engineering scholars and institutions (2001â2005)|This paper presents the findings of a five-year study of the top scholars and institutions in the systems and software engineering field, as measured by the quantity of papers published in the journals of the field in 2001–2005. The top scholar is Magne Jørgensen of Simula Research Laboratory, Norway, and the top institution is Korea Advanced Institute of Science and Technology, Korea.
81|6||The Web Resource Space Model, H. Zhuge. Springer (2007)|
81|7|http://www.sciencedirect.com/science/journal/01641212/81/7|MUSEMBLE: A novel music retrieval system with automatic voice query transcription and reformulation|So far, many researches have been done to develop efficient music retrieval systems, and query-by-humming has been considered as one of the most intuitive and effective query methods for music retrieval. For the voice humming to be a reliable query source, elaborate signal processing and acoustic similarity measurement schemes are necessary. On the other hand, recently, there has been an increased interest in query reformulation using relevance feedback with evolutionary techniques such as genetic algorithm for multimedia information retrieval. However, these techniques have not been exploited widely in the field of music retrieval. In this paper, we develop a novel music retrieval system called MUSEMBLE (MUSic enEMBLE) based on two distinct features: (i) A sung or hummed query is automatically transcribed into a sequence of pitch and duration pairs with improved accuracy for music representation. More specifically, we developed two new and unique techniques called WAE (windowed average energy) and dynamic ADF (amplitude-based difference function) onsets for more accurate note segmentation and onset/offset detection in acoustic signal, respectively. The former improved energy-based approaches such as AE by defining small but coherent windows with local and global threshold values. On the other hand, the latter improved the AF (amplitude function) that calculates the summation of the absolute values of signal differences for the clustering energy contour. (ii) A user query is reformulated using user relevance feedback with a genetic algorithm to improve retrieval performance. Even though we have especially focused on humming queries in this paper, MUSEMBLE provides versatile query and browsing interfaces for various kinds of users. We have carried out extensive experiments on the prototype system to evaluate the performance of our voice query transcription and genetic algorithm-based relevance feedback schemes. We demonstrate that our proposed method improves the retrieval accuracy up to 20–40% compared with other popular RF methods. We also show that both WAE and Dynamic ADF methods improve the transcription accuracy up to 95%.
81|7||Availability-based noncontiguous processor allocation policies for 2D mesh-connected multicomputers|Various contiguous and noncontiguous processor allocation policies have been proposed for mesh-connected multicomputers. Contiguous allocation suffers from high external processor fragmentation because it requires that the processors allocated to a parallel job be contiguous and have the same topology as the multicomputer. The goal of lifting the contiguity condition in noncontiguous allocation is reducing processor fragmentation. However, this can increase the communication overhead because the distances traversed by messages can be longer, and messages from different jobs can interfere with each other by competing for communication resources. The extra communication overhead depends on how the allocation request is partitioned and mapped to free processors. In this paper, we investigate a new class of noncontiguous allocation schemes for two-dimensional mesh-connected multicomputers. These schemes are different from previous ones in that request partitioning is based on the submeshes available for allocation. The available submeshes selected for allocation to a job are such that a high degree of contiguity among their processors is achieved. The proposed policies are compared to previous noncontiguous policies using detailed simulations, where several common communication patterns are considered. The results show that the proposed policies can reduce the communication overhead and improve performance substantially.
81|7||Improving the schedulability of soft real-time open dynamic systems: The inheritor is actually a debtor|This paper presents the Clearing Fund Protocol, a three layered protocol designed to schedule soft real-time sets of precedence related tasks with shared resources. These sets are processed in an open dynamic environment. Open because new applications may enter the system at any time and dynamic because the schedulability is tested on-line as tasks request admission. Top-down, the three layers are the Clearing Fund, the Bandwidth Inheritance and two versions of the Constant Bandwidth Server algorithms. Bandwidth Inheritance applies a priority inheritance mechanism to the Constant Bandwidth Server. However, a serious drawback is its unfairness. In fact, a task executing in a server can potentially steal the bandwidth of another server without paying any penalty. The main idea of the Clearing Fund Algorithm is to keep track of processor-time debts contracted by lower priority tasks that block higher priority ones and are executed in the higher priority servers by having inherited the higher priority. The proposed algorithm reduces the undesirable effects of those priority inversions because the blocked task can finish its execution in its own server or in the server of the blocking task, whichever has the nearest deadline. If demanded, debts are paid back in that way. Inheritors are therefore debtors. Moreover, at certain instants in time, all existing debts may be waived and the servers are reset making a clear restart of the system. The Clearing Fund Protocol showed definite better performances when evaluated by simulations against Bandwidth Inheritance, the protocol it tries to improve.
81|7||An efficient algorithm for mining temporal high utility itemsets from data streams|Utility of an itemset is considered as the value of this itemset, and utility mining aims at identifying the itemsets with high utilities. The temporal high utility itemsets are the itemsets whose support is larger than a pre-specified threshold in current time window of the data stream. Discovery of temporal high utility itemsets is an important process for mining interesting patterns like association rules from data streams. In this paper, we propose a novel method, namely THUI (Temporal High Utility Itemsets)-Mine, for mining temporal high utility itemsets from data streams efficiently and effectively. To the best of our knowledge, this is the first work on mining temporal high utility itemsets from data streams. The novel contribution of THUI-Mine is that it can effectively identify the temporal high utility itemsets by generating fewer candidate itemsets such that the execution time can be reduced substantially in mining all high utility itemsets in data streams. In this way, the process of discovering all temporal high utility itemsets under all time windows of data streams can be achieved effectively with less memory space and execution time. This meets the critical requirements on time and space efficiency for mining data streams. Through experimental evaluation, THUI-Mine is shown to significantly outperform other existing methods like Two-Phase algorithm under various experimental conditions.
81|7||Adaptive watermark mechanism for rightful ownership protection|Watermarking is used to protect the integrity and copyright of images. Conventional copyright protection mechanisms; however, are not robust enough or require complex computations to embed the watermark into the host image. In this article, we propose an adaptive copyright protection scheme without the use of discrete cosine transformation (DCT) and discrete wavelet transformation (DWT). This novel approach allows image owners to adjust the strength of watermarks through a threshold, so that the robustness of the watermark can be enhanced. Moreover, our scheme can resist various signal processing operations (such as blurring, JPEG compression, and noising) and geometric transformations (such as cropping, rotation, and scaling). The experimental results show that our scheme outperforms related works in most cases. Specifically, our scheme preserves the data lossless requirement, so it is suitable for medical and artistic images.
81|7||Cryptanalysis of the RCES/RSES image encryption scheme|
81|7||Resource management using multiple feedback loops in soft real-time distributed object systems|This paper describes a Resource Management System for a soft real-time distributed object system that is based on a three-level feedback loop. The Resource Management System employs a profiling algorithm that monitors the usage of the resources, a least laxity scheduling algorithm that schedules the methods of the tasks, and hot spot and cooling algorithms that allocate and migrate objects to balance the loads on the resources. The Resource Management System consists of a single Resource Manager for the distributed system, and a Profiler and a Scheduler located on each of the processors in the distributed system.
81|7||Assessment of high-integrity embedded automotive control systems using hardware in the loop simulation|Sensor-based driver assistance systems often have a safety-related role in modern automotive designs. In this paper we argue that the current generation of “Hardware in the Loop” (HIL) simulators have limitations which restrict the extent to which testing of such systems can be carried out, with the consequence that it is more difficult to make informed decisions regarding the impact of new technologies and control methods on vehicle safety and performance prior to system deployment. In order to begin to address this problem, this paper presents a novel, low-cost and flexible HIL simulator. An overview of the simulator is provided, followed by detailed descriptions of the models that are employed. The effectiveness of the simulator is then illustrated using a case study, in which we examine the performance and safety integrity of eight different designs of a representative distributed embedded control system (a throttle- and brake-by-wire system with adaptive cruise control capability). It is concluded that the proposed HIL simulator provides a highly effective and low-cost test environment for assessing and comparing new automotive control system implementations.
81|7||An efficient iconic indexing strategy for image rotation and reflection in image databases|Spatial relationships are important issues for similarity-based retrieval in many image database applications. With the popularity of digital cameras and the related image processing software, a sequence of images are often rotated or flipped. That is, those images are transformed in the rotation orientation or the reflection direction. However, many iconic indexing strategies based on symbolic projection are sensitive to rotation or reflection. Therefore, these strategies may miss the qualified images, when the query is issued in the orientation different from the orientation of the database images. To solve this problem, some researchers proposed a function to map the spatial relationship to its transformed one. However, this mapping consists of several conditional statements, which is time-consuming. Thus, in this paper, we propose an efficient iconic indexing strategy, in which we carefully assign a unique bit pattern to each spatial relationship and record the spatial information based on the bit patterns in a matrix. Without generating the rotated or flipped image, we can directly derive the index of the rotated or flipped image from the index of the original one by bit operations and matrix manipulation. In our performance study, we analyze the time complexity of our proposed strategy and show the efficiency of our proposed strategy according to the simulation results. Moreover, we implement a prototype to validate our proposed strategy.
81|7||A language for high-level description of adaptive web systems|Adaptive Web systems (AWS) are Web-based systems that can adapt their features such as, presentation, content, and structure, based on users’ behaviour and preferences, device capabilities, and environment attributes. A framework was developed in our research group to provide the necessary components and protocols for the development of adaptive Web systems; however, there were several issues and shortcomings (e.g. low productivity, lack of verification mechanisms, etc.) in using the framework that inspired the development of a domain-specific language for the framework. This paper focuses on the proposal, design, and implementation of AWL, the Adaptive Web Language, which is used to develop adaptive Web systems within our framework. Not only does AWL address the existing issues in the framework, but it also offers mechanisms to increase software quality attributes, especially, reusability. An example application named PENS (a personalized e-News system) is explained and implemented in AWL. AWL has been designed based on the analysis of the adaptive Web domain, having taken into account the principles of reuse-based software engineering (product-lines), domain-specific languages, and aspect-oriented programming. Specially, a novel design decision, inspired by aspect-oriented programming paradigm, allows separate specification of presentation features in an application from its adaptation features. The AWL’s design decisions and their benefits are explained.
81|7||Improved certificate-based encryption in the standard model|Certificate-based encryption has been recently proposed as a means to simplify the certificate management inherent to traditional public key encryption. In this paper, we present an efficient certificate-based encryption scheme which is fully secure in the standard model. Our construction is more efficient (in terms of computational cost and ciphertext size) than any of the previous constructions known without random oracles.
81|7||Recursive protocol for group-oriented authentication with key distribution|The authors propose a recursive protocol for group-oriented authentication with key exchange, in which a group of n entities can authenticate with each other and share a group session key. The proposed protocol has the following characteristics: First, it requires O(n) rounds of messages, O(log n) completion time, O(log n) waiting time, and O(n log n) communication overhead in average for the completion of the recursion. Second, it not only meets the five principles suggested by Diffie et al. [Diffie, W., van Oorschot, P.C., Wiener, M.J., 1992. Authentication and authenticated key exchange. Designs, Codes, and Cryptography 2 (2), 107–125] on the design of a secure key exchange protocol, but also achieves the properties of nondisclosure, independency, and integrity addressed by Janson and Tsudik [Janson, P., Tsudik, G., 1995. Secure and minimal protocols for authenticated key distribution. Computer Communications 18 (9), 645–653] for the authentication of the group session key. Third, we describe the beliefs of trustworthy entities involved in our authentication protocol and the evolution of these beliefs as a consequence of communication by using BAN logic. Finally, it is practical and efficient, because only one-way hash function and exclusive-or (XOR) operations are used in implementation.
81|7||A pairing SW implementation for Smart-Cards|The aim of this work is to show the feasibility of the primitives of the identity based cryptosystems for applications in Smart-Cards. Several observations are applied to easily choose many supersingular elliptic curves over a prime field Fp,p>3,p≡3mod4, in such a way that the size of the torsion subgroup, the curve order and the finite field characteristic are of minimal Hamming weight. We modify the Chudnovsky elliptic curve point representation to settle a dedicated coordinate system for pairings and to minimize the number of operations in the finite field. The encouraging timing results obtained for ST22 Smart-Card architecture show the feasibility of pairing primitives for embedded devices.
81|7||Why and how can human-related measures support software development processes?|In this paper we discuss why and how measures related to human aspects should be incorporated into software development processes. This perspective is based on the vast evidence that human aspects are the source of the majority of problems associated with software development projects. Having said that, we do not blame the humans involved in software development processes; rather, we suggest that human-related measures might be one means by which human aspects of software development processes can be supported.
81|8|http://www.sciencedirect.com/science/journal/01641212/81/8|Systematic approaches to understanding and evaluating design trade-offs|The use of trade-off analysis as part of optimising designs has been an emerging technique for a number of years. However, only recently has much work been done with respect to systematically deriving the understanding of the system problem to be optimised and using this information as part of the design process. As systems have become larger and more complex then a need has arisen for suitable approaches. The system problem consists of design choices, measures for individual values related to quality attributes and weights to balance the relative importance of each individual quality attribute. In this paper, a method is presented for establishing an understanding of a system problem using the goal structuring notation (GSN). The motivation for this work is borne out of experience working on embedded systems in the context of critical systems where the cost of change can be large and the impact of design errors potentially catastrophic. A particular focus is deriving an understanding of the problem so that different solutions can be assessed quantitatively, which allows more definitive choices to be made. A secondary benefit is it also enables design using heuristic search approaches which is another area of our research. The overall approach is demonstrated through a case study which is a task allocation problem.
81|8||A pattern language for designing e-business architecture|The pattern language for e-business provides a holistic support for developing software architectures for the e-business domain. The pattern language contains four related pattern categories: Business Patterns, Integration Patterns, Application Patterns, and Runtime Patterns. These pattern categories organise an e-business architecture into three layers—business interaction, application infrastructure and middleware infrastructure—and provide reusable design solutions to these layers in a top–down decomposition fashion. Business and Integration Patterns partition the business interaction layer into a set of subsystems; Application Patterns provide a high-level application infrastructure for these subsystems and separate business abstractions from their software solutions; Runtime Patterns then define a middleware infrastructure for the subsystems and shield design solutions from their implementations. The paper describes, demonstrates and evaluates this pattern language.
81|8||A work product pool approach to methodology specification and enactment|Software development methodologies advocated and used today, whether traditional and plan-based or contemporary and agile, usually focus on process steps i.e. they start with requirements and iteratively describe what steps are necessary to move to the next stage or phase, until the software application is delivered to the end user. Such a process-oriented view of methodologies, based on the metaphor that human organizations are “machines” that “execute” processes, often results in methodologies that are too rigid and hard to follow, and most often than not end up being ignored or bypassed. Our proposal here is that, since the ultimate aim of software development is to provide a software product, software development methodologies should be described in terms of the intermediate products that are necessary to reach such a final product, plus the needed micro-processes that, as necessary evils, will be required to produce the appropriate work products from other, previously created ones. Using this product-oriented approach, software development methodologies can be specified that are, at least, as flexible as lightweight, agile approaches and, at the same time, as powerful and scalable as plan-oriented ones.
81|8||Role engineering: From design to evolution of security schemes|This paper presents a methodology to design the RBAC (Role-Based Access Control) scheme during the design phase of an Information System. Two actors, the component developer and the security administrator, will cooperate to define and set up the minimal set of roles in agreement with the application constraints and the organization constraints that guarantee the global security policy of an enterprise. In order to maintain the global coherence of the existing access control scheme, an algorithm is proposed to detect the possible inconsistencies before the integration of a new component in the Information System.
81|8||Quantitative risk-based security prediction for component-based systems with explicitly modeled attack profiles|Systems and software architects require quantitative dependability evaluations, which allow them to compare the effect of their design decisions on dependability properties. For security, however, quantitative evaluations have proven difficult, especially for component-based systems. In this paper, we present a risk-based approach that creates modular attack trees for each component in the system. These modular attack trees are specified as parametric constraints, which allow quantifying the probability of security breaches that occur due to internal component vulnerabilities as well as vulnerabilities in the component’s deployment environment. In the second case, attack probabilities are passed between system components as appropriate to model attacks that exploit vulnerabilities in multiple system components. The probability of a successful attack is determined with respect to a set of attack profiles that are chosen to represent potential attackers and corresponding environmental conditions. Based on these attack probabilities and the structure of the modular attack trees, risk measures can be estimated for the complete system and compared with the tolerable risk demanded by stakeholders. The practicability of this approach is demonstrated with an example that evaluates the confidentiality of a distributed document management system.
81|8||Client-side selection of replicated web services: An empirical assessment|Replicating web services over physically distributed servers can offer client applications a number of QoS benefits, including higher availability and reduced response time. However, selecting the “best” service replica to invoke at the client-side is not a trivial task, as this requires taking into account factors such as local and external network conditions, and the servers’ current workload. This paper presents an empirical assessment of five representative client-side service selection policies for accessing replicated web services. The assessment measured the response time obtained with each of the five policies, at two different client configurations, when accessing a world-wide replicated service with four replicas located in three continents. The assessment’s results were analyzed both quantitatively and qualitatively. In essence, the results show that, in addition to the QoS levels provided by the external network and the remote servers, characteristics of the local client environment can have a significant impact on the performance of some of the policies investigated. In this regard, the paper presents a set of guidelines to help application developers in identifying a server selection policy that best suits a particular service replication scenario.
81|8||XML-based agent communication, migration and computation in mobile agent systems|This article presents the research work that exploits using XML (Extensible Markup Language) to represent different types of information in mobile agent systems, including agent communication messages, mobile agent messages, and other system information. The goal of the research is to build a programmable information base in mobile agent systems through XML representations. The research not only studies using XML in binary agent system space such as representing agent communication messages and mobile agent messages, but also explores interpretive XML data processing to avoid the need of an interface layer between script mobile agents and system data represented in XML. These XML-based information representations have been implemented in Mobile-C, a FIPA (The Foundation for Intelligent Physical Agents) compliant mobile agent platform. Mobile-C uses FIPA ACL (Agent Communication Language) messages for both inter-agent communication and inter-platform migration. Using FIPA ACL messages for agent migration in FIPA compliant agent systems simplifies agent platform, reduces development effort, and easily achieves inter-platform migration through well-designed communication mechanisms provided in the system. The ability of interpretive XML data processing allows mobile agents in Mobile-C directly accessing XML data information without the need of an extra interface layer.
81|8||Document recommendation for knowledge sharing in personal folder environments|Sharing sustainable and valuable knowledge among knowledge workers is a fundamental aspect of knowledge management. In organizations, knowledge workers usually have personal folders in which they organize and store needed codified knowledge (textual documents) in categories. In such personal folder environments, providing knowledge workers with needed knowledge from other workers’ folders is important because it increases the workers’ productivity and the possibility of reusing and sharing knowledge. Conventional recommendation methods can be used to recommend relevant documents to workers; however, those methods recommend knowledge items without considering whether the items are assigned to the appropriate category in the target user’s personal folders. In this paper, we propose novel document recommendation methods, including content-based filtering and categorization, collaborative filtering and categorization, and hybrid methods, which integrate text categorization techniques, to recommend documents to target worker’s personalized categories. Our experiment results show that the hybrid methods outperform the pure content-based and the collaborative filtering and categorization methods. The proposed methods not only proactively notify knowledge workers about relevant documents held by their peers, but also facilitate push-mode knowledge sharing.
81|8||Cronus: A platform for parallel code generation based on computational geometry methods|This paper describes Cronus, a platform for parallelizing general nested loops. General nested loops contain complex loop bodies (assignments, conditionals, repetitions) and exhibit uniform loop-carried dependencies. The novelty of Cronus is twofold: (1) it determines the optimal scheduling hyperplane using the QuickHull algorithm, which is more efficient than previously used methods, and (2) it implements a simple and efficient dynamic rule (successive dynamic scheduling) for the runtime scheduling of the loop iterations along the optimal hyperplane. This scheduling policy enhances data locality and improves the makespan. Cronus provides an efficient runtime library, specifically designed for communication minimization, that performs better than more generic systems, such as Berkeley UPC. Its performance was evaluated through extensive testing. Three representative case studies are examined: the Floyd–Steinberg dithering algorithm, the Transitive Closure algorithm, and the FSBM motion estimation algorithm. The experimental results corroborate the efficiency of the parallel code. The tests show speedup ranging from 1.18 (out of the ideal 4) to 12.29 (out of the ideal 16) on distributed-systems and 3.60 (out of 4) to 15.79 (out of 16) on shared-memory systems. Cronus outperforms UPC by 5–95% depending on the test case.
81|8||An experimental study of adaptive testing for software reliability assessment|Adaptive testing is a new form of software testing that is based on the feedback and adaptive control principle and can be treated as the software testing counterpart of adaptive control. Our previous work has shown that adaptive testing can be formulated and guided in theory to minimize the variance of an unbiased software reliability estimator and to achieve optimal software reliability assessment. In this paper, we present an experimental study of adaptive testing for software reliability assessment, where the adaptive testing strategy, the random testing strategy and the operational profile based testing strategy were applied to the Space program in four experiments. The experimental results demonstrate that the adaptive testing strategy can really work in practice and may noticeably outperform the other two. Therefore, the adaptive testing strategy can serve as a preferable alternative to the random testing strategy and the operational profile based testing strategy if high confidence in the reliability estimates is required or the real-world operational profile of the software under test cannot be accurately identified.
81|8||A quantitative approach for evaluating the quality of design patterns|In recent years, the influence of design patterns on software quality has attracted an increasing attention in the area of software engineering, as design patterns encapsulate valuable knowledge to resolve design problems, and more importantly to improve design quality. As the paradigm continues to increase in popularity, a systematic and objective approach to verify the design of a pattern is increasingly important. The intent session in a design pattern indicates the problem the design pattern wants to resolve, and the solution session describes the structural model for the problem. When the problem in the intent is a quality problem, the structure model should provide a solution to improve the relevant quality. In this work we provide an approach, based on object-oriented quality model, to validate if a design pattern is well-designed, i.e., it answers the question of the proposed structural model really resolves the quality problems described in the intent. We propose a validation approach to help pattern developers check if a design pattern is well-designed. In addition, a quantitative method is proposed to measure the effectiveness of the quality improvement of a design pattern that pattern users can determine which design patterns are applicable to meet their functional and quality requirements.
81|9|http://www.sciencedirect.com/science/journal/01641212/81/9|Editorial|
81|9||Analysis of architecture evaluation data|The output of 18 software architecture evaluations is analyzed. The goal of the analysis is to find patterns in the important quality attributes and risk themes identified in the evaluations. The major results are
81|9||Architectural knowledge discovery with latent semantic analysis: Constructing a reading guide for software product audits|Architectural knowledge is reflected in various artifacts of a software product. In a software product audit this architectural knowledge needs to be uncovered and its effects assessed in order to evaluate the quality of the software product. A particular problem is to find and comprehend the architectural knowledge that resides in the software product documentation. In this article, we discuss how the use of a technique called Latent Semantic Analysis can guide auditors through the documentation to the architectural knowledge they need. We validate the use of Latent Semantic Analysis for discovering architectural knowledge by comparing the resulting vector-space model with the mental model of documentation that auditors possess.
81|9||Software architecting without requirements knowledge and experience: What are the repercussions?|Whereas the relationship between Requirements Engineering and Software Architecture (SA) has been studied increasingly in recent years in terms of methods, notations, representations, tools, development paradigms and project experiences, that in terms of the human agents conducting these processes has not been explored scientifically. This paper describes the impact of requirements knowledge and experience (RKE) on software architecting tasks. Specifically, it describes an exploratory, empirical study involving 15 architecting teams, approximately evenly split between those teams with RKE and those without. Each team developed its own system architecture from the same given set of requirements in the banking domain. The subjects were all final year undergraduate or graduate students enrolled in a university-level course on software architectures. The overall results of this study suggest that architects with RKE develop higher-quality software architectures than those without, and that they have fewer architecture-development problems than did the architects without RKE. This paper identifies specific areas of both architecture design as well as the architecture-development process where the differences manifest between the RKE and non-RKE architects. The paper also describes the possible implications of the findings on the areas of hiring and training, pedagogy, and technology. The empirical study was carried out using the “mixed methods” approach, involving both quantitative and qualitative aspects of the investigation. A bi-product of this study is an architectural assessment instrument (included in the Appendix) for quantitative analysis of the quality of a software architecture. This paper also describes some new threads for future work.
81|9||Conflict detection and resolution for workflows constrained by resources and non-determined durations|The correctness of a workflow specification is critical for the automation of business processes. Therefore, errors in the specification should be detected and corrected at build-time. In this paper, we present a conflict verification and resolution approach for a kind of workflow constrained by resources and non-determined duration based on Petri net. In this kind of workflow, there are two timing functions for each activity to present the minimum and maximum duration of each activity, and the implementations of some activities require resources. Based on the Petri net model obtained, the earliest time to start each activity can be calculated and the key activities influencing the implementation of the workflow can be determined, with which the resource consistency between activities can be verified. Key-activity and waiting-short priority strategies are adopted to remove the resource conflicts between activities, which can ensure that most of the subsequent activities start as early as possible and that the whole workflow be finished in a shorter time. Through experiments, it is proved that the proposed removal strategy for resource conflicts is better than other strategies.
81|9||An embedding technique based upon block prediction|This paper presents a novel data hiding scheme for VQ compression images. This scheme first uses SMVQ prediction to classify encoding blocks into different types, then uses different codebooks and encoding strategies to perform encoding and data hiding simultaneously. In using SMVQ prediction, no extra data is required to identify the combination of encoding strategies and codebook, which helps improve compression performance. Furthermore, the proposed scheme adaptively combines VQ and SMVQ encoding characteristics to provide higher image quality of stego-images while size of the hidden payload remains the same. Experimental results show that the proposed scheme indeed outperforms other previously proposed schemes in image quality of the stego-images and compression performance.
81|9||Designing and evaluating interleaving decompressing and virus scanning in a stream-based mail proxy|A storage-based anti-virus access gateway is not scalable because it stores the entire mail under processing. This work designs and evaluates a stream-based mail proxy constructed from several open-source packages. This proxy processes mail in segments, and interleaves MIME parsing, decoding, decompressing and virus scanning. It is seven times faster than the storage-based one on forwarding, three times faster on virus scanning, and twice as faster on decompressing plus virus scanning. This proxy can keep nearly constant memory usage and work without disks, while the storage-based one requires memory and disk space proportional to the number of clients and the mail size.
81|9||A new region filtering and region weighting approach to relevance feedback in content-based image retrieval|A new region filtering and region weighting method, which filters out unnecessary regions from images and learns region importance from the region size and the spatial location of regions in an image, is proposed based on region representations. It weights the regions optimally and improves the performance of the region-based retrieval system based on relevance feedback. Due to the semantic gap between the low level feature representation and the high level concept in a query image, semantically relevant images may exhibit very different visual characteristics, and may be scattered in several clusters in the feature space. Our main goal is finding semantically related clusters and their weights to reduce this semantic gap. Experimental results demonstrate the efficiency and effectiveness of the proposed region filtering and weighting method in comparison with the area percentage method and region frequency weighted by inverse image frequency method, respectively.
81|9||Web proxy cache replacement scheme based on back-propagation neural network|Web proxy caches are used to reduce the strain of contemporary web traffic on web servers and network bandwidth providers. In this research, a novel approach to web proxy cache replacement which utilizes neural networks for replacement decisions is developed and analyzed. Neural networks are trained to classify cacheable objects from real world data sets using information known to be important in web proxy caching, such as frequency and recency. Correct classification ratios between 0.85 and 0.88 are obtained both for data used for training and data not used for training. Our approach is compared with Least Recently Used (LRU), Least Frequently Used (LFU) and the optimal case which always rates an object with the number of future requests. Performance is evaluated in simulation for various neural network structures and cache conditions. The final neural networks achieve hit rates that are 86.60% of the optimal in the worst case and 100% of the optimal in the best case. Byte-hit rates are 93.36% of the optimal in the worst case and 99.92% of the optimal in the best case. We examine the input-to-output mappings of individual neural networks and analyze the resulting caching strategy with respect to specific cache conditions.
81|9||The MPEG-7 Multimedia Database System (MPEG-7 MMDB)|Broadly used Database Management Systems (DBMS) propose multimedia extensions, like Oracle’s Multimedia (formerly interMedia). However, these extensions lack means for managing the requirements of multimedia data in terms of semantic meaningful querying, advanced indexing, content modeling and multimedia programming libraries.
81|9||A round- and computation-efficient three-party authenticated key exchange protocol|In three-party authenticated key exchange protocols, each client shares a secret only with a trusted server with assists in generating a session key used for securely sending messages between two communication clients. Compared with two-party authenticated key exchange protocols where each pair of parties must share a secret with each other, a three-party protocol does not cause any key management problem for the parties. In the literature, mainly there exist three issues in three-party authenticated key exchange protocols are discussed that need to be further improved: (1) to reduce latency, communication steps in the protocol should be as parallel as possible; (2) as the existence of a security-sensitive table on the server side may cause the server to become compromised, the table should be removed; (3) resources required for computation should be as few as possible to avoid the protocol to become an efficiency bottleneck. In various applications over networks, a quick response is required especially by light-weight clients in the mobile e-commerce. In this paper, a round- and computation-efficient three-party authenticated key exchange protocol is proposed which fulfils all of the above mentioned requirements.
81|9||Virtualization-based autonomic resource management for multi-tier Web applications in shared data center|As large data centers emerge, which host multiple Web applications, it is critical to isolate different application environments for security reasons and to provision shared resources effectively and efficiently to meet different service quality targets at minimum operational cost. To address this problem, we developed a novel architecture of resource management framework for multi-tier applications based on virtualization mechanisms. Key techniques presented in this paper include (1) establishment of the analytic performance model which employs probabilistic analysis and overload management to deal with non-equilibrium states; (2) a general formulation of the resource management problem which can be solved by incorporating both deterministic and stochastic optimizing algorithms; (3) deployment of virtual servers to partition resource at a much finer level; and (4) investigation of the impact of the failure rate to examine the effect of application isolation. Simulation experiments comparing three resource allocation schemes demonstrate the advantage of our dynamic approach in providing differentiated service qualities, preserving QoS levels in failure scenarios and also improving the overall performance while reducing the resource usage cost.
81|9||The consistency among facilitating factors and ERP implementation success: A holistic view of fit|Traditionally, various ERP implementation factors have been deemed critical to success within diverse business environments. The interaction relationships among these ERP implementation success factors, however, have been overlooked. The objective of this study is to explore the interaction patterns among the ERP implementation success factors from a covariation (co-alignment) perspective. We conceptualize the “consistency” among the factors that facilitate ERP implementation and evaluate them in terms of their positive impact on successful ERP implementation. The results from a field survey of 90 Taiwanese manufacturing firms show that the “consistency” among these facilitating factors of ERP implementation had a significant positive impact on ERP implementation success. The factors examined in this study include vendor support, consultant competence, ERP project team member competence, ERP project manager leadership, top management support, and user support. Implications for managers and researchers conclude this study.
82|1|http://www.sciencedirect.com/science/journal/01641212/82/1|Guest editorial|
82|1||The Palladio component model for model-driven performance prediction|One aim of component-based software engineering (CBSE) is to enable the prediction of extra-functional properties, such as performance and reliability, utilising a well-defined composition theory. Nowadays, such theories and their accompanying prediction methods are still in a maturation stage. Several factors influencing extra-functional properties need additional research to be understood. A special problem in CBSE stems from its specific development process: Software components should be specified and implemented independently from their later context to enable reuse. Thus, extra-functional properties of components need to be specified in a parametric way to take different influencing factors like the hardware platform or the usage profile into account. Our approach uses the Palladio component model (PCM) to specify component-based software architectures in a parametric way. This model offers direct support of the CBSE development process by dividing the model creation among the developer roles. This paper presents our model and a simulation tool based on it, which is capable of making performance predictions. Within a case study, we show that the resulting prediction accuracy is sufficient to support the evaluation of architectural design decisions.
82|1||Achieving efficiency, quality of service and robustness in multi-organizational Grids|Scalability, flexibility, quality of service provisioning, efficiency and robustness are the desired characteristics of most computing systems. Although the emerging Grid computing paradigm is scalable and flexible, achieving both efficiency and quality of service provisioning in Grids is a challenging task but is necessary for the wide adoption of Grids. Grid middleware should also be robust to uncertainties such as those in user-estimated runtimes of Grid applications. In this paper, we present a complete middleware framework for Grids that achieves user satisfaction by providing QoS guarantees for Grid applications, cost effectiveness by efficiently utilizing resources and robustness by intelligently handling uncertain runtimes of applications.
82|1||An overhead and resource contention aware analytical model for overloaded Web servers|A Web server, when overloaded, shows a severe degradation of goodput initially, with the eventual settling of goodput as load increases further. Traditional performance models have failed to capture this behavior. In this paper, we propose an analytical model, which is a two-stage and layered queuing model of the Web server, which is able to reproduce this behavior. We do this by explicitly modelling the overhead processing, the user abandonment and retry behavior, and the contention for resources, for the FIFO and LIFO queuing disciplines. We show that LIFO provides better goodput in most overload situations. We compare our model predictions with experimental results from a test bed and find that our results match well with measurements.
82|1||Performance analysis of security aspects by weaving scenarios extracted from UML models|Aspect-oriented modeling (AOM) allows software designers to describe features that address pervasive concerns separately as aspects, and to systematically incorporate the features into a design model using model composition techniques. The goal of this paper is to analyze the performance effects of different security features that may be represented as aspect models. This is part of a larger research effort to integrate methodologies and tools for the analysis of security and performance properties early in the software development process. In this paper, we describe an extension to the AOM approach that provides support for performance analysis. We use the performance analysis techniques developed previously in the PUMA project, which take as input UML models annotated with the standard UML Profile for Schedulability, Performance and Time (SPT), and transform them first into Core Scenario Model (CSM), and then into different performance models. The composition of the aspects with the primary (base) model is performed at the CSM level. A new formal definition of CSM properties and operations is described as a foundation for scenario-based weaving. The proposed approach is illustrated with an example that utilizes two standards, TPC-W and SSL.
82|1||A practical approach for performance-driven UML modelling of handheld devices â A case study|In this article, we present a performance engineering enhanced modelling methodology for designing embedded devices and describe the experiences we have gained in applying this methodology during the design of a DVB-H enabled handheld device. The methodology uses UML 2.0 to model the system following a strict separation of architectural and behavioural aspects of the system. For this purpose we employ the new composite structure diagram and show its advantages over already established approaches. This methodology specially aims on an easy application by non performance experts. From the model, a multiclass queueing network is generated for the analysis of the system performance. The configuration of hardware resources and resource demands is done using the standard SPT Profile which is extended where necessary. This makes queueing theory accessible to system designers even if they are not familiar with the underlying mathematics. In this way the acceptance of developers to use performance engineering in their daily work is increased. Special attention has been put on an easy evaluation of design alternatives. We describe our implementation and its seamless integration into a UML 2.0 CASE tool.
82|1||Developing reusable simulation core code for networking: The grid resource discovery example|In this work, first, we present a grid resource discovery protocol that discovers computing resources without the need for resource brokers to track existing resource providers. The protocol uses a scoring mechanism to aggregate and rank resource provider assets and Internet router data tables (called grid routing tables) for storage and retrieval of the assets. Then, we discuss the simulation framework used to model the protocol and the results of the experimentation. The simulator utilizes a simulation engine core that can be reused for other network protocol simulators considering time management, event distribution, and a simulated network infrastructure. The techniques for constructing the simulation core code using C++/CLR are also presented in this paper.
82|1||A security policy language for wireless sensor networks|Authenticated computer system users are only authorized to access certain data within the system. In the future, wireless sensor networks (WSNs) will need to restrict access to data as well. To date, WSN security has largely been based on encryption and authentication schemes. The WSN Authorization Specification Language (WASL) is a mechanism-independent composable WSN policy language that can specify arbitrary and composable security policies that are able to span and integrate multiple WSN policies. Using WASL, a multi-level security policy for a 1000 node network requires only 60 bytes of memory per node.
82|1||A model of domain-polymorph component for heterogeneous system design|Heterogeneous systems mix different technical domains such as signal processing, analog and digital electronics, software, telecommunication protocols, etc. Heterogeneous systems are composed of subsystems that are designed using different models of computation (MoC). These MoCs are the laws that govern the interactions of the components of a subsystem. The design of heterogeneous systems includes the design of each part of the system according to its specific MoC, and the connection of the parts in order to build the model representing the system. Indeed, this model allows the MoCs that govern different parts of system to coexist and interact.
82|1||Model-based performance analysis using block coverage measurements|The primary advantage of model-based performance analysis is its ability to facilitate sensitivity and predictive analysis, in addition to providing an estimate of the application performance. To conduct model-based analysis, it is necessary to build a performance model of an application which represents the application structure in terms of the interactions among its components, using an appropriate modeling paradigm. While several research efforts have been devoted to the development of the theoretical aspects of model-based analysis, its practical applicability has been limited despite the advantages it offers. This limited practical applicability is due to the lack of techniques available to estimate the parameters of the performance model of the application. Since the model parameters cannot be estimated in a realistic manner, the results obtained from model-based analysis may not be accurate.
82|1||CoRAL: A transparent fault-tolerant web service|The Web is increasingly used for critical applications and services. We present a client-transparent mechanism, called CoRAL, that provides high reliability and availability for Web service. CoRAL provides fault tolerance even for requests being processed at the time of server failure. The scheme does not require deterministic servers and can thus handle dynamic content. CoRAL actively replicates the TCP connection state while maintaining logs of HTTP requests and replies. In the event of a primary server failure, active client connections fail over to a spare, where their processing continues seamlessly. We describe key aspects of the design and implementation as well as several performance optimizations. Measurements of system overhead, failover performance, and preliminary validation using fault injection are presented.
82|1||An anomaly prevention approach for real-time task scheduling|This research responds to practical requirements in the porting of embedded software over platforms and the well-known multiprocessor anomaly. In particular, we consider the task scheduling problem when the system configuration changes. With mutual-exclusive resource accessing, we show that new violations of the timing constraints of tasks might occur even when a more powerful processor or device is adopted. The concept of scheduler stability and rules are then proposed to prevent scheduling anomaly from occurring in task executions that might be involved with task synchronization or I/O access. Finally, we explore policies for bounding the duration of scheduling anomalies.
82|1||Mining temporal interval relational rules from temporal data|Temporal data mining is still one of important research topic since there are application areas that need knowledge from temporal data such as sequential patterns, similar time sequences, cyclic and temporal association rules, and so on. Although there are many studies for temporal data mining, they do not deal with discovering knowledge from temporal interval data such as patient histories, purchaser histories, and web logs etc. We propose a new temporal data mining technique that can extract temporal interval relation rules from temporal interval data by using Allen’s theory: a preprocessing algorithm designed for the generalization of temporal interval data and a temporal relation algorithm for mining temporal relation rules from the generalized temporal interval data. This technique can provide more useful knowledge in comparison with conventional data mining techniques.
82|1||A scoped approach to traceability management|Traceability is the ability to describe and follow the life of a software artifact and a means for modeling the relations between software artifacts in an explicit way. Traceability has been successfully applied in many software engineering communities and has recently been adopted to document the transition among requirements, architecture and implementation. We present an approach to customize traceability to the situation at hand. Instead of automating tracing, or representing all possible traces, we scope the traces to be maintained to the activities stakeholders must carry out. We define core traceability paths, consisting of essential traceability links required to support the activities. We illustrate the approach through two examples: product derivation in software product lines, and release planning in software process management. By using a running software product line example, we explain why the core traceability paths identified are needed when navigating from feature to structural models and from family to product level and backward between models used in software product derivation. A feasibility study in release planning carried out in an industrial setting further illustrates the use of core traceability paths during production and measures the increase in performance of the development processes supported by our approach. These examples show that our approach can be successfully used to support both product and process traceability in a pragmatic yet efficient way.
82|10|http://www.sciencedirect.com/science/journal/01641212/82/10|Introduction|
82|10||Understanding the effects of requirements volatility in software engineering by using analytical modeling and software process simulation|This paper introduces an executable system dynamics simulation model developed to help project managers comprehend the complex impacts related to requirements volatility on a software development project. The simulator extends previous research and adds research results from an empirical survey, including over 50 new parameters derived from the associated survey data, to a base model. The paper discusses detailed results from two cases that show significant cost, schedule, and quality impacts as a result of requirements volatility. The simulator can be used as an effective tool to demonstrate the complex set of factor relationships and effects related to requirements volatility.
82|10||Experience on knowledge-based software engineering: A logic-based requirements language and its industrial applications|A formal requirements specification language plays an important role in software development. Not only can such language be used for stating requirements specification, but also can be used in many phases of software development life cycle. The FRORL project started from constructing a language with a solid logical foundation and further expanded to research in verification, validation, requirements analysis, debugging, and transformation. Research in this project aided in some industrial applications in which a code generation tool produced software for embedded systems. This article reports the experiences gained from this project and states the value of research in knowledge-based software engineering.
82|10||Dynamically reconfigurable hardwareâsoftware architecture for partitioning networking functions on the SoC platform|We present an issue of the dynamically reconfigurable hardware–software architecture which allows for partitioning networking functions on a SoC (System on Chip) platform. We address this issue as a partition problem of implementing network protocol functions into dynamically reconfigurable hardware and software modules. Such a partitioning technique can improve the co-design productivity of hardware and software modules. Practically, the proposed partitioning technique, which is called the ITC (Inter-Task Communication) technique incorporating the RT-IJC2 (Real-Time Inter-Job Communication Channel), makes it possible to resolve the issue of partitioning networking functions into hardware and software modules on the SoC platform. Additionally, the proposed partitioning technique can support the modularity and reuse of complex network protocol functions, enabling a higher level of abstraction of future network protocol specifications onto the SoC platform. Especially, the RT-IJC2 allows for more complex data transfers between hardware and software tasks as well as provides real-time data processing simultaneously for given application-specific real-time requirements. We conduct a variety of experiments to illustrate the application and efficiency of the proposed technique after implementing it on a commercial SoC platform based on the Altera’s Excalibur including the ARM922T core and up to 1 million gates of programmable logic.
82|10||Detecting artifact anomalies in business process specifications with a formal model|Many business process analysis models have been proposed, however there are few discussions for artifact usages in workflow specifications. A well-structured business process with sufficient resources might fail or yield unexpected results dynamically due to inaccurate artifact specification, e.g. an inconsistency between artifact and control flow, or contradictions between artifact operations. This paper, based on our previous work, presents a model for describing the input/output of a workflow process and analyzes the artifact usages upon the model. This work identifies and formulates thirteen cases of artifact usage anomalies affecting process execution and categorizes the cases into three types. Moreover, the methods for detecting these anomalies with time complexities O(n2), less than O(n3) in previous methods, are presented. Besides, the paper uses an example to demonstrate the processing of them.
82|10||Challenge and solutions of NAT traversal for ubiquitous and pervasive applications on the Internet|Network Address Translator (NAT) has brought up many changes and opportunities to the Internet. How do the ubiquitous and pervasive applications coexist with NAT and interoperate with each other? In this article, we discuss the essence of NAT sensitive applications as well as the challenge and response for various NAT traversal solutions. All questions pointed to redesign a new NAT framework with a major change to accommodate NAT problems all at once. We introduce a novel next generation NAT (NATng) framework, which consists of a Bi-directional NAT (BNAT) and a Domain Name System Application Level Gateway (DNS_ALG) with a Border Network Address Translator Control Protocol (BNATCP) function to control all BNATs. The above components coordinate and provide bidirectional access capability between intranet and Internet, so all private hosts can be addressed via Fully Qualified Domain Name (FQDN). Logistically, NATng extends the IPv4 address space from 232to248 or even more. It features high potential to solve the problems for ubiquitous and pervasive applications which may encounter IPv4 address exhaustion on the current Internet.
82|10||Modeling and verification of real-time embedded systems with urgency|Real-time embedded systems are often designed with different types of urgencies such as delayable or eager, that are modeled by several urgency variants of the timed automata model. However, most model checkers do not support such urgency semantics, except for the IF toolset that model checks timed automata with urgency against observers. This work proposes an Urgent Timed Automata (UTA) model with zone-based urgency semantics that gives the same model checking results as absolute urgency semantics of other existing urgency variants of the timed automata model, including timed automata with deadlines and timed automata with urgent transitions. A necessary and sufficient condition, called complete urgency, is formulated and proved for avoiding zone partitioning so that the system state graphs are simpler and model checking is faster. A novel zone capping method is proposed that is time-reactive, preserves complete urgency, satisfies all deadlines, and does not need zone partitioning. The proposed verification methods were implemented in the SGM CTL model checker and applied to real-time and embedded systems. Several experiments, comparing the state space sizes produced by SGM with that by the IF toolset, show that SGM produces much smaller state-spaces.
82|10||Tactics based approach for integrating non-functional requirements in object-oriented analysis and design|Non-Functional Requirements (NFRs) are rarely treated as “first-class” elements in software development as Functional Requirements (FRs) are. Often NFRs are stated informally and incorporated in the final software as an after-thought. We leverage existing research work for the treatment of NFRs to propose an approach that enables to systematically analyze and design NFRs in parallel with FRs. Our approach premises on the importance of focusing on tactics (the specific mechanisms used to fulfill NFRs) as opposed to focusing on NFRs themselves. The advantages of our approach include filling the gap between NFRs elicitation and NFRs implementation, systematically treating NFRs through grouping of tactics so that tactics in the same group can be addressed uniformly, remedying some shortcomings in existing work (by prioritizing NFRs and analyzing tradeoff among NFRs), and integration of FRs and NFRs by treating them as first-class entities.
82|10||Design and implementation of S-MARKS: A secure middleware for pervasive computing applications|As portable devices have become a part of our everyday life, more people are unknowingly participating in a pervasive computing environment. People engage with not a single device for a specific purpose but many devices interacting with each other in the course of ordinary activity. With such prevalence of pervasive technology, the interaction between portable devices needs to be continuous and imperceptible to device users. Pervasive computing requires a small, scalable and robust network which relies heavily on the middleware to resolve communication and security issues. In this paper, we present the design and implementation of S-MARKS which incorporates device validation, resource discovery and a privacy module.
82|10||Case study on distributed and fault tolerant system modeling based on timed automata|This article presents the modeling of a distributed fault-tolerant real-time application by timed automata. The application under consideration consists of several processors communicating via a Controller Area Network (CAN); each processor executes an application that consists of fault-tolerant tasks running on top of an operating system (e.g. OSEK/VDX compliant) and using inter-task synchronization primitives. For such a system, a model checking tool (e.g. UPPAAL) can be used to verify the complex time and logical properties formalized as safety or bounded liveness properties (e.g. end-to-end response time considering an occurrence of a fault). The proposed model reduces the size of the state-space by sharing clocks measuring the execution time of the tasks.
82|10||Developing platform specific model for MPSoC architecture from UML-based embedded software models|In this paper, we describe a technique to design UML-based software models for MPSoC architecture, which focuses on the development of the platform specific model of embedded software. To develop the platform specific model, we define a process for the design of UML-based software model and suggest an algorithm with precise actions to map the model to MPSoC architecture. In order to support our design process, we implemented our approach in an integrated tool. Using the tool, we applied our design technique to a target system. We believe that our technique provides several benefits such as improving parallelism of tasks and fast-and-valid mapping of software models to hardware architecture.
82|10||Flexible coordinator design for modeling resource sharing in multi-agent systems|
82|10||A method to build information systems engineering process metamodels|Several process metamodels exist. Each of them presents a different viewpoint of the same information systems engineering process. However, there are no existing correspondences between them. We propose a method to build unified, fitted and multi-viewpoint process metamodels for information systems engineering. Our method is based on a process domain metamodel that contains the main concepts of information systems engineering process field. This process domain metamodel helps selecting the needed metamodel concepts for a particular situational context. Our method is also based on patterns to refine the process metamodel. The process metamodel can then be instantiated according to the organisation’s needs. The resulting method is represented as a pattern system.
82|10||A high stego-image quality steganographic scheme with reversibility and high payload using multiple embedding strategy|Tian’s method is a breakthrough reversible data embedding scheme with high embedding capacity measured by bits per pixel (bpp) and good visual quality measured by peak signal-to-noise ratio (PSNR). However, the embedding capacity and visual quality of this method can be significantly improved. Thus, we propose a simple reversible steganographic scheme in spatial domain for digital images by using the multiple embedding strategy. The proposed method horizontally and vertically embeds one secret bit into one cover pixel pair. The experimental results show that the proposed reversible steganographic method achieves good visual quality and high embedding capacity. Specifically, with the one-layer embedding, the proposed method can obtain the embedding capacity of more than 0.5 bpp and the PSNR value greater than 54 dB for all test images. Especially, with the five-layer embedding, the proposed method has the embedding capacity of more than 2 bpp and the PSNR value higher than 52 dB for all test images. Therefore, the proposed method surpasses many existing reversible data embedding methods in terms of visual quality and embedding capacity.
82|11|http://www.sciencedirect.com/science/journal/01641212/82/11|TAIC PART 2007 and Mutation 2007 special issue editorial|
82|11||Modelling dynamic memory management in constraint-based testing|Constraint-based testing (CBT) is the process of generating test cases against a testing objective by using constraint solving techniques. When programs contain dynamic memory allocation and loops, constraint reasoning becomes challenging as new variables and new constraints should be created during the test data generation process. In this paper, we address this problem by proposing a new constraint model of C programs based on operators that model dynamic memory management. These operators apply powerful deduction rules on abstract states of the memory enhancing the constraint reasoning process. This allows to automatically generate test data respecting complex coverage objectives. We illustrate our approach on a well-known difficult example program that contains dynamic memory allocation/deallocation, structures and loops. We describe our implementation and provide preliminary experimental results on this example that show the highly deductive potential of the approach.
82|11||Evolutionary testing of software with function-assigned flags|Evolutionary structural testing, an approach to automatically generate relevant unit test data, encounters difficulties when the software being tested contains boolean variables. This issue, known as the flag problem, has been studied by many researchers. However, previous work does not address the issue of function-assigned flags which constitutes a special type of flag problem that often occurs in the context of object-orientation. This paper elaborates on a new approach to the flag problem that can also handle function-assigned flags while being applicable to the conventional flag problem, as well. It relies on a code transformation that leads to an improved fitness landscape which provides better guidance to the evolutionary search. We present seven case studies including a fitness landscape analysis and experimental results. The results show that the suggested code transformation improves evolutionary structural testing in the presence of function-assigned flags.
82|11||A practical evaluation of spectrum-based fault localization|Spectrum-based fault localization (SFL) shortens the test–diagnose–repair cycle by reducing the debugging effort. As a light-weight automated diagnosis technique it can easily be integrated with existing testing schemes. Since SFL is based on discovering statistical coincidences between system failures and the activity of the different parts of a system, its diagnostic accuracy is inherently limited. Using a common benchmark consisting of the Siemens set and the space program, we investigate this diagnostic accuracy as a function of several parameters (such as quality and quantity of the program spectra collected during the execution of the system), some of which directly relate to test design. Our results indicate that the superior performance of a particular similarity coefficient, used to analyze the program spectra, is largely independent of test design. Furthermore, near-optimal diagnostic accuracy (exonerating over 80% of the blocks of code on average) is already obtained for low-quality error observations and limited numbers of test cases. In addition to establishing these results in the controlled environment of our benchmark set, we show that SFL can effectively be applied in the context of embedded software development in an industrial environment.
82|11||Increasing diversity: Natural language measures for software fault prediction|While challenging, the ability to predict faulty modules of a program is valuable to a software project because it can reduce the cost of software development, as well as software maintenance and evolution. Three language-processing based measures are introduced and applied to the problem of fault prediction. The first measure is based on the usage of natural language in a program’s identifiers. The second measure concerns the conciseness and consistency of identifiers. The third measure, referred to as the QALP score, makes use of techniques from information retrieval to judge software quality. The QALP score has been shown to correlate with human judgments of software quality.
82|11||Mutation testing from probabilistic and stochastic finite state machines|Specification mutation involves mutating a specification, and for each mutation a test is derived that distinguishes the behaviours of the mutated and original specifications. This approach has been applied with finite state machine based models. This paper extends mutation testing to finite state machine models that contain non-functional properties. The paper describes several ways of mutating a finite state machine with probabilities (PFSM) or stochastic time (PSFSM) attached to its transitions and shows how we can generate test sequences that distinguish between such a model and its mutants. Testing then involves applying each test sequence multiple times, observing the resultant behaviours and using results from statistical sampling theory in order to compare the observed frequency and execution time of each output sequence with that expected.
82|11||Should software testers use mutation analysis to augment a test set?|Mutation testing has historically been used to assess the fault-finding effectiveness of a test suite or other verification technique. Mutation analysis, rather, entails augmenting a test suite to detect all killable mutants. Concerns about the time efficiency of mutation analysis may prohibit its widespread, practical use. The goal of our research is to assess the effectiveness of the mutation analysis process when used by software testers to augment a test suite to obtain higher statement coverage scores. We conducted two empirical studies and have shown that mutation analysis can be used by software testers to effectively produce new test cases and to improve statement coverage scores in a feasible amount of time. Additionally, we find that our user study participants view mutation analysis as an effective but relatively expensive technique for writing new test cases. Finally, we have shown that the choice of mutation tool and operator set can play an important role in determining how efficient mutation analysis is for producing new test cases.
82|11||Reversible data hiding for high quality images using modification of prediction errors|In this paper, a reversible data hiding scheme based on modification of prediction errors (MPE) is proposed. For the existing histogram-shifting based reversible data hiding techniques, though the distortion caused by embedding is low, the embedding capacity is limited by the frequency of the most frequent pixel. To remedy this problem, the proposed method modifies the histogram of prediction errors to prepare vacant positions for data embedding. The PSNR of the stego image produced by MPE is guaranteed to be above 48 dB, while the embedding capacity is, on average, almost five times higher than that of the well-known Ni et al. techniques with the same PSNR. Besides, MPE not only has the capability to control the capacity-PSNR, where fewer data bits need less error modification, but also can be applied to images with flat histogram. Experimental results indicate that MPE, which innovatively exploits the modification of prediction errors, outperforms the prior works not only in terms of larger payload, but also in terms of stego image quality.
82|11||DOM tree browsing of a very large XML document: Design and implementation|Browsing the DOM tree of an XML document is an act of following the links among the nodes of the DOM tree to find some desired nodes without any knowledge for search. When the structure of the XML document is not known to a user, browsing is the basic operation performed for referring the contents of the XML document. If the size of the XML document is very large, however, using a general-purpose XML parser for browsing the DOM tree of the XML document to access arbitrary node may suffer from the lack of memory space for constructing the large DOM tree. To alleviate this problem, we suggest a method to browse the DOM tree of a very large XML document by splitting the XML document into n small XML documents and generating sequentially the DOM tree of each of those small n XML documents. For later reference, the information of some nodes accessed from the DOM tree already generated has been also kept using the concept of their virtual nodes. With our suggested approach, the memory space necessary for browsing the DOM tree of a very large XML document is reduced such that it can be managed by a personal computer.
82|11||A wireless sensor system for validation of real-time automatic calibration of groundwater transport models|In this paper, we present the use of a wireless sensor network in a lab for subsurface contaminant plume monitoring with the objective of automatic calibration of groundwater transport models. A tank configured to simulate an aquifer was used as a testbed, and a 2D model was created based on the setup. To simulate a contaminant plume, an ion tracer was injected into the tank. Sensor probes capable of detecting the plume were buried inside the tank, and wireless motes used to take readings from the sensors and relay data to a base station. More importantly, a run-time fault detection and diagnosis for abnormal sensor readings is designed and integrated into the data acquisition system. Further, an adaptive data collection technique is integrated that is able to provide evidence about the effectiveness of the groundwater transport model in use. Results from the tracer tests are presented, as well as lessons gained.
82|11||Identifying some important success factors in adopting agile software development practices|Agile software development (ASD) is an emerging approach in software engineering, initially advocated by a group of 17 software professionals who practice a set of “lightweight” methods, and share a common set of values of software development. In this paper, we advance the state-of-the-art of the research in this area by conducting a survey-based ex-post-facto study for identifying factors from the perspective of the ASD practitioners that will influence the success of projects that adopt ASD practices. In this paper, we describe a hypothetical success factors framework we developed to address our research question, the hypotheses we conjectured, the research methodology, the data analysis techniques we used to validate the hypotheses, and the results we obtained from data analysis. The study was conducted using an unprecedentedly large-scale survey-based methodology, consisting of respondents who practice ASD and who had experience practicing plan-driven software development in the past. The study indicates that nine of the 14 hypothesized factors have statistically significant relationship with “Success”. The important success factors that were found are: customer satisfaction, customer collaboration, customer commitment, decision time, corporate culture, control, personal characteristics, societal culture, and training and learning.
82|11||Discovery of architectural layers and measurement of layering violations in source code|The layers architectural pattern has been widely adopted by the developer community in order to build large software systems. In reality, as the system evolves over time, rarely does the system remain conformed to the intended layers pattern, causing a significant degradation of the system maintainability. As a part of re-factoring such a system, practitioners often undertake a mostly manual exercise to discover the intended layers and organize the modules into these layers. In this paper, we present a method for semi-automatically detecting layers in the system and propose a quantitative measurement to compute the amount of non-conformance of the system from the set of layered design principles. We have applied the layer detection method and the non-conformance measurement on a set of open source and proprietary enterprise applications.
82|11||A robust DWT-based copyright verification scheme with Fuzzy ART|Protecting the intellectual property rights (IPR) of digital media is important because the illegal reproduction and modification of digital media has become increasingly serious. A robust DWT-based copyright verification scheme with Fuzzy ART that does not require the original image for ownership verification is proposed in this paper. The proposed scheme, which combines DWT, Fuzzy ART, and the quantization process, converts an image into a short robust table with the embedded ownership information. Unlike general classification, such as k-mean and fuzzy c-means, the number of clusters can be adaptively decided by the vigilance parameter of Fuzzy ART. Experimental results demonstrate that the proposed scheme is robust against common image processing, geometric distortions, and intentional attacks. The original image is not required to extract the embedded ownership image.
82|11||Practical design of a proxy agent to facilitate adaptive video streaming service across wired/wireless networks|Thanks to the growing of the wireless networks, the video streaming application becomes a ubiquitous joyful service. In a wireless communication network environment, the service traffic spans across the wired and wireless domains. In this article, we propose a practical design of a proxy agent – SPONGE (Stream Pooler Over a Network Graded Environment) sitting between the wireless User Equipments (UEs) and the video streaming server to facilitate the adaptive video streaming service across wired/wireless networks. To make the wireless streaming service more efficient, an input video session would be encoded as multiple qualities of video streams so that UEs with a similar receiving condition can share streams with the same service quality via SPONGE. SPONGE can alleviate the direct load on the original stream broadcasting server. Meanwhile, it can make each UE get an adaptive streaming service according to the network conditions of the UE by a reduced network condition feedback latency. Our theoretical analysis and simulation results show that SPONGE can help wireless streaming users get a smooth and better playback quality by a quick and accurate reaction to the network condition.
82|12|http://www.sciencedirect.com/science/journal/01641212/82/12|Full mobile agent interoperability in an IEEE-FIPA context|The existence of heterogeneous mobile agent systems hinders the interoperability of mobile agents. Several solutions exist, but they are limited in some aspects. This article proposes a full interoperability solution, in the context of the IEEE-FIPA agent standards, composed of three parts. The first part is a simple language-independent agent interface that enables agents to visit locations with different types of middlewares. The second part is a set of design models for the middlewares to support agents developed for different programming languages and architectures. And the third part is a method based on agents with multiple codes and a common agent data encoding mechanism to enable interoperability between middlewares that do not support the same programming languages. Furthermore two agent interoperability implementations, and its corresponding performance comparison, carried out over the JADE and AgentScape agent middlewares are presented.
82|12||New enhancements to the SOCKS communication network security protocol: Schemes and performance evaluation|In this paper we propose two new enhancements to the SOCKS protocol in the areas of IP multicasting and UDP tunneling. Most network firewalls deployed at the entrance to a private network block multicast traffic. This is because of potential security threats inherent with IP multicast. Multicasting is the backbone of many Internet technologies like voice and video conferencing, real time gaming, multimedia streaming, and online stock quotes, among others. There is a need to be able to safely and securely allow multicast streams to enter into and leave a protected enterprise network. Securing multicast streams is challenging. It poses many architectural issues. The SOCKS protocol is typically implemented in a network firewall as an application-layer gateway. Our first enhancement in the area of IP multicast to the SOCKS protocol is to enable the application of security and access control policies and safely allow multicast traffic to enter into the boundaries of a protected enterprise network. The second enhancement we propose is to allow the establishment of a tunnel between two protected networks that have SOCKS based firewalls to transport UDP datagrams.
82|12||Methodology evaluation framework for dynamic evolution in composition-based distributed applications|Dynamic evolution can be used to upgrade distributed applications without shutdown and restart as a way of improving service levels while minimising the loss of business revenue caused by the downtime. An evaluation framework assessing the level of support offered by existing methodologies in composition-based application (e.g. component-based and service-oriented) development is proposed. It was developed by an analysis of the literature and existing methodologies together with a refinement based on a survey of experienced practitioners and researchers. The use of the framework is demonstrated by applying it to twelve methodologies to assess their support for dynamic evolution.
82|12||A high capacity reversible data hiding scheme with edge prediction and difference expansion|To enhance the embedding capacity of a reversible data hiding system, in this paper, a novel multiple-base lossless scheme based on JPEG-LS pixel value prediction and reversible difference expansion will be presented. The proposed scheme employs a pixel value prediction mechanism to decrease the distortion caused by the hiding of the secret data. In general, the prediction error value tends to be much smaller in smooth areas than in edge areas, and more secret data embedded in smooth areas still meets better stego-image quality. The multiple-base notational system, on the other hand, is applied to increase the payload of the image. With the system, the payload of each pixel, determined by the complexity of its neighboring pixels, can be very different. In addition, the cover image processed by the proposed scheme can be fully recovered without any distortion. Experimental results, as shown in this paper, have demonstrated that the proposed method is capable of hiding more secret data while keeping the stego-image quality degradation imperceptible.
82|12||Constructing attribute weights from computer audit data for effective intrusion detection|Attributes construction and selection from audit data is the first and very important step for anomaly intrusion detection. In this paper, we present several cross frequency attribute weights to model user and program behaviors for anomaly intrusion detection. The frequency attribute weights include plain term frequency (TF) and various forms of term frequency-inverse document frequency (tfidf), referred to as Ltfidf, Mtfidf and LOGtfidf. Nearest Neighbor (NN) and k-NN methods with Euclidean and Cosine distance measures as well as principal component analysis (PCA) and Chi-square test method based on these frequency attribute weights are used for anomaly detection. Extensive experiments are performed based on command data from Schonlau et al. The testing results show that the LOGtfidf weight gives better detection performance compared with plain frequency and other types of weights. By using the LOGtfidf weight, the simple NN method and PCA method achieve the better masquerade detection results than the other 7 methods in the literature while the Chi-square test consistently returns the worst results. The PCA method is suitable for fast intrusion detection because of its capability of reducing data dimensionality while NN and k-NN methods are suitable for detection of a small data set because of its no need of training process. A HTTP log data set collected in a real environment and the sendmail system call data from University of New Mexico (UNM) are used as well and the results also demonstrate the effectiveness of the LOGtfidf weight for anomaly intrusion detection.
82|12||A systematic review of domain analysis solutions for product lines|Domain analysis is crucial and central to software product line engineering (SPLE) as it is one of the main instruments to decide what to include in a product and how it should fit in to the overall software product line. For this reason many domain analysis solutions have been proposed both by researchers and industry practitioners. Domain analysis comprises various modeling and scoping activities. This paper presents a systematic review of all the domain analysis solutions presented until 2007. The goal of the review is to analyze the level of industrial application and/or empirical validation of the proposed solutions with the purpose of mapping maturity in terms of industrial application, as well as to what extent proposed solutions might have been evaluated in terms of usability and usefulness. The finding of this review indicates that, although many new domain analysis solutions for software product lines have been proposed over the years, the absence of qualitative and quantitative results from empirical application and/or validation makes it hard to evaluate the potential of proposed solutions with respect to their usability and/or usefulness for industry adoption. The detailed results of the systematic review can be used by individual researchers to see large gaps in research that give opportunities for future work, and from a general research perspective lessons can be learned from the absence of validation as well as from good examples presented. From an industry practitioner view, the results can be used to gauge as to what extent solutions have been applied and/or validated and in what manner, both valuable as input prior to industry adoption of a domain analysis solution.
82|12||MHS: A distributed metadata management strategy|This paper proposes a novel distributed metadata management strategy to efficiently handle different metadata workloads. It can deliver high performance and scalable metadata service through four techniques, including directory conversion metadata, mimic hierarchical directory structure, flexible partition methods targeted different kinds of metadata of diverse characteristics, and the application of database to metadata backend. Using micro-benchmarks and a prototype system, we firstly demonstrate the performance superiority of our strategy compared to Lazy Hybrid, and then present the detailed performance results and analysis of our strategy on different MDS scales.
82|12||An energy-efficient mobile transaction processing method using random back-off in wireless broadcast environments|Broadcast is widely accepted as an efficient technique for disseminating data to a large number of mobile clients over a single or multiple channels. Due to the limited uplink bandwidth from mobile clients to server, conventional concurrency control methods cannot be directly applied. There has been many researches on concurrency control methods for wireless broadcast environments. However, they are mostly for read-only transactions or do not consider exploiting cache. They also suffer from the repetitive aborts and restarts of mobile transactions when the access patterns of mobile transactions are skewed. In this paper, we propose a new optimistic concurrency control method suitable for mobile broadcast environments. To prevent the repetitive aborts and restarts of mobile transactions, we propose a random back-off technique. To exploit the cache on mobile clients, our method keeps the read data set of mobile transactions and prefetches those data items when the mobile transactions are restarted. As other existing optimistic concurrency control methods for mobile broadcast environments does, it works for both read-only and update transactions. Read-only transactions are validated and locally committed without using any uplink bandwidth. Update transactions are validated with forward and backward validation, and committed after final validation consuming a small amount of uplink bandwidth. Our performance analysis shows that it significantly decreases uplink and downlink bandwidth usage compared to other existing methods.
82|12||Integrating knowledge flow mining and collaborative filtering to support document recommendation|Knowledge is a critical resource that organizations use to gain and maintain competitive advantages. In the constantly changing business environment, organizations must exploit effective and efficient methods of preserving, sharing and reusing knowledge in order to help knowledge workers find task-relevant information. Hence, an important issue is how to discover and model the knowledge flow (KF) of workers from their historical work records. The objectives of a knowledge flow model are to understand knowledge workers’ task-needs and the ways they reference documents, and then provide adaptive knowledge support. This work proposes hybrid recommendation methods based on the knowledge flow model, which integrates KF mining, sequential rule mining and collaborative filtering techniques to recommend codified knowledge. These KF-based recommendation methods involve two phases: a KF mining phase and a KF-based recommendation phase. The KF mining phase identifies each worker’s knowledge flow by analyzing his/her knowledge referencing behavior (information needs), while the KF-based recommendation phase utilizes the proposed hybrid methods to proactively provide relevant codified knowledge for the worker. Therefore, the proposed methods use workers’ preferences for codified knowledge as well as their knowledge referencing behavior to predict their topics of interest and recommend task-related knowledge. Using data collected from a research institute laboratory, experiments are conducted to evaluate the performance of the proposed hybrid methods and compare them with the traditional CF method. The results of experiments demonstrate that utilizing the document preferences and knowledge referencing behavior of workers can effectively improve the quality of recommendations and facilitate efficient knowledge sharing.
82|12||UWIS: An assessment methodology for usability of web-based information systems|A methodology for usability assessment and design of web-based information systems (UWIS) is proposed. It combines web-based service quality and usability dimensions of information systems. Checklist items with the highest and the lowest contribution to the usability performance of a web-based information system can be specified by UWIS. A case study by a student information system at Fatih University is carried out to validate the methodology. UWIS reveals a strong relationship between quality and usability which is assumed to exist by many researchers but not experimentally analyzed yet. This study depicts a strong relevance between web-based service quality and usability of web-based information systems. UWIS methodology can be used for designing more usable and higher quality web-based information systems.
82|12||A holistic approach to managing software change impact|Change is inevitable in the software product lifecycle. When a software change occurs, all of the stakeholders and related artifacts should be considered in determining the success of the change action in a collaborative development environment such as JAD (joint application development). In this regard, current implementation-based or homogeneous impact analyses are insufficient; therefore, this paper presents a holistic approach to change impact analysis in handling not only software contents but also other items such as requirements, documents and data. This approach characterizes product contents and relates heterogeneous items by using attributes and linkages. It also uses an object-oriented propagation mechanism to handle dynamic looping in determining the impact of changes. A prototype, EPIC, was built to realize this approach and these concepts. A walkthrough example is provided in order to verify the work of the proposed approach. An empirical study is presented to discuss the benefits of the proposed approach and the application of EPIC in a software company. Lessons learned from the case study and improvement issues of the proposed approach and the tool are also discussed.
82|12||Tool support for the rapid composition, analysis and implementation of reactive services|We present the integrated set of tools Arctis for the rapid development of reactive services. In our method, services are composed of collaborative building blocks that encapsulate behavioral patterns expressed as UML 2.0 collaborations and activities. Due to our underlying semantics in temporal logic, building blocks as well as their compositions can be transformed into formulas and model checked incrementally in order to guarantee that important system properties are kept. The process of model checking is fully automated. Error traces are presented to the users as easily understandable animations, so that no expertise in temporal logic is needed. In addition, the results of model checking are analyzed, so that in some cases automated diagnoses and fixes can be provided as well. The formal semantics also enables the correct, automatic synthesis of the activities to state machines which form the input of our code generators. Thus, the collaborative models can be fully automatically transformed into executable Java code. We present the development of a mobile treasure hunt system to exemplify the method and the tools.
82|12||Approach to designing bribery-free and coercion-free electronic voting scheme|
82|12||A real options approach for evaluation and justification of a hospital information system|Nowadays healthcare organizations globally recognize the importance of investing in information technologies to improve the quality of care delivery and reduce costs. The key drivers of healthcare sector such as continuously improving healthcare standards and insurance systems have introduced new requirements for hospitals, which in return provided a solid ground for decision-makers to consider implementing hospital information systems that are customized and improved versions of enterprise resource planning (ERP) systems designed according to the needs of the healthcare sector. The conventional discounted cash flow methods ignore the value of managerial and strategic flexibility inherent in these investments, which is crucial for justification of the investment decision. This study introduces a real options-based methodology which overcomes the limitations of traditional valuation methods and enables decision-makers to value an ERP system investment incorporating multiple options. The option valuation model developed in this study extends the binomial lattice framework to model a hospital information system (HIS) investment opportunity with compound options. The potential application of the proposed model is illustrated through evaluation of a real-world HIS investment.
82|12||Communication cost effective scheduling policies of nonclairvoyant jobs with load balancing in a grid|
82|12||Platform-independent modeling and prediction of application resource usage characteristics|Application resource usage models can be used in the decision making process for ensuring quality-of-service as well as for capacity planning, apart from their general use in performance modeling, optimization, and systems management. Current solutions for modeling application resource usage tend to address parts of the problem by either focusing on a specific application, or a specific platform, or on a small subset of system resources. We propose a simple and flexible approach for modeling application resource usage in a platform-independent manner that enables the prediction of application resource usage on unseen platforms. The technique proposed is application agnostic, requiring no modification to the application (binary or source) and no knowledge of application-semantics. We implement a Linux-based prototype and evaluate it using four different workloads including real-world applications and benchmarks. Our experiments reveal prediction errors that are bound within 6–24% of the observed for these workloads when using the proposed approach.
82|2|http://www.sciencedirect.com/science/journal/01641212/82/2|Expressing and organizing real-time specification patterns via temporal logics|Formal specification models provide support for the formal verification and validation of the system behaviour. This advantage is typically paid in terms of effort and time spent in learning and using formal methods and tools. The introduction and usage of patterns have a double impact. They stand for examples on how to cover classical problems with formal methods in many different notations, so that the user can shorten the time to understand if a formal method can be used to meet his purpose and how it can be used. Furthermore, they are used for shortening the specification time, by reusing and composing different patterns to cover the specification, thus producing more understandable specifications which refer to commonly known patterns. For these reasons, both interests in and usage of patterns are growing and a higher number of proposals for patterns and pattern classification/organization has appeared in the literature. This paper reports a review of the state of the art for real-time specification patterns, so as to organize them in a unified way, while providing some new patterns which complete the unified model. The proposed organization is based on some relationships among patterns as demonstrated in the paper. During the presentation the patterns have been formalized in TILCO-X, whereas in appendix a list of patterns with formalizations in several different logics such as TILCO, LTL, CTL, GIL, QRE, MTL, TCTL and RTGIL, is provided disguised as links to the locations where such formalizations can be recovered and/or are directly reported, if found not accessible in the literature; this allows the reader to have a detailed view of all the classified patterns, including the ones already added. Furthermore, an example has been proposed to highlight the usefulness of the new identified patterns completing the unified model.
82|2||Protecting mobile agents from external replay attacks|This paper presents a protocol for the protection of mobile agents against external replay attacks. This kind of attacks are performed by malicious platforms when dispatching an agent multiple times to a remote host, thus making it reexecute part of its itinerary. Current proposals aiming to address this problem are based on storing agent identifiers, or trip markers, inside agent platforms, so that future reexecutions can be detected and prevented. The problem of these solutions is that they do not allow the agent to perform legal migrations to the same platform several times. The aim of this paper is to address these issues by presenting a novel solution based on authorisation entities, which allow the agent to be reexecuted on the same platform a number of times determined at runtime. The proposed protocol is secure under the assumption that authorisation entities are trusted.
82|2||Frameworks for designing and implementing dependable systems using Coordinated Atomic Actions: A comparative study|This paper1 presents ways of implementing dependable distributed applications designed using the Coordinated Atomic Action (CAA) paradigm. CAAs provide a coherent set of concepts adapted to fault tolerant distributed system design that includes structured transactions, distribution, cooperation, competition, and forward and backward error recovery mechanisms triggered by exceptions. DRIP (Dependable Remote Interacting Processes) is an efficient Java implementation framework which provides support for implementing Dependable Multiparty Interactions (DMI). As DMIs have a softer exception handling semantics compared with the CAA semantics, a CAA design can be implemented using the DRIP framework. A new framework called CAA-DRIP allows programmers to exclusively implement the semantics of CAAs using the same terminology and concepts at the design and implementation levels. The new framework not only simplifies the implementation phase, but also reduces the final system size as it requires less number of instances for creating a CAA at runtime. The paper analyses both implementation frameworks in great detail, drawing a systematic comparison of the two. The CAAs behaviour is described in terms of Statecharts to better understand the differences between the two frameworks. Based on the results of the comparison, we use one of the frameworks to implement a case study belonging to the e-health domain.
82|2||Development of a team measure for tacit knowledge in software development teams|In this paper we operationally define and measure tacit knowledge at the team-level in the software development domain. Through a series of three empirical studies we developed and validated the team tacit knowledge measure (TTKM) for software developers. In the first study, initial scale items were developed using the repertory grid technique and content analysis. In Study 2, supplied repertory grids were administered to novices and experts to establish differential items, and Study 3 validated the TTKM on a sample of 48 industrial software development teams. In developing the TTKM we explored the relationships between tacit knowledge, explicit job knowledge and social interaction and their effect on team performance as measured by efficiency and effectiveness. In addition we assess the implications for managing software development teams and increasing team performance through social interaction.
82|2||A study of project selection and feature weighting for analogy based software cost estimation|A number of software cost estimation methods have been presented in literature over the past decades. Analogy based estimation (ABE), which is essentially a case based reasoning (CBR) approach, is one of the most popular techniques. In order to improve the performance of ABE, many previous studies proposed effective approaches to optimize the weights of the project features (feature weighting) in its similarity function. However, ABE is still criticized for the low prediction accuracy, the large memory requirement, and the expensive computation cost. To alleviate these drawbacks, in this paper we propose the project selection technique for ABE (PSABE) which reduces the whole project base into a small subset that consist only of representative projects. Moreover, PSABE is combined with the feature weighting to form FWPSABE for a further improvement of ABE. The proposed methods are validated on four datasets (two real-world sets and two artificial sets) and compared with conventional ABE, feature weighted ABE (FWABE), and machine learning methods. The promising results indicate that project selection technique could significantly improve analogy based models for software cost estimation.
82|2||A family of experiments to evaluate a functional size measurement procedure for Web applications|The objective of this paper is to empirically evaluate OOmFPWeb, a functional size measurement procedure for Web applications. We analyzed four data sets from a family of experiments conducted in Spain, Argentina and Austria. Results showed that OOmFPWeb is efficient when compared to current industry practices. OOmFPWeb produced reproducible functional size measurements and was perceived as easy to use and useful by the study participants, who also expressed their intention to use OOmFPWeb in the future. The analysis further supports the validity and reliability of the technology acceptance model (TAM)-based evaluation instrument used in the study.
82|2||A novel identity-based strong designated verifier signature scheme|Unlike ordinary digital signatures, a designated verifier signature scheme makes it possible for a signer to convince a designated verifier that she has signed a message in such a way that the designated verifier cannot transfer the signature to a third party. In a strong designated verifier signature scheme, no third party can even verify the validity of a designated verifier signature, since the designated verifier’s private key is required in the verifying phase. Firstly, this paper proposes the model of identity-based strong designated verifier signature scheme based on bilinear pairings by combining identity-based cryptosystem with the designated verifier signature scheme, and then, provides one concrete strong identity-based designated verifier signature scheme, which has short size of signature, low communication and computational cost. We provide security proofs for our scheme.
82|2||Adaptive disk scheduling with workload-dependent anticipation intervals|Anticipatory scheduling (AS) of I/O requests has become a viable choice for block-device schedulers in open-source OS-kernels as prior work has established its superiority over traditional disk-scheduling policies. An AS-scheduler selectively stalls the block-device right after servicing a request in hope that a new request for a nearby sector will be soon posted. Clearly, this decision may introduce delays if the anticipated I/O does not arrive on time. In this paper, we build on the success of the AS and propose an approach that minimizes the overhead of unsuccessful anticipations. Our suggested approach termed workload-dependent anticipation scheduling (WAS), determines the length of every anticipation period in an on-line fashion in order to reduce penalties by taking into account the evolving spatio-temporal characteristics of running processes as well as properties of the underlying computing system. We harvest the spatio-temporal features of individual processes and employ a system-wide process classification scheme that is re-calibrated on the fly. The resulting classification enables the disk scheduler to make informed decisions and vary the anticipation interval accordingly, on a per-process basis. We have implemented and incorporated WAS into the current Linux kernel. Through experimentation with a wide range of diverse workloads, we demonstrate WAS benefits and establish reduction of penalties over other AS-scheduler implementations.
82|2||Specifying behavioral semantics of UML diagrams through graph transformations|The Unified Modeling Language (UML) has been widely accepted as a standard for modeling software systems from various perspectives. The intuitive notations of UML diagrams greatly improve the communication among developers. However, the lack of a formal semantics makes it difficult to automate analysis and verification. This paper offers a graphical yet formal approach to specifying the behavioral semantics of statechart diagrams using graph transformation techniques. It supports many advanced features of statecharts, such as composite states, firing priority, history, junction, and choice. In our approach, a graph grammar is derived automatically from a state machine to summarize the hierarchy of states. Based on the graph grammar, the execution of a set of non-conflict state transitions is interpreted by a sequence of graph transformations. This facilitates verifying a design model against system requirements. To demonstrate our approach, we present a case study on a toll-gate system.
82|2||Comparative evaluation of contiguous allocation strategies on 3D mesh multicomputers|The performance of contiguous allocation strategies can be significantly affected by the type of the distribution adopted for job execution times. In this paper, the performance of the existing contiguous allocation strategies for 3D mesh multicomputers is re-visited in the context of heavy-tailed distributions (e.g., a Bounded Pareto distribution). The strategies are evaluated and compared using simulation experiments for both First-Come-First-Served (FCFS) and Shortest-Service-Demand (SSD) scheduling strategies under a variety of system loads and system sizes. The results show that the performance of the allocation strategies degrades considerably when job execution times follow a heavy-tailed distribution. Moreover, SSD copes much better than FCFS scheduling strategy in the presence of heavy-tailed job execution times. The results also reveal that allocation strategies that employ a list of allocated sub-meshes for both allocation and de-allocation exhibit low allocation overhead, and maintain good system performance in terms of average turnaround time and mean system utilization.
82|2||Quality attribute tradeoff through adaptive architectures at runtime|Quality attributes, e.g. performance and reliability, become more and more important for the development of software systems. One of the critical issues on quality assurance is how to make good enough tradeoffs between quality attributes that interfere with each other. Some architecture-based quality design and analysis methods are proposed to make tradeoffs at design time. However, many quality attributes depend on runtime contexts; it may be difficult and even impossible to make tradeoffs between them at design time. In this paper, we use an adaptive architecture model to capture candidate strategies for different quality attributes; the tradeoff, i.e. which strategies are more appropriate and thus applied, is postponed to runtime. The contribution of our approach is threefold. First, it makes use of existing architecture-based quality design and analysis methods to identify why and where quality attribute tradeoffs are necessary. Second, a traditional architecture description language is extended to support the modeling of an adaptive architecture, which records strategies for different quality attributes under different conditions. Third, a reflective middleware is used to monitor the runtime system, collect required information to determine appropriate strategies, and adapt the application’s architecture to achieve expected quality attributes. This approach is demonstrated on J2EE.
82|2||Exception handling refactorings: Directed by goals and driven by bug fixing|Exception handling design can improve robustness, which is an important quality attribute of software. However, exception handling design remains one of the less understood and considered parts in software development. In addition, like most software design problems, even if developers are requested to design with exception handling beforehand, it is very difficult to get the right design at the first shot. Therefore, improving exception handling design after software is constructed is necessary. This paper applies refactoring to incrementally improve exception handling design. We first establish four exception handling goals to stage the refactoring actions. Next, we introduce exception handling smells that hinder the achievement of the goals and propose exception handling refactorings to eliminate the smells. We suggest exception handling refactoring is best driven by bug fixing because it provides measurable quality improvement results that explicitly reveal the benefits of refactoring. We conduct a case study with the proposed refactorings on a real world banking application and provide a cost-effectiveness analysis. The result shows that our approach can effectively improve exception handling design, enhance software robustness, and save maintenance cost. Our approach simplifies the process of applying big exception handling refactoring by dividing the process into clearly defined intermediate milestones that are easily exercised and verified. The approach can be applied in general software development and in legacy system maintenance.
82|2||Differential fault analysis on the contracting UFN structure, with application to SMS4 and MacGuffin|The contracting unbalanced Feistel networks (UFN) is a particular structure in the block ciphers, where the “left half” and the “right half” are not of equal size, and the size of the domain of one half is larger than that of the range. This paper studies the security of the contracting UFN structure against differential fault analysis (DFA). We propose two basic byte-oriented fault models and two corresponding attacking methods. Then we implement the attack on two instances of the contracting UFN structure, the block ciphers SMS4 and MacGuffin. The experiments require 20 and 4 faulty ciphertexts to recover the 128-bit secret key of SMS4 in the two fault models, respectively. Under similar hypothesis, MacGuffin is breakable with 355 and 165 faulty ciphertexts, respectively. So our work not only builds up a general model of DFA on the contracting UFN structure and ciphers, but also provides a new reference for fault analysis on other block ciphers.
82|2||A mobile agent platform for distributed network and systems management|The mobile agent (MA) technology has been proposed for the management of networks and distributed systems as an answer to the scalability problems of the centralized paradigm. Management tasks may be assigned to an agent, which delegates and executes management logic in a distributed and autonomous fashion. MA-based management has been a subject of intense research in the past few years, reflected on the proliferation of MA platforms (MAPs) expressly oriented to distributed management. However, most of these platforms impose considerable burden on network and system resources and also lack of essential functionality, such as security mechanisms, fault tolerance, strategies for building network-aware MA itineraries and support for user-friendly customization of MA-based management tasks. In this paper, we discuss the design considerations and implementation details of a complete MAP research prototype that sufficiently addresses all the aforementioned issues. Our MAP has been implemented in Java and optimized for network and systems management applications. The paper also presents the evaluation results of our prototype in real and simulated networking environments.
82|3|http://www.sciencedirect.com/science/journal/01641212/82/3|An update to experimental models for validating computer technology|In 1998 a survey was published on the extent to which software engineering papers validate the claims made in those papers. The survey looked at publications in 1985, 1990 and 1995. This current paper updates that survey with data from 2000 to 2005. The basic conclusion is that the situation is improving. One earlier complaint that access to data repositories was difficult is becoming less prevalent and the percentage of papers including validation is increasing.
82|3||A new algorithm with segment protection and load balancing for single-link failure in multicasting survivable networks|In this paper, we investigate the problem of failure tolerated multicast requests in survivable networks and propose a new heuristic algorithm called segment protection with load balancing (SPLB) to address the single-link failure. In order to obtain better performances, in SPLB first we consider the techniques of cross-sharing and self-sharing to improve the resource utilization ratio, second we propose a segment protection routing algorithm to overcome the trap problem, and third we design a load balancing method to reduce the blocking probability. Compared with conventional algorithm, SPLB performs better performances. Simulation results meet our expectation.
82|3||Component-based software version management based on a Component-Interface Dependency Matrix|As much component-based software is developed, a software configuration management (SCM) tool for component-based software is necessary. In this paper, we propose a version management mechanism for impact analysis while components are upgraded. We separately version the components and interfaces based on a Component-Interface Dependency Matrix (CIDM), and analyze impacts according to their dependency relationship. The result of our simulation shows that CIDM is capable of managing large numbers of components without impedance mismatch. In addition, in a well-designed software system, using CIDM to analyze impacts can save resources in the software development phase.
82|3||Classification and evaluation of timed running schemas for workflow based on process mining|The system running logs of a workflow contain much information about the behavior and logical structure between activities. In this paper, a mining approach is proposed to discover the structural and temporal model for a workflow from its timed running logs. The mining results are represented in the formalized form of Petri nets extended with two timing factors that allows validation or verification the actual behaviors, especially the temporal constraints between activities. According to the reachability graph of the extended Petri net model mined, all running schemas of a workflow can be generated, which defines the temporal constraints between running activities. By calculating the earliest and latest start time of each activity, the earliest starting and latest existing time of each state in the running schema can be determined. Based on the temporal relations between the timing factors of each running state, the running schemas can be classified into six classes. The effects of the six classes of running schemas on the implementation of the whole workflow are evaluated so as to obtain the best one that can ensure the workflow is finished in the shortest time. The standards for the ideal, reliable and favorable running schemas and their existence conditions are discussed, which can be used to evaluate the running logs and control the future running of a workflow.
82|3||A HDWT-based reversible data hiding method|This paper presents a reversible data hiding method which provides a high payload and a high stego-image quality. The proposed method transforms a spatial domain cover image into a frequency domain image using the Haar digital wavelet transform (HDWT) method, compresses the coefficients of the high frequency band by the Huffman (or arithmetic) coding method, and then embeds the compression data and the secret data in the high frequency band. Since the high frequency band incorporates less energy than other bands of an image, it can be exploited to carry secret data. Furthermore, the proposed method utilizes the Huffman (or arithmetic) coding method to recover the cover image without any distortion. The proposed method is simple and the experimental results show that the designed method can give a high hiding capacity with a high quality of stego-image.
82|3||PAT: A pattern classification approach to automatic reference oracles for the testing of mesh simplification programs|Graphics applications often need to manipulate numerous graphical objects stored as polygonal models. Mesh simplification is an approach to vary the levels of visual details as appropriate, thereby improving on the overall performance of the applications. Different mesh simplification algorithms may cater for different needs, producing diversified types of simplified polygonal model as a result. Testing mesh simplification implementations is essential to assure the quality of the graphics applications. However, it is very difficult to determine the oracles (or expected outcomes) of mesh simplification for the verification of test results.
82|3||Managing requirements specifications for product lines â An approach and industry case study|Software product line development has emerged as a leading approach for software reuse. This paper describes an approach to manage natural-language requirements specifications in a software product line context. Variability in such product line specifications is modeled and managed using a feature model. The proposed approach has been introduced in the Swedish defense industry. We present a multiple-case study covering two different product lines with in total eight product instances. These were compared to experiences from previous projects in the organization employing clone-and-own reuse. We conclude that the proposed product line approach performs better than clone-and-own reuse of requirements specifications in this particular industrial context.
82|3||Reengineering for service oriented architectures: A strategic decision model for integration versus migration|Service Oriented Architecture (SOA) is a popular paradigm at present because it provides a standards-based conceptual framework for flexible and adaptable enterprise wide systems. This implies that most present systems need to be reengineered to become SOA compliant. However, SOA reengineering projects raise serious strategic as well as technical questions that require management oversight. This paper, based on practical experience with SOA projects, presents a decision model for SOA reengineering projects that combines strategic and technical factors with cost-benefit analysis for integration versus migration decisions. The paper identifies the key issues that need to be addressed in enterprise application reengineering projects for SOA, examines the strategic alternatives, explains how the alternatives can be evaluated based on architectural and cost-benefit considerations and illustrates the main ideas through a detailed case study.
82|3||An empirical approach to evaluating dependency locality in hierarchically structured software systems|In software development, especially component-based software development, dependency locality states that relevant software components should be at shorter distances than irrelevant components. This principle is used together with modularity and hierarchy to guide the design of large-scale complex software systems. In previous work, dependency locality and its correlation with design quality were studied by statically measuring the interactions between software components. This paper presents an empirical approach to evaluating the hierarchical structure of software systems through mining their revision history. Two metrics, spatial distance and temporal distance, are adapted to measure the dependencies between software components. The correlation of spatial distance and temporal distance between software components represents a factor that influences system design quality. More specially, a well designed system hierarchy should have a significant positive correlation while a non-significant positive correlation or a negative correlation would signify design flaws. In an application of this approach, we use Mantel test to study the dependency locality of six software systems from Apache projects.
82|3||A real-time network simulation infrastructure based on OpenVPN|We present an open and flexible software infrastructure that embeds physical hosts in a simulated network. In real-time network simulation, where real-world implementations of distributed applications and network services can run together with the network simulator that operates in real-time, real network packets are injected into the simulation system and subject to the simulated network conditions computed as a result of both real and virtual traffic traversing the network and competing for network resources. Our real-time simulation infrastructure has been implemented based on Open Virtual Private Network (OpenVPN), modified and customized to bridges traffic between the physical hosts and the simulated network. We identify the performance advantages and limitations of our approach via a set of experiments. We also present two interesting application scenarios to show the capabilities of the real-time simulation infrastructure.
82|3||Autonomic QoS control in enterprise Grid environments using online simulation|As Grid Computing increasingly enters the commercial domain, performance and quality of service (QoS) issues are becoming a major concern. The inherent complexity, heterogeneity and dynamics of Grid computing environments pose some challenges in managing their capacity to ensure that QoS requirements are continuously met. In this paper, a comprehensive framework for autonomic QoS control in enterprise Grid environments using online simulation is proposed. This paper presents a novel methodology for designing autonomic QoS-aware resource managers that have the capability to predict the performance of the Grid components they manage and allocate resources in such a way that service level agreements are honored. Support for advanced features such as autonomic workload characterization on-the-fly, dynamic deployment of Grid servers on demand, as well as dynamic system reconfiguration after a server failure is provided. The goal is to make the Grid middleware self-configurable and adaptable to changes in the system environment and workload. The approach is subjected to an extensive experimental evaluation in the context of a real-world Grid environment and its effectiveness, practicality and performance are demonstrated.
82|3||An efficient XML encoding and labeling method for query processing and updating on dynamic XML data|In this paper, we propose an efficient encoding and labeling scheme for XML, called EXEL, which is a variant of the region labeling scheme using ordinal and insert-friendly bit strings. We devise a binary encoding method to generate the ordinal bit strings, and an algorithm to make a new bit string inserted between bit strings without any influences on the order of preexisting bit strings. These binary encoding method and bit string insertion algorithm are the bases of the efficient query processing and the complete avoidance of re-labeling for updates. We present query processing and update processing methods based on EXEL. In addition, the Stack-Tree-Desc algorithm is used for an efficient structural join, and the String B-tree indexing is utilized to improve the join performance. Finally, the experimental results show that EXEL enables complete avoidance of re-labeling for updates while providing fairly reasonable query processing performance.
82|3||Search-order coding method with indicator-elimination property|Vector quantization (VQ) is a widely used technique for many applications especially for lossy image compression. Since VQ significantly reduces the size of a digital image, it can save the costs of storage space and image delivery. Search-order coding (SOC) was proposed for improving the performance of VQ in terms of compression rate. However, SOC requires extra data (i.e. indicators) to indicate source of codewords so the compression rate may be affected. To overcome such a drawback, in this paper, a search-order coding with the indicator-elimination property was proposed by using a technique of reversible data hiding. The proposed method is the first one using such a concept of data hiding to achieve a better compression rate of SOC. From experimental results, the performance of the SOC method can be successfully improved by the proposed indicator eliminated search-order coding method in terms of compression rate. In addition, compared with other relevant schemes, the proposed method is also more flexible than some existing schemes.
82|3||A lossy 3D wavelet transform for high-quality compression of medical video|In this paper, we present a lossy compression scheme based on the application of the 3D fast wavelet transform to code medical video. This type of video has special features, such as its representation in gray scale, its very few interframe variations, and the quality requirements of the reconstructed images. These characteristics as well as the social impact of the desired applications demand a design and implementation of coding schemes especially oriented to exploit them. We analyze different parameters of the codification process, such as the utilization of different wavelets functions, the number of steps the wavelet function is applied to, the way the thresholds are chosen, and the selected methods in the quantization and entropy encoder. In order to enhance our original encoder, we propose several improvements in the entropy encoder: 3D-conscious run-length, hexadecimal coding and the application of arithmetic coding instead of Huffman. Our coder achieves a good trade-off between compression ratio and quality of the reconstructed video. We have also compared our scheme with MPEG-2 and EZW, obtaining better compression ratios up to 119% and 46%, respectively for the same PSNR.
82|3||Gompertz software reliability model: Estimation algorithm and empirical validation|Gompertz curve has been used to estimate the number of residual faults in testing phases of software development, especially by Japanese software development companies. Since the Gompertz curve is a deterministic function, the curve cannot be applied to estimating software reliability which is the probability that software system does not fail in a prefixed time period. In this article, we propose a stochastic model called the Gompertz software reliability model based on non-homogeneous Poisson processes. The proposed model can be derived from the statistical theory of extreme-value, and has a similar asymptotic property to the deterministic Gompertz curve. Also, we develop an EM algorithm to determine the model parameters effectively. In numerical examples with software failure data observed in real software development projects, we evaluate performance of the Gompertz software reliability model in terms of reliability assessment and failure prediction.
82|3||On the similarity between requirements and architecture|Many would agree that there is a relationship between requirements engineering and software architecture. However, there have always been different opinions about the exact nature of this relationship. Nevertheless, all arguments have been based on one overarching notion: that of requirements as problem description and software architecture as the structure of a software system that solves that problem, with components and connectors as the main elements.
82|4|http://www.sciencedirect.com/science/journal/01641212/82/4|Tilte page|
82|4||Contents|
82|4||Software engineering challenges of the âNetâ generation|
82|4||Using Wikis to support the Net Generation in improving knowledge acquisition in capstone projects|Students have to cope with new technologies, changing environments, and conflicting changes in capstone projects. They often lack practical experience, which might lead to failing to achieve a project’s learning goals. In addition, the Net Generation students put new requirements upon software engineering education because they are digitally literate, always connected to the Internet and their social networks. They react fast and multitask, prefer an experimental working approach, are communicative, and need personalized learning and working environments. Reusing experiences from other students provides a first step towards building up practical knowledge and implementing experiential learning in higher education. In order to further improve knowledge acquisition during experience reuse, we present an approach based on Web 2.0 technologies that generates so-called learning spaces. This approach automatically enriches experiences with additional learning content and contextual information. To evaluate our approach, we conducted a controlled experiment, which showed a statistically significant improvement for knowledge acquisition of 204% compared to conventional experience descriptions. From a technical perspective, the approach provides a good basis for future applications that support learning at the workplace in academia and industry for the Net Generation.
82|4||Engaging the net generation with evidence-based software engineering through a community-driven web database|Software engineering faculty face the challenge of educating future researchers and industry practitioners regarding the generation of empirical software engineering studies and their use in evidence-based software engineering. In order to engage the Net generation with this topic, we propose development and population of a community-driven Web database containing summaries of empirical software engineering studies. We also present our experience with integrating these activities into a graduate software engineering course. These efforts resulted in the creation of “SEEDS: Software Engineering Evidence Database System”. Graduate students initially populated SEEDS with 216 summaries of empirical software engineering studies. The summaries were randomly sampled and reviewed by industry professionals who found the student-written summaries to be at least as useful as professional-written summaries. In fact, 30% more of the respondents found the student-written summaries to be “very useful”. Motivations, student and instructor-developed prototypes, and assessments of the resulting artifacts will be discussed.
82|4||Software engineering education: How far weâve come and how far we have to go|In this paper I trace the history of software engineering education and focus on some of the key players. I highlight what has been accomplished in degree programs and curricula, conferences and working groups, professionalism, certification, and industry–university collaboration. I also look at the challenges that lie ahead—the global reach of education, new delivery mechanisms, new professional efforts, and the need to engage in leadership in software engineering education. What new approaches should be considered? How can we educators maintain our vitality? How can we best nurture new educators and encourage others to join our profession?
82|4||Single development project|An often-cited problem in undergraduate software engineering courses states that some topics are difficult to teach in a university setting and, although laboratory work is a useful supplement to the lectures, it is difficult to make projects realistic and relevant. In recognition of this problem, and based on our past experience, we started preparing a new course by examining the pedagogies and curricular aspects of software engineering that are important for the Net Generation of software engineers.
82|4||Discovering vulnerabilities in control system humanâmachine interface software|As educators plan for curriculum enhancement and modifications to address the net-generation of software engineers, it will be important to communicate the necessity of considering software security engineering as applications are net-enabled. This paper presents a case study where commonly accepted software security engineering principles that have been published and employed for approximately 30 years, are not often seen in an important class of application software today. That class of software is commonly referred to as control system software or supervisory control and data acquisition (SCADA) software which is being used today within critical infrastructures and being net-enabled as it is modernized. This circumstance is driven by evolution and not intention. This paper details several vulnerabilities existing in a specific software application as a case study. These vulnerabilities are a result of not following widely-accepted secure software engineering practices which should have been considered by the software engineers developing the product studied. The applicability of these lessons to the classroom are also established with examples of how they are integrated into software engineering and computer science curricula.
82|4||Optimization methodology of dynamic data structures based on genetic algorithms for multimedia embedded systems|Modern multimedia application exhibit high resource utilization. In order to efficiently run this kind of applications in embedded systems, the dynamic memory subsystem needs to be optimized. A key role in this optimization is played by the dynamic data structures that reside in every real-life application. This paper presents a novel and automated way to optimize dynamic data structures. The search space is pruned using genetic algorithms that converge to the best multilayered data structure implementation for the targeted applications.
82|4||Mining frequent patterns in image databases with 9D-SPA representation|In this paper, we propose a novel algorithm, called 9DSPA-Miner, to mine frequent patterns from an image database, where every image is represented by the 9D-SPA representation. Our proposed method consists of three phases. First, we scan the database once and create an index structure. Next, the index structure is scanned to find all frequent patterns of length two. Finally, we use the frequent k-patterns (k â©¾ 2) to generate candidate (k + 1)-patterns and check if the support of each candidate generated is not less than the user-specified minimum support threshold by using the index structure. Then, the steps in the third phase are repeated until no more frequent patterns can be found. Since the 9DSPA-Miner algorithm uses the characteristics of the 9D-SPA representation to prune most of impossible candidates, the experiment results demonstrate that it is more efficient and scalable than the modified Apriori method.
82|4||Real-time task scheduling by multiobjective genetic algorithm|Real-time tasks are characterized by computational activities with timing constraints and classified into two categories: a hard real-time task and a soft real-time task. In hard real-time tasks, tardiness can be catastrophic. The goal of hard real-time tasks scheduling algorithms is to meet all tasks’ deadlines, in other words, to keep the feasibility of scheduling through admission control. However, in the case of soft real-time tasks, slight violation of deadlines is not so critical.
82|4||An event-driven high level model for the specification of laws in open multi-agent systems|The agent development paradigm poses many challenges to software engineering researchers, particularly when the systems are distributed and open. They have little or no control over the actions that agents can perform. Laws are restrictions imposed by a control mechanism to deal with uncertainty and to promote open system dependability. In this paper, we present a high level event-driven conceptual model of laws. XMLaw is an alternative approach to specifying laws in open multi-agent systems that presents high level abstractions and a flexible underlying event-based model. Thus XMLaw allows for flexible composition of the elements from its conceptual model and is flexible enough to accept new elements.
82|4||An identity based universal designated verifier signature scheme secure in the standard model|Universal designated verifier signature was first introduced by Steinfeld, Bull, Wang and Pieprzyk in Asiacrypt 2003. In the universal designated verifier signature scheme, any holder of a signature can designate the signature to any desired designated verifier, such that only the designated verifier will believe that the signature holder holds a valid signature. In SecUbiq’05 [Zhang Fangguo, Susilo Willy, Mu Yi, Chen Xiaofeng, 2005. Identity-based universal designated verifier signatures. In: The First International Workshop on Security in Ubiquitous Computing Systems, Nagasaki, Japan, LNCS 3823. Springer-Verlag, Berlin, pp. 825–834] first extended this notion to the identity based setting and proposed two identity based universal designated verifier signature schemes. However, the security of their scheme is based on the random oracle model. Up to now, there is no provably secure identity based universal designated verifier signature scheme in the standard model. In this paper, we propose the first identity based universal designated verifier signature scheme whose security can be proven in the standard model based on the hardness of the computational Diffie–Hellman (CDH) problem.
82|4||Reducing software requirement perception gaps through coordination mechanisms|Users and information system professionals view the world differently. This perception difference leads to an inability to fully define the information requirements of a new system. Practitioners understand this difficulty and look for solid approaches to address the problem. A model is developed that links coordination mechanisms and project partnering practices to perception gaps and project success. The premise is to use the model to confirm the expected relationships and examine coordination practices in particular for effectiveness in promoting common understanding. Survey results from information system project professionals indicate that the managerial interventions of coordination and partnering are successful in reducing the perception gaps and improving project performance. Prior research had not established a link. The results support the principle that organizations must install specific coordination techniques and implement partnering procedures prior to the commencement of project activities.
82|4||Early fire detection method in video for vessels|New generation vessels are equipped with fire detecting sensors; however, fire may not immediately be detected if it is far away from the sensors. The fire process therefore cannot be recorded. A video-based fire alarm system is developed to overcome the drawbacks of traditional fire detection equipment. This paper presents a video-based flame and smoke detection method for vessels. For flame detection, the dominant flame color lookup table (DFCLT) is created by using the fuzzy c-means clustering algorithm. The changed video frames are automatically selected and the changed regions deduced from these frames. An elementary, medium, or emergency flame alarm is then triggered by comparing the pixels of changed regions with the DFCLT. The changed video frames are automatically selected for smoke detection. The changed regions are deduced from these frames. If the shape of the changed region conforms to the characteristic which the top area is wider than the bottom area, a dangerous smoke alarm is sounded. The experimental results show that the proposed fire detection approach can detect dangerous flames and smoke, effectively and efficiently.
82|4||Using aspect orientation in legacy environments for reverse engineering using dynamic analysisâAn industrial experience report|This paper reports on the challenges of using aspect-oriented programming (AOP) to aid in re-engineering a legacy C application. More specifically, we describe how AOP helps in the important reverse engineering step which typically precedes a re-engineering effort. We first present a comparison of the available AOP tools for legacy C code bases, and then argue on our choice of Aspicere, our own AOP implementation for C. Then, we report on Aspicere’s application in reverse engineering a legacy industrial software system and we show how we apply a dynamic analysis to regain insight into the system. AOP is used for instrumenting the system and for gathering the data. This approach works and is conceptually very clean, but comes with a major quid pro quo: integration of AOP tools with the build system proves an important issue. This leads to the question of how to reconcile the notion of modular reasoning within traditional build systems with a programming paradigm which breaks this notion.
82|4||A new relevance feedback technique for iconic image retrieval based on spatial relationships|Due to the popularity of Internet and the growing demand of image access, the volume of image databases is exploding. Hence, we need a more efficient and effective image searching technology. Relevance feedback technique has been popularly used with content-based image retrieval (CBIR) to improve the precision performance, however, it has never been used with the retrieval systems based on spatial relationships. Hence, we propose a new relevance feedback framework to deal with spatial relationships represented by a specific data structure, called the 2D Be-string. The notions of relevance estimation and query reformulation are embodied in our method to exploit the relevance knowledge. The irrelevance information is collected in an irrelevant set to rule out undesired pictures and to expedite the convergence speed of relevance feedback. Our system not only handles picture-based relevance feedback, but also deals with region-based feedback mechanism, such that the efficacy and effectiveness of our retrieval system are both satisfactory.
82|4||Energy-efficient real-time object tracking in multi-level sensor networks by mining and predicting movement patterns|A number of studies have been written on sensor networks in the past few years due to their wide range of potential applications. Object tracking is an important topic in sensor networks; and the limited power of sensor nodes presents numerous challenges to researchers. Previous studies of energy conservation in sensor networks have considered object movement behavior to be random. However, in some applications, the movement behavior of an object is often based on certain underlying events instead of randomness completely. Moreover, few studies have considered the real-time issue in addition to the energy saving problem for object tracking in sensor networks. In this paper, we propose a novel strategy named multi-level object tracking strategy (MLOT) for energy-efficient and real-time tracking of the moving objects in sensor networks by mining the movement log. In MLOT, we first conduct hierarchical clustering to form a hierarchical model of the sensor nodes. Second, the movement logs of the moving objects are analyzed by a data mining algorithm to obtain the movement patterns, which are then used to predict the next position of a moving object. We use the multi-level structure to represent the hierarchical relations among sensor nodes so as to achieve the goal of keeping track of moving objects in a real-time manner. Through experimental evaluation of various simulated conditions, the proposed method is shown to deliver excellent performance in terms of both energy efficiency and timeliness.
82|4||A belief-theoretic framework for the collaborative development and integration of para-consistent conceptual models|Merging and integrating different conceptual models which have been collaboratively developed by domain experts and analysts with dissimilar perspectives on the same issue has been the subject of tremendous amount of research. In this paper, we focus on the fact that human analysts’ opinions possess a degree of uncertainty which can be exploited while integrating such information. We propose an underlying modeling construct which is the basis for transforming conceptual models into a manipulatable format. Based on this construct, methods for formally negotiating over and merging of conceptual models are proposed. The approach presented in this paper focuses on the formalization of uncertainty and expert reliability through the employment of belief theory. The proposed work has been evaluated for its effectiveness and usability. The evaluators (a group of Computer Science graduate students) believed that the proposed framework has the capability to fulfil its intended tasks. The obtained results from the performance perspective are also promising.
82|4||Use of an adaptable quality model approach in a production support environment|Many of the early quality models have followed a hierarchical approach; A set of factors that affect quality are defined but there is little scope for expansion. Later models allow collective decisions to be made as to what attributes constitute quality but comparison across projects is difficult as a result.
82|5|http://www.sciencedirect.com/science/journal/01641212/82/5|Identifying exogenous drivers and evolutionary stages in FLOSS projects|The success of a Free/Libre/Open Source Software (FLOSS) project has been evaluated in the past through the number of commits made to its configuration management system, number of developers and number of users. Most studies, based on a popular FLOSS repository (SourceForge), have concluded that the vast majority of projects are failures.
82|5||Refining component description by leveraging user query logs|How to help reusers retrieve components efficiently and conveniently is critical to the success of the component-based software development (CBSD). In the literature, many research efforts have been devoted to the improvement of component retrieval mechanisms. Although various retrieval methods have been proposed, nowadays retrieving software component by the description text is still prevalent in most real-world scenarios. Therefore, the quality of the component description text is vital for the component retrieval. Unfortunately, the descriptions of components often contain improper or even noisy information which could deteriorate the effectiveness of the retrieval mechanism. To alleviate the problem, in this paper, we propose an approach which can improve the component description by leveraging user query logs. The key idea of our approach is to refine the description of a component by extracting proper information from the user query logs. Two different strategies are proposed to carry out the information extraction. The first strategy extracts information for a component only from its own related query logs. Whereas our second strategy further takes logs from similar components into consideration. We performed an experimental study on two different data sets to evaluate the effectiveness of our approach. The experimental results demonstrate that by using either extraction strategy our approach can improve retrieval performance and our approach can be more effective by leveraging the second strategy which utilizes logs from similar components.
82|5||Dynamic SLAs management in service oriented environments|The increasing adoption of service oriented architectures across different administrative domains, forces service providers to use effective mechanisms and strategies of resource management in order for them to be able to guarantee the quality levels their customers demands during service provisioning. Service level agreements (SLA) are the most common mechanism used to establish agreements on the quality of a service (QoS) between a service provider and a service consumer. The WS-Agreement specification, developed by the Open Grid Forum, is a Web Service protocol to establish agreements on the QoS level to be guaranteed in the provision of a service. The committed agreement cannot be modified during service provision and is effective until all activities pertaining to it are finished or until one of the signing party decides to terminate it. In B2B scenarios where several service providers are involved in the composition of a service, and each of them plays both the parts of provider and customer, several one-to-one SLAs need to be signed. In such a rigid context the global QoS of the final service can be strongly affected by any violation on each single SLA. In order to prevent such violations, SLAs need to adapt to any possible needs that might come up during service provision. In this work we focus on the WS-Agreement specification and propose to enhance the flexibility of its approach. We integrate new functionality to the protocol that enable the parties of a WS-Agreement to re-negotiate and modify its terms during the service provision, and show how a typical scenario of service composition can benefit from our proposal.
82|5||Searching for similar trajectories in spatial networks|In several applications, data objects move on pre-defined spatial networks such as road segments, railways, and invisible air routes. Many of these objects exhibit similarity with respect to their traversed paths, and therefore two objects can be correlated based on their motion similarity. Useful information can be retrieved from these correlations and this knowledge can be used to define similarity classes. In this paper, we study similarity search for moving object trajectories in spatial networks. The problem poses some important challenges, since it is quite different from the case where objects are allowed to move freely in any direction without motion restrictions. New similarity measures should be employed to express similarity between two trajectories that do not necessarily share any common sub-path. We define new similarity measures based on spatial and temporal characteristics of trajectories, such that the notion of similarity in space and time is well expressed, and moreover they satisfy the metric properties. In addition, we demonstrate that similarity range queries in trajectories are efficiently supported by utilizing metric-based access methods, such as M-trees.
82|5||Design of DL-based certificateless digital signatures|Public-key cryptosystems without requiring digital certificates are very attractive in wireless communications due to limitations imposed by communication bandwidth and computational resource of the mobile wireless communication devices. To eliminate public-key digital certificate, Shamir introduced the concept of the identity-based (ID-based) cryptosystem. The main advantage of the ID-based cryptosystem is that instead of using a random integer as each user’s public key as in the traditional public-key systems, the user’s real identity, such as user’s name or email address, becomes the user’s public key. However, all identity-based signature (IBS) schemes have the inherent key escrow problem, that is private key generator (PKG) knows the private key of each user. As a result, the PKG is able to sign any message on the users’ behalf. This nature violates the “non-repudiation” requirement of digital signatures. To solve the key escrow problem of the IBS while still taking advantage of the benefits of the IBS, certificateless digital signature (CDS) was introduced. In this paper, we propose a generalized approach to construct CDS schemes. In our proposed CDS scheme, the user’s private key is known only to the user himself, therefore, it can eliminate the key escrow problem from the PKG. The proposed construction can be applied to all Discrete Logarithm (DL)-based signature schemes to convert a digital signature scheme into a CDS scheme. The proposed CDS scheme is secure against adaptive chosen-message attack in the random oracle model. In addition, it is also efficient in signature generation and verification.
82|5||Improvement of identity-based proxy multi-signature scheme|
82|5||The relation of requirements uncertainty and stakeholder perception gaps to project management performance|Researchers consider requirements uncertainty as a problem to be addressed during information system development by choosing an appropriate strategy to mitigate the uncertainty. However, this strategy avoids addressing issues present at the start of a project. Those include differences in perception between two prominent stakeholders: users and developers. The problems caused by this perception gap are demonstrated to be at least as significant as components of requirements uncertainty. A model is developed and empirically tested that shows a good portion of residual performance risks in a project are explained by perception gaps. These gaps present a new opportunity to address difficulties in a project before the development efforts begin.
82|5||Dynamic Web Service discovery architecture based on a novel peer based overlay network|Service Oriented Computing and its most famous implementation technology Web Services (WS) are becoming an important enabler of networked business models. Discovery mechanisms are a critical factor to the overall utility of Web Services. So far, discovery mechanisms based on the UDDI standard rely on many centralized and area-specific directories, which poses information stress problems such as performance bottlenecks and fault tolerance. In this context, decentralized approaches based on Peer to Peer overlay networks have been proposed by many researchers as a solution. In this paper, we propose a new structured P2P overlay network infrastructure designed for Web Services Discovery. We present theoretical analysis backed up by experimental results, showing that the proposed solution outperforms popular decentralized infrastructures for web discovery, Chord (and some of its successors), BATON (and it’s successor) and Skip-Graphs.
82|5||An index management using CHC-cluster for flash memory databases|Flash memories are one of the best media to support portable and desktop computers’ storage areas. Their features include non-volatility, low power consumption, and fast access time for read operations, features which are sufficient to present flash memories as major database storage components for portable computers. However, we need to improve traditional index management schemes based on B-Tree due to the relatively slow characteristics of flash memory operations compared to RAM memory. In order to achieve this goal, we propose a new index management scheme based on a compressed hot–cold clustering called CHC-Tree. The CHC-Tree-based index management scheme improves index operation performance by compressing the flash index nodes and clustering the hot–cold segments. The cold cluster compression techniques using unused free area in index node reduces the number of slow write operations in index node insert/delete processes. Our performance evaluation shows that our scheme significantly reduces the write operation overheads, improving the index update performance of B-Tree by 21.9%.
82|5||Evaluating two ways of calculating priorities in requirements hierarchies â An experiment on hierarchical cumulative voting|When developing large-scale software systems, there is often a large amount of requirements present, and they often reside on several hierarchical levels. In most cases, not all stated requirements can be implemented into the product due to different constraints, and the requirements must hence be prioritized. As requirements on different abstraction levels shall not be compared, prioritization techniques that are able to handle multi-level prioritization are needed. Different such techniques exist, but they seem to result in unfair comparisons when a hierarchy is unbalanced. In this paper, an empirical experiment is presented where an approach that compensate for this challenge is evaluated. The results indicate that some form of compensation is preferred, and that the subjects’ preference is not influenced by the amount of information given.
82|5||Determining factors that affect long-term evolution in scientific application software|One of the characteristics of scientific application software is its long lifetime of active maintenance. There has been little software engineering research into the development characteristics of scientific software and into the factors that support its successful long evolution. The research described in this paper introduces a novel model to examine the nature of change that influenced an example of industrial scientific software over its lifetime. The research uses the model to provide an objective analysis of factors that contributed to long-term evolution of the software system. Conclusions suggest that the architectural design of the software and the characteristics of the software development group played a major role in the successful evolution of the software. The novel model of change and the research method developed for this study are independent of the type of software under study.
82|5||A static API birthmark for Windows binary executables|A software birthmark is the inherent characteristics of a program extracted from the program itself. By comparing birthmarks, we can detect whether a program is a copy of another program or not. We propose a static API birthmark for Windows executables that utilizes sets of API calls identified by a disassembler statically. By comparing 49 Windows executables, we show that our birthmark can distinguish similar programs and detect copies. By comparing binaries generated by various compilers, we also demonstrate that our birthmark is resilient. We compare our birthmark with a previous Windows dynamic birthmark to show that it is more appropriate for GUI applications.
82|5||Improving reliability of cooperative concurrent systems with exception flow analysis|Developers of fault-tolerant distributed systems need to guarantee that fault tolerance mechanisms they build are in themselves reliable. Otherwise, these mechanisms might in the end negatively affect overall system dependability, thus defeating the purpose of introducing fault tolerance into the system. To achieve the desired levels of reliability, mechanisms for detecting and handling errors should be developed rigorously or formally. We present an approach to modeling and verifying fault-tolerant distributed systems that use exception handling as the main fault tolerance mechanism. In the proposed approach, a formal model is employed to specify the structure of a system in terms of cooperating participants that handle exceptions in a coordinated manner, and coordinated atomic actions serve as representatives of mechanisms for exception handling in concurrent systems. We validate the approach through two case studies: (i) a system responsible for managing a production cell, and (ii) a medical control system. In both systems, the proposed approach has helped us to uncover design faults in the form of implicit assumptions and omissions in the original specifications.
82|5||Autonomous mobile agent routing for efficient server resource allocation|Mobile agents are becoming increasingly important in the highly distributed applications frameworks seen today. Their routing/dispatching from node to node is a very important issue as we need to safeguard application efficiency, achieve better load balancing and resource utilization throughout the underlying network. Selecting the best target server for dispatching a mobile agent is, therefore, a multi-faceted problem that needs to be carefully tackled. In this paper we propose distributed, adaptive routing schemes (next node selection) for mobile agents. The proposed schemes overcome risks like load oscillations, i.e., agents simultaneously abandoning a congested node in search for other, less saturated node. We try to induce different routing decisions taken by agents to achieve load balancing and better utilization of network resources. We consider five different algorithms and evaluate them through simulations. Our findings are quite promising both from the user/application and the network/infrastructure perspective.
82|5||An entropy-based algorithm for data elimination in time-driven software instrumentation|While monitoring, instrumented long running parallel applications generate huge amount of instrumentation data. Processing and storing this data incurs overhead, and perturbs the execution. A technique that eliminates unnecessary instrumentation data and lowers the intrusion without loosing any performance information is valuable for tool developers. This paper presents a new algorithm for software instrumentation to measure the amount of information content of instrumentation data to be collected. The algorithm is based on entropy concept introduced in information theory, and it makes selective data collection for a time-driven software monitoring system possible.
82|6|http://www.sciencedirect.com/science/journal/01641212/82/6|Lightweight query-based analysis of workflow process dependencies|
82|6||A delegation-based approach for the unanticipated dynamic evolution of distributed objects|Large information systems are typically distributed and cater to several client programs, with different needs. Traditional approaches to software development and deployment cannot handle situations where (i) the needs of one client application evolve over time, diverging from the needs of others, and (ii) when the server application cannot be shutdown for maintenance. In this paper, we propose an experimental framework for the unanticipated dynamic evolution of distributed objects that enables us to: (i) extend the behavior of distributed objects during run-time, requiring no shutdown, and (ii) offer different functionalities to different applications simultaneously. In our approach, new client programs can invoke behavioral extensions to server objects that are visible only to them, while legacy applications may continue to use the non-extended versions of the server. Our approach has the advantage of: (i) requiring no changes to the host programming language or to the virtual machine, and (ii) providing a transparent programming model to the developer. In this paper, we describe the problem of unanticipated dynamic evolution of distributed objects, the principles underlying our approach, and our prototype implementations for Java and C#. We conclude by discussing related work, and the extent to which our approach can be used to support industrial strength unanticipated evolution.
82|6||CUDL language semantics: Updating FDB data|The semantics for data manipulation of the database language CUDL – Conceptual Universal Database Language – designed to manage dynamic database environments, are presented. This language conforms to the FDB (Frame DataBase) data model, offering a simple, easy and efficient platform for the use of the FDB model. Otherwise the management and operation of FDB data is laborious and time-consuming and it requires from the user a very good acquaintance of the proposed model, the structures and organisation of it as well as the processes of the management of elements that compose it. In this paper we present in depth the semantics of the way of handling the data, in order to search and transform information, in an FDB data source. We present the analysis of simple and complex cases that led us to synthesize valid and simple semantic rules that determine the data manipulation operations. The more sophisticated and demanding constructs, used in the language, for query specification, query processing and object manipulation are discussed and evaluated.
82|6||An efficient void resolution method for geographic routing in wireless sensor networks|Geographic routing is an attractive choice for routing data in wireless sensor networks because of lightweight and scalable characteristics. Most geographic routing approaches combine a greedy forwarding scheme and a void resolution method to detour a void area that has no active sensor. The previous solutions, including the well-known GPSR protocol, commonly use the right-hand rule for void resolution. However, the detour path produced by the right-hand rule is not energy-efficient in many cases. In this paper, we propose a new void resolution method, called void resolution-forwarding, which can overcome voids in the sensor network energy-efficiently. It exploits the quadrant-level right-hand rule to select the next hop for the current node during circumventing a void area. We show by experiments that the proposed method is efficient and scalable with respect to the various voids and network density and that it outperforms the GPSR protocol significantly.
82|6||Efficient self-certified proxy CAE scheme and its variants|Elaborating on the merits of proxy signature schemes and convertible authenticated encryption (CAE) schemes, we adopt self-certified public key systems to construct efficient proxy CAE schemes enabling an authorized proxy signer to generate an authenticated ciphertext on behalf of the original signer. To satisfy the requirement of confidentiality, only the designated recipient is capable of decrypting the ciphertext and verifying the proxy signature. A significant advantage of the proposed schemes is that the proxy signature conversion process takes no extra cost, i.e., when the case of a later dispute over repudiation occurs, the designated recipient can easily reveal the ordinary proxy signature for the public arbitration. If needed, the designated recipient can also convince anyone that he is the real recipient. In addition, integrating with self-certified public key systems, our schemes can earn more computational efficiency, since authenticating the public key and verifying the proxy signature can be simultaneously carried out within one-step.
82|6||An empirical analysis of the impact of software development problem factors on software maintainability|Many problem factors in the software development phase affect the maintainability of the delivered software systems. Therefore, understanding software development problem factors can help in not only reducing the incidence of project failure but can also ensure software maintainability. This study focuses on those software development problem factors which may possibly affect software maintainability. Twenty-five problem factors were classified into five dimensions; a questionnaire was designed and 137 software projects were surveyed. A K-means cluster analysis was performed to classify the projects into three groups of low, medium and high maintainability projects. For projects which had a higher level of severity of problem factors, the influence on software maintainability becomes more obvious. The influence of software process improvement (SPI) on project problems and the associated software maintainability was also examined in this study. Results suggest that SPI can help reduce the level of severity of the documentation quality and process management problems, and is only likely to enhance software maintainability to a medium level. Finally, the top 10 list of higher-severity software development problem factors was identified, and implications were discussed.
82|6||Software engineering technology innovation â Turning research results into industrial success|This paper deals with the innovation of software engineering technologies. These technologies are methods and tools for conducting software development and maintenance. We consider innovation as a process consisting of two phases, being technology creation and technology transfer. In this paper, we focus mainly on the transfer phase.
82|6||Design and implementation of a Byzantine fault tolerance framework for Web services|Many Web services are expected to run with high degree of security and dependability. To achieve this goal, it is essential to use a Web services compatible framework that tolerates not only crash faults, but Byzantine faults as well, due to the untrusted communication environment in which the Web services operate. In this paper, we describe the design and implementation of such a framework, called BFT-WS. BFT-WS is designed to operate on top of the standard SOAP messaging framework for maximum interoperability. It is implemented as a pluggable module within the Axis2 architecture, as such, it requires minimum changes to the Web applications. The core fault tolerance mechanisms used in BFT-WS are based on the well-known Castro and Liskov’s BFT algorithm for optimal efficiency. Our performance measurements confirm that BFT-WS incurs only moderate runtime overhead considering the complexity of the mechanisms.
82|6||An improved lossless data hiding scheme based on image VQ-index residual value coding|Copyright protection and information security have become serious problems due to the ever growing amount of digital data over the Internet. Reversible data hiding is a special type of data hiding technique that guarantees not only the secret data but also the cover media can be reconstructed without any distortion. Traditional schemes are based on spatial, discrete cosine transformation (DCT) and discrete wavelet transformation (DWT) domains. Recently, some vector quantization (VQ) based reversible data hiding schemes have been proposed. This paper proposes an improved reversible data hiding scheme based on VQ-index residual value coding. Experimental results show that our scheme outperforms two recently proposed schemes, namely side-match vector quantization (SMVQ)-based data hiding and modified fast correlation vector quantization (MFCVQ)-based data hiding.
82|6||Extending path summary and region encoding for efficient structural query processing in native XML databases|Optimizing query processing is always a challenging task in the XML database community. Current state-of-the-art approaches focus mainly on simple query. Yet, as the usage of XML shifts towards the data-oriented paradigm, more and more complex query processing needs to be supported. In this paper, we present TwigX-Guide, a hybrid system, which takes advantage of the beautiful features of path summary in DataGuide and region encoding in TwigStack to improve complex query processing. Experimental results indicate that TwigX-Guide can process complex queries on an average 38% better than the TwigStack algorithm, 31% better than TwigINLAB, 11% better than TwigStackList and about 9% better than TwigStackXB in terms of execution time.
82|6||Using phrases as features in email classification|In this paper, we report our experience on the use of phrases as basic features in the email classification problem. We performed extensive empirical evaluation using our large email collections and tested with three text classification algorithms, namely, a naive Bayes classifier and two k-NN classifiers using TF–IDF weighting and resemblance respectively. The investigation includes studies on the effect of phrase size, the size of local and global sampling, the neighbourhood size, and various methods to improve the classification accuracy. We determined suitable settings for various parameters of the classifiers and performed a comparison among the classifiers with their best settings. Our result shows that no classifier dominates the others in terms of classification accuracy. Also, we made a number of observations on the special characteristics of emails. In particular, we observed that public emails are easier to classify than private ones.
82|6||Measuring e-business dependability: The employee perspective|Electronic business (e-business) enables employees in organizations to complete jobs more efficiently if they can rely on the dependable functions and services delivered by e-business. Unfortunately, only a limited amount of research has explored the topic of employee perceived dependability of e-business, and the concept lacks measurement tools. Hence, this research develops a validated instrument for measuring e-business dependability (EBD) from an employee perspective. Based on a survey of 152 employees in six large semiconductor manufacturing companies in the Hsinchu Science-based Industrial Park in Taiwan, this research conceptualizes and operationalizes the construct of EBD, and then creates and refines an e-business dependability instrument (EBDI). After rigorous purification procedures, the 22-item EBD instrument with good reliability and validity is presented. Finally, this paper concludes with a discussion of potential applications of this EBD instrument. The e-business dependability instrument and its potential implications will not only aid researchers interested in designing and testing related e-business theories, but also provide direction for managers in improving the quality and performance of e-business.
82|7|http://www.sciencedirect.com/science/journal/01641212/82/7|nAIT: A source analysis and instrumentation framework for nesC|Automated software engineering methods support the construction, maintenance, and analysis of both new and legacy systems. Their application is commonplace in desktop- and enterprise-class systems due to the productivity and reliability benefits they afford. The contribution of this article is to present an applied foundation for extending the use of such methods to the flourishing domain of wireless sensor networks. The objective is to enable developers to construct tools that aid in understanding both the static and dynamic properties of reactive, event-based systems. We present a static analysis and instrumentation toolkit for the nesC language, the defacto standard for sensor network development. We highlight the novel aspects of the toolkit, analyze its performance, and provide representative case-studies that illustrate its use.
82|7||HIPaG: An energy-efficient in-network join for distributed condition tables in sensor networks|
82|7||Grid enabled MRP process improvement under distributed database environment|The material requirement planning (MRP) process is crucial when software packages, like enterprise resource planning (ERP) software, are used in the production planning for manufacturing enterprises to ensure that appropriate quantities of raw materials and subassemblies are provided at the right time. Whereas little attention has been paid to the architectural aspects of MRP process in academic studies, in practice, reports are often made of its time consuming characteristics due to intensive interactions with databases and difficulty in real time processing. This paper proposes a grid enabled MRP process in a distributed database environment and demonstrates the performance improvement of the proposed process by a simulation study.
82|7||Period sensitivity analysis and DâP domain feasibility region in dynamic priority systems|In the design of a real-time application it is fundamental to know how a change in the task parameters would affect the feasibility of the system. Relaxing the classical assumptions on static task sets with fixed periods and deadlines can give higher resource utilisation and better performance. But the changes on task parameters have to be done always maintaining feasibility. In practice, period and deadline modifications are only necessary on single tasks. Our work focuses on finding the feasibility region of deadlines and periods (called D–P feasibility region) for a single task in the context of dynamic, uniprocessor scheduling of hard real-time systems. This way, designers can choose the optimal deadline and period pairs that best fit application requirements. We provide an exact and an approximated algorithm to calculate this region. We will show that the approximated solution is very close to the exact one and it takes considerably less time.
82|7||An efficient approach for distributed dynamic channel allocation with queues for real-time and non-real-time traffic in cellular networks|We are witnessing these days a rapid growth of mobile users. Therefore, frequency spectrum must be efficiently utilized, as available frequency spectrum is limited. This paper proposes a channel allocation scheme with efficient bandwidth reservation, which initially reserves some channels for handoff calls, and later reserves the channels dynamically, based on the user mobility. The direction of user mobility may not be straight always, but the user may also go left, right or backwards. Thus, QoS can be improved, if the channel reservation is made based upon the user mobility and the location of the user. We devise here a new algorithm that deals with multiple traffic systems by modifying the existing DDCA algorithm [Krishna, P.V., Iyengar, N.Ch.S.N., 2008. Optimal channel allocation algorithm with efficient channel reservation for cellular networks. International Journal of Communication Networks and Distributed Systems 1 (1), 33–51]. This algorithm reserves more channels for hot cells, less number of channels for cold cells and an average number of channels for the medium cells. Furthermore, we maintain queues for all types of calls. We model the system by a three-dimensional Markov Chain and compute the QoS parameters in terms of the blocking probability of originating calls and the dropping probability of handoff calls. The results indicate that the proposed channel allocation scheme exhibits better performance by considering the above mentioned user mobility, type of cells, and maintaining of the queues for various traffic sources. In addition, it can be observed that our approach reduces the dropping probability by using reservation factor.
82|7||Functional metamodels for systems and software|The modeling, analysis and design of systems is generally based on many formalisms to describe discrete and/or continuous behaviors, and to map these descriptions into a specific platform. In this context, the article proposes the concept of functional metamodeling to capture, then to integrate modeling languages. The concept offers an alternative to standard Model Driven Engineering (MDE) and is well adapted to mathematical descriptions such as the ones found in system modeling. As an application, a set of functional metamodels is proposed for dataflows (usable to model continuous behaviors), state-transition systems (usable to model discrete behaviors) and a metamodel for actions (to model interactions with a target platform and concurrent execution). A model of a control architecture for a legged robot is proposed as an application of these modeling languages.
82|7||Building problem-solving environments with the Arches framework|The computational problems that scientists face are rapidly escalating in size and scope. Moreover, the computer systems used to solve these problems are becoming significantly more complex than the familiar, well-understood sequential model on their desktops. While it is possible to re-train scientists to use emerging high-performance computing (HPC) models, it is much more effective to provide them with a higher-level programming environment that has been specialized to their particular domain. By fostering interaction between HPC specialists and the domain scientists, problem-solving environments (PSEs) provide a collaborative environment. A PSE environment allows scientists to focus on expressing their computational problem while the PSE and associated tools support mapping that domain-specific problem to a high-performance computing system.
82|7||A scalable publish/subscribe system for large mobile ad hoc networks|Since nodes that compose mobile ad hoc networks (MANETs) does not have any prior knowledge about other nodes in many cases, the publish/subscribe communication paradigm that has the decoupling and asynchrony properties can be useful to share information between nodes. Existing publish/subscribe services for MANETs can be categorized into document flooding (DF), destination-based routing (DBR), and content-based routing (CBR). Although those approaches may work well when the size of network is small, all of them suffer from the performance decline as the size of the network increases. In this paper, we compare those approaches, and then propose a scalable publish/subscribe communication scheme in large MANETs by combining DF and CBR hierarchically. Our approach is to cluster all nodes in networks and to exploit CBR and DF for the intra- and inter-cluster communication, respectively. By using this approach, we can effectively utilize benefits of both approaches. Then, we present performance evaluation results which validate our idea with respect to system performance and scalability.
82|7||Security weakness of Tsengâs fault-tolerant conference-key agreement protocol|A fault-tolerant conference-key agreement protocol establishes a shared key among participants of a conference even when some malicious participants disrupt key agreement processes. Recently, Tseng proposed a new fault-tolerant conference-key agreement protocol that only requires a constant message size and a small number of rounds. In this paper, we show that the Tseng’s protocol cannot provide forward and backward confidentiality during a conference session for the proposed attack method. We also show that a simple countermeasure—re-randomizing short-term keys of some participants—to avoid the proposed attack can be broken by extending the proposed attack method.
82|7||Fair anonymous rewarding based on electronic cash|This manuscript presents an anonymous rewarding protocol based on electronic cash. In the protocol, a reward provider publishes a problem to solicit a satisfactory solution of the problem. The provider offers a reward to the first person that can supply her/him a satisfactory solution of the problem. A reward claimant possesses a possible solution of the problem and sends her/his solution to the provider to claim the reward. Among all the claimants, the first qualified claimant is selected to obtain the reward. Not only is the privacy of the honest claimants, including the selected claimant, protected against the provider, but also the provider cannot decline the selected claimant her/his entitled reward without being detected after acquiring the solution from the claimant. Especially, if the provider can convince the message forwarding servers in the protocol that some malicious claimant has offered a forged solution to interfere with the rewarding process, these servers will help the provider to trace the dishonest claimant.
82|7||Design pattern recovery through visual language parsing and source code analysis|In this paper we propose an approach for recovering structural design patterns from object-oriented source code. The recovery process is organized in two phases. In the first phase, the design pattern instances are identified at a coarse-grained level by considering the design structure only and exploiting a parsing technique used for visual language recognition. Then, the identified candidate patterns are validated by a fine-grained source code analysis phase. The recognition process is supported by a tool, namely design pattern recovery environment, which allowed us to assess the retrieval effectiveness of the proposed approach on six public-domain programs and libraries.
82|8|http://www.sciencedirect.com/science/journal/01641212/82/8|Design decisions and design rationale in software architecture|
82|8||Visualization and comparison of architecture rationale with semantic web technologies|
82|8||Quality-driven architecture development using architectural tactics|This paper presents a quality-driven approach to embodying non-functional requirements (NFRs) into software architecture using architectural tactics. Architectural tactics are reusable architectural building blocks, providing general architectural solutions for common issues pertaining to quality attributes. In this approach, architectural tactics are represented as feature models, and their semantics is defined using the Role-Based Metamodeling Language (RBML) which is a UML-based pattern specification notation. Given a set of NFRs, architectural tactics are selected and composed, and the composed tactic is used to instantiate an initial architecture for the application. The proposed approach addresses both the structural and behavioral aspects of architecture. We describe the approach using tactics for performance, availability and security to develop an architecture for a stock trading system. We demonstrate tool support for instantiating a composed tactic to generate an initial architecture of the stock trading system.
82|8||Enriching software architecture documentation|The effective documentation of Architectural Knowledge (AK) is one of the key factors in leveraging the paradigm shift toward sharing and reusing AK. However, current documentation approaches have severe shortcomings in capturing the knowledge of large and complex systems and subsequently facilitating its usage. In this paper, we propose to tackle this problem through the enrichment of traditional architectural documentation with formal AK. We have developed an approach consisting of a method and an accompanying tool suite to support this enrichment. We evaluate our approach through a quasi-controlled experiment with the architecture of a real, large, and complex system. We provide empirical evidence that our approach helps to partially solve the problem and indicate further directions in managing documented AK.
82|8||Managing architectural decision models with dependency relations, integrity constraints, and production rules|Software architects consider capturing and sharing architectural decisions increasingly important; many tacit dependencies exist in this architectural knowledge. Architectural decision modeling makes these dependencies explicit and serves as a foundation for knowledge management tools. In practice, however, text templates and informal rich pictures rather than models are used to capture the knowledge; a formal definition of model entities and their relations is missing in the current state of the art. In this paper, we propose such a formal definition of architectural decision models as directed acyclic graphs with several types of nodes and edges. In our models, architectural decision topic groups, issues, alternatives, and outcomes form trees of nodes connected by edges expressing containment and refinement, decomposition, and triggers dependencies, as well as logical relations such as (in)compatibility of alternatives. The formalization can be used to verify integrity constraints and to organize the decision making process; production rules and dependency patterns can be defined. A reusable architectural decision model supporting service-oriented architecture design demonstrates how we use these concepts. We also present tool support and give a quantitative evaluation.
82|8||Selecting highly optimal architectural feature sets with Filtered Cartesian Flattening|Feature modeling is a common method used to capture the variability in a configurable application. A key challenge developers face when using a feature model is determining how to select a set of features for a variant that simultaneously satisfy a series of resource constraints. This paper presents an approximation technique for selecting highly optimal feature sets while adhering to resource limits. The paper provides the following contributions to configuring application variants from feature models: (1) we provide a polynomial time approximation algorithm for selecting a highly optimal set of features that adheres to a set of resource constraints, (2) we show how this algorithm can incorporate complex configuration constraints; and (3) we present empirical results showing that the approximation algorithm can be used to derive feature sets that are more than 90%+ optimal.
82|8||Context-aware service engineering: A survey|Context constitutes an essential part of service behaviour, especially when interaction with end-users is involved. As observed from the literature, context handling in service engineering has been during recent years a field of intense research, which has produced several interesting approaches. In this paper, we present research efforts that attempt mainly to decouple context handling from the service logic. We enumerate all context management categories, but focus on the most appropriate for service engineering, namely source code level, model-driven and message interception, taking also into account the fact that these have not been dealt with in detail in other surveys. A representative example is used to illustrate more precisely how these approaches can be used. Finally, all three categories are compared based on a number of criteria.
82|8||FAST: Flash-aware external sorting for mobile database systems|Recently, flash memory has gained its popularity as storage on wide spectrum of computing devices such as cellular phones, digital cameras, digital audio players and PDAs. The integration of high-density flash memory has been accelerated twice every year for past few years. As flash memory’s capacity increases and its price drops, it is expected that flash memory will be more competitive with magnetic disk drives. Therefore, it is desirable to adapt disk-based algorithms to take advantage of the flash memory technology.
82|8||On the design of an global intrusion tolerance network architecture against the internet catastrophes|Today’s security communities face a daunting challenges – how to protect the Internet from new, unknown zero day worms. Due to their innovation, these worms are hard to be stopped by traditional security mechanisms. Therefore, instead of trying to prevent the intrusion of every such a thread, this paper proposes a new system architecture, named Virtual Machine based Intrusion Tolerance Network (VMITN), which will tolerate the new worm attack until administrators remove the vulnerability leveraged by the worm. The VMITN adopts a rough-set based recognition mechanism to detect zero day worms and a virtual machine based overlay network to mitigate attacks. We have implemented a concept proof prototype system and use NS-2 simulations to study the performance of the VMITN in a large scale network. The behavior of the famous Witty worm is simulated within the NS-2 module and the simulations result showed that our VMITN architecture can provide the reliability and survivability under severe worm attacks.
82|8||WSDL and UDDI extensions for version support in web services|Versioning is an important aspect of web service development, which has not been adequately addressed so far. In this article, we propose extensions to WSDL and UDDI to support versioning of web service interfaces at development-time and run-time. We address service-level and operation-level versioning, service endpoint mapping, and version sequencing. We also propose annotation extensions for developing versioned web services in Java. We have tested the proposed solution for versioning in two real-world environments and identified considerable improvements in service development and maintenance efficiency, improved service reuse, and simplified governance.
82|8||A stepwise optimization algorithm of clustered streaming media servers|The optimization of Clustered Streaming Media Servers (CSMS), which aims at using as few hardware resources and as cost-effective as possible, while providing satisfactory performance and QoS, has a great impact on the practicability and efficiency of CSMS. Based on the analysis and formulization of critical performance factors of CSMS and the relationship among the performance, QoS, and the costs in CSMS, a stepwise optimization algorithm is developed to solve the optimization problem efficiently. The algorithm is based on an approach that models the optimization problem into a directed acyclic graph and then addresses the complex optimization problem step by step. The algorithm applies a divide and conquer model that not only reduces the complexity of the optimization problem, but also accelerates the optimization process. Progressive information is collected in the process and used in solving the problem. Furthermore, a simulation system of CSMS is necessary for the optimization algorithm to generate the accurate information produced in the entire streaming service process. Thus, we designed and implemented such a simulation system based on the theoretical performance model of CSMS and the parameters measured in practical CSMS testbed. Finally, a case study of the optimization problem is given to demonstrate the process of the algorithm, and an appropriate plan for designing practical CSMS system is illustrated.
82|8||A reversible information hiding scheme using leftâright and upâdown chinese character representation|Techniques for text data hiding are different from image data hiding, video data hiding and audio data hiding. To break through the difficulty of text data hiding, Sun, Lou and Huang proposed a novel Chinese text data hiding scheme called the L-R scheme. In the L-R scheme, Sun et al. embedded secrets into Chinese characters that can be divided into left and right components. This paper describes how our proposed scheme extends the component concept to incorporate the up and down components of Chinese characters rather than the left and right components only, to significantly enhance hiding capacity. In addition, this paper adds a reversible function to Sun et al.’s L-R scheme to make it possible for receivers to obtain the original cover text and use it repeatedly for later transmission of secrets after the initial hidden secrets have been extracted. Finally, the extended scheme simplifies the extracting procedure and efficiently reduces the memory required on the receiver side during the secret extracting phase by using a new comparison method. Experimental results confirm the improved functions offered by the proposed scheme.
82|8||An assessment of systems and software engineering scholars and institutions (2002â2006)|This paper summarizes a survey of publications in the field of systems and software engineering from 2002 to 2006. The survey is an ongoing, annual event that identifies the top 15 scholars and institutions over a 5-year period. The rankings are calculated based on the number of papers published in TSE, TOSEM, JSS, SPE, EMSE, IST, and Software. The top-ranked institution is Korea Advanced Institute of Science and Technology, Korea, and the top-ranked scholar is Magne Jørgensen of Simula Research Laboratory, Norway.
82|9|http://www.sciencedirect.com/science/journal/01641212/82/9|Editorial|
82|9||Resource prioritization of code optimization techniques for program synthesis of wireless sensor network applications|Wireless sensor network (WSN) applications sense events in situ and compute results in-network. Their software components should run on platforms with stringent constraints on node resources. To meet these constraints, developers often design their programs by trial-and-error. Such manual process is time-consuming and error-prone.
82|9||Exploring alternatives for transition verification|When an implementation under test (IUT) is state-based, and its expected abstract behavior is given in terms of a finite state machine (FSM), a checking sequence generated from a specification FSM and applied to an IUT for testing can provide us with high-level confidence in the correct functional behavior of our implementation. One of the issues here is to generate efficient checking sequences in terms of their lengths. As a major characteristics, a checking sequence must contain all Î²-sequences for transition verification. In this paper, we discuss the possibility of reducing the lengths of checking sequences by making use of the invertible transitions in the specification FSM to increase the choice of Î²-sequences to be considered for checking sequence generation. We present a sufficient condition for adopting alternative Î²-sequences and illustrate typical ways of incorporating these alternative Î²-sequences into existing methods for checking sequence generation to reduce the lengths. Compared to the direct use of three existing methods, our experiments show that most of the time the saving gained by adopting alternative Î²-sequences falls in the range of 10–40%.
82|9||Issues in using model checkers for test case generation|The use of model checkers for automated software testing has received some attention in the literature: It is convenient because it allows fully automated generation of test suites for many different test objectives. On the other hand, model checkers were not originally meant to be used this way but for formal verification, so using model checkers for testing is sometimes perceived as a “hack”. Indeed, several drawbacks result from the use of model checkers for test case generation. If model checkers were designed or adapted to take into account the needs that result from the application to software testing, this could lead to significant improvements with regard to test suite quality and performance. In this paper we identify the drawbacks of current model checkers when used for testing. We illustrate techniques to overcome these problems, and show how they could be integrated into the model checking process. In essence, the described techniques can be seen as a general road map to turn model checkers into general purpose testing tools.
82|9||Adaptive random testing based on distribution metrics|Random testing (RT) is a fundamental software testing technique. Adaptive random testing (ART), an enhancement of RT, generally uses fewer test cases than RT to detect the first failure. ART generates test cases in a random manner, together with additional test case selection criteria to enforce that the executed test cases are evenly spread over the input domain. Some studies have been conducted to measure how evenly an ART algorithm can spread its test cases with respect to some distribution metrics. These studies observed that there exists a correlation between the failure detection capability and the evenness of test case distribution. Inspired by this observation, we aim to study whether failure detection capability of ART can be enhanced by using distribution metrics as criteria for the test case selection process. Our simulations and empirical results show that the newly proposed algorithms not only improve the evenness of test case distribution, but also enhance the failure detection capability of ART.
82|9||Fault-tolerant design for wide-area Mobile IPv6 networks|Mobile IPv6 provides the mobility management for IPv6 protocol. To establish a reliable Mobile IPv6 network, fault tolerance should be also considered in the network design. This paper presents an efficient fault-tolerant approach for Mobile IPv6 networks. In the proposed approach, if a failure is detected in the home agent (HA) of a mobile node, a preferable survival HA is selected to continuously serve the mobile node. The preferable survival HA is the HA that does not incur failure and is neighboring the current location of the mobile node. The proposed approach is based on the preference of each mobile node to achieve the fault tolerance of the HA. Finally, we perform simulations to evaluate the performance of the proposed approach.
82|9||Design and implementation of MLC NAND flash-based DBMS for mobile devices|Recently, Multi-Level Cell (MLC) NAND flash memory is becoming widely used as storage media for mobile devices such as mobile phones, MP3 players, PDAs and digital cameras. MLC NAND flash memory, however, has some restrictions that hard disk or Single-Level Cell (SLC) NAND flash memory do not have. Since most traditional database techniques assume hard disk, they may not provide the best attainable performance on MLC NAND flash memory. In this paper, we design and implement an MLC NAND flash-based DBMS for mobile devices, called AceDB Flashlight, which fully exploits the unique characteristics of MLC NAND flash memory. Our performance evaluations on an MLC NAND flash-based device show that the proposed DBMS significantly outperforms the existing ones.
82|9||Incremental integrity checking of UML/OCL conceptual schemas|Integrity constraints play a key role in the specification and development of software systems since they state conditions that must always be satisfied by the system at runtime. Therefore, software systems must include some kind of integrity checking component that ensures that all constraints still hold after the execution of any operation that modifies the system state. Integrity checking must be as efficient as possible not to seriously slow down the system performance at runtime. In this sense, this paper proposes a set of techniques to facilitate the efficient integrity checking of UML-based software specifications, usually complemented with a set of integrity constraints defined in Object Constraint Language (OCL) to express all rules that cannot be graphically defined. In particular, our techniques are able to determine, at design-time, when and how each constraint must be checked at runtime to avoid irrelevant verifications. We refer to these techniques as incremental because they minimize the subset of the system state that needs to be checked after each change by assuming that the system was initially in a consistent state and just reevaluating the elements that may have been affected by that change. We also show how the techniques can be integrated in a model-driven development framework to automatically generate a final implementation that automatically checks all constraints in an incremental way.
82|9||A comparison of issues and advantages in agile and incremental development between state of the art and an industrial case|Recent empirical studies have been conducted identifying a number of issues and advantages of incremental and agile methods. However, the majority of studies focused on one model (Extreme Programming) and small projects. To draw more general conclusions we conduct a case study in large-scale development identifying issues and advantages, and compare the results with previous empirical studies on the topic. The principle results are that (1) the case study and literature agree on the benefits while new issues arise when using agile in large-scale and (2) an empirical research framework is needed to make agile studies comparable.
82|9||One-step t-fault diagnosis for hypermesh optical interconnection multiprocessor systems|To maintain high reliability and availability, system-level diagnosis should be considered for the multiprocessor systems. The self-diagnosis problem of hypermesh, emerging potential optical interconnection networks for multiprocessor systems, is solved in this paper. We derive that the precise one-step diagnosability of kn-hypermesh is n(k − 1). Based on the principle of cycle decomposition, a one-step t-fault diagnosis algorithm for kn-hypermesh which runs in O(knn(k − 1)) time also is described.
82|9||An efficient three-party authenticated key exchange protocol using elliptic curve cryptography for mobile-commerce environments|For secure communications in public network environments, various three-party authenticated key exchange (3PAKE) protocols are proposed to provide the transaction confidentiality and efficiency. In 2008, Chen et al. proposed a round-efficient 3PAKE protocol to provide the computation and communication efficiency for user authentication and session key exchange. However, we discover that the computation costs and communication loads of their protocol are still high so that it cannot be applied to mobile communications. Therefore, we propose an efficient three-party authenticated key exchange protocol based upon elliptic curve cryptography for mobile-commerce environments. Because the elliptic curve cryptography is used, the proposed 3PAKE protocol has low computation costs and light communication loads. Compared with Chen et al.’s protocol, the proposed protocol is more suitable and practical for mobile-commerce environments.
82|9||Trading decryption for speeding encryption in Rebalanced-RSA|In 1982, Quisquater and Couvreur proposed an RSA variant, called RSA-CRT, based on the Chinese Remainder Theorem to speed up RSA decryption. In 1990, Wiener suggested another RSA variant, called Rebalanced-RSA, which further speeds up RSA decryption by shifting decryption costs to encryption costs. However, this approach essentially maximizes the encryption time since the public exponent e is generally about the same order of magnitude as the RSA modulus. In this paper, we introduce two variants of Rebalanced-RSA in which the public exponent e is much smaller than the modulus, thus reducing the encryption costs, while still maintaining low decryption costs. For a 1024-bit RSA modulus, our first variant (Scheme A) offers encryption times that are at least 2.6 times faster than that in the original Rebalanced-RSA, while the second variant (Scheme B) offers encryption times at least 3 times faster. In both variants, the decrease in encryption costs is obtained at the expense of slightly increased decryption costs and increased key generation costs. Thus, the variants proposed here are best suited for applications which require low costs in encryption and decryption.
82|9||A survey on security in JXTA applications|JXTA is an open-source initiative that allows to specify a set of collaboration and communication protocols which enable the creation and deployment of peer-to-peer (P2P) applications. This paper provides a survey on its current state regarding the topic of security. The study focuses on the security evaluation of standard peer operations within the JXTA network, highlighting which issues must be seriously taken into account in those applications sensitive to security.
82|9||Multimedia Internet Rekeying for secure session mobility in ubiquitous mobile networks|Session mobility is one of new critical issues in the ubiquitous mobile networking environment. Session mobility provides a user changing its ongoing multimedia session, e.g., Voice-over-Internet Protocol (VoIP), from the currently using device to another by adapting user’s demand. In session Initial Protocol (SIP)-based multimedia services supporting session mobility, SIP serves as a signaling control protocol to negotiate session control, whereas media is transmitted using Real-time Transport Protocol (RTP). For securing multimedia sessions, Multimedia Internet Keying (MIKEY) is embedded in SIP signaling to negotiate security parameters for Secure RTP (SRTP), whereas SRTP is used to protect media stream. Since session mobility allows an ongoing multimedia session to be transferred from one device to another, a new security problem is raised, i.e., sensitive parameters may remain in the previous device when the ongoing multimedia session has been transferred to the current device. Unfortunately, current MIKEY cannot bear the aforementioned security problem in session mobility. Therefore, we propose Multimedia Internet Rekeying (MIRKEY) for session mobility in the ubiquitous mobile networking environment. Although MIKEY can be executed again to carry out the rekeying of the session key and Crypto Session bundle (CSB) update, the sensitive parameters still remain in previous devices. MIRKEY contains a SBK to bind the participated user and multimedia session. Besides, SBK can persist in rekeying based on the key chain whenever a multimedia session is transferred to other devices. As a result, SBK is operative only in the specific device. As a result, MIRKEY can solve the newly raised security problem in session mobility. Furthermore, we verify MIRKEY using Burrows–Abadi–Needham (BAN) logic and realize it in the implemented ubiquitous multimedia service platform (UMSP).
82|9||Understanding developer and manager perceptions of function points and source lines of code|Although function points (FP) are considered superior to source lines of code (SLOC) for estimating software size and monitoring developer productivity, practitioners still commonly use SLOC. One reason for this is that individuals who fill different roles on a development team, such as managers and developers, may perceive the benefits of FP differently. We conducted a survey to determine whether a perception gap exists between managers and developers for FP and SLOC across several desirable properties of software measures. Results suggest managers and developers perceive the benefits of FP differently and indicate that developers better understand the benefits of using FP than managers.
82|9||Empirical analysis of biometric technology adoption and acceptance in Botswana|There has been a slight surge in the study of technology adoption in developing countries. However, little attention has been paid to the adoption of biometric security systems. This paper reports a study that analyzed the adoption of biometric technology in a developing country from an institutional point of view. The results show that job positions (managerial and operational) could influence perceptions of innovation characteristics (especially ease of use and usefulness) in the decision to adopt biometrics. However, the unified organizational analyses indicate that ease of use, communication, size and type of organizations have significant impacts on the decision to adopt biometrics.
83|1|http://www.sciencedirect.com/science/journal/01641212/83/1|Editorial for the JSS Top Scholar Special Issue|
83|1||A systematic and comprehensive investigation of methods to build and evaluate fault prediction models|This paper describes a study performed in an industrial setting that attempts to build predictive models to identify parts of a Java system with a high fault probability. The system under consideration is constantly evolving as several releases a year are shipped to customers. Developers usually have limited resources for their testing and would like to devote extra resources to faulty system parts. The main research focus of this paper is to systematically assess three aspects on how to build and evaluate fault-proneness models in the context of this large Java legacy system development project: (1) compare many data mining and machine learning techniques to build fault-proneness models, (2) assess the impact of using different metric sets such as source code structural measures and change/fault history (process measures), and (3) compare several alternative ways of assessing the performance of the models, in terms of (i) confusion matrix criteria such as accuracy and precision/recall, (ii) ranking ability, using the receiver operating characteristic area (ROC), and (iii) our proposed cost-effectiveness measure (CE).
83|1||Multi-faceted quality and defect measurement for web software and source contents|In this paper, we examine external failures and internal faults traceable to web software and source contents. We develop related defect and quality measurements based on different perspectives of customers, users, information or service hosts, maintainers, developers, integrators, and managers. These measurements can help web information and service providers with their quality assessment and improvement activities to meet the quality expectations of their customers and users. The different usages of our measurement framework by different stakeholders of web sites and web applications are also outlined and discussed. The data sources include existing web server logs and statistics reports, defect repositories from web application development and maintenance activities, and source files. We applied our approach to four diverse websites: one educational website, one open source software project website, one online catalog showroom for a small company, and one e-Commerce website for a large company. The results demonstrated the viability and effectiveness of our approach.
83|1||The effects of request formats on judgment-based effort estimation|In this paper we study the effects of a change from the traditional request “How much effort is required to complete X?” to the alternative “How much can be completed in Y work-hours?”. Studies 1 and 2 report that software professionals receiving the alternative format provided much lower, and presumably more optimistic, effort estimates of the same software development work than those receiving the traditional format. Studies 3 and 4 suggest that the effect belongs to the family of anchoring effects. An implication of our results is that project managers and clients should avoid the alternative estimation request format.
83|1||Whatâs up with software metrics? â A preliminary mapping study|Many papers are published on the topic of software metrics but it is difficult to assess the current status of metrics research.
83|1||Software project management anti-patterns|This paper explores the area of bad practices, namely anti-patterns, and their consequences in software project management (SPM). The paper surveys the multitude of anti-patterns that have been reported and documented up to now and stresses the need for tools to formally represent SPM anti-patterns, proposing specific formalisms for such purpose, namely Bayesian Belief Networks, Ontologies and Social Networks. It is also explained how the Web can provide an opportunity for capturing, storing, disseminating and ultimately avoiding SPM anti-patterns. As a consequence, anti-patterns may provide an excellent tool for educating active and future software managers. Finally, conclusions and future research trends are given.
83|1||Adaptive Random Testing: The ART of test case diversity|Random testing is not only a useful testing technique in itself, but also plays a core role in many other testing methods. Hence, any significant improvement to random testing has an impact throughout the software testing community. Recently, Adaptive Random Testing (ART) was proposed as an effective alternative to random testing. This paper presents a synthesis of the most important research results related to ART. In the course of our research and through further reflection, we have realised how the techniques and concepts of ART can be applied in a much broader context, which we present here. We believe such ideas can be applied in a variety of areas of software testing, and even beyond software testing. Amongst these ideas, we particularly note the fundamental role of diversity in test case selection strategies. We hope this paper serves to provoke further discussions and investigations of these ideas.
83|1||From integration to composition: On the impact of software product lines, global development and ecosystems|Three trends accelerate the increase in complexity of large-scale software development, i.e. software product lines, global development and software ecosystems. For the case study companies we studied, these trends caused several problems, which are organized around architecture, process and organization, and the problems are related to the efficiency and effectiveness of software development as these companies used too integration-centric approaches. We present five approaches to software development, organized from integration-centric to composition-oriented and describe the areas of applicability.
83|1||An extended XACML model to ensure secure information access for web services|More and more software systems based on web services have been developed. Web service development techniques are thus becoming crucial. To ensure secure information access, access control should be taken into consideration when developing web services. This paper proposes an extended XACML model named EXACML to ensure secure information access for web services. It is based on the technique of information flow control. Primary features offered by the model are: (1) both the information of requesters and that of web services are protected, (2) the access control of web services is more precise than just “allow or reject” policy in existing models, and (3) the model will deny non-secure information access during the execution of a web service even when a requester is allowed to invoke the web service.
83|1||An intelligent query processing for distributed ontologies|In this paper, we propose an intelligent distributed query processing method considering the characteristics of a distributed ontology environment. We suggest more general models of the distributed ontology query and the semantic mapping among distributed ontologies compared with the previous works. Our approach rewrites a distributed ontology query into multiple distributed ontology queries using the semantic mapping, and we can obtain the integrated answer through the execution of these queries. Furthermore, we propose a distributed ontology query processing algorithm with several query optimization techniques: pruning rules to remove unnecessary queries, a cost model considering site load balancing and caching, and a heuristic strategy for scheduling plans to be executed at a local site. Finally, experimental results show that our optimization techniques are effective to reduce the response time.
83|1||Assessing the impact of global variables on program dependence and dependence clusters|This paper presents results of a study of the effect of global variables on the quantity of dependence in general and on the presence of dependence clusters in particular. The paper introduces a simple transformation-based analysis algorithm for measuring the impact of globals on dependence. It reports on the application of this approach to the detailed assessment of dependence in an empirical study of 21 programs consisting of just over 50K lines of code. The technique is used to identify global variables that have a significant impact upon program dependence and to identify and characterize the ways in which global variable dependence may lead to dependence clusters. In the study, over half of the programs include such a global variable and a quarter have one that is solely responsible for a dependence cluster.
83|1||McTorrent: Using multiple communication channels for efficient bulk data dissemination in wireless sensor networks|This paper presents McTorrent, a reliable bulk data dissemination protocol for sensor networks. The protocol is designed to take advantage of multiple radio channels to reduce packet collisions and improve the latency of large object dissemination. We evaluated the performance of McTorrent via detailed simulations and experiments based upon an implementation on the TinyOS platform. Our results show that in comparison to Deluge, the de facto network reprogramming protocol for TinyOS, McTorrent significantly reduces the number of packet transmissions and the amount of time required to propagate a large data object through a sensor network.
83|1||Multi-layer bus minimization for SoC|The deployment of multiple processing elements such as a microprocessor or a Digital Signal Processor in embedded systems often results in significant communication overheads. The challenge lies in resolving the communication cost minimization problem, while simultaneously satisfying the timing constraints of job executions. In this paper, we explore bus-layer minimization problems by first identifying factors that contribute to the NP-hardness of these problems. Existing proposed algorithms and NP-hard problems are then identified and elucidated. A simulated annealing algorithm is proposed and compared with heuristics-based algorithms to provide further insights for system designers. Lastly, a series of extensive simulations is carried out and a case study is presented to show comparisons among different approaches and workloads.
83|1||An empirical investigation of architectural prototyping|Architectural prototyping is the process of using executable code to investigate stakeholders’ software architecture concerns with respect to a system under development. Previous work has established this as a useful and cost-effective way of exploration and learning of the design space of a system and in addressing issues regarding quality attributes, architectural risks, and the problem of knowledge transfer and conformance. However, the actual industrial use of architectural prototyping has not been thoroughly researched so far. In this article, we report from three studies of architectural prototyping in practice. First, we report findings from an ethnographic study of practicing software architects. Secondly, we report from a focus group on architectural prototyping involving architects from four companies. And, thirdly, we report from a survey study of 20 practicing software architects and software developers. Our findings indicate that architectural prototyping plays an important and frequent role in resolving problems experimentally, but less so in exploring alternative solutions. Furthermore, architectural prototypes include end-user or business related functionality rather than purely architectural functionality. Based on these observations we provide recommendations for effective industrial architectural prototyping.
83|1||Semantic oriented ontology cohesion metrics for ontology-based systems|Ontologies play a core role to provide shared knowledge models to semantic-driven applications targeted by Semantic Web. Ontology metrics become an important area because they can help ontology engineers to assess ontology and better control project management and development of ontology based systems, and therefore reduce the risk of project failures. In this paper, we propose a set of ontology cohesion metrics which focuses on measuring (possibly inconsistent) ontologies in the context of dynamic and changing Web. They are: Number of Ontology Partitions (NOP), Number of Minimally Inconsistent Subsets (NMIS) and Average Value of Axiom Inconsistencies (AVAI). These ontology metrics are used to measure ontological semantics rather than ontological structure. They are theoretically validated for ensuring their theoretical soundness, and further empirically validated by a standard test set of debugging ontologies. The related algorithms to compute these ontology metrics also are discussed. These metrics proposed in this paper can be used as a very useful complementarity of existing ontology cohesion metrics.
83|1||CCA2 secure (hierarchical) identity-based parallel key-insulated encryption without random oracles|
83|1||DoS-resistant ID-based password authentication scheme using smart cards|In this paper, we provide a defense mechanism to Kim–Lee–Yoo’s ID-based password authentication scheme, which is vulnerable to impersonation attacks and resource exhaustion attacks. Mutual authentication and communication privacy are regarded as essential requirements in today’s client/server-based architecture; therefore, a lightweight but secure mutual authentication method is introduced in the proposed scheme. Once the mutual authentication is successful, the session key will be established without any further computation. The proposed defense mechanism not only accomplishes the mutual authentication and the session key establishment, but also inherits the security advantages of Kim–Lee–Yoo’s scheme, e.g. it is secure against password guessing attacks and message replay attacks.
83|10|http://www.sciencedirect.com/science/journal/01641212/83/10|Varied PVD + LSB evading detection programs to spatial domain in data embedding systems|Image steganographic schemes based on the least-significant-bit (LSB) replacement method own the character of high capacity and good quality, but they are detected easily by some programs. Pixel-value differencing (PVD) approaches are one kind of methods to pass some program detections, but PVD approaches usually provide lower capacities and larger distortion. Accordingly, the combined method of PVD and LSB replacement was proposed to raise the capacity and the quality of PVD approaches over the past literatures. In this paper, not only we contribute a new exploration in spatial domain to benefiting both the capacity and the fidelity on the basis of varied LSB + PVD approaches, but also the risk of the RS-steganalysis detection program is evaded. Furthermore, proof works are conducted to proclaim the correctness of the general LSB + PVD method. Following up, the varied LSB + PVD approach is therefore applied to our scheme.
83|10||Performance Evaluation of Fast Handover in Mobile IPv6 Based on Link-Layer Information|Handover latency is the primary cause of packet loss resulting in performance degradation of standard Mobile IPv6. Mobile IPv6 with fast Handover enables a Mobile Node (MN) to quickly detect at the IP layer that it has moved to a new subnet by receiving link-related information from the link-layer; furthermore it gathers anticipative information about the new Access Point (AP) and the associated subnet prefix when the MN is still connected to the previous Corresponding Node (CN).
83|10||A model checker for WS-CDL|Service computing is becoming the prominent paradigm for distributed computing and electronic businesses. It enables developers to rapidly create their own software to meet the demands of their business processes, by composing existing services, especially Web services distributed on the Internet. WS-CDL is a W3C-proposed language for Web service composition, featuring the peer description of composite Web services amongst multiple participants. Since the traditional model checking methods based on the linear temporal logic (LTL) has limit in expressing the state-action relationship for a composite Web service model, this paper proposes a new approach, based upon the idea of Temporal Logic of Actions (TLA), to model check the composite Web services described in WS-CDL. In this paper, WS-CDL is extended by a new sub-language for expressing the temporal and action restriction properties, named TLA4CDL. The expressiveness of TLA4CDL is also discussed. The optimizing method called partial order reduction is introduced, followed by the discussion of the model checker algorithm. This leads to the development and implementation of the WS-CDL model checker. Finally, several test scenarios are provided in order to validate the WS-CDL model checker. The experimental results demonstrate this model checker is capable of detecting deadlock and verifying the specified constraint attributes by TLA4CDL. A comparison of experimental results with and without the partial order reduction method shows that our checker has better performance.
83|10||Using Scrum to guide the execution of software process improvement in small organizations|For software process improvement – SPI – there are few small organizations using models that guide the management and deployment of their improvement initiatives. This is largely because a lot of these models do not consider the special characteristics of small businesses, nor the appropriate strategies for deploying an SPI initiative in this type of organization. It should also be noted that the models which direct improvement implementation for small settings do not present an explicit process with which to organize and guide the internal work of the employees involved in the implementation of the improvement opportunities. In this paper we propose a lightweight process, which takes into account appropriate strategies for this type of organization. Our proposal, known as a “Lightweight process to incorporate improvements”, uses the philosophy of the Scrum agile method, aiming to give detailed guidelines for supporting the management and performance of the incorporation of improvement opportunities within processes and their putting into practice in small companies. We have applied the proposed process in two small companies by means of the case study research method, and from the initial results, we have observed that it is indeed suitable for small businesses.
83|10||A novel global harmony search algorithm for task assignment problem|The objective of task assignment problem (TAP) is to minimize the sum of interprocessor communication and task processing costs for a distributed system which subjects to several resource constraints. We use a novel global harmony search algorithm (NGHS) to solve this problem, and the NGHS algorithm has demonstrated higher efficiency than the improved harmony search algorithm (IHS) on finding the near optimal task assignment. We also devise a new method called normalized penalty function method to tradeo® the costs and the constraints. A large number of experiments show that our algorithm performs well on finding the near optimal task assignment, and it is a viable approach for the task assignment problem.
83|10||A novel DRM framework for peer-to-peer music content delivery|Due to rapid advances in the network communications field in recent years, the distribution of large-scale music contents has become easier and more efficient than ever before. However, the unauthorized distribution of copyright-protected content has emerged as a major concern. Accordingly, this paper presents a content distribution framework with a DRM capability for P2P networks. The robustness of the content distribution is ensured by using a network coding approach based on the Lagrange polynomial interpolation method. When the downloading peer within the network receives sufficient coded pieces, it not only reconstructs the associated blocks using a finite field Gaussian elimination method, but also creates its own copies of the coded pieces within these blocks and shares these copies amongst the other peers in the network. As a result, the distribution overhead imposed on the music provider is substantially reduced and the number of coded pieces within the network is significantly increased, thereby overcoming the “last piece problem” inherent in existing P2P schemes. In the DRM module of the framework, the RSA public-key cryptosystem is used to generate a unique digital fingerprint for every user within the network. The fingerprint is embedded within the music file in a protected form such that the music provider can establish the identification of any user performing an unauthorized distribution of the file. The experimental results confirm that the proposed framework provides an efficient and secure means of distributing large-scale copyright-protected music contents with no discernible degradation in the audio quality.
83|10||Survey of data management and analysis in disaster situations|The area of disaster management receives increasing attention from multiple disciplines of research. A key role of computer scientists has been in devising ways to manage and analyze the data produced in disaster management situations.
83|10||Image watermarking with a directed periodic pattern to embed multibit messages resilient to print-scan and compound attacks|In this paper, we propose a blind method to embed multibit watermarks in images, robust to various attacks on geometry and print-scan process. For carrying the multibit information, a method using directed watermark patterns is proposed. A message sequence is mapped to a directional angle of a periodic pattern, which is then embedded into image blocks. In the detection, using autocorrelation function, filtering, masking, and adaptive line search with Hough transform, the alignment of the autocorrelations peaks are detected from image blocks, and interpreted as a message. The method provides a robust and blind extraction of information after a print-scan attack. Additionally, the experiments with two laser printer and two printout material show that the method is robust to compound attacks, such as Stirmark random bending, shearing, aspect ratio change, cropping, translation, or JPEG combined with print-scan.
83|10||Software development team flexibility antecedents|The ability to respond to changes in the environment during the development of software is crucial in achieving a quality product. Putting the project team together to achieve the ability to react effectively requires an understanding of the nature of flexibility and capabilities that might promote the ability to respond to changing requirements and conditions. Based on dynamic capability theory, we build a model of software quality that is dependent on the flexibility of the team, with the flexibility of the team dependent on reactive and anticipatory capabilities of the team members. A questionnaire administered to 119 software development team members indicates strong linkages from reactive capabilities and mixed results for anticipatory capabilities to team flexibility. Both flexibility components of a comprehensive response and efficient response to changes are critical in achieving quality software. The items comprising the capabilities can serve to guide management in building flexible development teams.
83|10||How do architecture patterns and tactics interact? A model and annotation|Software architecture designers inevitably work with both architecture patterns and tactics. Architecture patterns describe the high-level structure and behavior of software systems as the solution to multiple system requirements, whereas tactics are design decisions that improve individual quality attribute concerns. Tactics that are implemented in existing architectures can have significant impact on the architecture patterns in the system. Similarly, tactics that are selected during initial architecture design significantly impact the architecture of the system to be designed: which patterns to use, and how they must be changed to accommodate the tactics. However, little is understood about how patterns and tactics interact. In this paper, we develop a model for the interaction of patterns and tactics that enables software architects to annotate architecture diagrams with information about the tactics used and their impact on the overall structure. This model is based on our in-depth analysis of the types of interactions involved, and we show several examples of how the model can be used to annotate different kinds of architecture diagrams. We illustrate the model and annotation by showing examples taken from real systems, and describe how the annotation was used in architecture reviews. Tactics and patterns are known architectural concepts; this work provides more specific and in-depth understanding of how they interact. Its other key contribution is that it explores the larger problem of understanding the relation between strategic decisions and how they need to be tailored in light of more tactical decisions.
83|10||A comprehensive engineering framework for guaranteeing component compatibility|Despite advances in software engineering methods and tools, understanding what software components do and ensuring that they work well together remains difficult. This is chiefly due to the lack of support for specifying component interfaces and software compositions formally. Due to these shortcomings, composed systems are subject to incompatibility errors, and software developers struggle to retrieve and understand relevant reusable entities. Constructs recently added to the Unified Modeling Language (UML) supported by validation tools can detect and solve behavioural incompatibility issues, while integrated support for characterisation using ontological techniques can describe what a component does. This paper presents a comprehensive software engineering framework that supports software composition at design time and runtime with compatibility guarantees. Our main contributions are (a) a model-driven development approach that combines UML modelling and ontology techniques for the specification of component properties, their validation and their transformation to code, (b) a middleware platform that supports component discovery, compatibility checking and deployment. Following the proposed approach gives benefits for software engineering, in particular in settings where multiple stakeholders are involved.
83|10||A software integration approach for designing and assessing dependable embedded systems|Embedded systems increasingly entail complex issues of hardware–software (HW–SW) co-design. As the number and range of SW functional components typically exceed the finite HW resources, a common approach is that of resource sharing (i.e., the deployment of diverse SW functionalities onto the same HW resources). Consequently, to result in a meaningful co-design solution, one needs to factor the issues of processing capability, power, communication bandwidth, precedence relations, real-time deadlines, space, and cost. As SW functions of diverse criticality (e.g. brake control and infotainment functions) get integrated, an explicit integration requirement need is to carefully plan resource sharing such that faults in low-criticality functions do not affect higher-criticality functions.
83|10||Multi-party covert communication with steganography and quantum secret sharing|In this paper, we address the “multi-party covert communication”, a stronger notion of security than standard secure multi-party communication. Multi-party covert communication guarantees that the process of it cannot be observed. We propose a scheme for steganographic communication based on a channel hidden within quantum secret sharing (QSS). According to our knowledge nobody has ever raised the scheme, providing us the motivation for this work. To an outside observer, participants will engage in a typical instance of QSS, just like the others. But when the session is over, covert multi-party communication has already been done. Further analysis shows that the amount of hidden information one can acquire is 0, even if either an outside observer guesses the covert communication is carrying on or a dishonest participant is eavesdropping.
83|10||Using quad smoothness to efficiently control capacityâdistortion of reversible data hiding|One of the main uses of data hiding is to protect secret messages being transmitted on the Internet. Reversible data hiding can fully recover the original host image after extracting the secret message. It is especially suitable for applications where, after extracting the secret message, the quality of the recovered host image cannot be compromised, such as for medical or military image data.
83|10||Security and privacy issues in the Portable Document Format|The Portable Document Format (PDF) was developed by Adobe in the early nineties and today it is the de-facto standard for electronic document exchange. It allows reliable reproductions of published materials on any platform and it is used by many governmental and educational institutions, as well as companies and individuals. PDF documents are also credited with being more secure than other document formats such as Microsoft Compound Document File Format or Rich Text Format.
83|10||A Superscalar software architecture model for Multi-Core Processors (MCPs)|Design of high-performance servers has become a research thrust to meet the increasing demand of network-based applications. One approach to design such architectures is to exploit the enormous computing power of Multi-Core Processors (MCPs) that are envisioned to become the state-of-the-art in processor architecture. In this paper, we propose a new software architecture model, called SuperScalar, suitable for MCP machines. The proposed SuperScalar model consists of multiple pipelined thread pools, where each pipelined thread pool consists of multiple threads, and each thread takes a different role. The main advantages of the proposed model are global information sharing by the threads and minimal memory requirement due to fewer threads.
83|10||Automated assembly of Internet-scale software systems involving autonomous agents|On the Internet, there is a great amount of distributed software entities deployed independently and behaving autonomously. This paper describes an automated approach to constructing Internet-scale software systems based on autonomous software agents. In the approach, the systems are modeled by interconnected divisions and cooperative roles. The approach adopts a dynamic trial-and-evaluation strategy to select high quality autonomous agents to undertake the responsibilities of roles, and implements a special mobile agent, called delegate, carrying the interaction information specified for responsibilities of roles to facilitate the interoperations among autonomous agents. The experiments show that the approach is highly scalable and improves the overall qualities of systems remarkably.
83|10||LESSON: A system for lecture notes searching and sharing over Internet|In this paper, we present a system LESSON for lecture notes searching and sharing, which is dedicated to both instructors and students for effectively supporting their Web-based teaching and learning activities. The LESSON system employs a metasearch engine for lecture notes searching from Web and a peer-to-peer (P2P) overlay network for lecture notes sharing among the users. A metasearch engine provides an unified access to multiple existing component search engines and has better performance than general-purpose search engines. With the help of a P2P overlay network, all computers used by instructors and students can be connected into a virtual society over the Internet and communicate directly with each other for lecture notes sharing, without any centralized server and manipulation. In order to merge results from multiple component search engines into a single ranked list, we design the RSF strategy that takes rank, similarity and features of lecture notes into account. To implement query routing decision for effectively supporting lecture notes sharing, we propose a novel query routing mechanism. Experimental results indicate that the LESSON system has better performance in lecture notes searching from Web than some popular general-purpose search engines and some existing metasearch schemes; while processing queries within the system, it outperforms some typical routing methods. Concretely, it can achieve relatively high query hit rate with low bandwidth consumption in different types of network topologies.
83|10||Embedding capacity raising in reversible data hiding based on prediction of difference expansion|Most of the proposed methods of reversible data hiding based on difference expansion require location maps to recover cover images. Although the location map can be compressed by a lossless compression algorithm, this lowers embedding capacity and increases computational cost during the procedures of embedding and extracting. The study presents an adaptive reversible data scheme based on the prediction of difference expansion. Since each cover pixel generally resembles its surrounding pixels, most of the difference values between the cover pixels and their corresponding predictive pixels are small; therefore, the proposed scheme gains from embedding capacity by taking full advantage of the large quantities of smaller difference values where secret data can be embedded. The proposed scheme offers several advantages, namely, (1) the location map is no more required, (2) the embedding capacity can be adjusted depending on the practical applications, and (3) the high embedding capacity with minimal visual distortion can be achieved. Moreover, the experimental results demonstrate that the proposed scheme yields high embedding capacity by comparing the related schemes that are proposed recently.
83|10||An ID-based aggregate signature scheme with constant pairing computations|An aggregate signature scheme allows n signatures on n distinct messages from n distinct users to aggregate a single signature. The main benefit of such schemes is that they allow bandwidth and computational savings. Since Boneh et al.’s aggregate signature scheme from pairings, there exist several trials for constructing ID-based aggregate signature schemes. However, their computational complexity for pairing computations grows linearly with the number of signers. In this paper, we propose an efficient ID-based aggregate signature scheme with constant pairing computations. We also give its security proof in the random oracle model under the Computational Diffie–Hellman assumption.
83|10||Coordination implications of software architecture in a global software development project|In this paper, we report on our experience of using design structure matrices (DSMs), derived from architecture models developed at early stages of the project, to reveal the coordination needs among globally distributed development teams. Our approach is to automatically transform the box-and-line style software architecture model into an augmented constraint network (ACN), from which a DSM can be automatically generated. After that, we represent the coordination structure among the team members as communication matrices (COMs). We then assess the consistency between the DSM and COMs. Analysis of data gathered during the Global Studio Project Version 3.0 revealed that the architectural DSM model, representing the software modular structure, is highly consistent with the COMs that represent the actual coordination structure, showing that an architectural DSM has the potential to help guide the task assignments in global software development projects.
83|10||Towards a general purpose architecture for UI generation|Many software projects spend a significant proportion of their time developing the User Interface (UI), therefore any degree of automation in this area has clear benefits. Such automation is difficult due principally to the diversity of architectures, platforms and development environments. Attempts to automate UI generation to date have contained restrictions which did not accommodate this diversity, leading to a lack of wide industry adoption or standardisation. The authors set out to understand and address these restrictions. We studied the issues of UI generation (especially duplication) in practice, using action research cycles guided by interviews, adoption studies and close collaboration with industry practitioners. In addressing the issues raised in our investigation, we identified five key characteristics any UI generation technique would need before it should expect wide adoption or standardisation. These can be summarised as: inspecting existing, heterogeneous back-end architectures; appreciating different practices in applying inspection results; recognising multiple, and mixtures of, UI widget libraries; supporting multiple, and mixtures of, UI adornments; applying multiple, and mixtures of, UI layouts. Many of these characteristics seem ignored by current approaches. In addition, we discovered an emergent feature of these characteristics that opens the possibility of a previously unattempted goal – namely, retrofitting UI generation to an existing application.
83|10||Actual vs. perceived effect of software engineering practices in the Italian industry|A commonly cited limitation of software engineering research consists in its detachment from the industrial practice. Several studies have analyzed a number of practices and identified their benefits and drawbacks but little is known about their dissemination in the industry. For a set of 18 practices commonly studied in the literature, this paper investigated diffusion, effect on the success, and perceived usefulness in 62 actual industrial projects from 28 Italian IT companies. In particular we proposed a classification of these perceptions and we were able to classify 14 practices. We found statistical evidence that 7 factors have an actual effect (positive for 6 of them, negative for one). Moreover 77% (10 out of 13) of the known good practices (e.g., importance of good project schedule or complete requirements’ list) are perceived consistently by the industry. For a few other practices (having a champion's support, using metrics, reducing quality) we noticed a lack of awareness in the industry. Starting from these observations we propose guidelines for industrial practice and suggestions for academic research.
83|10||Efficient utilization of elliptic curve cryptosystem for hierarchical access control|The elliptic curve cryptosystem (ECC) has recently received significant attention by researchers due to its high performance, low computational cost, and small key size. In this paper, an efficient key management and derivation scheme based on ECC is proposed to solve dynamic access problems in a user hierarchy. Compared to previous ECC based works, the proposed method does not require constructing interpolate polynomials, therefore, the computational complexity of key generation and key derivation is significantly reduced. At the same time, time complexity of adding/deleting security classes, modifying their relationships, and changing of secret keys is decreased in the proposed method.
83|10||Online discovery of Heart Rate Variability patterns in mobile healthcare services|Recent years, advances in day-to-day wearable sensors have led to the development of low powered physiological sensor platforms, which can be integrated in body area networks, a new enabling technology for real-time health monitoring. The bottleneck in health state awareness is the algorithm that has to interpret the sensor data. Nowadays Coronary Heart Disease (CHD) is still the leading cause of death. Many classification techniques such as decision tree and neural networks proposed for an early detection of individual at risk for CHD are not able to continuously detect heart state based on sensor data stream. In this study, we propose an online three-layer neural network to recognize Heart Rate Variability (HRV) patterns related to CHD risk in consideration of daily activities. ECG sensor data is preprocessed using Poincaré plot encoding. Incremental learning is utilized to train the network with new data without forgetting the previously learned patterns. The algorithm is named Poincaré-based HRV patterns discovering Incremental Artificial neural Network (PHIAN). When a sample is presented, the nodes in the hidden layer of PHIAN compete for determining the node with the highest similarity to the input. Error variables associated with the neuron units are used as criteria for new node insertion in hopes of allowing the network to learn new patterns and reducing classification error. However, the node insertion has to be stopped in the overlapping decision areas. We suppose that the overlaps between classes have lower probability than the centric part of the classes. Therefore, after a period of learning we remove the nodes with no neighbor. Plus, the error probability density is taken into account instead of input probability density. Finally, the predictive capability of PHIAN is compared with three previous classification models, namely Self-Organizing Map (SOM), Growing Neural Gas (GNG), and Multilayer Perceptron (MLP) in terms of classification error and network structure. The results show that PHIAN outperforms the existing techniques. Our proposed model can be efficiently applied to early detection of abnormal conditions and prevent the abnormal becoming serious.
83|10||A multicriteria approach for risks assessment in ERP maintenance|Enterprise resource planning (ERP) systems cannot remain static after their implementation, they need maintenance. ERP maintenance is a key process required by the rapidly changing business environment and the usual software maintenance needs. However, these projects are highly complex and risky. So, the risks management associated with ERP maintenance projects is crucial to attain a satisfactory performance. Unfortunately, ERP maintenance risks have not been studied in depth. For this reason, this paper presents a general risks taxonomy. It gathers together the risks affecting the performance of ERP maintenance. Moreover, the authors use the analytic hierarchy process (AHP) methodology to analyze the risks factors identified. It helps managers, vendors, consultants, auditors, users and IT staff to manage ERP maintenance better. Results suggest that the most critical stage in ERP maintenance is the first phase, which receives, identifies, classifies and ranks the software modification. The most important hazards in ERP maintenance are the cooperation and commitment of ERP users and managers.
83|10||The relation between EA effectiveness and stakeholder satisfaction|Enterprise Architecture (EA) is increasingly being used by large organizations to get a grip on the complexity of their business processes, information systems and technical infrastructure. Although seen as an important instrument to help solve major organizational problems, effectively applying EA seems no easy task. Active participation of EA stakeholders is one of the main critical success factors for EA. This participation depends on the degree in which EA helps stakeholders achieve their individual goals. A highly related topic is effectiveness of EA, the degree in which EA helps to achieve the collective goals of the organization. In this article we present our work regarding EA stakeholder satisfaction and EA effectiveness, and compare these two topics. We found that, regarding EA, the individual goals of stakeholders map quite well onto the collective goals of the organization. In a case study we conducted, we found that the organization is primarily concerned with the final results of EA, while individual stakeholders also worry about the way the architects operate.
83|10||Generating blogs out of product catalogues: An MDE approach|Blogs can be used as a conduit for customer opinions and, in so doing, building communities around products. We attempt to realise this vision by building blogs out of product catalogues. Unfortunately, the immaturity of blog engines makes this endeavour risky. This paper presents a model-driven approach to face this drawback. This implies the introduction of (meta)models: the catalogue model, based on the standard Open Catalog Format, and blog models, that elaborate on the use of blogs as conduits for virtual communities. Blog models end up being realised through blog engines. Specifically, we focus on two types of engines: a hosted blog platform and a standalone blog platform, both in Blojsom. However, the lack of standards in a broad and constantly evolving blog-engine space, hinders both the portability and the maintainability of the solution. Hence, we resort to the notion of “abstract platform” as a way to depart from the peculiarities of specific blog engines. Additionally, the paper measures the reuse gains brought by MDE in comparison with the manual coding of blogs.
83|10||Dependency-aware maintenance for highly available service-oriented grid|When the scale of computational system grow from a single machine to a Grid with potentially thousands of heterogeneous nodes, the interdependencies among the resources and software components make management and maintenance activities much more complicated. One of the most important challenges to overcome is how to balance maintenance of the system and the global system availability. In this paper, a novel mechanism is proposed, the Cobweb Guardian, which provides solutions not only to reduce the effects of maintenance but to remove the effects of dependencies on system availability due to deployment dependencies, invocation dependencies, and environment dependencies. By using the Cobweb Guardian, Grid administrators can execute the maintenance tasks safely at runtime whilst ensuring high system availability. The results of our evaluations show that our proposed dependency-aware maintenance mechanism can significantly increase the throughput and the availability of the whole system at runtime.
83|10||Modular analysis and modelling of risk scenarios with dependencies|The risk analysis of critical infrastructures such as the electric power supply or telecommunications is complicated by the fact that such infrastructures are mutually dependent. We propose a modular approach to the modelling and analysis of risk scenarios with dependencies. Our approach may be used to deduce the risk level of an overall system from previous risk analyses of its constituent systems. A custom made assumption-guarantee style is put forward as a means to describe risk scenarios with external dependencies. We also define a set of deduction rules facilitating various kinds of reasoning, including the analysis of mutual dependencies between risk scenarios expressed in the assumption-guarantee style.
83|11|http://www.sciencedirect.com/science/journal/01641212/83/11|Interplay between usability and software development|
83|11||Work-domain knowledge in usability evaluation: Experiences with Cooperative Usability Testing|Usability evaluation helps to determine whether interactive systems support users in their work tasks. However, knowledge about those tasks and, more generally, about the work-domain is difficult to bring to bear on the processes and outcome of usability evaluation. One way to include such work-domain knowledge might be Cooperative Usability Testing, an evaluation method that consists of (a) interaction phases, similar to classic usability testing, and (b) interpretation phases, where the test participant and the moderator discuss incidents and experiences from the interaction phases. We have studied whether such interpretation phases improve the relevance of usability evaluations in the development of work-domain specific systems. The study included two development cases. We conclude that the interpretation phases generate additional insight and redesign suggestions related to observed usability problems. Also, the interpretation phases generate a substantial proportion of new usability issues, thereby providing a richer evaluation output. Feedback from the developers of the evaluated systems indicates that the usability issues that are generated in the interpretation phases have substantial impact on the software development process. The benefits of the interpretation phases may be explained by the access these provide both to the test participants’ work-domain knowledge and to their experiences as users.
83|11||Exploring the benefits of the combination of a software architecture analysis and a usability evaluation of a mobile application|Designing easy to use mobile applications is a difficult task. In order to optimize the development of a usable mobile application, it is necessary to consider the mobile usage context for the design and the evaluation of the user–system interaction of a mobile application. In our research we designed a method that aligns the inspection method “Software ArchitecTure analysis of Usability Requirements realizatioN” SATURN and a mobile usability evaluation in the form of a user test. We propose to use mobile context factors and thus requirements as a common basis for both inspection and user test. After conducting both analysis and user test, the results described as usability problems are mapped and discussed. The mobile context factors identified define and describe the usage context of a mobile application. We exemplify and apply our approach in a case study. This allows us to show how our method can be used to identify more usability problems than with each method separately. Additionally, we could confirm the validity and identified the severity of usability problems found by both methods. Our work presents how a combination of both methods allows to address usability issues in a more holistic way. We argue that the increased quantity and quality of results can lead to a reduction of the number of iterations required in early stages of an iterative software development process.
83|11||Measuring effectiveness of HCI integration in software development processes|Integrating human–computer interaction (HCI) activities in software engineering (SE) processes is an often-expressed desire. Two metrics to demonstrate the impact of integrating HCI activities in SE processes are proposed. Usability Goals Achievement Metric (UGAM) is a product metric that measures the extent to which the design of a product achieves its user-experience goals. Index of Integration (IoI) is a process metric that measures the extent of integration of the HCI activities in the SE process. Both the metrics have an organizational perspective and can be applied to a wide range of products and projects. An attempt has been made to keep the metrics easy to use in the industrial context. While the two metrics were proposed mainly to establish a correlation between the two and thereby demonstrate the effectiveness of integration of HCI in SE processes, several other applications seem likely. The two metrics were evaluated in three independent studies: a classroom-based evaluation with two groups of students, a qualitative feedback from three industry projects, and a quantitative evaluation using 61 industry projects. The metrics were found to be useful, easy to use, and helpful in making the process more systematic. Our studies showed that the two metrics correlate well with each other and that IoI is a good predictor of UGAM. Regression analysis showed that IoI has a somewhat greater effect on UGAM in projects that use the agile process model than the waterfall process and in the projects that are executed as a contracted software development service than in the projects in product companies. UGAM also correlated well with the traditional usability evaluations.
83|11||SPI success factors within product usability evaluation|This article presents an experience report where we compare 8 years of experience of product related usability testing and evaluation with principles for software process improvement (SPI). In theory the product and the process views are often seen to be complementary, but studies of industry have demonstrated the opposite. Therefore, more empirical studies are needed to understand and improve the present situation. We find areas of close agreement as well as areas where our work illuminates new characteristics. It has been identified that successful SPI is dependent upon being successfully combined with a business orientation. Usability and business orientation also have strong connections although this has not been extensively addressed in SPI publications. Reasons for this could be that usability focuses on product metrics whilst today's SPI mainly focuses on process metrics. Also because today's SPI is dominated by striving towards a standardized, controllable, and predictable software engineering process; whilst successful usability efforts in organisations are more about creating a creative organisational culture advocating a useful product throughout the development and product life cycle. We provide a study and discussion that supports future development when combining usability and product focus with SPI, in particular if these efforts are related to usability process improvement efforts.
83|11||An encoding method for both image compression and data lossless information hiding|Reversible image data hiding technology means the cover image can be totally recovered after the embedded secret data is extracted. In this paper, we propose a reversible image data hiding scheme based on vector quantization (VQ) compressed images. The secret bits are embedded into the VQ index table by modifying the index value according to the difference of neighboring indices. The data hiding capacity and the size of the final codestream (embedded result) are a trade-off, and it can be decided by users. In other words, the proposed scheme has flexible hiding capacity ability. To estimate the performance, the proposed scheme was compared with the scheme proposed by Wang and Lu (2009). The results of the comparison showed that our proposed scheme is superior to the scheme proposed by Wang and Lu in both data hiding capacity and bit rate.
83|11||CLPL: Providing software infrastructure for the systematic and effective construction of complex collaborative learning systems|Over the last decade, e-Learning and in particular Computer-Supported Collaborative Learning (CSCL) needs have been evolving accordingly with more and more demanding pedagogical and technological requirements. As a result, high customization and flexibility are a must in this context, meaning that collaborative learning practices need to be continuously adapted, adjusted, and personalized to each specific target learning group. These very demanding needs of the CSCL domain represent a great challenge for the research community on software development to satisfy.
83|11||Monetary pricing of software development risks: A method and empirical illustration|The ability to price (monetize) software development risks can benefit various aspects of software development decision-making. This paper presents a risk pricing method that estimates two parameters for every individual risk factor: extra cost incurred per unit exposure, and project sensitivity, to that factor. Since variability is a widely used measure of risk in finance and decision sciences, the method derives risk pricing parameters by relating variability in risk factors to variability in project cost. This approach rests on the fact that a parametric cost estimator predicts project cost by adjusting the “nominal” cost of a project based on the expected values of risk factors (cost drivers), but the actual project cost often deviates from prediction because the actual values of risk factors normally deviate from expectations. In addition, to illustrate the viability of the method, the paper applies the method empirically with COCOMO data, to approximate risk pricing parameters for four risk factors (Personnel Capability, Process Maturity, Technology Platform, and Application Task). Importantly, though, the method could work equally well with data recorded based on other parametric cost estimators. The paper also discusses several areas that can benefit from benchmark risk pricing parameters of the kind we obtain.
83|11||Composition of architectural models: Empirical analysis and language support|Managing the architectural description (AD) of a complex software system and maintaining consistency among the different models is a demanding task. To understand the underlying problems, we analyse several non-trivial software architectures. The empirical study shows that a substantial amount of information of ADs is repeated, mainly by integrating information of different models in new models. Closer examination reveals that the absence of rigorously specified dependencies among models and the lack of support for automated composition of models are primary causes of management and consistency problems in software architecture. To tackle these problems, we introduce an approach in which compositions of models, together with relations among models, are explicitly supported in the ADL. We introduce these concepts formally and discuss a proof-of-concept instantiation of composition in xADL and its supporting tools. The approach is evaluated by comparing the original and revised ADs in an empirical study. The study indicates that our approach reduces the number of manually specified elements by 29%, and reduces the number of manual changes to elements for several realistic change scenarios by 52%.
83|11||Software Process Improvement as organizational change: A metaphorical analysis of the literature|Software Process Improvement (SPI) typically involves rather complex organizational changes. Acknowledging that managers can approach these changes in quite different ways, this paper addresses the following question: what perspectives do the research literature offer on SPI as organizational change and how is this knowledge presented and published? To answer this question, we analyzed SPI research publications with a main emphasis on organizational change using Gareth Morgan's organizational metaphors (1996) as analytical lenses. In addition, we characterized each article along the following dimensions: knowledge orientation (normative versus descriptive), theoretical emphasis (high versus low), main audience (practitioner versus academic), geographical origin (Scandinavia, the Americas, Europe, or the Asia-Pacific), and publication level (high versus low ranked journal). The review demonstrates that the literature as a whole is firmly grounded in both theory and practice, it appropriately targets both practitioner and academic audiences, and Scandinavian and American researchers are the main contributors.
83|11||A new real time disk-scheduling method based on GSR algorithm|Disk scheduling has an important role in QOS guarantee of soft real-time environments such as video-on-demand and multimedia servers. Since now, some disk-scheduling algorithms have been proposed to schedule real-time disk requests. One of the most recent algorithms is global seek-optimizing real-time (GSR) that schedules the disk requests with different ready times by a global regrouping scheme. In the present paper, we propose a real-time disk-scheduling algorithm based on GSR that is called IGSR (improved GSR). IGSR creates the scan-groups of the requests and tries to find a good feasible schedule by optimized grouping with considering another chance for tasks that miss their deadlines at initial grouping. With regard to the admission policy of tasks, two different version of proposed method are presented: the first one has been designed for the case that all the disk requests available simultaneously and second one has been designed for the case that requests are admitted dynamically (GSR does not support the second one). It means that in the second case, the request queue may change when a task is running but in the first one it does not change. Simulation results showed IGSR outperformed GSR and some other related works in terms of maximum supportable streams, number of missed deadlines, and disk throughput.
83|11||Task allocation for maximizing reliability of distributed computing systems using honeybee mating optimization|This paper deals with the problem of task allocation (i.e., to which processor should each task of an application be assigned) in heterogeneous distributed computing systems with the goal of maximizing the system reliability. The problem of finding an optimal task allocation is known to be NP-hard in the strong sense. We propose a new swarm intelligence technique based on the honeybee mating optimization (HBMO) algorithm for this problem. The HBMO based approach combines the power of simulated annealing, genetic algorithms with a fast problem specific local search heuristic to find the best possible solution within a reasonable computation time. We study the performance of the algorithm over a wide range of parameters such as the number of tasks, the number of processors, the ratio of average communication time to average computation time, and task interaction density of applications. The effectiveness and efficiency of our algorithm are demonstrated by comparing it with recently proposed task allocation algorithms for maximizing system reliability available in the literature.
83|11||Software engineering projects may fail before they are started: Post-mortem analysis of five cancelled projects|Software project cancellations are often caused by mistakes made during the project, and such cancellations make a strong economic impact. We analyzed five cancelled software engineering projects. One case was an internal product development project of a company that sells products to its customers. The other four cases were different software engineering projects, and outcomes of these projects were planned to be delivered to external customers.
83|11||An ant swarm-inspired energy-aware routing protocol for wireless ad-hoc networks|Primitive routing protocols for ad-hoc networks are “power hungry” and can therefore consume considerable amount of the limited amount of battery power resident in the nodes. Thus, routing in ad-hoc networks is very much energy-constrained. Continuous drainage of energy degrades battery performance as well. If a battery is allowed to intermittently remain in an idle state, it recovers some of its lost charge due to the charge recovery effect, which, in turn, results in prolonged battery life.
83|11||A web personalizing technique using adaptive data structures: The case of bursts in web visits|The explosive growth in the size and use of the World Wide Web continuously creates new great challenges and needs. The need for predicting the users’ preferences in order to expedite and improve the browsing though a site can be achieved through personalizing of the Websites. Recommendation and personalization algorithms aim at suggesting WebPages to users based on their current visit and past users’ navigational patterns. The problem that we address is the case where few WebPages become very popular for short periods of time and are accessed very frequently in a limited temporal space. Our aim is to deal with these bursts of visits and suggest these highly accessed pages to the future users that have common interests. Hence, in this paper, we propose a new web personalization technique, based on advanced data structures.
83|11||Software architecture awareness in long-term software product evolution|Software architecture has been established in software engineering for almost 40 years. When developing and evolving software products, architecture is expected to be even more relevant compared to contract development. However, the research results seem not to have influenced the development practice around software products very much. The architecture often only exists implicitly in discussions that accompany the development. Nonetheless many of the software products have been used for over 10, or even 20 years. How do development teams manage to accommodate changing needs and at the same time maintain the quality of the product? In order to answer this question, grounded theory study based on 15 semi-structured interviews was conducted in order to find out about the wide spectrum of architecture practices in software product developing organisations. Our results indicate that a chief architect or central developer acts as a ‘walking architecture’ devising changes and discussing local designs while at the same time updating his own knowledge about problematic aspects that need to be addressed. Architecture documentation and representations might not be used, especially if they replace the feedback from on-going developments into the ‘architecturing’ practices. Referring to results from Computer Supported Cooperative Work, we discuss how explicating the existing architecture needs to be complemented by social protocols to support the communication and knowledge sharing processes of the ‘walking architecture’.
83|11||HSP: A solution against heap sprays|Heap sprays are a new buffer overflow attack (BOA) form that can significantly increase the successful chance of a BOA even though the attacked process is protected by a lot of state-of-the-art anti-BOA mechanisms, such as ASLR, non-executable stack/DEP, signature-based IDSes, and type-safe languages. In this paper, we propose a glibc-and-ASLR-based solution to heap sprays—Heap Spray Protector (HSP). HSP controls the number and location of int 80 instructions in a process and hides the whereabouts of the only legal int 80 instruction; hence, HSP makes it difficult for attackers to issue a system call, let alone a heap spray attack. Moreover HSP can help ASLR defend against memory information leaking attacks. Furthermore, because HSP only modifies the glibc library and the kernel, it does not need to modify any source code or executable file. Finally, HSP allows attackers to execute as much code as possible before an attack can really create some damage; therefore, it enables the attacked hosts to collect more information about attackers which may be useful to block future attacks. Experimental results show HSP implemented on a Linux platform can effectively defend a system against heap sprays with less than 4.56% performance overhead.
83|11||Vertical partitioning for flash and HDD database systems|Recent advances in flash memory technology have greatly enhanced the capability of flash memory to address the I/O bottleneck problem. Flash memory has exceptional I/O performance compared to the hard disk drive (HDD). The superiority of flash memory is especially visible when dealing with random read patterns. Even though the cost of flash memory is higher than that of HDD storage, the popularity of flash memory is increasing at such a pace that it is becoming a common addition to the average computer. Recently, flash memory has been made into larger devices called solid state drives (SSDs). Although these devices can offer capacities comparable to HDDs, they are considerably more expensive per byte.
83|11||A replicated survey of software testing practices in the Canadian province of Alberta: What has changed from 2004 to 2009?|Software organizations have typically de-emphasized the importance of software testing. In an earlier study in 2004, our colleagues reported the results of an Alberta-wide regional survey of software testing techniques in practice. Five years after that first study, the authors felt it is time to replicate the survey and analyze what has changed and what not from 2004 to 2009. This study was conducted during the summer of 2009 by surveying software organizations in the Canadian province of Alberta. The survey results reveal important and interesting findings about software testing practices in Alberta, and point out what has changed from 2004 to 2009 and what not. Note that although our study is conducted in the province of Alberta, we have compared the results to few international similar studies, such as the ones conducted in the US, Turkey, Hong Kong and Australia, The study should thus be of interest to all testing professionals world-wide. Among the findings are the followings: (1) almost all companies perform unit and system testing with a slight increase since 2004, (2) automation of unit, integration and systems tests has increased sharply since 2004, (3) more organization are using observations and expert opinion to conduct usability testing, (4) the choices of test-case generation mechanisms have not changed much from 2004, (5) JUnit and IBM Rational tools are the most widely used test tools, (6) Alberta companies still face approximately the same defect-related economic issues as do companies in other jurisdictions, (7) Alberta software firms have improved their test automation capability since 2004, but there is still some room for improvement, and (8) compared to 2004, more companies are spending more effort on pre-release testing.
83|11||Perturbation-based user-input-validation testing of web applications|User-input-validation (UIV) is the first barricade that protects web applications from application-level attacks. Most UIV test tools cannot detect semantics-related vulnerabilities in validators, such as filling a five-digit number to a field that accepts a year. To address this issue, we propose a new approach to generate test inputs for UIV based on the analysis of client-side information. In particular, we use input-field information to generate valid inputs, and then perturb valid inputs to generate invalid test inputs. We conducted an empirical study to evaluate our approach. The empirical result shows that, in comparison to existing vulnerability scanners, our approach is more effective than existing vulnerability scanners in finding semantics-related vulnerabilities of UIV for web applications.
83|11||A replicated and refined empirical study of the use of friends in C++ software|The friend mechanism is widely used in C++ software even though the potential benefits of its use are disputed and little is known about when, where and why it is employed in practice. Furthermore, there is limited empirical analysis of its impact in object-oriented software, with only one study (Counsell and Newson, 2000) reported at journal level.
83|11||Seeing eye to eye? An exploratory study of free open source software usersâ perceptions|This study develops a typology that permits the classification of free open source software (FOSS) users into market segments. Based on the typology the nature and extent of perception differences between core FOSS users and FOSS users in other market segments is examined. Significant perception differences are observed between users in different market segments. Consequently, the potential barriers to FOSS adoption are identified and recommendations that may drive greater FOSS adoption are proposed.
83|11||Provably secure authenticated key exchange protocol under the CDH assumption|Constructing a secure key exchange protocol is one of the most challenging problems in information security. We propose a provably secure two-round two-party authenticated key exchange (2AKE) protocol based on the well-studied CDH assumption in eCK model to provide the strongest definition of security for key exchange protocol when using the matching session to define the partnership. The underlying hardness assumption (CDH assumption) of our protocol is weaker than these of four other provably secure 2AKE protocols in CK model or eCK model and the computational cost of our protocol is reasonable. We also present a three-round variant of our protocol to realize key conformation.
83|11||Bad news reporting on troubled IT projects: Reassessing the mediating role of responsibility in the basic whistleblowing model|Whistleblowing theory has been used in the information systems literature to help explain willingness to report bad news on troubled projects. According to whistleblowing theory, an individual first assesses the situation to determine if any action needs to be taken, then considers whether there is any personal responsibility to act, and this, in turn, shapes his/her choice of action. Information systems researchers have interpreted and used the basic whistleblowing model in a way that suggests that responsibility fully mediates the relationship between assessment and choice of action. While prior research has shown support for the basic whistleblowing model, the question of whether responsibility partially or fully mediates the relationship between the assessment and choice of action has not been tested empirically. This research provides theoretical justification for a partial mediation model. Using three different datasets, we provide empirical support for the partial mediation model.
83|11||A pattern-based prediction: An empirical approach to predict end-to-end network latency|Understanding latency in network-based applications has received considerable attention to provide consistent and acceptable levels of services. This paper presents an empirical approach, a pattern-based prediction method, to predict end-to-end network latency. The key idea of the approach is to utilize past history of latency and their variation patterns in latency predictions. After some preliminary study on simple numerical prediction models we examine the effectiveness of the proposed method with real latency data and various definitions of network stability. Our results show that the pattern-based method outperforms any single numerical model obtaining an overall prediction accuracy of 86.2%.
83|11||Development of Java based RFID application programmable interface for heterogeneous RFID system|Developing RFID based applications is a painstakingly difficult endeavor. The difficulties include non-standard software and hardware peripherals from vendors, interoperability problems between different operating systems as well as lack of expertise in terms of low-level programming for RFID (i.e. steep learning curve). In order to address these difficulties, a reusable RFIDTM API (RFID Tracking & Monitoring Application Programmable Interface) for heterogeneous RFID system has been designed and implemented. The API has been successfully employed in a number of application prototypes including tracking of inventories as well as human/object tracking and tagging. Here, the module has been tested on a number of different types and configuration of active and passive readers including that LF and UHF Readers.
83|11||Adaptive ridge regression system for software cost estimating on multi-collinear datasets|Cost estimation is one of the most critical activities in software life cycle. In past decades, a number of techniques have been proposed for cost estimation. Linear regression is yet the most frequently applied method in the literature. However, a number of studies point out that linear regression is prone to low prediction accuracy. The low prediction accuracy is due to a number of reasons such as non-linearity and non-normality. One less addressed reason is the multi-collinearities which may lead to unstable regression coefficients. On the other hand, it has been reported that multi-collinearity spreads widely across the software engineering datasets. To tackle this problem and improve regression's accuracy, we propose a holistic problem-solving approach (named adaptive ridge regression system) integrating data transformation, multi-collinearity diagnosis, ridge regression technique and multi-objective optimization. The proposed system is tested on two real world datasets with the comparisons with OLS regression, stepwise regression and other machine learning methods. The results indicate that adaptive ridge regression system can significantly improve the performance of regressions on multi-collinear datasets and produce more explainable results than machine learning methods.
83|11||A property based specification formalism classification|Specification formalisms may be classified through some common properties. Specification formalism classification may be used as a basis for the evaluation of the adequacy of formal specification languages within specific application domains. System modelers may use this classification to determine if the system modeling requirements can be met by a particular class of specification formalism. This paper evaluates existing specification formalism classification approaches by various researchers and highlights certain observations regarding the suggested classification schemes. The paper then proposes a specification formalism classification method using specification language properties. The proposed classification approach, loosely following the prototype theory for classification, maps specification language properties to specification formalisms and determines which language properties can best be achieved by different formalisms.
83|11||Disciplined and free-spirited: âTime-out behaviourâ at the Agile conference|In this article we observe and try to understand a peculiar duality in the agile community, whereby on the one hand, we see a serious professional community working hard to improve the quality of software products and submitting to the strictest discipline of high professional standards, while on the other hand, in its conferences, we see the same community adopting a playful free-spirited stance. Invoking an anthropological perspective, we propose that both the serious professional aspects and the playful free-spirited atmosphere at the conference, as well as the connection between the two, can all be seen to emerge from the fundamental principles of the agile community as expressed by its Manifesto.
83|12|http://www.sciencedirect.com/science/journal/01641212/83/12|TAIC-PART 2009 â Testing: Academic & Industrial Conference â Practice And Research Techniques: Special Section Editorial|
83|12||Two case studies in grammar-based test generation|Grammar-based test generation (GBTG) has seen extensive study and practical use since the 1970s. GBTG was introduced to generate source code for testing compilers from context-free grammars specifying language syntax. More recently, GBTG has been applied to many other testing problems, including the generation of eXtensible Markup Language (XML) documents and the generation of packets for testing communications protocols. Recent research has shown how to integrate covering-array techniques such as pairwise testing into GBTG tools. While the integration offers considerable power to the tester, there are few practical demonstrations in the literature. We present two case studies showing how to use grammars and covering arrays for automated software testing. The first case study exposes HTML injection vulnerabilities in an RSS feed parser. The second case study determines the effectiveness of network firewalls when faced with TCP flag attacks. The case studies illustrate the use of covering arrays in a GBTG context, the use of visualization to understand large test logs, and the issues and tradeoffs in the design of fully automated GBTG test suites.
83|12||An empirical investigation into branch coverage for C programs using CUTE and AUSTIN|Automated test data generation has remained a topic of considerable interest for several decades because it lies at the heart of attempts to automate the process of Software Testing. This paper reports the results of an empirical study using the dynamic symbolic-execution tool, CUTE, and a search based tool, AUSTIN on five non-trivial open source applications. The aim is to provide practitioners with an assessment of what can be achieved by existing techniques with little or no specialist knowledge and to provide researchers with baseline data against which to measure subsequent work. To achieve this, each tool is applied ‘as is’, with neither additional tuning nor supporting harnesses and with no adjustments applied to the subject programs under test. The mere fact that these tools can be applied ‘out of the box’ in this manner reflects the growing maturity of Automated test data generation. However, as might be expected, the study reveals opportunities for improvement and suggests ways to hybridize these two approaches that have hitherto been developed entirely independently.
83|12||Bottom-up reuse for multi-level testing|Lifecycle models divide the test process into consecutive test levels that are considered independently. This strict separation obstructs the view on the test process as a whole and fails to reflect the commonalities across test levels. Multi-level testing is an emerging approach that addresses the challenge of integrating test levels, putting particular emphasis on embedded systems. In this paper, we introduce a test level integration strategy based on reuse that is called bottom-up reuse. In addition, we present a test level instrument that seamlessly supports this strategy: multi-level test cases. We also provide a case study that reflects the positive results we have obtained in practice so far and demonstrates the feasibility of our test level integration approach. Bottom-up reuse and multi-level test cases lead to testing earlier on in the development process while reducing the effort required by test specification, test design, and test implementation.
83|12||Efficient multi-objective higher order mutation testing with genetic programming|It is said 90% of faults that survive manufacturer’s testing procedures are complex. That is, the corresponding bug fix contains multiple changes. Higher order mutation testing is used to study defect interactions and their impact on software testing for fault finding. We adopt a multi-objective Pareto optimal approach using Monte Carlo sampling, genetic algorithms and genetic programming to search for higher order mutants which are both hard-to-kill and realistic. The space of complex faults (higher order mutants) is much larger than that of traditional first order mutations which correspond to simple faults, nevertheless search based approaches make this scalable. The problems of non-determinism and efficiency are overcome. Easy to detect faults may become harder to detect when they interact and impossible to detect single faults may be brought to light when code contains two such faults. We use strong typing and BNF grammars in search based mutation testing to find examples of both in ancient heavily optimised every day C code.
83|12||A robust and flexible digital rights management system for home networks|A robust and flexible Digital Rights Management system for home networks is presented. In the proposed system, the central authority delegates its authorization right to the local manager in a home network by issuing a proxy certificate, and the local manager flexibly controls the access rights of home devices on digital contents with its proxy certificate. Furthermore, the proposed system provides a temporary accessing facility for external devices and achieves strong privacy for home devices. For the validation of delegated rights and the revocation of compromised local managers, a hybrid mechanism combining OCSP validation and periodic renewal of proxy certificates is also presented.
83|12||An exploratory study of architectural effects on requirements decisions|The question of the “manner in which an existing software architecture affects requirements decision-making” is considered important in the research community; however, to our knowledge, this issue has not been scientifically explored. We do not know, for example, the characteristics of such architectural effects. This paper describes an exploratory study on this question. Specific types of architectural effects on requirements decisions are identified, as are different aspects of the architecture together with the extent of their effects. This paper gives quantitative measures and qualitative interpretation of the findings. The understanding gained from this study has several implications in the areas of: project planning and risk management, requirements engineering (RE) and software architecture (SA) technology, architecture evolution, tighter integration of RE and SA processes, and middleware in architectures. Furthermore, we describe several new hypotheses that have emerged from this study, that provide grounds for future empirical work. This study involved six RE teams (of university students), whose task was to elicit new requirements for upgrading a pre-existing banking software infrastructure. The data collected was based on a new meta-model for requirements decisions, which is a bi-product of this study.
83|12||Component Point: A system-level size measure for Component-Based Software Systems|System-level size measures are particularly important in software project management as tasks such as planning and estimating the cost and schedule of software development can be performed more accurately when a size estimate of the entire system is available. However, due to the black-box nature of components, the traditional software measures are not adequate for Component-Based Software Systems (CBSS). In this paper, we describe a Function Point-like measure, named Component Point (CP), for measuring the system-level size of a CBSS specified in the Unified Modelling Language. Our approach integrates three software measures and extends an existing size measure from the more matured Object-Oriented paradigm to the related and relatively young CBSS discipline. We then apply the proposed measure to a Global Positioning System and demonstrate its viability in sizing a CBSS. An empirical analysis is also provided in order to prove the validity and usefulness of the CP measure.
83|12||Accelerated collection of sensor data by mobility-enabled topology ranks|We study the problem of fast and energy-efficient data collection of sensory data using a mobile sink, in wireless sensor networks in which both the sensors and the sink move. Motivated by relevant applications, we focus on dynamic sensory mobility and heterogeneous sensor placement. Our approach basically suggests to exploit the sensor motion to adaptively propagate information based on local conditions (such as high placement concentrations), so that the sink gradually “learns” the network and accordingly optimizes its motion. Compared to relevant solutions in the state of the art (such as the blind random walk, biased walks, and even optimized deterministic sink mobility), our method significantly reduces latency (the improvement ranges from 40% for uniform placements, to 800% for heterogeneous ones), while also improving the success rate and keeping the energy dissipation at very satisfactory levels.
83|12||Code analyzer for an online course management system|The online course management system (OCMS) assists online instruction in various aspects, including testing, course discussion, assignment submission, and assignment grading. This paper proposes a plagiarism detection system whose design is integrated with an OCMS. Online assignment submission is prone to easy plagiarism, which can seriously influence the quality of learning. In the past, plagiarism was detected manually, making it very time-consuming. This research thus focuses on developing a system involving code standardization, textual analysis, structural analysis, and variable analysis for evaluating and comparing programming codes. An agent system serves as a daemon to analyze the program codes for OCMS. For textual analysis, the Fingerprinting Algorithm was used for text comparison. Structurally, a formal algebraic expression and a dynamic control structure tree (DCS Tree) were utilized to rebuild and evaluate the program structure. For variables, not only the relevant information for each variable was recorded, but also the programming structure was analyzed where the variables are positioned. By applying a similarity measuring method, a similarity value was produced for each program in the three aspects mentioned above. This research implements an Online Detection Plagiarism System (ODPS) providing a web-based user interface. This system can be applied independently for assignment analysis of Java programs. After three comparison experiments with other researches, the results demonstrated the ODPS has many advantages and good performance. Meanwhile, a combined approach is proven that it is better than a single approach for source codes of various styles.
83|12||Novel segmentation algorithm in segmenting medical images|The aim of this paper is to develop an effective fuzzy c-means (FCM) technique for segmentation of Magnetic Resonance Images (MRI) which is seriously affected by intensity inhomogeneities that are created by radio-frequency coils. The weighted bias field information is employed in this work to deal the intensity inhomogeneities during the segmentation of MRI. In order to segment the general shaped MRI dataset which is corrupted by intensity inhomogeneities and other artifacts, the effective objective function of fuzzy c-means is constructed by replacing the Euclidean distance with kernel-induced distance. In this paper, the initial cluster centers are assigned using the proposed center initialization algorithm for executing the effective FCM iteratively. To assess the performance of proposed method in comparison with other existed methods, experiments are performed on synthetic image, real breast and brain MRIs. The clustering results are validated using Silhouette accuracy index. The experimental results demonstrate that our proposed method is a promising technique for effective segmentation of medical images.
83|12||Decision support for moving from a single product to a product portfolio in evolving software systems|Successful software systems continuously evolve to accommodate ever-changing needs of customers. Accommodating the feature requests of all the customers in a single product increases the risks and costs of software maintenance. A possible approach to mitigate these risks is to transition the evolving software system (ESS) from a single system to a portfolio of related product variants, each addressing a specific customers’ segment. This evolution should be conducted such that the extent of modifications required in ESS's structure is reduced. The proposed method COPE+ uses preferences of customers on product features to generate multiple product portfolios each containing one product variant per segment of customers. Recommendations are given to the decision maker to update the product portfolios based on structural analysis of ESS. Product portfolios are compared with the ESS using statechart representations to identify the level of similarity in their behaviors. A proof of concept is presented by application to an open-source text editing system. Structural and behavioral analysis of candidate portfolios helped the decision maker to select one portfolio out of three candidates.
83|12||Enhancing middleware support for architecture-based development through compositional weaving of styles|Architecture-based software development has been shown as an effective approach for managing the implementation complexity of large-scale software systems. Architecture-based development is often achieved with the help of a middleware, which provides implementation-level counterparts for the architectural modeling constructs. Such a middleware automatically ensures that implemented system accurately embodies the properties encoded in its architectural models. However, existing middlewares do not provide sufficient support for architectural styles. This is due to the crosscutting structure of styles that impacts the behavior of every other architectural construct, and hence the corresponding middleware facilities. We present an aspect-oriented approach that alleviates this problem by weaving the stylistic concerns with the rest of the middleware. The approach decouples stylistic concerns from other middleware facilities, which in turn improves the middleware's understandability and flexibility, and enables rapid composition of hybrid styles. We evaluate the approach and describe our experiences by providing support for several well-known styles using two open-source middleware platforms.
83|12||A perfect maze based steganographic method|In steganography, several different types of media have been used as carriers, such as images, audios and video streams, to hide secret data. Nevertheless, various novel media and applications have been developed due to the rapid growth of internet. In this paper, we select maze games as carrier media to conceal secret data. The original idea of embedding data in a maze is proposed by Niwayama et al. Their method has two disadvantages. One is the very small embedding capacity; the other is that the stego maze is not perfect. Here, we propose an improved algorithm for increasing the embedding capacity and preserving the “perfect” property.
83|12||Design and realization of ad-hoc VoIP with embedded p-SIP server|In this paper we design and implement the pseudo session initiation protocol (p-SIP) server embedded in each mobile node to provide the ad-hoc voice over Internet protocol (VoIP) services. The implemented p-SIP server, being compatible with common VoIP user agents, integrates the standard SIP protocol with SIP presence to handle SIP signaling and discovery mechanism in the ad-hoc VoIP networks. The ad-hoc VoIP signaling and voice traffic performances are analyzed using E-model R rating value for up to six hops in the implemented test-bed. We also conduct the interference experiments to imitate the practical ad-hoc VoIP environment. The analyzed results demonstrate the realization of ad-hoc VoIP services by using p-SIP server.
83|12||Two robust remote user authentication protocols using smart cards|With the rapid growth of electronic commerce and enormous demand from variants of Internet based applications, strong privacy protection and robust system security have become essential requirements for an authentication scheme or universal access control mechanism. In order to reduce implementation complexity and achieve computation efficiency, design issues for efficient and secure password based remote user authentication scheme have been extensively investigated by research community in these two decades. Recently, two well-designed password based authentication schemes using smart cards are introduced by Hsiang and Shih (2009) and Wang et al. (2009), respectively. Hsiang et al. proposed a static ID based authentication protocol and Wang et al. presented a dynamic ID based authentication scheme. The authors of both schemes claimed that their protocol delivers important security features and system functionalities, such as mutual authentication, data security, no verification table implementation, freedom on password selection, resistance against ID-theft attack, replay attack and insider attack, as well as computation efficiency. However, these two schemes still have much space for security enhancement. In this paper, we first demonstrate a series of vulnerabilities on these two schemes. Then, two enhanced protocols with corresponding remedies are proposed to eliminate all identified security flaws in both schemes.
83|12||Consistent query answers from virtually integrated XML data|When data sources are virtually integrated, there is no common and centralized method to maintain global consistency, so inconsistencies with regard to global integrity constraints are very likely to occur. In this paper, we consider the problem of defining and computing consistent query answers when queries are posed to virtual XML data integration systems, which are specified following the local-as-view approach. We propose a powerful XML constraint model to define global constraints, which can express keys and functional dependencies, and which also extends the newly introduced conditional functional dependencies to XML. We provide an approach to defining XML views, which supports not only edge-path mappings but also data-value bindings to express the join operator. We give formal definitions of repair and consistent query answers with the XML data integration settings. Given a query on the global system, we present a two-step method to compute consistent query answers. First, the given query is transformed using the global constraints, such that to run the transformed query on the original global system will generate exactly the consistent query answers. Because the global instance is not materialized, the query on the global instance is then rewritten in the form of queries on the underlying data sources by reversing rules in view definitions. We illustrate that the XPath query transformations can be implemented in XQuery. Finally, we implement prototypes of our method and evaluate our algorithms in the experiments.
83|12||Temperature-aware task scheduling algorithm for soft real-time multi-core systems|High temperature will affect the stability and performance of multi-core processors. A temperature-aware scheduling algorithm for soft real-time multi-core systems is proposed in this paper, namely LTCEDF (Low Thermal Contribution Early Deadline First). According to the core temperature and thread thermal contribution, LTCEDF performs thread migration and exchange to avoid thermal saturation and to keep temperature equilibrium among all the cores. The core temperature calculation method and the thread thermal contribution prediction method are presented. LTCEDF is simulated on ATMI simulator platform. Simulation results show that LTCEDF can not only minimize the thermal penalty, but also meet real-time guarantee. Moreover, it can create a more uniform power density map than other thermal-aware algorithms, and significantly reduce thread migration frequency.
83|12||Domain-specific language modelling with UML profiles by decoupling abstract and concrete syntaxes|UML profiling presents some acknowledged deficiencies, among which the lack of expressiveness of the profiled notations, together with the high coupling between abstract and concrete syntaxes outstand. These deficiencies may cause distress among UML-profile modellers, who are often forced to extend from unsuitable metaclasses for mere notational reasons, or even to model domain-specific languages from scratch just to avoid the UML-profiling limitations.
83|12||Fault coverage of Constrained Random Test Selection for access control: A formal analysis|A probabilistic model of fault coverage is presented. This model is used to analyze the variation in the fault detection effectiveness associated with the use of two test selection strategies: heuristics-based and Constrained Random Test Selection (CRTS). These strategies arise in the context of conformance test suite generation for Role Based Access Control (RBAC) systems. The proposed model utilizes coverage matrix based approach for fault coverage analysis. First, two boundary instances of fault distribution are considered and then generalized. The fault coverages of the test suites generated using the heuristics-based and the CRTS strategies, applied to a sample RBAC policy, are then compared through simulation. Finally the simulation results are correlated with a case study.
83|12||A uniform random test data generator for path testing|Path-oriented Random Testing (PRT) aims at generating a uniformly spread out sequence of random test data that execute a single control flow path within a program. The main challenge of PRT lies in its ability to build efficiently such a test suite in order to minimize the number of rejects (test data that execute another control flow path). We address this problem with an original divide-and-conquer approach based on constraint reasoning over finite domains, a well-recognized Constraint Programming technique. Our approach first derives path conditions by using backward symbolic execution and computes a tight over-approximation of their associated subdomain by using constraint propagation and constraint refutation. Second, a uniform random test data generator is extracted from this approximated subdomain. We implemented this approach and got experimental results that show the practical benefits of PRT based on constraint reasoning. On average, we got a two-order magnitude CPU time improvement over standard Random Testing on a set of paths extracted from classical benchmark programs.
83|12||Effective processing of continuous group-by aggregate queries in sensor networks|Aggregate queries are one of the most important queries in sensor networks. Especially, group-by aggregate queries can be used in various sensor network applications such as tracking, monitoring, and event detection. However, most research has focused on aggregate queries without a group-by clause.
83|12||Unreliable transport protocol using congestion control for high-speed networks|Currently there is no control for the real-time traffic of multimedia applications using UDP (User Datagram Protocol) in high-speed networks. Therefore, although a number of high-speed TCP (Transmission Control Protocol) protocols have been developed for gigabit-speed (or faster) links, the real-time traffic could also congest the network and result in unfairness and throughput degradation of TCP traffic. In this paper, a new unreliable transport protocol, FAST DCCP, is presented for the real-time traffic in high-speed networks. FAST DCCP is based on the DCCP protocol and adopts the FAST scheme to realize congestion control. Some modifications have been made to the mechanisms inherited from DCCP so as to let the proposed protocol can efficiently operate under a large size window. In addition, an enhanced protocol, EEFAST DCCP, using the measurements of one-way delay to dynamically adjust the window size is also proposed to improve the throughput of FAST DCCP with the effect of reverse traffic. Simulation results show that FAST DCCP not only can satisfy the requirements of real-time data delivery, but also perform well in bandwidth utilization and fairness in high-speed networks. Meanwhile, EEFAST DCCP is able to effectively conquer the throughput degradation caused by the reverse traffic.
83|12||A local variance-controlled reversible data hiding method using prediction and histogram-shifting|The stego image quality produced by the histogram-shifting based reversible data hiding technique is high; however, it often suffers from lower embedding capacity compared to other types of reversible data hiding techniques. In 2009, Tsai et al. solved this problem by exploiting the similarity of neighboring pixels to construct a histogram of prediction errors; data embedding is done by shifting the error histogram. However, Tsai et al.’s method does not fully exploit the correlation of the neighboring pixels. In this paper, a set of basic pixels is employed to improve the prediction accuracy, thereby increasing the payload. To further improve the image quality, a threshold is used to select only low-variance blocks to join the embedding process. According to the experimental results, the proposed method provides a better or comparable stego image quality than Tsai et al.’s method and other existing reversible data hiding methods under the same payload.
83|12||Erratum to âMeans-ends and whole-part traceability analysis of safety requirementsâ [J. Syst. Software 83 (2010) 1612â1621]|
83|12||Corrigendum to âA modeling approach on the TelosB WSN platform power consumptionâ [J. Syst. Software 83 (2010) 1355â1363]|
83|2|http://www.sciencedirect.com/science/journal/01641212/83/2|Computer software and applications|
83|2||Fault localization through evaluation sequences|Predicate-based statistical fault-localization techniques find fault-relevant predicates in a program by contrasting the statistics of the evaluation results of individual predicates between failed runs and successful runs. While short-circuit evaluations may occur in program executions, treating predicates as atomic units ignores this fact, masking out various types of useful statistics on dynamic program behavior. In this paper, we differentiate the short-circuit evaluations of individual predicates on individual program statements, producing one set of evaluation sequences per predicate. We then investigate experimentally the effectiveness of using these sequences to locate faults by comparing existing predicate-based techniques with and without such differentiation. We use both the Siemens program suite and four real-life UNIX utility programs as our subjects. The experimental results show that the proposed use of short-circuit evaluations can, on average, improve predicate-based statistical fault-localization techniques while incurring relatively small performance overhead.
83|2||A family of code coverage-based heuristics for effective fault localization|Locating faults in a program can be very time-consuming and arduous, and therefore, there is an increased demand for automated techniques that can assist in the fault localization process. In this paper a code coverage-based method with a family of heuristics is proposed in order to prioritize suspicious code according to its likelihood of containing program bugs. Highly suspicious code (i.e., code that is more likely to contain a bug) should be examined before code that is relatively less suspicious; and in this manner programmers can identify and repair faulty code more efficiently and effectively. We also address two important issues: first, how can each additional failed test case aid in locating program faults; and second, how can each additional successful test case help in locating program faults. We propose that with respect to a piece of code, the contribution of the first failed test case that executes it in computing its likelihood of containing a bug is larger than or equal to that of the second failed test case that executes it, which in turn is larger than or equal to that of the third failed test case that executes it, and so on. This principle is also applied to the contribution provided by successful test cases that execute the piece of code. A tool, ÏDebug, was implemented to automate the computation of the suspiciousness of the code and the subsequent prioritization of suspicious code for locating program faults. To validate our method case studies were performed on six sets of programs: Siemens suite, Unix suite, space, grep, gzip, and make. Data collected from the studies are supportive of the above claim and also suggest Heuristics III(a), (b) and (c) of our method can effectively reduce the effort spent on fault localization.
83|2||Formal specification of the variants and behavioural features of design patterns|The formal specification of design patterns is widely recognized as being vital to their effective and correct use in software development. It can clarify the concepts underlying patterns, eliminate ambiguity and thereby lay a solid foundation for tool support. This paper further advances a formal meta-modeling approach that uses first-order predicate logic to specify design patterns. In particular, it specifies both structural and behavioural features of design patterns and systematically captures the variants in a well-structured format. The paper reports a case study involving the formal specification of all 23 patterns in the Gang of Four catalog. It demonstrates that the approach improves the accuracy of pattern specifications by covering variations and clarifying the ambiguous parts of informal descriptions.
83|2||Measuring behavioral dependency for improving change-proneness prediction in UML-based design models|Several studies have explored the relationship between the metrics of the object-oriented software and the change-proneness of the classes. This knowledge can be used to help decision-making among design alternatives or assess software quality such as maintainability. Despite the increasing use of complex inheritance relationships and polymorphism in object-oriented software, there has been less emphasis on developing metrics that capture the aspect of dynamic behavior. Considering dynamic behavior metrics in conjunction with existing metrics may go a long way toward obtaining more accurate predictions of change-proneness. To address this need, we provide the behavioral dependency measure using structural and behavioral information taken from UML 2.0 design models. Model-based change-proneness prediction helps to make high-quality software by exploiting design models from the earlier phase of the software development process. The behavioral dependency measure has been evaluated on a multi-version medium size open-source project called JFlex. The results obtained show that the proposed measure is a useful indicator and can be complementary to existing object-oriented metrics for improving the accuracy of change-proneness prediction when the system contains high degree of inheritance relationships and polymorphism.
83|2||Embedded architecture description language|In the state-of-the-art hardware/software (HW/SW) co-design of embedded systems, there is a lack of sufficient support for architectural specifications across HW/SW boundaries. Such an architectural specification ought to capture both hardware and software components and their interactions, and facilitate effective design exploitation of HW/SW trade-offs and scalable HW/SW co-verification. In this paper, we present the embedded architecture description language (EADL). EADL is based on a component model for embedded systems that unifies hardware and software components. EADL does not dictate execution and interface semantics of hardware and software components while supporting flexible platform-oriented semantics instantiation. EADL supports concise representation of embedded system architectures and also formulation of architectural patterns of embedded systems. Besides facilitating design reuse, architectural patterns also facilitate verification reuse via association of property templates with these patterns. Effectiveness of EADL has been demonstrated by its successful application in integrating component-based co-design, co-simulation, co-verification, and co-synthesis.
83|2||Design, analysis, and deployment of omnipresent Formal Trust Model (FTM) with trust bootstrapping for pervasive environments|The rapid decrease in the size of mobile devices, coupled with an increase in capability, has enabled a swift proliferation of small and very capable devices into our daily lives. With such a prevalence of pervasive computing, the interaction among portable devices needs to be continuous and invisible to device users. As these devices become better connected, collaboration among them will play a vital role in sharing resources in an ad-hoc manner. The sharing of resources works as a facilitator for pervasive devices. However, this ad hoc interaction among devices provides the potential for security breaches. Trust can fight against such security violations by restricting malicious nodes from participating in interactions. Therefore, we need a unified trust relationship model between entities, which captures both the needs of the traditional computing world and the world of pervasive computing where the continuum of trust is based on identity, physical context or a combination of both. Here, we present a context specific and reputation-based trust model along with a brief survey of trust models suitable for peer-to-peer and ad-hoc environments. This paper presents a multi-hop recommendation protocol and a flexible behavioral model to handle interactions. One other contribution of this paper is the integration of an initial trust model; this model categorizes services or contexts in different security levels based on their security needs, and these security needs are considered in trust bootstrapping. The other major contribution of this paper is a simple method of handling malicious recommendations. This paper also illustrates the implementation and evaluation of our proposed formal trust model.
83|2||EDGES: Efficient data gathering in sensor networks using temporal and spatial correlations|In this paper, we present an approximate data gathering technique, called EDGES, for sensor networks that utilizes temporal and spatial correlations. The goal of EDGES is to efficiently obtain the sensor reading within a certain error bound. To do this, EDGES utilizes the multiple model Kalman filter, which is for the non-linear data distribution, as an approximation approach. The use of the Kalman filter allows EDGES to predict the future value using a single previous sensor reading in contrast to the other statistical models such as the linear regression and multivariate Gaussian. In order to extend the lifetime of networks, EDGES utilizes the spatial correlation. In EDGES, we group spatially close sensors as a cluster. Since a cluster header in a network acts as a sensor and router, a cluster header wastes its energy severely to send its own reading and/or data coming from its children. Thus, we devise a redistribution method which distributes the energy consumption of a cluster header using the spatial correlation. In some previous works, the fixed routing topology is used or the roles of nodes are decided at the base station and this information propagates through the whole network. But, in EDGES, the change of a cluster is notified to a small portion of the network. Our experimental results over randomly generated sensor networks with synthetic and real data sets demonstrate the efficiency of EDGES.
83|2||Verification and validation of declarative model-to-model transformations through invariants|In this paper we propose a method to derive OCL invariants from declarative model-to-model transformations in order to enable their verification and analysis. For this purpose we have defined a number of invariant-based verification properties which provide increasing degrees of confidence about transformation correctness, such as whether a rule (or the whole transformation) is satisfiable by some model, executable or total. We also provide some heuristics for generating meaningful scenarios that can be used to semi-automatically validate the transformations.
83|2||Class movement and re-location: An empirical study of Java inheritance evolution|Inheritance is a fundamental feature of the Object-Oriented (OO) paradigm. It is used to promote extensibility and reuse in OO systems. Understanding how systems evolve, and specifically, trends in the movement and re-location of classes in OO hierarchies can help us understand and predict future maintenance effort. In this paper, we explore how and where new classes were added as well as where existing classes were deleted or moved across inheritance hierarchies from multiple versions of four Java systems. We observed first, that in one of the studied systems the same set of classes was continuously moved across the inheritance hierarchy. Second, in the same system, the most frequent changes were restricted to just one sub-part of the overall system. Third, that a maximum of three levels may be a threshold when using inheritance in a system; beyond this level very little activity was observed, supporting earlier theories that, beyond three levels, complexity becomes overwhelming. We also found evidence of ‘collapsing’ hierarchies to bring classes up to shallower levels. Finally, we found that larger classes and highly coupled classes were more frequently moved than smaller and less coupled classes. Statistical evidence supported the view that larger classes and highly coupled classes were less cohesive than smaller classes and lowly coupled classes and were thus more suitable candidates for being moved (within an hierarchy).
83|2||A cocktail protocol with the Authentication and Key Agreement on the UMTS|At present, the Universal Mobile Telecommunications System (UMTS) is very popular in most parts of the world. It is a third-generation mobile communication technique known for its ability to conduct user authentication and for its security of communication with the use of Authentication and Key Agreement (AKA) protocol. A mobile station (MS), a service network (SN) and a home environment (HE) use the protocol to authenticate each other and make an agreement with a session key. With the UMTS-AKA protocol standard, all authentication vectors (AV) produced by the HE are transferred to the SN for mutual authentication with the MS. In this scenario, authentication is exposed to two kinds of defects. One defect is computational overhead concentrating on the HE and the other is the communication overhead for delivering the AVs. To overcome these congenital defects, this study proposes a unique UMTS-AKA protocol called the cocktail-AKA protocol. The goal of this protocol is to allow the SN to share some medicated authentication vectors (MAV) that are calculated in advance and combined with a prescription at the authentication stage. So, the HE only needs to produce a prescription authentication vector (PAV). Once the authentication stage is initiated, the SN distributes MAV and PAV and produces an effective AV for mutual authentication with the MS. The cocktail-AKA protocol can overcome both the aforesaid defects.
83|2||An evaluation of timed scenario notations|There is a general consensus on the importance of good Requirements Engineering (RE) for achieving high quality software. The modeling and analysis of requirements have been the main challenges during the development of complex systems. Although semi-formal, scenario driven approaches have raised the awareness and use of requirement engineering techniques, mostly because of their intuitive representation. Scenarios are a well established approach to describe functional requirements, uncovering hidden requirements and trade-offs, as well as validating and verifying requirements.
83|3|http://www.sciencedirect.com/science/journal/01641212/83/3|Reviewers are a sparse and precious resource|
83|3||A comparative study of architecture knowledge management tools|Recent research suggests that architectural knowledge, such as design decisions, is important and should be recorded alongside the architecture description. Different approaches have emerged to support such architectural knowledge (AK) management activities. However, there are different notions of and emphasis on what and how architectural activities should be supported. This is reflected in the design and implementation of existing AK tools. To understand the current status of software architecture knowledge engineering and future research trends, this paper compares five architectural knowledge management tools and the support they provide in the architecture life-cycle. The comparison is based on an evaluation framework defined by a set of 10 criteria. The results of the comparison provide insights into the current focus of architectural knowledge management support, their advantages, deficiencies, and conformance to the current architectural description standard. Based on the outcome of this comparison a research agenda is proposed for future work on AK tools.
83|3||Timed Property Sequence Chart|Property Sequence Chart (PSC) is a novel scenario-based notation, which has been recently proposed to represent temporal properties of concurrent systems. This language balances expressive power and simplicity of use. However, the current version of PSC just represents the order of events and lacks the ability to express timing properties. In real-time systems, it is well known that these timing requirements are very important and need to be specified clearly. Thus, in this paper, we define timed PSC (TPSC) and give the semantics of TPSC in terms of Timed Büchi Automaton (TBA). Then, we measure the expressive power of TPSC based on the recently proposed real-time specification patterns. Finally, we illustrate the use of TPSC in the context of a web service application which requires timing requirements.
83|3||Identification of refactoring opportunities introducing polymorphism|
83|3||Summary queries for frequent itemsets mining|There are many advanced techniques that can efficiently mine frequent itemsets using a minimum-support. However, the question that remains unanswered is whether the minimum-support can really help decision makers to make decisions. In this paper, we study four summary queries for frequent itemsets mining, namely, (1) finding a support-average of itemsets, (2) finding a support-quantile of itemsets, (3) finding the number of itemsets that greater/less than the support-average, i.e., an approximated distribution of itemsets, and (4) finding the relative frequency of an itemset (compared its frequency with that of other itemsets in the same dataset). With these queries, a decision maker will know whether an itemset in question is greater/less than the support-quantile; the distribution of itemsets; and the frequentness of an itemset. Processing these summary queries is challenging, because the minimum-support constraint cannot be used to prune infrequent itemsets. In this paper, we propose several simple yet effective approximation solutions. We conduct extensive experiments for evaluating our strategy, and illustrate that the proposed approaches can well model and capture the statistical parameters (summary queries) of itemsets in a database.
83|3||P/S-CoM: Building correct by design Publish/Subscribe architectural styles with safe reconfiguration|We present P/S-CoM, a formal approach supporting the correct modeling of Publish/Subscribe architectural styles and safe reconfiguration of dynamic architectures for event-based communication. We elaborate a set of patterns and we define the corresponding composition rules to build correct by design Publish/Subscribe styles. The defined patterns and rules respect the principle of information dissemination guaranteeing that the produced information reaches all the subscribed consumers. The patterns are modeled as graphs and the semantics of each pattern and each rule is specified formally in Z notations. We implement these specifications under the Z-Eves theorem prover which we use to prove specification consistency. The Z specification of the designed architectural style is also built by composition by applying the composition rules coded in Z. We consider the interconnection topology between event dispatchers as well as the subscription model using elementary refinements of the style specification. Moreover, we model the reconfiguration of Publish/Subscribe architecture via guarded graph-rewriting rules whose body specifies the structural constraints and whose guards define the pre- and post-conditions ensuring in this way the preservation of stylistic constraints. Similarly, we interpret reconfiguration rules in Z notations, and we implement these rules under Z-Eves for proving that all reconfigurations are style preserving. This results in a unified formal approach which handles both the static and the dynamic aspects of Publish/Subscribe software architectures.
83|3||A fast and progressive algorithm for skyline queries with totally- and partially-ordered domains|We devise a skyline algorithm that can efficiently mitigate the enormous overhead of processing millions of tuples on totally- and partially-ordered domains (henceforth, TODs and PODs). With massive datasets, existing techniques spend a significant amount of time on a dominance comparison because of both a large number of skyline points and the unprogressive method of skyline computing with PODs. (If data has high dimensionality, the situation is undoubtedly aggravated.) The progressiveness property turns out to be the key feature for solving all remaining problems. This article presents a FAST-SKY algorithm that deals successfully with these two obstacles and improves skyline query processing time strikingly, even with high-dimensional data. Progressive skyline evaluation with PODs is guaranteed by new index structures and topological sorting order. A stratification technique is adopted to index data on PODs, and we propose two new index structures: stratified R-trees (SR-trees) for low-dimensional data and stratified MinMax treaps (SM-treaps) for high-dimensional data. A fast dominance comparison is achieved by using a reporting query instead of a dominance query, and a dimensionality reduction technique. Experimental results suggest that in general cases (anti-correlated and uniform distributions) FAST-SKY is orders of magnitude faster than existing algorithms.
83|3||SALSA: QoS-aware load balancing for autonomous service brokering|The evolution towards “Software as a Service”, facilitated by various web service technologies, has led to applications composed of a number of service building blocks. These applications are dynamically composed by web service brokers, but rely critically on proper functioning of each of the composing subparts which is not entirely under control of the applications themselves. The problem at hand for the provider of the service is to guarantee non-functional requirements such as service access and performance to each customer. To this end, the service provider typically divides the load of incoming service requests across the available server infrastructure. In this paper we describe an adaptive load balancing strategy called SALSA (Simulated Annealing Load Spreading Algorithm), which is able to guarantee for different customer priorities, such as default and premium customers, that the services are handled in a given time and this without the need to adapt the servers executing the service logic themselves. It will be shown that by using SALSA, web service brokers are able to autonomously meet SLAs, without a priori over-dimensioning resources. This will be done by taking into account a real time view of the requests by measuring the Poisson arrival rates at that moment and selectively drop some requests from default customers. This way the web servers’ load is reduced in order to guarantee the service time for premium customers and provide best effort to default customers. We compared the results of SALSA with weighted round-robin (WRR), nowadays the most used load balancing strategy, and it was shown that the SALSA algorithm requires slightly more processing than WRR but is able to offer guarantees – contrary to WRR – by dynamically adapting its load balancing strategy.
83|3||Survivable ATM mesh networks: Techniques and performance evaluation|The Capacity and Flow Assignment problem in self-healing ATM networks is an interesting one from a Generalized Multi-Protocol Label Switching (GMPLS) prospective since IP and ATM protocols are destined to co-exist together in this unified platform. This paper continues the investigation of the path-based design approach of the network survivability problem in existing ATM mesh networks. Our contribution consists in quantifying (1) the effects the selection of candidate paths per node pair has on the restoration ratio, (2) the effect of restoration schemes on the restoration ratio, (3) the effect of failure scenarios on the restoration ratio, and finally (4) the effect of network connectivity on the restoration ratio. Numerical results are presented under representative network topologies, various traffic demands and spare capacity distribution schemes. They provide additional guidelines for the design of survivable ATM mesh-type networks, from a network reliability viewpoint.
83|3||A modern approach to multiagent development|Multiagent systems (MAS) development frameworks aim at facilitating the development and administration of agent-based applications. Currently relevant tools, such as JADE, offer huge possibilities but they are generally linked to a specific technology (commonly Java). This fact may limit some application domains when deploying MAS, such as low efficiency or programming language restrictions. To contribute to the evolution of multiagent development tools and to overcome these constraints, we introduce a multiagent platform based on the FIPA standards and built on top of a modern object-oriented middleware. Experimental results prove the scalability and the short response-time of the proposal and justify the design and development of modern tools to contribute the multiagent technology.
83|3||The Linux kernel as a case study in software evolution|We use 810 versions of the Linux kernel, released over a period of 14 years, to characterize the system’s evolution, using Lehman’s laws of software evolution as a basis. We investigate different possible interpretations of these laws, as reflected by different metrics that can be used to quantify them. For example, system growth has traditionally been quantified using lines of code or number of functions, but functional growth of an operating system like Linux can also be quantified using the number of system calls. In addition we use the availability of the source code to track metrics, such as McCabe’s cyclomatic complexity, that have not been tracked across so many versions previously. We find that the data supports several of Lehman’s laws, mainly those concerned with growth and with the stability of the process. We also make some novel observations, e.g. that the average complexity of functions is decreasing with time, but this is mainly due to the addition of many small functions.
83|3||Modeling and managing the variability of Web service-based systems|Web service-based systems are built orchestrating loosely coupled, standardized, and internetworked programs. If on the one hand, Web services address the interoperability issues of modern information systems, on the other hand, they enable the development of software systems on the basis of reuse, greatly limiting the necessity for reimplementation. Techniques and methodologies to gain the maximum from this emerging computing paradigm are in great need. In particular, a way to explicitly model and manage variability would greatly facilitate the creation and customization of Web service-based systems. By variability we mean the ability of a software system to be extended, changed, customized or configured for use in a specific context.
83|3||Progressive sharing for a secret image|Based on the wavelet transform, a new progressive sharing scheme is proposed to share a secret image into several shadow images using SPIHT encoding processes and Shamir’s threshold scheme. Quality refinement of the recovered image is achieved by the data consumed from the threshold number (r) of shadow images and each single shadow image reveals no information about the secret image. The size of each shadow image is smaller than 1/r of the secret image and any number of shadow images that is less than r reveals no information about the secret image. The proposed approach is secure for image sharing and provides excellent peak signal-to-noise ratio (PSNR) versus rate performance. Experimental results have demonstrated the promising performance of this method in progressive sharing.
83|3||Thank you_list of reviewers|
83|4|http://www.sciencedirect.com/science/journal/01641212/83/4|Maintaining and checking parity in highly available Scalable Distributed Data Structures|Access to data stored in distributed main memory is much faster than access to local disks. Highly available, Scalable Distributed Data Structures (SDDS) utilize this fast access. They counteract the effects of failed or unavailable nodes by storing data redundantly. Since main memory per node is limited, they generate this redundancy by storing parity data calculated with erasure correcting codes instead of using replication. We present here a way to maintain parity that is about 10 times faster than using the traditional 2PC scheme. We also present a scheme that can diagnose a mismatch between parity and user data with very little network traffic.
83|4||A practical distinguisher for the Shannon cipher|In this paper, we present a practical linear distinguisher on the Shannon stream cipher. Shannon is a synchronous stream cipher that uses at most 256-bit secret key. In the specification for Shannon, designers state that the intention of the design is to make sure that there are no distinguishing attacks on Shannon requiring less than 280 keystream words and less than 2128 computations. In this work we use the Crossword Puzzle attack technique to construct a distinguisher which requires a keystream of length about 231 words with workload about 231.
83|4||ATTEST: ATTributes-based Extendable STorage|The total amount of stored information on disks has increased tremendously in recent years. With storage devices getting cheaper and government agencies requiring strict data retention policies, it is clear that this trend will continue for several years to come. This progression creates a challenge for system administrators who must determine several aspects of storage policy with respect to provisioning, backups, retention, redundancy, security, performance, etc. These decisions are made for an entire file system, logical volume, or storage pool. However, this granularity is too large and can sacrifice storage efficiency and performance – particularly since different files have different requirements. In this paper, we advocate that storage policy decisions be made on a finer granularity. We describe ATTEST, an extendable stackable storage architecture, that allows storage policy decisions to be made at a file granularity and at all levels of the storage stack through the use of attributes that enable plugin policy modules and application aware storage functionality. We present an implementation of ATTEST that shows minimal impact on overall performance.
83|4||Agent-oriented software patterns for rapid and affordable robot programming|Robotic systems are often quite complex to develop; they are huge, heavily constrained from the non-functional point of view and they implement challenging algorithms. The lack of integrated methods with reuse approaches leads robotic developers to reinvent the wheel each time a new project starts. This paper proposes to reuse the experience done when building robotic applications, by catching it into design patterns. These represent a general mean for (i) reusing proved solutions increasing the final quality, (ii) communicating the knowledge about a domain and (iii) reducing the development time and effort. Despite of this generality, the proposed repository of patterns is specific for multi-agent robotic systems. These patterns are documented by a set of design diagrams and the corresponding implementing code is obtained through a series of automatic transformations. Some patterns extracted from an existing and freely available repository are presented. The paper also discusses an experimental set-up based on the construction of a complete robotic application obtained by composing some highly reusable patterns.
83|4||An empirical examination of application frameworks success based on technology acceptance model|Framework-based development is currently regarded as one of the most promising software development approaches when it comes to improvements in lead time, productivity and quality. However, many frameworks and projects based on frameworks still report failures, which indicate that there are problems related to both frameworks technology and frameworks usage. The objective of our research was to examine the major drivers that have an impact on a framework’s acceptance and a framework’s success. We used the technology acceptance model (TAM) and Seddon’s information systems success model as our underlying theory. Data collected from an online survey of 389 active framework users was used to test hypothesized models. Data analysis was performed via structural equation modeling. Our findings support the post-adoption version of TAM and the relationship between continuous use and the successful use of systems, where more use also means more net benefits. We found that the successful use of frameworks is mainly dependent on two factors: continuous framework usage intention and the perceived usefulness of the framework. Several practical and theoretical implications can be foreseen including advances in framework development guidelines and insight into the relationship between the acceptance and success of frameworks.
83|4||Power optimization for dynamic configuration in heterogeneous web server clusters|To reduce the environmental impact, it is essential to make data centers green, by turning off servers and tuning their speeds for the instantaneous load offered, that is, determining the dynamic configuration in web server clusters. We model the problem of selecting the servers that will be on and finding their speeds through mixed integer programming; we also show how to combine such solutions with control theory. For proof of concept, we implemented this dynamic configuration scheme in a web server cluster running Linux, with soft real-time requirements and QoS control, in order to guarantee both energy-efficiency and good user experience. In this paper, we show the performance of our scheme compared to other schemes, a comparison of a centralized and a distributed approach for QoS control, and a comparison of schemes for choosing speeds of servers.
83|4||Pseudo software: A mediating instrument for modeling software requirements|A new synthesis of software requirements models called pseudo software is proposed with the aim to cut requirements-related errors. Pseudo software achieves this aim by serving as a mediating instrument to empower stakeholders to participate in requirements elicitation and validation through model construction and manipulation, and to provide guidance to the development team to correctly interpret the requirements in the downstream development activities. Pseudo software obtains its traits as a mediating instrument through the choice of requirements information bits and the use of multimodal representations with tool support to integrate the requirements. Using historical data of fifty projects in the enterprise computing domain, pseudo software is shown to effectively cut the requirements-related errors committed by both the customer and the development team.
83|4||Redirection based recovery for MPLS network systems|To provide a reliable backbone network, fault tolerance should be considered in the network design. For a multiprotocol label switching (MPLS) based backbone network, the fault-tolerant issue focuses on how to protect the traffic of a label switched paths (LSP) against node and link failures. In IETF, two well-known recovery mechanisms (protection switching and rerouting) have been proposed. To further enhance the fault-tolerant performance of the two recovery mechanisms, the proposed approach utilizes the failure-free LSPs to transmit the traffic of the failed LSP (the affected traffic). To avoid affecting the original traffic of each failure-free LSP, the proposed approach applies the solution of the minimum cost flow to determine the amount of affected traffic to be transmitted by each failure-free LSP. For transmitting the affected traffic along a failure-free working LSP, IP tunneling technique is used. We also propose a permission token scheme to solve the packet disorder problem. Finally, simulation experiments are performed to show the effectiveness of the proposed approach.
83|4||Visual comparison of software cost estimation models by regression error characteristic analysis|The well-balanced management of a software project is a critical task accomplished at the early stages of the development process. Due to this requirement, a wide variety of prediction methods has been introduced in order to identify the best strategy for software cost estimation. The selection of the best technique is usually based on measures of error whereas in more recent studies researchers use formal statistical procedures. The former approach can lead to unstable and erroneous results due to the existence of outlying points whereas the latter cannot be easily presented to non-experts and has to be carried out by an expert with statistical background. In this paper, we introduce the regression error characteristic (REC) analysis, a powerful visualization tool with interesting geometrical properties, in order to validate and compare different prediction models easily, by a simple inspection of a graph. Moreover, we propose a formal framework covering different aspects of the estimation process such as the calibration of the prediction methodology, the identification of factors that affect the error, the investigation of errors on certain ranges of the actual cost and the examination of the distribution of the cost for certain errors. Application of REC analysis to the ISBSG10 dataset for comparing estimation by analogy and linear regression illustrates the benefits and the significant information obtained.
83|4||RO-cash: An efficient and practical recoverable pre-paid offline e-cash scheme using bilinear pairings|Nowadays, various electronic commerce services can be found and are widely used in the Internet due to the mature of information and network technologies. A practical and flexible electronic payment system is one of the key success factors in electronic commerce. Most of the pre-paid anonymous e-cash schemes do not provide lost money recovery. Also, the communication and computation cost of most existing anonymous payment schemes is still high. In this paper, we propose RO-cash, a practical and flexible recoverable pre-paid offline e-cash scheme using bilinear pairing. In RO-cash, we use bilinear pairings in elliptic curves to reduce the computation and communication cost. By distributing the power of a single bank authority to the bank manager and the auditor, the untraceability can be preserved even if the bank manager is not honest in lost-coin recovery. Since our proposed scheme is an offline scheme, it is suitable for the real world environment in which the network connection between a shop and the bank server may not be available during the payment phase. Also, since our proposed scheme is a general scheme, other e-cash schemes with nice properties, e.g. D-cash, PayWord, AOMPS, ownership-claimable e-cash, scheduled e-cash, etc., can be directly applied to our scheme without changing the underlying system structure. This will make our scheme more flexible for attracting customers to use different value-added services.
83|4||Design and analysis of GUI test-case prioritization using weight-based methods|Testing the correctness of a GUI-based application is more complex than the conventional code-based application. In addition to testing the underlying codes of the GUI application, the space of possible combinations of events with a large GUI-input sequence also requires creating numerous test cases to confirm the adequacy of the GUI testing. Running all GUI test cases and then fixing all found bugs may be time-consuming and delaying the project completion. Hence, it is important to advance the test cases that uncover the most faults as fast as possible in the testing process. Test-case prioritization has been proposed and used in recent years because it can improve the rate of fault detection during the testing phase. However, few studies have discussed the problem of GUI test-case prioritization. In this paper, we propose a weighted-event flow graph for solving the non-weighted GUI test case and ranking GUI test cases based on weight scores. The weighted scores can either be ranked from high to low or be ordered by dynamic adjusted scores. Finally, three experiments are performed, and experimental results show that the adjusted-weight method can obtain a better fault-detection rate.
83|4||On the ability of complexity metrics to predict fault-prone classes in object-oriented systems|
83|4||Petri net modeling and deadlock analysis of parallel manufacturing processes with shared-resources|Multiple resource-sharing is a common situation in parallel and complex manufacturing processes and may lead to deadlock states. To alleviate this issue, this paper presents the method of modeling parallel processing flows, sharing limited number of resources, in flexible manufacturing systems (FMSs). A new class of Petri net called parallel process net with resources (PPNRs) is introduced for modeling such FMSs. PPNRs have the capacity to model the more complex resource-sharing among parallel manufacturing processes. Furthermore, this paper presents the simple diagnostic and remedial procedures for deadlocks in PPNRs. The proposed technique for deadlock detection and recovery is based on transition vectors which have the power of determining the structural aspects as well as the process flow condition in PPNRs. Moreover, the proposed technique for dealing with deadlocks is not a siphon-based thus the large-scale PPNRs for real-life FMSs can be tackled. Finally, the proposed method of modeling and deadlock analysis in the FMS having parallel processing is demonstrated by a practical example.
83|4||Using hybrid algorithm for Pareto efficient multi-objective test suite minimisation|Test suite minimisation techniques seek to reduce the effort required for regression testing by selecting a subset of test suites. In previous work, the problem has been considered as a single-objective optimisation problem. However, real world regression testing can be a complex process in which multiple testing criteria and constraints are involved. This paper presents the concept of Pareto efficiency for the test suite minimisation problem. The Pareto-efficient approach is inherently capable of dealing with multiple objectives, providing the decision maker with a group of solutions that are not dominated by each other. The paper illustrates the benefits of Pareto efficient multi-objective test suite minimisation with empirical studies of two and three objective formulations, in which multiple objectives such as coverage and past fault-detection history are considered. The paper utilises a hybrid, multi-objective genetic algorithm that combines the efficient approximation of the greedy approach with the capability of population based genetic algorithm to produce higher-quality Pareto fronts.
83|4||An improved impossible differential cryptanalysis of Zodiac|In this paper, we introduce a new impossible differential cryptanalysis of Zodiac that is considerably more effective than the one in the previous work (Hong et al., 2002). Using two new 13-round impossible differential characteristics and the early abort technique, this 3R-Attack breaks 128-bit key full-round Zodiac with complexity less than 271.3 encryptions, which is practical. This result is approximately 248 times better than what mentioned in the earlier work. Our result reveals depth of Zodiac’s weakness against impossible differential cryptanalysis due to its poor diffusion layer. We also obtain a tighter upper bound for time complexity.
83|4||Corrigendum to âA wireless sensor system for validation of real-time automatic calibration of groundwater transport modelsâ [J. Syst. Software 82 (2009) 1859â1868]|
83|5|http://www.sciencedirect.com/science/journal/01641212/83/5|Stability assessment of aspect-oriented software architectures: A quantitative study|Design of stable software architectures has increasingly been a deep challenge to software developers due to the high volatility of their concerns and respective design decisions. Architecture stability is the ability of the high-level design units to sustain their modularity properties and not succumb to modifications. Architectural aspects are new modularity units aimed at improving design stability through the modularization of otherwise crosscutting concerns. However, there is no empirical knowledge about the positive and negative influences of aspectual decompositions on architecture stability. This paper presents an exploratory analysis of the influence exerted by aspect-oriented composition mechanisms in the stability of architectural modules addressing typical crosscutting concerns, such as error handling and security. Our investigation encompassed a comparative analysis of aspectual and non-aspectual decompositions based on different architectural styles applied to an evolving multi-agent software architecture. In particular, we assessed various facets of components’ and compositions’ stability through such alternative designs of the same multi-agent system using conventional quantitative indicators. We have also investigated the key characteristics of aspectual decompositions that led to (in)stabilities being observed in the target architectural options. The evaluation focused upon a number of architecturally-relevant changes that are typically performed through real-life maintenance tasks.
83|5||A classification and comparison of model checking software architecture techniques|Software architecture specifications are used for many different purposes, such as documenting architectural decisions, predicting architectural qualities before the system is implemented, and guiding the design and coding process. In these contexts, assessing the architectural model as early as possible becomes a relevant challenge. Various analysis techniques have been proposed for testing, model checking, and evaluating performance based on architectural models. Among them, model checking is an exhaustive and automatic verification technique, used to verify whether an architectural specification conforms to expected properties. While model checking is being extensively applied to software architectures, little work has been done to comprehensively enumerate and classify these different techniques.
83|5||Fast convergence to network fairness|Most AQM algorithms, such as RED, assure fairness through randomness in congestion notification. However, randomness results in fair allocation of network resources only when time limitations are not considered. This is not compatible with the current Internet, where traffic oscillations are frequent and the demand for fair treatment is rather urgent, due to short duration of most applications. Given the short duration of most modern Internet applications, fast convergence to fairness is necessitated. In this paper, we use fairness as the major criterion to adjust traffic and present a corresponding algorithm of active queue management, which is called Explicit Global Congestion Notifier (EGCN). EGCN notifies flows almost simultaneously about incipient congestion by marking packets arriving at the router’s queue, when the load in the network increases and buffer overflow is expected. This is a new approach compared with the random notification policy of RED or ECN. EGCN distributes the burden to adjust backward to more flows and consequently allows for smoother window adjustments. We elaborate on the properties of system-wide response in terms of fairness, smoothness and efficiency. Simulation results demonstrate a clear-cut advantage of the proposed scheme.
83|5||Trapdoor security in a searchable public-key encryption scheme with a designated tester|We study a secure searchable public-key encryption scheme with a designated tester (dPEKS). The contributions of this paper are threefold. First, we enhance the existing security model to incorporate the realistic abilities of dPEKS attackers. Second, we introduce the concept of “trapdoor indistinguishability” and show that trapdoor indistinguishability is a sufficient condition for thwarting keyword-guessing attacks. This answers the open problem of how to construct PEKS (dPEKS) schemes that are provably secure against keyword-guessing attacks. Finally, we propose a dPEKS scheme that is secure in the enhanced security model. The scheme is the first dPEKS scheme that is secure against keyword-guessing attacks.
83|5||Compiler-assisted leakage-aware loop scheduling for embedded VLIW DSP processors|As feature size shrinks, leakage energy consumption has become an important concern. In this paper, we develop a compiler-assisted instruction-level scheduling technique to reduce leakage energy consumption for applications with loops on VLIW architecture. In the proposed technique, we obtain the schedule with minimum leakage energy from the ones that are generated by repeatedly regrouping a loop based on rotation scheduling and bipartite-matching. We conduct experiments on a set of benchmarks from DSPstone, Mediabench, Netbench, and MiBench based on the power model of the VLIW processors. The results show that our algorithm can achieve significant leakage energy saving compared with the previous work.
83|5||Defining and controlling the heterogeneity of a cluster: The Wrekavoc tool|The experimental validation and the testing of solutions that are designed for heterogeneous environments are challenging. We introduce Wrekavoc as an accurate tool for this purpose: it runs unmodified applications on emulated multi-site heterogeneous platforms. Its principal technique consists in downgrading the performance of the platform characteristics in a prescribed way. The platform characteristics include the compute nodes themselves (CPU and memory) and the interconnection network for which a controlled overlay network above the homogeneous cluster is built. In this article we describe the tool, its performance, its accuracy and its scalability. Results show that Wrekavoc is a very versatile tool that is useful to perform high-quality experiments (in terms of reproducibility, realism, control, etc.).
83|5||Measuring design complexity of semantic web ontologies|Ontology languages such as OWL are being widely used as the Semantic Web movement gains momentum. With the proliferation of the Semantic Web, more and more large-scale ontologies are being developed in real-world applications to represent and integrate knowledge and data. There is an increasing need for measuring the complexity of these ontologies in order for people to better understand, maintain, reuse and integrate them. In this paper, inspired by the concept of software metrics, we propose a suite of ontology metrics, at both the ontology-level and class-level, to measure the design complexity of ontologies. The proposed metrics are analytically evaluated against Weyuker’s criteria. We have also performed empirical analysis on public domain ontologies to show the characteristics and usefulness of the metrics. We point out possible applications of the proposed metrics to ontology quality control. We believe that the proposed metric suite is useful for managing ontology development projects.
83|5||A family of languages for architecture constraint specification|During software development, architecture decisions should be documented so that quality attributes guaranteed by these decisions and required in the software specification could be persisted. An important part of these architectural decisions is often formalized using constraint languages which differ from one stage to another in the development process. In this paper, we present a family of architectural constraint languages, called ACL. Each member of this family, called a profile, can be used to formalize architectural decisions at a given stage of the development process. An ACL profile is composed of a core constraint language, which is shared with the other profiles, and a MOF architecture metamodel. In addition to this family of languages, this paper introduces a transformation-based interpretation method of profiles and its associated tool.
83|5||A novel data hiding scheme based on modulus function|Four criteria are generally used to evaluate the performance of data hiding scheme: the embedding capacity, the visual quality of the stego-image, the security, and the complexity of the data-embedding algorithm. However, data hiding schemes seldom take all these factors into consideration. This paper proposes a novel data hiding scheme that uses a simple modulus function to address all the performance criteria listed above. According to the input secret keys, the encoder and decoder use the same set-generation functions Hr() and Hc() to first generate two sets Kr and Kc. A variant Cartesian product is then created using Kr and Kc. Each cover pixel then forms a pixel group with its neighboring pixels by exploiting an efficient modulus function; the secret data are then embedded or extracted via a mapping process between the variant of the Cartesian product and each pixel group. The proposed scheme offers several advantages, namely (1) the embedding capacity can be scaled, (2) a good visual quality of the stego-image can be achieved, (3) the computational cost of embedding or extracting the secret data is low and requires little memory space, (4) secret keys are used to protect the secret data and (5) the problem of overflow or underflow does not occur, regardless of the nature of the cover pixels.
83|5||Differential fault analysis on Camellia|Camellia is a 128-bit block cipher published by NTT and Mitsubishi in 2000. On the basis of the byte-oriented model and the differential analysis principle, we propose a differential fault attack on the Camellia algorithm. Mathematical analysis and simulating experiments show that our attack can recover its 128-bit, 192-bit or 256-bit secret key by introducing 30 faulty ciphertexts. Thus our result in this study describes that Camellia is vulnerable to differential fault analysis. This work provides a new reference to the fault analysis of other block ciphers.
83|5||A simple, least-time, and energy-efficient routing protocol with one-level data aggregation for wireless sensor networks|The area of wireless sensor networks (WSN) is currently attractive in the research community area due to its applications in diverse fields such as defense security, civilian applications and medical research. Routing is a serious issue in WSN due to the use of computationally-constrained and resource-constrained micro-sensors. These constraints prohibit the deployment of traditional routing protocols designed for other ad hoc wireless networks. Any routing protocol designed for use in WSN should be reliable, energy-efficient and should increase the lifetime of the network. We propose a simple, least-time, energy-efficient routing protocol with one-level data aggregation that ensures increased life time for the network. The proposed protocol was compared with popular ad hoc and sensor network routing protocols, viz., AODV (35 and 12), DSR (Johnson et al., 2001), DSDV (Perkins and Bhagwat, 1994), DD (Intanagonwiwat et al., 2000) and MCF (Ye et al., 2001). It was observed that the proposed protocol outperformed them in throughput, latency, average energy consumption and average network lifetime. The proposed protocol uses absolute time and node energy as the criteria for routing, this ensures reliability and congestion avoidance.
83|5||A novel user-participating authentication scheme|When utilizing services over public networks, a remote user authentication mechanism forms a first line of defense by rejecting illegal logins from unauthorized users. On-line applications over the Internet such as E-learning, on-line games, etc. are ever more common; remote user participation via networks plays a vital role in security and should be guaranteed. Without this countermeasure, malicious users are likely to enable agents to communicate with remote on-line systems. While existing remote user authentication schemes rarely address this issue, this paper highlights the problem of guaranteeing remote user participation. This proposed user authentication scheme benefits from combining CAPTCHA techniques and visual secret sharing to ensure deliberate human interaction. This scheme provides mutual authentication and is secure against certain known attacks, as well as low in computation cost.
83|5||A symbolic fault-prediction model based on multiobjective particle swarm optimization|In the literature the fault-proneness of classes or methods has been used to devise strategies for reducing testing costs and efforts. In general, fault-proneness is predicted through a set of design metrics and, most recently, by using Machine Learning (ML) techniques. However, some ML techniques cannot deal with unbalanced data, characteristic very common of the fault datasets and, their produced results are not easily interpreted by most programmers and testers. Considering these facts, this paper introduces a novel fault-prediction approach based on Multiobjective Particle Swarm Optimization (MOPSO). Exploring Pareto dominance concepts, the approach generates a model composed by rules with specific properties. These rules can be used as an unordered classifier, and because of this, they are more intuitive and comprehensible. Two experiments were accomplished, considering, respectively, fault-proneness of classes and methods. The results show interesting relationships between the studied metrics and fault prediction. In addition to this, the performance of the introduced MOPSO approach is compared with other ML algorithms by using several measures including the area under the ROC curve, which is a relevant criterion to deal with unbalanced data.
83|6|http://www.sciencedirect.com/science/journal/01641212/83/6|Special Issue on Software Architecture and Mobility|
83|6||Software architecture and mobility: A roadmap|Modern software-intensive systems are characterized not only by the movement of data, as has been the case in traditional distributed systems, but also by the movement of users, devices, and code. Developing effective, efficient, and dependable systems in the mobile setting is challenging. Existing architectural principles need to be adapted and novel architectural paradigms devised. In this paper, we give an overview of the intersection of the areas of software architecture and mobility. We consider mobility from two related perspectives: (1) mobile software, which represents the computing functionality designed to migrate across hardware devices at runtime and execute on mobile hardware platforms, and (2) mobile systems, which are computing applications that include mobile software and hardware elements. We study the advances in both these areas, highlight representative existing solutions, and identify several remaining research challenges.
83|6||Commentary on âSoftware architectures and mobility: A Roadmapâ|
83|6||Computer supported cooperative work and âSoftware architectures and mobility: A Roadmapâ|
83|6||Multi-layer faults in the architectures of mobile, context-aware adaptive applications|Modern hand-held devices are equipped with multiple context sensors exploited by increasingly sophisticated software applications, called Context-Aware Adaptive Applications (CAAAs), that adapt automatically to changes in the surrounding environment, such as by responding to the location and speed of the user. The architecture of CAAAs is typically layered and incorporates a context-awareness component to support processing of context values and triggering of adaptive changes. While this layered architecture is very natural for the design and implementation of CAAAs, it exhibits new kinds of failures that arise as a result of faults that are specific to the choice of technology for specific layers. In this paper we investigate the occurrence of such faults and failures that manifest across architectural layers, and we describe samples of such failures in four CAAAs.
83|6||Dealing with variability in context-aware mobile software|Mobile context-aware software pose a set of challenging requirements to developers as these applications exhibit novel features, such as handling varied sensing devices and dynamically adapting to the user’s context (e.g. his or her location), and evolve quickly according to technological advances.
83|6||Ambient-PRISMA: Ambients in mobile aspect-oriented software architecture|This work presents an approach called Ambient-PRISMA for modelling and developing distributed and mobile applications. Ambient-PRISMA enriches an aspect-oriented software architectural approach called PRISMA with the ambient concept from Ambient Calculus. Ambients are introduced in PRISMA as specialized kinds of connectors that offer mobility services to architectural elements (components and connectors) and are able to coordinate a boundary, which models the notion of location. Mobility of architectural elements is supported by reconfiguring the software architecture. This paper presents a metamodel that introduces ambients to design aspect-oriented software architectural models for mobile systems. The design of models is performed using an Aspect-Oriented Architecture Description Language. A middleware called Ambient-PRISMANET which maps the metamodel to .NET technology and supports the distributed runtime environment needed for executing mobile applications is also presented. In addition, a CASE Tool which allows users to specify the aspect-oriented architectural models in a graphical way and generate .NET code is provided. In this way, we explain how Ambient-PRISMA follows Model Driven Engineering. An example of an auction system is used throughout the article to illustrate the work.
83|6||Promoting the development of secure mobile agent applications|In this paper, we present a software architecture and a development environment for the implementation of applications based on secure mobile agents. Recent breakthroughs in mobile agent security have unblocked this technology, but there is still one important issue to overcome: the complexity of programming applications using these security solutions. Our proposal aims to facilitate and speed up the process of implementing cryptographic protocols, and to allow the reuse of these protocols for the development of secure mobile agents. As a result, the proposed architecture and development environment promote the use of mobile agent technology for the implementation of secure distributed applications.
83|6||An architecture-driven software mobility framework|Software architecture has been shown to provide an appropriate level of granularity for assessing a software system’s quality attributes (e.g., performance and dependability). Similarly, previous research has adopted an architecture-centric approach to reasoning about and managing the run-time adaptation of software systems. For mobile and pervasive software systems, which are known to be innately dynamic and unpredictable, the ability to assess a system’s quality attributes and manage its dynamic run-time behavior is especially important. In the past, researchers have argued that a software architecture-based approach can be instrumental in facilitating mobile computing. In this paper, we present an integrated architecture-driven framework for modeling, analysis, implementation, deployment, and run-time migration of software systems executing on distributed, mobile, heterogeneous computing platforms. In particular, we describe the framework’s support for dealing with the challenges posed by both logical and physical mobility. We also provide an overview of our experience with applying the framework to a family of distributed mobile robotics systems. This experience has verified our envisioned benefits of the approach, and has helped us to identify several avenues of future work.
83|6||A novel XML keyword query approach using entity subtree|Keyword query is an important means to find object information in XML document. Most of the existing keyword query approaches adopt the subtrees rooted at the smallest lowest common ancestors of the keyword matching nodes as the basic result units. The structural relationships among XML nodes are excessively emphasized but the semantic relevance is not fully exploited.
83|6||Scheduling multiple task graphs with end-to-end deadlines in distributed real-time systems utilizing imprecise computations|In order to meet the inherent need of real-time applications for high quality results within strict timing constraints, the employment of effective scheduling techniques is crucial in distributed real-time systems. In this paper, we evaluate by simulation the performance of strategies for the dynamic scheduling of composite jobs in a homogeneous distributed real-time system. Each job that arrives in the system is a directed acyclic graph of component tasks and has an end-to-end deadline. For each scheduling policy, we provide an alternative version which allows imprecise computations, taking into account the effects of input error on the processing time of the component tasks of a job. The simulation results show that the alternative versions of the algorithms outperform their respective counterparts. To our knowledge, an imprecise computations approach for the dynamic scheduling of multiple task graphs with end-to-end deadlines and input error has never been discussed in the literature before.
83|6||Robust lossless image watermarking based on Î±-trimmed mean algorithm and support vector machine|This paper presents a robust lossless watermarking technique, based on Î±-trimmed mean algorithm and support vector machine (SVM), for image authentication. SVM is trained to memorize relationship between the watermark and the image-dependent watermark other than embedding watermark into the host image. While needing to authenticate the ownership of the image, the trained SVM is used to recover the watermark and then the recovered watermark is compared with the original watermark to determine the ownership. Meanwhile, the robustness can be enhanced using Î±-trimmed mean operator against attacks. Experimental results demonstrate that the technique not only possesses the robustness to resist on image-manipulation attacks under consideration but also, in average, is superior to other existing methods being considered in the paper.
83|6||Efficient evaluation of query rewriting plan over materialized XML view|The query rewriting plan generation over XML views has received wide attention recently. However, little work has been done on efficient evaluation of the query rewriting plans, which is not trivial since the plan may contain an exponential size of sub-plans. This paper investigates the reason for the potentially exponential number of sub-plans, and then proposes a new space-efficient form called ABCPlan (Plan with Automata Based Combinations) to equivalently represent the original query rewriting plan. ABCPlan contains a set of buckets containing suffix paths in the query tree and an automata to indicate the combination of the suffix paths from different buckets as valid query rewriting sub-plans. We also design an evaluation method called ABCScan, which constructs a unified evaluation tree for the ABCPlan and handles the evaluation tree in one scan of the XML view. In the evaluation, we introduce node existence automata to encode the structure of the sub-tree and convert the satisfaction of the ABCPlan into the intersection problem of deterministic finite automata. The experiments show that ABCPlan based method outperforms existing methods significantly in terms of scalability and efficiency.
83|6||Selection of strategies in judgment-based effort estimation|We currently know little about the factors that motivate the selection and change of strategy in judgment-based effort estimation. A better understanding of these issues may lead to more accurate judgment-based effort estimates and motivates the four experiments reported in this paper. The experiments’ two main results are the identification of the importance of “estimation surprises” (large estimation errors) to motivate estimation strategy change and the large individual variation in the initial choice of estimation strategy. The individual variation seems not only to be a result of differences in previous experiences, but also a result of differences in the mental “accessibility” of the strategies. We found, for example, that the use of a type of strategy was increased when we instructed a developer to use the same type of strategy on unrelated tasks immediately before.
83|6||Software metadata: Systematic characterization of the memory behaviour of dynamic applications|Development of new embedded systems requires tuning of the software applications to specific hardware blocks and platforms as well as to the relevant input data instances. The behaviour of these applications heavily relies on the nature of the input data samples, thus making them strongly data-dependent. For this reason, it is necessary to extensively profile them with representative samples of the actual input data. An important aspect of this profiling is done at the dynamic data type level, which actually steers the designers choice of implementation of these data types. The behaviour of the applications is then characterized, through an analysis phase, as a collection of software metadata that can be used to optimize the system as a whole. In this paper we propose to represent the behaviour of data-dependent applications to enable optimizations, rather than to analyze their structure or to define the engineering process behind them. Moreover, we specifically limit ourselves to the scope of applications dominated by dynamically allocated data types running on embedded systems. We characterize the software metadata that these optimizations require, and we present a methodology, as well as appropriate techniques, to obtain this information from the original application. The optimizations performed on a complete case study, utilizing the extracted software metadata, achieve overall improvements of up to 42% in the number of cycles spent accessing memory when compared to code optimized only with the static techniques applied by GNU G++.
83|7|http://www.sciencedirect.com/science/journal/01641212/83/7|Editorial for the JSS SPLC 2008 Special Issue|
83|7||Automating the construction of domain-specific modeling languages for object-oriented frameworks|The extension of frameworks with domain-specific modeling languages (DSML) has proved to be an effective way of improving the productivity in software product-line engineering. However, developing and evolving a DSML is typically a difficult and time-consuming task because it requires to develop and maintain a code generator, which transforms application models into framework-based code. In this paper, we propose a new approach for extending object-oriented frameworks that aims to alleviate this problem. The approach is based on developing an additional aspect-oriented layer that encodes a DSML for building framework-based applications, eliminating the need of implementing a code generator. We further show how a language workbench is capable of automating the construction of DSMLs using the proposed layer.
83|7||Automated diagnosis of feature model configurations|Software product-lines (SPLs) are software platforms that can be readily reconfigured for different project requirements. A key part of an SPL is a model that captures the rules for reconfiguring the software. SPLs commonly use feature models to capture SPL configuration rules. Each SPL configuration is represented as a selection of features from the feature model. Invalid SPL configurations can be created due to feature conflicts introduced via staged or parallel configuration or changes to the constraints in a feature model. When invalid configurations are created, a method is needed to automate the diagnosis of the errors and repair the feature selections.
83|7||Structuring the modeling space and supporting evolution in software product line engineering|The scale and complexity of product lines means that it is practically infeasible to develop a single model of the entire system, regardless of the languages or notations used. The dynamic nature of real-world systems means that product line models need to evolve continuously to meet new customer requirements and to reflect changes of product line artifacts. To address these challenges, product line engineers need to apply different strategies for structuring the modeling space to ease the creation and maintenance of models. This paper presents an approach that aims at reducing the maintenance effort by organizing product lines as a set of interrelated model fragments defining the variability of particular parts of the system. We provide support to semi-automatically merge fragments into complete product line models. We also provide support to automatically detect inconsistencies between product line artifacts and the models representing these artifacts after changes. Furthermore, our approach supports the co-evolution of models and their respective meta-models. We discuss strategies for structuring the modeling space and show the usefulness of our approach using real-world examples from our ongoing industry collaboration.
83|7||A feature-oriented approach for developing reusable product line assets of service-based systems|Service orientation (SO) is a relevant promising candidate for accommodating rapidly changing user needs and expectations. One of the goals of adopting SO is the improvement of reusability, however, the development of service-based system in practice has uncovered several challenging issues, such as how to identify reusable services, how to determine configurations of services that are relevant to users’ current product configuration and context, and how to maintain service validity after configuration changes. In this paper, we propose a method that addresses these issues by adapting a feature-oriented product line engineering approach. The method is notable in that it guides developers to identify reusable services at the right level of granularity and to map users’ context to relevant service configuration, and it also provides a means to check the validity of services at runtime in terms of invariants and pre/post-conditions of services. Moreover, we propose a heterogeneous style based architecture model for developing such systems.
83|7||Handling over-fitting in test cost-sensitive decision tree learning by feature selection, smoothing and pruning|Cost-sensitive learning algorithms are typically designed for minimizing the total cost when multiple costs are taken into account. Like other learning algorithms, cost-sensitive learning algorithms must face a significant challenge, over-fitting, in an applied context of cost-sensitive learning. Specifically speaking, they can generate good results on training data but normally do not produce an optimal model when applied to unseen data in real world applications. It is called data over-fitting. This paper deals with the issue of data over-fitting by designing three simple and efficient strategies, feature selection, smoothing and threshold pruning, against the TCSDT (test cost-sensitive decision tree) method. The feature selection approach is used to pre-process the data set before applying the TCSDT algorithm. The smoothing and threshold pruning are used in a TCSDT algorithm before calculating the class probability estimate for each decision tree leaf. To evaluate our approaches, we conduct extensive experiments on the selected UCI data sets across different cost ratios, and on a real world data set, KDD-98 with real misclassification cost. The experimental results show that our algorithms outperform both the original TCSDT and other competing algorithms on reducing data over-fitting.
83|7||A service-based architecture for dynamically reconfigurable workflows|In the last few years, business process management systems have been employed for handling information systems of ever increasing complexity. As a consequence, the adoption of modelling languages enabling smooth and seamless transitions among the various phases of the process lifecycle, the ability of exploiting coordination schema over distributed execution contexts and the support for dynamic evolution and reconfiguration have become software engineering issues of great importance. This paper proposes the use of PN-Engine, a decentralized Petri nets execution engine, as a business process enactment engine. PN-Engine, which is based on the Jini service architecture, supports the decentralized execution of process models specified as Petri nets (PNs) enhanced with modular constructs and offers suitable mechanisms for dealing with the aforementioned design issues. PN-Engine allows to deploy and enact a new version of an existing process model without requiring the stopping/removal of older instances that are still running. The paper presents a novel approach enabling a decentralized migration procedure where concurrent portions of older instances migrate asynchronously to the new process model. Advantages of the proposed approach are demonstrated by means of an example concerning a workflow for a wine-production process.
83|7||A catalogue of component connectors to support development with reuse|Component-based development is based on the idea of building software systems by composing pre-existing components. Connectors are the ‘glue’ for composing components. Therefore, it is important to consider connectors as first-class entities and provide adequate descriptions of them to facilitate their understanding and promote their reuse. We have defined a catalogue of component connectors to support the process of ‘development with reuse’. The categories and connector types in the catalogue were obtained through an analysis of the activities involved in this process as well as considering the syntax and semantics of a new component model.
83|7||TALISMAN MDE: Mixing MDE principles|The Model-Driven Engineering approach is progressively gaining popularity in the software engineering community as it raises the level of abstraction in software development. In TALISMAN MDE framework, we combine the principles of the two most important initiatives, Model-Driven Architecture and Software Factories. Both have their pros and cons, and we select the best from each in TALISMAN MDE. To show the advantages of TALISMAN MDE, we have developed a systems generator and used it to create applications for controlling food traceability. The applications are being used in dairies with different manufacturing processes, using software developed specifically for each dairy by working only with models, without additional programming.
83|7||Robust fuzzy CPU utilization control for dynamic workloads|In a number of real-time applications such as target tracking, precise workloads are unknown a priori but may dynamically vary, for example, based on the changing number of targets to track. It is important to manage the CPU utilization, via feedback control, to avoid severe overload or underutilization even in the presence of dynamic workloads. However, it is challenge to model a real-time system for feedback control, as computer systems cannot be modeled via physics laws. In this paper, we present a novel closed-loop approach for utilization control based on formal fuzzy logic control theory, which is very effective to support the desired performance in a nonlinear dynamic system without requiring a system model. We mathematically prove the stability of the fuzzy closed-loop system. Further, in a real-time kernel, we implement and evaluate our fuzzy logic utilization controller as well as two existing utilization controllers based on the linear and model predictive control theory for an extensive set of workloads. Our approach supports the specified average utilization set-point, while showing the best transient performance in terms of utilization control among the tested approaches.
83|7||Agile monitoring using the line of balance|There is a need to collect, measure, and present progress information in all projects, and Agile projects are no exception. In this article, the authors show how the line of balance, a relatively obscure indicator, can be used to gain insights into the progress of projects not provided by burn down charts or cumulative flow diagrams, two of the most common indicators used to track and report progress in Agile projects. The authors also propose to replace the original plan-based control point lead-time calculations with dynamic information extracted from a version control system and introduce the concept of the ideal plan to measure progress relative to both, end of iteration milestones and project completion date.
83|7||Analyzing architectural styles|The backbone of most software architectures and component integration frameworks is one or more architectural styles that provide a domain-specific design vocabulary and a set of constraints on how that vocabulary is used. Today’s architectural styles are increasingly complex, involving rich vocabularies and numerous constraints. Hence, designing a sound and appropriate style becomes an intellectually challenging activity. Unfortunately, although there are numerous tools to help in the analysis of architectures for individual systems, relatively less work has been done on tools to help in the design of architectural styles. In this paper we address this gap by showing how to map an architectural style, expressed formally in an architectural description language, into a relational model that can then be checked for properties, such as whether a style is consistent, whether a style satisfies some predicates over its architectural structure, and whether two styles are compatible for composition.
83|7||A novel adaptive steganography based on local complexity and human vision sensitivity|This paper presents a novel adaptive steganographic scheme that is capable of both preventing visual degradation and providing a large embedding capacity. The embedding capacity of each pixel is dynamically determined by the local complexity of the cover image, allowing us to maintain good visual quality as well as embedding a large amount of secret messages. We classify pixels into three levels based on the variance of the local complexity of the cover image. When determining which level of local complexity a pixel should belong to, we take human vision sensitivity into consideration. This ensures that the visual artifacts appeared in the stego image are imperceptible, and the difference between the original and stego image is indistinguishable by the human visual system. The pixel classification assures that the embedding capacity offered by a cover image is bounded by the embedding capacity imposed on three levels that are distinguished by two boundary thresholds values. This allows us to derive a combination ratio of the maximal embedding capacity encountered with at each level. Consequently, our scheme is capable of determining two threshold values according to the desired demand of the embedding capacity requested by the user. Experimental results demonstrated that our adaptive steganographic algorithm produces insignificant visual distortion due to the hidden message. It provides high embedding capacity superior to that offered by a number of existing schemes. Our algorithm can resist the RS steganalysis attack, and it is statistically invisible for the attack of histogram comparison. The proposed scheme is simple, efficient and feasible for adaptive steganographic applications.
83|7||Bioinformatics algorithm development for Grid environments|A Grid environment can be viewed as a virtual computing architecture that provides the ability to perform higher throughput computing by taking advantage of many computers geographically dispersed and connected by a network. Bioinformatics applications stand to gain in such a distributed environment in terms of increased availability, reliability and efficiency of computational resources. There is already considerable research in progress toward applying parallel computing techniques on bioinformatics methods, such as multiple sequence alignment, gene expression analysis and phylogenetic studies. In order to cope with the dimensionality issue, most machine learning methods either focus on specific groups of proteins or reduce the size of the original data set and/or the number of attributes involved. Grid computing could potentially provide an alternative solution to this problem, by combining multiple approaches in a seamless way. In this paper we introduce a unifying methodology coupling the strengths of the Grid with the specific needs and constraints of the major bioinformatics approaches. We also present a tool that implements this process and allows researchers to assess the computational needs for a specific task and optimize the allocation of available resources for its efficient completion.
83|7||The effect of testing location on usability testing performance, participant stress levels, and subjective testing experience|The effect of testing location on usability test elements such as stress levels and user experience is not clear. A comparison between traditional lab testing and synchronous remote testing was conducted. The present study investigated two groups of users in remote and traditional settings. Within each group participants completed two tasks, a simple task and a complex task. The dependent measures were task time taken, number of critical incidents reported, and user-reported anxiety score. Task times differed significantly between the physical location condition; this difference was not meaningful for real world application, and likely introduced by overhead regarding synchronous remote testing methods. Critical incident reporting counts did not differ in any condition. No significant differences were found in user reported stress levels. Subjective assessments of the study and interface also did not differ significantly. Study findings suggest a similar user testing experience exists for remote and traditional laboratory usability testing.
83|7||A weighted common structure based clustering technique for XML documents|XML has recently become very popular as a means of representing semistructured data and as a standard for data exchange over the Web, because of its varied applicability in numerous applications. Therefore, XML documents constitute an important data mining domain. In this paper, we propose a new method of XML document clustering by a global criterion function, considering the weight of common structures. Our approach initially extracts representative structures of frequent patterns from schemaless XML documents using a sequential pattern mining algorithm. Then, we perform clustering of an XML document by the weight of common structures, without a measure of pairwise similarity, assuming that an XML document is a transaction and frequent structures extracted from documents are items of the transaction. We conducted experiments to compare our method with previous methods. The experimental results show the effectiveness of our approach.
83|7||Software process improvement through the Lean Measurement (SPI-LEAM) method|Software process improvement methods help to continuously refine and adjust the software process to improve its performance (e.g., in terms of lead-time, quality of the software product, reduction of change requests, and so forth). Lean software development propagates two important principles that help process improvement, namely identification of waste in the process and considering interactions between the individual parts of the software process from an end-to-end perspective. A large shift of thinking about the own way of working is often required to adopt lean. One of the potential main sources of failure is to try to make a too large shift about the ways of working at once. Therefore, the change to lean has to be done in a continuous and incremental way. In response to this we propose a novel approach to bring together the quality improvement paradigm and lean software development practices, the approach being called Software Process Improvement through the Lean Measurement (SPI-LEAM) Method. The method allows to assess the performance of the development process and take continuous actions to arrive at a more lean software process over time. The method is under implementation in industry and an initial evaluation of the method has been performed.
83|7||A cusum change-point detection algorithm for non-stationary sequences with application to data network surveillance|We adapt the classic cusum change-point detection algorithm to handle non-stationary sequences that are typical with network surveillance applications. The proposed algorithm uses a defined timeslot structure to take into account time varying distributions, and uses historical samples of observations within each timeslot to facilitate a nonparametric methodology. Our proposed solution includes an on-line screening feature that fully automates the implementation of the algorithm and eliminates the need for manual oversight up until the point where root cause analysis begins.
83|8|http://www.sciencedirect.com/science/journal/01641212/83/8|Special issue: Performance evaluation and optimization of ubiquitous computing and networked systems|
83|8||Improving routing protocol performance in delay tolerant networks using extended information|Delay tolerant networks (DTNs) are wireless mobile networks that do not guarantee the existence of a path between a source and a destination at any time. When two nodes move within each other’s transmission range during a period of time, they can contact each other. The contact of nodes can be periodical, predictable and nonpredictable. In this paper, we assume the contact of nodes is nonpredictable so that it can reflect the most flexible way of nodes movement. Due to the uncertainty and time-varying nature of DTNs, routing poses special challenges. Some existing schemes use utility functions to steer the routing in the right direction. We find that these schemes do not capture enough information of the network. Thus, we develop an extended information model that can capture more mobility information and use regression functions for data processing. Experimental results from both our own simulator and real wireless trace data show that our routing algorithms based on the extended information model can increase the delivery ratio and reduce the delivery latency of routing compared with existing ones.
83|8||Performance analysis of opportunistic broadcast for delay-tolerant wireless sensor networks|This paper investigates a class of mobile wireless sensor networks that are unconnected most of the times; we refer to them as delay–tolerant wireless sensor networks (DTWSN). These networks inherit their characteristics from both delay tolerant networks (DTN) and traditional wireless sensor networks. After introducing DTWSNs, three main problems in the design space of these networks are discussed: routing, data gathering, and neighbor discovery. A general protocol is proposed for DTWSNs based on opportunistic broadcasting in delay-tolerant networks with radio device on–off periods. Three performance measures are defined in the study: the energy for sending queries to ask for data from possible neighbors (querying energy), data transfer energy, and absorption time (delay). A simple yet accurate approximation for the data-transfer energy is proposed. An analytic model is provided to evaluate the querying energy per contact (epc). Simulation results for the data propagation delay show that the querying energy per contact measure obtained from the analytic model is proportional to the product of the querying energy and the delay. A practical rule of thumb for an optimal query interval in terms of delay and energy is derived from different parts of the study.
83|8||GLBM: A new QoS aware multicast scheme for wireless mesh networks|Wireless mesh networks (WMNs) have been attracting significant attention due to their promising technology. The WMN technology is becoming a major avenue for the fourth generation of wireless mobility. Communication in large-scale wireless networks can create bottlenecks for scalable implementations of computationally intensive applications. A class of crucially important communication patterns that have already received considerable attention in this regard are group communication operations, since these inevitably place a high demand on network bandwidth and have a consequent impact on algorithm execution times. Multicast communication has been among the most primitive group capabilities of any message passing in networks. It is central to many important distributed applications in science and engineering and fundamental to the implementation of higher-level communication operations such as gossip, gather, and barrier synchronisation. Existing solutions offered for providing multicast communications in WMN have severe restriction in terms of almost all performance characteristics. Consequently, there is a need for the design and analysis of new efficient multicast communication schemes for this promising network technology. Hence, the aim of this study is to tackle the challenges posed by the continuously growing need for delivering efficient multicast communication over WMN. In particular, this study presents a new load balancing aware multicast algorithm with the aim of enhancing the QoS in the multicast communication over WMNs. Our simulations experiments show that our proposed multicast algorithm exhibits superior performance in terms of delay, jitter and throughput, compared to the most well known multicast algorithms.
83|8||Communication modeling of multicast in all-port wormhole-routed NoCs|Multicast is one of the most frequently used collective communication operations in multi-core SoC platforms. Bus as the traditional interconnect architecture for SoC development has been highly efficient in delivering multicast messages. Since the bus is non-scalable, it can not address the bandwidth requirements of the large SoCs. The networks on-chip (NoCs) emerged as a scalable alternative to address the increasing communication demands of such systems. However, due to its hop-to-hop communication, the NoCs may not be able to deliver multicast operations as efficiently as buses do. Adopting multi-port routers has been an approach to improve the performance of the multicast operations in interconnection networks. This paper presents a novel analytical model to compute communication latency of the multicast operation in wormhole-routed interconnection networks employing asynchronous multi-port routers scheme. The model is applied to the Quarc NoC and its validity is verified by comparing the model predictions against the results obtained from a discrete-event simulator developed using OMNET++.
83|8||On the performance of real-time multi-item request scheduling in data broadcast environments|On-demand broadcast is an effective wireless data dissemination technique to enhance system scalability and capability to handle dynamic data access patterns. Previous studies on time-critical on-demand data broadcast were conducted under the assumption that each client requests only one data item at a time. With the rapid growth of time-critical information dissemination services in emerging applications, there is an increasing need for systems to support efficient processing of real-time multi-item requests. Little work, however, has been done. In this paper, we study the behavior of six representative single-item request based scheduling algorithms in time-critical multi-item request environments. The results show that the performance of all algorithms deteriorates when dealing with multi-item requests. We observe that data popularity, which is an effective factor to save bandwidth and improve performance in scheduling single-item requests, becomes a hindrance to performance in multi-item request environments. Most multi-item requests scheduled by these algorithms suffer from a starvation problem, which is the root of performance deterioration. Based on our analysis, a novel algorithm that considers both request popularity and request timing requirement is proposed. The performance results of our simulation study show that the proposed algorithm is superior to other classical algorithms under a variety of circumstances.
83|8||Performance evaluation of bag of gangs scheduling in a heterogeneous distributed system|Distributed systems deliver a cost-effective and scalable solution to the increasing performance intensive applications by utilizing several shared resources. Gang scheduling is considered to be an efficient time-space sharing scheduling algorithm for parallel and distributed systems. In this paper we examine the performance of scheduling strategies of jobs which are bags of independent gangs in a heterogeneous system. A simulation model is used to evaluate the performance of bag of gangs scheduling in the presence of high priority jobs implementing migrations. The simulation results reveal the significant role of the implemented migration scheme as a load balancing factor in a heterogeneous environment. Another significant aspect of implementing migrations presented in this paper is the reduction of the fragmentation caused in the schedule by gang scheduled jobs and the alleviation of the performance impact of the high priority jobs.
83|8||A modeling approach on the TelosB WSN platform power consumption|Substantial characteristics of wireless sensor networks, such as autonomy and miniature size, are achieved at the expense of restricted energy resources. Optimal resource management is thus among the most important challenges in WSNs development and its success requires accurate and practical models based on detailed insight concerning the factors contributing to the overall power consumption of a WSN mote. To achieve such awareness, that will enable models development, appropriate measuring test-beds and methodologies are needed, facilitating reliable and accurate power consumption measurements of critical functionalities.
83|8||Design and evaluation of a novel MAC layer handoff protocol for IEEE 802.11 wireless networks|In recent years, the IEEE 802.11 wireless network family has become one of the most important set of standards in the wireless communications industry. IEEE 802.11 compliant devices are inexpensive and easier to configure and deploy than other wireless technologies. In an IEEE 802.11 wireless network, wireless terminals can move freely. As a result, when the wireless terminal moves away from its current access point, it must switch to another access point to maintain the active connection. This is known as the MAC layer handoff process. MAC layer handoff latency should be minimized to support real-time applications and to provide mobile devices with seamless roaming in IEEE 802.11 wireless networks. This paper proposes a novel MAC layer handoff protocol over IEEE 802.11 wireless networks by introducing advertisement messages sent from other mobile nodes and from which wireless terminals are able to receive the information of access points in their neighborhood. A mobile node can try to associate with access points based on the prediction before starting the probe process. The experimental results demonstrate that our solution can reduce MAC layer handoff latency to meet the requirements of real-time applications.
83|8||Signal strength based routing for power saving in mobile ad hoc networks|The transmission bandwidth between two nodes in mobile ad hoc networks is important in terms of power consumption. However, the bandwidth between two nodes is always treated the same, regardless of what the distance is between the two nodes. If a node equips a GPS device to determine the distance between two nodes, the hardware cost and the power consumption increase. In this paper, we propose using a bandwidth-based power-aware routing protocol with signal detection instead of using GPS devices to determine the distance. In our proposed routing protocol, we use the received signal variation to predict the transmission bandwidth and the lifetime of a link. Accordingly, the possible amount of data that can be transmitted and the remaining power of nodes in the path after data transmission can be predicted. By predicting the possible amount of data that can be transmitted and the remaining power of nodes after data transmission, we can design a bandwidth-based power-aware routing protocol that has power efficiency and that prolongs network lifetime. In our simulation, we compare our proposed routing protocol with two signal-based routing protocols, SSA and ABR, and a power-aware routing protocol, MMBCR, in terms of the throughput, the average transmission bandwidth, the number of rerouting paths, the path lifetime, the power consumed when a byte is transmitted, and the network lifetime (the ratio of active nodes).
83|8||Architecture analysis of enterprise systems modifiability â Models, analysis, and validation|Enterprise architecture (EA) models can be used in order to increase the general understanding of enterprise systems and to perform various kinds of analysis. This paper presents instantiated architectural models based on a metamodel for enterprise systems modifiability analysis, i.e. for assessing the cost of making changes to enterprise-wide systems. The instantiated architectural models detailed are based on 21 software change projects conducted at four large Nordic companies. Probabilistic relational models (PRMs) are used for formalizing the EA analysis approach. PRMs enable the combination of regular entity-relationship modeling aspects with means to perform enterprise architecture analysis under uncertainty. The modifiability metamodel employed in the analysis is validated with survey and workshop data (in total 110 experts were surveyed) and with the data collected in the 21 software change projects. Validation indicates that the modifiability metamodel contains the appropriate set of elements. It also indicates that the metamodel produces estimates within a 75% accuracy in 87% of the time and has a mean accuracy of 88% (when considering projects of 2000 man-hours or more).
83|8||Handling communications in process algebraic architectural description languages: Modeling, verification, and implementation|Architectural description languages are a useful tool for modeling complex software systems at a high level of abstraction. If based on formal methods, they can also serve for enabling the early verification of various properties such as component coordination and for guiding the synthesis of code correct by construction. This is the case with process algebraic architectural description languages, which are process calculi enhanced with the main architectural concepts. However, the techniques with which those languages have been equipped are mainly conceived to work with synchronous communications only. The objective of this paper is threefold. On the modeling side, we show how to enhance the expressiveness of a typical process algebraic architectural description language by including the capability of representing nonsynchronous communications in such a way that the usability of the original language is preserved. On the verification side, we show how to modify techniques for analyzing the absence of coordination mismatches like the compatibility check for acyclic topologies and the interoperability check for cyclic topologies in such a way that those checks are valid also for nonsynchronous communications. On the implementation side, we show how to generate multithreaded object-oriented software in the presence of synchronous and nonsynchronous communications in such a way that the properties proved at the architectural level are preserved at the code level.
83|8||A new approach for componentâs port modeling in software architecture|The component’s interaction points with the external world play a fundamental role in the specification of an application’s architecture. Current software architecture approaches consider an interaction point as an atomic element in the specification of interconnections, despite the complexity of its structure and the attached behavior. It is not possible in current component models to deal separately with an element of an interaction point when such an element is needed alone for specifying a specific logic. To support such logic and the specification of a wide range of early ideas in the process of elaborating a software system, the Integrated Approach to Software Architecture (IASA) uses an interaction point model which provides facilities to manipulate any structural or behavioral element defining an interaction point. In addition, such facilities represent the fundamental foundation of the native support by IASA of Aspect Oriented Software Architectures (AOSA) specifications.
83|8||A differential cryptanalysis of YenâChenâWu multimedia cryptography system|Recently, Yen et al. presented a new chaos-based cryptosystem for multimedia transmission named “multimedia cryptography system” (MCS). No cryptanalytic results have been reported so far. This paper presents a differential attack to break MCS, which requires only seven chosen plaintexts. The complexity of the attack is O(N)O(N), where N is the size of plaintext. Experimental results are also given to show the real performance of the proposed attack.
83|8||A distributed platform for personalized advertising in digital interactive TV environments|Advertising plays an important role in modern free markets. Furthermore, advertising is moving towards the establishment of one-to-one marketing relationships. Thus, personalized advertisement is currently considered as a hot topic in product promotion as it can be proved beneficial for all the key players, such as the advertisers, the advertised companies, as well as the consumers. Interactive TV and WWW can provide the means for personalized advertising. But of course, special systems and platforms for personalization must be first developed. This paper proposes a prototype system which efficiently achieves the personalization of the advertisements in the environment of digital interactive TV. Thus, the environment for the exploitation of the proposed system are examined, the details in design and implementation are given, while extensive operation testing and evaluation are provided proving its high applicability in real business environments.
83|8||Image watermarking method in multiwavelet domain based on support vector machines|A novel image watermarking method in multiwavelet domain based on support vector machines (SVMs) is proposed in this paper. The special frequency band and property of image in multiwavelet domain are employed for the watermarking algorithm. After performed single-level multiwavelet decomposition on each image block of an image, a mean value modulation method, which modulates mean value relationship of multiwavelet coefficients in two approximation sub-bands, is used for carrying watermark embedding. The mean value modulation method can more effectively reduce image distortion than that of conventional single coefficient. At watermark detector, SVMs is used to learn the mean value relationship. Due to good learning ability of SVMs, watermark can be correctly extracted under several different attacks. The experimental results show proposed algorithm is robust to common attacks such as JPEG, low-pass filtering, noise addition, rotation and scaling, etc.
83|8||Requirement-based approach for groupware environments design|Groupware applications have special features that, if they were taken into account from the very beginning, could reasonably improve the quality of the system. Such features concern human–computer–human interactions, i.e. a further step in the human–computer interaction field: communication, collaboration, cooperation and coordination, time, space, and awareness are issues to be considered. This paper presents a novel approach to gather requirements for groupware applications. The proposal is based on a methodology that includes the use of templates to gather the information regarding the different types of requirements. The requirements templates have been extended to include new information to give account of specific features concerning groupware applications. The information gathered is managed in a CASE tool we have developed; then general and specific diagrams are automatic or semi-automatically generated.
83|8||Quality adaptive end-to-end packet scheduling to avoid playout interruptions in Internet video streaming systems|In Internet multimedia streaming, the quality of the delivered media can be adapted to the Quality of Service provided by the underlying network, thanks to encoding algorithms. These allow a fine grained enhancement of a low quality base layer at streaming time. The main objective that should be satisfied in such systems is to avoid the starvation of the decoding process and consequent playout interruptions. In this work, we tackle the problem using a control theoretic approach. In particular, we design and implement the novel end-to-end Quality Adaptive Scheduler for properly distributing the network available bandwidth among base and enhancement layers. The developed solution can be adopted in many contexts given that it has been designed without assumptions on the delivered media nor on the protocol stack. Anyway, to test its effectiveness, we have casted it in a H.264/AVC SVC based video streaming architecture for unicast Internet applications. The performance of the scheduler has been experimentally evaluated in both a controlled testbed and several “wild” Internet scenarios, including also UMTS and satellite radio links. Results have clearly demonstrated that our Quality Adaptive Scheduler is able to significantly improve the performance of the video streaming system in all operative conditions.
83|8||Historical index structure for reducing insertion and search cost in LBS|A major issue in LBS (Location Based Service) is the handling of numerous historical moving object data, affecting query performance and service quality in application systems. In order to store and search lots of data rapidly, an effective index structure is required for improving not only the insertion method, but also the search performance.
83|8||Per-flow optimal service selection for Web services based processes|With the development of the Service-Oriented Computing (SOC) paradigm, flexible business processes can be defined from independently developed services. Multiple services corresponding to the same functionality but characterized by different Quality of Service (QoS) attributes can be offered by different service providers and the best set of Web services can be selected at run-time in order to maximize the QoS for end users.
83|8||Intrusion detection for mobile devices using the knowledge-based, temporal abstraction method|In this paper, a new approach for detecting previously unencountered malware targeting mobile device is proposed. In the proposed approach, time-stamped security data is continuously monitored within the target mobile device (i.e., smartphones, PDAs) and then processed by the knowledge-based temporal abstraction (KBTA) methodology. Using KBTA, continuously measured data (e.g., the number of sent SMSs) and events (e.g., software installation) are integrated with a mobile device security domain knowledge-base (i.e., an ontology for abstracting meaningful patterns from raw, time-oriented security data), to create higher level, time-oriented concepts and patterns, also known as temporal abstractions. Automatically-generated temporal abstractions are then monitored to detect suspicious temporal patterns and to issue an alert. These patterns are compatible with a set of predefined classes of malware as defined by a security expert (or the owner) employing a set of time and value constraints. The goal is to identify malicious behavior that other defensive technologies (e.g., antivirus or firewall) failed to detect. Since the abstraction derivation process is complex, the KBTA method was adapted for mobile devices that are limited in resources (i.e., CPU, memory, battery). To evaluate the proposed modified KBTA method a lightweight host-based intrusion detection system (HIDS), combined with central management capabilities for Android-based mobile phones, was developed. Evaluation results demonstrated the effectiveness of the new approach in detecting malicious applications on mobile devices (detection rate above 94% in most scenarios) and the feasibility of running such a system on mobile devices (CPU consumption was 3% on average).
83|8||A distributed server architecture supporting dynamic resource provisioning for BPM-oriented workflow management systems|Workflow management systems have been widely used in many business process management (BPM) applications. There are also a lot of companies offering commercial software solutions for BPM. However, most of them adopt a simple client/server architecture with one single centralized workflow-management server only. As the number of incoming workflow requests increases, the single workflow-management server might become the performance bottleneck, leading to unacceptable response time. Development of parallel servers might be a possible solution. However, a parallel server architecture with a fixed-number of servers cannot efficiently utilize computing resources under time-varying system workloads. This paper presents a distributed workflow-management server architecture which adopts dynamic resource provisioning mechanisms to deal with the probable performance bottleneck. We implemented a prototype system of the proposed architecture based on a commercial workflow management system, Agentflow. A series of experiments were conducted on the prototype system for performance evaluation. The experimental results indicate that the proposed architecture can deliver scalable performance and effectively maintain stable request response time under a wide range of incoming workflow request workloads.
83|9|http://www.sciencedirect.com/science/journal/01641212/83/9|Introduction to the special issue|
83|9||In Memoriam: Dr. Chandra Kintala|
83|9||Memory leak analysis of mission-critical middleware|Memory leaks are recognized to be one of the major causes of memory exhaustion problems in complex software systems. This paper proposes a practical approach to detect aging phenomena caused by memory leaks in distributed objects Off-The-Shelf middleware, which are commonly used to develop critical applications. The approach, which is validated on a real-world case study from the Air Traffic Control domain, defines algorithms and ad hoc support tools to perform data filtering and to find the best trade off between experimentation time and statistical accuracy of aging trend estimates. Experiments show that fixing memory leaks is not always the key to solve memory exhaustion problems.
83|9||Methods and opportunities for rejuvenation in aging distributed software systems|In this paper we describe several methods for detecting the need for software rejuvenation in mission critical systems that are subjected to worm infection, and introduce new software rejuvenation algorithms. We evaluate these algorithms’ effectiveness using both simulation studies and analytic modeling, by assessing the probability of mission success. The system under study emulates a Mobile Ad-Hoc Network (MANET) of processing nodes. Our analysis determined that some of our rejuvenation algorithms are quite effective in maintaining a high probability of mission success while the system is under explicit attack by a worm infection.
83|9||Analysis of service availability for time-triggered rejuvenation policies|In this paper we investigate the effect of three time-triggered system rejuvenation policies on service availability using a queuing model. The model is formulated as an extended stochastic Petri net using a variety of distributions for times between state changes. We define a metric for steady-state service availability and derive how it can be estimated from the models in a hybrid approach combining simulation and analytical reasoning. We further analyze time-to-failure of systems with rejuvenation. Experiments show that the optimal rejuvenation interval as well as the achievable service availability improvement depend significantly on system utilization. The experiments also show that service availability can deviate significantly from steady-state system availability. For low utilization all rejuvenation policies perform well. For medium utilization, one policy is significantly inferior to the other two, while for high utilization, no rejuvenation should be performed at all.
83|9||Comprehensive evaluation of aperiodic checkpointing and rejuvenation schemes in operational software system|This paper examines comprehensive evaluation of aperiodic time-based checkpointing and rejuvenation schemes maximizing the steady-state system availability in an operational software system. We consider two kinds of maintenance policies: checkpointing prior to rejuvenating (CPTR) and rejuvenating prior to checkpointing (RPTC). These schemes are complementary from each other to schedule checkpoints and rejuvenation points. In addition, under a periodic full maintenance operation, we show that aperiodic checkpointing or rejuvenation scheme is optimal to maximize the steady-state system availability by applying the dynamic programming. In numerical examples, CPTR and RPTC are comparatively examined with same overhead parameters, and the effects of CPTR and RPTC on maximizing the steady-state system availability are investigated.
83|9||Software for protection system of VR-1 training reactor|The article deals with the software for a new protection system of the VR-1 training reactor which consists of the independent power protection and the operational power measuring systems. Both systems are computer-based, and they are diverse in sensors, hardware and software. They are responsible for nuclear safety of the reactor, so the quality requirements for their software are strict. The software was developed in accordance with nuclear standards. During the development, both software products were carefully tested, and after the integration of hardware/software, they were validated with the simulation of input signals and the checking of their responses.
83|9||Means-ends and whole-part traceability analysis of safety requirements|Safety is a system property, hence the high-level safety requirements are incorporated into the implementation of system components. In this paper, we propose an optimized traceability analysis method which is based on the means-ends and whole-part concept of the approach for cognitive systems engineering to trace these safety requirements. A system consists of hardware, software, and humans according to a whole-part decomposition. The safety requirements of a system and its components are enforced or implemented through a means-ends lifecycle. To provide evidence of the safety of a system, the means-ends and whole-part traceability analysis method will optimize the creation of safety evidence from the safety requirements, safety analysis results, and other system artifacts produced through a lifecycle. These sources of safety evidence have a causal (cause-consequence) relationship between each other. The failure mode and effect analysis (FMEA), the hazard and operability analysis (HAZOP), and the fault tree analysis (FTA) techniques are generally used for safety analysis of systems and their components. These techniques cover the causal relations in a safety analysis. The causal relationships in the proposed method make it possible to trace the safety requirements through the safety analysis results and system artifacts. We present the proposed approach with an example, and described the usage of TRACE and NuSRS tools to apply the approach.
83|9||Quantifying security risk level from CVSS estimates of frequency and impact|Modern society relies on and profits from well-balanced computerized systems. Each of these systems has a core mission such as the correct and safe operation of safety critical systems or innovative and effective operation of e-commerce systems. It might be said that the success of these systems depends on their mission. Although the concept of “well-balanced” has a slightly different meaning for each of these two categories of systems, both have to meet customer needs, deliver capabilities and functions according to expectations and generate revenue to sustain today’s highly competitive market. Tighter financial constraints are forcing safety critical systems away from dedicated and expensive communication regimes, such as the ownership and operation of dedicated communication links, towards reliance on third parties and standardized means of communication. As a consequence, knowledge about their internal structures and operations is more widely and publicly available and this can make them more prone to security attacks. These systems are, therefore, moving towards a remotely exploitable environment and the risks associated with this must be controlled.
84|1|http://www.sciencedirect.com/science/journal/01641212/84/1|Special issue on the information networking and services|
84|1||A topology control protocol based on eligibility and efficiency metrics|The question of fairness in wireless sensor networks is not studied very well. It is not unusual to observe in the literature fairness traded for low latency or reliability. However, a disproportional use of some critical nodes as relaying nodes can cause premature network fragmentation. This paper investigates fairness in multi-hop wireless sensor networks and proposes a topology control protocol that enables nodes to exhaust their energy fairly. Moreover, it demonstrates that whereas the number of neighboring nodes with which a node should cooperate depends on the density of the network, increasing this number beyond a certain amount does not contribute to network connectivity.
84|1||An analytical model of broadcast in QoS-aware wormhole-routed NoCs|Networks-on-Chip (NoC) emerged to address the technological and design issues related to development of large systems-on-chip (SoCs). Due to diversity of the application's performance requirements, most NoC architectures offer supports for quality of service (QoS). Also, to utilize the available bandwidth efficiently, they might implement mechanisms for delivering collective communication operations. This paper presents an analytical model to predict the average latency of wormhole-routed prioritized broadcast communication in NoCs. The model assumes that the network uses all-port routers scheme and offers differentiated services-based QoS. To verify the analysis, the model predictions are compared against the results obtained from a discrete-event simulator developed using OMNET++.
84|1||An experimental study of peer behavior in a pure P2P network|Dynamic peer behavior in P2P networks has a large impact on the performance of the network because each peer acts simultaneously as a server and a client in an overlay network. However, peer behavior in pure P2P networks is not well known because there is no management system and the network size is large. We have proposed a measurement method to collect information efficiently, and a method of analyzing the peer behavior with or without file uploading for pure P2P file sharing networks. These methods have been applied to Winny, which is the most popular P2P file sharing application in Japan. The analysis results show that the network is composed of about 30% of peers observed in 24 h, and 50% of peers contribute to the network as file uploaders. Selfish peers, which leave the network after downloading, are observed especially in the morning. The file uploading peers and the peers joining after midnight remain in the network for a long period. We also give mathematical fitting for the distribution of the peer lifetime and the frequency of joining the network. These results show that the distribution of lifetime does not have a heavy tail, and the behavior of peers joining the network is not a Poisson process.
84|1||Fault-tolerant flocking for a group of autonomous mobile robots|Consider a system composed of mobile robots that move on the plane, each of which independently executing its own instance of an algorithm. Given a desired geometric pattern, the flocking problem consists in ensuring that the robots form this pattern and maintain it while moving together on the plane. In this paper, we explore flocking in the presence of faulty robots, where the desired pattern is a regular polygon. We propose a distributed fault tolerant flocking algorithm assuming a semi-synchronous model with a k-bounded scheduler, in the sense that no robot is activated no more than k times between any two consecutive activations of any other robot.
84|1||Performance analysis of an integrated scheduling scheme in the presence of bursty MMPP traffic|Contemporary communication networks are expected to support multimedia applications which require diversified Quality-of-Services (QoS). An integrated scheduling discipline of Priority Queueing (PQ) and Generalized Processor Sharing (GPS), referred to as P–G, has recently emerged as a promising scheme for cost-effective QoS differentiation. In this paper, we develop a new analytical model for the integrated P–G system subject to bursty traffic. The Markov-Modulated Poisson Process (MMPP) is adopted to capture traffic burstiness because it can qualitatively model time-varying arrival rate and important correlation between inter-arrival times. To derive the desired performance metrics for individual sessions, the integrated P–G system is decomposed into a set of Single-Server Single-Queue (SSSQ) systems. Specifically, the integrated system is first divided into an SSSQ system and a GPS system. Next, a bounding approach is adopted to decompose the GPS system into individual SSSQ systems. Extensive comparisons between analytical and simulation results validate the accuracy of the analytical model. To demonstrate its merits, the model is used to investigate the configuration of the GPS weights under the QoS constraints of different traffic flows.
84|1||A comparative study on simulation vs. real time deployment in wireless sensor networks|Increasing deployment density and shrinking size of wireless sensor nodes requires small equipped battery size. This means emerging wireless sensor nodes must compete for efficient energy utilization. Medium Access Control (MAC) protocols play a vital role in energy consumption of sensor node as it controls the radio activities. Customized or open source simulators play an important role to measure the performance effectiveness of MAC protocols based on the fact that they are flexible, reduce experimental overhead and cost. Nevertheless, these benefits come at the cost of results accuracy. In this paper, we investigate differences of the behaviour of our agent based S-MAC protocols in real deployment compared to the results produced using our custom based simulator, which ignores the lower layers effects such as packet collision and overhearing. We use network simulator 2 (ns2), an open source simulator, which provides a complete protocol stack. We further try to find and explain the rationale of the variance of results produced by real deployment and that of simulators.
84|1||Using Grid services to parallelize IBM's Generic Log Adapter|Since their definition in the Open Grid Services Architecture, Grid services has been used in many Grid-enabled applications to leverage the computational power offered by Grid Systems. An important research issue addressed in this regard is how to increase the efficiency of the Grid services for a massive processing and scientific computing computations arising in data intensive computations, for example the processing of large log data files arising in “problem determination” in today's IT computing environments.
84|1||Double-layered schema integration of heterogeneous XML sources|Schema integration aims to create a mediated schema as a unified representation of existing heterogeneous sources sharing a common application domain. These sources have been increasingly written in XML due to its versatility and expressive power. Unfortunately, these sources often use different elements and structures to express the same concepts and relations, thus causing substantial semantic and structural conflicts. Such a challenge impedes the creation of high-quality mediated schemas and has not been adequately addressed by existing integration methods. In this paper, we propose a novel method, named XINTOR, for automating the integration of heterogeneous schemas. Given a set of XML sources and a set of correspondences between the source schemas, our method aims to create a complete and minimal mediated schema: it completely captures all of the concepts and relations in the sources without duplication, provided that the concepts do not overlap. Our contributions are fourfold. First, we resolve structural conflicts inherent in the source schemas. Second, we introduce a new statistics-based measure, called path cohesion, for selecting concepts and relations to be a part of the mediated schema. The path cohesion is statistically computed based on multiple path quality dimensions such as average path length and path frequency. Third, we resolve semantic conflicts by augmenting the semantics of similar concepts with context-dependent information. Finally, we propose a novel double-layered mediated schema to retain a wider range of concepts and relations than existing mediated schemas, which are at best either complete or minimal, but not both. Performed on both real and synthetic datasets, our experimental results show that XINTOR outperforms existing methods with respect to (i) the mediated-schema quality using precision, recall, F-measure, and schema minimality; and (ii) the execution performance based on execution time and scale-up performance.
84|1||A formal approach for the specification and verification of trustworthy component-based systems|Software systems are increasingly becoming ubiquitous affecting the way we experience the world. Embedded software systems, especially those used in smart devices, have become an essential constituent of the technological infrastructure of modern societies. Such systems, in order to be trusted in society, must be proved to be trustworthy. Trustworthiness is a composite non-functional property that implies safety, timeliness, security, availability, and reliability. This paper presents a formal approach for the development of trustworthy component-based systems. The approach involves a formal component model for the specification of component’s structure, functional, and non-functional (trustworthiness) properties, a model transformation technique for the automatic generation of component behavior using the specified structure and restricted by the specified properties, and a unified formal verification method for safety, security, reliability and availability properties using model checking.
84|1||High capacity data hiding schemes for medical images based on difference expansion|Since the difference expansion (DE) technique was proposed, many researchers tried to, improve its performance in terms of hiding capacity and visual quality. In this paper, a new scheme, based on DE is proposed in order to increase the hiding capacity for medical images. One of the characteristics of medical images, among the other types of images, is the large smooth regions. Taking advantage of this characteristic, our scheme divides the image into two regions; smooth region and non-smooth region. For the smooth region, a high embedding capacity scheme is applied, while the original DE method is applied to the non-smooth region. Sixteen DICOM images of different modalities were used for testing the proposed schemes. The results showed that the proposed scheme has higher hiding capacity compared to the original schemes.
84|1||Efficient key management for preserving HIPAA regulations|The protection of patients’ health information is a very important issue in the information age. Health Insurance Portability and Accountability Act (HIPAA) of privacy and security regulations are two crucial provisions in the protection of healthcare privacy, especially electronic medical information. For the quality and efficiency of the electronic services, it is necessary to construct better performance for the user and the trusted party. Based on elliptic curve cryptography (ECC) and complying with HIPAA regulations, this article presents an efficient key management scheme to facilitate inter-operations among the applied cryptographic mechanisms. In addition, the proposed scheme can achieve the complete functionality which includes: (1) a dictionary of key tables is not required for users and other units; (2) users can freely choose their own passwords; (3) users can freely update their passwords after the registration phase; (4) the computational cost is very low for users and the trusted center or server; (5) users are able to access their individual medical information through the authorization process; (6) case of consent exceptions intended to facilitate emergency applications or other possible exceptions can also be dealt with easier.
84|1||Identity-based strong designated verifier signature revisited|Designated verifier signature (DVS) allows the signer to persuade a verifier the validity of a statement but prevent the verifier from transferring the conviction. Strong designated verifier signature (SDVS) is a variant of DVS, which only allows the verifier to privately check the validity of the signer’s signature. In this work we observe that the unforgeability model considered in the existing identity-based SDVS schemes is not strong enough to capture practical attacks, and propose to consider another model which is shown to be strictly stronger than the old one. We then propose a new efficient construction of identity-based SDVS scheme, which is provably unforgeable under the newly proposed definition, based on the hardness of Computational Diffie–Hellman problem in the random oracle model. Our scheme is perfectly non-transferable in the sense that the signer and the designated verifier can produce identically distributed signatures on the same message. Besides, it is the first IBSDVS scheme that is non-delegatable with respect to (an identity-based variant of) the definition proposed by Lipmaa et al. (ICALP 2005).
84|1||Effective rank aggregation for metasearching|
84|1||Firmsâ involvement in Open Source projects: A trade-off between software structural quality and popularity|Open Source (OS) was born as a pragmatic alternative to the ideology of Free Software and it is now increasingly seen by companies as a new approach to developing and making business upon software. Whereas the role of firms is clear for commercial OS projects, it still needs investigation for projects based on communities. This paper analyses the impact of firms’ participation on popularity and internal software design quality for 643 SourceForge.net projects. Results show that firms’ involvement improves the ranking of OS projects, but, on the other hand, puts corporate constraints to OS developing practices, thus leading to lower structural software design quality.
84|1||An assessment of systems and software engineering scholars and institutions (2003â2007 and 2004â2008)|An ongoing, annual survey of publications in systems and software engineering identifies the top 15 scholars and institutions in the field over a 5-year period. Each ranking is based on the weighted scores of the number of papers published in TSE, TOSEM, JSS, SPE, EMSE, IST, and Software of the corresponding period. This report summarizes the results for 2003–2007 and 2004–2008. The top-ranked institution is Korea Advanced Institute of Science and Technology, Korea for 2003–2007, and Simula Research Laboratory, Norway for 2004–2008, while Magne Jørgensen is the top-ranked scholar for both periods.
84|10|http://www.sciencedirect.com/science/journal/01641212/84/10|A geographic routing hybrid approach for void resolution in wireless sensor networks|Geographic routing is one of the most suitable routing strategies for large scale wireless sensor networks due to its low overhead and high scalability features. A geographic routing scheme usually combines a geographic greedy forwarding with a recovery mechanism to solve the local minima problem. Solutions proposed in the literature commonly combine greedy forwarding with the well known face routing for achieving this goal. However, the average path length in number of hops produced by face routing could be much worse than the optimal topological path in most realistic scenarios. In this paper, we propose a new intermediate procedure between the geographic greedy mode and the recovery mode in order to improve routing efficiency in number of hops, without network overhead. It exploits the optimal topological route to base stations, obtained by beacon messages, as a resource to find better routes than the ones created by face routing. We show by simulations that the proposed hybrid approach leads to a significant improvement of routing performance when applied to combined greedy-face routing algorithms.
84|10||Adaptable Decentralized Service Oriented Architecture|In the Service Oriented Architecture (SOA), BPEL specified business processes are executed by non-scalable centralized orchestration engines. In order to address the scalability issue, decentralized orchestration engines are applied, which decentralize BPEL processes into static fragments at design time without considering runtime requirements. The fragments are then encapsulated into runtime components such as agents. There are a variety of attitudes towards workflow decentralization; however, only a few of them produce adaptable fragments with runtime environment. In this paper, producing runtime adaptable fragments is presented in two aspects. The first one is frequent-path adaptability that is equal to finding closely interrelated activities and encapsulating them in the same fragment to omit the communication cost of the activities. Another aspect is proportional-fragment adaptability, which is analogous to the proportionality of produced fragments with number of workflow engine machines. It extenuates the internal communication among the fragments on the same machine. An ever-changing runtime environment along with the mentioned adaptability aspects may result in producing a variety of process versions at runtime. Thus, an Adaptable and Decentralized Workflow Execution Framework (ADWEF) is introduced that proposes an abstraction of adaptable decentralization in the SOA orchestration layer. Furthermore, ADWEF architectures Type-1 and Type-2 are presented to support the execution of fragments created by two decentralization methods, which produce customized fragments known as Hierarchical Process Decentralization (HPD) and Hierarchical Intelligent Process Decentralization (HIPD). However, mapping the current system conditions to a suitable decentralization method is considered as future work. Evaluations of the ADWEF decentralization methods substantiate both adaptability aspects and demonstrate a range of improvements in response-time, throughput, and bandwidth-usage compared to previous methods.
84|10||Formal analysis of an electronic voting system: An experience report|We have seen that several currently deployed e-voting systems share critical failures in their design and implementation that render their technical and procedural controls insufficient to guarantee trustworthy voting. The application of formal methods would greatly help to better address problems associated with assurance against requirements and standards. More specifically, it would help to thoroughly specify and analyze the underlying assumptions and security specific properties, and it would improve the trustworthiness of the final systems. In this article, we show how such techniques can be used to model and reason about the security of one of the currently deployed e-voting systems in the U.S.A named ES&S. We used the ASTRAL language to specify the voting process of ES&S machines and the critical security requirements for the system. Proof obligations that verify that the specified system meets the critical requirements were automatically generated by the ASTRAL Software Development Environment (SDE). The PVS interactive theorem prover was then used to apply the appropriate proof strategies and discharge the proof obligations. We also believe that besides analyzing the system against its requirements, it is equally important to perform an analysis under malicious circumstances where the execution model is augmented with attack behaviors. Thus, we extend the formal specification of the system by specifying attacks that have been shown to successfully compromise the system, and we then repeat the formal verification. This is helpful in detecting missing requirements or unwarranted assumptions about the specification of the system. In addition, this allows one to sketch countermeasure strategies to be used when the system behaves differently than it should and to build confidence about the system under development. Finally, we acknowledge the main problem that arises in e-voting system specification and verification: modeling attacks is very difficult because the different types of attack often cut across the structure of the original behavior models, thus making (incremental or compositional) verification very difficult.
84|10||New and efficient knowledge discovery of partial periodic patterns with multiple minimum supports|The problem of mining partial periodic patterns is an important issue with many applications. Previous studies to find these patterns encounter efficiency and effectiveness problem. The efficiency problem is that most previous methods were proposed to find frequent partial periodic patterns by extending the well-known Apriori-like algorithm. However, these methods generate many candidate partial periodic patterns to calculate the patterns’ supports, spending much time for discovering patterns. The effective problem is that only one minimum support threshold is set to find frequent partial periodic patterns but the results is not practical for real-world. In real-life circumstances, some rare or specific events may occur with lower frequencies but their occurrences may offer some vital information to be referred in decision making. If the minimum support is set too high, the associations between events along with higher and lower frequencies cannot be evaluated so that significant knowledge will be ignored. In this study, an algorithm to overcome these two problems has been proposed to generating redundant candidate patterns and setting only one minimum support threshold. The algorithm greatly improves the efficiency and effectiveness. First, it eliminates the need to generate numerous candidate partial periodic patterns thus reducing database scanning. Second, the minimum support threshold of each event can be specified based in its real-life occurring frequency.
84|10||Confidential deniable authentication using promised signcryption|In a deniable authentication protocol, a receiver is convinced that a received message is indeed from a particular sender, but cannot prove this to any third party. Deniable authentication protocols satisfy deniability and intended receiver properties. Among the proposed deniable authentication protocols, non-interactive protocols are more efficient than interactive protocols by reducing communication cost. The Hwang and Ma, and the Hwang and Chao non-interactive protocols provide sender anonymity. Recently some interactive protocols provide confidentiality while no non-interactive protocols do. However, the transferred data may damage sender or receiver anonymity. To provide confidentiality and anonymity efficiently, the first promised signcryption scheme is proposed. Using our promised signcryption scheme, we propose the first efficient non-interactive deniable authentication protocol with confidentiality, sender anonymity, and sender protection.
84|10||An efficient CRT-RSA algorithm secure against power and fault attacks|RSA digital signatures based on the Chinese Remainder Theorem (CRT) are subject to power and fault attacks. In particular, modular exponentiation and CRT recombination are prone to both attacks. However, earlier countermeasures are susceptible to the possibility of advanced and sophisticated attacks. In this paper, we investigate state-of-the-art countermeasures against power and fault attacks from the viewpoint of security and efficiency. Then, we show possible vulnerabilities to fault attacks. Finally, we propose new modular exponentiation and CRT recombination algorithms secure against all known power and fault attacks. Our proposal improves efficiency by replacing arithmetic operations with logical ones to check errors in the CRT recombination step. In addition, since our CRT-RSA algorithm does not require knowledge of the public exponent, it guarantees a more versatile implementation.
84|10||An aspect-oriented reference architecture for Software Engineering Environments|Reusable and evolvable Software Engineering Environments (SEEs) are essential to software production and have increasingly become a need. In another perspective, software architectures and reference architectures have played a significant role in determining the success of software systems. In this paper we present a reference architecture for SEEs, named RefASSET, which is based on concepts coming from the aspect-oriented approach. This architecture is specialized to the software testing domain and the development of tools for that domain is discussed. This and other case studies have pointed out that the use of aspects in RefASSET provides a better Separation of Concerns, resulting in reusable and evolvable SEEs.
84|10||A meet-in-the-middle attack on reduced-round ARIA|In this paper, the meet-in-the-middle attack against block cipher ARIA is presented for the first time. Some new 3-round and 4-round distinguishing properties of ARIA are found. Based on the 3-round distinguishing property, we can apply the meet-in-the-middle attack with up to 6 rounds for all versions of ARIA. Based on the 4-round distinguishing property, we can mount a successful attack on 8-round ARIA-256 for the first time. Furthermore, the 4-round distinguishing property could be improved which leads to a 7-round attack on ARIA-192. Compared with the existing cryptanalytic results on ARIA, the meet-in-the-middle attack has a huge precomputation and memory complexities. However, we can do the precomputation once and for all. These results show that 8-round ARIA-256 is not immune to the meet-in-the-middle attack.
84|10||On software verification for sensor nodes|We consider software written for networked, wireless sensor nodes, and specialize software verification techniques for standard C programs in order to locate programming errors in sensor applications before the software's deployment on motes. Ensuring the reliability of sensor applications is challenging: low-level, interrupt-driven code runs without memory protection in dynamic environments. The difficulties lie with (i) being able to automatically extract standard C models out of the particular flavours of embedded C used in sensor programming solutions, and (ii) decreasing the resulting program's state space to a degree that allows practical verification times.
84|10||Boosting adaptivity of fault-tolerant scheduling for real-time tasks with service requirements on clusters|Thank to the excellent extensibility and usability, computer clusters have become the dominating platform for parallel computing. Fault-tolerance is mandatory for safety-critical applications running on clusters. In this paper we propose a service-aware and adaptive fault-tolerant scheduling algorithm using overlapping technologies (SAO in short) that can tolerate a node’s permanent failure at any time instant for real-time tasks with service requirements in heterogeneous clusters. SAO adopts the primary/backup model and considers the timing constraints, service requirements, and system resource utilization. To improve system resource utilization, we employ backup-backup (BB in short) and primary-backup (PB in short) overlapping technologies and analyze the overlapping constraints. In addition, SAO has high system adaptivity by dynamically adjusting the service levels of tasks based on system load. Furthermore, to improve resource utilization and schedulability, SAO makes backup copies adopt passive execution scheme or decrease the overlapping execution time of the primary copy and backup copy of a task as much as possible. Compared with a baseline algorithm SAWO (a service-aware and adaptive fault-tolerant scheduling algorithm without using overlapping technologies) and an existing algorithm DYFARS with simulation experiments, SAO achieves an average of 51.25% improvement in performability.
84|10||Provably secure and efficient authentication techniques for the global mobility network|Recently, several authentication techniques have been developed for the global mobility network (GLOMONET), which provides mobile users with global roaming services. Due to the hardware limitations, the mobile user cannot support the heavy encryption and decryption. This investigation adjusts the entity in the roaming scenario that selects the session key to be used to different types of authentication schemes in GLOMONET and presents two provably secure and efficient authentication protocols for roaming services. One protocol is based on synchronized clocks, while the other uses random numbers. Compared to related approaches, the proposed authentication protocols not only reduce the number of transmissions, but also diminish the computational cost involved in encryption and decryption. Thus, they are more suitable for GLOMONET.
84|10||A general (k, n) scalable secret image sharing scheme with the smooth scalability|A novel (k, n) scalable secret image sharing (SSIS) scheme was proposed to encrypt a secret image into n shadow images. One can gradually reconstruct a secret image by stacking k or more shadows, but he/she cannot conjecture any information from fewer than k shadows. The advantage of a (k, n)-SSIS scheme is that it provides the threshold property (i.e., k is a threshold value necessary to start in to reveal the secret) as well as the scalability (i.e., the information amount of a reconstructed secret is proportional to the number of shadows used in decryption). All previous (k, n)-SSIS schemes did not have the smooth scalability so that the information amount can be “smoothly” proportional to the number of shadows. In this paper, we consider the smooth scalability in (k, n)-SSIS scheme.
84|10||Exploiting social networks to provide privacy in personalized web search|Web search engines (WSE) have become an essential tool for searching information on the Internet. In order to provide personalized search results for the users, WSEs store all the queries which have been submitted by the users and the search results which they have selected. The AOL scandal in 2006 proved that this information contains personally identifiable information which represents a privacy threat for the users who have generated it. In this way, AOL released a file containing twenty million queries made by 658,000 persons and several of those users were successfully tracked. In this paper, we propose a P2P protocol that exploits social networks in order to protect the privacy of the users from the profiling mechanisms of the WSEs. The proposed scheme has been designed considering the presence of users who do not follow the protocol (i.e., adversaries). In order to evaluate the privacy of the users, we have designed a new measure (the profile exposure level (PEL)). Finally, we have used the AOL’s file in order to simulate the behavior of our scheme with real queries which have been generated by real users. Our tests show that our scheme is usable in practice and that it preserves the privacy of the users even in the presence of adversaries.
84|10||A grid-based coverage approach for target tracking in hybrid sensor networks|Most existing work on the coverage problem of wireless sensor networks focuses on improving the coverage of the whole sensing field. In target tracking, the interested coverage area is the emerging region of a motorized target, not the whole sensing field. As the motorized target moves, the emerging region is also dynamically changed. In this paper, we propose a grid-based and distributed approach for providing large coverage for a motorized target in a hybrid sensor network. The large coverage is achieved by moving mobile sensor nodes in the network. To minimize total movement cost, the proposed approach needs to solve the following problems: the minimum number of mobile sensor nodes used for healing coverage holes and the best matching between mobile sensor nodes and coverage holes. In the proposed approach, the above two problems are first transformed into the modified circle covering and minimum cost flow problems, respectively. Then, two polynomial-time algorithms are presented to efficiently solve these two modified graph problems, respectively. Finally, we perform simulation experiments to show the effectiveness of proposed approach in providing the coverage for a motorized target in a hybrid sensor network.
84|10||Identification of extract method refactoring opportunities for the decomposition of methods|The extraction of a code fragment into a separate method is one of the most widely performed refactoring activities, since it allows the decomposition of large and complex methods and can be used in combination with other code transformations for fixing a variety of design problems. Despite the significance of Extract Method refactoring towards code quality improvement, there is limited support for the identification of code fragments with distinct functionality that could be extracted into new methods. The goal of our approach is to automatically identify Extract Method refactoring opportunities which are related with the complete computation of a given variable (complete computation slice) and the statements affecting the state of a given object (object state slice). Moreover, a set of rules regarding the preservation of existing dependences is proposed that exclude refactoring opportunities corresponding to slices whose extraction could possibly cause a change in program behavior. The proposed approach has been evaluated regarding its ability to capture slices of code implementing a distinct functionality, its ability to resolve existing design flaws, its impact on the cohesion of the decomposed and extracted methods, and its ability to preserve program behavior. Moreover, precision and recall have been computed employing the refactoring opportunities found by independent evaluators in software that they developed as a golden set.
84|10||Improving security of q-SDH based digital signatures|In Eurocrypt 2009, Hohenberger and Waters pointed out that a complexity assumption, which restricts the adversary to a single correct response, seems inherently more reliable than their flexible counterparts. The q-SDH assumption is less reliable than standard assumptions because its solution allows exponential answers. On the other hand, the q-SDH assumption exhibits the nice feature of tight reduction in security proof. In this paper, we propose a variant of the q-SDH assumption, so that its correct answers are polynomial and no longer exponentially many. The new assumption is much more reliable and weaker than the original q-SDH assumption. We propose a new digital signature scheme that can tightly reduce the security to the proposed assumption in the standard model. We show that our signature scheme shares most properties with the q-SDH based signature schemes. We also propose a new approach to construct fully secure signatures from weakly secure signature against known-message attacks. Although our security transformation is conditional and not completely generic, it offers another efficient approach to construct fully secure signatures.
84|10||Policy-based Awareness Management (PAM): Case study of a wireless communication system at a hospital|The present paper evaluates the use of software agents to identify relevance of information, called awareness. This evaluation is based on existing policies and scenarios in the context of wireless communication of a hospital in Norway. The study is to address the lack of literature for experimental studies on a method to employ software agents for awareness identification. Research in computer supported cooperative work indicates the significant contributions of software agents to assist individuals. There are bodies of work that show awareness provides the means for software agents in which effective cooperation can take place. In addition, the role of the methods to identify awareness is emphasized in the literature of both computer supported cooperative work and software agents. This paper explains a step-wise process, called Policy-based Awareness Management, which allows agents to use policies as a source to identify awareness and thus change their behaviors accordingly. The contribution of this method is based on the concepts proposed by the logic of general awareness. The present study applies Directory Enabled Networks-next generation as the policy structure for the method. The paper evaluates the process via its application to identify the relevance of information in wireless communication scenarios in a hospital. The present study conducts observations, interviews and discussions on the wireless communication system of the hospital to identify the different scenarios happening in the system. The paper presents a set of simulations on these scenarios and concludes that the method is effective and cost-efficient.
84|10||Engineering the authoring of usable service front ends|This paper presents a method and the associated authoring tool for supporting the development of interactive applications able to access multiple Web Services, even from different types of interactive devices. We show how model-based descriptions are useful for this purpose and describe the associated automatic support along with the underlying rules. The proposed environment is able to aid in the design of new interactive applications that access pre-existing Web Services, which may contain annotations supporting the user interface development. This is achieved through the use of task models as a starting point for the design and development of the corresponding implementations. We also provide an example to better illustrate the features of the approach, and report on two evaluations conducted to assess the support tool.
84|11|http://www.sciencedirect.com/science/journal/01641212/84/11|Mobile applications: Status and trends|
84|11||The âAlways Best Packet Switchingâ architecture for SIP-based mobile multimedia services|This paper presents a distributed architecture for the provision of seamless and responsive mobile multimedia services. This architecture allows its user applications to use concurrently all the wireless network interface cards (NICs) a mobile terminal is equipped with. In particular, as mobile multimedia services are usually implemented using the UDP protocol, our architecture enables the transmission of each UDP datagram through the “most suitable” (e.g. most responsive, least loaded) NIC among those available at the time a datagram is transmitted. We term this operating mode of our architecture Always Best Packet Switching (ABPS). ABPS enables the use of policies for load balancing and recovery purposes. In essence, the architecture we propose consists of the following two principal components: (i) a fixed proxy server, which acts as a relay for the mobile node and enables communications from/to this node regardless of possible firewalls and NAT systems, and (ii) a proxy client running in the mobile node responsible for maintaining a multi-path tunnel, constructed out of all the node's NICs, with the above mentioned fixed proxy server. We show how the architecture supports multimedia applications based on the SIP and RTP/RTCP protocols, and avoids the typical delays introduced by the two way message/response handshake of the SIP signaling protocol. Experimental results originated from the implementation of a VoIP application on top of the architecture we propose show the effectiveness of our approach.
84|11||Mobiiscape: Middleware support for scalable mobility pattern monitoring of moving objects in a large-scale city|With the explosive proliferation of mobile devices such as smartphones, tablets, and sensor nodes, location-based services are getting even more attention than before, considered as one of the killer applications in the upcoming mobile computing era. Developing location-based services necessarily requires an effective and scalable location data processing technology. In this paper, we present Mobiiscape, a novel location monitoring system that collectively monitors mobility patterns of a large number of moving objects in a large-scale city to support city-wide mobility-aware applications. Mobiiscape provides an SQL-like query language named Moving Object Monitoring Query Language (MQL) that allows applications to intuitively specify Mobility Pattern Monitoring Queries (MPQs). Further, Mobiiscape provides a set of scalable location monitoring techniques to efficiently process a large number of MPQs over a large number of location streams. The scalable processing techniques include a (1) Place Border Index, a spatial index for quickly searching for relevant queries upon receiving location streams, (2) Place-Based Window, a spatial-purpose window for efficiently detecting primitive mobility patterns, (3) Shared NFA, a shared query processing technique for efficiently matching complex mobility patterns, and (4) Attribute Pre-matching Bitmap, an in-memory data structure for efficiently filtering out moving objects based on their attributes. We have implemented a Mobiiscape prototype system. Then, we show the usefulness of the system by implementing promising location-based applications based on it such as a ubiquitous taxicab service and a location-based advertising. Also, we demonstrate the performance benefit of the system through extensive evaluation and comparison.
84|11||Dynamic deployment and quality adaptation for mobile augmented reality applications|With the increasing popularity of smartphones and netbooks, more and more applications are developed for the mobile platform. Notwithstanding the recent advances in mobile hardware, most mobile devices still lack sufficient resources (e.g. CPU power and memory) to execute complex multimedia applications such as augmented reality. Application developers also have difficulties to cope with the changing device context (e.g. network connectivity and remaining battery life) and the many different hardware platforms and operating systems to run applications on. Therefore, we introduce the concept where the developer can provide different configurations of an application, each having different resource requirements and a different quality offered to the end user. The middleware framework presented in this paper will select and deploy the configuration offering the best quality possible for the current connectivity and available resources. As these change over time, the framework will dynamically adapt the configuration and deployment at runtime, enhancing the quality by offloading parts of the application when a remote server is discovered, or gracefully degrading the quality when the network connection is lost. Based on experimental results on the augmented reality use case the performance and effectiveness of our middleware has been characterized in different scenarios.
84|11||State of the art of frameworks and middleware for facilitating mobile and ubiquitous learning development|The emergence of mobile and ubiquitous technologies as important tools to complement formal learning has been accompanied by a growing interest in their educational benefits and applications. Mobile devices can be used to promote learning anywhere and anytime, to foster social learning and knowledge sharing, or to visualize augmented reality applications for learning purposes. However, the development of these applications is difficult for many researchers because it requires understanding many different protocols; dealing with distributed schemas, processes, platforms, and services; learning new programming languages; and interacting with different hardware sensors and drivers. For that reason, the use of frameworks and middleware that encapsulate part of this complexity appears to be fundamental to the further development of mobile learning projects. This study analyzes the state of the art of frameworks and middleware devoted to simplifying the development of mobile and ubiquitous learning applications. The results can be useful to many researchers involved in the development of projects using these technologies by providing an overview of the features implemented in each of these frameworks.
84|11||A more efficient and secure ID-based remote mutual authentication with key agreement scheme for mobile devices on elliptic curve cryptosystem|Recently, Yang and Chang proposed an identity-based remote login scheme using elliptic curve cryptography for the users of mobile devices. We have analyzed the security aspects of the Yang and Chang's scheme and identified some security flaws. Also two improvements of the Yang and Chang's scheme have been proposed recently, however, it has been found that the schemes have similar security flaws as in the Yang and Chang's scheme. In order to remove the security pitfalls of the Yang and Chang and the subsequent schemes, we proposed an enhanced remote user mutual authentication scheme that uses elliptic curve cryptography and identity-based cryptosystem with three-way challenge-response handshake technique. It supports flawless mutual authentication of participants, agreement of session key and the leaked key revocation capability. In addition, the proposed scheme possesses low power consumption, low computation cost and better security attributes. As a result, the proposed scheme seems to be more practical and suitable for mobile users for secure Internet banking, online shopping, online voting, etc.
84|11||A secure energy-efficient m-banking application for mobile devices|Mobile banking is one of the emerging services in telecommunications due to the explosive increase in the number of mobile customers around the world. Solutions for mobile banking are varied, ranging from the use of Wireless Transport Layer Security, Security Socket Layer, or application-layer based options. Whereas security at the transport layer is a good choice for e-banking, using it in a mobile device presents several disadvantages such as high energy consumption. In this work, we present a secure energy-efficient m-banking solution for mobile devices. We propose an application-layer protocol whose message formats and message exchanges are designed to reduce time processing, bandwidth use, and energy consumption. Through experimentation, we demonstrate that our secure solution reduces power and energy consumption in more than 30% compared to a secure web-access from the mobile device.
84|11||Meetings through the cloud: Privacy-preserving scheduling on mobile devices|Mobile devices are increasingly being used to store and manage users’ personal information, as well as to access popular third-party context-based services. Very often, these applications need to determine common availabilities among a set of user schedules, in order to allow colleagues, business partners and people to meet. The privacy of the scheduling operation is paramount to the success of such applications, as often users do not want to share their personal schedule details with other users or third-parties.
84|11||A survey on privacy in mobile participatory sensing applications|The presence of multimodal sensors on current mobile phones enables a broad range of novel mobile applications. Environmental and user-centric sensor data of unprecedented quantity and quality can be captured and reported by a possible user base of billions of mobile phone subscribers worldwide. The strong focus on the collection of detailed sensor data may however compromise user privacy in various regards, e.g., by tracking a user’s current location. In this survey, we identify the sensing modalities used in current participatory sensing applications, and assess the threats to user privacy when personal information is sensed and disclosed. We outline how privacy aspects are addressed in existing sensing applications, and determine the adequacy of the solutions under real-world conditions. Finally, we present countermeasures from related research fields, and discuss their applicability in participatory sensing scenarios. Based on our findings, we identify open issues and outline possible solutions to guarantee user privacy in participatory sensing.
84|11||Towards privacy-enhanced mobile communitiesâArchitecture, concepts and user trials|With the advent of mobile broadband technologies and capable mobile devices, social communities become a ubiquitous environment for people to stay in contact and share information with friends and fellows. This provides new opportunities for communities and their providers (e.g. regarding advertising) but also implies new question regarding the privacy and trust of their users. We argue that a balance needs to be found between these (partially) diverging interests and motivate, why a new approach to identity management and users privacy is necessary in this context. Based on requirements retrieved by real-life communities, we describe an architecture including privacy enhancing concepts and advanced privacy respecting advertising, which addresses such requirements. We further describe the architectures’ prototypical implementation, and present for the first time evaluation results based on user trials with two different mobile communities.
84|11||Implementing collaborative learning activities in the classroom supported by one-to-one mobile computing: A design-based process|Mobile devices such as PDAs, smartphones and tablet computers are becoming increasingly popular, setting out opportunities for new ways of communicating and collaborating. Research initiatives have ascertained the potential of mobile devices in education, and particularly, the benefits of incorporating them in the classroom for eliciting collaborative learning and active student participation. However, the development of technology-supported learning environments poses challenges to education researchers, practitioners, and software technologists in creating educational tools that respond to real needs of instructors and learners, meet clearly defined didactic purposes, and are practical for the intended audience. This article reports on a technology for facilitating the implementation of collaborative learning environments in the classroom supported by one-to-one mobile computing. The approach encompasses a framework supporting the design and implementation of the mobile software, and a design-based process that guides interdisciplinary efforts utilizing the framework, towards creating effective pedagogical models based on collaborative learning. The proposed design-based process allowed us to develop pedagogical models that respond to real needs of learners and instructors, where development is grounded on rigorous scientific research, allowing to reuse both knowledge and software, and showing an improvement of the mobile software built based on continuous experimentation and evaluation. A case study illustrating the application of the technology is presented and plans for future research are discussed.
84|11||Mobile applications in an aging society: Status and trends|Today, many countries, including several European states, the USA, and Japan, are aging; both the number and the percentage of elderly people are increasing. To create a cohesive and inclusive intergenerational society, technological products and services must be adapted to the needs and preferences of these citizens. Mobile phones are promising tools to improve the quality of life for the elderly. This work presents a review of the status of mobile functionalities and applications that can satisfy the requirements and needs of older people and improve their quality of life. This analysis of the state of the art enables us to identify the strengths and weaknesses of the current systems as well as discover trends and promising future lines of research. This paper outlines several needs that should be met to improve the quality of research in this area. This work provides a basis for researchers, designers, and mobile phone service providers to think about the existing needs of the elderly, the developing trends in the field and the opportunities that mobile applications offer to improve the quality of life of the elderly and to support a cohesive and inclusive society.
84|11||Implementing multiplayer pervasive installations based on mobile sensing devices: Field experience and user evaluation from a public showcase|In this work we discuss Fun in Numbers, a software platform for implementing multiplayer games and interactive installations, that are based on the use of ad hoc mobile sensing devices. We utilize a detailed log of a three-day long public showcase as a basis to discuss the implementation issues related to a set of games and installations, which are examples of this unique category of applications, utilizing a blend of technologies. We discuss their fundamental concepts and features, also arguing that they have many aspects and potential uses. The architecture of the platform and implementation details are highlighted in this work, along with detailed descriptions of the protocols used. Our experiments shed light on a number of key issues, such as network scaling and real-time performance, and we provide experiments regarding cross-layer software issues. We additionally provide data showing that such games and installations can be efficiently supported by our platform, with as many as 50 concurrent players in the same physical space. These results are backed up by a user evaluation study from a large sample of 136 visitors, which shows that such applications can be seriously fun.
84|11||Measuring air quality in city areas by vehicular wireless sensor networks|This paper considers a micro-climate monitoring scenario, which usually requires deploying a large number of sensor nodes to capture environmental information. By exploiting vehicular sensor networks (VSNs), it is possible to equip fewer nodes on cars to achieve fine-grained monitoring. Specifically, when a car is moving, it could conduct measurements at different locations, thus collecting lots of sensing data. To achieve this goal, this paper proposes a VSN architecture to collect and measure air quality for micro-climate monitoring in city areas, where nodes’ mobility may be uncontrollable (such as taxis). In the proposed VSN architecture, we address two network-related issues: (1) how to adaptively adjust the reporting rates of mobile nodes to satisfy a target monitoring quality with less communication overhead and (2) how to exploit opportunistic communications to reduce message transmissions. We propose algorithms to solve these two issues and verify their performances by simulations. In addition, we also develop a ZigBee-based prototype to monitor the concentration of carbon dioxide (CO2) gas in city areas.
84|11||Score optimization and template updating in a biometric technique for authentication in mobiles based on gestures|This article focuses on the evaluation of a biometric technique based on the performance of an identifying gesture by holding a telephone with an embedded accelerometer in his/her hand. The acceleration signals obtained when users perform gestures are analyzed following a mathematical method based on global sequence alignment. In this article, eight different scores are proposed and evaluated in order to quantify the differences between gestures, obtaining an optimal EER result of 3.42% when analyzing a random set of 40 users of a database made up of 80 users with real attempts of falsification. Moreover, a temporal study of the technique is presented leeding to the need to update the template to adapt the manner in which users modify how they perform their identifying gesture over time. Six updating schemes have been assessed within a database of 22 users repeating their identifying gesture in 20 sessions over 4 months, concluding that the more often the template is updated the better and more stable performance the technique presents.
84|11||Status and trends of mobile-health applications for iOS devices: A developer's perspective|Modern smart mobile devices offer media-rich and context-aware features that are highly useful for electronic-health (e-health) applications. It is therefore not surprising that these devices have gained acceptance as target devices for e-health applications, turning them into m-health (mobile-health) apps. In particular, many e-health application developers have chosen Apple's iOS mobile devices such as iPad, iPhone, or iPod Touch as the target device to provide more convenient and richer user experience, as evidenced by the rapidly increasing number of m-health apps in Apple's App Store. In this paper, the top two hundred of such apps from the App Store were examined from a developer's perspective to provide a focused overview of the status and trends of iOS m-health apps and an analysis of related technology, architecture, and user interface design issues. The top 200 apps were classified into different groups according to their purposes, functions, and user satisfaction. It was shown that although the biggest group of apps was medical information reference apps that were delivered from or related to medical articles, websites, or journals, mobile users disproportionally favored tracking tools. It was clear that m-health apps still had plenty of room to grow to take full advantage of unique mobile platform features and truly fulfill their potential. In particular, introduction of two- or three-dimensional visualization and context-awareness could further enhance m-health app's usability and utility. This paper aims to serve as a reference point and guide for developers and practitioners interested in using iOS as a platform for m-health applications, particular from the technical point of view.
84|12|http://www.sciencedirect.com/science/journal/01641212/84/12|A feature-based approach for modeling role-based access control systems|Role-based access control (RBAC) is a popular access control model for enterprise systems due to its flexibility and scalability. There are many RBAC features available, each providing a different function. Not all features are needed for an RBAC system. Depending on the requirements, one should be able to configure features on a need basis, which reduces development complexity and thus fosters development. However, there have not been suitable methods that enable systematic configuration of RBAC features for system development. This paper presents an approach for configuring RBAC features using a combination of feature modeling and UML modeling. Feature modeling is used for capturing the structure of features and configuration rules, and UML modeling is used for defining the semantics of features. RBAC features are defined based on design principles of partial inheritance and compatibility, which facilitates feature composition and verification. We demonstrate the approach using a banking application and present tool support developed for the approach.
84|12||An automated approach to reducing test suites for testing retargeted C compilers for embedded systems|With widespread use of embedded processors, there is an increasing need to develop compilers for them in a timely manner. Retargeting has been an effective approach to constructing new compilers by modifying the back-end of existing compilers. An efficient testing method for retargeted compilers using intermediate codes is proposed in this paper. The concepts of the intermediate-code-based approach are described, and it is demonstrated that the proposed approach is efficient and effective enough. We have developed an automated tool, PLOOSE, which generates test suites based on source-code coverage criteria and then reduces them based on intermediate-code coverage. In addition, case studies are presented which reveal that a test suite based on source codes can be considerably reduced using intermediate codes. Moreover, by mutation analysis, it was found that the size of the test suite has been reduced by over 90% on average, but the fault detection capability of the reduced test suites is still approximately 80% of that of the original test suites. In particular, the proposed reduction method can be useful for testing compilers in the early stage of the development of retargeted compilers.
84|12||Examining the influences of external expertise and in-house computer/IT knowledge on ERP system success|External expertise and adequate levels of internal computer skills and knowledge are essential factors that can contribute to the success of complex information technology (IT) systems, including enterprise resource planning (ERP). Studies examining the effects of external expertise and in-house or internal computer/IT knowledge on the success of ERP packages are rare. This present study was designed to fill this gap in research. A relevant research model was developed to test fifteen (15) hypothesized paths or relationships among the study's variables. Data was collected in a cross-sectional field survey of 109 firms in two European countries. The partial least squares (PLS) technique was used for data analysis. The PLS results supported eleven (11) out of the fifteen (15) hypotheses. Essentially, this research's results confirmed that external expertise (an exogenous factor) and internal computer/IT knowledge (endogenous factors) are pertinent to success enhancement of ERP system success for adopting organizations. The implications of the findings for both practice and research are discussed, and possible areas of future research identified.
84|12||Is there convergence in the field of UI generation?|For many software projects, the construction of the User Interface (UI) consumes a significant proportion of their development time. Any degree of automation in this area therefore has clear benefits. But it is difficult to achieve such automation in a way that will be widely adopted by industry because of the diversity of UIs, software architectures, platforms and development environments. In a previous article, the authors identified five key characteristics any UI generator would need in order to address this diversity. We asserted that, without these characteristics, a UI generator should not expect wide industry adoption or standardisation. We supported this assertion with evidence from industry adoption studies. A further source of validation would be to see if other research teams, who were also conducting industry field trials, were independently converging on this same set of characteristics. Conversely, it would be instructive if they were found to be converging on a different set of characteristics. In this article, the authors look for such evidence of convergence by interviewing the team behind one of the research community's most significant UI generators: Naked Objects. We observe strong signs of convergence, which we believe signal the beginning of a general purpose architecture for UI generation, one that both industry and the research community could standardise upon.
84|12||A fast, GPU based, dictionary attack to OpenPGP secret keyrings|We describe the implementation, based on the Compute Unified Device Architecture (CUDA) for Graphics Processing Units (GPU), of a novel and very effective approach to quickly test passphrases used to protect private keyrings of OpenPGP cryptosystems.
84|12||Modelling and analysis of pipelined circuit switching in interconnection networks with bursty traffic and hot-spot destinations|Pipelined circuit switching (PCS) that combines the advantages of both circuit switching and wormhole switching is an efficient method for passing messages in interconnection networks. Analytical modelling is a cost-effective tool and plays an important role in achieving a clear understanding of the network performance. However, most of the existing models for PCS are unable to capture the realistic nature of message behaviours generated by real-world applications, which have a significant impact on the design and performance of communication networks. This paper presents a new analytical model for PCS in interconnection networks in the presence of bursty and correlated message arrivals coupled with hot-spot destinations, which can capture the bursty message arrival process and non-uniform distribution of message destinations. Such a traffic pattern has been found in many practical communication environments. The accuracy of the proposed analytical model is validated through extensive simulation experiments. The model is then applied to investigate the effects of the bursty message arrivals and hot-spot destinations on the performance of interconnection networks with PCS.
84|12||Trust-based minimum cost opportunistic routing for Ad hoc networks|Recently, opportunistic routing has received much attention as a new design direction. It can exploit the wireless broadcast and more highly reliable opportunistic forwarding, so as to substantially increase the throughput of network. Due to dynamic topology, distributed collaboration, limited bandwidth and computing ability, the absence of enough physical protection in Ad hoc networks, opportunistic routing is vulnerable to attacks by malicious nodes. In order to alleviate the malicious behaviors, we incorporate the concept of trust to Ad hoc networks, build a simple trust model to evaluate neighbors’ forwarding behavior and apply this model to opportunistic routing for Ad hoc networks. A new trusted opportunistic forwarding model is proposed by choosing the trusted and highest priority candidate forwarder, then a trusted minimum cost routing algorithm (MCOR) is formally formulated, the correctness and effectiveness of this algorithm from theoretical analysis are also approved. Finally, MCOR algorithm is verified by simulation using nsclick software and compared its performance with the classic protocols: ExOR, TAODV and Watchdog-DSR. The simulation results show that MCOR scheme can detect and mitigate node misbehaviors. Furthermore, MCOR scheme outperforms the other protocols in terms of: throughput, delay, Expected ETX, security-gains and cost of routing.
84|12||Evaluating the impacts of dynamic reconfiguration on the QoS of running systems|A major challenge in dynamic reconfiguration of a running system is to understand in advance the impact on the system's Quality of Service (QoS). For some systems, any unexpected change to QoS is unacceptable. In others, the possibility of dissatisfaction increases due to the impaired performance of the running system or unpredictable errors in the resulting system. In general it is difficult to choose a reasonable reconfiguration approach to satisfy a particular domain application. Our investigation on this issue for dynamic approaches is four-fold. First, we define a set of QoS characteristics to identify the evaluation criteria. Second, we design a set of abstract reconfiguration strategies bringing existing and new approaches into a unified evaluation context. Third, we design a reconfiguration benchmark to expose a rich set of QoS problems. Finally, we test the reconfiguration strategies against the benchmark and evaluate the test results. The analysis of acquired results helps to understand dynamic reconfiguration approaches in terms of their impact on the QoS of running systems and possible enhancements for newer QoS capability.
84|12||Scalable and efficient web services composition based on a relational database|Recently, there has been growing interest in web services composition. Web services composition gives us a possibility to fulfil the user request when no single web service can satisfy the functionality required by the user. In this paper, we propose a new system called PSR for the scalable and efficient web services composition search using a relational database. In contrast to previous work, the PSR system pre-computes web services composition using joins and indices and also supports semantic matching of web services composition. We demonstrate that our pre-computing web services composition approach in RDBMS yields lower execution time for processing user queries despite of and shows good scalability when handling a large number of web services and user queries.
84|12||Performance evaluation of noncontiguous allocation algorithms for 2D mesh interconnection networks|Several noncontiguous allocation strategies have been proposed for 2D mesh-connected multicomputers. These allocation strategies differ in their ability to detect free submeshes and in the degree of contiguity that exists among the submeshes they allocate to the same job. The previous Adaptive Noncontiguous Allocation (ANCA) policy was evaluated based on a proposed formula that estimates the job execution time when the job is allocated noncontiguous submeshes. Using this formula, simulation results had shown that ANCA could outperform the preceding Multiple Buddy Strategy (MBS). However, the execution times of jobs under noncontiguous allocation depend on message sizes, the number of messages sent, message contention and distances messages traverse. In this paper, we evaluate ANCA for different communication patterns using an event-driven simulator operating at the flit level, which allows for a more realistic evaluation that takes into account the shape of allocation and contention among messages. Moreover, we compare the performance of ANCA with that of other noncontiguous allocation strategies, MBS, Greedy Available Busy List (GABL), and Paging(0). The results show that ANCA is inferior to the remaining strategies, and that GABL has the best performance results, expressed in terms of the average turnaround time and mean system utilization performance parameters.
84|12||A variable strength interaction test suites generation strategy using Particle Swarm Optimization|This paper highlights a novel strategy for generating variable-strength (VS) interaction test suites, called VS Particle Swarm Test Generator (VS-PSTG). As the name suggests, VS-PSTG adopts Particle Swarm Optimization to ensure optimal test size reduction. To determine its efficiency in terms of the size of the generated test suite, VS-PSTG was subjected to well-known benchmark configurations. Comparative results indicate that VS-PSTG gives competitive results as compared to existing strategies. An empirical case study was conducted on a non-trivial software system to show the applicability of the strategy and to determine the effectiveness of the generated test suites to detect faults.
84|12||Communication-efficient leader election in crashârecovery systems|This work addresses the leader election problem in partially synchronous distributed systems where processes can crash and recover. More precisely, it focuses on implementing the Omega failure detector class, which provides a leader election functionality, in the crash–recovery failure model. The concepts of communication efficiency and near-efficiency for an algorithm implementing Omega are defined. Depending on the use or not of stable storage, the property satisfied by unstable processes, i.e., those that crash and recover infinitely often, varies. Two algorithms implementing Omega are presented. In the first algorithm, which is communication-efficient and uses stable storage, eventually and permanently unstable processes agree on the leader with correct processes. In the second algorithm, which is near-communication-efficient and does not use stable storage, processes start their execution with no leader in order to avoid the disagreement among unstable processes, that will agree on the leader with correct processes after receiving a first message from the leader.
84|12||A high quality image sharing with steganography and adaptive authentication scheme|With the rapid growth of numerous multimedia applications and communications through Internet, secret image sharing has been becoming a key technology for digital images in secured storage and confidential transmission. However, the stego-images are obtained by directly replacing the least-significant-bit planes (LSB) of cover-images with secret data and authentication code in most schemes, which will result in the distortion of the stego-images. In this paper, we proposed a novel secret image sharing scheme by applying optimal pixel adjustment process to enhance the image quality under different payload capacity and various authentication bits conditions. The experimental results showed that the proposed scheme has improved the image quality of stego images by 4.71%, 9.29%, and 11.10%, as compared with the schemes recently proposed by Yang et al., Chang et al., and Lin and Tsai. We also provide several experiments to demonstrate the efficacy of authentication capability of the proposed scheme. In other words, our scheme maintains the secret image sharing and authentication ability while enhances the image quality.
84|12||A genetic algorithm for optimized feature selection with resource constraints in software product lines|Software product line (SPL) engineering is a software engineering approach to building configurable software systems. SPLs commonly use a feature model to capture and document the commonalities and variabilities of the underlying software system. A key challenge when using a feature model to derive a new SPL configuration is determining how to find an optimized feature selection that minimizes or maximizes an objective function, such as total cost, subject to resource constraints. To help address the challenges of optimizing feature selection in the face of resource constraints, this paper presents an approach that uses G enetic A lgorithms for optimized FE ature S election (GAFES) in SPLs. Our empirical results show that GAFES can produce solutions with 86–97% of the optimality of other automated feature selection algorithms and in 45–99% less time than existing exact and heuristic feature selection techniques.
84|12||Evolutionary generation of test data for many paths coverage based on grouping|Path-oriented test data generation is an important issue of software testing, but the efficiency of existing methods needs to be further improved. We focus on the problem of generating test data for many paths coverage, and present a method of evolutionary generation of test data for many paths coverage based on grouping. First, target paths are divided into several groups according to their similarities, and each group forms a sub-optimization problem, which transforms a complicated optimization problem into several simpler sub-optimization problems; then a domain-based fitness is designed when genetic algorithms are employed to solve these problems; finally, these sub-optimization problems are simplified along with the process of generating test data, hence improving the efficiency of generating test data. Having analyzed the performance of our method theoretically, we apply it in some typical programs under test, and compare it with some previous methods. The experimental results show that our method has advantage in the number of evaluations and uncovered target paths.
84|12||ReuseToolâAn extensible tool support for object-oriented framework reuse|Object-oriented frameworks have become a popular paradigm used to improve the software development lifecycle. They promote reuse by providing a semi-complete architecture that can be extended through an instantiation process to integrate the needs of the new software application. Instantiation processes are typically enacted in an ad-hoc manner, which may lead to tedious and error-prone procedures. This work leverages our previous work on the definition of RDL, a language to facilitate the description of instantiation process, and describe the ReuseTool, which is an extensible tool to execute RDL programs and assist framework reuse by manipulating UML Diagrams. The ReuseTool integrates a RDL Compiler and a Workflow Engine to control most of the activities required to extend a framework design and, therefore, incorporates application-specific needs. This work also describes how the tool can be extended to incorporate new reuse activities and provides information of its use based on an exploratory Case Study.
84|12||Checking enforcement of integrity constraints in database applications based on code patterns|Integrity constraints (including key, referential and domain constraints) are unique features of database applications. Integrity constraints are crucial for ensuring accuracy and consistency of data in a database. It is important to perform integrity constraint enforcement (ICE) at the application level to reduce the risk of database corruption. We have conducted an empirical analysis of open-source PHP database applications and found that ICE does not receive enough attention in real-world programming practice. We propose an approach for automatic detection of ICE violations at the application level based on identification of code patterns. We define four patterns that characterize the structures of code implementing integrity constraint enforcement. Violations of these patterns indicate the missing of integrity constraint enforcement. Our work contributes to quality improvement of database applications. Our work also demonstrates that it is feasible to effectively identify bugs or problematic code by mining code patterns in a specific domain/application area.
84|12||An empirical investigation on the reusability of design patterns and software packages|Nowadays open-source software communities are thriving. Successful open-source projects are competitive and the amount of source code that is freely available offers great reuse opportunities to software developers. Thus, it is expected that several requirements can be implemented based on open source software reuse. Additionally, design patterns, i.e. well-known solution to common design problems, are introduced as elements of reuse. This study attempts to empirically investigate the reusability of design patterns, classes and software packages. Thus, the results can help developers to identify the most beneficial starting points for white box reuse, which is quite popular among open source communities. In order to achieve this goal we conducted a case study on one hundred (100) open source projects. More specifically, we identified 27,461 classes that participate in design patterns and compared the reusability of each of these classes with the reusability of the pattern and the package that this class belongs to. In more than 40% of the cases investigated, design pattern based class selection, offers the most reusable starting point for white-box reuse. However there are several cases when package based selection might be preferable. The results suggest that each pattern has different level of reusability.
84|12||Delegatable secret handshake scheme|Secret handshake, which was introduced by Balfanz et al., is proposed for mutually anonymous authentication among untrusted parties. In this paper, a new secret handshake scheme is designed for supporting delegations. The definitions and security requirements are first formalized for delegatable secret handshakes. Based on a variant of message recoverable signature without public verification, a delegatable secret handshake scheme is presented. By assuming the intractability of the Discrete Logarithm Representation Problem and the q-Strong Diffie–Hellman Problem, our new scheme is proven secure under the random oracle model.
84|12||Interactive conditional proxy re-encryption with fine grain policy|Conditional proxy re-encryption (C-PRE) allows a semi-trusted proxy to convert a ciphertext satisfying one conditional set by sender into an encryption of the same message intended for a different recipient than the one that was originally intended to. In ISC 2009, Weng, Yang, Tang, Deng, and Bao proposed an efficient CCA secure C-PRE scheme, and left an open problem on how to construct CCA-secure C-PRE schemes supporting “OR” and “AND” gates over conditions. In this paper, we made the first attempt in constructing C-PRE schemes with richer policy, and hence addressing the problem raised by Weng et al. Nevertheless, our scheme is an interactive scheme. The ‘interactive setting’ used in our scheme refers to the case where the re-encryption key generation algorithm requires the involvement of the private key of the delegator and delegatee. As a consequence, we call our new cryptographic primitive as interactive conditional proxy re-encryption with fine grain policy (ICPRE-FG). This notion basically enhances the notion of PRE by enabling the features from the attribute-based encryption (ABE). That means, our ICPRE-FG has been constructed from a careful combination of the existing PRE and ABE techniques. In an ICPRE-FG system, each ciphertext is labeled by the delegator with a set of descriptive conditions and each re-encryption key is associated with an access tree that specifies which type of ciphertexts the key can re-encrypt. We formalize the security model of ICPRE-FG, and then we present a new and efficient construction of ICPRE-FG scheme with CCA-security under the well-studied assumption in the random oracle model.
84|12||Tensor Field Model for higher-order information retrieval|There is an implied assumption for keywords in the traditional Information Retrieval (IR) models: keywords are parallel to each other. In fact, there are some relations between terms in quite a few queries, and in these queries, perhaps one term is subordinate to another term according to the inner meanings of information needs. This is “Higher-order IR”(HIR) defined in this paper, and we call traditional IR “first-order IR” instead. Some research fields such as Public Opinion Analysis, Chain of Events Analysis and Trend Analysis which reflect the vague concept of HIR are all special form of HIR. Apparently, traditional IR models cannot deal with HIR directly. We need a new HIR model to represent and organize the documents, queries and relevance relationship between them. In this paper, we propose “Tensor Field Model” (TFM), and its perspectives are field theory in physics and multilinear algebra in maths. We construct the tensor representations of documents and queries in TFM, presenting some key concepts such as term field, tensor product of term array and term field constant. Empirical results show that TFM is appropriate for HIR theoretically and formally compared with traditional models which simplify the HIR problems as first-order IR problems to some extent.
84|12||Design of a Java spatial extension for relational databases|Jaspa (Java Spatial) is a novel spatial extension for relational database management systems (RDBMSs). It is the result of a research project that aims to accomplish two goals: firstly, to fill the absence in the Free and Open Source Software (FOSS) world of a solid Java-based alternative to PostGIS, the leading spatial extension written in C. Secondly, to exploit the advantages of Java and the Java geospatial libraries over C in terms of portability and easiness to extend. Java programming for geospatial purposes is a flowering field and similar solutions to Jaspa have recently appeared, but none of them can equate with PostGIS due to lack of functionalities. Jaspa currently implements almost all PostGIS functionality. The next step will be the enrichment of the software with more sophisticated features: storage of spatial data in a topological structure within the RDBMS, cluster tolerance and geodetic functionalities. Jaspa is being developed at the Department of Cartographic Engineering, Geodesy and Photogrammetry of the Universidad Politécnica de Valencia and it has been published under an Open Source license on the OSOR.eu repository. This paper has been written by its creators with the aim of introducing users to its main capabilities.
84|12||Zero-laxity based real-time multiprocessor scheduling|It has been widely studied how to schedule real-time tasks on multiprocessor platforms. Several studies have developed optimal scheduling policies for implicit deadline task systems. So far however, studies have failed to develop effective scheduling strategies for more general task systems such as constrained deadline tasks. We argue that a narrow focus on deadline satisfaction (urgency) is the primary reason for this lack of success. In particular, few studies have considered the impact on scheduling of the restriction that a job cannot simultaneously execute on more than one core (parallelism). In this paper we look at one such simple, but effective, characterization of urgency and parallelism – the zero laxity first policy (ZL policy). We study in detail how beneficial the ZL policy is to schedulability. We then develop an improved schedulability test for any algorithm that employs the ZL policy, and prove that the test dominates previously known tests. Our simulation results show that the improved ZL schedulability test outperforms the existing ones.
84|12||Understanding the relevance of micro-structures for design patterns detection|One important issue concerning design patterns in reverse engineering is their detection to support program comprehension, design recovery, system (re-)documentation, and software evolution. The objectives of this paper are to identify and analyze different types of building blocks of design patterns and to evaluate if the detection of these building blocks (called micro-structures) is relevant for the detection of occurrences of the design patterns. This analysis is useful to understand how the different types of micro-structures can be combined to better comprehend design patterns and to improve their detection. To achieve the objectives, the paper provides a description of different micro-structures, an analysis of their relevance in different design motifs, and a statistical analysis on the number and types of micro-structures present in different design patterns. Finally, we investigate if the detection of some design patterns can be performed only through the detection of a combined set of micro-structures, or other techniques should be exploited.
84|12||P2P-based multidimensional indexing methods: A survey|P2P-based multidimensional index (MI) is a hotspot which absorbs many researchers to dedicate them into. However, no summarization or review on this technology has been made at present. To the best of our knowledge, this is the first work on reviewing P2P-based MI. This paper innovatively adopts visualization technique to show the research groups and then analyzes investigating style of research groups. Based on evolution of P2P-based MI inheriting from centralized MI and P2P, we divide P2P-based MI methods into 4 categories: extending centralized MI, extending P2P, combining centralized MI and P2P, and miscellaneous. For each category, the paper selects classical techniques and describes them in detail. This is the first time of doing the classification job over massive related works. Finally, load balancing and update strategies are described and discussed for they are important factors related to performance. We believe many researchers will get benefits from our work for further studies.
84|12||How do we measure and improve the quality of a hierarchical ontology?|Hierarchical ontologies enable organising information in a human–machine understandable form, but constructing them for reuse and maintainability remains difficult. Often supporting tools available lack formal methodological underpinning and their developers are not supported by any concomitant metrics. The paper presents a formal underpinning to provide quality metrics of a taxonomy hierarchical ontology and proposes a methodology for semi-automatic building of maintainable taxonomies. Users provide terms to be used to describe different ontological elements as well as their attributes and their ranges of values. The methodology uses the formalised metrics to assess the quality of the users input and proposes changes according to given quality constraints. The paper illustrates the metrics and the methodology in constructing and repairing two medium size well-known taxonomies.
84|12||Adjusting Fuzzy Similarity Functions for use with standard data mining tools|Data mining is crucial in many areas and there are ongoing efforts to improve its effectiveness in both the scientific and the business world. There is an obvious need to improve the outcomes of mining techniques such as clustering and other classifiers without abandoning the standard mining tools that are popular with researchers and practitioners alike. Currently, however, standard tools do not have the flexibility to control similarity relations between attribute values, a critical feature in improving mining-clustering results. The study presented here introduces the Similarity Adjustment Model (SAM) where adjusted Fuzzy Similarity Functions (FSF) control similarity relations between attribute values and hence ameliorate clustering results obtained with standard data mining tools such as SPSS and SAS. The SAM draws on principles of binary database representation models and employs FSF adjusted via an iterative learning process that yields improved segmentation regardless of the choice of mining-clustering algorithm. The SAM model is illustrated and evaluated on three common datasets with the standard SPSS package. The datasets were run with several clustering algorithms. Comparison of “Naïve” runs (which used original data) and “Fuzzy” runs (which used SAM) shows that the SAM improves segmentation in all cases.
84|2|http://www.sciencedirect.com/science/journal/01641212/84/2|Organizational structures supported by agent-oriented methodologies|Agent technology is a software paradigm that permits to implement large and complex distributed applications. In order to assist the development of multi-agent systems, agent-oriented methodologies (AOM) have been created in the last years to support modelling more and more complex applications. Even though agents are perceived as autonomous entities that act according to some objectives, they are also members of a society, and have to exchange information with other agents and maintain some relationships at an organizational level. Modern AOMs should be able to capture and represent organizational structures, defining interaction and collaboration patterns between agents, their internal roles and dependencies between groups of agents. This paper analyses the most notable AOMs, paying attention to the support and possibilities that they offer for modelling organizational structures with different levels of complexity. This work can help developers to select the most appropriate methodology taking into account the social and organizational requirements of the multi-agent system to be deployed.
84|2||Toward architecture-based context-aware deployment and adaptation|Software systems are increasingly expected to dynamically self-adapt to the changing environments. One of the main adaptation mechanisms is dynamic recomposition of application components. This paper addresses the key issues that arise when context knowledge is used to steer the run-time (re)composition process so as to match the new environmental conditions. In order to integrate such knowledge into this process, A Continuous Context-Aware Deployment and Adaptation (ACCADA) framework is proposed. To support run-time component composition, the essential runtime abstractions of the underlying component model are studied. By using a layered modeling approach, our framework gradually incorporates design-time as well as run-time knowledge into the component composition process. Service orientation is employed to facilitate the changes of adaptation policy. Results show that our framework has significant advantages over traditional approaches in light of flexibility, resource usage and lines of code. Although our experience was based on the OSGi middleware, we believe our findings to be general to architecture-based management systems using reflective component models.
84|2||One-time signature scheme from syndrome decoding over generic error-correcting codes|We describe a one-time signature scheme based on the hardness of the syndrome decoding problem, and prove it secure in the random oracle model. Our proposal can be instantiated on general linear error correcting codes, rather than restricted families like alternant codes for which a decoding trapdoor is known to exist.
84|2||A context-aware reflective middleware framework for distributed real-time and embedded systems|Context-aware reflective middleware (CARM), which supports application reconfiguration, has been an appealing technique for building distributed real-time and embedded (DRE) systems as it can adapt their behaviors to changing environments at run time. However, existing CARM frameworks impose dependence restrictions and reconfiguration overhead, which makes the reconfiguration time of these frameworks too long (normally in the range of seconds or more) to satisfy the stringent real-time requirements of DRE systems. To improve the reconfiguration efficiency for supporting DRE systems, we have designed a new CARM framework – MARCHES (Middleware for Adaptive Robust Collaborations across Heterogeneous Environments and Systems), which offers an original structure of multiple component chains to reduce local behavior change time and a novel synchronization protocol using active messages to reduce distributed behavior synchronization time. MARCHES uses a layered architecture and provides both component-level and system-level reflection to incorporate standard components, a hierarchical event notification model to evaluate contexts, and a lightweight XML-based script language to describe and manage adaptation policies. The MARCHES framework and supported applications have been implemented on PC and PDA platforms. Based on a novel theoretical model, we have analyzed the reconfiguration efficiency of MARCHES and compared it with those of peer CARM frameworks: MobiPADS and CARISMA. Quantitative empirical results show that the reconfiguration time of MARCHES is reduced from seconds to hundreds of microseconds. Evaluations demonstrate that MARCHES is robust, scalable and generates a small memory footprint, which makes it suitable for supporting DRE systems.
84|2||Cryptanalysis of an (hierarchical) identity based parallel key-insulated encryption scheme|Recently, Ren and Gu (2010) proposed an identity-based parallel key-insulated encryption (IBPKIE) scheme, and further extended their IBPKIE scheme to a hierarchical identity-based parallel key-insulated encryption (HIBPKIE) scheme. They claimed that both schemes are secure against adaptive chosen-ciphertext attacks without random oracles. However, in this paper, by giving concrete attacks, we indicate that their schemes are even not secure against chosen-plaintext attacks.
84|2||Adjusting transport segmentation policy of DTN Bundle Protocol under synergy with lower layers|We assess Delay-Tolerant Network (DTN) performance in space under the scope of adjusting protocol data unit (PDU) size at various layers. We quantify the importance of combinatively adjusting size of DTN bundles, transport packets, and link frames. Through simulations, our paper reveals trade-offs that involve file delivery time, transmission effort of sending nodes, and memory resources release rate. Based on our findings, we propose a transport adaptation scheme that dynamically adjusts DTN bundle and transport packet size by means of heuristic search. To our knowledge, this is the first study to examine transport segmentation policy and interaction among various layers of the DTN protocol stack.
84|2||Antecedents to IT personnel's intentions to leave: A systematic literature review|This paper undertakes a systematic review to gain insight into existing studies on the turnover of information technology (IT) personnel. Our systematic review of 72 studies from 1980 to 2008 examines the background and trend of research into IT personnel's intentions to leave their workplaces, in addition to providing a taxonomy of the determinants of their intentions to quit as captured in IT literature. We note a huge growth in the number of academic papers on the topic since 1998. Moreover, most of the research on IT turnover has been undertaken in North America, followed by Asia. Based on the 72 extracted studies, we found a total of 70 conceptually distinct IT turnover drivers. We classified them into the 5 broad categories of individual, organisational, job-related, psychological, and environmental, each containing three to four sub-categories. Finally, this paper presents insightful recommendations for IT practitioners as well as for the research community.
84|2||Taxonomy and classification of automatic monitoring of program security vulnerability exploitations|Software applications (programs) are implemented in a wide variety of languages and run on different execution environments. Programs contain vulnerabilities which can be detected before their deployment. Nevertheless, there exist some program vulnerabilities, which do not surface until a program is operational. No matter how much effort has been put during the development phases, building large vulnerability-free programs has proven extremely difficult in practice. Given that, it is very important to have a tool that can be used for online monitoring of programs in the operational stage. The tool can help to mitigate the consequences of some vulnerability exploitations, by early detection of attacks at runtime. Currently, many monitoring approaches have been proposed and applied in practice. However, there is no classification of these approaches to understand their common characteristics and limitations. In this paper, we present a taxonomy and classification of the state of the art approaches employed for monitoring program vulnerability exploitations (or attacks). We first classify the existing approaches based on a set of characteristics which are common in online attack detection approaches. Then, we present a taxonomy by classifying the approaches based on monitoring aspects that primarily differentiate among the approaches. We also discuss open issues and future research direction in the area of program vulnerability exploitation monitoring. The study will enable practitioners and researchers to differentiate among existing monitoring approaches. It will provide a guideline to consider the desired characteristics while developing monitoring approaches.
84|2||Analogy-based software effort estimation using Fuzzy numbers|Early stage software effort estimation is a crucial task for project bedding and feasibility studies. Since collected data during the early stages of a software development lifecycle is always imprecise and uncertain, it is very hard to deliver accurate estimates. Analogy-based estimation, which is one of the popular estimation methods, is rarely used during the early stage of a project because of uncertainty associated with attribute measurement and data availability.
84|2||Key activities for product derivation in software product lines|More and more organizations adopt software product lines to leverage extensive reuse and deliver a multitude of benefits such as increased quality and productivity and a decrease in cost and time-to-market of their software development. When compared to the vast amount of research on developing product lines, relatively little work has been dedicated to the actual use of product lines to derive individual products, i.e., the process of product derivation. Existing approaches to product derivation have been developed independently for different aims and purposes. While the definition of a general approach applicable to every domain may not be possible, it would be interesting for researchers and practitioners to know which activities are common in existing approaches, i.e., what are the key activities in product derivation. In this paper we report on how we compared two product derivation approaches developed by the authors in two different, independent research projects. Both approaches independently sought to identify product derivation activities, one through a process reference model and the other through a tool-supported derivation approach. Both approaches have been developed and validated in research industry collaborations with different companies. Through the comparison of the approaches we identify key product derivation activities. We illustrate the activities’ importance with examples from industry collaborations. To further validate the activities, we analyze three existing product derivation approaches for their support for these activities. The validation provides evidence that the identified activities are relevant to product derivation and we thus conclude that they should be considered (e.g., as a checklist) when developing or evaluating a product derivation approach.
84|2||Bridging metamodels and ontologies in software engineering|Over the last several years, metamodels and ontologies have been developed in parallel isolation. Ontological thinking, largely from the research field of artificial intelligence, has been increasingly investigated by software engineering researchers, more familiar with the idea of a metamodel. Here, we investigate the literature on both metamodelling and ontologies in order to identify ways in which they can be made compatible and linked in such a way as to benefit both communities and create a contribution to a coherent underpinning theory for software engineering. Analysis of a large number of theoretical and semi-theoretical approaches using as a framework a multi-level modelling construct identifies strengths, weaknesses, incompatibilities and inconsistencies within the extant literature. A metamodel deals with conceptual definitions while an ontology deals with real-world descriptors of business entities and is thus better named “domain ontology”. A specific kind of ontology (foundational or high-level) provides “metalevel” concepts for the domain ontologies. In other words, a foundational ontology may be used at the same abstraction level as a metamodel and a domain ontology at the same abstraction level as a (design) model, with each pair linked via an appropriate semantic mapping.
84|2||Distributed adaptive top-k monitoring in wireless sensor networks|Top-k monitoring queries are useful in many wireless sensor network applications. A query of this type continuously returns a list of k ordered nodes with the highest (or lowest) sensor readings. To process these queries, a well-known approach is to install a filter at each sensor node to avoid unnecessary transmissions of sensor readings. In this paper, we propose a new top-k monitoring method, named Distributed Adaptive Filter-based Monitoring. In this method, we first propose a new query reevaluation algorithm that works distributedly in the network to reduce the communication cost of sending probe messages. Then, we present an adaptive filter updating algorithm which is based on predicted benefits to lower down the transmission cost of sending updated filters to the sensor nodes. Experimental results on real data traces show that our proposed method performs much better than the other existing methods in terms of both network lifetime and average energy consumption.
84|2||User requirements modeling and analysis of software-intensive systems|The increasing complexity of software systems makes Requirements Engineering activities both more important and more difficult. This article is about user requirements development, mainly the activities of documenting and analyzing user requirements for software-intensive systems. These are modeling activities that are useful for further Requirements Engineering activities. Current techniques for requirements modeling present a number of problems and limitations. Based on these shortcomings, a list of requirements for requirements modeling languages is proposed. The proposal of this article is to show how some extensions to SysML diagrams and tables can fulfill most of these requirements. The approach is illustrated by a list of user requirements for a Road Traffic Management System.
84|3|http://www.sciencedirect.com/science/journal/01641212/84/3|Medical image security and EPR hiding using Shamir's secret sharing scheme|Medical applications such as telediagnosis require information exchange over insecure networks. Therefore, protection of the integrity and confidentiality of the medical images is an important issue. Another issue is to store electronic patient record (EPR) in the medical image by steganographic or watermarking techniques. Studies reported in the literature deal with some of these issues but not all of them are satisfied in a single method. A medical image is distributed among a number of clinicians in telediagnosis and each one of them has all the information about the patient's medical condition. However, disclosing all the information about an important patient's medical condition to each of the clinicians is a security issue. This paper proposes a (k, n) secret sharing scheme which shares medical images among a health team of n clinicians such that at least k of them must gather to reveal the medical image to diagnose. Shamir's secret sharing scheme is used to address all of these security issues in one method. The proposed method can store longer EPR strings along with better authenticity and confidentiality properties while satisfying all the requirements as shown in the results.
84|3||A novel statistical time-series pattern based interval forecasting strategy for activity durations in workflow systems|Forecasting workflow activity durations is of great importance to support satisfactory QoS in workflow systems. Traditionally, a workflow system is often designed to facilitate the process automation in a specific application domain where activities are of the similar nature. Hence, a particular forecasting strategy is employed by a workflow system and applied uniformly to all its workflow activities. However, with newly emerging requirement to serve as a type of middleware services for high performance computing infrastructures such as grid and cloud computing, more and more workflow systems are designed to be general purpose to support workflow applications from many different domains. Due to such a problem, the forecasting strategies in workflow systems must adapt to different workflow applications which are normally executed repeatedly such as data/computation intensive scientific applications (mainly with long-duration activities) and instance intensive business applications (mainly with short-duration activities). In this paper, with a systematic analysis of the above issues, we propose a novel statistical time-series pattern based interval forecasting strategy which has two different versions, a complex version for long-duration activities and a simple version for short-duration activities. The strategy consists of four major functional components: duration series building, duration pattern recognition, duration pattern matching and duration interval forecasting. Specifically, a novel hybrid non-linear time-series segmentation algorithm is designed to facilitate the discovery of duration-series patterns. The experimental results on real world examples and simulated test cases demonstrate the excellent performance of our strategy in the forecasting of activity duration intervals for both long-duration and short-duration activities in comparison to some representative time-series forecasting strategies in traditional workflow systems.
84|3||Typical Virtual Appliances: An optimized mechanism for virtual appliances provisioning and management|A computing infrastructure requirement in the cloud computing environment can be specified and composed using virtual appliances, which forms the infrastructure-as-a-service (IaaS). Due to the diversity of user requirements, a large number of virtual appliances may be needed. We propose a mechanism called Typical Virtual Appliances (TVAs), an efficient method for providing virtual appliances. In this paper, we present the concept of TVAs and formulate it as an optimization problem with given constraints. With analysis of the software download logs of real web sites, we discover that the number of user requirements follows a quadratic polynomial distribution, and the user requirements are clustered in nature. According to this finding, we develop a clustering-based TVAs generation algorithm, and we show that this algorithm can achieve the optimal result. The clustering algorithm can generate TVAs, which can be transformed to other virtual appliances easily and efficiently. We further design a TVA Management System (TVAMS) to support this mechanism. The simulation results show that our method can meet most of the user requirements efficiently with low storage overhead.
84|3||Huffman-code strategies to improve MFCVQ-based reversible data hiding for VQ indexes|Data hiding, which embeds secret data into cover media, is one type of technology used to achieve the multimedia security. A reversible data hiding method has the characteristic that the cover media can be completely reconstructed after secret data are extracted. Recently, some reversible data hiding schemes have focused on the vector quantization (VQ)-compressed format. Using the modified fast correlation vector quantization (MFCVQ) concept, Lu et al. proposed a reversible data hiding scheme for VQ-index tables. In this paper, a new MFCVQ-based scheme is proposed. Specifically, our method will enlarge the embedding capacities by embedding multiple bits into a VQ index, thereby reducing the compressed bit rates by applying the Huffman-code concept and the 0-centered classification. Experimental results indicate that this method has greater pure embedding capacities and fewer compressed bit rates than that of previous MFCVQ-based methods.
84|3||Identifying Extract Class refactoring opportunities using structural and semantic cohesion measures|Approaches for improving class cohesion identify refactoring opportunities using metrics that capture structural relationships between the methods of a class, e.g., attribute references. Semantic metrics, e.g., C3 metric, have also been proposed to measure class cohesion, as they seem to complement structural metrics. However, until now semantic relationships between methods have not been used to identify refactoring opportunities. In this paper we propose an Extract Class refactoring method based on graph theory that exploits structural and semantic relationships between methods. The empirical evaluation of the proposed approach highlighted the benefits provided by the combination of semantic and structural measures and the potential usefulness of the proposed method as a feature for software development environments.
84|3||A systematic literature review of software quality cost research|Software quality costs have not received as much attention from the research community as other economic aspects of software development. Over the last three decades, a number of articles on this topic have appeared in a range of journals, but comprehensive overviews of this body of research are not available.
84|3||Adaptive reversible image watermarking scheme|This paper presents an adaptive block sized reversible image watermarking scheme. A reversible watermarking approach recovers the original image from a watermarked image after extracting the embedded watermarks. Without loss of generality, the proposed scheme segments an image of size 2N × 2N adaptively to blocks of size 2L × 2L, where L starts from a user-defined number to 1, according to their block structures. If possible, the differences between central ordered pixel and other pixels in each block are enlarged to embed watermarks. The embedded quantity is determined by the largest difference in a block and watermarks are embedded into LSB bits of above differences. Experimental results show that the proposed adaptive block size scheme has higher capacity than conventional fixed block sized method.
84|3||Mining significant factors affecting the adoption of SaaS using the rough set approach|Despite that Software as a Service (SaaS) seems to be the most tempting solution among different types of cloud services, yet it has not been adopted to-date with as much alacrity as was originally expected. A variety of factors may influence the adoption of SaaS solutions. The objective of this study is thus to explore the significant factors affecting the adoption of SaaS for vendors and enterprise users. An analytical framework is proposed containing two approaches—Technology Acceptance Model (TAM) and Rough Set Theory (RST). An empirical study on the IT/MIS enterprises in Taiwan is carried out. The results have revealed a considerable amount of meaningful information, which not only facilitates the SaaS vendors to grasp users’ needs and concerns about SaaS adoption, but also helps the managers to introduce effective marketing strategies and actions to promote the growth of SaaS market. Based on the findings, some managerial implications are discussed.
84|3||A comparison of deterministic and probabilistic methods for indoor localization|Received signal strength indication fingerprinting (RSSIF) is an indoor localization technique that exploits the prevalence of wireless local area networks (WLANs). Past research into RSSIF systems has seen the development of a number of algorithmic methods that provide effective indoor positioning. A key limitation, however, is that the performance of these methods is heavily dependent on practical implementation parameters and the nature of the test-bed environment. As a result, past research has tend to only compare algorithms of the same paradigm using a specific test-bed, and thus making it difficult to judge and compare their performance objectively. There is, therefore, a critical need for a study that addresses this gap in the literature. To this end, this paper compares a range of RSSIF methods, drawn from both probabilistic and deterministic paradigms, on a common test-bed. We evaluate their localization efficiency and accuracy, and also propose a number of improvements and modifications. In particular, we report on the impact of dense and transient access points (APs) – two problems that stem from the popularity of WLANs. Our results show methods that average the distance to the k nearest neighbors in signal space perform well with reduced dimensions. Moreover, we show the benefits of using the standard deviation of RSSI values to exclude transient APs. Other than that, we outline a shortcoming of the Bayesian algorithm in uncontrolled environments with highly variable APs and RSSI values, and propose an extension that uses a mode filter to restore its accuracy with increasing samples.
84|3||Missing data imputation by utilizing information within incomplete instances|This paper proposes to utilize information within incomplete instances (instances with missing values) when estimating missing values. Accordingly, a simple and efficient nonparametric iterative imputation algorithm, called the NIIA method, is designed for iteratively imputing missing target values. The NIIA method imputes each missing value several times until the algorithm converges. In the first iteration, all the complete instances are used to estimate missing values. The information within incomplete instances is utilized since the second imputation iteration. We conduct some experiments for evaluating the efficiency, and demonstrate: (1) the utilization of information within incomplete instances is of benefit to easily capture the distribution of a dataset; and (2) the NIIA method outperforms the existing methods in accuracy, and this advantage is clearly highlighted when datasets have a high missing ratio.
84|3||Oblivious transfer with timed-release receiverâs privacy|Oblivious transfer (OT) is an important primitive used in many cryptographic protocols. According to the traditional OT’s security requirements, the privacy of the receiver is protected forever, which, however, limits the applications of OT in some areas. In this paper, we propose a new OT with timed-release receiver’s privacy in which the sender has N messages, of which the receiver can choose to receive one or more, in such a way that (a) the receiver only learns about the requested messages, and (b) the sender learns the receiver’s selections after a designated time. Thus the receiver’s privacy is just protected within a period of time. Besides, we also present a concrete scheme combining Tzeng’s OT scheme with Casassa Mont et al.’s timed-release encryption scheme.
84|3||Resource discovery in a Grid system: Directing requests to trustworthy virtual organizations based on global trust values|This paper studies the resource discovery problem in a Grid system, in which global trust values play a crucial role. The proposed mechanism suggests that routers and resources comprise virtual organizations (VOs) within a Grid system, where a router controls locally a number of resources in each virtual organization. Global trust values are assigned to the system’s VOs. These trust values show whether a VO and subsequently its local resources are trustworthy or not. Our primary goal is to discover the appropriate resource for a specific request and then effectively direct this request to a trustworthy VO that controls locally the appropriate resource. Furthermore, the trust-aware resource discovery mechanism also manages the cases of dynamic changes in the trustworthiness of VOs. For instance, VOs that in the past were untrustworthy could now be trustworthy. The proposed mechanism is capable of detecting these dynamic changes, so that the directing of requests occurs in an up-to-date way. Finally, this paper presents the performance evaluation of the proposed trust-aware resource discovery mechanism by providing a number of simulation tests in Grid systems of different sizes.
84|3||Software engineering education: A study on conducting collaborative senior project development|Project and teamwork training is recognized as an important aspect in software engineering (SE) education. Senior projects, which often feature industrial involvement, serve the function of a ‘capstone course’ in SE curricula, by offering comprehensive training in collaborative software development. Given the characteristics of student team projects and the social aspects of software development, instructional issues in such a course must include: how to encourage teamwork, how to formalize and streamline stakeholder participation, and how to monitor students’ work, as well as sustain their desired collaborative effort throughout the development. In this paper, we present an exploratory study which highlights a particular case and introduces the meetings-flow approach. In order to investigate how this approach could contribute to the project's results, we examined its quantitative benefits in relation to the development of the project. We also conducted focus group interviews to discuss the humanistic findings and educational effects pertaining to this approach.
84|3||A novel general framework for automatic and cost-effective handling of recoverable temporal violations in scientific workflow systems|Due to the complex nature of scientific workflow environments, temporal violations often take place and may severely reduce the timeliness of the execution's results. To handle temporal violations in an automatic and cost-effective fashion, two interdependent fundamental issues viz. the definition of fine-grained recoverable temporal violations and the design of light-weight effective exception handling strategies need to be resolved. However, most existing works study them separately without defining a comprehensive framework. To address such a problem, with the probability based temporal consistency model which defines the range of recoverable temporal violations, a novel general automatic and cost-effective exception handling framework is proposed in this paper where fine-grained temporal violations are defined based on the empirical function for the capability lower bounds of the exception handling strategies. To serve as a representative case study, a concrete example exception handling framework which consists of three levels of fine-grained temporal violations and their corresponding exception handling strategies is presented. The effectiveness of the example framework is evaluated by large scale simulation experiments conducted in the SwinDeW-G scientific grid workflow system. The experimental results demonstrate that the example framework can significantly reduce the overall average violation rates of local temporal constraints and global temporal constraints to 0.127% and 0.167% respectively.
84|3||Self-adapting workflow reconfiguration|Because web services are highly interoperable, they are capable of providing uniform access to underlying technologies, allowing developers to choose between competing services. Workflow languages, such as BPEL, compose and sequence Web service invocations resulting in meaningful, and sometimes, repeated tasks. Their prevalence means there may be multiple Web services that perform the same operation with some better than others depending on the situation. Their potential for being unavailable at critical workflow execution times forces a reliance on such redundant services. One remedy for unavailability and situational awareness constraints is using quality of service factors and user-directed preferences to assign priorities to workflows and services to perform run-time replacement. In this paper we describe a novel approach to self-adapting workflow reconfiguration. We discuss the implementation of our approach embodied by the Next-generation Workflow Toolkit that supports runtime workflow reconfiguration using BPEL with a commercial workflow engine. A key design feature is the decoupling of user-directed changes regarding service priority from the actual workflow execution, allowing NeWT to effectively manage and recover from workflow changes at any time. We evaluate NeWT by comparing the same example across multiple commercial systems that claim reconfiguration capabilities.
84|3||Publisher's note|
84|3||Publisher's note|
84|4|http://www.sciencedirect.com/science/journal/01641212/84/4|Special issue on the best papers of QSIC 2009|
84|4||An approach to analyzing the software process change impact using process slicing and simulation|When a software process is changed, a project manager needs to perform two types of change impact analysis activities: one for identifying the affected elements of a software process which is affected by the change and the other for analyzing the quantitative impact of the change on the project performance. We propose an approach to obtain the affected elements of a software process using process slicing and developing a simulation model based on the affected elements to quantitatively analyzing the change using simulation. We suggest process slicing to obtain the elements affected by the change. Process slicing identifies the affected elements of a software process using a process dependency model. The process dependency model contains activity control dependencies, artifact information dependencies, and role replacement dependencies. We also suggest transformation algorithms to automatically derive the simulation model from the process model containing the affected elements. The quantitative analysis can be performed by simulating the simulation model. In addition, we provide the tool to support our approach. We perform a case study to validate the usefulness of our approach. The result of the case study shows that our approach can reduce the effort to identify the elements affected by changes and examine alternatives for the project.
84|4||Testing and validating machine learning classifiers by metamorphic testing|Machine learning algorithms have provided core functionality to many application domains – such as bioinformatics, computational linguistics, etc. However, it is difficult to detect faults in such applications because often there is no “test oracle” to verify the correctness of the computed outputs. To help address the software quality, in this paper we present a technique for testing the implementations of machine learning classification algorithms which support such applications. Our approach is based on the technique “metamorphic testing”, which has been shown to be effective to alleviate the oracle problem. Also presented include a case study on a real-world machine learning application framework, and a discussion of how programmers implementing machine learning algorithms can avoid the common pitfalls discovered in our study. We also conduct mutation analysis and cross-validation, which reveal that our method has high effectiveness in killing mutants, and that observing expected cross-validation result alone is not sufficiently effective to detect faults in a supervised classification program. The effectiveness of metamorphic testing is further confirmed by the detection of real faults in a popular open-source classification program.
84|4||BDTEX: A GQM-based Bayesian approach for the detection of antipatterns|The presence of antipatterns can have a negative impact on the quality of a program. Consequently, their efficient detection has drawn the attention of both researchers and practitioners. However, most aspects of antipatterns are loosely specified because quality assessment is ultimately a human-centric process that requires contextual data. Consequently, there is always a degree of uncertainty on whether a class in a program is an antipattern or not. None of the existing automatic detection approaches handle the inherent uncertainty of the detection process. First, we present BDTEX (Bayesian Detection Expert), a Goal Question Metric (GQM) based approach to build Bayesian Belief Networks (BBNs) from the definitions of antipatterns. We discuss the advantages of BBNs over rule-based models and illustrate BDTEX on the Blob antipattern. Second, we validate BDTEX with three antipatterns: Blob, Functional Decomposition, and Spaghetti code, and two open-source programs: GanttProject v1.10.2 and Xerces v2.7.0. We also compare the results of BDTEX with those of another approach, DECOR, in terms of precision, recall, and utility. Finally, we also show the applicability of our approach in an industrial context using Eclipse JDT and JHotDraw and introduce a novel classification of antipatterns depending on the effort needed to map their definitions to automatic detection approaches.
84|4||Simultaneous debugging of software faults|(Semi-)automated diagnosis of software faults can drastically increase debugging efficiency, improving reliability and time-to-market. Current automatic diagnosis techniques are predominantly of a statistical nature and, despite typical defect densities, do not explicitly consider multiple faults, as also demonstrated by the popularity of the single-fault benchmark set of programs. We present a reasoning approach, called Zoltar-M(ultiple fault), that yields multiple-fault diagnoses, ranked in order of their probability. Although application of Zoltar-M to programs with many faults requires heuristics (trading-off completeness) to reduce the inherent computational complexity, theory as well as experiments on synthetic program models and multiple-fault program versions available from the software infrastructure repository (SIR) show that for multiple-fault programs this approach can outperform statistical techniques, notably spectrum-based fault localization (SFL). As a side-effect of this research, we present a new SFL variant, called Zoltar-S(ingle fault), that is optimal for single-fault programs, outperforming all other variants known to date.
84|4||On the estimation of adequate test set size using fault failure rates|Test set size in terms of the number of test cases is an important consideration when testing software systems. Using too few test cases might result in poor fault detection and using too many might be very expensive and suffer from redundancy. We define the failure rate of a program as the fraction of test cases in an available test pool that result in execution failure on that program. This paper investigates the relationship between failure rates and the number of test cases required to detect the faults. Our experiments based on 11 sets of C programs suggest that an accurate estimation of failure rates of potential fault(s) in a program can provide a reliable estimate of adequate test set size with respect to fault detection and should therefore be one of the factors kept in mind during test set construction. Furthermore, the model proposed herein is fairly robust to incorrect estimations in failure rates and can still provide good predictive quality. Experiments are also performed to observe the relationship between multiple faults present in the same program using the concept of a failure rate. When predicting the effectiveness against a program with multiple faults, results indicate that not knowing the number of faults in the program is not a significant concern, as the predictive quality is typically not affected adversely.
84|4||XML-manipulating test case prioritization for XML-manipulating services|A web service may evolve autonomously, making peer web services in the same service composition uncertain as to whether the evolved behaviors are compatible with its original collaborative agreement. Although peer services may wish to conduct regression testing to verify the agreed collaboration, the source code of the former service may be inaccessible to them. Owing to the black-box nature of peer services, traditional code-based approaches to regression testing are inapplicable. In addition, traditional techniques assume that a regression test suite for verifying a web service is available. The location to store a regression test suite is also a problem. On the other hand, we note that the rich interface specifications of a web service provide peer services with a means to formulate black-box testing strategies. In this paper, we provide a strategy for black-box service-oriented testing. We also formulate new test case prioritization strategies using tags embedded in XML messages to reorder regression test cases, and reveal how the test cases use the interface specifications of web services. We experimentally evaluate the effectiveness of these black-box strategies in revealing regression faults in modified WS-BPEL programs. The results show that the new techniques can have a high chance of outperforming random ordering. Moreover, our experiment shows that prioritizing test cases based on WSDL tag coverage can achieve a smaller variance than that based on the number of tags in XML messages in regression test cases, even though their overall fault detection rates are similar.
84|4||Selection of third party software in Off-The-Shelf-based software developmentâAn interview study with industrial practitioners|The success of software development using third party components highly depends on the ability to select a suitable component for the intended application. The evidence shows that there is limited knowledge about current industrial OTS selection practices. As a result, there is often a gap between theory and practice, and the proposed methods for supporting selection are rarely adopted in the industrial practice. This paper's goal is to investigate the actual industrial practice of component selection in order to provide an initial empirical basis that allows the reconciliation of research and industrial endeavors. The study consisted of semi-structured interviews with 23 employees from 20 different software-intensive companies that mostly develop web information system applications. It provides qualitative information that help to further understand these practices, and emphasize some aspects that have been overlooked by researchers. For instance, although the literature claims that component repositories are important for locating reusable components; these are hardly used in industrial practice. Instead, other resources that have not received considerable attention are used with this aim. Practices and potential market niches for software-intensive companies have been also identified. The results are valuable from both the research and the industrial perspectives as they provide a basis for formulating well-substantiated hypotheses and more effective improvement strategies.
84|4||A policy-based publish/subscribe middleware for sense-and-react applications|With the inclusion of actuators on wireless nodes, Wireless Sensor Networks (WSNs) are starting to change from sense-and-report platforms to sense-and-react platforms. Applications for such platforms are characterised by actuator nodes that are able to react to data collected by sensor nodes. Sensor and actuator nodes use a variety of interactions, for example, intra-node, inter-node (1-hop to n-hop), and global (all nodes). As a result, the functionality that coordinates the activities of the different nodes towards common goals has to be efficiently distributed in the WSN itself. In addition, multiple sense-and-react applications are being deployed within the same WSN, with each application characterised by different requirements and constraints. The design and implementation of these applications is becoming an increasingly complex task that would benefit from new approaches.
84|4||Bringing white-box testing to Service Oriented Architectures through a Service Oriented Approach|The attractive feature of Service Oriented Architecture (SOA) is that pieces of software conceived and developed by independent organizations can be dynamically composed to provide richer functionality. The same reasons that enable flexible compositions, however, also prevent the application of some traditional testing approaches, making SOA validation challenging and costly. Web services usually expose just an interface, enough to invoke them and develop some general (black-box) tests, but insufficient for a tester to develop an adequate understanding of the integration quality between the application and the independent web services. To address this lack we propose an approach that makes web services more transparent to testers through the addition of an intermediary service that provides coverage information. The approach, named Service Oriented Coverage Testing (SOCT), provides testers with feedback about how much a service is exercised by their tests without revealing the service internals. In SOCT, testing feedback is offered itself as a service, thus preserving SOA founding principles of loose coupling and implementation neutrality. In this paper we motivate and define the SOCT approach, and implement an instance of it. We also perform a study to asses SOCT feasibility and provide a preliminary evaluation of its viability and value.
84|4||A data hiding scheme using the varieties of pixel-value differencing in multimedia images|In this paper, a capacity promoting technique is proposed for embedding data in an image using pixel-value differencing (PVD). The PVD scheme embeds data by changing the difference value between two adjacent pixels so that more data is embedded into two pixels located in the edge area, than in the smooth area. In order to increase the embedding capacity, a new approach is proposed in this paper by searching edge area more flexibly. Instead of processing a pair of pixels at a time as proposed by Wu and Tsai, two pairs of pixels in a block are processed at the same time. In addition, we proposed a pixel-value shifting scheme to further increase the chances for embedding data. Our scheme exploits the edge areas more efficiently, thus leading to an increase in embedding capacity as shown by experimental results compared to Wu and Tsai's method. Also, the embedding result of our scheme passes the Fridrich et al.’s detection. Besides, according to the distribution of difference values, more practical range partitions are suggested for improving capacity.
84|4||Secure key management scheme for dynamic hierarchical access control based on ECC|An access control mechanism in a user hierarchy is used to provide the management of sensitive information for authorized users. The users and their own information can be organized into a number of disjoint sets of security classes according to their responsibilities. Each security class in a user hierarchy is assigned an encryption key and can derive the encryption keys of all lower security classes according to predefined partially ordered relation. In 2006, Jeng and Wang proposed an efficient key management scheme based on elliptic curve cryptosystems. This paper, however, pointed out that Jeng–Wang scheme is vulnerable to the so-called compromising attack that the secret keys of some security classes can be compromised by any adversary if some public information modified. We further proposed a secure key management scheme based on elliptic curve cryptosystems to eliminate the pointed out the security leak and provide better security requirements. As compared with Jeng and Wang's scheme (Jeng and Wang, 2006), the proposed scheme has the following properties. (i) It is simple to execute the key generation and key derivation phases. (ii) It is easily to address dynamic access control when a security class is added into or deleted from the hierarchy. (iii) It is secure against some potential attacks. (iv) The required storage of the public/secret parameters is constant.
84|4||Factors influencing clients in the selection of offshore software outsourcing vendors: An exploratory study using a systematic literature review|Offshore software development outsourcing is a modern business strategy for developing high quality software at low cost.
84|4||Supporting real-time supply chain decisions based on RFID data streams|While RFID technology has been widely praised for its ability to streamline supply chain processes, little attention has been given to its unique data capturing characteristics to support real-time decision making. Being able to efficiently perform complex real-time analysis on top of RFID event streams is a key challenge for modern applications. This provides management with a novel data analysis mechanism to allow better, tactical, on time, well-informed decisions. The two main issues in RFID data management (RFDM) concern expressibility (how to simply and concisely express stream queries) and performance (how to efficiently evaluate stream queries). In this paper we claim that a spreadsheet-like query model, where formulation is done in a column-wise fashion, can express intuitively a large class of useful and practical RFDM queries. We propose a simple SQL extension to do that and show how these queries can be evaluated efficiently. We finally discuss a prototype called COSTES (COntinuous SpreadsheeT-likE computations), which implements our SQL extensions and evaluation algorithms. Presentation takes place within the context of two representative RFID applications, namely shelf availability and in-store sales promotions.
84|5|http://www.sciencedirect.com/science/journal/01641212/84/5|Supporting runtime software architecture: A bidirectional-transformation-based approach|Runtime software architectures (RSA) are architecture-level, dynamic representations of running software systems, which help monitor and adapt the systems at a high abstraction level. The key issue to support RSA is to maintain the causal connection between the architecture and the system, ensuring that the architecture represents the current system, and the modifications on the architecture cause proper system changes. The main challenge here is the abstraction gap between the architecture and the system. In this paper, we investigate the synchronization mechanism between architecture configurations and system states for maintaining the causal connections. We identify four required properties for such synchronization, and provide a generic solution satisfying these properties. Specifically, we utilize bidirectional transformation to bridge the abstraction gap between architecture and system, and design an algorithm based on it, which addresses issues such as conflicts between architecture and system changes, and exceptions of system manipulations. We provide a generative tool-set that helps developers implement this approach on a wide class of systems. We have successfully applied our approach on JOnAS JEE system to support it with C2-styled runtime software architecture, as well as some other cases between practical systems and typical architecture models.
84|5||Real-time perceptual watermarking architectures for video broadcasting|Existing secure embedded systems are primarily cryptography based. However, for effective Digital Rights Management (DRM) of multimedia in the framework of embedded systems, both watermarking and cryptography are necessary. In this paper, a watermarking algorithm and corresponding VLSI architectures are presented that will insert a broadcaster's logo into video streams in real-time to facilitate copyrighted video broadcasting and Internet protocol television (IP-TV). The VLSI architecture is prototyped using a hardware description language (HDL) and when realized in silicon can be deployed in any multimedia producing consumer electronics equipment to enable real-time DRM right at the source. The watermark is inserted into the video stream before MPEG-4 compression, resulting in simplified hardware requirements and superior video quality. The watermarking processing is performed in the frequency (DCT) domain. The system is initially simulated and validated in MATLAB/Simulink® and subsequently prototyped on an Altera® Cyclone-II FPGA using VHDL. Its maximum throughput is 43 frames/s at a clock speed of 100 MHz which makes it suitable for emerging real-time digital video broadcasting applications such as IP-TV. The watermarked video is of high quality, with an average Peak-Signal-to-Noise Ratio (PSNR) of 21.8 dB and an average Root-Mean-Square Error (RMSE) of 20.6.
84|5||A model of job satisfaction for collaborative development processes|Modern software development relies on collaborative work as a means for sharing knowledge, distributing tasks and responsibilities, reducing risk of failures, and increasing the overall quality of the software product. Such objectives are achieved with a continuous share of the programmers’ daily working life that inevitably influences the programmers’ job satisfaction. One of the major challenges in process management is to determine the causes of this satisfaction. Traditional research models job satisfaction with social aspects of collaborative work like communication, work sustainability, and work environment.
84|5||Shape analysis for power signal cryptanalysis on secure components|This paper presents an application of pattern recognition techniques in reverse engineering for smart cards. The aim of the study is to design algorithms based on shape classification and to determine instructions executed on a chip as well as processed data sets. Information is extracted from the power consumption in order to recover secret information. Then geometrical features are determined and a syntactic analysis is achieved in order to recover secret algorithms and data. Some examples are given showing how code execution can be reversed on a recent secure component. These examples are essentially focused on instruction recovery but the algorithms also work on data recovery or on a combination of both instruction and data recovery.
84|5||Periphery deployment for wireless sensor systems with guaranteed coverage percentage|With the availability of tiny wireless sensors, it is now possible to track moving objects by placing such sensors on the targets, collecting needed data, and transmitting sensed data back to the sink for storage and analysis. For applications such as vessel clearance surveillance, landslide detection, conveyer monitoring, and body gesture tracking, the motions of the targets are often confined to a certain region, such as the water way or the mountain slope. To collect the data from the wireless sensors, base stations are usually needed, which are deployed at fixed positions around the monitored region. Unfortunately, due to issues such as potential interference, high packaging and deployment cost, and low reliability, many such applications could only deploy the base stations on the periphery of the monitored region. The question is how to deploy the base stations on the periphery so that they can cover the most area inside the monitored area. We formulate the periphery deployment problem and analyze the performance bound in terms of coverage percentage under both ideal and practical deployment conditions. Then, we describe a deployment procedure to solve the periphery deployment problem in polynomial time. The proposed algorithms are evaluated through extensive simulations drawn from a watercourse monitoring system. The results show that the proposed algorithms can reduce the size of the deployment set by 17% compared to the traditional area-coverage algorithms, and the coverage percentage is improved by 1.18 times.
84|5||The optimization of success probability for software projects using genetic algorithms|The software development process is usually affected by many risk factors that may cause the loss of control and failure, thus which need to be identified and mitigated by project managers. Software development companies are currently improving their process by adopting internationally accepted practices, with the aim of avoiding risks and demonstrating the quality of their work.
84|5||Advanced quality prediction model for software architectural knowledge sharing|In the field of software architecture, a paradigm shift is occurring from describing the outcome of architecting process to describing the Architectural Knowledge (AK) created and used during architecting. Many AK models have been defined to represent domain concepts and their relationships, and they can be used for sharing and reusing AK across organizations, especially in geographically distributed contexts. However, different AK domain models can represent concepts that are different, thereby making effective AK sharing challenging. In order to understand the mapping quality from one AK model to another when more than one AK model coexists, AK sharing quality prediction based on the concept differences across AK models is necessary. Previous works in this area lack validation in the actual practice of AK sharing. In this paper, we carry out validation using four AK sharing case studies. We also improve the previous prediction models. We developed a new advanced mapping quality prediction model, this model (i) improves the prediction accuracy of the recall rate of AK sharing quality; (ii) provides a better balance between prediction effort and accuracy for AK sharing quality.
84|5||Secret image sharing with authentication-chaining and dynamic embedding|A popular technique to share a secret image among n participants is to divide it first into some shadow images and then embed the shadows in n”cover” images. The resulting “stego” images, which contain the embedded data, are distributed among intended recipients. In order not to attract any attacker’s attention, it is important to apply a suitable embedding such that high quality stego images are produced. Moreover, to ensure the integrity of stego data, a robust authentication mechanism which can detect tampering with high probability should be implemented.
84|5||Dynamic adaptation of response-time models for QoS management in autonomic systems|In transactional systems, the objectives of quality of service regarding are often specified by Service Level Objectives (SLOs) that stipulate a response time to be achieved for a percentile of the transactions. Usually, there are different client classes with different SLOs. In this paper, we extend a technique that enforces the fulfilment of the SLOs using admission control. The admission control of new user sessions is based on a response-time model. The technique proposed in this paper dynamically adapts the model to changes in workload characteristics and system configuration, so that the system can work autonomically, without human intervention. The technique requires no knowledge about the internals of the system; thus, it is easy to use and can be applied to many systems. Its utility is demonstrated by a set of experiments on a system that implements the TPC-App benchmark. The experiments show that the model adaptation works correctly in very different situations that include large and small changes in response times, increasing and decreasing response times, and different patterns of workload injection. In all this scenarios, the technique updates the model progressively until it adjusts to the new situation and in intermediate situations the model never experiences abnormal behaviour that could lead to a failure in the admission control component.
84|5||Exploring implicit parallelism in class diagrams|As multicore processors are becoming more wide-spread, leveraging of parallelism is once again becoming an important concern during the software development process. Substantial refactoring is required to parallelize legacy sequential software in order to exploit the advantages offered by parallel processing. In this study, guidelines are offered to aid in parallelizing object-oriented programs by analyzing their designs as represented in UML class diagrams. We define often occurring patterns of class-dependencies and demonstrate their characteristics in class diagrams by investigating their properties. We present example instances exhibiting the usage of these patterns in class diagrams. Through analyzing the runtime aspects of these instances, we have identified how they impact the parallelization of object oriented software. Taking these lessons into account when refactoring existing object-oriented software can significantly reduce time and effort required. We have evaluated our method by applying it to three popular design patterns and a real-world case study.
84|5||Reliability-driven deployment optimization for embedded systems|One of the crucial aspects that influence reliability of embedded systems is the deployment of software components to hardware nodes. If the hardware architecture is designed prior to the customized software architecture, which is often the case in product-line manufacturing (e.g. in the automotive domain), the system architect needs to resolve a nontrivial task of finding a (near-)optimal deployment balancing the reliabilities of individual services implemented on the software level.In this paper, we introduce an approach to automate this task. As distinct to related approaches, which typically stay with quantification of reliability for a specific deployment, we target multi-criteria optimization and provide the architect with near-optimal (non-dominated) deployment alternatives with respect to service reliabilities. Toward this goal, we annotate the software and hardware architecture with necessary reliability-relevant attributes, design a method to quantify the quality of individual deployment alternatives, and implement the approach employing an evolutionary algorithm.
84|5||A design pattern coupling role and component concepts: Application to medical software|One of the challenges in software development regards the appropriate coupling of separated code elements in order to correctly build initially expected high-level software functionalities. In this context, we address issues related to the dynamic composition of such code elements (i.e. how they are dynamically plugged together) as well as their collaboration (i.e. how they work together). We also consider the limitation of build-level dependencies, to avoid the entire re-compilation and re-deployment of a software when modifying it or integrating new functionalities. To solve these issues, we propose a new design pattern coupling role and component concepts and illustrate its relevance for medical software. Compared to most related works focusing on few role concepts while ignoring others, the proposed pattern integrates many role concepts as first-class entities, including in particular a refinement of the notion of collaboration. Another significant contribution of our proposal concerns the coupling of role and component concepts. Roles are related to the functional aspects of a target software program (composition and collaboration of functional units). Components correspond to the physical distribution of code elements with limited build-level dependencies. As illustrated in this paper, such a coupling enables to instantiate a software program using a generic main program together with a description file focusing on software functionalities only. Related code elements are transparently retrieved and composed at run-time before appropriately collaborating, regardless the specificity of their distribution over components.
84|5||Load and storage balanced posting file partitioning for parallel information retrieval|Many recent major search engines on Internet use a large-scale cluster to store a large database and cope with high query arrival rate. To design a large scale parallel information retrieval system, both performance and storage cost has to be taken into integrated consideration. Moreover, a quantitative method to design the cluster in systematical way is required. This paper proposes posting file partitioning algorithm for these requirements. The partitioning follows the partition-by-document-ID principle to eliminate communication overhead. The kernel of the partitioning is a data allocation algorithm to allocate variable-sized data items for both load and storage balancing. The data allocation algorithm is proven to satisfy a load balancing constraint with asymptotical 1-optimal storage cost. A probability model is established such that query processing throughput can be calculated from keyword popularities and data allocation result. With these results, we show a quantitative method to design a cluster systematically. This research provides a systematical approach to large-scale information retrieval system design. This approach has the following features: (1) the differences to ideal load balancing and storage balancing are negligible in real-world application. (2) Both load balancing and storage balancing can be taken into integrated consideration without conflicting. (3) The data allocation algorithm is capable to deal with data items of variable-sizes and variable loads. An algorithm having all these features together is never achieved before and is the key factor for achieving load and storage balanced workstation cluster in a real-world environment.
84|6|http://www.sciencedirect.com/science/journal/01641212/84/6|Non-parametric statistical fault localization|Fault localization is a major activity in program debugging. To automate this time-consuming task, many existing fault-localization techniques compare passed executions and failed executions, and suggest suspicious program elements, such as predicates or statements, to facilitate the identification of faults. To do that, these techniques propose statistical models and use hypothesis testing methods to test the similarity or dissimilarity of proposed program features between passed and failed executions. Furthermore, when applying their models, these techniques presume that the feature spectra come from populations with specific distributions. The accuracy of using a model to describe feature spectra is related to and may be affected by the underlying distribution of the feature spectra, and the use of a (sound) model on inapplicable circumstances to describe real-life feature spectra may lower the effectiveness of these fault-localization techniques. In this paper, we make use of hypothesis testing methods as the core concept in developing a predicate-based fault-localization framework. We report a controlled experiment to compare, within our framework, the efficacy, scalability, and efficiency of applying three categories of hypothesis testing methods, namely, standard non-parametric hypothesis testing methods, standard parametric hypothesis testing methods, and debugging-specific parametric testing methods. We also conduct a case study to compare the effectiveness of the winner of these three categories with the effectiveness of 33 existing statement-level fault-localization techniques. The experimental results show that the use of non-parametric hypothesis testing methods in our proposed predicate-based fault-localization model is the most promising.
84|6||An efficient shuffling based eVoting scheme|Among the two popular solutions to electronic voting, homomorphic tallying and shuffling, the latter employs a simpler vote format, is more efficient (especially for the voters) and practical. However, the existing shuffling-based eVoting schemes still have two drawbacks: low efficiency in large-scale applications (especially for the talliers) and weak robustness. In this paper, a novel shuffling protocol is proposed and a new eVoting scheme is designed based on it. The shuffling protocol modifies the shuffling scheme in Crypto 2005, optimising its encryption algorithm and parameter setting, improving its efficiency and overcoming its drawbacks. Thus an efficient shuffling protocol suitable for eVoting is obtained. In the second half of the paper, a delicate application of the shuffling protocol to eVoting is designed such that robustness is strengthened and efficiency is improved.
84|6||A novel image watermarking in redistributed invariant wavelet domain|Most existing digital watermarking algorithms, which are based on the Discrete Wavelet Transform, are not robust to geometric distortions, even if for some special distortions, such as multiples of 90° rotation of integers and image flipping, which change the location of pixels but have no effect on the value of the image. Therefore, to solve the problem, according to Haar wavelet transform theory, the redistributed invariant wavelet domain is constructed and proofed in this paper; a novel image watermarking algorithm, based on the invariant domain, is proposed to eliminate such distortions. The experimental results showed that the proposed algorithm not only can resist the common image processing operations, but also successfully resist the distortions that result from multiples of 90° rotations of integers and image flipping.
84|6||A web search-centric approach to recommender systems with URLs as minimal user contexts|In service-oriented computing, a recommender system can be wrapped as a web service with machine-readable interface. However, owing to the cross-organizational privacy issue, the internal dataset of an organization is seldom exposed to external services. In this paper, we propose a higher level recommender strategy INSERT that guides the underlying external universal recommender to suggest a set of indexes. INSERT then matches the title of each top-ranked index entry with the domain-specific keywords in the organization's internal dataset, and further directs the universal recommender to verify the popularity of such matching. INSERT finally makes recommendation based on the verification results. INSERT also employs URLs taken from a client as user contexts, which is challenging because URLs contain little content. Our experiment shows that this strategy is feasible and effective.
84|6||A synergistic model-driven approach for persistence modeling with UML|The Model Driven Development (MDD) approach proposes that models (and model-to-model transformations) play the main role on system development. However, there is not a consensual notation to model persistence based upon object-relational mapping frameworks: while UML lacks specific resources for persistence modeling, the entity-relationship model does not make reference to the dynamic concepts existing in UML.
84|6||Software product roadmapping in a volatile business environment|Product roadmapping enhances the product development process by enabling early information and long-term decision making about the products in order to deliver the right products to the right markets at the right time. However, relatively little scientific knowledge is available on the application and usefulness of product roadmapping in software product development context. This study develops a framework for software product roadmapping, which is then used to study the critical aspects of the product roadmapping process. The collection of empirical evidence includes both quantitative and qualitative data which sheds further insight into the complexities involved in product roadmapping. Results revealed that organizations view the product roadmap mainly as a tool for strategic decision making as it aims at showing the future directions of the company's products. However, only a few companies appear to have an explicit approach for handling the mechanisms for creating and maintaining such a roadmap. Finally, it is suggested that the strategic importance of product roadmapping is likely to increase in the future and, as a conclusion, a new type of agility is required in order to survive in the turbulent and competitive software business environment.
84|6||Function point measurement from Web application source code based on screen transitions and database accesses|A function point (FP) is a unit of measurement that expresses the degree of functionality that an information system provides to a user. Many software organizations use FPs to estimate the effort required for software development. However, it is essential that the definition of 1 FP be based on the software development experience of the organization. In the present study, we propose a method by which to automatically extract data and transaction functions from Web applications under several conditions using static analysis. The proposed method is based on the International Function Point Users Group (IFPUG) method and has been developed as an FP measurement tool. We applied the proposed method to several Web applications and examined the difference between FP counts obtained by the tool and those obtained by a certified FP specialist (CFPS). The results reveal that the numbers of data and transaction functions extracted by the tool is approximately the same as the numbers of data and transaction functions extracted by the specialist.
84|6||Task assignment in heterogeneous computing systems using an effective iterated greedy algorithm|A fundamental issue affecting the performance of a parallel application running on a heterogeneous computing system is the assignment of tasks to the processors in the system. The task assignment problem for more than three processors is known to be NP-hard, and therefore satisfactory suboptimal solutions obtainable in an acceptable amount of time are generally sought. This paper proposes a simple and effective iterative greedy algorithm to deal with the problem with goal of minimizing the total sum of execution and communication costs. The main idea in this algorithm is to improve the quality of the assignment in an iterative manner using results from previous iterations. The algorithm first uses a constructive heuristic to find an initial assignment and iteratively improves it in a greedy way. Through simulations over a wide range of parameters, we have demonstrated the effectiveness of our algorithm by comparing it with recent competing task assignment algorithms in the literature.
84|6||A syntactic approach to twig-query matching on XML streams|Query matching on XML streams is challenging work for querying efficiency when the amount of queried stream data is huge and the data can be streamed in continuously. In this paper, the method Syntactic Twig-Query Matching (STQM) is proposed to process queries on an XML stream and return the query results continuously and immediately. STQM matches twig queries on the XML stream in a syntactic manner by using a lexical analyzer and a parser, both of which are built from our lexical-rules and grammar-rules generators according to the user's queries and document schema, respectively. For query matching, the lexical analyzer scans the incoming XML stream and the parser recognizes XML structures for retrieving every twig-query result from the XML stream. Moreover, STQM obtains query results without a post-phase for excluding false positives, which are common in many streaming query methods. Through the experimental results, we found that STQM matches the twig query efficiently and also has good scalability both in the queried data size and the branch degree of the twig query. The proposed method takes less execution time than that of a sequence-based approach, which is widely accepted as a proper solution to the XML stream query.
84|6||A framework for developing home automation systems: From requirements to code|This article presents an integrated framework for the development of home automation systems following the model-driven approach. By executing model transformations the environment allows developers to generate executable code for specific platforms. The tools presented in this work help developers to model home automation systems by means of a domain specific language which is later transformed into code for home automation specific platforms. These transformations have been defined by means of graph grammars and template engines extended with traceability capabilities. Our framework also allows the models to be reused for different applications since a catalogue of requirements is provided. This framework enables the development of home automation applications with techniques for improving the quality of both the process and the models obtained. In order to evaluate the benefits of the approach, we conducted a survey among developers that used the framework. The analysis of the outcome of this survey shows which conditions should be fulfilled in order to increase reusability.
84|6||An energy-efficient MAC protocol with downlink traffic scheduling strategy in IEEE 802.11 infrastructure WLANs|
84|6||Managing crosscutting concerns in component based systems using a model driven development approach|In the last few years, Model-Driven Development (MDD), Aspect-Oriented Software Development (AOSD), and Component-Based Software Development (CBSD) have become interesting alternatives for the design and construction of complex distributed applications. Although these methodological approaches share the principle of separation of concerns and their further integration as key factors to obtaining high-quality and evolvable large software systems, they usually each address this principle from their own particular perspective. In the present work, we combine Component-Based and Aspect-Oriented Software Developments in a Model Driven software process targeted at the development of complex systems. This process constitutes an enhancement of the separation of concerns by allowing the isolation of crosscutting concerns in both Platform Independent and Platform Specific models. Following a pure MDD philosophy, a set of model transformations are used to generate the system, from preliminary models to the final source code for the Corba Component Model platform. A twofold empirical analysis was used to evaluate the approach’s benefits in terms of two internal quality attributes: modularity and complexity. Conclusions were drawn from this evaluation regarding other quality attributes correlated with these two – stability, changeability, error-proneness, and reusability. An Eclipse plug-in was developed to drive the development of the entire system from early modeling to late deployment stages.
84|6||The reliability estimation, prediction and measuring of component-based software|Reliability is a key driver of safety-critical systems such as health-care systems and traffic controllers. It is also one of the most important quality attributes of the systems embedded into our surroundings, e.g. sensor networks that produce information for business processes. Therefore, the design decisions that have a great impact on the reliability of a software system, i.e. architecture and components, need to be thoroughly evaluated. This paper addresses software reliability evaluation during the design and implementation phases; it provides a coherent approach by combining both predicted and measured reliability values with heuristic estimates in order to facilitate a smooth reliability evaluation process. The approach contributes by integrating the component-level reliability evaluation activities (i.e. the heuristic reliability estimation, model-based reliability prediction and model-based reliability measuring of components) and the system-level reliability prediction activity to support the incremental and iterative development of reliable component-based software systems. The use of the developed reliability evaluation approach with the supporting tool chain is illustrated by a case study. The paper concludes with a summary of lessons learnt from the case studies.
84|7|http://www.sciencedirect.com/science/journal/01641212/84/7|Reusable software components for accelerator-based clusters|
84|7||Scalable, statistical storage allocation for extensible inverted file construction|An Inverted file is a commonly used index for both archival databases and free text where no updates are expected. Applications like information filtering and dynamic environments like the Internet require inverted files to be updated efficiently. Recently, extensible inverted files are proposed which can be used for fast online indexing. The effective storage allocation scheme for such inverted files uses the arrival rate to preallocate storage. In this article, this storage allocation scheme is improved by using information about both the arrival rates and their variability to predict the storage needed, as well as scaling the storage allocation by a logarithmic factor. The resultant, final storage utilization rate can be as high as 97–98% after indexing about 1.6 million documents. This compares favorably with the storage utilization rate of the original arrival rate storage allocation scheme. Our evaluation shows that the retrieval time for extensible inverted file on solid state disk is on average similar to the retrieval time for in-memory extensible inverted file. When file seek time is not an issue, our scalable storage allocation enables extensible inverted files to be used as the main index on disk. Our statistical storage allocation may be applicable to novel situations where the arrival of items follows a binomial, Poisson or normal distribution.
84|7||Is my model right? Let me ask the expert|Defining a domain model is a costly and error-prone process. It requires that the knowledge possessed by domain experts be suitably captured by modeling experts. Eliciting what is in the domain expert’s mind and expressing it using a modeling language involve substantial human effort. In the process, conceptual errors may be introduced that are hard to detect without a suitable validation methodology. This paper proposes an approach to support such validation, by reducing the knowledge gap that separates modeling experts and domain experts. While our methodology still requires the domain expert’s judgement, it partially automates the validation process by generating a set of yes/no questions from the model. Answers differing from expected ones point to elements in the model which require further consideration and can be used to guide the dialogue between domain experts and modeling experts. Our methodology was implemented as a tool and was applied to a real case study, within the IPERMOB project.
84|7||Model-driven development of industrial process control applications|This article presents model-driven development and domain-specific modeling applied to the development of industrial process control applications. The approach is based on established practices of the industrial automation and control domain, a compatible UML profile, and an integrated and standards based tool environment for modeling and transformation execution. The methods provide means for developing applications using domain-specific modeling concepts to increase productivity and enhance platform independent solution reuse. The approach has been implemented to support industrial practices and to be able to utilize existing control system platforms. During demonstrations and an assessment event with industry professionals the methods have been successfully applied to the development of small-scale process control applications. In this paper, attention is also paid on discussion of the practical applicability and benefits of the approach for engineering and development of industrial process control applications.
84|7||Procedural security analysis: A methodological approach|This article introduces what we call procedural security analysis, an approach that allows for a systematic security assessment of (business) processes. The approach is based on explicit reasoning on asset flows and is implemented by building formal models to describe the nominal procedures under analysis, by injecting possible threat-actions of such models, and by assuming that any combination of threats can be possible in all steps into such models. We use the NuSMV input language to encode the asset flows, which are amenable for formal analysis. This allows us to understand how the switch to a new technological solution changes the requirements of an organization, with the ultimate goal of defining the new processes that ensure a sufficient level of security.
84|7||A benchmarking environment for performance evaluation of tree-based rekeying algorithms|While a vast number of solutions to multicast group rekeying were published in the last years, a common base to evaluate these solutions and compare them with each other is still missing. This paper presents a unified and comprehensive way to evaluate the performance of different rekeying algorithms. A rekeying benchmark estimates rekeying costs from a system point of view, which allows a reliable comparison between different rekeying algorithms. For this purpose, two metrics are defined: the Join Rekeying Time (JRT) and the Disjoin Rekeying Time (DRT). By means of four simulation modes, these metrics are estimated in relation to both the group size and the group dynamics. A benchmark prototype, implemented in Java, demonstrates the merit of this unified assessment method by means of two comprehensive case studies.
84|7||Dynamic deployment of context-aware access control policies for constrained security devices|Securing the access to a server, guaranteeing a certain level of protection over an encrypted communication channel, executing particular counter measures when attacks are detected are examples of security requirements. Such requirements are identified based on organizational purposes and expectations in terms of resource access and availability and also on system vulnerabilities and threats. All these requirements belong to the so-called security policy. Deploying the policy means enforcing, i.e., configuring, those security components and mechanisms so that the system behavior be finally the one specified by the policy. The deployment issue becomes more difficult as the growing organizational requirements and expectations generally leave behind the integration of new security functionalities in the information system: the information system will not always embed the necessary security functionalities for the proper deployment of contextual security requirements. To overcome this issue, our solution is based on a central entity approach which takes in charge unmanaged contextual requirements and dynamically redeploys the policy when context changes are detected by this central entity.
84|7||Simulation-based analysis of middleware service impact on system reliability: Experiment on Java application server|Being a popular runtime infrastructure in the era of Internet, middleware provides more and more services to support the development, deployment and management of distributed systems. At the same time, the reliability of middleware services has a significant impact on the overall reliability of the system. Different services have different impacts and different service fault-tolerance solutions have different costs and risks. Therefore, the identification of the services that greatly affect the whole system reliability is the major obstacle to achieving reliable middleware-based systems. In this paper, we present an analytical framework to automatically reason and quantify such impacts when deploying the target system. In this framework, faults are represented by exceptions in modern programming languages; service failures are simulated by software fault injection; reliability impacts are measured by scenarios. This framework is demonstrated on multiple JEE application servers, including JBoss, JonAS and PKUAS. The experiments on two JEE blueprint applications, namely JPS and ECperf, show the feasibility, the applicability and the usability of this framework.
84|7||An algorithm for capturing variables dependences in test suites|The use of dynamic dependence analysis spans several areas of software research including software testing, debugging, fault localization, and security. Many of the techniques devised in these areas require the execution of large test suites in order to generate profiles that capture the dependences that occurred between given types of program elements. When the aim is to capture direct and indirect dependences between finely granular elements, such as statements and variables, this process becomes highly costly due to: (1) the large number of elements, and (2) the transitive nature of the indirect dependence relationship.
84|7||Impossible differential cryptanalysis of 13-round CLEFIA-128|Block cipher plays an important role in the domain of information security. CLEFIA is a 128-bit block cipher proposed by SONY Corporation in FSE 2007. Using the previous 9-round impossible differentials, the redundancy in the key schedule and the early-abort technique, we present the first successful impossible differential cryptanalysis of 13-round CLEFIA-128 in this paper. The data and time complexities of the attack with the whitening layers are 2119.4 and 2125.52, respectively. And for the attack without the whitening layers, more relationships among the subkeys can be used, thus the data and time complexities are reduced to 2111.3 and 2117.5, respectively. As far as we know, the presented results are the best compared with the previously published cryptanalytic results on reduced-round CLEFIA-128.
84|7||Threshold visual secret sharing by random grids|A new visual secret sharing (VSS) approach by random grids (RG-based VSS), proposed by Kafri and Keren (1987), has drawn close attention recently. With almost all advantages of visual cryptography-based VSS, RG-based VSS benefits more from keeping the same size of secret images without the problem of pixel expansion from which VC-based VSS suffer. In this paper, a threshold RG-based VSS scheme aiming at providing the wide-use version is presented. This is the first effort to develop the technique. The experimental results and theoretically analysis in visual quality and security show that the proposed scheme performs well.
84|7||Parsed use case descriptions as a basis for object-oriented class model generation|Object-oriented analysis and design has become a major approach in the design of software systems. Recent developments in CASE tools provide help in documenting the analysis and design stages and in detecting incompleteness and inconsistency in analysis. However, these tools do not contribute to the initial and difficult stage of the analysis process of identifying the objects/classes, attributes and relationships used to model the problem domain. This paper presents a tool, Class-Gen, which can partially automate the identification of objects/classes from natural language requirement specifications for object identification. Use case descriptions (UCDs) provide the input to Class-Gen which parses and analyzes the text written in English. A parsed use case description (PUCD) is generated which is then used as the basis for the construction of an initial UML class model representing object classes and relationships identified in the requirements. PUCDs enable the extraction of nouns, verbs, adjectives and adverbs from traditional UCDs for the identification process. Finally Class-Gen allows the initial class model to be refined manually. Class-Gen has been evaluated against a collection of unseen requirements. The results of the evaluation are encouraging as they demonstrate the potential for such tools to assist with the software development process.
84|7||Optimized QoS-aware replica placement heuristics and applications in astronomy data grid|This paper studies the Quality-of-Service (QoS)-aware replica placement problem in a general graph model. Since the problem was proved NP-hard, heuristic algorithms are the current solutions to the problem. However, these algorithms cannot always find the effective replica placement strategy. We propose two algorithms that can obtain better results within the given time period. The first algorithm is called Cover Distance algorithm, which is based on the Greedy Cover algorithm. The second algorithm is an optimized genetic algorithm, in which we use random heuristic algorithms to generate initial population to avoid enormous useless searching. Then, the 0-Greedy-Delete algorithm is used to optimize the genetic algorithm solutions. According to the performance evaluation, our Cover Distance algorithm can obtain relatively better solution in time critical scenarios. Whereas, the optimized genetic algorithm is better when the replica cost is of higher priority than algorithm execution time. The QoS-aware data replication heuristic algorithms are applied into the data distribution service of an astronomy data grid pipeline prototype, and the operation process is studied in detail.
84|7||An empirical study of software architecturesâ effect on product quality|Software architecture is concerned with the structure of software systems and is generally agreed to influence software quality. Even so, little empirical research has been performed on the relationship between software architecture and software quality. Based on 1141 open source Java projects, we calculate three software architecture metrics (measuring classes per package, normalized distance, and a new metric introduced by us concerning the excess of coupling degree) and analyze to which extent these metrics are related to product metrics (defect ratio, download rate, methods per class, and method complexity). We conclude that there are a number of significant relationships between product metrics and architecture metrics. In particular, the number of open defects depends significantly on all our architecture measures.
84|7||APDL: A reference XML schema for process-centered definition of RFID solutions|Despite the proliferation of RFID systems and applications, there is still no easy way to develop, integrate and deploy non-trivial RFID solutions. Indeed, the latter comprise various middleware modules (e.g., data collection and filtering, generation of business events, integration with enterprise applications), which must be deployed and configured independently. In this paper we introduce APDL (AspireRFID Process Description Language), an XML based specification for describing and configuring RFID solutions. Using APDL one can minimize the steps and effort required to integrate and configure an RFID solution, since it unifies all the configuration parameters and steps comprising an RFID deployment. APDL supports several configuration parameters defined in the scope of the EPCglobal architecture and related standards. However, it extends beyond the EPCglobal architecture, to a wider class of RFID solutions. Furthermore, APDL is amendable by visual tools, which obviates the need to carry out low-level programming tasks in order to deploy an RFID solution. These tools are also presented and evaluated in the paper.
84|8|http://www.sciencedirect.com/science/journal/01641212/84/8|BSN: An automatic generation algorithm of social network data|In recent years, there has been considerable interest in the analysis of social network data. In this paper, we propose a novel automatic generation algorithm of social network data – the Biclustering Algorithm for Social Network Data algorithm. The algorithm introduces biclustering to social network analysis for automatic identification of associations among a group of actors and entities. The algorithm is different from existing ones in that it employs a combination of min–max and pattern searching procedures to construct hierarchical biclusters and discover the relationships among these actors, in order to easily interpret social network data. The algorithm is not subject to convexity limitations, and does not need to use derivatives information.
84|8||The effects of scheduling, workload type and consolidation scenarios on virtual machine performance and their prediction through optimized artificial neural networks|The aim of this paper is to study and predict the effect of a number of critical parameters on the performance of virtual machines (VMs). These parameters include allocation percentages, real-time scheduling decisions and co-placement of VMs when these are deployed concurrently on the same physical node, as dictated by the server consolidation trend and the recent advances in the Cloud computing systems. Different combinations of VM workload types are investigated in relation to the aforementioned factors in order to find the optimal allocation strategies. What is more, different levels of memory sharing are applied, based on the coupling of VMs to cores on a multi-core architecture. For all the aforementioned cases, the effect on the score of specific benchmarks running inside the VMs is measured. Finally, a black box method based on genetically optimized artificial neural networks is inserted in order to investigate the degradation prediction ability a priori of the execution and is compared to the linear regression method.
84|8||Developing an efficient query system for encrypted XML documents|XQuery is a query and functional programming language that is designed for querying the data in XML documents. This paper addresses how to efficiently query encrypted XML documents using XQuery, with the key point being how to eliminate redundant decryption so as to accelerate the querying process. We propose a processing model that can automatically translate the XQuery statements for encrypted XML documents. The implementation and experimental results demonstrate the practicality of the proposed model.
84|8||An information presentation method based on tree-like super entity component|Information systems are increasingly oriented in the direction of large-scale integration due to the explosion of multi-source information. It is therefore important to discuss how to reasonably organize and present information from multiple structures and sources on the same information system platform. In this study, we propose a 3C (Components, Connections, Container) component model by combining white-box and black-box methods, design a tree-like super entity based on the model, present its construction and related algorithm, and take a tree-like super entity as the information organization method for multi-level entities. In order to represent structural, semi-structural and non-structural data on the same information system platform, an information presentation method based on an editable e-book component has been developed by combining the tree-like super entity component, QQ-style menu and 1/K switch connection component, which has been successfully applied in the Flood Protection Project Information System of the Yangtze River in China.
84|8||A family of implementation-friendly BN elliptic curves|For the last decade, elliptic curve cryptography has gained increasing interest in industry and in the academic community. This is especially due to the high level of security it provides with relatively small keys and to its ability to create very efficient and multifunctional cryptographic schemes by means of bilinear pairings. Pairings require pairing-friendly elliptic curves and among the possible choices, Barreto–Naehrig (BN) curves arguably constitute one of the most versatile families.
84|8||An approach to process continuous location-dependent queries on moving objects with support for location granules|Location-based services have attracted the attention of important research in the field of mobile computing. Specifically, different mechanisms have been proposed to process location-dependent queries. In the above mentioned context, it is usually assumed that the location data are expressed at a fine geographic precision. However, a different granularity may be more appropriate in certain situations. Thus, a location resolution higher than required may even be inconvenient or not understandable by the user (for example, if the user expects a city name as an answer and instead the system provides the latitude/longitude coordinates). Moreover, if the locations presented to the user need to be refreshed automatically as the objects move, it is obvious that maintaining up-to-date GPS-like geographic coordinates would be more expensive in terms of processing and communication. Unfortunately, the existing approaches assume queries whose locations are always given with maximum precision (i.e., GPS locations).
84|8||Deriving business processes with service level agreements from early requirements|When designing a service-based business process employing loosely coupled services, one is not only interested in guaranteeing a certain flow of work, but also in how the work will be performed. This involves the consideration of non-functional properties which go from execution time and costs, to trust and security. Ideally, a designer would like to have guarantees over the behavior of the services involved in the process. These guarantees are the object of Service Level Agreements.
84|8||On a security model of conjunctive keyword search over encrypted relational database|We study a security model for searching documents containing each of several keywords (conjunctive keyword search) over encrypted documents. A conjunctive keyword search protocol consists of three entities: a data supplier, a storage system such as database, and a user of storage system. A data supplier uploads encrypted documents on a storage system, and then a user of the storage system searches documents containing each of several keywords with a private trapdoor. That is, a valid user is able to use boolean combinations of queries.
84|8||Priority scheduling of requests to web portals|Web portals work as a point of access to a large volume of information on the web. This paper focuses on the performance of Web portals in an E-commerce environment which involves the processing of a large number of users’ requests. It proposes a class-based priority scheme which classifies users’ requests into high and low priorities. In E-commerce, some requests (e.g. buy) are generally considered more important than others (e.g. search or browse). We contend that the requests received from a Web portal should generally get higher priority as such requests are more likely to lead to purchases. We believe that assigning such priorities at multiple service levels can improve the performance of Web portals’ requests of higher priority. The proposed scheme is formally specified and implemented, and performance results are obtained and compared to a server that does not prioritise requests. The results show significant performance improvements in the processing of high priority requests.
84|8||An overview on test generation from functional requirements|Despite the fact that the test phase is described in the literature as one of the most relevant for quality assurance in software projects, this test phase is not usually developed, among others, with enough resources, time or suitable techniques.
84|8||Framework for evaluation and selection of the software packages: A hybrid knowledge based system approach|Evaluation and selection of the software packages is complicated and time consuming decision making process. Selection of inappropriate software package can turn out to be costly and adversely affects business processes and functioning of the organization. In this paper we describe (i) generic methodology for software selection, (ii) software evaluation criteria, and (iii) hybrid knowledge based system (HKBS) approach to assist decision makers in evaluation and selection of the software packages. The proposed HKBS approach employs an integrated rule based and case based reasoning techniques. Rule based reasoning is used to capture user needs of the software package and formulate a problem case. Case based reasoning is used to retrieve and compare candidate software packages with the user needs of the package. This paper also evaluates and compares HKBS approach with the widely used existing software evaluation techniques such as analytic hierarchy process (AHP) and weighted scoring method (WSM).
84|8||A robust digital audio watermarking scheme using wavelet moment invariance|It is a challenging work to design a robust audio watermarking scheme against various attacks. Wavelet moment invariances are new features combining the moment invariant features and the wavelet features, and they have some excellent characteristics, such as the ability to capture local information, robustness against common signal processing, and the linear relationship between a signal and its wavelet moments etc. Based on wavelet moment and synchronization code, we propose a new digital audio watermarking algorithm with good auditory quality and reasonable resistance against most attacks in this paper. Firstly, the origin digital audio is segmented and then each segment is cut into two parts. Secondly, with the spatial watermarking technique, synchronization code is embedded into the statistics average value of audio samples in the first part. And then, map 1D digital audio signal in the second part into 2D form, and calculate its wavelet moments. Finally, the watermark bit is embedded into the average value of modulus of the low-order wavelet moments. Meanwhile combining the two adjacent synchronization code searching technology, the algorithm can extract the watermark without the help from the origin digital audio signal. Simulation results show that the proposed watermarking scheme is not only inaudible and robust against common signals processing such as MP3 compression, noise addition, resampling, and re-quantization etc., but also robust against the desynchronization attacks such as random cropping, amplitude variation, pitch shifting, and jittering etc.
84|9|http://www.sciencedirect.com/science/journal/01641212/84/9|Guest Editors Introduction to the Special Issue|
84|9||The lonesome architect|Although the benefits are well-known and undisputed, sharing architectural knowledge is not something architects automatically do. In an attempt to better understand what architects really do and what kind of support they need for sharing knowledge, we have conducted large-scale survey research. The results of our study indicate that architects can be characterized as rather lonesome decision makers who mainly consume, but neglect documenting and actively sharing architectural knowledge. Acknowledging this nature of architects suggests ways to develop more effective support for architectural knowledge sharing.
84|9||Composing enterprise mashup components and services using architecture integration patterns|Enterprise mashups leverage various source of information to compose new situational applications. The architecture of such applications must address integration issues: it needs to deal with heterogeneous local and/or public data sources, and build value-added applications on existing corporate IT systems. In this paper, we leverage enterprise architecture integration patterns to compose reusable mashup components. We present a service oriented architecture that addresses reusability and integration needs for building enterprise mashup applications. Key techniques to customize this architecture are developed for mashups with themed data on location maps. The usage of this architecture is illustrated by a property valuation application derived from a real-world scenario. We demonstrate and discuss how this state-of-the-art architecture design method can be applied to enhance the design and development of emerging enterprise mashups.
84|9||Defining and documenting execution viewpoints for a large and complex software-intensive system|An execution view is an important asset for developing large and complex systems. An execution view helps practitioners to describe, analyze, and communicate what a software system does at runtime and how it does it. In this paper, we present an approach to define and document viewpoints that guide the construction and use of execution views for an existing large and complex software-intensive system. This approach includes the elicitation of the organization's requirements for execution views, the initial definition and validation of a set of execution viewpoints, and the documentation of the execution viewpoints. The validation and application of the approach have helped us to produce mature viewpoints that are being used to support the construction and use of execution views of the Philips Healthcare MRI scanner, a representative large software-intensive system in the healthcare domain.
84|9||A secure fragile watermarking scheme based on chaos-and-hamming code|In this work, a secure fragile watermarking scheme is proposed. Images are protected and any modification to an image is detected using a novel hybrid scheme combining a two-pass logistic map with Hamming code. For security purposes, the two-pass logistic map scheme contains a private key to resist the vector quantization (VQ) attacks even though the embedding scheme is block independent. To ensure image integrity, watermarks are embedded into the to-be-protected images which are generated using Hamming code technique. Experimental results show that the proposed scheme has satisfactory protection ability and can detect and locate various malicious tampering via image insertion, erasing, burring, sharpening, contrast modification, and even though burst bits. Additionally, experiments prove that the proposed scheme successfully resists VQ attacks.
84|9||Strongly unforgeable proxy signature scheme secure in the standard model|Many proxy signature schemes have been found in the literature. They (except one) were proven secure in the random oracle model, which has received a lot of criticism. In this paper, we propose a new construction of proxy signature which is s trongly unforgeable in the standard model with the computational Diffie–Hellman assumption in bilinear groups. Compared with the only proxy signature without random oracles, our scheme demonstrates stronger security, shorter system parameters and higher efficiency. As far as we know, this is the first proxy signature scheme, which possesses the property of strong unforgeability in the standard model.
84|9||Enterprise architecture patterns for business process support analysis|The field of enterprise architectures lacks architecture patterns that would support analysis of a given enterprise architecture, comparison of different enterprise architecture solutions and provide guidelines for development of a target enterprise architecture based on the analysis of existing enterprise architecture. In this paper, we focus on business process support analysis using information derived from enterprise architecture description. We give a systematic overview of important aspects. We establish and formally define foundational enterprise architecture patterns for business process support analysis. They are implementation independent and enable more efficient qualitative architecture analysis of business process support, which is the basis for achieving more optimal business operation. We have defined the patterns using the standard enterprise architecture language – ArchiMate. They are formalized in a way that enables their implementation in enterprise architecture tools. This is an important characteristic that allows for efficient work by automatic detection of different, more or less suitable, architecture structures. We have derived the patterns based on real-world enterprise architecture descriptions and have used and verified them in enterprise architecture analysis and planning projects for four large organizations. The enterprise architecture analysis patterns address an important research issue in the field of enterprise architectures that has so far not been systematically researched.
84|9||FeGC: An efficient garbage collection scheme for flash memory based storage systems|NAND flash memory is a promising storage media that provides low-power consumption, high density, high performance, and shock resistance. Due to these versatile features, NAND flash memory is anticipated to be used as storage in enterprise-scale systems as well as small embedded devices. However, unlike traditional hard disks, flash memory should perform garbage collection that consists of a series of erase operations. The erase operation is time-consuming and it usually degrades the performance of storage systems seriously. Moreover, the number of erase operations allowed to each flash memory block is limited. This paper presents a new garbage collection scheme for flash memory based storage systems that focuses on reducing garbage collection overhead, and improving the endurance of flash memory. The scheme also reduces the energy consumption of storage systems significantly. Trace-driven simulations show that the proposed scheme performs better than various existing garbage collection schemes in terms of the garbage collection time, the number of erase operations, the energy consumption, and the endurance of flash memory.
84|9||Enhancing grid-density based clustering for high dimensional data|We propose an enhanced grid-density based approach for clustering high dimensional data. Our technique takes objects (or points) as atomic units in which the size requirement to cells is waived without losing clustering accuracy. For efficiency, a new partitioning is developed to make the number of cells smoothly adjustable; a concept of the ith-order neighbors is defined for avoiding considering the exponential number of neighboring cells; and a novel density compensation is proposed for improving the clustering accuracy and quality. We experimentally evaluate our approach and demonstrate that our algorithm significantly improves the clustering accuracy and quality.
84|9||Practitioner perceptions of Open Source software in the embedded systems area|There is a growing body of research to show that, with the advent of so-called professional Open Source, attitudes within many organisations towards adopting Open Source software have changed. However, there have been conflicting reports on the extent to which this is true of the embedded software systems sector—a large sector in Europe. This paper reports on attitudes towards Open Source software within that sector. Our results show a high level of acceptance of Open Source products with large, well established communities, and not only at the level of the operating system. Control over the software is seen as fundamentally important. Other key perceptions with Open Source are an easing of long-term maintenance problems and ready availability of support. The classical strengths of Open Source, namely mass inspection, ease of conducting trials, longevity and source code access for debugging, were at the forefront of thinking. However, there was an acknowledgement that more guidelines are needed for assessing Open Source software and incorporating it into products.
84|9||A multi-purpose digital image watermarking using fractal block coding|In this paper, a new multi-purpose watermarking technique is presented which satisfies both verification and authentication purposes simultaneously by embedding a binary watermark into the image. The proposed method uses a special type of fractal block coding with a local search region with contrast scaling and mean of range block as its parameters. It also utilizes Fuzzy C-Mean clustering to specify the watermark bits. To overcome the high computational complexity of fractal coding, a new simple coding method is also presented which improves the robustness of the watermarking and decreases the run time of fractal block coding in the watermarking procedure. To measure the fragility and robustness of the method to signal distortions such as JPEG compression, median filter, and additive noise, some experiments were employed. The experimental results showed that the proposed method has provided a sensitive authentication and a reliable verification.
84|9||Revealing bullying patterns in multi-agent systems|Multi-agent systems communication is arguably difficult to be designed. An equal balance of communication among agents is usually desirable. When this does not happen, it affects the quality of service, causing poor performances in terms of response times. The aim of this work is to detect undesirable patterns of communication in these types of systems, using the metaphor of bullying. Though observed in social environments, it can also be applied to a multi-agent system scenario: it consists of some agents’ continuous communications toward the same agents. This causes an unbalanced communication, overloading the receiver, which results in higher response times. Some metrics have been designed to measure the agents’ communication, and some rules classify agents accordingly into several patterns regarding the bullying metaphor. Furthermore, the experimental results prove that the metrics based on the bullying metaphor are strongly related with the quality of service of multi-agent systems. The experimental results also show that the causes of a bad quality of service can be figured out by means of this bullying metaphor and their metrics and classification rules.
85|1|http://www.sciencedirect.com/science/journal/01641212/85/1|Special Issue on Dynamic Analysis and Testing of Embedded Software|
85|1||InRob: An approach for testing interoperability and robustness of real-time embedded software|Advances in digital technologies have contributed for significant reduction in accidents caused by hardware failures. However, the growing complexity of functions performed by embedded software has increased the number of accidents caused by software faults in critical systems. Moreover, due to the highly competitive market, software intensive subsystems are usually developed by different suppliers. Often these subsystems are required to interact with each other in order to provide a collaborative service. Testing approaches for subsystems integration support verification of the quality of service, focusing on the subsystems interfaces. The increasing complexity and tight coupling of real-time subsystems make integration testing unmanageable. The ad-hoc approach for testing is becoming less effective and more expensive. This article presents an integration testing approach denominated InRob, designed to verify the interoperability and robustness related to timing constraints of real-time embedded software. InRob guides the construction of services, based on formal models, aiming at the specifications of interoperability and robustness of test cases related to delays and time-outs of the messages exchanged in the interfaces of interconnected subsystems. The proposed formalism supports automatic test cases generation by verifying the relevant properties in the service behavioral model. As timing constraints are critical properties of aerospace systems, the feasibility of InRob is showed in the integration testing process of a telescope onboard in a satellite. The process is instantiated with existing testing tools and the case study is the software embedded in the telescope.
85|1||Test coverage optimization for large code problems|Software developers frequently conduct regression testing on a series of major, minor, or bug-fix software or firmware releases. However, retesting all test cases for each release is time-consuming. For example, it takes about 36 test-bed-days to thoroughly exercise a test suite made up of 2320 test cases for the MPLS testing area that contains 57,758 functions in Cisco IOS. The cost is infeasible for a series of regression testing on the MPLS area. Thus, the test suite needs to be reduced intelligently, not just randomly, and its fault detection capability must be kept as much as possible. The mode of safe regression test selection approach is adopted for seeking a subset of modification-traversing test cases to substitute for fault-revealing test cases. The algorithms, CW-NumMin, CW-CostMin, and CW-CostCov-B, apply the safe-mode approach in selecting test cases for achieving full-modified function coverage. It is assumed that modified functions are fault-prone, and the fault distribution of the testing area is Pareto-like. Moreover, we also assume that once a subject program is getting more mature, its fault concentration will become stronger. Only function coverage criterion is adopted because of the scalability of a software system with large code. The metrics of test’s function reachability and function’s test intensity are defined in this study for algorithms. Both CW-CovMax and CW-CostMin algorithms are not safe-mode, but the approaches they use still attempt to obtain a test suite with a maximal amount of function coverage under certain constraints, i.e. the effective-confidence level and time restriction. We conclude that the most effective algorithm in this study can significantly reduce the cost (time) of regression testing on the MPLS testing area to 1.10%, on the average. Approaches proposed here can be effectively and efficiently applied to the regression testing on bug-fix releases of a software system with large code, especially to the releases having very few modified functions with low test intensities.
85|1||Lightweight embedded software performance analysis method by kernel hack and its industrial field study|Despite advances in software testing technologies, there are still limitations in directly applying them to embedded software. Since the operational environment of embedded software has severe resource constraints, it is necessary to develop a lightweight testing method that has little impact on the operational environment of embedded software. We propose an agent-based performance analysis method to hack kernel performance counters that manage the system's execution information. The proposed method enables us to collect data required for analyzing performance bottlenecks and identify the causes and locations of bottlenecks with little impact on the test target system's operational environment.
85|1||Automatic testing environment for multi-core embedded softwareâATEMES|Software testing during the development process of embedded software is not only complex, but also the heart of quality control. Multi-core embedded software testing faces even more challenges. Major issues include: (1) how demanding efforts and repetitive tedious actions can be reduced; (2) how resource restraints of embedded system platform such as temporal and memory capacity can be tackled; (3) how embedded software parallelism degree can be controlled to empower multi-core CPU computing capacity; (4) how analysis is exercised to ensure sufficient coverage test of embedded software; (5) how to do data synchronization to address issues such as race conditions in the interrupt driven multi-core embedded system; (6) high level reliability testing to ensure customer satisfaction. To address these issues, this study develops an automatic testing environment for multi-core embedded software (ATEMES). Based on the automatic mechanism, the system can parse source code, instrument source code, generate testing programs for test case and test driver, support generating primitive, structure and object types of test input data, multi-round cross-testing, and visualize testing results. To both reduce test engineer's burden and enhance his efficiency when embedded software testing is in process, this system developed automatic testing functions including unit testing, coverage testing, multi-core performance monitoring. Moreover, ATEMES can perform automatic multi-round cross-testing benchmark testing on multi-core embedded platform for parallel programs adopting Intel TBB library to recommend optimized parallel parameters such as pipeline tokens. Using ATEMES on the ARM11 multi-core platform to conduct testing experiments, the results show that our constructed testing environment is effective, and can reduce burdens of test engineer, and can enhance efficiency of testing task.
85|1||Erratum to âPerformance evaluation of fast handover in mobile IPv6 based on link layer informationsâ [J. Syst. Softw. 83 (2010) 1644â1650]|
85|1||A power efficiency routing and maintenance protocol in wireless multi-hop networks|In wireless multi-hop networks, selecting a path that has a high transmission bandwidth or a high delivery rate of packets can reduce power consumption and shorten transmission delay during data transmission. There are two factors that influence the transmission bandwidth: the signal strength of the received packets and contentions in the contention-based MAC layer. These two factors may cause more power to be consumed during data transmission. We analyze these two factors and propose a power-aware routing protocol called MTPCR. MTPCR discovers the desired routing path that has reduced power consumption during data transmission. In addition to finding a desired path to reduce power consumption, MTPCR also takes into account the situations in which the transmission bandwidth of the routing path may decrease, resulting in much power consumption during data transmission because of the mobility of nodes in a network. MTPCR is thus useful in a network: it analyzes power consumption during data transmission with the help of neighboring nodes, and it uses a path maintenance mechanism to maintain good path bandwidth. The density of nodes in a network is used to determine when to activate the path maintenance mechanism in order to reduce the overhead of this mechanism. With the proposed path maintenance mechanism, power consumption during data transmission can be efficiently reduced, as well as the number of path breakages. In our simulation, we compared our proposed routing protocol, MTPCR, with the following protocols: two classical routing protocols, AODV and DSR; two power-aware routing protocols, MMBCR and xMBCR; and one multiple path routing protocol, PAMP. The comparisons are made in terms of throughput of the routing path, power consumption in path discovery, power consumption in data transmission, and network lifetime.
85|1||Self-tuning of disk inputâoutput in operating systems|One of the most difficult and hard to learn tasks in computer system management is tuning the kernel parameters in order to get the maximum performance. Traditionally, this tuning has been set using either fixed configurations or the subjective administrator's criteria. The main bottleneck among the subsystems managed by the operating systems is disk input/output (I/O). An evolutionary module has been developed to perform the tuning of this subsystem automatically, using an adaptive and dynamic approach. Any computer change, both at the hardware level, and due to the nature of the workload itself, will make our module adapt automatically and in a transparent way. Thus, system administrators are released from this kind of task and able to achieve some optimal performances adapted to the framework of each of their systems. The experiment made shows a productivity increase in 88.2% of cases and an average improvement of 29.63% with regard to the default configuration of the Linux operating system. A decrease of the average latency was achieved in 77.5% of cases and the mean decrease in the request processing time of I/O was 12.79%.
85|1||Keyword clustering for user interest profiling refinement within paper recommender systems|To refine user interest profiling, this paper focuses on extending scientific subject ontology via keyword clustering and on improving the accuracy and effectiveness of recommendation of the electronic academic publications in online services. A clustering approach is proposed for domain keywords for the purpose of the subject ontology extension. Based on the keyword clusters, the construction of user interest profiles is presented on a rather fine granularity level. In the construction of user interest profiles, we apply two types of interest profiles: explicit profiles and implicit profiles. The explicit profiles are obtained by relating users’ interest-topic relevance factors to users’ interest measurements of these topics computed by a conventional ontology-based method, and the implicit profiles are acquired on the basis of the correlative relationships among the topic nodes in topic network graphs. Three experiments are conducted which reveal that the uses of the subject ontology extension approach as well as the two types of interest profiles satisfyingly contribute to an improvement in the accuracy of recommendation.
85|1||SimFuzz: Test case similarity directed deep fuzzing|Fuzzing is widely used to detect software vulnerabilities. Blackbox fuzzing does not require program source code. It mutates well-formed inputs to produce new ones. However, these new inputs usually do not exercise deep program semantics since the possibility that they can satisfy the conditions of a deep program state is low. As a result, blackbox fuzzing is often limited to identify vulnerabilities in input validation components of a program. Domain knowledge such as input specifications can be used to mitigate these limitations. However, it is often expensive to obtain such knowledge in practice. Whitebox fuzzing employs heavy analysis techniques, i.e., dynamic symbolic execution, to systematically generate test inputs and explore as many paths as possible. It is powerful to explore new program branches so as to identify more vulnerabilities. However, it has fundamental challenges such as unsolvable constraints and is difficult to scale to large programs due to path explosion. This paper proposes a novel fuzzing approach that aims to produce test inputs to explore deep program semantics effectively and efficiently. The fuzzing process comprises two stages. At the first stage, a traditional blackbox fuzzing approach is applied for test data generation. This process is guided by a novel test case similarity metric. At the second stage, a subset of the test inputs generated at the first stage is selected based on the test case similarity metric. Then, combination testing is applied on these selected test inputs to further generate new inputs. As a result, less redundant test inputs, i.e., inputs that just explore shallow program paths, are created at the first stage, and more distinct test inputs, i.e., inputs that explore deep program paths, are produced at the second stage. A prototype tool SimFuzz is developed and evaluated on real programs, and the experimental results are promising.
85|1||Dependability analysis in the Ambient Assisted Living Domain: An exploratory case study|Ambient Assisted Living (AAL) investigates the development of systems involving the use of different types of sensors, which monitor activities and vital signs of lonely elderly people in order to detect emergency situations or deviations from desirable medical patterns. Instead of requiring the elderly person to manually push a button to request assistance, state-of-the-art AAL solutions automate the process by ‘perceiving’ lonely elderly people in their home environment through various sensors and performing appropriate actions under the control of the underlying software. Dependability in the AAL domain is a critical requirement, since poor system availability, reliability, safety, or integrity may cause inappropriate emergency assistance to potentially have fatal consequences. Nevertheless, contemporary research has not focused on assessing dependability in this domain. This work attempts to fill this gap presenting an approach which relies on modern quantitative and qualitative dependability analysis techniques based on software architecture. The analysis method presented in this paper consists of conversion patterns from Unified Modeling Language (UML) behavior models of the AAL software architecture into a formal executable specification, based on a probabilistic process algebra description language, which enables a sound quantitative and qualitative analysis. The UML models specify system component interactions and are annotated with component failure probabilities and system usage profile information. The resulting formal specification is executed on PRISM, a model checking tool adequate for the purpose of our analysis in order to identify a set of domain-specific dependability properties expressed declaratively in Probabilistic Computational Tree Logic (PCTL). The benefits of using these techniques are twofold. Firstly, they allow us to seamlessly integrate the analysis during subsequent software lifecycle stages in critical scenarios. Secondly, we identify the components which have the highest impact on software system dependability, and therefore, be able to address software architecture and individual software component problems prior to implementation and the occurrence of critical errors.
85|1||Controlling software architecture erosion: A survey|Software architectures capture the most significant properties and design constraints of software systems. Thus, modifications to a system that violate its architectural principles can degrade system performance and shorten its useful lifetime. As the potential frequency and scale of software adaptations increase to meet rapidly changing requirements and business conditions, controlling such architecture erosion becomes an important concern for software architects and developers. This paper presents a survey of techniques and technologies that have been proposed over the years either to prevent architecture erosion or to detect and restore architectures that have been eroded. These approaches, which include tools, techniques and processes, are primarily classified into three generic categories that attempt to minimise, prevent and repair architecture erosion. Within these broad categories, each approach is further broken down reflecting the high-level strategies adopted to tackle erosion. These are: process-oriented architecture conformance, architecture evolution management, architecture design enforcement, architecture to implementation linkage, self-adaptation and architecture restoration techniques consisting of recovery, discovery and reconciliation. Some of these strategies contain sub-categories under which survey results are presented.
85|1||DyDAP: A dynamic data aggregation scheme for privacy aware wireless sensor networks|End-to-end data aggregation, without degrading sensing accuracy, is a very relevant issue in wireless sensor networks (WSN) that can prevent network congestion to occur. Moreover, privacy management requires that anonymity and data integrity are preserved in such networks. Unfortunately, no integrated solutions have been proposed so far, able to tackle both issues in a unified and general environment. To bridge this gap, in this paper we present an approach for dynamic secure end-to-end data aggregation with privacy function, named DyDAP. It has been designed starting from a UML model that encompasses the most important building blocks of a privacy-aware WSN, including aggregation policies. Furthermore, it introduces an original aggregation algorithm that, using a discrete-time control loop, is able to dynamically handle in-network data fusion to reduce the communication load. The performance of the proposed scheme has been verified using computer simulations, showing that DyDAP avoids network congestion and therefore improves WSN estimation accuracy while, at the same time, guaranteeing anonymity and data integrity.
85|1||Using compressed index structures for processing moving objects in large spatio-temporal databases|This paper develops a novel, compressed B+-tree based indexing scheme that supports the processing of moving objects in one-, two-, and multi- dimensional spaces. The past, current, and anticipated future trajectories of movements are fully indexed and well organized. No parameterized functions and geometric representations are introduced in our data model so that update operations are not required and the maintenance of index structures can be accomplished by basic insertion and deletion operations. The proposed method has two contributions. First, the spatial and temporal attributes of trajectories are accurately preserved and well organized into compact index structures with very efficient memory space utilization and storage requirement. Second, index maintenance overheads are more economical and query performance is more responsive than those of conventional methods. Both analytical and empirical studies show that our proposed indexing scheme outperforms the TPR-tree.
85|1||The changing industry structure of software development for consumer electronics and its consequences for software architectures|During the last decade the structure of the consumer electronics industry has been changing profoundly. Current consumer electronics products are built using components from a large variety of specialized firms, whereas previously each product was developed by a single, vertically integrated company. Taking a software development perspective, we analyze the transition in the consumer electronics industry using case studies from digital televisions and mobile phones. We introduce a model consisting of five industry structure types and describe the forces that govern the transition between types and we describe the consequences for software architectures.
85|1||Appraisal and reporting of security assurance at operational systems level|In this paper we discuss the issues relating the evaluation and reporting of security assurance of runtime systems. We first highlight the shortcomings of current initiatives in analyzing, evaluating and reporting security assurance information. Then, the paper proposes a set of metrics to help capture and foster a better understanding of the security posture of a system. Our security assurance metric and its reporting depend on whether or not the user of the system has a security background. The evaluation of such metrics is described through the use of theoretical criteria, a tool implementation and an application to a case study based on an insurance company network.
85|1||ID-based proxy signature scheme with message recovery|A proxy signature scheme, introduced by Mambo, Usuda and Okamoto, allows an entity to delegate its signing rights to another entity. Identity based public key cryptosystems are a good alternative for a certificate based public key setting, especially when efficient key management and moderate security are required. From inception several ID-based proxy signature schemes have been discussed, but no more attention has been given to proxy signature with message recovery. In this paper, we are proposing provably secure ID-based proxy signature scheme with message recovery and we have proved that our scheme is secure as existential forgery-adaptively chosen message and ID attack. As proposed scheme is efficient in terms of communication overhead and security, it can be a good alternative for certificate based proxy signatures, used in various applications such as wireless e-commerce, mobile agents, mobile communication and distributed shared object systems, etc.
85|10|http://www.sciencedirect.com/science/journal/01641212/85/10|Guest editorsâ introduction to the special issue on automated software evolution|
85|10||Using Pig as a data preparation language for large-scale mining software repositories studies: An experience report|The Mining Software Repositories (MSR) field analyzes software repository data to uncover knowledge and assist development of ever growing, complex systems. However, existing approaches and platforms for MSR analysis face many challenges when performing large-scale MSR studies. Such approaches and platforms rarely scale easily out of the box. Instead, they often require custom scaling tricks and designs that are costly to maintain and that are not reusable for other types of analysis. We believe that the web community has faced many of these software engineering scaling challenges before, as web analyses have to cope with the enormous growth of web data. In this paper, we report on our experience in using a web-scale platform (i.e., Pig) as a data preparation language to aid large-scale MSR studies. Through three case studies, we carefully validate the use of this web platform to prepare (i.e., Extract, Transform, and Load, ETL) data for further analysis. Despite several limitations, we still encourage MSR researchers to leverage Pig in their large-scale studies because of Pig's scalability and flexibility. Our experience report will help other researchers who want to scale their analyses.
85|10||Towards automated traceability maintenance|Traceability relations support stakeholders in understanding the dependencies between artifacts created during the development of a software system and thus enable many development-related tasks. To ensure that the anticipated benefits of these tasks can be realized, it is necessary to have an up-to-date set of traceability relations between the established artifacts. This goal requires the creation of traceability relations during the initial development process. Furthermore, the goal also requires the maintenance of traceability relations over time as the software system evolves in order to prevent their decay. In this paper, an approach is discussed that supports the (semi-) automated update of traceability relations between requirements, analysis and design models of software systems expressed in the UML. This is made possible by analyzing change events that have been captured while working within a third-party UML modeling tool. Within the captured flow of events, development activities comprised of several events are recognized. These are matched with predefined rules that direct the update of impacted traceability relations. The overall approach is supported by a prototype tool and empirical results on the effectiveness of tool-supported traceability maintenance are provided.
85|10||Dependency solving: A separate concern in component evolution management|Maintenance of component-based software platforms often has to face rapid evolution of software components. Component dependencies, conflicts, and package managers with dependency solving capabilities are the key ingredients of prevalent software maintenance technologies that have been proposed to keep software installations synchronized with evolving component repositories. We review state-of-the-art package managers and their ability to keep up with evolution at the current growth rate of popular component-based platforms, and conclude that their dependency solving abilities are not up to the task.
85|10||Identification and application of Extract Class refactorings in object-oriented systems|Refactoring is recognized as an essential practice in the context of evolutionary and agile software development. Recognizing the importance of the practice, modern IDEs provide some support for low-level refactorings. A notable exception in the list of supported refactorings is the “Extract Class” refactoring, which is conceived to simplify large, complex, unwieldy and less cohesive classes.
85|10||Model-driven support for product line evolution on feature level|Software Product Lines (SPL) are an engineering technique to efficiently derive a set of similar products from a set of shared assets. In particular in conjunction with model-driven engineering, SPL engineering promises high productivity benefits. There is however, a lack of support for systematic management of SPL evolution, which is an important success factor as a product line often represents a long term investment. In this article, we present a model-driven approach for managing SPL evolution on feature level. To reduce complexity we use model fragments to cluster related elements. The relationships between these fragments are specified using feature model concepts itself leading to a specific kind of feature model called EvoFM. A configuration of EvoFM represents an evolution step and can be transformed to a concrete instance of the product line (i.e., a feature model for the corresponding point in time). Similarly, automatic transformations allow the derivation of an EvoFM from a given set of feature models. This enables retrospective analysis of historic evolution and serves as a starting point for introduction of EvoFM, e.g., to plan future evolution steps.
85|10||Automated, highly-accurate, bug assignment using machine learning and tossing graphs|Empirical studies indicate that automating the bug assignment process has the potential to significantly reduce software evolution effort and costs. Prior work has used machine learning techniques to automate bug assignment but has employed a narrow band of tools which can be ineffective in large, long-lived software projects. To redress this situation, in this paper we employ a comprehensive set of machine learning tools and a probabilistic graph-based model (bug tossing graphs) that lead to highly-accurate predictions, and lay the foundation for the next generation of machine learning-based bug assignment. Our work is the first to examine the impact of multiple machine learning dimensions (classifiers, attributes, and training history) along with bug tossing graphs on prediction accuracy in bug assignment. We validate our approach on Mozilla and Eclipse, covering 856,259 bug reports and 21 cumulative years of development. We demonstrate that our techniques can achieve up to 86.09% prediction accuracy in bug assignment and significantly reduce tossing path lengths. We show that for our data sets the Naïve Bayes classifier coupled with product–component features, tossing graphs and incremental learning performs best. Next, we perform an ablative analysis by unilaterally varying classifiers, features, and learning model to show their relative importance of on bug assignment accuracy. Finally, we propose optimization techniques that achieve high prediction accuracy while reducing training and prediction time.
85|10||On the relationship between comment update practices and Software Bugs|When changing source code, developers sometimes update the associated comments of the code (a consistent update), while at other times they do not (an inconsistent update). Similarly, developers sometimes only update a comment without its associated code (an inconsistent update). The relationship of such comment update practices and software bugs has never been explored empirically. While some (in)consistent updates might be harmless, software engineering folklore warns of the risks of inconsistent updates between code and comments, because these updates are likely to lead to out-of-date comments, which in turn might mislead developers and cause the introduction of bugs in the future. In this paper, we study comment update practices in three large open-source systems written in C (FreeBSD and PostgreSQL) and Java (Eclipse). We find that these practices can better explain and predict future bugs than other indicators like the number of prior bugs or changes. Our findings suggest that inconsistent changes are not necessarily correlated with more bugs. Instead, a change in which a function and its comment are suddenly updated inconsistently, whereas they are usually updated consistently (or vice versa), is risky (high probability of introducing a bug) and should be reviewed carefully by practitioners.
85|10||Towards automated debugging in software evolution: Evaluating delta debugging on real regression bugs from the developersâ perspectives|Delta debugging has been proposed to isolate failure-inducing changes when regressions occur. In this work, we focus on evaluating delta debugging in practical settings from developers’ perspectives. A collection of real regressions taken from medium-sized open source programs is used in our evaluation. Towards automated debugging in software evolution, a tool based on delta debugging is created and both the limitations and costs are discussed.
85|10||Preserving knowledge in software projects|Up-to-date preservation of project knowledge like developer communication and design documents is essential for the successful evolution of software systems. Ideally, all knowledge should be preserved, but since projects only have limited resources, and software systems continuously grow in scope and complexity, one needs to prioritize the subsystems and development periods for which knowledge preservation is more urgent. For example, core subsystems on which the majority of other subsystems build are obviously prime candidates for preservation, yet if these subsystems change continuously, picking a development period to start knowledge preservation and to maintain knowledge for over time become very hard. This paper exploits the time dependence between code changes to automatically determine for which subsystems and development periods of a software project knowledge preservation would be most valuable. A case study on two large open source projects (PostgreSQL and FreeBSD) shows that the most valuable subsystems to preserve knowledge for are large core subsystems. However, the majority of these subsystems (1) are continuously foundational, i.e., ideally for each development period knowledge should be preserved, and (2) experience substantial changes, i.e., preserving knowledge requires substantial effort.
85|10||Convex optimization framework for intermediate deadline assignment in soft and hard real-time distributed systems|It is generally challenging to determine end-to-end delays of applications for maximizing the aggregate system utility subject to timing constraints. Many practical approaches suggest the use of intermediate deadline of tasks in order to control and upper-bound their end-to-end delays. This paper proposes a unified framework for different time-sensitive, global optimization problems, and solves them in a distributed manner using Lagrangian duality. The framework uses global viewpoints to assign intermediate deadlines, taking resource contention among tasks into consideration. For soft real-time tasks, the proposed framework effectively addresses the deadline assignment problem while maximizing the aggregate quality of service. For hard real-time tasks, we show that existing heuristic solutions to the deadline assignment problem can be incorporated into the proposed framework, enriching their mathematical interpretation.
85|10||Architecture-driven reliability optimization with uncertain model parameters|It is currently considered good software engineering practice to decide between design alternatives based on quantitative architecture evaluations for different quality attributes, such as reliability and performance. However, the results of these quantitative architecture evaluations are dependent on design-time estimates for a series of model-parameters, which may not be accurate and have to be estimated subject heterogeneous uncertain factors. As a result, sub-optimal design decisions may be taken. To overcome this problem, we present a novel robust optimization approach that deals with parameter uncertainties at the design phase of software-intensive systems. This work specifically focuses on architecture-based reliability evaluation models. The proposed approach is able to find good solutions that restrict the impact of parameter uncertainties, and thus provides better decision support. The accuracy and scalability of the presented approach is validated with an industrial case study and a series of experiments with generated examples in different problem sizes.
85|10||The influence of SPI on business success in software SMEs: An empirical study|In this paper, we present the findings of a study into the relationship between software process improvement (SPI) and business success in software development small- to medium-sized companies (software SMEs). A number of earlier related studies investigated the benefits of SPI in software SMEs, particularly in terms of improvements in product quality and adherence to budgetary and schedule constraints. However, only limited or indirect research has examined the relationship between SPI and business success. In this study, we adopt the Holistic Scorecard (HSC) (Sureshchandar and Leisten, 2005) as a business success reference framework, thus examining both the financial and the non-financial aspects of business success. In addition, we utilise ISO/IEC 12207 (ISO/IEC, 2008) as a comprehensive reference framework for the investigation of SPI activity in software SMEs. Through the use of new metrics introduced in this paper, the study findings establish that there is a positive association between SPI and business success in software SMEs, highlighting the importance of SPI in successful software SMEs. This is the first time that this relationship has been demonstrated using empirical data, and therefore, the findings represent a valuable new addition to the body of knowledge.
85|10||A framework for model-driven development of information systems: Technical decisions and lessons learned|In recent years, the impact of the model-driven engineering (MDE) paradigm has resulted in the advent of a number of model-based methodological proposals that leverage the use of models at any stage of the development cycle. Apart from promoting the role of models, MDE is notable for leveraging the level of automation along the development process. For this to be achieved there is a need of supporting frameworks, tools or environments. This way, while accompanying any methodological proposal of the corresponding technical support has been traditionally recognized as a good practice, it becomes a mandatory requirement in MDE contexts. To address this task, this work presents in a systematic and reasoned way the set of methodological and technical decisions that drove the specification of M2DAT, a technical solution for model-driven development of Information Systems and its reference implementation: M2DAT-DB, a DSL toolkit for model-driven development of modern DB schemas. The objective of this work is to put forward the conclusions and decisions derived from the experience of the authors when designing and building such framework. As a result, this work will help not only MDE practitioners, but also SE practitioners wishing to bring the advantages of MDE to their fields of interest.
85|10||A compression-based text steganography method|In this study, capacity and security issues of text steganography have been considered to improve by proposing a novel approach. For this purpose, a text steganography method that employs data compression has been proposed. Because of using textual data in steganography, the employed data compression algorithm has to be lossless. Accordingly, LZW data compression algorithm has been chosen due to its frequent use in the literature and significant compression ratio. The proposed method constructs – uses stego keys and employs Combinatorics-based coding in order to increase security. Secret information has been hidden in the chosen text from the previously constructed text base that consists of naturally generated texts. Email has been chosen as communication channel between the two parties, so the stego cover has been arranged as a forward mail platform. By means of the proposed scheme, capacity has been reached to 7.042% for the secret message containing 300 characters (or 300·8 bits). Finally, comparison of the proposed scheme with the other contemporary methods in the literature has been carried out. Experimental results show that the proposed scheme provided a significant increment in terms of capacity.
85|10||High capacity reversible data hiding scheme based upon discrete cosine transformation|In this paper, we propose a reversible data hiding scheme based on the varieties of coefficients of discrete cosine transformation of an image. Cover images are decomposed into several different frequencies, and the high-frequency parts are embedded with secret data. We use integer mapping to implement our 2-dimensional discrete cosine transformation. Thus, the image recovered from the modified coefficients can be transformed back to the correct data-hidden coefficients. Since the distribution of 2-dimensional DCT coefficients looks close to Gaussian distribution centralized at zero, it is a natural candidate for embedding secret data using the histogram shifting approach. Thus, our approach shifts the positive coefficients around zero to the right and the negative coefficients around zero to the left in order to leave a space to hide the secret data. The experimental comparisons show that, compared to Chang et al. and Lin et al.'s method, the embedding capacity and quality of the stego-image of the proposed method is a great improvement.
85|10||An experimental comparison of different real-time schedulers on multicore systems|In this work, an experimental comparison among the Rate Monotonic (RM) and Earliest Deadline First (EDF) multiprocessor real-time schedulers is performed, with a focus on soft real-time systems. We generated random workloads of synthetic periodic task sets and executed them on a big multi-core machine, using Linux as Operating System, gathering an extensive amount of data related to their exhibited performance under various real-time scheduling strategies. The comparison involves the fixed-priority scheduler for multiprocessors as available in the Linux kernel (with priorities set so as to achieve RM), and on our own implementation of EDF, both configured in global, partitioned and clustered mode. The impact of the various scheduling strategies on the performance of the applications, as well as the generated scheduling overheads, are compared presenting an extensive set of experimental results. These provide a comprehensive view of the performance achievable by the different schedulers under various workload conditions.
85|10||3 + 1 Challenges for the future of universities|Universities are looking for effective strategies to cope with the global changes that have extended across the world in the past years. Existing approaches to research and education are increasingly perceived as unable or at least insufficient to capture and take into account the complexity and the dynamism of the globalized society. This is particularly true for the ICT sector, which has been radically transformed by technologies such as mobile devices, ubiquitous connectivity, and pervasive ICT. Indeed, as these technologies are inherently disruptive, they are profoundly impacting and transforming the economy and the entire society in general.
85|11|http://www.sciencedirect.com/science/journal/01641212/85/11|Strong fuzzy c-means in medical image data analysis|This paper presents a robust fuzzy c-means (FCM) for an automatic effective segmentation of breast and brain magnetic resonance images (MRI). This paper obtains novel objective functions for proposed robust fuzzy c-means by replacing original Euclidean distance with properties of kernel function on feature space and using Tsallis entropy. By minimizing the proposed effective objective functions, this paper gets membership partition matrices and equations for successive prototypes. In order to reduce the computational complexity and running time, center initialization algorithm is introduced for initializing the initial cluster center. The initial experimental works have done on synthetic image and benchmark dataset to investigate the effectiveness of proposed, and then the proposed method has been implemented to differentiate the different region of real breast and brain magnetic resonance images. In order to identify the validity of proposed fuzzy c-means methods, segmentation accuracy is computed by using silhouette method. The experimental results show that the proposed method is more capable in segmentation of medical images than existed methods.
85|11||Ontology driven bee's foraging approach based self adaptive online recommendation system|Online recommendation system is the modern software system used in all the e-commerce sites to capture the user intent and recommend the web pages that contain user expected information. The important challenges for such a system must include a need of being self-adaptive because the needs for online users may change dynamically. Classifier plays a very important role to improve the overall system accuracy. Here, we proposed the Ontology driven bee's foraging approach (ODBFA) that accurately classify the current user activity to any of the navigation profiles and predict the navigations that most likely to be visited by online users.
85|11||Improved results on impossible differential cryptanalysis of reduced-round Camellia-192/256|As an international standard adopted by ISO/IEC, the block cipher Camellia has been used in various cryptographic applications. In this paper, we reevaluate the security of Camellia against impossible differential cryptanalysis. Specifically, we propose several 7-round impossible differentials with the FL/FL−1 layer. Based on one of them, we mount impossible differential attacks on 11-round Camellia-192 and 12-round Camellia-256. The data complexities of our attacks on 11-round Camellia-192 and 12-round Camellia-256 are about 2120 chosen plaintexts and 2119.8 chosen plaintexts, respectively. The corresponding time complexities are approximately 2167.1 11-round encryptions and 2220.87 12-round encryptions. As far as we know, our attacks are 216.9 times and 219.13 times faster than the previously best known ones but have slightly more data.
85|11||Cross-layer end-to-end label switching protocol for WiMAXâMPLS heterogeneous networks|The integration of WiMAX networks and multi-protocol label switching (MPLS) networks, called WiMPLS networks, is the trend for nomadic Internet access in the fourth generation (4G) wireless networks. The base station (BS) in such heterogeneous networks will play the role of bridge and router between the IEEE 802.16 subscriber stations (SSs) and MPLS networks. However, there is no such integrated solution so far and the switching efficiency of the BS should be considered as well. This paper, therefore, adopts a cross-layer fashion (from network layer to MAC layer) to design the end-to-end label switching protocol (ELSP) for filling this gap. ELSP provides the mechanism of end-to-end (SS-to-SS) and layer 2 switching transfer for switching performance enhancement by assigning the SS with the MPLS labels (M-labels). The M-label can be carried by the IEEE 802.16e extended subheader within the MAC protocol data unit (MPDU), which is fully compliant with the IEEE 802.16e standard. The security issue caused by M-label usage is also concerned and solved in this paper. This paper also reveals an extra advantage that the switching delay of the BS achieved by ELSP can be as low as hardware-accelerated IP lookup mechanism, e.g., ternary content addressable memory (TCAM). Simulation results show that ELSP efficiently improves the end-to-end transfer delay as well as the throughput for WiMPLS heterogeneous networks.
85|11||A variable-length model for masquerade detection|Masquerade detection is now one of the major concerns of system security research and its difficulty is to model user behavior on the nonstationary audit data. Many previous works represent the user behavior based on fixed-length models. In this paper, we propose a variable-length model to overcome their weakness in the precision and adaptability of user profiling. In the model, the user's normal behavior is profiled by Markov chain with states of variable-length sequences. At first multiple shell command streams of different lengths are generated and different shell command sequences are hierarchically merged into several sets to form the library of general sequences. Then the variable-length behavioral patterns of a valid user are mined and the Markov chain is constructed. While performing detection, the probabilities of short state sequences are calculated, smoothed with sliding windows, and finally used to classify the monitored user's activity as normal or abnormal. Our experiments with standard datasets such as Purdue data and SEA data reveal that the proposed model can achieve higher detection accuracy, require less memory and take shorter time than the other traditional methods and is amenable for real-time intrusion detection.
85|11||JCSI: A tool for checking secure information flow in Java Card applications|This paper describes a tool for checking secure information flow in Java Card applications. The tool performs a static analysis of Java Card CAP files and includes a CAP viewer. The analysis is based on the theory of abstract interpretation and on a multi-level security policy assignment. Actual values of variables are abstracted into security levels, and bytecode instructions are executed over an abstract domain. The tool can be used for discovering security issues due to explicit or implicit information flows and for checking security properties of Java Card applications downloaded from untrusted sources.
85|11||Interpretation problems related to the use of regression models to decide on economy of scale in software development|Many research studies report an economy of scale in software development, i.e., an increase in productivity with increasing project size. Several software practitioners seem, on the other hand, to believe in a diseconomy of scale, i.e., a decrease in productivity with increasing project size. In this paper we argue that violations of essential regression model assumptions in the research studies to a large extent may explain this disagreement. Particularly illustrating is the finding that the use of the production function (Size = a·Effortb), instead of the factor input model (Effort = a·Sizeb), would most likely have led to the opposite result, i.e., a tendency towards reporting diseconomy of scale in the research studies. We conclude that there are good reasons to warn against the use of regression analysis parameters to investigate economies of scale and to look for other analysis methods when studying economy of scale in software development contexts.
85|11||From Teleo-Reactive specifications to architectural components: A model-driven approach|
85|11||Performance analysis of SCOOP programs|To support developers in writing reliable and efficient concurrent programs, novel concurrent programming abstractions have been proposed in recent years. Programming with such abstractions requires new analysis tools because the execution semantics often differs considerably from established models. We present a performance analyzer that is based on new metrics for programs written in SCOOP, an object-oriented programming model for concurrency. We discuss how the metrics can be used to discover performance issues, and we use the tool to optimize a concurrent robotic control software.
85|11||Grouping target paths for evolutionary generation of test data in parallel|Generating test data covering multiple paths using multi-population parallel genetic algorithms is a considerable important method. The premise on which the method above is efficient is appropriately grouping target paths. Effective methods of grouping target paths, however, have been absent up to date. The problem of grouping target paths for generation of test data covering multiple paths is investigated, and a novel method of grouping target paths is presented. In this method, target paths are divided into several groups according to calculation resources available and similarities among target paths, making a small difference in the number of target paths belonging to different groups, and a great similarity among target paths in the same group. After grouping these target paths, a mathematical model is built for parallel generation of test data covering multiple paths, and a multi-population genetic algorithm is adopted to solve the model above. The proposed method is applied to several benchmark or industrial programs, and compared with a previous method. The experimental results show that the proposed method can make full use of calculation resources on the premise of meeting the requirement of path coverage, improving the efficiency of generating test data.
85|11||Nearest neighbor selection for iteratively kNN imputation|Existing kNN imputation methods for dealing with missing data are designed according to Minkowski distance or its variants, and have been shown to be generally efficient for numerical variables (features, or attributes). To deal with heterogeneous (i.e., mixed-attributes) data, we propose a novel kNN (k nearest neighbor) imputation method to iteratively imputing missing data, named GkNN (gray kNN) imputation. GkNN selects k nearest neighbors for each missing datum via calculating the gray distance between the missing datum and all the training data rather than traditional distance metric methods, such as Euclidean distance. Such a distance metric can deal with both numerical and categorical attributes. For achieving the better effectiveness, GkNN regards all the imputed instances (i.e., the missing data been imputed) as observed data, which with complete instances (instances without missing values) together to iteratively impute other missing data. We experimentally evaluate the proposed approach, and demonstrate that the gray distance is much better than the Minkowski distance at both capturing the proximity relationship (or nearness) of two instances and dealing with mixed attributes. Moreover, experimental results also show that the GkNN algorithm is much more efficient than existent kNN imputation methods.
85|11||An endurance solution for solid state drives with cache|NAND flash memory has become one of the most popular storage media for portable devices, such as MP3 players, MMC cards and solid state drives (SSDs). Due to erase-before-write characteristics of NAND flash memory, wear-leveling strategy is very important in determining the performance and lifetime of NAND flash memory in solid state drives. In this paper, to prolong the lifetime and improve the performance of SSDs with cache, we propose an effective wear-leveling algorithm based on a novel “triple-pool” design. Comparing with previous wear-leveling algorithms, experimental results show that our algorithm lengthens the lifetime and reduces the write amplification.
85|11||Percolation-based routing in the Internet|The uncontrollable growth of the Internet, breaking through meshing and multi-homing practices the existing topology-based prefix aggregation mechanisms, creates the necessity of revisiting some fundamental aspects in the inter-domain routing model due to severe scalability issues in routing table size. In this paper, we at first analyze the root causes of these problems and then exploit a promising solution based on on-demand routing and on a widely known uniform caching and searching algorithm. Such algorithm is based on bond percolation, a mathematical phase transition model well-suited for random walk searches in power law networks, automatically shielding nodes with limited connectivity from large traffic volumes and reducing the total traffic to scale sub-linearly with the network size. The proposed solution introduces limited modifications to the BGP protocol, ensuring backward compatibility and allowing gradual deployment throughout the Internet. It dramatically reduces the routing table size requirements in all the nodes participating to the search network while allowing reliable and efficient on-demand discovery of unknown routing information, as demonstrated through extensive simulation experiments.
85|11||Software architecture evolution through evolvability analysis|Software evolvability is a multifaceted quality attribute that describes a software system's ability to easily accommodate future changes. It is a fundamental characteristic for the efficient implementation of strategic decisions, and the increasing economic value of software. For long life systems, there is a need to address evolvability explicitly during the entire software lifecycle in order to prolong the productive lifetime of software systems. However, designing and evolving software architectures are the challenging task. To improve the ability to understand and systematically analyze the evolution of software system architectures, in this paper, we describe software architecture evolution characterization, and propose an architecture evolvability analysis process that provides replicable techniques for performing activities to aim at understanding and supporting software architecture evolution. The activities are embedded in: (i) the application of a software evolvability model; (ii) a structured qualitative method for analyzing evolvability at the architectural level; and (iii) a quantitative evolvability analysis method with explicit and quantitative treatment of stakeholders’ evolvability concerns and the impact of potential architectural solutions on evolvability. The qualitative and quantitative assessments manifested in the evolvability analysis process have been applied in two large-scale industrial software systems at ABB and Ericsson, with experiences and reflections described.
85|11||Optimizing virtual machines using hybrid virtualization|Minimizing virtualization overhead and improving the reliability of virtual machines are challenging when establishing virtual machine cluster. Paravirtualization and hardware-assisted virtualization are two mainstream solutions for modern system virtualization. Hardware-assisted virtualization is superior in CPU and memory virtualization and becoming the leading solution, yet paravirtualization is still valuable in some aspects as it is capable of shortening the disposal path of I/O virtualization. Thus we propose the hybrid virtualization which runs the paravirtualized guest in the hardware-assisted virtual machine container to take advantage of both. Experiment results indicate that our hybrid solution outweighs origin paravirtualization by nearly 30% in memory intensive test and 50% in microbenchmarks. Meanwhile, compared with the origin hardware-assisted virtual machine, hybrid guest owns over 16% improvement in I/O intensive workloads.
85|11||Managing data dependencies in service compositions|Composing services into service-based systems requires the design of coordination logic, which describes all service interactions realizing the composition. Coordination can be defined as the management of dependencies; in a services context we can discriminate between ‘control flow’ that manages sequence dependencies and ‘data flow’ for managing data dependencies. Current research fails to address the management of data dependencies in a systematic way and mostly treats it as subordinate to sequence dependencies. In this article a ‘data flow’ pattern language is presented that provides a systematic way of designing the data flow aspects of a coordination scenario, orthogonally to the way in which the control flow is designed. Starting from a set of fundamental and basic building blocks, each data dependency will yield a data flow design that takes a set of design criteria (e.g. loose coupling, data confidentiality, etc.) into account. The pattern language is evaluated in three ways. First, it is shown that every potential coordination scenario for managing a data dependency can be composed by the set of patterns. Second, the pattern language was applied in a real-life insurance case to show how it can guide the design of complex data flows. Third, the patterns were implemented in a tool that provides configurable model-to-code transformations for automatically generating BPEL coordination scenarios. In this tool both the data flow and control flow can be designed separately using different sets of patterns.
85|11||AIOLOS: Middleware for improving mobile application performance through cyber foraging|As the popularity of smartphones and tablets increases, the mobile platform is becoming a very important target for application developers. Despite recent advances in mobile hardware, most mobile devices fail to execute complex multimedia applications (such as image processing) with an acceptable level of user experience. Cyber foraging is a well-known computing technique to enhance the capabilities of mobile devices, where the mobile device offloads parts of the application to a nearby discovered server in the network.
85|11||Masquerade attacks based on user's profile|This paper presents a set of methods for building masquerade attacks. Each method takes into account the profile of the user to be impersonated, thus capturing an intruder strategy. Knowledge about user behavior is extracted from several statistics, including the frequency at which a user types a specific group of commands. It is then expressed by rules, which are applied to synthesize computer sessions that mimic the attack as ordinary user behavior. The masquerade attack datasets have been validated by making a set of Intrusion Detection Systems (IDS) try to detect user impersonation, this way showing the capabilities of each masquerade synthesis method for evading detection. Results demonstrate that a better performance of masquerade attacks can be obtained by using methods based on behavioral rules rather than those based only on a single statistic. Summing up, masquerade attacks exhibit a good strategy for bypassing an IDS.
85|11||Execution of natural language requirements using State Machines synthesised from Behavior Trees|This paper defines a transformation from Behavior Tree models to UML state machines. Behavior Trees are a graphical modelling notation for capturing and formalising dynamic system behaviour described in natural language requirements. But state machines are more widely used in software development, and are required for use with many tools, such as test case generators. Combining the two approaches provides a formal path from natural language requirements to an executable model of the system. This in turn facilitates requirements validation and transition to model-driven software development methods. The approach is demonstrated by defining a mapping from Behavior Trees to UML state machines using the ATLAS Transformation Language (ATL) in the Eclipse Modeling Framework. A security-alarm system case study is used to illustrate the use of Behavior Trees and execution to debug requirements.
85|11||Authentication of images for 3D cameras: Reversibly embedding information using intelligent approaches|In this work, a reversible watermarking approach for authentication of 3D cameras based on computational intelligence is presented. Two intelligent techniques based on differential evolution (DE) and hybrid DE are employed to optimize the tradeoff between watermark imperceptibility and capacity. The proposed approach is suitable for images of 3D cameras. These cameras generally work on the concept of time-of-flight and not only produce the 2D image but also generate the corresponding depth map. In this approach, the depth map is considered as secret information and is hidden in the integer wavelet transform of the corresponding 2D image. The proposed technique is prospective for authenticating 3D camera images and allows the secure transmission of its depth map. It has the advantage of the lossless recovery of original 2D image as and when needed. The watermarking of the 2D images is based on integer wavelet transform and threshold optimization. The threshold map thus obtained using the intelligent optimization approaches is not only used for watermark embedding, but is also utilized for authentication purpose by correlating it with the corresponding 2D transformed image. Experiments conducted on images and depth maps obtained using 3D camera validate the proposed concept.
85|12|http://www.sciencedirect.com/science/journal/01641212/85/12|Introduction to the special issue on state of the art in engineering self-adaptive systems|
85|12||An evaluation of multi-model self-managing control schemes for adaptive performance management of software systems|
85|12||Self-control of the time complexity of a constraint satisfaction problem solver program|This paper presents the self-controlling software paradigm and reports on its use to control the branch and bound based constraint satisfaction problem solving algorithm. In this paradigm, an algorithm is first conceptualized as a dynamical system and then a feedback control loop is added to control its behavior. The loop includes a Quality of Service component that assesses the performance of the algorithm during its run time and a controller that adjusts the parameters of the algorithm in order to achieve the control goal. Although other approaches – generally termed as “self-*” – make use of control loops, this use is limited to the structure of the software system, rather than to its behavior and its dynamics. This paper advocates the analysis of dynamics of any program with control loops. The self-controlling software paradigm is evaluated on two different NP-hard constraint satisfaction and optimization problems. The results of the evaluation show an improvement in the performance due to the added control loop for both of the tested constraint satisfaction problems.
85|12||Self-tuning of software systems through dynamic quality tradeoff and value-based feedback control loop|Quality requirements of a software system cannot be optimally met, especially when it is running in an uncertain and changing environment. In principle, a controller at runtime can monitor the change impact on quality requirements of the system, update the expectations and priorities from the environment, and take reasonable actions to improve the overall satisfaction. In practice, however, existing controllers are mostly designed for tuning low-level performance indicators instead of high-level requirements. By maintaining a live goal model to represent runtime requirements and linking the overall satisfaction of quality requirements to an indicator of earned business value, we propose a control-theoretic self-tuning method that can dynamically tune the preferences of different quality requirements, and can autonomously make tradeoff decisions through our Preference-Based Goal Reasoning procedure. The reasoning procedure results in an optimal configuration of the variation points by selecting the right alternative of OR-decomposed goals and such a configuration is mapped onto corresponding system architecture reconfigurations. The effectiveness of our self-tuning method is evaluated by earned business value, comparing our results with those obtained using static and ad hoc methods.
85|12||Achieving dynamic adaptation via management and interpretation of runtime models|In this article, we present a generic model-centric approach for realizing fine-grained dynamic adaptation in software systems by managing and interpreting graph-based models of software at runtime. We implemented this approach as the Graph-based Runtime Adaptation Framework (GRAF), which is particularly tailored to facilitate and simplify the process of evolving and adapting current software towards runtime adaptivity. As a proof of concept, we present case study results that show how to achieve runtime adaptivity with GRAF and sketch the framework's capabilities for facilitating the evolution of real-world applications towards self-adaptive software. The case studies also provide some details of the GRAF implementation and examine the usability and performance of the approach.
85|12||Specification and monitoring of data-centric temporal properties for service-based systems|Service-based systems operate in a very dynamic environment. To guarantee functional and non-functional objective at runtime, an adaptation mechanism is usually expected to monitor software changes, make appropriate decisions, and act accordingly. However, existing runtime monitoring solutions consider only the constraints on the sequence of messages exchanged between partner services and ignore the actual data contents inside the messages. As a result, it is difficult to monitor some dynamic properties such as how message data of interest is processed between different participants. To address this issue, we propose an efficient, non-intrusive online monitoring approach to dynamically analyze data-centric properties for service-oriented applications involving multiple participants. By introducing Par-BCL – a Parametric Behavior Constraint Language for Web services – to define monitoring parameters, various data-centric temporal behavior properties for Web services can be specified and monitored. This approach broadens the monitored patterns to include not only message exchange orders, but also data contents bound to the parameters. To reduce runtime overhead, we statically analyze the monitored properties and combine two different indexing mechanisms to optimize monitoring. The experiments show that our solution is efficient and promising.
85|12||Adaptive application offloading using distributed abstract class graphs in mobile environments|Self-adaptation of software has been used as a mechanism to address complexity and constraint in mobile and pervasive computing environments. Adaptive offloading is a software adaptation mechanism in which an application dynamically distributes portions of itself to remote devices to achieve context specific optimizations. The feasibility of using adaptive offloading in pervasive environments is determined by the computational efficiency of adaptation algorithms and the efficacy of their decisions. However, existing state-of-the-art approaches incur overheads from storing, updating and partitioning complete application graphs on each device, which limits their utility and scalability in resource constrained mobile environments. Hence, this paper presents a novel distributed approach to application representation in which each device maintains a graph consisting only of components in its memory space, while maintaining abstraction elements for components in remote devices. This approach removes the need to store and update complete application graphs on each device and reduces the cost of partitioning an application during adaptation. In addition, an extension to an existing application graph partitioning heuristic is proposed to utilize this representation approach. An evaluation involving computationally heavy open-source applications adapting in a heterogeneous collaboration showed that the new approach reduced graph update network cost by 100%, collaboration-wide memory cost by between 37% and 50%, power usage by between 63% and 93%, and adaptation time by between 19.47% and 98%, while improving efficacy of adaptation by 12% and 34% for two of the considered applications.
85|12||HPobSAM for modeling and analyzing IT Ecosystems â Through a case study|The next generation of software systems includes systems composed of a large number of distributed, decentralized, autonomous, interacting, cooperating, organically grown, heterogeneous, and continually evolving subsystems, which we call IT Ecosystems. Clearly, we need novel models and approaches to design and develop such systems which can tackle the long-term evolution and complexity problems. In this paper, our framework to model IT Ecosystems is a combination of centralized control (top-down) and self-organizing (bottom-up) approach. We use a flexible formal model, HPobSAM, that supports both behavioral and structural adaptation/evolution. We use a detailed, close to real-life, case study of a smart airport to show how we can use HPobSAM in modeling, analyzing and developing an IT Ecosystem. We provide an executable formal specification of the model in Maude, and use LTL model checking and bounded state space search provided by Maude to analyze the model. We develop a prototype of our case study designed by HPobSAM using Java and Ponder2. Due to the complexity of the model, we cannot check all properties at design time using Maude. We propose a new approach for run-time verification of our case study, and check different types of properties which we could not verify using model checking. As our model uses dynamic policies to control the behavior of systems which can be modified at runtime, it provides us a suitable capability to react to the property violation by modification of policies.
85|12||LossEstimate: Distributed failure estimation in wireless networks|The ongoing evolution of software-intensive distributed systems to ultra-large-scale (ULS) systems require innovative methods for building, running, and managing these systems. Component self-adaptation and self-configuration properties are thus becoming mandatory requirements in order to cope with application complexity. An increasing number of systems, such as video content distribution, make use of distributed feed-back mechanisms to build-up intelligent, robust and self-managing services. Technology wise, with the wide-spread usage of wireless communication interfaces on today's mobile devices, communication failures are an ever increasing nuisance in the design of distributed self-adaptive services and applications.
85|12||QoS and energy management with Petri nets: A self-adaptive framework|Energy use is becoming a key design consideration in computing infrastructures and services. In this paper we focus on service-based applications and we propose an adaptation framework that can be used to reduce power consumption according to the observed workload. The adaptation guarantees a trade-off between energy consumption and system performance. The approach is based on the principle of proportional energy consumption obtained by scaling down energy for unused resources, considering both the number of servers switched on and their operating frequencies. Stochastic Petri nets are proposed for the modeling of the framework concerns, their analyses give results about the trade-offs. The application of the approach to a simple case study shows its usefulness and practical applicability. Finally, different types of workloads are analyzed with validation purposes.
85|12||Adam: Identifying defects in context-aware adaptation|Context-aware applications, as a typical type of self-adaptive software systems, are receiving increasing attention. These applications continually adapt to environmental changes in an autonomic way. However, their adaptation may contain defects when the complexity of modeling all environmental changes is beyond a developer's ability. Such defects can cause failure to the adaptation and result in application crash or freezing. Relating these failures back to responsible defects is challenging. In this paper we propose a novel approach, called Adam, to assist identifying defects in the context-aware adaptation. Adam monitors runtime errors for an application, logs relevant error information, and relates them to responsible defects in this application. To make our Adam approach feasible, we investigate the error types that are commonly exhibited by various failures reported in context-aware applications. Adam detects these errors in order to identify responsible defects in context-aware applications. To detect these errors, Adam formally models the adaptation semantics for context-aware applications, and integrates into them a set of assertion checkers with respect to these error types. We experimentally evaluated Adam through three context-aware applications. The experiments reported promising results that Adam can effectively detect errors, identify their responsible defects in applications, and give useful hints on how these defects can be fixed.
85|12||Analysing monitoring and switching problems for adaptive systems|In the field of pervasive and ubiquitous computing, context-aware adaptive systems need to monitor changes in their environment in order to detect violations of requirements and switch their behaviour in order to continue satisfying requirements. In a complex and rapidly changing environment, identifying what to monitor and deciding when and how to switch behaviours effectively is difficult and error prone. The goal of our research is to provide systematic and, where possible, automated support for the software engineer developing such adaptive systems.
85|12||A development framework and methodology for self-adapting applications in ubiquitous computing environments|Today software is the main enabler of many of the appliances and devices omnipresent in our daily life and important for our well being and work satisfaction. It is expected that the software works as intended, and that the software always and everywhere provides us with the best possible utility. This paper discusses the motivation, technical approach, and innovative results of the MUSIC project. MUSIC provides a comprehensive software development framework for applications that operate in ubiquitous and dynamic computing environments and adapt to context changes. Context is understood as any information about the user needs and operating environment which vary dynamically and have an impact on design choices. MUSIC supports several adaptation mechanisms and offers a model-driven application development approach supported by a sophisticated middleware that facilitates the dynamic and automatic adaptation of applications and services based on a clear separation of business logic, context awareness and adaptation concerns. The main contribution of this paper is a holistic, coherent presentation of the motivation, design, implementation, and evaluation of the MUSIC development framework and methodology.
85|12||Stitch: A language for architecture-based self-adaptation|Requirements for high availability in computing systems today demand that systems be self-adaptive to maintain expected qualities-of-service in the presence of system faults, variable environmental conditions, and changing user requirements. Autonomic computing tackles the challenge of automating tasks that humans would otherwise have to perform to achieve this goal. However, existing approaches to autonomic computing lack the ability to capture routine human repair tasks in a way that takes into account the business context humans use in selecting an appropriate form of adaptation, while dealing with timing delays and uncertainties in outcome of repair actions. In this article, we present Stitch, a language for representing repair strategies within the context of an architecture-based self-adaptation framework. Stitch supports the explicit representation of repair decision trees together with the ability to express business objectives, allowing a self-adaptive system to select a strategy that has optimal utility in a given context, even in the presence of potential timing delays and outcome uncertainty.
85|12||A semantic translation method for data communication protocols|Protocol translation is a method for transforming pieces of information from a source protocol into relevant target protocol formats in order to communicate between heterogeneous legacy systems in interoperability environments. There are some existing protocol translation technologies for network protocols and simple data messages, but these have semantic information problems such as information distortion or incorrect information translations when applied to semantic messages. To deal with these problems, we propose a method that translates messages to semantically equivalent messages. We devised the method to be practical in implementing semantic information-sensitive gateway systems. We conducted a comparative experiment with the translation approach developed in the defense domain. We found that our semantic translation method provides more accurate and efficient message translations in interoperability environments.
85|12||Side channel analysis attacks using AM demodulation on commercial smart cards with SEED|We investigate statistical side channel analysis attacks on the SEED block cipher implemented in two commercial smart cards used in a real-world electronic payment system. The first one is a contact-only card and the second one is a combination card. Both cards have no masking scheme at algorithm level and the combination card supports only hiding techniques in hardware level. Our results show that an unprotected implementation of SEED allows one to recover the secret key with low number of power or electromagnetic traces. Moreover, this paper clearly confirms that, although hiding countermeasures such as random current and random noise may increase the number of power traces needed for a successful attack, it is difficult to provide sufficient resistance to side channel attacks for itself. We believe that our results in this research will also be beneficial to the analysis and protection of other algorithms and commercial smart cards.
85|12||Grindstone4Spam: An optimization toolkit for boosting e-mail classification|Resulting from the huge expansion of Internet usage, the problem of unsolicited commercial e-mail (UCE) has grown astronomically. Although a good number of successful content-based anti-spam filters are available, their current utilization in real scenarios is still a long way off. In this context, the SpamAssassin filter offers a rule-based framework that can be easily used as a powerful integration and deployment tool for the fast development of new anti-spam strategies. This paper presents Grindstone4Spam, a publicly available optimization toolkit for boosting SpamAssassin performance. Its applicability has been verified by comparing its results with those obtained by the default SpamAssassin software as well as four well-known anti-spam filtering techniques such as Naïve Bayes, Flexible Bayes, Adaboost and Support Vector Machines in two different case studies. The performance of the proposed alternative clearly outperforms existing approaches working in a cost-sensitive scenario.
85|12||Technology flexibility as enabler of robust application development in community source: The case of Kuali and Sakai|Technology flexibility has been an important topic in software engineering since the start of computerized business applications, which require frequent changes to system specifications due to ever changing business requirements. Achieving a higher degree of technology flexibility has been a long-running challenge to software engineers and project managers. Recently, there has been a new software development approach called “community source” consisting of numerous development partners that are also users of the software. In community source, technology flexibility is even more important than usual due to the increase in complexity and uncertainty of software requirements by its many development partners in the community. In this paper, we investigate two community source cases, i.e., Kuali and Sakai, and examine how technology flexibility is achieved in application software engineering. The principles generated from this study should offer useful insights to the continuous efforts toward making more robust business applications in support of agile enterprises.
85|2|http://www.sciencedirect.com/science/journal/01641212/85/2|Special issue with selected papers from the 23rd Brazilian Symposium on Software Engineering|
85|2||Towards understanding the underlying structure of motivational factors for software engineers to guide the definition of motivational programs|In this article, factors influencing the motivation of software engineers is studied with the goal of guiding the definition of motivational programs.
85|2||Applying and evaluating concern-sensitive design heuristics|Manifestation of crosscutting concerns in software systems is often an indicative of design modularity flaws and further design instabilities as those systems evolve. Without proper design evaluation mechanisms, the identification of harmful crosscutting concerns can become counter-productive and impractical. Nowadays, metrics and heuristics are the basic mechanisms to support their identification and classification either in object-oriented or aspect-oriented programs. However, conventional mechanisms have a number of limitations to support an effective identification and classification of crosscutting concerns in a software system. In this paper, we claim that those limitations are mostly caused by the fact that existing metrics and heuristics are not sensitive to primitive concern properties, such as either their degree of tangling and scattering or their specific structural shapes. This means that modularity assessment is rooted only at conventional attributes of modules, such as module cohesion, coupling and size. This paper proposes a representative suite of concern-sensitive heuristic rules. The proposed heuristics are supported by a prototype tool. The paper also reports an exploratory study to evaluate the accuracy of the proposed heuristics by applying them to seven systems. The results of this exploratory analysis give evidences that the heuristics offer support for: (i) addressing the shortcomings of conventional metrics-based assessments, (ii) reducing the manifestation of false positives and false negatives in modularity assessment, (iii) detecting sources of design instability, and (iv) finding the presence of design modularity flaws in both object-oriented and aspect-oriented programs. Although our results are limited to a number of decisions we made in this study, they indicate a promising research direction. Further analyses are required to confirm or refute our preliminary findings and, so, this study should be seen as a stepping stone on understanding how concerns can be useful assessment abstractions. We conclude this paper by discussing the limitations of this exploratory study focusing on some situations which hinder the accuracy of concern-sensitive heuristics.
85|2||Identifying thresholds for object-oriented software metrics|Despite the importance of software metrics and the large number of proposed metrics, they have not been widely applied in industry yet. One reason might be that, for most metrics, the range of expected values, i.e., reference values are not known. This paper presents results of a study on the structure of a large collection of open-source programs developed in Java, of varying sizes and from different application domains. The aim of this work is the definition of thresholds for a set of object-oriented software metrics, namely: LCOM, DIT, coupling factor, afferent couplings, number of public methods, and number of public fields. We carried out an experiment to evaluate the practical use of the proposed thresholds. The results of this evaluation indicate that the proposed thresholds can support the identification of classes which violate design principles, as well as the identification of well-designed classes. The method used in this study to derive software metrics thresholds can be applied to other software metrics in order to find their reference values.
85|2||Automating the product derivation process of multi-agent systems product lines|Agent-oriented software engineering and software product lines are two promising software engineering techniques. Recent research work has been exploring their integration, namely multi-agent systems product lines (MAS-PLs), to promote reuse and variability management in the context of complex software systems. However, current product derivation approaches do not provide specific mechanisms to deal with MAS-PLs. This is essential because they typically encompass several concerns (e.g., trust, coordination, transaction, state persistence) that are constructed on the basis of heterogeneous technologies (e.g., object-oriented frameworks and platforms). In this paper, we propose the use of multi-level models to support the configuration knowledge specification and automatic product derivation of MAS-PLs. Our approach provides an agent-specific architecture model that uses abstractions and instantiation rules that are relevant to this application domain. In order to evaluate the feasibility and effectiveness of the proposed approach, we have implemented it as an extension of an existing product derivation tool, called GenArch. The approach has also been evaluated through the automatic instantiation of two MAS-PLs, demonstrating its potential and benefits to product derivation and configuration knowledge specification.
85|2||To lock, or not to lock: That is the question|Mechanisms to control concurrent access over project artefacts are needed to execute the software development process in an organized way. These mechanisms are implemented by concurrency control policies in version control systems that may inhibit (i.e. ‘to lock’) or allow (i.e., ‘not to lock’) parallel development. This work presents a novel approach named Orion that analyzes the project's historical changes and suggests the most appropriate concurrency control policy for each software element. This suggestion aims at minimizing conflict situations and thus improving the productivity of the development team. In addition, it identifies critical elements that do not work well with any of these policies and are candidates to refactoring. We evaluated Orion through two experimental studies and the results, which indicated it was effective, led us to a prototype implementation. Apart from the Orion approach this paper also presents the planning, execution, and analysis stages of the evaluation, and details of prototype internals.
85|2||A novel color image encryption algorithm based on DNA sequence operation and hyper-chaotic system|A new color image encryption algorithm based on DNA (Deoxyribonucleic acid) sequence addition operation is presented. Firstly, three DNA sequence matrices are obtained by encoding the original color image which can be converted into three matrices R, G and B. Secondly, we use the chaotic sequences generated by Chen's hyper-chaotic maps to scramble the locations of elements from three DNA sequence matrices, and then divide three DNA sequence matrices into some equal blocks respectively. Thirdly, we add these blocks by using DNA sequence addition operation and Chen's hyper-chaotic maps. At last, by decoding the DNA sequence matrices and recombining the three channels R, G and B, we get the encrypted color image. The simulation results and security analysis show that our algorithm not only has good encryption effect, but also has the ability of resisting exhaustive attack, statistical attack and differential attack.
85|2||A hybrid channel assignment strategy to QoS support of video-streaming over multi-channel ad hoc networks|Currently multi-channel multi-interface ad hoc networks (multi-channel ad hoc networks) have received amount of interest, especially under the context of real-time traffics, such as video applications. Channel assignment is one of the key challenges in multi-channel ad hoc networks. In this paper, based on a representative hybrid channel assignment strategy named as HMCP, a statistic link load based hybrid channel assignment strategy, SLL-HCA, is presented to obtain a better channel assignment metric than that in HMCP. SLL-HCA is based on the HMCP protocol and adopts the statistic link load metric to ensure load balancing in a two-hop neighborhood, and to prevent both the hidden node problem and the exposed node problem. In addition, an enhanced strategy, VE-SLL-HCA, by setting harsher channel assignment conditions and reserving lower interference of routing path for video-streaming traffic than other non-video traffic, is proposed to improve the QoS support of video-streaming over multi-channel ad hoc networks. Simulation results show that SLL-HCA achieves better throughput performance than HMCP in the presence of background traffic including CBR traffic or VBR traffic; and, the PSNR QoS metric for video-streaming is enhanced when using VE-SLL-HCA compared with SLL-HCA and HMCP, which confirms the validity of the proposed strategy for video-streaming traffics. Moreover, the simulation results also indicate that non-video traffic unlikely suffers an unacceptable performance in terms of throughput.
85|2||Lossless data hiding in JPEG bitstream|This paper proposes a method of embedding secret data into JPEG bitstream by Huffman code mapping. Although JPEG defined 162 different variable length code (VLC) for AC coefficients, many codes are not used during image compression. According to the statistical results of VLC usage in a cover, we map the unused codes to used codes. The relationships of code mapping are performed by modifying the Huffman values defined in the file header. During data hiding, we replace the codes appearing in bitstream by the mapped codes according to the secret bits. The proposed embedding method preserves the image with no quality distortion and provides more embedding capacity.
85|2||An efficient short certificate-based signature scheme|Certificate-based cryptography combines the merits of traditional public key infrastructure (PKI) and identity-based cryptography. It does not have the key escrow problem in identity-based cryptography, and eliminates the certificate revocation problem and third-party queries in traditional PKI. In this paper, we first refine the security model of certificate-based signatures introduced in EuroPKI’07. We then present a short certificate-based signature scheme, which is proven to be existentially unforgeable against adaptive chosen message attacks in the random oracle model. Our scheme requires only one pairing operation (and three pre-computable pairing operations) in signature generation and verification. In addition, the signature size of our scheme is only one group element. To the best of our knowledge, the signature size of our scheme is the shortest and the computational cost is the lowest when compared with other concrete certificate-based signature schemes in the literature. This makes our scheme possess strong applicability in situations with limited bandwidth and power-constrained devices.
85|2||Towards developing consistent misuse case models|Secure software development should begin at the early stages of the development life cycle. Misuse case modeling is a technique that stems from traditional use case modeling, which facilitates the elicitation and modeling functional security requirements at the requirements phase. Misuse case modeling is an effective vehicle to potentially identify a large subset of these threats. It is therefore crucial to develop high quality misuse case models otherwise end system developed will be vulnerable to security threats. Templates to describe misuse cases are populated with syntax-free natural language content. The inherent ambiguity of syntax-free natural language coupled with the crucial role of misuse case models in development can have a very detrimental effect. This paper proposes a structure that will guide misuse case authors towards developing consistent misuse case models. This paper also presents a process that utilizes this structure to ensure the consistency of misuse case models as they evolve, eliminating potential damages caused by inconsistencies. A tool was developed to provide automation support for the proposed structure and process. The feasibility and application of this approach were demonstrated using two real-world case studies.
85|2||Provably secure three-party password authenticated key exchange protocol in the standard model|Three-party password authenticated key exchange protocol is a very practical mechanism to establish secure session key through authenticating each other with the help of a trusted server. Most three-party password authenticated key exchange protocols only guarantee security in the random oracle model. However, a random oracle based cryptographic construction may be insecure when the oracle is replaced by real function. Moreover, some previous unknown attacks appear with the advance of the adversary capability. Therefore, a suitable standard model which can imitate a wider variety of attack scenarios for 3PAKE protocol is needed. Aim at resisting dictionary attack, unknown key-share attack and password-compromise impersonation attack, an expanded standard model for 3PAKE protocol is given. Meanwhile, through applying ElGamal encryption scheme and pseudorandom function, a specific three-party password authenticated key exchange protocol is proposed. The security of the proposed protocol is proven in the new standard model. The result shows that the present protocol has stronger security by comparing with other existing protocols, which covers the following security properties: (1) semantic security, (2) key privacy, (3) client-to-server authentication, (4) mutual authentication, (5) resistance to various known attacks, and (6) forward security.
85|2||Reconciling software development models: A quasi-systematic review|The purpose of this paper is to characterize reconciliation among the plan-driven, agile, and free/open source software models of software development.
85|2||Concept vector for semantic similarity and relatedness based on WordNet structure|We define WordNet based hierarchy concept tree (HCT) and hierarchy concept graph (HCG), HCT contains hyponym/hypernym kind of relation in WordNet while HCG has more meronym/holonym kind of edges than in HCT, and present an advanced concept vector model for generalizing standard representations of concept similarity in terms of WordNet-based HCT. In this model, each concept node in the hierarchical tree has ancestor and descendent concept nodes composing its relevancy nodes, thus a concept node is represented as a concept vector according to its relevancy nodes’ local density and the similarity of the two concepts is obtained by computing the cosine similarity of their vectors. In addition, the model is adjustable in terms of multiple descendent concept nodes. This paper also provides a method by which this concept vector may be applied with regard to HCG into HCT. With this model, semantic similarity and relatedness are computed based on HCT and HCG. The model contains structural information inherent to and hidden in the HCT and HCG. Our experiments showed that this model compares favorably to others and is flexible in that it can make comparisons between any two concepts in a WordNet-like structure without relying on any additional dictionary or corpus information.
85|2||Intrusion-resilient identity-based signature: Security definition and construction|Traditional identity-based signatures depend on the assumption that secret keys are absolutely secure. Once a secret key is exposed, all signatures associated with this secret key have to be reissued. Therefore, limiting the impact of key exposure in identity-based signature is an important task. In this paper, we propose to integrate the intrusion-resilient security into identity-based signatures to deal with their key exposure problem. Compared with forward-secure identity-based signatures and key-insulated identity-based signatures, our proposal can achieve higher security. The proposed scheme satisfies that signatures in any other time periods are secure even after arbitrarily many compromises of base and signer, as long as the compromises do not happen simultaneously. Furthermore, the intruder cannot generate signatures pertaining to previous time periods, even if she compromises base and signer simultaneously to get all their secret information. The scheme enjoys nice average performance. There are no cost parameters including key setup time, key extract time, base (signer) key update time, base (signer) key refresh time, signing time, verifying time, and signature size, public parameter size, base (signer) storage size having complexity more than O(log T) in terms of the total number of time periods T in this scheme. We also give the security definition of intrusion-resilient identity-based signature scheme and prove that our scheme is secure based on this security definition in the random oracle model assuming CDH problem is hard.
85|2||Reversible data hiding of high payload using local edge sensing prediction|This paper tries to analyse a new framework in lossless data hiding research. Its key is how adaptively to get better difference image architectures for given applications. A unique sampled pattern is introduced and described in term of high-similar interpolation image. The proposed method updates a type of conventional algorithm, smoothly accesses to current reconstruction implementation, and outperforms solo weighing approach, particularly in the perceptual quality. Combinational weight factor can be adaptively adjusted by estimating interpolation errors around neighbor pixels. Seeking higher peak value in the difference-image is also our concerns. As an entire algorithm, reversible data hiding based on histogram-difference-shift is also reported. Simulations results demonstrate and verify that our approach is much effective than some recent methods with good generalization performance.
85|2||Collision-based flexible image encryption algorithm|This paper presents a novel image encryption algorithm, which is flexible to encrypt digital images with different precisions. The algorithm is construct from three collision-based dynamical systems: Hardy–Pomeau–Pazzis (HPP) model, rotary element (RE) model and billiard-particle model. Both the HPP and RE model are efficient and can support parallel computing; the billiard-particle is an excellent model for round key expansion. These models are revised to best suits image encryption. Analyses and simulations demonstrate that the proposed algorithm has satisfactory properties of randomness and sensitivity.
85|2||Design patterns selection: An automatic two-phase method|Over many years of research and practices in software development, hundreds of software design patterns have been invented and published. Now, a question which naturally arises is how software developers select the right design patterns from all relevant patterns to solve design problems in the software design phase. To address this issue, in this paper, we propose a two-phase method to select a right design pattern. The proposed method is based on a text classification approach that aims to show an appropriate way to suggest the right design pattern(s) to developers for solving each given design problem. There are two advantages of the proposed method in comparison to previous works. First, there is no need for semi-formal specifications of design patterns and second, the suitable design patterns are suggested with their degree of similarity to the design problem. To evaluate the proposed method, we apply it on real problems and several case studies. The experimental results show that the proposed method is promising and effective.
85|2||CONFIDDENT: A model-driven consistent and non-redundant layer-3 firewall ACL design, development and maintenance framework|Design, development, and maintenance of firewall ACLs are very hard and error-prone tasks. Two of the reasons for these difficulties are, on the one hand, the big gap that exists between the access control requirements and the complex and heterogeneous firewall platforms and languages and, on the other hand, the absence of ACL design, development and maintenance environments that integrate inconsistency and redundancy diagnosis. The use of modelling languages surely helps but, although several ones have been proposed, none of them has been widely adopted by industry due to a combination of factors: high complexity, unsupported firewall important features, no integrated model validation stages, etc. In this paper, CONFIDDENT, a model-driven design, development and maintenance framework for layer-3 firewall ACLs is proposed. The framework includes different modelling stages at different abstraction levels. In this way, non-experienced administrators can use more abstract models while experienced ones can refine them to include platform-specific features. CONFIDDENT includes different model diagnosis stages where the administrators can check the inconsistencies and redundancies of their models before the automatic generation of the ACL to one of the many of the market-leader firewall platforms currently supported.
85|3|http://www.sciencedirect.com/science/journal/01641212/85/3|Novel approaches in the design and implementation of system/software architectures|
85|3||Changing attitudes towards the generation of architectural models|Architectural design is an important activity, but the understanding of how it is related to requirements modeling is rather limited. It is worth noting that goal orientation is an increasingly recognized paradigm for eliciting, modeling, specifying, and analyzing software requirements. However, it is not clear how goal models are related to architectural models. In this paper we present an approach based on model transformations to derive architectural structural specifications from system goals. The source and target languages are respectively the i* (iStar) modeling language and the Acme architectural description language. A real case study is used to show the feasibility of our approach.
85|3||A proposal to detect errors in Enterprise Application Integration solutions|Enterprise Application Integration (EAI) solutions comprise a set of specific-purpose processes that implement exogenous message workflows. The goal is to keep a number of applications’ data in synchrony or to develop new functionality on top of them. Such solutions are prone to errors because they are highly distributed and usually involve applications that were not designed with integration concerns in mind. This has motivated many authors to work on provisioning EAI solutions with fault-tolerance capabilities. In this article we analyse EAI solutions from two orthogonal perspectives: viewpoint (orchestration versus choreography) and execution model (process- versus task-based model). A review of the literature shows that current proposals are bound to a specific viewpoint or execution model or have important limitations. To address the problem, we have devised an error monitor that can be used to provision EAI solutions with fault-tolerance capabilities. Our theoretical analysis proves that the algorithms we use are computationally tractable, and our experimental results prove that they are efficient enough to be used in situations in which the workload is very high.
85|3||Enabling correct design and formal analysis of Ambient Assisted Living systems|Ambient Assisted Living (AAL) systems intend to provide services that enable people with specific needs to live an independent and safe life. Emergency treatment services are critical, time-constrained, and require compliance to numerous non-functional (or quality) requirements. In conventional approaches, often, non-functional requirements are kept outside the modeling scope and as such, their verification is also overlooked. For this reason, the specification and verification of Non-functional requirements (NFR) in this kind of services is a key issue. This paper presents a verification approach based on timed traces semantics and a methodology based on UML-RT models (MEDISTAM-RT) to check the fulfillment of non-functional requirements, such as timeliness and safety (deadlock freeness), and to assure the correct functioning of the AAL systems. We validate this approach by its application to an Emergency Assistance System for monitoring people suffering from cardiac alteration with syncope.
85|3||A reusable structural design for mobile collaborative applications|Architecting mobile collaborative applications has always been a challenge for designers. However, counting on a structural design as a reference can help developers to reduce risks and efforts involved in system design. This article presents a reusable architecture which helps modeling the communication and coordination services required by mobile collaborative applications to support collaboration among users. This architecture has been used as a basis for the design of several mobile systems. Two of them are presented in this article to show the applicability of the proposal to real world collaborative systems.
85|3||Deriving detailed design models from an aspect-oriented ADL using MDD|Software architects can separate crosscutting concerns more appropriately by using an aspect-oriented ADL, concretely AO-ADL. This paper illustrates how aspect-orientation and model-driven development technologies can be used to enhance the system design phase; by automatically deriving detailed designs that take into account the “aspects” identified at the architectural level. Specifically, we have defined model-to-model transformation rules to automatically generate either aspect-oriented or object-oriented UML 2.0 models, closing the gap between ADLs and the notations used at the detailed design phase. By using AO-ADL it is possible to specify separately crosscutting concerns and base functionality. Another advantage of using AO-ADL is that it allows the specification of parameterizable architectures, promoting the definition of architectural templates. AO-ADL, then, enforces the specification of crosscutting concerns as separate architectural templates, which can be later instantiated and integrated with the core functionality of the system being developed. The AO-ADL language and the transformation rules from AO-ADL to UML 2.0 are available throughout the AO-ADL Tool Suite, which can be used to progressively refine and elaborate aspect-oriented software architectures. These refined architectures are the starting point of the detailed design phase. This means that our approach provides support to automatically generate a skeleton of the detailed design that preserves the information about the crosscutting and the non-crosscutting functionalities identified and modelled at the architecture level.
85|3||Towards supporting the software architecture life cycle|Software architecture is a central element during the whole software life cycle. Among other things, software architecture is used for communication and documentation, for design, for reasoning about important system properties, and as a blueprint for system implementation. This is expressed by the software architecture life cycle, which emphasizes architecture-related activities like architecture design, implementation, and analysis in the context of a software life cycle. While individual activities of the software architecture life cycle are supported very well, a seamless approach for supporting the whole life cycle is still missing. Such an approach requires the integration of disparate information, artifacts, and tools into one consistent information model and environment. In this article we present such an approach. It is based on a semi-formal architecture model, which is used in all activities of the architecture life cycle, and on a set of extensible and integrated tools supporting these activities. Such an integrated approach provides several benefits. Potentially redundant activities like the creation of multiple architecture descriptions are avoided, the captured information is always consistent and up-to-date, extensive tracing between different information is possible, and interleaving activities in incremental development and design are supported.
85|3||Empirical findings on team size and productivity in software development|The size of software project teams has been considered to be a driver of project productivity. Although there is a large literature on this, new publicly available software repositories allow us to empirically perform further research. In this paper we analyse the relationships between productivity, team size and other project variables using the International Software Benchmarking Standards Group (ISBSG) repository. To do so, we apply statistical approaches to a preprocessed subset of the ISBSG repository to facilitate the study. The results show some expected correlations between productivity, effort and time as well as corroborating some other beliefs concerning team size and productivity. In addition, this study concludes that in order to apply statistical or data mining techniques to these type of repositories extensive preprocessing of the data needs to be performed due to ambiguities, wrongly recorded values, missing values, unbalanced datasets, etc. Such preprocessing is a difficult and error prone activity that would need further guidance and information that is not always provided in the repository.
85|3||Wireless sensor network-based fire detection, alarming, monitoring and prevention system for Bord-and-Pillar coal mines|Fire is a major concern for those who work in underground coal mines. Coal mine fire can occur at any time and often results in partial or total evacuation of mine personnel and could result in the loss of lives. Therefore, having a warning system that is capable of detecting fire and generating an alarm is important. In this paper, we present the response time of a proposed system for detecting fire hazard in a Bord-and-Pillar coal mine panel (Hustrulid and Bullock, 2001). It uses wireless sensor networks (WSNs), and can be used to detect the exact fire location and spreading direction, and also provide the fire prevention system to stop the spread of fire to save the natural resources and the mining personnel from fire. The proposed system is capable of early detection of fire and generating alarm in case of emergencies. The performance of the proposed system has been evaluated through rigorous simulations. Simulation results show that the average network delay varies almost linearly with the increasing the number of hops.
85|3||A lightweight framework for describing software practices|In order to maximise software project outcomes, software organisations adapt development methodologies and implement practices in a way that is appropriate for project contexts. This suggests that ‘Best Practice’ is context-dependent. To better understand the contextual nature of best practice, we want to explore how organisations actually go about achieving software objectives. We require a research framework that captures this information in a way that makes no assumptions about practices and that is descriptive in nature. We have developed a framework based on the perspective that practices exist to meet specific objectives. We have experimented with the framework by using it to capture the practices of three New Zealand organisations and by application to an idealised XP process. Our capture of organisational practices revealed interesting mechanisms for further study, including a dependence upon informal practices linked with strong communication and the idea of ‘push’ versus ‘pull’ for information elicitation. Our capture of XP exposed some context-dependent risks.
85|3||Making sense of business process descriptions: An experimental comparison of graphical and textual notations|How effective is a notation in conveying the writer's intent correctly? This paper identifies understandability of design notations as an important aspect which calls for an experimental comparison. We compare the success of university students in interpreting business process descriptions, for an established graphical notation (BPMN) and for an alternative textual notation (based on written use-cases). Because a design must be read by diverse communities, including technically trained professionals such as developers and business analysts, as well as end-users and stakeholders from a wider business setting, we used different types of participants in our experiment. Specifically, we included those who had formal training in process description, and others who had not. Our experiments showed significant increases by both groups in their understanding of the process from reading the textual model. This was not so for the graphical model, where only the trained readers showed significant increases. This finding points at the value of educating readers of graphical descriptions in that particular notation when they become exposed to such models in their daily work.
85|3||Automatic execution of business process models: Exploiting the benefits of Model-driven Engineering approaches|The business goals of an enterprise process are traced to business process models with the aim of being carried out during the execution stage. The automatic translation from these models to fully executable code which can be simulated and round-trip engineered is still an open challenge in the Business Process Management field. Model-driven Engineering has proposed a set of methodologies with which to solve the existing gap between business analysts and software developers, but the expected results have not as yet been achieved. In this paper, a new approach to solve this challenge is proposed. This approach is based on the integration of SOD-M, a model-driven method for the development of service-oriented systems, and DENEB, a platform for the development and execution of flexible business processes. SOD-M provides business analysts with a methodology that can be used to transform their business goals into composition service models, a type of model that represents business processes. The use of the Eclipse Modelling Framework and the ATLAS Transformation Language allows this model to be automatically transformed into a DENEB workflow model, resulting in a business process that is coded by a class of high-level Petri-nets and is directly executable in DENEB. The application of the proposal presented herein is illustrated by means of a real system related to the management of medical images.
85|3||A history-based cost-cognizant test case prioritization technique in regression testing|Software testing is typically used to verify whether the developed software product meets its requirements. From the result of software testing, developers can make an assessment about the quality or the acceptability of developed software. It is noted that during testing, the test case is a pair of input and expected output, and a number of test cases will be executed either sequentially or randomly. The techniques of test case prioritization usually schedule test cases for regression testing in an order that attempts to increase the effectiveness. However, the cost of test cases and the severity of faults are usually varied. In this paper, we propose a method of cost-cognizant test case prioritization based on the use of historical records. We gather the historical records from the latest regression testing and then propose a genetic algorithm to determine the most effective order. Some controlled experiments are performed to evaluate the effectiveness of our proposed method. Evaluation results indicate that our proposed method has improved the fault detection effectiveness. It can also been found that prioritizing test cases based on their historical information can provide high test effectiveness during testing.
85|3||An efficient RSA-based certificateless signature scheme|Until now, the only known construction of certificateless signature scheme is mainly based on the rather new and untested assumptions related to bilinear maps. But the implementations of pairings are more time-consuming than exponentiation operator in a RSA group. As an industry standard cryptographic algorithm, RSA is widely applied in real-life scenarios and provides many interfaces for the applied software. However, to the best of our knowledge, there does not exist RSA-based certificateless signature scheme. To overcome this problem, we present a RSA-based construction of certificateless signature scheme in the paper. And the scheme is shown to be secure in the random oracles model. The security of the scheme is closely related to the RSA problem and the discrete logarithm problem.
85|3||Further observation on proxy re-encryption with keyword search|Recently Shao et al. proposed an interesting cryptographic primitive called proxy re-encryption with keyword search (PRES). The main novelty is simultaneously realizing the functionality of proxy re-encryption and keyword search in one primitive. In this paper, we further extend their research by introducing a new primitive: constrained single-hop unidirectional proxy re-encryption supporting conjunctive keywords search (CPRE-CKS). Our results are as following: (1) In Shao's PRES scheme, the proxy can re-encrypt all the second level ciphertext. While in our CPRE-CKS proposal, the proxy can only re-encrypt those second level ciphertexts which contain the corresponding keywords. (2) We give the definition and security model for CPRE-CKS, and propose a concrete scheme and prove its security. (3) On the way to construct a secure CPRE-CKS scheme, we found a flaw in the security proof of Hwang et al.'s public key encryption with conjunctive keyword search (PECK) scheme proposed in Pairing’07.
85|3||Achieving key privacy without losing CCA security in proxy re-encryption|In proxy re-encryption (PRE), a semi-trusted proxy can transform a ciphertext under the delegator's public key into another ciphertext that the delegatee can decrypt by his/her own private key. However, the proxy cannot access the plaintext. Due to its transformation property, proxy re-encryption can be used in many applications, such as encrypted email forwarding. Some of these applications require that the underlying PRE scheme is CCA-secure and key-private. However, to the best of our knowledge, none of the existing PRE schemes satisfy this security requirement in the standard model. In this paper, based on the 5-Extended Decision Bilinear Diffie–Hellman assumption and Decision Diffie–Hellman assumption, we propose the first such PRE scheme, which solves an open problem left by  Ateniese et al. (2009).
85|3||Organizational adoption of open source software|Organizations and individuals can use open source software (OSS) for free, they can study its internal workings, and they can even fix it or modify it to make it suit their particular needs. These attributes make OSS an enticing technological choice for a company. Unfortunately, because most enterprises view technology as a proprietary differentiating element of their operation, little is known about the extent of OSS adoption in industry and the key drivers behind adoption decisions. In this article we examine factors and behaviors associated with the adoption of OSS and provide empirical findings through data gathered from the US Fortune-1000 companies. The data come from each company's web browsing and serving activities, gathered by sifting through more than 278 million web server log records and analyzing the results of thousands of network probes. We show that the adoption of OSS in large US companies is significant and is increasing over time through a low-churn transition, advancing from applications to platforms. Its adoption is a pragmatic decision influenced by network effects. It is likelier in larger organizations and those with many less productive employees, and is associated with IT and knowledge-intensive work and operating efficiencies.
85|3||Evolution and change management of XML-based systems|XML is de-facto a standard language for data exchange. Structure of XML documents exchanged among different components of a system (e.g. services in a Service-Oriented Architecture) is usually described with XML schemas. It is a common practice that there is not only one but a whole family of XML schemas each applied in a particular logical execution part of the system. In such systems, the design and later maintenance of the XML schemas is not a simple task.
85|3||Fully CCA2 secure identity-based broadcast encryption with black-box accountable authority|
85|3||Bridging the gap between requirements and design: An approach based on Problem Frames and SysML|The relation between the requirements specification and the design has been widely investigated with the aim to bridge the gap between the two artifacts. The goal is to find effective mechanisms to generate the system design starting from the analysis and specification of the requirements.
85|3||A dynamic layout of sliding window for frequent itemset mining over data streams|Mining frequent patterns over data streams is an interesting and challenging problem due to the emergence of new applications and limited resources of main memory and processing power. In this study, a novel sliding window based method for efficient mining of frequent patterns over data streams is proposed. This method provides a dynamic layout of sliding window by utilizing a set of simple lists for items existing within the window. For every item within the window, the most memory efficient list type based on its frequency is selected to store its occurrence information. A novel window adjustment technique including list type conversions is used to control the memory usage when the concept change occurs. At any time, if a user issues a request for frequent patterns in the recent window, a suitable approach based on the current content of the window is selected for the mining process. In comparison with recently proposed algorithms, empirical results show the superiority of the proposed method with multiple orders of magnitude in terms of runtime and memory usage.
85|3||Gateway-oriented password-authenticated key exchange protocol in the standard model|A gateway-oriented password-based authenticated key exchange (GPAKE) is a 3-party protocol, which allows a client and a gateway to establish a common session key with the help of an authentication server. GPAKE protocols are suitable for mobile communication environments such as GSM (Global System for Mobile Communications) and 3GPP (The Third Generation Partnership Project). To date, most of the published protocols for GPAKE have been proven secure in the random oracle model. In this paper, we present the first provably-secure GPAKE protocol in the standard model. It is based on the 2-party password-authenticated key exchange protocol of Jiang and Gong. The protocol is secure under the DDH assumption (without random oracles). Furthermore, it can resist undetectable on-line dictionary attacks. Compared with previous solutions, our protocol achieves stronger security with similar efficiency.
85|4|http://www.sciencedirect.com/science/journal/01641212/85/4|Decision tree classifiers sensitive to heterogeneous costs|
85|4||Rolling-horizon scheduling for energy constrained distributed real-time embedded systems|Energy-efficient scheduling approaches are critical to battery driven real-time embedded systems. Traditional energy-aware scheduling schemes are mainly based on the individual task scheduling. Consequently, the scheduling space for each task is small, and the schedulability and energy saving are very limited, especially when the system is heavily loaded. To remedy this problem, we propose a novel rolling-horizon (RH) strategy that can be applied to any scheduling algorithm to improve schedulability. In addition, we develop a new energy-efficient adaptive scheduling algorithm (EASA) that can adaptively adjust supply voltages according to the system workload for energy efficiency. Both the RH strategy and EASA algorithm are combined to form our scheduling approach, RH-EASA. Experimental results show that in comparison with some typical traditional scheduling schemes, RH-EASA can achieve significant energy savings while meeting most task deadlines (namely, high schedulability) for distributed real-time embedded systems with dynamic workloads.
85|4||A documentation framework for architecture decisions|In this paper, we introduce a documentation framework for architecture decisions. This framework consists of four viewpoint definitions using the conventions of ISO/IEC/IEEE 42010, the new international standard for the description of system and software architectures. The four viewpoints, a Decision Detail viewpoint, a Decision Relationship viewpoint, a Decision Chronology viewpoint, and a Decision Stakeholder Involvement viewpoint satisfy several stakeholder concerns related to architecture decision management.
85|4||Data management for component-based embedded real-time systems: The database proxy approach|We introduce the concept of database proxies intended to mitigate the gap between two disjoint productivity-enhancing techniques: component based software engineering (CBSE) and real-time database management systems (RTDBMS). The two techniques promote opposing design goals and their coexistence is neither obvious nor intuitive. CBSE promotes encapsulation and decoupling of component internals from the component environment, whilst an RTDBMS provide mechanisms for efficient and predictable global data sharing. A component with direct access to an RTDBMS is dependent on that specific RTDBMS and may not be useable in an alternative environment. For components to remain encapsulated and reusable, database proxies decouple components from an underlying database residing in the component framework, while providing temporally predictable access to data maintained in a database. Our approach provide access to features such as extensive data modeling tools, predictable access to hard real-time data, dynamic access to soft real-time data using standardized queries and controlled data sharing; thus allowing developers to employ the full potential of both CBSE and an RTDBMS. Our approach primarily targets embedded systems with a subset of functionality with real-time requirements. The implementation results show that the benefits of using proxies do not come at the expense of significant run-time overheads or less accurate timing predictions.
85|4||Formally based semi-automatic implementation of an open security protocol|This paper presents an experiment in which an implementation of the client side of the SSH Transport Layer Protocol (SSH-TLP) was semi-automatically derived according to a model-driven development paradigm that leverages formal methods in order to obtain high correctness assurance. The approach used in the experiment starts with the formalization of the protocol at an abstract level. This model is then formally proved to fulfill the desired secrecy and authentication properties by using the ProVerif prover. Finally, a sound Java implementation is semi-automatically derived from the verified model using an enhanced version of the Spi2Java framework. The resulting implementation correctly interoperates with third party servers, and its execution time is comparable with that of other manually developed Java SSH-TLP client implementations. This case study demonstrates that the adopted model-driven approach is viable even for a real security protocol, despite the complexity of the models needed in order to achieve an interoperable implementation.
85|4||The novel bilateral â Diffusion image encryption algorithm with dynamical compound chaos|Chaos may be degenerated because of the finite precision effect, hence the new compound two-dimensional chaotic function is presented by exploiting two one-dimensional chaotic functions which are switched randomly. A new chaotic sequence generator is designed by the compound chaos which is proved by Devaney's definition of chaos. The properties of dynamical compound chaotic functions and LFSR are also proved rigorously. A novel bilateral-diffusion image encryption algorithm is proposed based on dynamical compound chaotic function and LFSR, which can produce more avalanche effect and more large key space. The entropy analysis, differential analysis, statistical analysis, cipher random analysis, and cipher sensitivity analysis are introduced to test the security of new scheme. Many experiment results show that the novel image encryption method passes SP 800-22 and DIEHARD standard tests and solves the problem of short cycle and low precision of one-dimensional chaotic function.
85|4||Perpetual development: A model of the Linux kernel life cycle|Software evolution is widely recognized as an important and common phenomenon, whereby the system follows an ever-extending development trajectory with intermittent releases. Nevertheless there have been only few lifecycle models that attempt to portray such evolution. We use the evolution of the Linux kernel as the basis for the formulation of such a model, integrating the progress in time with growth of the codebase, and differentiating between development of new functionality and maintenance of production versions. A unique element of the model is the sequence of activities involved in releasing new production versions, and how this has changed with the growth of Linux. In particular, the release follow-up phase before the forking of a new development version, which was prominent in early releases of production versions, has been eliminated in favor of a concurrent merge window in the release of 2.6.x versions. We also show that a piecewise linear model with increasing slopes provides the best description of the growth of Linux. The perpetual development model is used as a framework in which commonly recognized benefits of incremental and evolutionary development may be demonstrated, and to comment on issues such as architecture, conservation of familiarity, and failed projects. We suggest that this model and variants thereof may apply to many other projects in addition to Linux.
85|4||An efficient and secure multi-server authentication scheme with key agreement|Remote user authentication is used to validate the legitimacy of a remote log-in user. Due to the rapid growth of computer networks, many network environments have been becoming multi-server based. Recently, much research has been focused on proposing remote password authentication schemes based on smart cards for securing multi-server environments. Each of these schemes used either a nonce or a timestamp technique to prevent the replay attack. However, using the nonce technique to withstand the replay attack is potentially susceptible to the man-in-the-middle attack. Alternatively, when employing the timestamp method to secure remote password authentication, it will require the cost of implementing clock synchronization. In order to solve the above two issues, this paper proposes a self-verified timestamp technique to help the smart-card-based authentication scheme not only effectively achieve password-authenticated key agreement but also avoid the difficulty of implementing clock synchronization in multi-server environments. A secure authenticated key agreement should accomplish both mutual authentication and session key establishment. Therefore, in this paper we further give the formal proof on the execution of the proposed authenticated key agreement scheme.
85|4||Intelligent reversible watermarking in integer wavelet domain for medical images|The prime requirement of reversible watermarking scheme is that the system should be able to restore the cover work to its original state after extracting the hidden information. Reversible watermarking approaches, therefore, have wide applications in medical and defense imagery. In this paper, an intelligent reversible watermarking approach GA-RevWM for medical images is proposed. GA-RevWM is based on the concept of block-based embedding using genetic algorithm (GA) and integer wavelet transform (IWT). GA based intelligent threshold selection scheme is applied to improve the imperceptibility for a fixed payload or vice versa. The experimental results show that GA-RevWM provides significant improvement in terms of imperceptibility for a desired level of payload against the existing approaches.
85|4||Factors affecting the success of Open Source Software|With the rapid rise in the use of Open Source Software (OSS) in all types of applications, it is important to know which factors can lead to OSS success. OSS projects evolve and transform over time; therefore success must be examined longitudinally over a period of time. In this research, we examine two measures of project success: project popularity and developer activity, of 283 OSS projects over a span of 3 years, in order to observe changes over time. A comprehensive research model of OSS success is developed which includes both extrinsic and intrinsic attributes. Results show that while many of the hypothesized relationships are supported, there were marked differences in some of the relationships at different points in time lending support to the notion that different factors need to be emphasized as the OSS project unfolds over time.
85|4||High performance dynamic voltage/frequency scaling algorithm for real-time dynamic load management|Modern cyber-physical systems assume a complex and dynamic interaction between the real world and the computing system in real-time. In this context, changes in the physical environment trigger changes in the computational load to execute. On the other hand, task migration services offered by networked control systems require also management of dynamic real-time computing load in nodes. In such systems it would be difficult, if not impossible, to analyse off-line all the possible combinations of processor loads. For this reason, it is worthwhile attempting to define new flexible architectures that enable computing systems to adapt to potential changes in the environment.
85|4||Free and Open Source Software versus Internet content filtering and censorship: A case study|This study critically investigates the main characteristics and features of anti-filtering packages provided by Free and Open Source Software (FOSS). For over a decade, the digital communities around the globe have used FOSS packages not only as an inexpensive way to access to information available on Internet, but also to disseminate thoughts, opinions and concerns about various socio-political and economic matters. Proxy servers and FOSS played a vital role in helping citizens in repressed countries to bypass the state imposed Internet content filtering and censorship practices. On the one hand, proxy servers act as redirectors to websites, and on the other hand, many of these servers are the main source for downloading FOSS anti-filtering software packages. These packages can provide secure web surfing via anonymous web access, data encryption, IP address masking, location concealment, browser history and cookie clean-ups but they also provide proxy software updates as well as domain name updates.
85|4||Debugging applications created by a Domain Specific Language: The IPAC case|
85|4||Attribute-based strong designated-verifier signature scheme|In a strong designated-verifier signature scheme, only the verifier who is designated by a signer can check the validity of a signature via her/his private keys, which process contrasts with the public verifiability of a typical signature scheme. In applications in multi-user communication environments, a user may want to sign a document such that only some specified parties can confirm the signature. To do so, the signer can generate many designated-verifier signatures on a document for various designated verifiers, or she/he signs the document, encrypts the signature, and then sends the encrypted messages to the specified parties. However, the signer has to generate multiple signatures, as determined by the numbers of the designated verifiers in the former method and the latter method, signs a document and then encrypts it, that could lose their designation (which phenomenon is called source hiding). To solve the above problems, this study proposes an attribute-based strong designated-verifier signature scheme with source hiding, such that a signer needs to generate only one signature for a group of designated verifiers with attributes at specified values. To the best of the authors’ knowledge, no attribute-based strong designated-verifier signature scheme has been formally presented before.
85|4||EClass: An execution classification approach to improving the energy-efficiency of software via machine learning|Energy efficiency at the software level has gained much attention in the past decade. This paper presents a performance-aware frequency assignment algorithm for reducing processor energy consumption using Dynamic Voltage and Frequency Scaling (DVFS). Existing energy-saving techniques often rely on simplified predictions or domain knowledge to extract energy savings for specialized software (such as multimedia or mobile applications) or hardware (such as NPU or sensor nodes). We present an innovative framework, known as EClass, for general-purpose DVFS processors by recognizing short and repetitive utilization patterns efficiently using machine learning. Our algorithm is lightweight and can save up to 52.9% of the energy consumption compared with the classical PAST algorithm. It achieves an average savings of 9.1% when compared with an existing online learning algorithm that also utilizes the statistics from the current execution only. We have simulated the algorithms on a cycle-accurate power simulator. Experimental results show that EClass can effectively save energy for real life applications that exhibit mixed CPU utilization patterns during executions. Our research challenges an assumption among previous work in the research community that a simple and efficient heuristic should be used to adjust the processor frequency online. Our empirical result shows that the use of an advanced algorithm such as machine learning can not only compensate for the energy needed to run such an algorithm, but also outperforms prior techniques based on the above assumption.
85|4||A fast algorithm for Huffman decoding based on a recursion Huffman tree|This paper focuses on the time efficiency of Huffman decoding. In this paper, we utilize numerical interpretation to speed up the decoding process. The proposed algorithm firstly transforms the given Huffman tree into a recursion Huffman tree. Then, with the help of the recursion Huffman tree, the algorithm has the possibility to decode more than one symbol at a time if the minimum code length is less than or equal to half of the width of the processing unit. When the minimum code length is larger than the half of the width of the processing unit, the proposed method can still increase the average symbols decoded in one table access (thus speeding up the decoding time). In fact, the experimental results of the test files show that the average number of decoded symbols at one time for the proposed method ranges from 1.91 to 2.13 when the processing unit is 10. The experimental comparisons show that, compared to the conventional binary tree search method and the level-compressed Huffman decoding method, the decoding time of the proposed method is a great improvement.
85|4||Improved preimage attack on one-block MD4|MD4 is a hash function designed by Rivest in 1990. The design philosophy of many important hash functions, such as MD5, SHA-1 and SHA-2, originated from that of MD4. We propose an improved preimage attack on one-block MD4 with the time complexity 295 MD4 compression function operations, as compared to the 21071 complexity of the previous attack by Aoki et al. (SAC 2008). The attack is based on previous methods, but introduces new techniques. We also use the same techniques to improve the pseudo-preimage and preimage attacks on Extended MD4 with 225.2 and 212.6 improvement factor, as compared to previous attacks by Sasaki et al. (ACISP 2009).
85|4||Handling timing constraints violations in soft real-time applications as exceptions|In this paper, an exception-based programming paradigm is envisioned to deal with timing constraints violations occurring in soft real-time and multimedia applications written in the C language. In order to prove viability of the approach, a mechanism allowing to use such paradigm has been designed and implemented as an open-source library of C macros making use of the standard POSIX API (a few Linux-specific optimizations are also briefly discussed).
85|4||Modularity analysis of use case implementations|A component-based decomposition can result in implementations having use cases code tangled with other concerns and scattered across components. Modularity mechanisms such as aspects, mixins, and virtual classes have been proposed to address this kind of problem. One can use such mechanisms to group together code related to a single use case. This paper quantitatively analyzes the impact of this kind of use case modularization. We apply one specific technique, aspect oriented programming, to modularize the use case implementations of two information systems that conform to the layered architecture pattern. We extract traditional and contemporary metrics – including cohesion, coupling, and separation of concerns – to analyze modularity in terms of quality attributes such as changeability, support for independent development, and pluggability. Our findings indicate that the results of a given modularity analysis depend on other factors beyond the chosen system, metrics, and the applied modularity technique.
85|5|http://www.sciencedirect.com/science/journal/01641212/85/5|A Self-adaptive hierarchical monitoring mechanism for Clouds|While Cloud computing offers the potential to dramatically reduce the cost of software services through the commoditization of IT assets and on-demand usage patterns, one has to consider that Future Internet applications raise the need for environments that can facilitate real-time and interactivity and thus pose specific requirements to the underlying infrastructure. The latter, should be able to efficiently adapt resource provisioning to the dynamic Quality of Service (QoS) demands of such applications. To this direction, in this paper we present a monitoring system that facilitates on-the-fly self-configuration in terms of both the monitoring time intervals and the monitoring parameters. The proposed approach forms a multi-layered monitoring framework for measuring QoS at both application and infrastructure levels targeting trigger events for runtime adaptability of resource provisioning estimation and decision making. Besides, we demonstrate the operation of the implemented mechanism and evaluate its effectiveness using a real-world application scenario, namely Film Post Production.
85|5||The impact of accounting for special methods in the measurement of object-oriented class cohesion on refactoring and fault prediction activities|
85|5||DRMFS: A file system layer for transparent access semantics of DRM-protected contents|In many digital rights management (DRM) schemes, only a specialized application can decode DRM-protected contents. This restriction is harmful to users because they want to use their purchased digital contents with their preferred applications. To relax this restriction, DRM technology should provide transparent access semantics of DRM-protected contents to authorized applications. Some previous schemes achieve limited transparent access semantics but have efficiency and applicability problems. In this paper, we propose a DRM control scheme at the file system layer (DRMFS) that achieves transparent access semantics of DRM-protected contents with efficiency, applicability, and portability. Since DRMFS is working at the file system layer, any authorized application can access DRM-protected content in the same way as using general files. To implement a prototype of DRMFS, we use the Filesystem in Userspace (FUSE) library that is a well known library used to develop user level file systems. We explain details of the implementation and evaluate its performance. The evaluation results show that DRMFS has acceptable overheads.
85|5||Noisy data elimination using mutual k-nearest neighbor for classification mining|k nearest neighbor (kNN) is an effective and powerful lazy learning algorithm, notwithstanding its easy-to-implement. However, its performance heavily relies on the quality of training data. Due to many complex real-applications, noises coming from various possible sources are often prevalent in large scale databases. How to eliminate anomalies and improve the quality of data is still a challenge. To alleviate this problem, in this paper we propose a new anomaly removal and learning algorithm under the framework of kNN. The primary characteristic of our method is that the evidence of removing anomalies and predicting class labels of unseen instances is mutual nearest neighbors, rather than k nearest neighbors. The advantage is that pseudo nearest neighbors can be identified and will not be taken into account during the prediction process. Consequently, the final learning result is more creditable. An extensive comparative experimental analysis carried out on UCI datasets provided empirical evidence of the effectiveness of the proposed method for enhancing the performance of the k-NN rule.
85|5||UniSpaCh: A text-based data hiding method using Unicode space characters|This paper proposes a text-based data hiding method to insert external information into Microsoft Word document. First, the drawback of low embedding efficiency in the existing text-based data hiding methods is addressed, and a simple attack, DASH, is proposed to reveal the information inserted by the existing text-based data hiding methods. Then, a new data hiding method, UniSpaCh, is proposed to counter DASH. The characteristics of Unicode space characters with respect to embedding efficiency and DASH are analyzed, and the selected Unicode space characters are inserted into inter-sentence, inter-word, end-of-line and inter-paragraph spacings to encode external information while improving embedding efficiency and imperceptivity of the embedded information. UniSpaCh is also reversible where the embedded information can be removed to completely reconstruct the original Microsoft Word document. Experiments were carried out to verify the performance of UniSpaCh as well as comparing it to the existing space-manipulating data hiding methods. Results suggest that UniSpaCh offers higher embedding efficiency while exhibiting higher imperceptivity of white space manipulation when compared to the existing methods considered. In the best case scenario, UniSpaCh produces output document of size almost 9 times smaller than that of the existing method.
85|5||Efficient audit service outsourcing for data integrity in clouds|Cloud-based outsourced storage relieves the client's burden for storage management and maintenance by providing a comparably low-cost, scalable, location-independent platform. However, the fact that clients no longer have physical possession of data indicates that they are facing a potentially formidable risk for missing or corrupted data. To avoid the security risks, audit services are critical to ensure the integrity and availability of outsourced data and to achieve digital forensics and credibility on cloud computing. Provable data possession (PDP), which is a cryptographic technique for verifying the integrity of data without retrieving it at an untrusted server, can be used to realize audit services.
85|5||Coopetitive relationships in cross-functional software development teams: How to model and measure?|Understanding simultaneous cooperative and competitive (coopetitive) dynamics in cross-functional software development teams is fundamental to the success of software development process. The recent coopetition research is, however, hampered by a lack of conceptual focus, and the corresponding inconsistent treatment of the constructs associated with cross-functional coopetitive relationships. This study conceptualizes and operationalizes the multi-dimensional construct of cross-functional coopetition, and then presents an instrument for measuring this construct. Cross-functional coopetition is conceptualized with 5 distinct and independent constructs; 3 of them are related to cross-functional cooperation, and 2 are associated with cross-functional competition. The data collected from 115 software development project managers in Australia confirms the applicability of the constructs and their measures. This study contributes to the extant literature by providing a consensus on the conceptualization of cross-functional coopetitive behaviors, particularly in multi-party software development teams. The conceptual basis for cross-functional coopetition and its instrument will aid researchers and project managers interested in understanding coopetition in cross-functional collaborative contexts. Research and practical implications are discussed.
85|5||Sonata: Flexible connections between interaction and business spaces|Every interactive system features a functional core and a user interface. Over the years, several types of software architectures for connecting these conceptual elements have been proposed, all of which fail to conciliate two essential qualities: enabling both business and interaction objects reuse, and limiting the amount of communication-specific code in reusable objects.
85|5||Random grid-based visual secret sharing for general access structures with cheat-preventing ability|Conventional visual secret sharing (VSS) encodes a secret image into shares which are m times as big as the secrets. m is called pixel expansion. Random grid (RG) is an approach to solve pixel expansion problem. However, the existing VSS methods using RGs are confined to (2,n),(n,n) and (k,n). In this paper, RG-based VSS schemes for general access structures are proposed. The proposed algorithms can encode one secret image into n random grids while qualified sets can recover the secret visually. Furthermore, a cheating immune method is also presented to provided extra ability of cheat-preventing for RG-based VSS. Experimental results demonstrate that both the RG-based VSS for general access structures and cheating immune method are effective. More complicated sharing strategies can be implemented.
85|5||An adaptive model-free resource and power management approach for multi-tier cloud environments|With the development of cloud environments serving as a unified infrastructure, the resource management and energy consumption issues become more important in the operations of such systems. In this paper, we investigate adaptive model-free approaches for resource allocation and energy management under time-varying workloads and heterogeneous multi-tier applications. Specifically, we make use of measurable metrics, including throughput, rejection amount, queuing state, and so on, to design resource adjustment schemes and to make control decisions adaptively. The ultimate objective is to guarantee the summarized revenue of the resource provider while saving energy and operational costs. To validate the effectiveness, performance evaluation experiments are performed in a simulated environment, with realistic workloads considered. Results show that with the combination of long-term adaptation and short-term adaptation, the fluctuation of unpredictable workloads can be captured, and thus the total revenue can be preserved while balancing the power consumption as needed. Furthermore, the proposed approach can achieve better effect and efficiency than the model-based approaches in dealing with real-world workloads.
85|5||Efficient and robust probabilistic guarantees for real-time tasks|This paper presents a new method for providing probabilistic real-time guarantees to tasks scheduled through resource reservations. Previous work on probabilistic analysis of reservation-based schedulers is extended by improving the efficiency and robustness of the probability computation. Robustness is improved by accounting for a possibly incomplete knowledge of the distribution of the computation times (which is typical in realistic applications). The proposed approach computes a conservative bound for the probability of missing deadlines, based on the knowledge of the probability distributions of the execution times and of the inter-arrival times of the tasks. In this paper, such a bound is computed in realistic situations, comparing it with simulative results and with the exact computation of deadline miss probabilities (without pessimistic bounds). Finally, the impact of the incomplete knowledge of the execution times distribution is evaluated.
85|5||A graphical-based password keystroke dynamic authentication system for touch screen handheld mobile devices|Since touch screen handheld mobile devices have become widely used, people are able to access various data and information anywhere and anytime. Most user authentication methods for these mobile devices use PIN-based (Personal Identification Number) authentication, since they do not employ a standard QWERTY keyboard for conveniently entering text-based passwords. However, PINs provide a small password space size, which is vulnerable to attacks. Many studies have employed the KDA (Keystroke Dynamic-based Authentication) system, which is based on keystroke time features to enhance the security of PIN-based authentication. Unfortunately, unlike the text-based password KDA systems in QWERTY keyboards, different keypad sizes or layouts of mobile devices affect the PIN-based KDA system utility. This paper proposes a new graphical-based password KDA system for touch screen handheld mobile devices. The graphical password enlarges the password space size and promotes the KDA utility in touch screen handheld mobile devices. In addition, this paper explores a pressure feature, which is easy to use in touch screen handheld mobile devices, and applies it in the proposed system. The experiment results show: (1) EER is 12.2% in the graphical-based password KDA proposed system. Compared with related schemes in mobile devices, this effectively promotes KDA system utility; (2) EER is reduced to 6.9% when the pressure feature is used in the proposed system. The accuracy of authenticating keystroke time and pressure features is not affected by inconsistent keypads since the graphical passwords are entered via an identical size (50 mm × 60 mm) human–computer interface for satisfying the lowest touch screen size and a GUI of this size is displayed on all mobile devices.
85|5||Data embedding using pixel value differencing and diamond encoding with multiple-base notational system|We propose a new data hiding method that adaptively embeds data into pixel pairs using the diamond encoding (DE) technique. Because the human eyes tolerate more changes in edge and texture areas than in smooth areas, and pixel pairs in these areas often possess larger differences, the method exploits pixel value differences (PVD) to estimate the base of digits to be embedded into pixel pairs. Pixel pairs with larger differences are embedded with digits in larger base than those pixel pairs with smaller differences to maximize the payload and image quality. Two sophisticated pixel pair adjustment processes are provided to maintain the division consistency and to eliminate the overflow/underflow problem. Experimental results reveal that the proposed method offers better embedding performance compared to prior PVD-based works in terms of payload and image quality.
85|5||Improving test efficiency through system test prioritization|Software testing is an expensive process consuming at least 50% of the total development cost. Among the types of testing, system testing is the most expensive and complex. Companies are frequently faced with budgetary constraints, which may limit their ability to effectively complete testing efforts before delivering a software product. We build upon prior test case prioritization research and present a system-level approach to test case prioritization called Prioritization of Requirements for Test (PORT). PORT prioritizes system test cases based on four factors for each requirement: customer priority, implementation complexity, fault proneness, and requirements volatility. Test cases for requirements with higher priority based upon a weighted average of these factors are executed earlier in system test. An academic feasibility study and three post hoc industrial studies were conducted. Results indicate that PORT can be used to improve the rate of failure detection when compared with a random and operational profile-driven random approach. Furthermore, we investigated the contribution of the prioritization factors towards the improved rate of failure detection and found customer priority was the most significant contributor. Tool support is provided for the PORT scheme which allows for automatic collection of the four factor values and the resultant test case prioritization.
85|5||Thresholds for error probability measures of business process models|
85|5||A loss recovery approach for reliable application layer multicast|
85|5||A signature-based Grid index design for main-memory RFID database applications|A large-scale RFID application often requires a highly efficient database system in data processing. This research is motivated by the strong demand for an efficient index structure design for main-memory database systems of RFID applications. In this paper, a signature-based Grid index structure is proposed for efficient data queries and storage. An efficient methodology is proposed to locate duplicates and to execute batch deletions and range queries based on application domain knowhow. The capability of the design is implemented in an open source main-memory database system H2 and evaluated by realistic workloads of RFID applications.
85|6|http://www.sciencedirect.com/science/journal/01641212/85/6|A decade of agile methodologies: Towards explaining agile software development|Ever since the agile manifesto was created in 2001, the research community has devoted a great deal of attention to agile software development. This article examines publications and citations to illustrate how the research on agile has progressed in the 10 years following the articulation of the manifesto. Specifically, we delineate the conceptual structure underlying agile scholarship by performing an analysis of authors who have made notable contributions to the field. Further, we summarize prior research and introduce contributions in this special issue on agile software development. We conclude by discussing directions for future research and urging agile researchers to embrace a theory-based approach in their scholarship.
85|6||Coordination in co-located agile software development projects|Agile software development provides a way to organise the complex task of multi-participant software development while accommodating constant project change. Agile software development is well accepted in the practitioner community but there is little understanding of how such projects achieve effective coordination, which is known to be critical in successful software projects. A theoretical model of coordination in the agile software development context is presented based on empirical data from three cases of co-located agile software development. Many practices in these projects act as coordination mechanisms, which together form a coordination strategy. Coordination strategy in this context has three components: synchronisation, structure, and boundary spanning. Coordination effectiveness has two components: implicit and explicit. The theoretical model of coordination in agile software development projects proposes that an agile coordination strategy increases coordination effectiveness. This model has application for practitioners who want to select appropriate practices from agile methods to ensure they achieve coordination coverage in their project. For the field of information systems development, this theory contributes to knowledge of coordination and coordination effectiveness in the context of agile software development.
85|6||Obstacles to decision making in Agile software development teams|The obstacles facing decision making in Agile development are critical yet poorly understood. This research examines decisions made across four stages of the iteration cycle: Iteration Planning, Iteration Execution, Iteration Review and Iteration Retrospective. A mixed method approach was employed, whereby a focus group was initially conducted with 43 Agile developers and managers to determine decisions made at different points of the iteration cycle. Subsequently, six illustrative mini cases were purposefully conducted as examples of the six obstacles identified in these focus groups. This included interviews with 18 individuals in Agile projects from five different organizations: a global consulting organization, a multinational communications company, two multinational software development companies, and a large museum organization. This research contributes to Agile software development literature by analyzing decisions made during the iteration cycle and identifying six key obstacles to these decisions. Results indicate the six decision obstacles are unwillingness to commit to decisions; conflicting priorities; unstable resource availability; and lack of: implementation; ownership; empowerment. These six decision obstacles are mapped to descriptive decision making principles to demonstrate where the obstacles affect the decision process. The effects of these obstacles include a lack of longer-term, strategic focus for decisions, an ever-growing backlog of delayed work from previous iterations, and a lack of team engagement.
85|6||Understanding post-adoptive agile usage: An exploratory cross-case analysis|While past research has contributed to the understanding of how organizations adopt agile methodologies (AM), little is known about their post-adoptive usage in organizations. By integrating theories from systems development methodologies, diffusion of innovations, and agile methodology literature, this paper proposes a new model that identifies a set of critical factors pertinent to post-adoptive usage of agile practices. This model is used to inform analysis of post-adoptive usage of agile practices in two major organizations. The results indicate relative advantage, team attitude and technical competence, championing, and top management support (TMS) are the key factors determining the extent to which agile practices can be assimilated into an organization. Specifically, both findings and this model confirm that the deeper the assimilation of agile practices into the organization, the better understanding of how assimilation leads to specific improvements in its systems development outcomes.
85|6||Reconciling perspectives: A grounded theory of how people manage the process of software development|Social factors are significant cost drivers for the process of software development. In this field study we generate a grounded theory of how people manage the process of software development. The main concern of engineers involved in the process of software development is getting the job done. To get the job done, people engage in a four-stage process of Reconciling Perspectives. Reconciling Perspectives represents an attempt to converge individuals’ points of view or perspectives about a software project. The process emphasizes the importance of individuals’ abilities to both reach out and engage in negotiations and create shelter from environmental noise to bring a software project to fruition.
85|6||âLeagileâ software development: An experience report analysis of the application of lean approaches in agile software development|In recent years there has been a noticeable shift in attention from those who use agile software development toward lean software development, often labelled as a shift “from agile to lean”. However, the reality may not be as simple or linear as this label implies. To provide a better understanding of lean software development approaches and how they are applied in agile software development, we have examined 30 experience reports published in past agile software conferences in which experiences of applying lean approaches in agile software development were reported. The analysis identified six types of lean application. The results of our study show that lean can be applied in agile processes in different manners for different purposes. Lean concepts, principles and practices are most often used for continuous agile process improvement, with the most recent introduction being the kanban approach, introducing a continuous, flow-based substitute to time-boxed agile processes.
85|6||Automatic test case selection for regression testing of composite service based on extensible BPEL flow graph|Services are highly reusable, flexible and loosely coupled components whose changes make the evolution and maintenance of composite services more complex. The changes of composite service mainly cover three types, i.e., the processes, bindings, and interfaces. In this article, an approach is proposed to select test cases for regression testing of different versions of BPEL (business process execution language) composite service where these changes are involved. The approach identifies the changes by performing control flow analysis and comparing the paths in a new version of composite service with those in the old one using a kind of eXtensible BPEL flow graph (XBFG). Message sequence is appended to XBFG path so that XBFG can fully describe the behavior of composite service. The binding and predicate constraint information added in different XBFG elements can be used for path selection and even for test case generation. Both theoretic analysis and case study show that the proposed approach is effective.
85|6||Efficient (n, t, n) secret sharing schemes|Recently, Harn and Lin introduced a notion of strong t-consistency of a (t, n) secret sharing scheme and proposed a strong (n, t, n) verifiable secret sharing (VSS). In this paper, we propose a strong (n, t, n) VSS which is more efficient than Harn and Lin's VSS. Using the same approach, we propose a (n, t, n) multi-secret sharing scheme (MSS) to allow shareholders to share n − t + 1 secrets. Also, the proposed (n, t, n) MSS can be modified to include the verifiable feature. All proposed schemes are unconditionally secure and are based on Shamir's (t, n) secret sharing scheme.
85|6||An improved swarm optimized functional link artificial neural network (ISO-FLANN) for classification|Multilayer perceptron (MLP) (trained with back propagation learning algorithm) takes large computational time. The complexity of the network increases as the number of layers and number of nodes in layers increases. Further, it is also very difficult to decide the number of nodes in a layer and the number of layers in the network required for solving a problem a priori. In this paper an improved particle swarm optimization (IPSO) is used to train the functional link artificial neural network (FLANN) for classification and we name it ISO-FLANN. In contrast to MLP, FLANN has less architectural complexity, easier to train, and more insight may be gained in the classification problem. Further, we rely on global classification capabilities of IPSO to explore the entire weight space, which is plagued by a host of local optima. Using the functionally expanded features; FLANN overcomes the non-linear nature of problems. We believe that the combined efforts of FLANN and IPSO (IPSO + FLANN = ISO − FLANN) by harnessing their best attributes can give rise to a robust classifier. An extensive simulation study is presented to show the effectiveness of proposed classifier. Results are compared with MLP, support vector machine(SVM) with radial basis function (RBF) kernel, FLANN with gradiend descent learning and fuzzy swarm net (FSN).
85|6||Mining frequent patterns from dynamic data streams with data load management|In this paper, we study the practical problem of frequent-itemset discovery in data-stream environments which may suffer from data overload. The main issues include frequent-pattern mining and data-overload handling. Therefore, a mining algorithm together with two dedicated overload-handling mechanisms is proposed. The algorithm extracts basic information from streaming data and keeps the information in its data structure. The mining task is accomplished when requested by calculating the approximate counts of itemsets and then returning the frequent ones. When there exists data overload, one of the two mechanisms is executed to settle the overload by either improving system throughput or shedding data load. From the experimental data, we find that our mining algorithm is efficient and possesses good accuracy. More importantly, it could effectively manage data overload with the overload-handling mechanisms. Our research results may lead to a feasible solution for frequent-pattern mining in dynamic data streams.
85|6||Sharetouch: A system to enrich social network experiences for the elderly|The Sharetouch system is designed for raising users’ participation in community events. We put three subsystems into Sharetouch: (1) community pond, (2) Waterball interactive game, and (3) multimedia sharing. Sharetouch is based on an optical touch device designed by the Joyplux Company with an infrared LED and camera. This device can support multi-touch functions within a large display area. The software of Sharetouch was developed within XNA and .NET frameworks. We project the users as fish in our community pond. Sharetouch displays all the friends as fish when the users log into the system. Therefore, the number of fish equals the number of friends of the users. This design encourages users to make more friends to increase the number of fish. Waterball is a game that combines virtual images and real objects. The concept is based on the Nintendo Wii games, as players hold controllers (real objects) to play the games (virtual images). We also apply the concept of the cloud flash drive to multimedia sharing to avoid the trouble of carrying a real flash disk. This study employed the TAM measure to measure the validity of Sharetouch in this social platform. Our findings indicated that all proposed hypotheses had a positive and significant impact on the intention of older people to interact with Sharetouch. Unlike the computer-based system, Sharetouch is created as a user-friendly interface system. Sharetouch can enrich the users’ social network experiences through its hardware and software architectures.
85|6||A family of case studies on business process mining using MARBLE|Business processes, most of which are automated by information systems, have become a key asset in organizations. Unfortunately, uncontrolled maintenance implies that information systems age overtime until they need to be modernized. During software modernization, ageing systems cannot be entirely discarded because they gradually embed meaningful business knowledge, which is not present in any other artifact. This paper presents a technique for recovering business processes from legacy systems in order to preserve that knowledge. The technique statically analyzes source code and generates a code model, which is later transformed by pattern matching into a business process model. This technique has been validated over a two-year period in several industrial modernization projects. This paper reports the results of a family of case studies that were performed to empirically validate the technique using analysis and meta-analysis techniques. The family of case studies demonstrates that the technique is feasible in terms of effectiveness and efficiency.
85|6||Quasi-static fault-tolerant scheduling schemes for energy-efficient hard real-time systems|This paper investigates fault tolerance and dynamic voltage scaling (DVS) in hard real-time systems. The authors present quasi-static task scheduling algorithms that consist of offline components and online components. The offline components are designed the way they enable the online components to achieve energy savings by using the dynamic slack due to variations in task execution times and uncertainties in fault occurrences. The proposed schemes utilize a fault model that considers the effects of voltage scaling on transient fault rate. Simulation results based on real-life task sets and processor data sheets show that the proposed scheduling schemes achieve energy savings of up to 50% over the state-of-art low-energy offline scheduling techniques and incur negligible runtime overheads. A hard real-time real-life test bed has been developed allowing the validation of the proposed algorithms.
85|6||Comparison of scheduling schemes for on-demand IaaS requests|Infrastructure-as-a-service (IaaS) is one of emerging powerful cloud computing services provided by IT industry at present. This paper considers the interaction aspects between on-demand requests and the allocation of virtual machines in a server farm operated by a specific infrastructure owner. We formulate an analytic performance model of the server farm taking into account the quality of service (QoS) guaranteed to users and the operational energy consumption in the server farm. We compare several scheduling algorithms from the aspect of the average energy consumption and heat emission of servers as well as the blocking probabilities of on-demand requests. Based on numerical results of a comparison of different allocation strategies, a saving on the energy consumption is possible in the operational range (where on-demand requests do not face unpleasant blocking probability) with the allocation of virtual machines to physical servers based on the priority.
85|6||Strongly secure certificateless short signatures|Short certificateless signatures have come into limelight in recent years. On the one hand, the property of certificateless eliminates the certificate management problem in traditional PKI and the key-escrow problem in some ID-based signature schemes. On the other hand, due to the short signature length, short certificateless signatures can be applied to systems where signatures are typed in by human or systems with low-bandwidth channels and/or low-computation power, such as PDAs or cell phones. However, there has been a trade-off between short certificateless signature schemes and their security levels. All existing short certificateless signature schemes can only be proven secure against a normal type adversary rather than a stronger one, who can obtain valid certificateless signatures under public keys replaced by the adversary. In this paper, we solve this open problem by given an efficient strongly secure short certificateless signature scheme. The proposed scheme has the following features. Firstly, it is strongly unforgeable. Secondly, the security can be reduced to the Computational Diffie–Hellman (CDH) assumption – a classic complexity assumption. Lastly, the proposed scheme is provably secure against adversaries with access to a super signing oracle which generates valid certificateless signatures of messages and public keys chosen by the adversary (without providing the corresponding secret values).
85|6||A symbolic analysis framework for static analysis of imperative programming languages|We present a generic symbolic analysis framework for imperative programming languages. Our framework is capable of computing all valid variable bindings of a program at given program points. This information is invaluable for domain-specific static program analyses such as memory leak detection, program parallelization, and the detection of superfluous bound checks, variable aliases and task deadlocks.
85|6||Goal alignment in process improvement|Process improvement should improve an organisation's ability to achieve its business goals. While mapping an organisation's strategic goals through various layers of management is common, such mapping does not seem to continue through to their processes that create value to the organisation. Despite a number of process improvement methods being available, and almost two decades of experience with those methods, many process improvement projects do not end successfully.
85|7|http://www.sciencedirect.com/science/journal/01641212/85/7|Software ecosystems: Taking software development beyond the boundaries of the organization|
85|7||A longitudinal case study of an emerging software ecosystem: Implications for practice and theory|
85|7||From proprietary to open sourceâGrowing an open source ecosystem|In today's business and software arena, Free/Libre/Open Source Software has emerged as a promising platform for software ecosystems. Following this trend, more and more companies are releasing their proprietary software as open source, forming a software ecosystem of related development projects complemented with a social ecosystem of community members. Since the trend is relatively recent, there are few guidelines on how to create and maintain a sustainable open source ecosystem for a proprietary software. This paper studies the problem of building open source communities for industrial software that was originally developed as closed source. Supporting processes, guidelines and best practices are discussed and illustrated through an industrial case study. The research is paving the road for new directions in growing a thriving open source ecosystem.
85|7||Understanding the role of licenses and evolution in open architecture software ecosystems|The role of software ecosystems in the development and evolution of open architecture systems whose components are subject to different licenses has received insufficient consideration. Such systems are composed of components potentially under two or more licenses, open source or proprietary or both, in an architecture in which evolution can occur by evolving existing components, replacing them, or refactoring. The software licenses of the components both facilitate and constrain the system's ecosystem and its evolution, and the licenses’ rights and obligations are crucial in producing an acceptable system. Consequently, software component licenses and the architectural composition of a system help to better define the software ecosystem niche in which a given system lies. Understanding and describing software ecosystem niches for open architecture systems is a key contribution of this work. An example open architecture software system that articulates different niches is employed to this end. We examine how the architecture and software component licenses of a composed system at design time, build time, and run time help determine the system's software ecosystem niche and provide insight and guidance for identifying and selecting potential evolutionary paths of system, architecture, and niches.
85|7||Shades of gray: Opening up a software producing organization with the open software enterprise model|Software producing organizations are frequently judged by others for being ‘open’ or ‘closed’, where a more ‘closed’ organization is seen as being detrimental to its software ecosystem. These qualifications can harm the reputation of these companies, for they are deemed to promote vendor lock-in, use closed data formats, and are seen as using intellectual property laws to harm others. These judgements, however, are frequently based on speculation and the need arises for a method to establish openness of an organization, such that decisions are no longer based on prejudices, but on an objective assessment of the practices of a software producing organization. In this article the open software enterprise model is presented that enables one to establish the degree of openness of a software producing organization. The model has been evaluated in five interviews, is illustrated using three case studies, and shows that organizational openness and transparency are complex variables, that should not be determined based on belief or prejudice. Furthermore, the model can be used by software producing organizations as a reference for further opening up their business, to stimulate the surrounding software ecosystem, and further their business goals.
85|7||Scaling up software architecture analysis|This paper will show how architecture design and analysis techniques rest on a small number of foundational principles. We will show how those principles have been instantiated as a core set of techniques. These techniques, combined together, have resulted in several highly successful architecture analysis and design methods. Finally, we will show how these foundations, and the techniques that instantiate them, can be re-combined for new purposes addressing problems of ever-increasing scale, specifically: addressing the highly complex problems of analyzing software-intensive ecosystems.
85|7||Methodological construction of product-form stochastic Petri nets for performance evaluation|Product-forms in Stochastic Petri nets (SPNs) are obtained by a compositional technique for the first time, by combining small SPNs with product-forms in a hierarchical manner. In this way, performance engineering methodology is enhanced by the greatly improved efficiency endowed to the steady-state solution of a much wider range of Markov models. Previous methods have relied on analysis of the whole net and so are not incremental—hence they are intractable in all but small models. We show that the product-form condition for open nets depends, in general, on the transition rates, whereas closed nets have only structural conditions for a product-form, except in rather pathological cases. Both the “building blocks” formed by the said small SPNs and their compositions are solved for their product-forms using the Reversed Compound Agent Theorem (RCAT), which, to date, has been used exclusively in the context of process-algebraic models. The resulting methodology provides a powerful, general and rigorous route to product-forms in large stochastic models and is illustrated by several detailed examples.
85|7||Distributed goal-oriented computing|For current computing frameworks, the ability to dynamically use the resources that are allocated in the network has become a key success factor. As long as the size of the network increases, it is more difficult to find how to solve the problems that the users are presenting. Users usually do know what they want to do, but they do not know how to do it. If the user knows its goals it could be easier to help him with a different approach. In this work we present a new computing paradigm based on goals. This paradigm is called Distributed goal-oriented computing paradigm. To implement this paradigm an execution framework for a goal-oriented operating system has been designed. In this paradigm users express their goals and the OS is in charge of helping the achievement of these goals by means of a service-oriented approach.
85|7||Profiling all paths: A new profiling technique for both cyclic and acyclic paths|As an important technique in dynamic program analysis, path profiling collects the execution frequency of different paths, and has been widely used in a variety of areas. However, existing intra-procedural profiling techniques cannot effectively deal with loops, i.e., they are limited in either working with acyclic paths, or with a small number of loop iteration. This paper presents a new profiling technique called PAP (Profiling All Paths), which can profile all finite-length paths within a procedure. PAP consists of two basic phases, the probe instrumentation phase which assigns a unique pathid to each path, and the backwalk phase which uses the pathids to determine the corresponding executed paths. Furthermore, breakpoints are introduced to store the probe value which may overflow during long executions, and the number of probes is reduced based on the integration of PAP with an existing profiling technique. From our case study and experiments, PAP is found to be effective and efficient in profiling both cyclic and acyclic paths.
85|7||Dynamic refinement of search engines results utilizing the user intervention|Nowadays, modern search engines quite satisfactorily answer users’ queries, but the top results returned are not always relevant to the data the user is actually looking for. Hence, considerable efforts are made by search engines in order to rank the most relevant to the query results at the top. This work addresses the above problem and improves the performance of a search engine, especially when it comes to queries which have for example twofold meanings. The matter which the user is interested in is identified based on the results that he/she chooses, and then the most relevant ones are ranked higher. In addition, the results are recognized not only as text but also as semantic entities, which contain various semantic features. The semantic relation between results and text coverage are used as the main tool to achieve an optimized ranking, as opposed to other research papers so far. As a result, a new meta search application is developed, which, given a set of terms, combines Google results and then reorganizes (re-ranks) them based on the disambiguation offered by user clicks. In particular, after a ranking is achieved, the user makes a choice (click), the ranking is updated and the process is repeated. In order to prove our claims, apart from the description of the algorithm for refining the ranking of results, a web application has been developed, which was used to test the effectiveness of the system proposed.
85|7||Job allocation strategies for energy-aware and efficient Grid infrastructures|Complex distributed architectures, like Grid, supply effective platforms to solve computations on huge datasets, often at the cost of increased power consumption. This energy issue affects the sustainability of the infrastructures and increases their environmental impact. On the other hand, due to Grid heterogeneity and scalability, possible power savings could be achieved if effective energy-aware allocation policies were adopted. These policies are meant to implement a better coupling between application requirements and the Grid resources, also taking energy parameters into account. In this paper, we discuss different allocation strategies which address jobs submitted to Grid resources, subject to efficiency and energy constraints. Our aim is to analyze the potential benefits that can be obtained from the adoption of a metric able to capture both performance and energy-savings. Based on an experimental study, we simulated two alternative scenarios aimed at comparing the behavior of different strategies for allocating jobs to resources. Moreover we introduced the Performance/Energy Trade-off function as a useful means to evaluate the tendency of an allocation strategy toward efficiency or power consumption. Our conclusion seems to suggest that performance and energy-savings are not always enemies, and these objectives may be combined if suitable energy metrics are adopted.
85|7||Balancing software engineering education and industrial needs|In the world of information and communications technologies the demand for professionals with software engineering skills grows at an exponential rate. On this ground, we have conducted a study to help both academia and the software industry form a picture of the relationship between the competences of recent graduates of undergraduate and graduate software engineering programmes and the tasks that these professionals are to perform as part of their jobs in industry. Thanks to this study, academia will be able to observe which skills demanded by industry the software engineering curricula do or do not cater for, and industry will be able to ascertain which tasks a recent software engineering programme graduate is well qualified to perform. The study focuses on the software engineering knowledge guidelines provided in SE2004 and GSwE2009, and the job profiles identified by Career Space.
85|7||Blackboard architecture to integrate components and agents in heterogeneous distributed eLearning systems: An application for learning to program|To build complete and complex eLearning systems, eLearning engineers are used to applying standards that facilitate sharing information as well as distributed service-oriented architectures that provide reuse and interoperability by means of component integration. These concepts lead us to a Component-based Development Process that will allow us to implement tools that give full support to the teaching/learning process, taking advantage of the synergy effect created by the integration of the different components. Thus, throughout this article we analyse the proposals from the most relevant consortia concerned with eLearning standards, showing their service oriented approaches and the middleware technologies which can be used to implement them. This analysis will demonstrate that the use of middleware technologies that use the definition of services’ interface can limit the reuse and interoperability requisites desired by the main standards consortia. Then, we will show a proposal which tries to solve this shortfall, using a blackboard-based architecture for integrating and communicating heterogeneous distributed components, as well as a user environment that also allows us to perform component integration. As an example, we will demonstrate how we have built an application for learning to program by applying our approach and following a Component-based Development Process to implement different components (services, agents, clients, etc.) that integrate it. Hence, we will argue that using blackboard architecture and a Component-based Development Process helps us to solve the identified shortcomings.
85|7||Octopus: An Upperware based system for building personal pervasive environments|As of today, there is no operating system suitable for pervasive computing. Such system must integrate and coordinate heterogeneous devices and systems but, at the same time, it should provide a single system image to let the user feel that there is only a single “pervasive” computing environment. Such illusion must consider the Internet as the system backbone, because users move. The challenge is providing a novel system while permitting the seamless integration of traditional legacy systems, which may be required to run on many computers and devices, if only to run their applications. We argue that to build such a system, we should abandon Middleware and use a different technology, that we call Upperware. To back up our claim, we have built an actual system using Upperware: the Octopus. The Octopus has been in use for several years both to build pervasive applications like smart spaces and to provide a general-purpose computing environment. We have been using it through wide area networks, on a daily basis. In this paper we discuss the Upperware approach and present the Octopus as an actual system built out of Upperware, including some evaluation results.
85|7||Malware characteristics and threats on the internet ecosystem|Malware encyclopedias now play a vital role in disseminating information about security threats. Coupled with categorization and generalization capabilities, such encyclopedias might help better defend against both isolated and clustered specimens.In this paper, we present Malware Evaluator, a classification framework that treats malware categorization as a supervised learning task, builds learning models with both support vector machines and decision trees and finally, visualizes classifications with self-organizing maps. Malware Evaluator refrains from using readily available taxonomic features to produce species classifications. Instead, we generate attributes of malware strains via a tokenization process and select the attributes used according to their projected information gain. We also deploy word stemming and stopword removal techniques to reduce dimensions of the feature space. In contrast to existing approaches, Malware Evaluator defines its taxonomic features based on the behavior of species throughout their life-cycle, allowing it to discover properties that previously might have gone unobserved. The learning and generalization capabilities of the framework also help detect and categorize zero-day attacks. Our prototype helps establish that malicious strains improve their penetration rate through multiple propagation channels as well as compact code footprints; moreover, they attempt to evade detection by resorting to code polymorphism and information encryption. Malware Evaluator also reveals that breeds in the categories of Trojan, Infector, Backdoor, and Worm significantly contribute to the malware population and impose critical risks on the Internet ecosystem.
85|7||Loop fusion and reordering for register file optimization on stream processors|
85|7||Coding-error based defects in enterprise resource planning software: Prevention, discovery, elimination and mitigation|Software defects due to coding errors continue to plague the industry with disastrous impact, especially in the enterprise application software category. Identifying how much of these defects are specifically due to coding errors is a challenging problem. In this paper, we investigate the best methods for preventing new coding defects in enterprise resource planning (ERP) software, and discovering and fixing existing coding defects. A large-scale survey-based ex-post-facto study coupled with experiments involving static code analysis tools on both sample code and real-life million lines of code open-source ERP software were conducted for such purpose. The survey-based methodology consisted of respondents who had experience developing ERP software. This research sought to determine if software defects could be merely mitigated or totally eliminated, and what supporting policies, procedures and infrastructure were needed to remedy the problem. In this paper, we introduce a hypothetical framework developed to address our research questions, the hypotheses we have conjectured, the research methodology we have used, and the data analysis methods used to validate the stated hypotheses. Our study revealed that: (a) the best way for ERP developers to discover coding-error based defects in existing programs is to choose an appropriate programming language; perform a combination of manual and automated code auditing, static code analysis, and formal test case design, execution and analysis, (b) the most effective ways to mitigate defects in an ERP system is to track the defect densities in the ERP software, fix the defects found, perform regression testing, and update the resulting defect density statistics, and (c) the impact of epistemological and legal commitments on the defect densities of ERP systems is inconclusive.
85|8|http://www.sciencedirect.com/science/journal/01641212/85/8|Improving VRSS-based vulnerability prioritization using analytic hierarchy process|The number of vulnerabilities discovered in computer systems has increased explosively. Thus, a key question for system administrators is which vulnerabilities to prioritize. The need for vulnerability prioritization in organizations is widely recognized. The significant role of the vulnerability evaluation system is to separate vulnerabilities from each other as far as possible. There are two major methods to assess the severity of vulnerabilities: qualitative and quantitative methods. In this paper, we first describe the design space of vulnerability evaluation methodology and discuss the measures of well-defined evaluation framework. We analyze 11,395 CVE vulnerabilities to expose the differences among three current vulnerability evaluation systems (X-Force, CVSS and VRSS). We find that vulnerabilities are not separated from each other as much as possible. In order to increase the diversity of the results, we firstly enable vulnerability type to prioritize vulnerabilities using analytic hierarchy process on the basis of VRSS. We quantitatively characterize the vulnerability type and apply the method on the set of 11,395 CVE vulnerabilities. The results show that the quality of the quantitative scores can be improved with the help of vulnerability type.
85|8||Complex event processing with T-REX|Several application domains involve detecting complex situations and reacting to them. This asks for a Complex Event Processing (CEP) middleware specifically designed to timely process large amounts of event notifications as they flow from the peripheral to the center of the system, to identify the composite events relevant for the application. To answer this need we designed T-Rex, a new CEP middleware that combines expressiveness and efficiency. On the one hand, it adopts a language (TESLA) explicitly conceived to easily and naturally describe composite events. On the other hand, it provides an efficient event detection algorithm based on automata to interpret TESLA rules. Our evaluation shows that the T-Rex engine can process a large number of complex rules with a reduced overhead, even in the presence of challenging workloads.
85|8||Adaptive co-scheduling for periodic application and update transactions in real-time database systems|In this paper, we study the co-scheduling problem of periodic application transactions and update transactions in real-time database systems for surveillance of critical events. To perform the surveillance functions effectively, it is important to meet the deadlines of the application transactions and maintain the quality of the real-time data objects for their executions. Unfortunately, these two goals are conflicting and difficult to be achieved at the same time. To address the co-scheduling problem, we propose a real-time co-scheduling algorithm, called Adaptive Earliest Deadline First Co-Scheduling (AEDF-Co). In AEDF-Co, a dynamic scheduling approach is adopted to adaptively schedule the update and application jobs based on their deadlines. The performance goal of AEDF-Co is to determine a schedule for given sets of periodic application and update transactions such that the deadline constraints of all the application transactions are satisfied and at the same time the quality of data (QoD) of the real-time data objects is maximized. Extensive simulation experiments have been performed to evaluate the performance of AEDF-Co. The results show that by adaptively adjusting the release times of update jobs and scheduling the update and application jobs dynamically based on their urgencies, AEDF-Co is effective in achieving the performance goals and maximizing the overall system performance.
85|8||On âExploring alternatives for transition verificationâ|
85|8||Utilizing Layered Taxation to provide incentives in P2P streaming systems|Incentive mechanism plays an essential role in guaranteeing the performance of P2P streaming systems. This paper proposed a Credit-line based Layered Taxation (CLT) incentive mechanism, which is fully distributed and does not rely on any central server for credit management. The CLT mechanism also avoids the problem of reputation accumulation in reputation-based systems and trade limitation in the reciprocity-based systems. In addition, the formed layered topology and taxation strategy can inspire peers to contribute their bandwidth, and maximize both individual utility and system utility. Our simulation results indicate that, with our proposed incentive mechanism, cooperative peers can gain much better performance, while free riders would be cleared out of the system.
85|8||SEProf: A high-level software energy profiling tool for an embedded processor enabling power management functions|Energy efficiency has become one of the most important design issues for embedded systems. To examine the power consumption of an embedded system, an energy profiling tool is highly demanded. Although a number of energy profiling tools have been proposed, they are not directly applicable to the embedded processors with power management functions that are widely utilized in battery-operated embedded systems to reduce power consumption. Hence, this study presents a high-level energy profiling tool, called SEProf, that estimates the energy consumption of an embedded system running multithread software and a multitasking operating system (OS) that supports power management functions. This study implements the proposed SEProf in Linux 2.6.19 and evaluates its performance on an ARM11 MPCore processor. Experimental results demonstrate that the proposed tool can provide accurate energy profiling results with a low profiling overhead.
85|8||Investigating intentional distortions in software cost estimation â An exploratory study|Cost estimation of software projects is an important activity that continues to be a source of problems for practitioners despite improvement efforts. Most of the research on estimation has focused on methodological issues while the research focused on human factors primarily has targeted cognitive biases or perceived inhibitors. This paper focuses on the complex organizational context of estimation and investigates whether estimates may be distorted, i.e. intentionally changed for reasons beyond legitimate changes due to changing prerequisites such as requirements or scope. An exploratory study was conducted with 15 interviewees at six large companies that develop software-intensive products. The interviewees represent five stakeholder roles in estimation, with a majority being project or line managers. Document analysis was used to complement the interviews and provided additional context. The results show that both estimate increase and estimate decrease exist and that some of these changes can be explained as intentional distortions. The direction of the distortion depends on the context and the stakeholders involved. The paper underlines that it is critical to consider also human and organizational factors when addressing estimation problems and that intentional estimate distortions should be given more and direct attention.
85|8||Ordering features by category|Precedence, whereby features are serialized and execute sequentially in response to an event, is a common method for coordinating features that would otherwise interact. However, the effectiveness of precedence lies in the system designer's ability to order features such that their sequential execution results in desired system behaviour. The task of evaluating feature orderings is expensive: a set of n features means that there are n! feature orderings to consider.
85|8||Context-oriented programming: A software engineering perspective|The implementation of context-aware systems can be supported through the adoption of techniques at the architectural level such as middlewares or component-oriented architectures. It can also be supported by suitable constructs at the programming language level. Context-oriented programming (COP) is emerging as a novel paradigm for the implementation of this kind of software, in particular in the field of mobile and ubiquitous computing. The COP paradigm tackles the issue of developing context-aware systems at the language-level, introducing ad hoc language abstractions to manage adaptations modularization and their dynamic activation. In this paper we review the state of the art in the field of COP in the perspective of the benefits that this technique can provide to software engineers in the design and implementation of context-aware applications.
85|8||Generalized aggregate Quality of Service computation for composite services|This article addresses the problem of estimating the Quality of Service (QoS) of a composite service given the QoS of the services participating in the composition. Previous solutions to this problem impose restrictions on the topology of the orchestration models, limiting their applicability to well-structured orchestration models for example. This article lifts these restrictions by proposing a method for aggregate QoS computation that deals with more general types of unstructured orchestration models. The applicability and scalability of the proposed method are validated using a collection of models from industrial practice.
85|8||An encoding scheme based on fractional number for querying and updating XML data|In order to facilitate the XML query processing, several labeling schemes have been proposed to directly determine the structural relationships between two arbitrary XML nodes without accessing the original XML documents. However, the existing XML labeling schemes have to re-label the pre-existing nodes or re-calculate the label values when a new node is inserted into the XML document during an update process. In this paper, we devise a novel encoding scheme based on the fractional number to encode the labels of the XML nodes. Moreover, we propose a mapping method to convert our proposed fractional number based encoding scheme to bit string based encoding scheme with the intention to minimize the label size and save the storage space. By applying our proposed bit string encoding scheme to the range-based labeling scheme and the prefix labeling scheme, the process of re-labeling the pre-existing nodes can be avoided when nodes are inserted as leaf nodes and sibling nodes without affecting the order of XML nodes. In addition, we propose an algorithm to control the increment of label size when new nodes are inserted frequently at a fix place of an XML tree. Experimental results show that our proposed bit string encoding scheme provides efficient support to the process of XML updating without sacrificing the query performance when it is applied to the range-based labeling schemes.
85|8||A user-friendly secret image sharing scheme with reversible steganography based on cellular automata|Secret image sharing is a mechanism to protect a secret image among a group of participants by encrypting the secret into shares and decrypting the secret with sufficient shares. Conventional schemes generate meaningless shares, which are hard to identify and lead to suspicion of secret image encryption. To overcome these problems, sharing schemes with steganography were presented. The meaningless shared data were embedded into the cover image to form stego images. However, distorted stego images cannot be reverted to original. In this work, a novel secret image sharing scheme with reversible steganography is proposed. Main contribution of this work is that two-dimensional reversible cellular automata with memory is utilized to encrypt a secret image into shared data, which are then embedded into cover image for forming stego images. By collecting sufficient stego images, not only the secret image is lossless reconstructed, but also distorted stego image is reverted to original. Simulation results shows that low computation cost and pleasing stego image quality are also achieved by the proposed scheme.
85|8||Performance evaluation of moment-based watermarking methods: A review|This paper includes a theoretical analysis and performance investigation of representative moment-based watermarking systems. The main contribution of this work, along with the review of the moment-based watermarking algorithms’ main properties (advantages and disadvantages), is the sensitivity analysis of those methods regarding some crucial parameters. Through a designed set of specific experiments, the influence of moment order and moment family (Zernike, Pseudo-Zernike, Wavelet, Krawtchouk, Tchebichef, Legendre, Fourier–Mellin) on each methods’ performance is investigated and evaluated by applying geometric and signal processing attacks through the well-known benchmark Stirmark. Moreover, a comparative study regarding to methods’ robustness, imperceptibility and algorithms’ efficiency (time performance) is achieved. Experimental results show that moment's order calibration along with specific moment families enhances methods’ performances and brings forth better tradeoffs between robustness and imperceptibility. Additionally, important issues which have been revealed through methods’ implementation are discussed and proper separate solutions are proposed.
85|8||Propagating changes between aligned process models|There is a wide variety of drivers for business process modelling initiatives, reaching from organisational redesign to the development of information systems. Consequently, a common business process is often captured in multiple models that overlap in content due to serving different purposes. Business process management aims at flexible adaptation to changing business needs. Hence, changes of business processes occur frequently and have to be incorporated in the respective process models. Once a process model is changed, related process models have to be updated accordingly, despite the fact that those process models may only be loosely coupled. In this article, we introduce an approach that supports change propagation between related process models. Given a change in one process model, we leverage the behavioural abstraction of behavioural profiles for corresponding activities in order to determine a change region in another model. Our approach is able to cope with changes in pairs of models that are not related by hierarchical refinement and show behavioural inconsistencies. We evaluate the applicability of our approach with two real-world process model collections. To this end, we either deduce change operations from different model revisions or rely on synthetic change operations.
85|8||Mitigating starvation of Linux CPU-bound processes in the presence of network I/O|In prior research work, it has been demonstrated that Linux can starve CPU-bound processes in the presence of network I/O. The starvation of Linux CPU-bound processes occurs under the two Linux schedulers, namely the 2.6 O(1) scheduler and the more recent 2.6 Completely Fair Scheduler (CFS). In this paper, we analyze the underlying root causes of this starvation problem and we propose effective solutions that can mitigate such starvation. We present detailed implementations of our proposed solutions for both O(1) and CFS Linux schedulers. We empirically evaluate the effectiveness of our proposed solutions in terms of execution time and incoming traffic load. For our experimental study and analysis, we consider two types of mainboard architectures: Uni-Processing (UP) and Symmetric Multi-Processing (SMP). Our empirical results show that the proposed solutions are highly effective in mitigating the starvation problem for CPU-bound processes with no negative impact on the performance of network I/O-bound processes.
85|8||Validated templates for specification of complex LTL formulas|
85|8||Analysis and application of an outsourcing risk framework|There is much reported research on risk, and risk management but research on strategic IT system development outsourcing risk, from the client perspective, remains unclear. A literature-based conceptual risk framework for strategic IT system development outsourcing from the client perspective has been developed. We then investigate, (1) critical client risks for strategic IT system development outsourcing projects, and (2) the most common critical client risk factors for such projects. In order to identify any serious omissions in our framework an initial validation of the risk framework is provided through a review of nine published cases of unsuccessful strategic IT system development outsourcing projects. The risks critical to a client are associated with complexity, contract, execution, financial, legal, the organizational environment, planning and control, scope and requirements, the team, and the user. Risks manifest in all nine published cases, include (1) complexity and (2) the team. Three risk factors not previously identified in the initial framework are included in a revised framework. The risk framework assisted us in identifying a number of critical risk factors affecting the outcome of strategic IT system development outsourcing projects.
85|8||Using enterprise architecture and technology adoption models to predict application usage|Application usage is an important parameter to consider in application portfolio management. This paper presents an enterprise architecture analysis framework which can be used to assess application usage. The framework, in the form of an architecture metamodel, incorporates variables from the previously published Technology Acceptance Model (TAM) and the Task-Technology Fit (TTF) model. The paper describes how the metamodel has been tailored for a specific domain, viz. industry maintenance management. The metamodel was tested in the maintenance management domain through a survey with 55 respondents at five companies. Data collected in the survey showed that the domain-specific metamodel is able to explain variations in maintenance management application usage. Integrating the TAM and TTF variables with an architecture metamodel allows architects to reuse research results smoothly, thereby aiding them in producing good application portfolio decision-support.
85|9|http://www.sciencedirect.com/science/journal/01641212/85/9|Special issue: Selected papers from the 9th Working IEEE/IFIP Conference on Software Architecture (WICSA 2011)|
85|9||Collaborative prioritization of architectural concerns|Efficient architecture work involves balancing the degree of architecture documentation with attention to needs, costs, agility and other factors. This paper presents a method for prioritizing architectural concerns in the presence of heterogeneous stakeholder groups in large organizations that need to evolve existing architecture. The method involves enquiry, analysis, and deliberation using collaborative and analytical techniques. Method outcomes are action principles directed to managers, and assessment of user needs directed to architects, along with evidence. The method results from 4 years of action research at Ericsson AB with the purpose of adding missing views to architecture documentation and removing superfluous ones. It is illustrated on a case where 29 senior engineers and managers within Ericsson prioritized 37 architectural concerns areas to arrive at 8 action principles, 5 prioritized improvement areas, and 24 improvement suggestions. Feedback from the organization is that the method has been effective in prioritizing architectural concerns, that data collection and analysis is more extensive compared to traditional prioritization practices, but that extensive analysis seems inevitable in architecture improvement work.
85|9||RCDA: Architecting as a risk- and cost management discipline|We propose to view architecting as a risk- and cost management discipline. This point of view helps architects identify the key concerns to address in their decision making, by providing a simple, relatively objective way to assess architectural significance. It also helps business stakeholders to align the architect's activities and results with their own goals. We examine the consequences of this point of view on the architecture process. The point of view is the basis of RCDA, the Risk- and Cost Driven Architecture approach. So far, more than 150 architects have received RCDA training. For a majority of the trainees, RCDA has a significant positive impact on their architecting work.
85|9||Reference architecture, metamodel, and modeling principles for architectural knowledge management in information technology services|Capturing and sharing design knowledge such as architectural decisions is becoming increasingly important in firms providing professional Information Technology (IT) services such as enterprise application development and strategic outsourcing. Methods, models, and tools supporting explicit knowledge management strategies have been proposed in recent years; however, several challenges remain unaddressed. In this paper, we extend our previous work to overcome these challenges and to satisfy the requirements of an additional user group, presales architects that are responsible for IT service solution proposals. In strategic outsourcing, such solution proposals require complex, contractually relevant design decisions concerning many different resources such as IT infrastructures, people, and real estate. To support both presales and project architects, we define a common reference architecture and a decision process-oriented metamodel. We also present a tool implementation of these concepts and discuss their application to outsourcing proposals and application development projects. Finally, we establish twelve decision modeling principles and practices that capture the practical experience gained and lessons learned during the application of our decision modeling concepts to both proposal development and architecture design work on projects.
85|9||Industrial architectural assessment using TARA|Scenario based architectural assessment is a well-established approach for assessing architectural designs. However scenario-based methods are not always usable in an industrial context, where in our experience, they can be perceived as complicated and expensive to use. In this paper we explore why this may be the case and define a simpler technique called TARA, which has been designed for use in situations where scenario based methods are unlikely to be successful. The method is illustrated through an experience report that explains how it was applied to the assessment of two quantitative financial analysis systems, and its strengths, weaknesses and relationship to other methods are briefly discussed.
85|9||Dynamic service placement and replication framework to enhance service availability using team formation algorithm|The motivation of this work is to reduce the complexity in managing and administering services in the ever growing distributed environment via automated service placement and replication with team formation algorithm. The team formation algorithm is designed in a way that it would continuously search for resources with better performance and pool resources together to achieve better availability. The main intention of this work is not to replace the human administrators but to provide a better alternative in managing services in dynamic distributed environment. Instructions from the administrators are still required but at a different level. Administrators are freed from making low level decisions such as to decide the actual placement of the services and design the failover capabilities for each of the services. The evaluation results showed that the framework is capable of managing resources according to the requirements given by administrator, even during in the event of multiple consecutive resources failure. The proposed solution had the probability of 72.1% of its services that are still available after 83.3% of the available resources were shut down while conventional failover solution using three redundant units had only the probability of 40.8% of services that are still available.
85|9||Learning extended FSA from software: An empirical assessment|A number of techniques that infer finite state automata from execution traces have been used to support test and analysis activities. Some of these techniques can produce automata that integrate information about the data-flow, that is, they also represent how data values affect the operations executed by programs.
85|9||Cryptanalyzing a chaos-based image encryption algorithm using alternate structure|Recently, a chaos-based image encryption algorithm with an alternate structure (IEAS) was proposed. This paper applies the differential cryptanalysis on the IEAS and finds that some of its properties favor the differential attack which can recover an equivalent secret key with only a few number of chosen plain-images. Detailed procedures for cryptanalyzing IEAS with a lower round number are presented. Both theoretical analysis and experimental results are provided to show the vulnerability of IEAS against differential attack. In addition, some other security defects of IEAS, including insensitivity with respect to changes of plain-images and insufficient size of the key space, are also pointed out and verified.
85|9||On using planning poker for estimating user stories|While most studies in psychology and forecasting stress the possible hazards of group processes when predicting effort and schedule, agile software development methods recommend the use of a group estimation technique called planning poker for estimating the size of user stories and developing release and iteration plans. It is assumed that the group discussion through planning poker helps in identifying activities that individual estimators could overlook, thus providing more accurate estimates and reducing the over-optimism that is typical for expert judgment-based methods. In spite of the widespread use of agile methods, there is little empirical evidence regarding the accuracy of planning poker estimates. In order to fill this gap a study was conducted requiring 13 student teams to develop a Web-based student records information system. All teams were given the same set of user stories which had to be implemented in three Sprints. Each team estimated the stories using planning poker and the estimates provided by each team member during the first round were averaged to obtain the statistical combination for further comparison. In the same way the stories were estimated by a group of experts. The study revealed that students’ estimates were over-optimistic and that planning poker additionally increased the over-optimism. On the other hand, the experts’ estimates obtained through planning poker were much closer to actual effort spent and tended to be more accurate than the statistical combination of their individual estimates. The results indicate that the optimism bias caused by group discussion diminishes or even disappears as the expertise of the people involved in the group estimation process increases.
85|9||Differential fault analysis of ARIA in multi-byte fault models|
85|9||A computer assisted method for leukocyte nucleus segmentation and recognition in blood smear images|The number or ratio of leukocytes in the blood is often an indicator of diseases. The leukocyte differential count is the process of diagnosing diseases by counting the number and ratio of leukocytes in the blood. However, leukocytes are usually manually classified in laboratories by using microscopes. It is a painstaking and subjective task for biologists. In order to improve the recognition accuracy and decrease the time consuming, an automatic leukocyte detective system is essential for assisting biologists in diagnosing diseases. This paper provides a method to detect and recognize leukocyte automatically. In general, leukocytes are categorized into five groups, including lymphocytes, monocytes, eosinophils, basophils and neutrophils. Each category plays a different role in the human immune system. The nucleus contains the main composition of a leukocyte and it can be used as an important feature to classify a disease. In this paper, the nuclei are used to identify five types of leukocyte. The leukocyte cell nucleus enhancer is proposed to segment the region we are interested in by enhancing the region of the leukocyte nucleus and suppressing the other region of the blood smear images. The segmental threshold of leukocyte nuclei is based on the Otsu's method. In addition, the erythrocyte size is estimated not only to assist the filtering out of the non-leukocyte objects but also to classify the leukocytes. Then the co-occurrence matrix is used as a textural measure of segmented images. In addition to the texture measure, we take into account of shape measures. In the recognition steps, we reduce features by principle component analysis (PCA) to obtain suitable feature to distinguish the five types of leukocytes. The genetic algorithm based k-means clustering approach is used to classify the five kinds of leukocyte in the reduced dimensions. The experimental results show that even though only leukocyte nucleus features are used for classification in our method, we achieve a high and promised accurate recognition rate.
85|9||Fast and accurate link prediction in social networking systems|Online social networks (OSNs) recommend new friends to registered users based on local-based features of the graph (i.e. based on the number of common friends that two users share). However, OSNs do not exploit all different length paths of the network. Instead, they consider only pathways of maximum length 2 between a user and his candidate friends. On the other hand, there are global-based approaches, which detect the overall path structure in a network, being computationally prohibitive for huge-sized social networks. In this paper we provide friend recommendations, also known as the link prediction problem, by traversing all paths of a limited length, based on the “algorithmic small world hypothesis”. As a result, we are able to provide more accurate and faster friend recommendations. We also derive variants of our method that apply to different types of networks (directed/undirected and signed/unsigned). We perform an extensive experimental comparison of the proposed method against existing link prediction algorithms, using synthetic and three real data sets (Epinions, Facebook and Hi5). We also show that a significant accuracy improvement can be gained by using information about both positive and negative edges. Finally, we discuss extensively various experimental considerations, such as a possible MapReduce implementation of FriendLink algorithm to achieve scalability.
85|9||Security analysis of image cryptosystems only or partially based on a chaotic permutation|
85|9||A feedback-based decentralised coordination model for distributed open real-time systems|Moving towards autonomous operation and management of increasingly complex open distributed real-time systems poses very significant challenges. This is particularly true when reaction to events must be done in a timely and predictable manner while guaranteeing Quality of Service (QoS) constraints imposed by users, the environment, or applications. In these scenarios, the system should be able to maintain a global feasible QoS level while allowing individual nodes to autonomously adapt under different constraints of resource availability and input quality.
85|9||Understanding socially oriented roles and goals through motivational modelling|Technology has the potential to transform our home life, but only if it addresses the needs of its users. Understanding and modelling social needs is a challenge. For example, how do we understand, model, and then evaluate a system that must support needs such as “being fun”? In this paper, we define a systematic and repeatable process and method for understanding the roles and goals within a social domain for the purpose of informing technology design. We use the case study of building technology that supports meaningful interactions between grandparents and grandchildren separated by distance. Rather than attempt to define the roles of grandparents and grandchildren, and their associated goals, we study the roles and goals of activities in which grandparents and grandchildren typically engage, such as storytelling and gifting, and define role and goals models from the resulting data. The data obtained from the study of these activities provides a form of validation of the models. From these, we gain a better understanding of this complex social relationship, and how software systems can be built to support it. The models that emerge during the process are useful boundary objects, allowing knowledge to be shared across and between the disparate stakeholder communities, including end users, software engineers, and field researchers, and serve as inputs to the design process.
85|9||A systematic literature review of stakeholder identification methods in requirements elicitation|This paper presents a systematic review of relevant published studies related to topics in Requirements Engineering, specifically, concerning stakeholder identification methods in requirements elicitation, dated from 1984 to 2011. Addressing four specific research questions, this systematic literature review shows the following evidence gathered from these studies: current status of stakeholder identification in software requirement elicitation, the best practices recommended for its performance, consequences of incorrect identification in requirements quality, and, aspects which need to be improved. Our findings suggest that the analyzed approaches still have serious limitations in terms of covering all aspects of stakeholder identification as an important part of requirements elicitation. However, through correctly identifying and understanding the stakeholders, it is possible to develop high quality software.
85|9||Benefits of supplementing use case narratives with activity diagramsâAn exploratory study|Use case narratives modeling the complex functionality of a given system often extend for several pages due to the need to include numerous alternative scenario specifications. In such situations, it is difficult to ensure the completeness and validity of the process logic embedded in such lengthy text narratives. This exploratory study investigates the benefits of supplementing each complex and lengthy use case narrative with an activity diagram for analysts and clients during requirements gathering and analysis. Our findings indicate that the process logic in corresponding activity diagrams is more complete and offers a greater degree of validity than that used in use case narratives. In addition, the quality of the process logic in these artifacts is not negatively affected by a use case narrative's length or complexity when they are used together to capture system requirements. Our research provides empirical evidence of beneficial improvements in the quality of these widely used artifacts that subsequently help eliminate or minimize inconsistencies among the requirements specified in different artifacts.
85|9||Corrigendum to âGateway-oriented password-authenticated key exchange protocol in the standard modelâ [J. Syst. Softw. 85 (March (3)) (2012) 760â768]|
86|1|http://www.sciencedirect.com/science/journal/01641212/86/1|Signs of a thriving journal|
86|1||Failure prediction based on log files using Random Indexing and Support Vector Machines|The impact of failures on software systems can be substantial since the recovery process can require unexpected amounts of time and resources. Accurate failure predictions can help in mitigating the impact of failures. Resources, applications, and services can be scheduled to limit the impact of failures. However, providing accurate predictions sufficiently ahead is challenging. Log files contain messages that represent a change of system state. A sequence or a pattern of messages may be used to predict failures.
86|1||iTravel: A recommender system in mobile peer-to-peer environment|Recommender systems in mobile tourism have attracted considerable interest during the past decade. However, most existing recommender systems in mobile tourism fail to exploit information, evaluations or ratings provided by other tourists of similar interests. In this research, we propose to facilitate attraction recommendation task by exploring other tourists’ ratings on their visited attractions. The proposed approach employs mobile peer-to-peer communications for exchanging ratings via their mobile devices. A cost-effective travel recommender system—iTravel—thus is developed to provide tourists with on-tour attraction recommendation. We propose three data exchange methods that allow users to effectively exchange their ratings toward visited attractions. Simulated experiments are performed to evaluate the proposed data exchange methods and a user study is conducted to validate the usability of the proposed iTravel system.
86|1||A simulation model for strategic management process of software projects|In this study, a simulation model for the strategic management process of software development projects is presented. The proposed model simulates the implications of strategic decisions on factors such as cost, risk, budget and schedule of software projects. The main advantage of the proposed model is that it provides an integrated framework wherein risk management, cost estimation, and project management planning for the strategic management process of software development projects are linked. The results of the simulation of the project management planning determine the budget and schedule required for a project. Different strategic management decisions pose different sets of risks, each of which require different cost commitments. Hence, each strategic decision requires a project management plan with its own unique budget and schedule of software development. Thus, the simulation model estimates the risk and cost under different strategic decisions and maps them according to the project management plans. Therefore, the proposed integrated framework helps identify the best strategic option for the development and management of software projects. The proposed simulation model is nonspecific because it contains generic plug and play components that facilitate the use of any set of risk assessment, cost estimation models and project management tools. Therefore, it provides a flexible solution to software organisations and managers of software development projects. The simulation model is applied to a case study, which showed the effect of different strategic decisions on the risk and cost of the different phases of software development and ultimately on the budget and schedule required to complete the project. It therefore provides critical insights in identifying the best strategy for the development of software projects.
86|1||Common carotid artery condition recognition technology using waveform features extracted from ultrasound spectrum images|Medical image recognition algorithms have been widely applied to help with the diagnoses of various diseases, reducing human resource investment while enhancing diagnostic accuracy. This paper proposes a new scheme that specifies in the reading of ultrasound spectrum images of common carotid artery blood flow. The proposed scheme automatically extracts effective waveform features from the images for diagnostic purposes by using five criteria, which are ratio of waveform region, waveform region area target under horizontal baseline, waveform region area under horizontal baseline, highest point of waveform region, and lowest point of waveform region. Traditionally used by physicians to differentiate between normal blood flow patterns and five abnormal blood flow types, these five criteria are now employed by the new scheme to digitally diagnose vascular disorders at an accuracy rate as high as 0.97.
86|1||Collusion resilient spread spectrum watermarking in M-band wavelets using GA-fuzzy hybridization|This paper proposes a collusion resilient optimized spread spectrum (SS) image watermarking scheme using genetic algorithms (GA) and multiband (M-band) wavelets. M-band decomposition of the host image offers advantages of better scale-space tiling and good energy compactness. This bandpass-like decomposition makes watermarking robust against frequency selective fading-like gain (intelligent collusion) attack. On the other hand, GA would determine threshold value of the host coefficients (process gain i.e. the length of spreading code) selection for watermark casting along with the respective embedding strengths compatible to the gain of frequency response. First, a single bit watermark embedding algorithm is developed using independent and identically distributed (i.i.d) Gaussian watermark. This is further modified to design a high payload system for binary watermark image using a set of binary spreading code patterns. Watermark decoding performance is improved by multiple stage detection through cancelation of multiple bit interference (MBI) effect. Fuzzy logic is used to classify decision magnitudes in multiple group combined interference cancelation (MGCIC) used in the intermediate stage(s). Simulation results show convergence of GA and validate relative performance gain achieved in this algorithm compared to the existing works.
86|1||Improved multi-precision squaring for low-end RISC microcontrollers|We present an enhanced multi-precision squaring algorithm for low-end RISC microcontrollers. Generally, they have many general-purpose registers and limited bus size (8–32 bits). The proposed scheme employs a new technique, “lazy doubling” with optimizing computing sequences; so, it is significantly faster than the previous algorithms. Mathematical analysis shows that the number of clocks required by the proposed algorithm is about 67% of those required by the carry-catcher squaring algorithm. To the best of our knowledge this is known to be the fastest squaring algorithm. Experimental results on the ATmega128 microprocessor show that our algorithm is about 1.5 times faster than the carry-catcher squaring algorithm in terms of the number of clocks required. As squaring is a key operation in public key cryptography, the proposed algorithm can contribute to lowering power consumption in secure WSNs (wireless sensor networks) or secure embedded systems.
86|1||Refactoring legacy AJAX applications to improve the efficiency of the data exchange component|The AJAX paradigm encodes data exchange XML formats. Recently, JSON has also become a popular data exchange format. XML has numerous benefits including human-readable structures and self-describing data. However, JSON provides significant performance gains over XML due to its light weight nature and native support for JavaScript. This is especially important for Rich Internet Applications (RIA). Therefore, it is necessary to change the data format from XML to JSON for efficiency purposes. This paper presents a refactoring system (XtoJ) to safely assist programmers migrate existing AJAX-based applications utilizing XML into functionally equivalent AJAX-based applications utilizing JSON. We empirically demonstrate that our transformation system significantly improves the efficiency of AJAX applications.
86|1||RDOTE â Publishing Relational Databases into the Semantic Web|A necessary step for the evolution of the traditional Web into a Semantic Web is the transformation of the vast quantities of data, currently residing in Relational Databases into semantically aware data. In addition, in cases where new ontology schemata are developed, considerable experimentation with real data for testing the consistency of classes, properties and entailment rules is required. During the last decade, there has been intense research and development in creating methodologies and tools able to map Relational Databases with the Resource Description Framework (RDF). Although some systems have gained wider acceptance in the Semantic Web community, they either require users to learn a declarative language for encoding mappings or, in case they support friendly user interfaces, they provide limited expressivity.
86|1||Improvement of trace-driven I-Cache timing attack on the RSA algorithm|The previous I-Cache timing attacks on the RSA algorithm which exploit the instruction path of a cipher are mostly proof-of-concept, and it is harder to put them into practice than D-Cache timing attacks. We propose a trace-driven timing attack model on the RSA algorithm via spying on the whole I-Cache, instead of the partial instruction cache to which the multiplication function mapped, by analyzing the complications in the previous I-Cache timing attack on the RSA algorithm. Then, an improved analysis algorithm of the exponent using the characteristic of the window size in SWE algorithm is provided, which could further reduce the search space of the key bits than the former. We further demonstrate how to recover the private key d from the scattered known bits of dp and dq, through demonstrating some conclusions and validating it by experimentation. In addition, an error detection mechanism to detect some erroneous decisions of the operation sequences is provided to reduce the number of the erroneous recovered bits, and improve the precision of decision. We implement an I-Cache timing attack on RSA of OpenSSL in a practical environment, the experimental results show that the feasibility and effectiveness of I-Cache timing attack can be improved.
86|1||A content-aware bridging service for publish/subscribe environments|The OMG DDS (Data Distribution Service) standard specifies a middleware for distributing real-time data using a publish-subscribe data-centric approach. Until now, DDS systems have been restricted to a single and isolated DDS domain, normally deployed within a single multicast-enabled LAN. As systems grow larger, the need to interconnect different DDS domains arises. In this paper, we consider the problem of communicating disjoint data-spaces that may use different schemas to refer to similar information. In this regard, we propose a DDS interconnection service capable of bridging DDS domains as well as adapting between different data schemas. A key benefit of our approach is that is compliant with the latest OMG specifications, thus the proposed service does not require any modifications to DDS applications. The paper identifies the requirements for DDS data-spaces interconnection, presents an architecture that responds to those requirements, and concludes with experimental results gathered on our prototype implementation. We show that the impact of the service on the communications performance is well within the acceptable limits for most real-world uses of DDS (latency overhead is of the order of hundreds of microseconds). Reported results also indicate that our service interconnects remote data-spaces efficiently and reduces the network traffic almost N times, with N being the number of final data subscribers.
86|1||From chaos to the systematic harmonization of multiple reference models: A harmonization framework applied in two case studies|At the present time, we can observe that in an effort to deal with the issue of quality, a variety of models, standards and methodologies have been developed to give support in different domains of the IT industry. This wide range of heterogeneous models makes it possible to resolve multiple needs. In recent years, as the integration of different models has increased, organizations have started to note that their business and technical processes can be aligned with more than one model. Currently, however, we are not aware of any other attempts to provide an explicit and systematic solution that would allow us to address the issue of harmonization of multiple reference models in such a way as to satisfy the needs of the companies. In the quest to help support the work of harmonization of multiple models, this paper presents (i) a framework that defines elements needed to support the harmonization of multiple reference models, (ii) a process, which is the backbone and way of integrating all the elements defined in the framework thus allowing the implementation of a harmonization project to be guided systematically, harmonizing multiple models through the configuration of a harmonization strategy, and (iii) a set of methods, which allows us to know “what to do”, as well as “how to put” two or more models in consonance with each other. The experience of the application of our proposal is illustrated in two case studies. The findings obtained show that the harmonization process has enabled us to harmonize and put the models involved in consonance with each other.
86|1||Towards an early software estimation using log-linear regression and a multilayer perceptron model|Software estimation is a tedious and daunting task in project management and software development. Software estimators are notorious in predicting software effort and they have been struggling in the past decades to provide new models to enhance software estimation. The most critical and crucial part of software estimation is when estimation is required in the early stages of the software life cycle where the problem to be solved has not yet been completely revealed. This paper presents a novel log-linear regression model based on the use case point model (UCP) to calculate the software effort based on use case diagrams. A fuzzy logic approach is used to calibrate the productivity factor in the regression model. Moreover, a multilayer perceptron (MLP) neural network model was developed to predict software effort based on the software size and team productivity. Experiments show that the proposed approach outperforms the original UCP model. Furthermore, a comparison between the MLP and log-linear regression models was conducted based on the size of the projects. Results demonstrate that the MLP model can surpass the regression model when small projects are used, but the log-linear regression model gives better results when estimating larger projects.
86|1||Empirical validation of a usability inspection method for model-driven Web development|Web applications should be usable in order to be accepted by users and to improve their success probability. Despite the fact that this requirement has promoted the emergence of several usability evaluation methods, there is a need for empirically validated methods that provide evidence about their effectiveness and that can be properly integrated into early stages of Web development processes. Model-driven Web development processes have grown in popularity over the last few years, and offer a suitable context in which to perform early usability evaluations due to their intrinsic traceability mechanisms. These issues have motivated us to propose a Web Usability Evaluation Process (WUEP) which can be integrated into model-driven Web development processes. This paper presents a family of experiments that we have carried out to empirically validate WUEP. The family of experiments was carried out by 64 participants, including PhD and Master's computer science students. The objective of the experiments was to evaluate the participants’ effectiveness, efficiency, perceived ease of use and perceived satisfaction when using WUEP in comparison to an industrial widely used inspection method: Heuristic Evaluation (HE). The statistical analysis and meta-analysis of the data obtained separately from each experiment indicated that WUEP is more effective and efficient than HE in the detection of usability problems. The evaluators were also more satisfied when applying WUEP, and found it easier to use than HE. Although further experiments must be carried out to strengthen these results, WUEP has proved to be a promising usability inspection method for Web applications which have been developed by using model-driven development processes.
86|1||Semantic ranking of web pages based on formal concept analysis|A web crawler is an important research component in a search engine. In this paper, a new method for measuring the similarity of formal concept analysis (FCA) concepts and a new notion of a web page's rank are proposed that use an information content approach based on users’ web logs. First, an extension similarity and an intension similarity that analyze a user's browsing pattern and their hyperlinks are proposed. Second, the information content similarity between two nouns is computed automatically by examining their ISA and Part-Of hierarchy and using a user's web log. A method for computing the semantic similarity between two concepts in two different concept lattices (the base concept lattice and the current concept lattice) and finding the semantic ranking of web pages is proposed. Last, our experiment demonstrates that our crawler is more suitable for crawling focused web pages. It proves that the semantic ranking of web pages is useful and efficient for making a web crawler's choice of a web page for continuing work.
86|1||Improving Graph Cuts algorithm to transform sequence of stereo image to depth map|Recently, 3D display systems are getting considerable attentions not only from theater but also from home. 3D multimedia content development plays very important role in helping to setup a visual reality entertainment program. Lenticular Autostereoscopic display is one of the 3D-TV having the following advantages such as improving 3D viewing experience, supporting wider viewing angles for multiple viewers, and no requiring any special glasses. However, most of the current 3D movie and camera do not support the Autostereoscopic function. Therefore, we proposed a system that can transform the current 3D stereoscopic image sequence to the depth map sequence. These sequences can be warped into the multiplexed image by DIBR (Depth Image Based Rendering), and show with Autostereoscopic.
86|1||Chaos-based detection of LDoS attacks|A low-rate denial of service (LDoS) attack behaves as a small signal in periodic pulses with low average rate, which hides in normal TCP traffic stealthily. LDoS attacks reduce link throughput and degrade QoS of a target. An approach of detecting LDoS attacks is proposed based on Duffing oscillator in chaos systems. The approach detects LDoS attacks by adopting the technology of digital signal processing (DSP), which takes an LDoS attack as a small signal and normal TCP traffic as background noise. Duffing oscillator is used to detect LDoS attacks in normal TCP traffic. Simulations show that the LDoS attacks can be detected through diagram of the chaotic state, and the period and pulse width of LDoS attacks can be estimated.
86|1||A lossless copyright authentication scheme based on BesselâFourier moment and extreme learning machine in curvature-feature domain|To overcome some drawbacks existing in current zero-watermarking methods, a lossless copyright authentication scheme is proposed in this paper. This scheme designs a multiple zero-watermarking algorithm based on Bessel–Fourier moment and extreme learning machine (ELM) in curvature-feature domain, develops a method for image feature enhancement and noise suppression in curvature-feature domain, and presents a simple algorithm which uses Bessel–Fourier moment phase to estimate the rotation angle of the rotation-attacked image. The experimental results, involving five types of images, indicate the proposed scheme has better overall performance compared to other five current methods, especially in the aspects of resisting high ratio cropping and large angle rotation attacks. Finally, some related factors including phase and magnitude components, feature vector dimension and ELM optimization are considered in the algorithm performance evaluation.
86|1||Analytical architecture-based performability evaluation of real-time software systems|Real-time systems are usually employed in dynamic and harsh environments. Real-time software, as one important part of such systems, potentially suffers from two problems: unpredictability in the timing behavior which affects the software performance, and logical faults which affect the software reliability. The former problem is mitigated by improving the software algorithm, architecture, and code. The latter problem is also relieved via software redundancy methods, even though these methods may adversely affect the software performance and architectural complexity. Despite these problems, it is expected to have a guaranteed service level in real-time systems, which the service is defined as the successful completion of the software mission within its deadline. In this paper, we propose two architecture-based analytical methods for simultaneous performance and reliability (performability) evaluation of real-time component-based software: one is accurate and the other is approximate. The accurate method is sound and precise but more complex in the computations, while the approximate method is easy-to-follow with reasonable amounts of computations. Examples of different configurations have been presented to show how well the latter method approximates the former one. Some performability sensitivity analyses with respect to the software component properties have also been done for better depiction of the importance of employing the proposed analytical methods in finding and eliminating the software performability bottlenecks.
86|10|http://www.sciencedirect.com/science/journal/01641212/86/10|Quality optimisation of software architectures and design specifications|This special issue of the Journal of Systems and Software presents novel software architecture optimisation frameworks. The majority of the approaches consider the problem of optimising conflicting quality attributes simultaneously. Other approaches focus on effectively searching for better software architectures by either using smart problem-dependent heuristics or by combining the expression power of ADLs with architecture optimisation.
86|10||Automatic optimisation of system architectures using EAST-ADL|There are many challenges which face designers of complex system architectures, particularly safety–critical or real-time systems. The introduction of Architecture Description Languages (ADLs) has helped to meet these challenges by consolidating information about a system and providing a platform for modelling and analysis capabilities. However, managing this wealth of information can still be problematic, and evaluation of potential design decisions is still often performed manually. Automatic architectural optimisation can be used to assist this decision process, enabling designers to rapidly explore many different options and evaluate them according to specific criteria. In this paper, we present a multi-objective optimisation approach based on EAST-ADL, an ADL in the automotive domain, with the goal of combining the advantages of ADLs and architectural optimisation. The approach is designed to be extensible and leverages the capabilities of EAST-ADL to provide support for evaluation according to different factors, including dependability, timing/performance, and cost. The technique is applied to an illustrative example system featuring both hardware and software perspectives, demonstrating the potential benefits of this concept to the design of embedded system architectures.
86|10||Efficient optimization of large probabilistic models|The development of safety critical systems often requires design decisions which influence not only dependability, but also other properties which are often even antagonistic to dependability, e.g., cost. Finding good compromises considering different goals while at the same time guaranteeing sufficiently high safety of a system is a very difficult task.
86|10||MOO: An architectural framework for runtime optimization of multiple system objectives in embedded control software|Today's complex embedded systems function in varying operational conditions. The control software adapts several control variables to keep the operational state optimal with respect to multiple objectives. There exist well-known techniques for solving such optimization problems. However, current practice shows that the applied techniques, control variables, constraints and related design decisions are not documented as a part of the architecture description. Their implementation is implicit, tailored for specific characteristics of the embedded system, tightly integrated into and coupled with the control software, which hinders its reusability, analyzability and maintainability. This paper presents an architectural framework to design, document and realize multi-objective optimization in embedded control software. The framework comprises an architectural style together with its visual editor and domain-specific analysis tools, and a code generator. The code generator generates an optimizer module specific for the given architecture and it employs aspect-oriented software development techniques to seamlessly integrate this module into the control software. The effectiveness of the framework is validated in the context of an industrial case study from the printing systems domain.
86|10||S-IDE: A tool framework for optimizing deployment architecture of High Level Architecture based simulation systems|One of the important problems in High Level Architecture (HLA) based distributed simulation systems is the allocation of the different simulation modules to the available physical resources. Usually, the deployment of the simulation modules to the physical resources can be done in many different ways, and each deployment alternative will have a different impact on the performance. Although different algorithmic solutions have been provided to optimize the allocation with respect to the performance, the problem has not been explicitly tackled from an architecture design perspective. Moreover, for optimizing the deployment of the simulation system, tool support is largely missing. In this paper we propose a method for automatically deriving deployment alternatives for HLA based distributed simulation systems. The method extends the IEEE Recommended Practice for High Level Architecture Federation Development and Execution Process by providing an approach for optimizing the allocation at the design level. The method is realized by the tool framework, S-IDE (Simulation-IDE) that we have developed to provide an integrated development environment for deriving a feasible deployment alternative based on the simulation system and the available physical resources at the design phase. The method and the tool support have been validated using a case study for the development of a traffic simulation system.
86|10||Hybrid multi-attribute QoS optimization in component based software systems|Design decisions for complex, component-based systems impact multiple quality of service (QoS) properties. Often, means to improve one quality property deteriorate another one. In this scenario, selecting a good solution with respect to a single quality attribute can lead to unacceptable results with respect to the other quality attributes. A promising way to deal with this problem is to exploit multi-objective optimization where the objectives represent different quality attributes. The aim of these techniques is to devise a set of solutions, each of which assures an optimal trade-off between the conflicting qualities. Our previous work proposed a combined use of analytical optimization techniques and evolutionary algorithms to efficiently identify an optimal set of design alternatives with respect to performance and costs. This paper extends this approach to more QoS properties by providing analytical algorithms for availability-cost optimization and three-dimensional availability-performance-cost optimization. We demonstrate the use of this approach on a case study, showing that the analytical step provides a better-than-random starting population for the evolutionary optimization, which lead to a speed-up of 28% in the availability-cost case.
86|10||Quality-driven optimization of system architecture: Industrial case study on an automotive sub-system|Due to the complexity of today's embedded systems and time-to-market competition between companies developing embedded systems, system architects have to perform a complex task. To design a system which meets all its quality requirements becomes increasingly difficult because of customer demand for new innovative user functions. Methods and tools are needed to assist the architect during system design.
86|10||Supporting end-to-end quality of service properties in OMG data distribution service publish/subscribe middleware over wide area networks|Assuring end-to-end quality-of-service (QoS) in distributed real-time and embedded (DRE) systems is hard due to the heterogeneity and scale of communication networks, transient behavior, and the lack of mechanisms that holistically schedule different resources end-to-end. This paper makes two contributions to research focusing on overcoming these problems in the context of wide area network (WAN)-based DRE applications that use the OMG Data Distribution Service (DDS) QoS-enabled publish/subscribe middleware. First, it provides an analytical approach to bound the delays incurred along the critical path in a typical DDS-based publish/subscribe stream, which helps ensure predictable end-to-end delays. Second, it presents the design and evaluation of a policy-driven framework called Velox. Velox combines multi-layer, standards-based technologies—including the OMG DDS and IP DiffServ—to support end-to-end QoS in heterogeneous networks and shield applications from the details of network QoS mechanisms by specifying per-flow QoS requirements. The results of empirical tests conducted using Velox show how combining DDS with DiffServ enhances the schedulability and predictability of DRE applications, improves data delivery over heterogeneous IP networks, and provides network-level differentiated performance.
86|10||On the reliability of mapping studies in software engineering|Systematic literature reviews and systematic mapping studies are becoming increasingly common in software engineering, and hence it becomes even more important to better understand the reliability of such studies.
86|10||Power-aware scheduling algorithms for sporadic tasks in real-time systems|In this paper, we consider the canonical sporadic task model with the system-wide energy management problem. Our solution uses a generalized power model, in which the static power and the dynamic power are considered. We present a static solution to schedule the sporadic task set, assuming worst-case execution time for each sporadic tasks release, and propose a dynamic solution to reclaim the slacks left by the earlier completion of tasks than their worst-case estimations. The experimental results show that the proposed static algorithm can reduce the energy consumption by 20.63%–89.70% over the EDF* algorithm and the dynamic algorithm consumes 2.06%–24.89% less energy than that of the existing DVS algorithm.
86|10||Reversible watermarking method based on asymmetric-histogram shifting of prediction errors|This paper tries to provide a new perspective for the research of reversible watermarking based on histogram shifting of prediction errors. Instead of obtaining one prediction error for the current pixel, we calculate multiple prediction errors by designing a multi-prediction scheme. An asymmetric error histogram is then constructed by selecting the suitable one from these errors. Compared with traditional symmetric histogram, the asymmetric error histogram reduces the amount of shifted pixels, thus improving the watermarked image quality. Moreover, a complementary embedding strategy is proposed by combining the maximum and minimum error histograms. As the two error histograms shift in the opposite directions during the embedding, some watermarked pixels will be restored to their original values, thus the image quality is further improved. Experimental findings also show that the proposed method re-creates watermarked images of higher quality that carry larger embedding capacity compared to conventional symmetric histogram methods, such as Tsai et al.’s and Luo et al.’s works.
86|10||Affinity-aware modeling of CPU usage with communicating virtual machines|Use of virtualization in Infrastructure as a Service (IaaS) environments provides benefits to both users and providers: users can make use of resources following a pay-per-use model and negotiate performance guarantees, whereas providers can provide quick, scalable and hardware-fault tolerant service and also utilize resources efficiently and economically. With increased acceptance of virtualization-based systems, an important issue is that of virtual machine migration-enabled consolidation and dynamic resource provisioning. Effective resource provisioning can result in higher gains for users and providers alike. Most hosted applications (for example, web services) are multi-tiered and can benefit from their various tiers being hosted on different virtual machines. These mutually communicating virtual machines may get colocated on the same physical machine or placed on different machines, as part of consolidation and flexible provisioning strategies. In this work, we argue the need for network affinity-awareness in resource provisioning for virtual machines. First, we empirically quantify the change in CPU resource usage due to colocation or dispersion of communicating virtual machines for both Xen and KVM virtualization technologies. Next, we build models based on these empirical measurements to predict the change in CPU utilization when transitioning between colocated and dispersed placements. Due to the modeling process being independent of virtualization technology and specific applications, the resultant model is generic and application-agnostic. Via extensive experimentation, we evaluate the applicability of our models for synthetic and benchmark application workloads. We find that the models have high prediction accuracy — maximum prediction error within 2% absolute CPU usage.
86|10||Code smells as system-level indicators of maintainability: An empirical study|Code smells are manifestations of design flaws that can degrade code maintainability. So far, no research has investigated if these indicators are useful for conducting system-level maintainability evaluations.
86|10||Common weaving approach in mainstream languages for software security hardening|In this paper, we propose a novel aspect-oriented approach based on GIMPLE, a language-independent and a tree-based representation generated by the GNU Compiler Collection (GCC), for the systemization of application security hardening. The security solutions are woven into GIMPLE representations in a systematic way, eliminating the need for manual hardening that might generate a considerable number of errors. To achieve this goal, we present a formal specification for GIMPLE weaving and the implementation strategies of the proposed weaving semantics. Syntax for a common aspect-oriented language that is abstract and multi-language support together with syntax for a core set for GIMPLE constructs are presented to express the weaving semantics. GIMPLE weaving accompanied by a common aspect-oriented language (1) allows security experts providing security solutions using this common language, (2) lets developers focus on the main functionality of programs by relieving them from the burden of security issues, (3) unifies the matching and the weaving processes for mainstream languages, and (4) facilitates introducing new security features in AOP languages. We handle the correctness and the completeness of GIMPLE weaving in two different ways. In the first approach, we prove them according to the rules and algorithms provided in this paper. In the second approach, we accommodate Kniesel's discipline that ensures that security solutions specified by our approach are applied at all and only the required points in source code, taking into consideration weaving interactions and interferences. Finally, we explore the viability and the relevance of our propositions by applying the defined approach for systematic security hardening to develop case studies.
86|10||SCRUMIAâAn educational game for teaching SCRUM in computing courses|Due to the increasing use of agile methods, teaching SCRUM as an agile project management methodology has become more and more important. In order to teach students to be able to apply SCRUM in concrete situations, often educational (simulation) games are used. However, most of these games have been developed more for professional trainings than taking into consideration typical restrictions of university courses (such as, class duration and low financial resources for instructional materials). Therefore, we present a manual paper and pencil game to reinforce and teach the application of SCRUM in undergraduate computing programmes complementing theoretical lectures. The game has been developed following a systematic instructional design process and based on our teaching experience. It has been applied several times in two undergraduate project management courses. We evaluated motivation, user experience and the game's contribution to learning through case studies on Kirkpatrick's level one based on the perception of the students. First results indicate the potential of the game to contribute to the learning of SCRUM in an engaging way, keeping students immersed in the learning task. In this regard, the game offers a low-budget alternative to complement traditional instructional strategies for teaching SCRUM in the classroom.
86|10||Cell-related location area planning for 4G PCS networks with variable-order Markov model|Location management is a critical issue in personal communication service (PCS) networks, tracking the location of user equipment (UE) with the goal of minimizing total signaling cost. Previous work can be classified into two categories: static and dynamic. Static schemes partition networks into fixed size LAs. However, these schemes are inefficient because they do not take UEs’ mobility and the call arrival rate into account. On the other hand, focusing on individual UEs, dynamic schemes have minimized the location management cost. However, they are difficult to implement because recording the individual information of numerous UEs and planning each of their LAs consume uncontrollable cost. Because of these reasons, we propose a cell-based scheme between static and dynamic schemes. Considering people usually stay in specific zones for long periods and the movement of UEs usually presents a strong moving direction in areas, this study presents a distributed algorithm by employing variable-order Markov models to find the mobility characteristic shared by UEs to plan better LAs with lower location management cost. When the order of Markov model is set to 1, our method is equal to a pure cell-centric LAP scheme; while the order of Markov model is high, it is more like a profile-based dynamic scheme. So, the setting of the order actually is a trade-off problem between the overall location management cost and the computing complexity. We present to retrieve a balance by using the expected location management cost and the number of total states of Markov models. In simulations, the origin–destination matrix (O–D matrix) from the Taipei Rapid Transit Corporation is used for representing the association between two cells. Simulation results demonstrate that the proposed scheme achieves good performance.
86|10||Reversible data hiding based on PDE predictor|In this paper, we propose a prediction-error expansion based reversible data hiding by using a new predictor based on partial differential equation (PDE). For a given pixel, PDE predictor uses the mean of its four nearest neighboring pixels as initial prediction, and then iteratively updates the prediction until the value goes stable. Specifically, for each pixel, by calculating the gradients of four directions, the direction with small magnitude of gradient will be weighted larger in the iteration process, and finally a more accurate prediction can be obtained. Since PDE predictor can better exploit image redundancy, the proposed method introduces less distortion for embedding the same payload. Experimental results show that our method outperforms some state-of-the-art methods.
86|11|http://www.sciencedirect.com/science/journal/01641212/86/11|The Meta-Protocol framework|A communication protocol is a set of rules shared by two or more communicating parties on the sequence of operations and the format of messages to be exchanged. Standardization organizations define protocols in the form of recommendations (e.g., RFC) written in technical English, which requires a manual translation of the specification into the protocol implementation. This human translation is error-prone due in part to the ambiguities of natural language and in part due to the complexity of some protocols. To mitigate these problems, we divided the expression of a protocol specification into two parts. First, we designed an XML-based protocol specification language (XPSL) that allows for the high-level specification of a protocol—expressed as a Finite State Machine (FSM)—using Component-Based Software Engineering (CBSE) principles. Then, the components required by the protocol are specified in any suitable technical language (formal or informal). In addition, we developed the multi-layer Meta-Protocol framework, which allows for on-the-fly protocol discovery and negotiation, distribution of protocol specifications and components, and automatic protocol implementation in any programming language.
86|11||Minimizing test-point allocation to improve diagnosability in business process models|Diagnosability analysis aims to determine whether observations available during the execution of a system are sufficient to precisely locate the source of a problem. Previous work deals with the diagnosability problem in contexts such as circuits and systems, but no with the adaptation of the diagnosability problem to business processes. In order to improve the diagnosability, a set of test points needs to be allocated. Therefore, the aim of this contribution is to determine a test-point allocation to obtain sufficient observable data in the dataflow to allow the discrimination of faults for a later diagnosis process. The allocation of test points depends on the strategies of the companies, for this reason we defined two possibilities: to improve the diagnosability of a business process for a fixed number of test points and the minimization of the number of test points for a given level of diagnosability. Both strategies have been implemented in the Test-Point Allocator tool in order to facilitate the integration of the test points in the business process model life cycle. Experimental results indicate that diagnosability of business processes can be improved by allocating test points in an acceptable time.
86|11||Genetic algorithm and difference expansion based reversible watermarking for relational databases|In this paper, we present a new robust and reversible watermarking approach for the protection of relational databases. Our approach is based on the idea of difference expansion and utilizes genetic algorithm (GA) to improve watermark capacity and reduce distortion. The proposed approach is reversible and therefore, distortion introduced after watermark insertion can be fully restored. Using GA, different attributes are explored to meet the optimal criteria rather than selecting less effective attributes for watermark insertion. Checking only the distortion tolerance of two attributes for a selected tuple may not be useful for watermark capacity and distortion therefore, distortion tolerance of different attributes are explored. Distortion caused by difference expansion can help an attacker to predict watermarked attribute. Thus, we have incorporated tuple and attribute-wise distortion in the fitness function of GA, making it tough for an attacker to predict watermarked attribute. From experimental analysis, it is concluded that the proposed technique provides improved capacity and reduced distortion compared to existing approaches. Problem of false positives and change in attribute order at detection side is also resolved. Additionally, the proposed technique is resilient against a wide range of attacks such as addition, deletion, sorting, bit flipping, tuple-wise-multifaceted, attribute-wise-multifaceted, and additive attacks.
86|11||Model-based cache-aware dispatching of object-oriented software for multicore systems|In recent years, processor technology has evolved towards multicore processors, which include multiple processing units (cores) in a single package. Those cores, having their own private caches, often share a higher level cache memory dedicated to each processor die. This multi-level cache hierarchy in multicore processors raises the importance of cache utilization problem. Assigning parallel-running software components with common data to processor cores that do not share a common cache increases the number of cache misses. In this paper we present a novel approach that uses model-based information to guide the OS scheduler in assigning appropriate core affinities to software objects at run-time. We build graph models of software and cache hierarchies of processors and devise a graph matcher algorithm that provides mapping between these two graphs. Using this mapping we obtain candidate core sets that each software object can be affiliated with at run-time. These affiliations are determined based on the idea that software components that have the potential to share common data at run-time should run on cores that share a common cache. We also develop an object dispatcher algorithm that keeps track of object affiliations at run-time and dispatches objects by using the information from the compile-time graph matcher. We apply our approach on design pattern implementations and two different application program running on servers using CFS scheduling. Our results show that cache-aware dispatching based on information obtained from software model, decreases number of cache misses significantly and improves CFS’ scheduling performance.
86|11||A high-performance reversible data-hiding scheme for LZW codes|Hiding a message in compression codes can reduce transmission costs and simultaneously make the transmission more secure. In this paper, we propose a high-performance, data-hiding Lempel–Ziv–Welch (HPDH-LZW) scheme, which reversibly embeds data in LZW compression codes by modifying the value of the compression codes, where the value of the LZW code either remains unchanged or is changed to the original value of the LZW code plus the LZW dictionary size according to the data to be embedded. Compared to other information-hiding schemes based on LZW compression codes, the proposed scheme achieves better hiding capacity by increasing the number of symbols available to hide secrets and also achieves faster hiding and extracting speeds due to the lower computation requirements. Our experimental results with the proposed scheme have confirmed both its high embedding capacity and its high speed when hiding and extracting data.
86|11||Virtualized Web server cluster self-configuration to optimize resource and power use|This work proposes a reusable architecture that enables the self-configuration of a supporting infrastructure for Web server clusters using virtual machines. The goal of the architecture is to ensure service quality, evaluating how broadly it complies with the application's operating restrictions and proportionally acting on the configuration of physical servers (hosts) or virtual machines. In addition, through the rational use of resources, the proposal aims at saving energy. A prototype of the architecture was developed and a performance evaluation carried out with two different resource management approaches. This evaluation shows how fully functional and advantageous the proposal is in terms of using resources, avoiding waste, yet maintaining the application's quality of service within acceptable levels. The architecture also shows to be flexible enough to accept, with a reasonable amount of effort, different resource self-configuration policies.
86|11||The lean gap: A review of lean approaches to large-scale software systems development|Lean approaches to product development (LPD) have had a strong influence on many industries and in recent years there have been many proponents for lean in software development as it can support the increasing industry need of scaling agile software development. With it's roots in industrial manufacturing and, later, industrial product development, it would seem natural that LPD would adapt well to large-scale development projects of increasingly software-intensive products, such as in the automotive industry. However, it is not clear what kind of experience and results have been reported on the actual use of lean principles and practices in software development for such large-scale industrial contexts. This was the motivation for this study as the context was an ongoing industry process improvement project at Volvo Car Corporation and Volvo Truck Corporation.
86|11||A novel approach to evaluate software vulnerability prioritization|The aim of this study is to formulate an analysis model which can express the security grades of software vulnerability and serve as a basis for evaluating danger level of information program or filtering hazardous weaknesses of the system and improve it to counter the threat of different danger factors. Through the utilization of fuzzy analytic hierarchy process (FAHP), we will organize the crossover factors of the software blind spots and build an evaluation framework. First of all, via the fuzzy Delphi method the aspects and relative determinants affecting security will be filtered out. Then we will identify the value equation of each factor and settle down the fuzzy synthetic decision making model of software vulnerability. Thanks to this model we will be able to analyze the various degrees to which the vulnerability is affecting the security and this information will serve as a basis for future ameliorations of the system itself. The higher the security score obtained therefore imply securer system. Beside this, this study also propose an improvement from the traditional fuzzy synthetic decision making model for measuring the fuzziness between enhancement and independence of various aspects and criteria. Furthermore taking into consideration the subjectivity of human in reality and constructing the fuzzy integral decision making model. Through case study, we show that the evaluation model in question is practical and can be applied on the new software vulnerabilities and measure their degree of penetration. The fuzzy integral decision making emphasize through formulation the multiply-add effect between different factors influencing information security.
86|11||Adaptive reversible watermarking with improved embedding capacity|Embedding capacity is one of the most important issues of the reversible watermarking. However, the theoretical maximum embedding capacity of most reversible watermarking algorithms is only 1.0 bits per pixel (bpp). To achieve a higher capacity, we have to modify the least significant bit (LSB) multiple times which definitely lowers the quality of the embedded image. To this end, this paper proposes a novel reversible watermarking algorithm by employing histogram shifting and adaptive embedding. Specifically, the amount of the embedded watermark is adaptively determined in terms of the context of each pixel. For pixels with small prediction error, we modify the second, third and even the fourth LSBs as well to embed more than one watermark bit. Consequently, the proposed method achieves the embedding capacity larger than 1.0 bpp in single-pass embedding as well as bringing relatively low embedding distortion. The superiority of the proposed method is experimental verified by comparing with other existing schemes.
86|11||Adapting system execution traces to support analysis of software system performance properties|UNITE is a method and tool that analyzes software system performance properties, e.g., end-to-end response time, throughput, and service time, via system execution traces. UNITE, however, assumes that a system execution trace contains properties (e.g., identifiable keywords, unique message instances, and enough variation among the same event types) to support performance analysis. With proper planning, it is possible to ensure that properties required to support such analysis are incorporated in the generated system execution trace. It, however, is not safe to assume this to be the case with many existing software systems.
86|11||Low bit-rate information hiding method based on search-order-coding technique|Information hiding method with low bit rate is important in secure communications. To reduce bit rate we propose a new embedding method in this paper based on SOC (search-order coding) compression technique. Compared to Chang et al.’s scheme in 2004, our scheme completely avoids the transform from SOC coding to OIV (original index values) coding to significantly reduce bit rate. In order to further reduce bit rate, Chang et al. proposed a reversible data hiding scheme using hybrid encoding strategies by introducing the side-match vector quantization (SMVQ) in 2013. But it needed additional 1 bit indicator to distinguish the two statuses to determine OIV is belonged to G1 or G2. This overhead gave a large burden to compression rate and could not reduce the bit rate significantly. In contrast, our scheme completely avoids this indicator. The experimental results show that the proposed method can efficiently reduce the bit rate and have the same embedding capacity compared with Chang et al.’s scheme in 2004 and Chang et al.’s scheme in 2013. Moreover, our proposed scheme can also achieve a better performance in both the embedding capacity and bit rate than other related VQ-based information hiding schemes.
86|11||A secure palm vein recognition system|With the increasing needs in security systems, vein recognition is one of the important and reliable solutions of identity security for biometrics-based identification systems. The obvious and stable line-feature-based approach can be used to clearly describe a palm vein patterns for personal identification. In this paper, a directional filter bank involving different orientations is designed to extract the vein pattern and the minimum directional code (MDC) is employed to encode the line-based vein features in binary code. In addition, there are many non vein pixels in the vein image and those pixels are unmeaning for vein recognition. To improve the accuracy, the non-vein pixels are detected by evaluating the directional filtering magnitude (DFM) and considered the non-orientation code. A total of 5120 palm vein images from 256 persons are used to verify the validity of the proposed palm vein recognition approach. High accuracies (>99%) and low equal error rate (0.54%) obtained by the proposed method show that our proposed approach is feasible and effective for palm vein recognition.
86|11||A systematic review on the functional testing of semantic web services|Semantic web services are gaining more attention as an important element of the emerging semantic web. Therefore, testing semantic web services is becoming a key concern as an essential quality assurance measure. The objective of this systematic literature review is to summarize the current state of the art of functional testing of semantic web services by providing answers to a set of research questions. The review follows a predefined procedure that involves automatically searching 5 well-known digital libraries. After applying the selection criteria to the results, a total of 34 studies were identified as relevant. Required information was extracted from the studies and summarized. Our systematic literature review identified some approaches available for deriving test cases from the specifications of semantic web services. However, many of the approaches are either not validated or the validation done lacks credibility. We believe that a substantial amount of work remains to be done to improve the current state of research in the area of testing semantic web services.
86|11||A domain-specific language for context modeling in context-aware systems|Context-awareness refers to systems that can both sense and react based on their environment. One of the main difficulties that developers of context-aware systems must tackle is how to manage the needed context information. In this paper we present MLContext, a textual Domain-Specific Language (DSL) which is specially tailored for modeling context information. It has been implemented by applying Model-Driven Development (MDD) techniques to automatically generate software artifacts from context models. The MLContext abstract syntax has been defined as a metamodel, and model-to text transformations have been written to generate the desired software artifacts. The concrete syntax has been defined with the EMFText tool, which generates an editor and model injector.
86|11||SAAD, a content based Web Spam Analyzer and Detector|Web Spam is one of the main difficulties that crawlers have to overcome and therefore one of the main problems of the WWW. There are several studies about characterising and detecting Web Spam pages. However, none of them deals with all the possible kinds of Web Spam. This paper shows an analysis of different kinds of Web Spam pages and identifies new elements that characterise it, to define heuristics which are able to partially detect them. We also discuss and explain several heuristics from the point of view of their effectiveness and computational efficiency. Taking them into account, we study several sets of heuristics and demonstrate how they improve the current results. Finally, we propose a new Web Spam detection system called SAAD (Spam Analyzer And Detector), which is based on the set of proposed heuristics and their use in a C4.5 classifier improved by means of Bagging and Boosting techniques. We have also tested our system in some well known Web Spam datasets and we have found it to be very effective.
86|11||Supporting concept location through identifier parsing and ontology extraction|Identifier names play a key role in program understanding and in particular in concept location. Programmers can easily “parse” identifiers and understand the intended meaning. This, however, is not trivial for tools that try to exploit the information in the identifiers to support program understanding. To address this problem, we resort to natural language analyzers, which parse tokenized identifier names and provide the syntactic relationships (dependencies) among the terms composing the identifiers. Such relationships are then mapped to semantic relationships.
86|11||Real-time risk monitoring in business processes: A sensor-based approach|This article proposes an approach for real-time monitoring of risks in executable business process models. The approach considers risks in all phases of the business process management lifecycle, from process design, where risks are defined on top of process models, through to process diagnosis, where risks are detected during process execution. The approach has been realized via a distributed, sensor-based architecture. At design-time, sensors are defined to specify risk conditions which when fulfilled, are a likely indicator of negative process states (faults) to eventuate. Both historical and current process execution data can be used to compose such conditions. At run-time, each sensor independently notifies a sensor manager when a risk is detected. In turn, the sensor manager interacts with the monitoring component of a business process management system to prompt the results to process administrators who may take remedial actions. The proposed architecture has been implemented on top of the YAWL system, and evaluated through performance measurements and usability tests with students. The results show that risk conditions can be computed efficiently and that the approach is perceived as useful by the participants in the tests.
86|12|http://www.sciencedirect.com/science/journal/01641212/86/12|Introduction to the JSS special issue of Web 2.0 engineering: New practices and emerging Challenges|
86|12||Performance improvement of web caching in Web 2.0 via knowledge discovery|Web 2.0 systems are more unpredictable and customizable than traditional web applications. This causes that performance techniques, such as web caching, limit their improvements. Our study was based on the hypotheses that the use of web caching in Web 2.0 applications, particularly in content aggregation systems, can be improved by adapting the content fragment designs. We proposed to base this adaptation on the analysis of the characterization parameters of the content elements and on the creation of a classification algorithm. This algorithm was deployed with decision trees, created in an off-line knowledge discovery process. We also defined a framework to create and adapt fragments of the web documents to reduce the user-perceived latency in web caches. The experiment results showed that our solution had a remarkable reduction in the user-perceived latency even losses in the cache hit ratios and in the overhead generated on the system, in comparison with other web cache schemes.
86|12||Re-engineering legacy Web applications into RIAs by aligning modernization requirements, patterns and RIA features|Rich Internet Applications (RIAs) have become a common platform for Web developments. Its adoption has been accelerated thanks to different factors, among others, the appearance of patterns for typical RIA behaviors and the extension of different Model Driven Web Engineering methodologies to introduce RIA concepts. The real fact is that more and more developers are switching to RIA technologies and, thus, the modernization of legacy Web applications into RIAs has become a trend topic. However, this modernization process lacks of a systematic approach. Currently, it is done in an ad hoc manner, being expensive and error-prone. This work presents a systematic process to modernize legacy Web applications into RIAs. The process is based on the use of traceability matrices that relate modernization requirements, RIA features and patterns. Performing some operations on these matrices, they provide the analyst with the necessary information about the suitability of a pattern or set of patterns to address a given requirement. This work also introduces two measures, the degree of requirement realization and the degree of pattern realization, which are used to discuss the pattern selection. Finally, the applicability of the approach is evaluated by using it in several Web systems.
86|12||Modeling users on the World Wide Web based on cognitive factors, navigation behavior and clustering techniques|This paper focuses on modeling users’ cognitive styles based on a set of Web usage mining techniques on user navigation patterns and clickstream data. Main aim is to investigate whether specific clustering techniques can group users of particular cognitive style using measures obtained from psychometric tests and content navigation behavior. Three navigation metrics are proposed and utilized to find identifiable groups of users that have similar navigation patterns in relation to their cognitive style. The proposed work has been evaluated with two user studies which entail a psychometric-based survey for extracting the users’ cognitive styles, combined with a real usage scenario of users navigating in a controlled Web 2.0 environment. A total of 106 participants of age between 17 and 25 participated in the study providing interesting insights with respect to cognitive styles and navigation behavior of users. Studies like the reported one can be useful for modeling users and assist adaptive Web 2.0 environments to organize and present information and functionalities in an adaptive format to diverse user groups.
86|12||A model-driven approach to develop high performance web applications|The evolution of web technologies in the last few years has contributed to the improvement of web applications, and with the appearance of AJAX and Web 2.0 technology, a new breed of applications for the Internet has emerged: widgets, gadgets and mashups are some examples of this trend. However, as web applications become more and more complex, development costs are increasing in an exponential rate, and we believe that considering a software engineering methodology in the development process of this type of applications, contributes to the solution of this problem. In order to solve this question, this paper proposes a model-driven architecture to support web application development from the design to the implementation model. With this aim, the following tasks have been performed: first a new profile extends UML with new concepts extracted from the web domain, then a new framework supports web application development by composing heterogeneous web elements, and finally, a transformation model generates web applications from the UML extension proposed. The main contribution of this work is a cost and complexity reduction due to the incorporation of a model-driven architecture into the web application development process, but other advantages that can be mentioned are a high performance degree achieved by a prefetching cache mechanism, and a high reusability, since web elements can be reused in different web applications.
86|12||Detecting Web requirements conflicts and inconsistencies under a model-based perspective|Web requirements engineering is an essential phase in the software project life cycle for the project results. This phase covers different activities and tasks that in many situations, depending on the analyst's experience or intuition, help getting accurate specifications. One of these tasks is the conciliation of requirements in projects with different groups of users. This article presents an approach for the systematic conciliation of requirements in big projects dealing with a model-based approach. The article presents a possible implementation of the approach in the context of the NDT (Navigational Development Techniques) Methodology and shows the empirical evaluation in a real project by analysing the improvements obtained with our approach. The paper presents interesting results that demonstrate that we can get a reduction in the time required to find conflicts between requirements, which implies a reduction in the global development costs.
86|12||Evaluating the perceived and estimated quality in use of Web 2.0 applications|Web 2.0 refers to a new generation of web applications where individuals are able to participate, collaborate, and share created artefacts. Despite the fact that Web 2.0 applications are widely used for both educational and professional purposes, a consolidated methodology for their evaluation is still not available. This paper presents and discusses the results of two empirical studies on the case of mind mapping and diagramming Web 2.0 applications. Both studies employed logging actual use method to measure the estimated quality in use, while the retrospective thinking aloud method and an online questionnaire were applied to assess the perceived quality in use. Achieved analytical results showed that the results of the estimated and the perceived quality in use match partially, which indicates that quality in use should be measured with both subjective and objective instruments. The work presented in this paper is the first step towards a comprehensive methodology for evaluating the quality in use of Web 2.0 applications. Consequently, the usage of the proposed quality in use model for other types of Web 2.0 applications as well as contexts of use needs to be investigated in order to draw generalizable conclusions.
86|12||Reliability guaranteed energy-aware frame-based task set execution strategy for hard real-time systems|In this paper, we study the problem of how to execute a real-time frame-based task set on DVFS-enabled processors so that the system's reliability can be guaranteed and the energy consumption of executing the task set is minimized. To ensure the reliability requirement, processing resources are reserved to re-execute tasks when transient faults occur. However, different from commonly used approaches that the reserved processing resources are shared by all tasks in the task set, we judiciously select a subset of tasks to share these reserved resources for recovery purposes. In addition, we formally prove that for a give task set, the system achieves the highest reliability and consumes the least amount of energy when the task set is executed with a uniform frequency (or neighboring frequencies if the desired frequency is not available). Based on the theoretic conclusion, rather than heuristically searching for appropriate execution frequency for each individual task as used in many research work, we directly calculate the optimal execution frequency for the task set. Our simulation results have shown that with our approach, we can not only guarantee the same level of system reliability, but also have up to 15% energy saving improvement over other fault recovery-based approaches existed in the literature. Furthermore, as our approach does not require frequent frequency changes, it works particularly effective on processors where frequency switching overhead is large and not negligible.
86|12||What are the roles of software product managers? An empirical investigation|Software product management covers both technical and business activities to management of products like roadmaps, strategic, tactical, and release planning. In practice, one product manager is seldom responsible for all these activities but several persons share the responsibilities. Therefore, it is important to understand the boundaries of product managers’ work in managing software products, as well as the impact a product manager has on the company business. The purpose of the study is to clarify what roles of software product managers exist and understand how these roles are interrelated with each other and the whole structure and business of an organization. The study is designed as an interpretative qualitative study using grounded theory as the research method. Based on the gathered data we developed a framework that reveals the role of a product manager in the organization and shows how this role can evolve by extending the level of responsibilities. Using the framework, we identified four stereotypical roles of product managers in the studied organizations: experts, strategists, leaders, and problem solvers. The presented framework shows that product managers’ roles are not limited to the conception of the “mini-CEO.” The results allow product managers and top management to collaborate effectively by assigning responsibilities and managing expectations by having a common tool for understanding the role of product managers in the organization.
86|12||A decision support framework for metrics selection in goal-based measurement programs: GQM-DSFMS|Software organizations face challenges in managing and sustaining their measurement programs over time. The complexity of measurement programs increase with exploding number of goals and metrics to collect. At the same time, organizations usually have limited budget and resources for metrics collection. It has been recognized for quite a while that there is the need for prioritizing goals, which then ought to drive the selection of metrics. On the other hand, the dynamic nature of the organizations requires measurement programs to adapt to the changes in the stakeholders, their goals, information needs and priorities. Therefore, it is crucial for organizations to use structured approaches that provide transparency, traceability and guidance in choosing an optimum set of metrics that would address the highest priority information needs considering limited resources. This paper proposes a decision support framework for metrics selection (DSFMS) which is built upon the widely used Goal Question Metric (GQM) approach. The core of the framework includes an iterative goal-based metrics selection process incorporating decision making mechanisms in metrics selection, a pre-defined Attributes/Metrics Repository, and a Traceability Model among GQM elements. We also discuss alternative prioritization and optimization techniques for organizations to tailor the framework according to their needs. The evaluation of the GQM-DSFMS framework was done through a case study in a CMMI Level 3 software company.
86|12||A language-independent approach to black-box testing using Erlang as test specification language|Integration of reused, well-designed components and subsystems is a common practice in software development. Hence, testing integration interfaces is a key activity, and a whole range of technical challenges arise from the complexity and versatility of such components.
86|12||Domain-Specific Modeling Languages to improve framework instantiation|Frameworks are reusable software composed of concrete and abstract classes that implement the functionality of a domain. Applications reuse frameworks to enhance quality and development efficiency. However, frameworks are hard to learn and reuse. Application developers must understand the complex class hierarchy of the framework to instantiate it properly. In this paper, we present an approach to build a Domain-Specific Modeling Language (DSML) of a framework and use it to facilitate framework reuse during application development. The DSML of a framework is built by identifying the features of this framework and the information required to instantiate them. Application generators transform models created with the DSML into application code, hiding framework complexities. In this paper, we illustrate the use of our approach in a framework for the domain of business resource transactions and a experiment that evaluated the efficiency obtained with our approach.
86|12||The presence and development of competency in IT programs|Information technology (IT) programs are coordinated IT projects with a common business objective or underlying similar theme. Driving success in an IT program requires that the projects all work to achieve more global organizational goals than those of each individual project. These goals are better achieved in the presence of critical program team competences that include personnel development, dissemination of methodologies, and a key customer focus. These competences need to be developed to promote higher program performance where programs are dedicated to achieving business objectives of an organization. We propose a model based on the human resource model that considers the development of the critical competences when essential self and social competences are present in team members. Participation mechanisms of interpersonal cooperation and mutual support assist in the development of the critical competences. The model is supported by data collected from both quantitative survey and qualitative interviews with matched pairs of IT program managers and IT project managers. The results confirm the need to insure the presence of certain competences in team members and the construction of an environment that builds mutual support and cooperation. The human resource model is thus extended to include the inter-team environment of IT programs and further variables important to vendor competence.
86|12||Effective scheduling strategies for boosting performance on rule-based spam filtering frameworks|Despite the enormous importance of e-mail to current worldwide communication, the increase of spam deliveries has had a significant adverse effect for all its users. In order to adequately fight spam, both the filtering industry and scientific community have developed and deployed the fastest and most accurate filtering techniques. However, the increasing volume of new incoming messages needing classification together with the lack of adequate support for anti-spam services on the cloud, make filtering efficiency an absolute necessity. In this context, and given the extensive utilization and increasing significance of rule-based filtering frameworks for the anti-spam domain, this work studies and analyses the importance of both existing and novel scheduling strategies to make the most of currently available anti-spam filtering techniques. Results obtained from the experiments demonstrated that some scheduling alternatives resulted in time savings of up to 26% for filtering messages, while maintaining the same classification accuracy.
86|12||A study of cyclic dependencies on defect profile of software components|Empirical evidence shows that dependency cycles among software components are pervasive in real-life software systems, although such cycles are known to be detrimental to software quality attributes such as understandability, testability, reusability, build-ability and maintainability.
86|12||Chaos-based selective encryption for H.264/AVC|Encryption techniques are usually employed to maintain the secrecy of the video streams transmitted via a public network. However, full encryption using strong cryptographic algorithms is usually not necessary if the purpose is to destroy the commercial value by preventing pleasant viewing. For this purpose, selective encryption is preferred as its operating efficiency is higher. Here, a chaos-based selective encryption scheme implemented on the H.264/AVC standard is proposed. The scheme employs multiple Rényi chaotic maps to generate a pseudorandom bit sequence which is used to mask the selected H.264/AVC syntax elements. It provides sufficient protection against full reconstruction while keeping the format compliance property so as not to cause decoding error without the key. The operating efficiency is high due to the low computational complexity of the Rényi chaotic map, as justified by the simulation results using video clips at various resolutions. Moreover, the security analyses show that the proposed algorithm is highly sensitive to the secret key and possesses good perceptual security.
86|12||Software generated device exception for more intensive device-related software testing: An industrial field study|It is more important to properly handle exceptions, than to prevent exceptions from occurring, because they arise from so many different causes. In embedded systems, a vast number of exceptions are caused by hardware devices. In such cases, numerous software components are involved in these hardware device-originated exceptions, ranging from the device itself to the device driver, the kernel, and applications. Therefore, it takes a lot of time to debug software that fails to handle exceptions. This paper proposes a lightweight device exception testing method, and a related automation tool, AMOS v3.0. The proposed method artificially triggers more realistic device exceptions in runtime, and monitors how software components handle exceptions in detail. AMOS v3.0 has been applied to the exception testing of car-infotainment systems in an automobile company. The results based on this industrial field study have revealed that 39.13% of the failures in exception handling were caused by applications, 36.23% of the failures were caused by device drivers, and 24.64% were derived from the kernel. We conclude that the proposed method is highly effective, in that it can allow developers to identify the root cause of failure for exception handling.
86|2|http://www.sciencedirect.com/science/journal/01641212/86/2|Matthew effect, ABC analysis and project management of scale-free information systems|The Matthew effect widely exists in natural and artificial systems. This paper studies the Matthew effect emerging from large information systems by investigating their topological complexity. The Matthew effect of the topological complexity reveals scale-free behavior that can be characterized by the general scale-free model [Physics Letters A 303 (2002) 337–344]. The poor–rich demarcation of the Matthew effect is analyzed to classify modules of information systems based on ABC analysis that is consistent with the ABC classification of human resources. Thus the Matthew effect is applied to envisage the critical fusion of high performance and low cost in software engineering. Finally, the Matthew effect of two typical open-source software systems written in Java and C/C++ languages is also investigated.
86|2||A robust blind color image watermarking in quaternion Fourier transform domain|Most of the existing color image watermarking schemes were designed to mark the image luminance component only, which have some disadvantages: (i) they are sensitive to color attacks because of ignoring the correlation between different color channels, (ii) they are always not robust to geometric distortions for neglecting the watermark desynchronization. It is a challenging work to design a robust color image watermarking scheme. Based on quaternion Fourier transform and least squares support vector machine (LS-SVM), we propose a robust blind color image watermarking in quaternion Fourier transform domain, which has good visual quality. Firstly, the original color image is divided into color image blocks. Then, the fast quaternion Fourier transform is performed on the color image block. Finally, the digital watermark is embedded into original color image by adaptively modulating the real quaternion Fourier transform coefficients of color image block. For watermark decoding, the LS-SVM correction with pseudo-Zernike moments is utilized. Experimental results show that the proposed color image watermarking is not only robust against common image processing operations such as filtering, JPEG compression, histogram equalization, and image blurring, but also robust against the geometrical distortions.
86|2||Efficient support of dynamic inheritance for class- and prototype-based languages|Dynamically typed languages are becoming increasingly popular for different software development scenarios where runtime adaptability is important. Therefore, existing class-based platforms such as Java and .Net have been gradually incorporating dynamic features to support the execution of these languages. The implementations of dynamic languages on these platforms commonly generate an extra layer of software over the virtual machine, which reproduces the reflective prototype-based object model provided by most dynamic languages. Simulating this model frequently involves a runtime performance penalty, and makes the interoperation between class- and prototype-based languages difficult.
86|2||3E: Energy-efficient elastic scheduling for independent tasks in heterogeneous computing systems|Reducing energy consumption is a major design constraint for modern heterogeneous computing systems to minimize electricity cost, improve system reliability and protect environment. Conventional energy-efficient scheduling strategies developed on these systems do not sufficiently exploit the system elasticity and adaptability for maximum energy savings, and do not simultaneously take account of user expected finish time. In this paper, we develop a novel scheduling strategy named energy-efficient elastic (3E) scheduling for aperiodic, independent and non-real-time tasks with user expected finish times on DVFS-enabled heterogeneous computing systems. The 3E strategy adjusts processors’ supply voltages and frequencies according to the system workload, and makes trade-offs between energy consumption and user expected finish times. Compared with other energy-efficient strategies, 3E significantly improves the scheduling quality and effectively enhances the system elasticity.
86|2||Histogram-shifting-imitated reversible data hiding|This paper proposes a novel reversible data hiding scheme based on the histogram-shifting-imitated approach. Instead of utilizing the peak point of an image histogram, the proposed scheme manipulates the peak points of segments based on image intensity. The secret data can be embedded into the cover image by changing the peak point pixel value into other pixel value in the same segment. The proposed method uses a location map to guarantee the correct extraction of the secret data. Since the modification of the pixel value is limited within each segment, the quality of the stego image is only related to the size of the segmentation, which means after embedding data into the cover image, it can be reused to do the multi-layer data embedding while maintaining the high quality of the final stego image. The experimental results of comparison with other existing schemes demonstrate the performance of the proposed scheme is superior to the others.
86|2||A covert communication method via spreadsheets by secret sharing with a self-authentication capability|A new covert communication method with a self-authentication capability for secret data hiding in spreadsheets using the information sharing technique is proposed. At the sender site, a secret message is transformed into shares by Shamir's (k, n)-threshold secret sharing scheme with n = k + 1, and the generated k + 1 shares are embedded into the number items in a spreadsheet as if they are part of the spreadsheet content. And at the receiver site, every k shares among the k + 1 ones then are extracted from the stego-spreadsheet to recover k + 1 copies of the secret, and the consistency of the k + 1 copies in value is checked to determine whether the embedded shares are intact or not, achieving a new type of blind self-authentication of the embedded secret. By dividing the secret message into segments and applying to each segment the secret sharing scheme, the integrity and fidelity of the hidden secret message can be verified, achieving a covert communication process with the double functions of information hiding and self-authentication. Experimental results and discussions on data embedding capacity, authentication precision, and steganalysis issues are also included to show the feasibility of the proposed method.
86|2||A zero-watermark scheme with geometrical invariants using SVM and PSO against geometrical attacks for image protection|This paper proposes a zero-watermark scheme with geometrical invariants using support vector machine (SVM) classifier against geometrical attacks for image authentication. Here geometrical attacks merely address rotation, scale, and translation (RST) operations on images. The proposed scheme is called the SVM-based zero-watermark (SZW) scheme hereafter. The SZW method makes no changes to original images while embedding the owner signature of images so as to achieve high transparency. Moreover, in order to promote the robustness to RST operations, it integrates the discrete Fourier transform (DFT) with the log-polar mapping (LPM) for finding out RST invariants of images. The SZW method then generates the secret key for a host image via performing a logical operation exclusive disjunction, an exclusive-or (XOR) operation, on the original watermark and a set of the characteristics of the RST invariants of the host image. Subsequently, a trained SVM (TSVM) is regarded as a mapping so that it can memorize the relationships between the set of characteristics of RST invariants and the secret key. During the watermark-extraction process of the SZW method, the TSVM is first fed with the set of characteristics of RST invariants of the watermarked image to get the estimated secret key. The SZW method then extracts the estimated watermark by performing the XOR operation on the set of characteristics of RST invariants and the estimated secret key. Consequently, the SZW method requires no original image while retrieving watermarks. In the paper, the particle swarm optimization (PSO) algorithm is also employed to search for a set of nearly optimal parameters of the SVM. Finally, the experimental results show that, in average, the SZW method outperforms other existing methods against RST attacks under consideration here.
86|2||Layer assessment of object-oriented software: A metric facilitating white-box reuse|Software reuse has the potential to shorten delivery times, improve quality and reduce development costs. However software reuse has been proven challenging for most organizations. The challenges involve both organizational and technical issues. In this work we concentrate on the technical issues and we propose a new metric facilitating the reuse of object-oriented software based on the popular Chidamber and Kemerer suite for object-oriented design. We derive this new metric using linear regression on a number of OSS java projects. We compare and contrast this new metric with three other metrics proposed in the literature. The purpose of the proposed metric is to assist a software developer during the development of a software system in achieving reusability of classes considered important for future reuse and also in providing assistance during re-architecting and componentization activities of existing systems.
86|2||A high performance inter-domain communication approach for virtual machines|In virtualization technology field, researches mainly focus on strengthening the isolation barrier between virtual machines (VMs) that are co-resident within a single physical machine. At the same time, there are many kinds of distributed communication-intensive applications such as web services, transaction processing, graphics rendering and high performance grid applications, which need to communicate with other virtual machines at the same platform. Unfortunately, current inter-VM communication method cannot adequately satisfy the requirement of such applications. In this paper, we present the design and implementation of a high performance inter-VM communication method called IVCOM based on Xen virtual machine environment. In para-virtualization, IVCOM achieves high performance by bypassing some protocol stacks and privileged domain, shunning page flipping and providing a direct and high-performance communication path between VMs residing in the same physical machine. But in full-virtualization, IVCOM applies a direct communication channel between domain 0 and Hardware Virtualization based VM (HV2M) and can greatly reduce the VM entry/exit operations, which has improved the HV2M performance. In the evaluation of para-virtualization consisting of a few of benchmarks, we observe that IVCOM can reduce the inter-VM round trip latency by 70% and increase throughput by up to 3 times, which prove the efficiency of IVCOM in para-virtualized environment. In the full-virtualized one, IVCOM can reduce 90% VMX transition operations in the communication between domain 0 and HV2M.
86|2||Effective pattern-driven concurrency bug detection for operating systems|As multi-core hardware has become more popular, concurrent programming is being more widely adopted in software. In particular, operating systems such as Linux utilize multi-threaded techniques heavily to enhance performance. However, current analysis techniques and tools for validating concurrent programs often fail to detect concurrency bugs in operating systems (OSes) due to the complex characteristics of OSes. To detect concurrency bugs in OSes in a practical manner, we have developed the COncurrency Bug dETector (COBET) framework based on composite bug patterns augmented with semantic conditions. The effectiveness, efficiency, and applicability of COBET were demonstrated by detecting 10 new bugs in file systems, device drivers, and network modules of Linux 2.6.30.4 as confirmed by the Linux maintainers.
86|2||A novel VQ-based reversible data hiding scheme by using hybrid encoding strategies|Reversible data hiding is a special algorithm that not only guarantees the confidential data will be extracted accurately but also allows the original cover image to be reconstructed without distortion after the confidential data are completely extracted. This paper proposes a new index compression and reversible data hiding scheme based on side-match vector quantization (SMVQ) and search order coding (SOC). In this proposed scheme, the confidential data are embedded into the transformed index table of a cover image. During the extracting phase, simple steps are employed to extract the confidential data and reconstruct the original cover image. The experimental results show that with a codebook size of 256, the average compression rate of the proposed scheme is 0.325 bpp, which is superior to that of the methods proposed by Chen and Huang (0.426 bpp) and Chang et al. (0.429 bpp). Additionally, the embedding and extracting times of our scheme are 5.491 s and 0.352 s, respectively, demonstrating that the execution time of the proposed scheme is much faster than that of the methods of Chen and Huang and Chang et al. Moreover, our scheme achieves better performance than other selected reversible data hiding schemes with respect to the data embedding and data compression rates.
86|2||On using adversary simulators to evaluate global fixed-priority and FPZL scheduling of multiprocessors|There are many real-time systems where it is useful to have an estimate for the worst-case response time of each task. Simulators can be used to establish a lower bound on the worst-case response time. But classic simulators apply arrival patterns originally conceived for uniprocessor and fail to achieve a good estimate for the worst-case response time when multiprocessors are used. An adversary simulator generates arrival patterns to stress the processing capacity of the system and, in this way, to obtain tighter estimates. In this paper we present a new heuristic for adversary simulators specifically designed for fixed-priority zero-laxity (FPZL) scheduling. This new adversary algorithm is simple and fast, and it works with both deadline monotonic (DMPO) and deadline minus computation monotonic (DCMPO) priority assignment policies. The evaluation shows that the adversary simulator proposed in this paper is more effective when FPZL scheduling is used. We also compare four scheduling approaches (FP-DMPO, FP-DCMPO, FPZL-DMPO and FPZL-DCMPO) using an appropriate adversary simulator for each one.
86|2||Constraint-based specification of model transformations|Model transformations are a central element of model-driven development (MDD) approaches. The correctness, modularity and flexibility of model transformations is critical to their effective use in practical software development. In this paper we describe an approach for the automated derivation of correct-by-construction transformation implementations from high-level specifications. We illustrate this approach on a range of model transformation case studies of different kinds (re-expression, refinement, quality improvement and abstraction transformations) and describe ways in which transformations can be composed and evolved using this approach.
86|2||SF-PMIPv6: A secure fast handover mechanism for Proxy Mobile IPv6 networks|An efficient mobility management mechanism is one of the major challenges for ubiquitous computing. Recently, the IETF NETLMM working group proposed Proxy Mobile IPv6 (PMIPv6), a network-based localized mobility management protocol to support mobility management without the participation of mobile nodes (MNs) in any mobility-related signaling. Unfortunately, PMIPv6 still suffers from high packet losses and long authentication latency during handover. To address these issues, we propose a secure authentication mechanism and fast handover scheme called SF-PMIPv6 for PMIPv6 networks. The scheme provides low handover latency, supports local authentication procedures, resolves the packet loss problem, and deals with out-of-sequence packets. Moreover, SF-PMIPv6 is a robust authentication scheme that resists various attacks. Our simulation results demonstrate that it provides a better solution than existing schemes.
86|2||Design of component-based real-time applications|This paper presents the key aspects of a model-based methodology that is proposed for the design of component-based applications with hard real-time requirements. The methodology relies on RT-CCM (Real-time Container Component Model), a component technology aimed to make the timing behaviour of the applications predictable and inspired in the Lightweight CCM specification of the OMG. Some new mechanisms have been introduced in the underlying framework that make it possible to schedule the execution of code and the transmission of messages of an application while guaranteeing that the application will meet its timing requirements when executed. The added mechanisms also enable the application designer to configure this scheduling without interfering with the opacity typically required in component management. Moreover, the methodology includes a process for generating the real-time model of a component-based application as a composition of the reusable real-time models of the components that form it. From the analysis of this model the application designer obtains the configuration values that must be applied to the component instances and the elements of the framework in order to make the application fulfil its timing requirements.
86|2||A reliability optimization method for RAID-structured storage systems based on active data migration|The reliability of RAID system can utilize reconstruction operations to recover data when the disk fails; however, it will result in longer recovery time and negative impact on system performances. Moreover, the possibility of secondary failure will be increased during the recovery period. This paper proposed a new reliability optimization method for RAID system with Active data Migration called RAM, which allocates reserved space in every disk and arranges to redirect the data in the non-repairable sectors to the reserved space called Internal-disk Data Migration. When the number of the bad sectors in the disk exceeds threshold or the disk is in poor reliability, the system will copy data of the unreliable disk to a new one called External-disk Data Migration, this process can avoid lengthy data reconstruction. It can dynamically adjust migration speed to reduce the impact on the front end performance. When the I/O load of system is bursting, the I/O requests from user have high-priority. The overall results indicate that the RAM can improve reliability of the system with little influence.
86|2||Invertible secret image sharing for gray level and dithered cover images|Secret image sharing approaches have been extended to obtain covers from stego images after the revealing procedure. Lin et al.’s work in 2009 uses modulus operator to decrease the share image distortion while providing recovery of original covers. Their work use gray level or binary image as cover. Stego images have approximately 43 dB and 38 dB PSNR for gray level and binary covers respectively. Lin et al.’s work in 2010 provides enhanced embedding capacity but does not support binary covers. Gray level covers’ PSNR is reported approximately 40 dB. The proposed method enhances the visual quality of stego images regardless of intensity range of covers. Exploiting Modification Direction method is used to hide the shared values into covers. The method also utilizes modulus operator to recover original cover pixels. Stego image PSNR is approximately 47 dB for gray level covers. The method provides 4–7 dB increase respectively on the stego image quality compared to others. Stego images have also higher PSNR (43 dB) for dithered covers. The proposed method generates stego images with higher PSNR regardless of the intensity range of the cover image.
86|2||Building ubiquitous computing applications using the VERSAG adaptive agent framework|In this article, we describe a novel approach to build ubiquitous computing applications using adaptive software agents. Towards this, we propose VERSAG, a novel agent framework and architecture which combines agent mobility with the ability to dynamically change the internal structure and capabilities of agents and leads to highly versatile and lightweight software agents. We describe the framework in depth and provide design and implementation details of our prototype implementation. A case study scenario is used to illustrate the functional benefits achievable through the use of this framework in ubiquitous computing environments. Further experimental evaluation confirms the efficiency and feasibility of the VERSAG framework which outperforms traditional mobile agents, and also demonstrates applicability of the proposed framework to agent based systems where varying capabilities are required by agents over their lifecycle.
86|2||Promoting cooperation in service-oriented MAS through social plasticity and incentives|In distributed environments where entities only have a partial view of the system, cooperation plays a key issue. In the case of decentralized service discovery in open service-oriented multi-agent systems, agents only know about the services they provide and their direct neighbors. Therefore, they need the cooperation of their neighbors in order to locate the required services. However, cooperation is not always present in open and distributed systems. Non-cooperative agents pursuing their own goals could refuse to forward queries from other agents to avoid the cost of this action; therefore, the efficiency of the decentralized service discovery could be seriously damaged. In this paper, we propose the combination of local structural changes and incentives in order to promote cooperation in the service discovery process. The results show that, even in scenarios where the predominant behavior is not collaborative the cooperation emerges.
86|2||COTS integration and estimation for ERP|This paper presents a comprehensive set of effort and schedule estimating models for predicting Enterprise Resource Planning (ERP) implementations, available in the open literature. The first set of models uses product size to predict ERP software engineering effort as well as total integration effort. Product size is measured in terms of the number of report, interface, conversion, and extension (RICE) objects configured and customized within the commercial ERP tool. Total integration effort captures software engineering plus systems engineering, program management, change management, development test & evaluation, and training development. The second set of models predicts the duration of ERP implementation stages in terms of RICE objects, staffing, and the number of test cases. The statistical models are based on data collected from 20 programs implemented within the federal government over the course of nine years beginning in 2000. The data was collected during the time period from 2006 to 2010. The models focus on the vendor's implementation team, and therefore should be applicable to commercial ERP implementations. Finally, ERP adopters/customers can use these models to validate Vendor's Implementation Team cost proposals or estimates.
86|2||A posteriori operation detection in evolving software models|As every software artifact, also software models are subject to continuous evolution. The operations applied between two successive versions of a model are crucial for understanding its evolution. Generic approaches for detecting operations a posteriori identify atomic operations, but neglect composite operations, such as refactorings, which leads to cluttered difference reports.
86|2||Efficient reversible data hiding algorithm based on gradient-based edge direction prediction|In this paper, we present an efficient RDH algorithm based on a new gradient-based edge direction prediction (GEDP) scheme. Since the proposed GEDP scheme can generate more accurate prediction results, the prediction errors tend to form a sharper Laplacian distribution. Therefore, the proposed algorithm can guarantee larger embedding capacity and produce better quality of marked images. The determination of appropriate thresholds is also a critical issue for a RDH algorithm, so we design a new systematic way to tackle this problem. In addition, a modified embedding order determination strategy is presented to reduce the distortion of a marked image. Based on typical test images, experimental results demonstrate the superior properties of the proposed algorithm in terms of embedding capacity and marked image quality.
86|2||Image sharing method for gray-level images|In 1994, Naor and Shamir firstly proposed the concept of visual secret sharing. By using a codebook to encode a binary image into sharing images, nobody can obtain the original information from any one of the shared images unless superimposing all shared images. Although the above method can protect the security of the binary image, pixel expansion and lossy recovery are two unsolved problem. To improve the disadvantages mentioned above, a new image sharing method is proposed in this paper. The proposed method firstly use linear equations of Hill cipher to divide an image into several sub-images. Then the concept of the random grid is applied to the sub-images and to construct the shared images. Experimental result shows that the proposed scheme can effectively improve the above drawbacks.
86|3|http://www.sciencedirect.com/science/journal/01641212/86/3|A mapping study to investigate component-based software system metrics|A component-based software system (CBSS) is a software system that is developed by integrating components that have been deployed independently. In the last few years, many researchers have proposed metrics to evaluate CBSS attributes. However, the practical use of these metrics can be difficult. For example, some of the metrics have concepts that either overlap or are not well defined, which could hinder their implementation. The aim of this study is to understand, classify and analyze existing research in component-based metrics, focusing on approaches and elements that are used to evaluate the quality of CBSS and its components from a component consumer's point of view. This paper presents a systematic mapping study of several metrics that were proposed to measure the quality of CBSS and its components. We found 17 proposals that could be applied to evaluate CBSSs, while 14 proposals could be applied to evaluate individual components in isolation. Various elements of the software components that were measured are reviewed and discussed. Only a few of the proposed metrics are soundly defined. The quality assessment of the primary studies detected many limitations and suggested guidelines for possibilities for improving and increasing the acceptance of metrics. However, it remains a challenge to characterize and evaluate a CBSS and its components quantitatively. For this reason, much effort must be made to achieve a better evaluation approach in the future.
86|3||An improved DCT-based perturbation scheme for high capacity data hiding in H.264/AVC intra frames|Recently, Ma et al. proposed an efficient error propagation-free discrete cosine transform-based (DCT-based) data hiding algorithm that embeds data in H.264/AVC intra frames. In their algorithm, only 46% of the 4 × 4 luma blocks can be used to embed hidden bits. In this paper, we propose an improved error propagation-free DCT-based perturbation scheme that fully exploits the remaining 54% of luma blocks and thereby doubles the data hiding capacity of Ma et al.'s algorithm. Further, in order to preserve the visual quality and increase the embedding capacity of the embedded video sequences, a new set of sifted 4 × 4 luma blocks is considered in the proposed DCT-based perturbation scheme. The results of experiments on twenty-six test video sequences confirm the embedding capacity superiority of the proposed improved algorithm while keeping the similar human visual effect in terms of SSIM (structural similarity) index.
86|3||A sliding window based algorithm for frequent closed itemset mining over data streams|Frequent pattern mining over data streams is an important problem in the context of data mining and knowledge discovery. Mining frequent closed itemsets within sliding window instead of complete set of frequent itemset is very interesting since it needs a limited amount of memory and processing power. Moreover, handling concept change within a compact set of closed patterns is faster. However, it requires flexible and efficient data structures as well as intuitive algorithms. In this paper, we have introduced an effective and efficient algorithm for closed frequent itemset mining over data streams operating in the sliding window model. This algorithm uses a novel data structure for storing transactions of the window and corresponding frequent closed itemsets. Moreover, the support of a new frequent closed itemset is efficiently computed and an old pattern is removed from the monitoring set when it is no longer frequent closed itemset. Extensive experiments on both real and synthetic data streams show that the proposed algorithm is superior to previously devised algorithms in terms of runtime and memory usage.
86|3||Optimization of adaptation plans for a service-oriented architecture with cost, reliability, availability and performance tradeoff|A service-based system may require adaptation for several reasons, such as service evolution (e.g., a new version may be available), hardware volatility (e.g., network quality changes), and varying user demands and new requirements (e.g., a new functionality or a different level of quality of service). Therefore, it is suitable to dynamically adapt a service-based system in an automated manner. However, service adaptations often do not consider software quality attributes and, if they do, they relay on a single attribute in isolation. In this paper, we present an optimization model, which aims to minimize the adaptation costs of a Service-Oriented Architecture (SOA), in correspondence with a certain change scenario (i.e., a set of new requirements) under reliability, availability and performance tradeoff. The model predicts the quality of the new SOA obtained by changing both its structure and behavior. Specifically, it suggests how to replace existing services with available instances and/or adding new services, and how to remove or introduce interaction(s) between existing services and/or new services. We show how our model works on a smartphone mobile application example, and through the sensitivity analysis we highlight its potential to drive architectural decisions.
86|3||Sirius: A heuristic-based framework for measuring web usability adapted to the type of website|The unquestionable relevance of the web in our society has led to an enormous growth of websites offering all kinds of services to users. In this context, while usability is crucial in the development of successful websites, many barely consider the recommendations of experts in order to build usable designs. Including the measurement of usability as part of the development process stands out among these recommendations. One of the most accepted methods for usability evaluation by experts is heuristic evaluation. There is abundant literature on this method. However, there is a lack of clear and specific guidelines to be used in the development and evaluation process. This is probably an important factor contributing to the aforementioned generalized deficiency in web usability.
86|3||Improving feature location using structural similarity and iterative graph mapping|Locating program element(s) relevant to a particular feature is an important step in efficient maintenance of a software system. The existing feature location techniques analyse each feature independently and perform a one-time analysis after being provided an initial input. As a result, these techniques are sensitive to the quality of the input. In this paper, we propose to address the above issues in feature location using an iterative context-aware approach. The underlying intuition is that features are not independent of each other, and the structure of source code resembles the structure of features. The distinguishing characteristics of the proposed approach are: (1) it takes into account the structural similarity between a feature and a program element to determine feature-element relevance and (2) it employs an iterative process to propagate the relevance of the established mappings between a feature and a program element to the neighbouring features and program elements. We evaluate our approach using two different systems, DirectBank, a small-scale industry financial system, and Linux kernel, a large-scale open-source operating system. Our evaluation suggests that the proposed approach is more robust and can significantly increase the recall of feature location with only a minor decrease of precision.
86|3||Optimal univariate microaggregation with data suppression|
86|3||Beyond ATAM: Early architecture evaluation method for large-scale distributed systems|The development of large-scale distributed software systems involves substantial investment and is exposed to a high level of risk. Early architectural decisions define how the system is organised in terms of permanent data management, data communication, data input and output, coarse-grained modularisation and allocation within the organisational structure. Such a system's “back-bone” has been referred to as the System Organisation Pattern. Analysing architecture early in the development life cycle can help identify significant technical risks and mitigate them at a minimal cost. However, architecture assessment methods, such as the Architecture Trade-off Analysis Method, cannot easily be applied very early for architecture defined only conceptually. In addition, the influence of the System Organisation Pattern on the detailed properties of the final system cannot be precisely quantified, which makes applying known architecture analysis methods even more difficult. The Early Architecture Evaluation Method has been developed to assess the System Organisation Pattern much earlier than an ATAM-based assessment would be possible, i.e. in the inception phase of the Rational Unified Process. The method defines an architecture evaluation process, at the heart of which is an assessment model based on the Goal-Question-Metric scheme. The method identifies substantial risks posed by the architectural decisions comprising the System Organisation Pattern. The method has been evaluated on seven real-life examples of large-scale systems.
86|3||SMSCrypto: A lightweight cryptographic framework for secure SMS transmission|Despite the continuous growth in the number of smartphones around the globe, Short Message Service (SMS) still remains as one of the most popular, cheap and accessible ways of exchanging text messages using mobile phones. Nevertheless, the lack of security in SMS prevents its wide usage in sensitive contexts such as banking and health-related applications. Aiming to tackle this issue, this paper presents SMSCrypto, a framework for securing SMS-based communications in mobile phones. SMSCrypto encloses a tailored selection of lightweight cryptographic algorithms and protocols, providing encryption, authentication and signature services. The proposed framework is implemented both in Java (target at JVM-enabled platforms) and in C (for constrained SIM Card processors) languages, thus being suitable for a wide range of scenarios. In addition, the signature model adopted does not require an on-line infrastructure and the inherent overhead found in the Public Key Infrastructure (PKI) model, facilitating the development of secure SMS-based applications. We evaluate the proposed framework on a real phone and on SIM Card-comparable microcontroller.
86|3||BotMosaic: Collaborative network watermark for the detection of IRC-based botnets|Recent research has made great strides in the field of detecting botnets. However, botnets of all kinds continue to plague the Internet, as many ISPs and organizations do not deploy these techniques. We aim to mitigate this state by creating a very low-cost method of detecting infected bot host. Our approach is to leverage the botnet detection work carried out by some organizations to easily locate collaborating bots elsewhere.
86|3||A reversible data hiding method by histogram shifting in high quality medical images|Enormous demands for recognizing complicated anatomical structures in medical images have been demanded on high quality of medical image such as each pixel expressed by 16-bit depth. Now, most of data hiding algorithms are still applied in 8-bit depth medical images. We proposed a histogram shifting method for image reversible data hiding testing on high bit depth medical images. Among image local block pixels, we exploit the high correlation for smooth surface of anatomical structure in medical images. Thus, we apply a different value for each block of pixels to produce a difference histogram to embed secret bits. During data embedding stage, the image blocks are divided into two categories due to two corresponding embedding strategies. Via an inverse histogram shifting mechanism, the original image will be accurately recovered after the hidden data extraction. Due to requirements of medical images for data hiding, we proposed six criteria: (1) well-suited for high quality medical images, (2) without salt-and-pepper, (3) applicable to medical image with smooth surface, (4) well-suited sparse histogram of intensity levels, (5) free location map, (6) ability of adjusting data embedding capacity, PSNR and Inter-Slice PSNR. We proposed a data hiding methods satisfying above 6 criteria.
86|3||Efficient Hamming weight-based side-channel cube attacks on PRESENT|The side-channel cube attack (SCCA) is a powerful cryptanalysis technique that combines the side-channel and cube attack. This paper proposes several advanced techniques to improve the Hamming weight-based SCCA (HW-SCCA) on the block cipher PRESENT. The new techniques utilize non-linear equations and an iterative scheme to extract more information from leakage. The new attacks need only 28.95 chosen plaintexts to recover 72 key bits of PRESENT-80 and 29.78 chosen plaintexts to recover 121 key bits of PRESENT-128. To the best of our knowledge, these are the most efficient SCCAs on PRESENT-80/128. To show the feasibility of the proposed techniques, real attacks have been conducted on PRESENT on an 8-bit microcontroller, which are the first SCCAs on PRESENT on a real device. The proposed HW-SCCA can successfully break PRESENT implementations even if they have some countermeasures such as random delay and masking.
86|3||A RAMCloud Storage System based on HDFS: Architecture, implementation and evaluation|Few cloud storage systems can handle random read accesses efficiently. In this paper, we present a RAMCloud Storage System, RCSS, to enable efficient random read accesses in cloud environments. Based on the Hadoop Distributed File System (HDFS), RCSS integrates the available memory resources in an HDFS cluster to form a cloud storage system, which backs up all data on HDFS-managed disks, and fetches data from disks into memory for handy accesses when files are opened for read or specified by users for memory storage. We extend the storage capacity of RCSS to that of the substrate disk-based HDFS by multiplexing all the available memory resources. Furthermore, RCSS supports MapReduce, which is a popular cloud computing paradigm. By serving data from memory instead of disks, RCSS can yield high random I/O performance with low latency and high throughput, and can achieve good availability and scalability as HDFS.
86|3||SQLIA detection and prevention approach for RFID systems|While SQL injection attacks have been plaguing web application systems for years, the possibility of them affecting RFID systems was only identified very recently. However, very little work exists to mitigate this serious security threat to RFID-enabled enterprise systems. At the same time, the drop in RFID tag prices coupled with the increase in storage capacity of the tags have motivated users to store more and more data on the tags for ease of access. This in turn has increased the ability that attackers have of leveraging the tags to try and mount SQLIA based malware attacks on RFID systems thereby increasing the potential threat that RFID-enabled systems pose to the enterprise systems. In this paper, we propose a detection and prevention method from RFID tag-born SQLIA attacks. We have tested all possible types of dynamic queries that may be generated in RFID systems with all possible types of attacks that can be mounted on those systems. We present an analysis and evaluation of the proposed approach to demonstrate its effectiveness in mitigating SQLIA attack.
86|3||Constrained frequent pattern mining on univariate uncertain data|In this paper, we propose a new algorithm called CUP-Miner (Constrained Univariate Uncertain Data Pattern Miner) for mining frequent patterns from univariate uncertain data under user-specified constraints. The discovered frequent patterns are called constrained frequent U2 patterns (where “U2” represents “univariate uncertain”). In univariate uncertain data, each attribute in a transaction is associated with a quantitative interval and a probability density function. The CUP-Miner algorithm is implemented in two phases: In the first phase, a U2P-tree (Univariate Uncertain Pattern tree) is constructed by compressing the target database transactions into a compact tree structure. Then, in the second phase, the constrained frequent U2 pattern is enumerated by traversing the U2P-tree with different strategies that correspond to different types of constraints. The algorithm speeds up the mining process by exploiting five constraint properties: succinctness, anti-monotonicity, monotonicity, convertible anti-monotonicity, and convertible monotonicity. Our experimental results demonstrate that CUP-Miner outperforms the modified CAP algorithm, the modified FIC algorithm, the modified U2P-Miner algorithm, and the modified Apriori algorithm.
86|3||Knowledge discovery of weighted RFM sequential patterns from customer sequence databases|In today's business environment, there is tremendous interest in the mining of interesting patterns for superior decision making. Although many successful customer relationship management (CRM) applications have been developed based on sequential pattern mining techniques, they basically assume that the importance of each customer is the same. Previous studies in CRM show that not all customers make the same contribution to a business, and it is indispensible to evaluate customer value before developing effective marketing strategies. Therefore, this study includes the concepts of recency, frequency, and monetary (RFM) analysis in the sequential pattern mining process. For a given subsequence, each customer sequence contributes its own recency, frequency, and monetary scores to represent customer importance. An efficient algorithm is developed to discover sequential patterns with high recency, frequency, and monetary scores. Empirical results show that the proposed method is efficient and can effectively discover more valuable patterns than conventional frequent pattern mining.
86|3||A delay-constrained and priority-aware channel assignment algorithm for efficient multicast in wireless mesh networks|Many popular applications of wireless mesh networks (WMNs) depend on delay-constraint multicast communication. To support such multicast communication, this paper proposes a distributed and polynomial-time heuristic channel assignment algorithm for WMNs. The proposed algorithm considers that the multicast session requests arrive dynamically and have different priorities. When a delay-constrained multicast session is issued, the multicast tree corresponding to the session is first established. The proposed algorithm divides the path delay constraint of the multicast tree into a number of the node-based delay constraints. This algorithm also devises multiple channel selection criteria to exploit all available channels of the WMN. Using these selection criteria, each node on the multicast tree can select the best channel to meet its node delay constraint and minimize the total interference for all existing multicast sessions. In the interference minimization, the priority factor is taken into account to prevent high-priority multicast sessions from incurring more interference than low-priority multicast sessions. Finally, this paper performs simulations to demonstrate the effectiveness of the proposed heuristic channel assignment algorithm through comparison with the optimal solution.
86|3||A computer virus spreading model based on resource limitations and interaction costs|Computer viruses are major threats to Internet security and privacy, therefore many researchers are addressing questions linked to virus propagation properties, spreading models, epidemic dynamics, tipping points, and control strategies. We believe that two important factors – resource limitations and costs – are being overlooked in this area due to an overemphasis on power-law connectivity distributions of scale-free networks affecting computer virus epidemic dynamics and tipping points. The study show (a) a significant epidemic tipping point does exists when resource limitations and costs are considered, with the tipping point exhibiting a lower bound; (b) when interaction costs increase or usable resources decrease, epidemic tipping points in scale-free networks grow linearly while density curves decrease linearly; (c) regardless of whether Internet user resources obey delta, uniform, or normal distributions, they retain the same epidemic dynamics and tipping points as long as the average value of those resources remains unchanged across different scale-free networks; (d) it is possible to control the spread of a computer virus in a scale-free network if resources are restricted and if costs associated with infection events are significantly increased through the use of a throttling strategy.
86|3||Securing color information of an image by concealing the color palette|This paper deals with a method to protect the color information of images by providing free access to the corresponding gray level images. Only with a secret key and the gray level images, it is then possible to view the images in color. The approach is based on a color reordering algorithm after a quantization step. Based on a layer scanning algorithm, the color reordering generates gray level images and makes it possible to embed the color palette into the gray level images using a data hiding algorithm. This work was carried out in the framework of a project aimed at providing limited access to the private digital painting database of the Louvre Museum in Paris, France.
86|3||Triple-image encryption scheme based on one-time key stream generated by chaos and plain images|A triple color image encryption scheme based on chaos is designed. After decomposing the red, green and blue components of the three color images into three grayscale images, recombine them into one color image, then bitwise circularly shift each color pixel for randomly finite number of times. Finally, encrypt the shifted image by three pseudo-random sequences generated by the chaos with dynamical initial conditions, which are generated by the 256-bit long hash value that dependent on the three plain images. Numerical simulation has been performed to test the validity of the scheme, it's suitable for encrypting images in batches.
86|3||Communities of Web service registries: Construction and management|The last few years have seen a democratization in the use of Internet technologies, mainly Web services, for electronic B2B transactions. This has triggered an increase in the number of companies’ Web service registries. In this paper, we propose to use communities as a means to organize Web service registries in such a context. We provide an automatic and implicit approach to create communities of Web service registries using registries’ WSRD descriptions. We also define the needed management operations to ensure the communities consistency during a registry/community life-cycle. Experiments we have made show the feasibility and validity of our community creation approach as well as the specified managing operations.
86|3||AFChecker: Effective model checking for context-aware adaptive applications|Context-aware adaptive applications continually sense and adapt to their changing environments. A large body of such applications relies on user-configured adaptation rules to customize their behavior. We call them rule-based context-aware applications (or RBAs for short). Due to the complexity required for adequately modeling environmental dynamics, adaptation faults are common in these RBAs. One promising approach to detecting such faults is to build a state transition model for an RBA, and exhaustively explore the model's state space. However, it can suffer from numerous false positives. For example, 78.6% of 784 reported faults for one popular RBA – PhoneAdapter, turn out to be false in a real deployment. In this paper, we address this false positive problem by inferring a domain model and an environment model for an RBA. The two models capture the hidden features inside user-configured adaptation rules as well as the RBA's running environment. We formulate these features as deterministic constraints and probabilistic constraints to prune false positives and effectively prioritize remaining faults. Our experiments on two real RBAs report that this approach successfully removes 46.5% of false positives and ranks 86.2% of true positives to the top of the fault list.
86|3||Corrigendum to âA variable-length model for masquerade detectionâ [J. Syst. Softw. 85 (2012) 2470â2478]|
86|4|http://www.sciencedirect.com/science/journal/01641212/86/4|Software Engineering in Brazil: Retrospective and prospective views|
86|4||25 years of software engineering in Brazil: Beyond an insider's view|The software engineering area is facing a growing number of challenges due to the continuing increase in software size and complexity. The challenges are addressed by the very relevant and high quality publications of the Brazilian Symposium on Software Engineering (SBES), in the past 25 editions. This article summarizes the findings from two different mapping studies about these 25 SBES editions. It also reports the results of an expert opinion survey with the most important Brazilian researchers in the software engineering (SE) area. The survey reinforces the findings of the mapping studies. It also provides guidance for future research. In addition, the studies report several findings that confirmed the validity of the research methods applied. All of these findings are important input to the current Brazilian SE scenario. Our findings also suggest that greater attention should be given to the SE area, by improving researchers’ interaction with industry and increasing collaboration between researchers, especially internationally.
86|4||Contributions to the emergence and consolidation of Agent-oriented Software Engineering|Many of the issues addressed with multi-agent approaches, such as distributed coordination and self-organization, are now becoming part of industrial and business systems. However, Multiagent Systems (MASs) are still not widely adopted in industry owing to the lack of a connection between MAS and software engineering. Since 2000, there is an effort to bridge this gap and to produce software engineering techniques for agent-based systems that guide the processes of design, development and maintenance. In Brazil, Agent-oriented Software Engineering (AOSE) was first investigated by the research group in the Software Engineering Laboratory (LES) at PUC-Rio, which after one decade of study in this area has built an AOSE community. This paper presents the history of AOSE at LES by discussing the sub-areas of MAS Software Engineering research and development that have been focus of the LES research group. We give examples of relevant results and present a subset of the extensive literature the group has produced during the last decade. We also report how we faced the challenges that emerged from our research by organizing and developing a research community at the intersection of software engineering, programming and MASs with a concern for scalability of solutions.
86|4||The crosscutting impact of the AOSD Brazilian research community|Aspect-Oriented Software Development (AOSD) is a paradigm that promotes advanced separation of concerns and modularity throughout the software development lifecycle, with a distinctive emphasis on modular structures that cut across traditional abstraction boundaries. In the last 15 years, research on AOSD has boosted around the world. The AOSD-BR research community (AOSD-BR stands for AOSD in Brazil) emerged in the last decade, and has provided different contributions in a variety of topics. However, despite some evidence in terms of the number and quality of its outcomes, there is no organized characterization of the AOSD-BR community that positions it against the international AOSD Research community and the Software Engineering Research community in Brazil.
86|4||A scoping study on the 25 years of research into software testing in Brazil and an outlook on the future of the area|Over the past 25 years the Brazilian Symposium on Software Engineering (SBES) has evolved to become the most important event on software engineering in Brazil. Throughout these years, SBES has gathered a large body of studies in software testing. Aimed at providing an insightful understanding of what has already been published in such event, we have synthesized its 25-year history of research on software testing. Using information drawn from this overview we highlighted which software testing topics have been the most extensively surveyed in SBES literature. We have also devised a co-authorship network to depict the most prolific research groups and researchers. Moreover, by performing a citation analysis of the selected studies we have tried to ascertain the importance of SBES in a wider scenario. Finally, using the information extracted from the studies, we have shed light on the state-of-the-art of software testing in Brazil and provided an outlook on its foreseeable future.
86|4||Evaluation studies of software testing research in Brazil and in the world: A survey of two premier software engineering conferences|This paper reports on a historical perspective of the evaluation studies present in software testing research published in the Brazilian Symposium on Software Engineering (SBES) in comparison to the International Conference on Software Engineering (ICSE). The survey characterizes the software testing-related papers published in the 25-year history of SBES, investigates the types of evaluation presented in these publications, and how the rate of evaluations has evolved over the years. A similar analysis within the same period is made for ICSE, allowing for a comparison between the national and international scenario. Results show that the rate of papers that present evaluation studies in SBES has significantly increased over the years. However, among the papers that described some kind of evaluation, only around 20% performed more rigorous evaluations (i.e. case studies, quasi experiments, or controlled experiments). Such percentage is low when compared to ICSE, which presented 40% of papers with more rigorous evaluations within the same period. Nevertheless, we noticed that both venues still lack the publication of research reporting controlled experiments: only a single paper in each conference presented this type of evaluation.
86|4||Search Based Software Engineering: Review and analysis of the field in Brazil|Search Based Software Engineering (SBSE) is the field of software engineering research and practice that applies search based techniques to solve different optimization problems from diverse software engineering areas. SBSE approaches allow software engineers to automatically obtain solutions for complex and labor-intensive tasks, contributing to reduce efforts and costs associated to the software development. The SBSE field is growing rapidly in Brazil. The number of published works and research groups has significantly increased in the last three years and a Brazilian SBSE community is emerging. This is mainly due to the Brazilian Workshop on Search Based Software Engineering (WOES), co-located with the Brazilian Symposium on Software Engineering (SBES). Considering these facts, this paper presents results of a mapping we have performed in order to provide an overview of the SBSE field in Brazil. The main goal is to map the Brazilian SBSE community on SBES by identifying the main researchers, focus of the published works, fora and frequency of publications. The paper also introduces SBSE concerns and discusses trends, challenges, and open research problems to this emergent area. We hope the work serves as a reference to this novel field, contributing to disseminate SBSE and to its consolidation in Brazil.
86|4||Relevance and perspectives of AAL in Brazil|Population aging has been taking place in many countries across the globe and more recently in emerging countries. In this context, Ambient Assisted Living (AAL) has become one focus of attention, including methods, products, services, and AAL software systems that support the everyday lives of elderly people, promoting mainly their independence and dignity. From the perspective of computer science, efforts are already being dedicated to adequately developing AAL systems. However, in spite of its relevance, AAL has not been properly investigated in emerging countries, including Brazil. Thus, the contribution of this paper is to present the main perspectives of research in AAL, in particular in the area of software engineering, considering that the Brazilian population is also subject to the aging process. The main intention of this paper is to raise the interest of Brazilian researchers, as well as government and industry, for this important area.
86|4||A Brazilian survey on UML and model-driven practices for embedded software development|This paper brings statistical findings from a survey about the use of UML modeling and model-driven approaches for the design of embedded software in Brazil. The survey provides evidences regarding the maturity of use of UML and model-driven approaches, how they are employed, and which and where the professionals who use them are. Technical, organizational, and social aspects were investigated and documented by making use of a descriptive research method. Such aspects seemingly reflect the opinions of software engineers on how they perceive the impact of using UML and model-driven approaches on productivity and quality in embedded software development. Results show that most participants are clearly aware of the modeling approach value, even though they practice it only to a limited degree. Most respondents who make use of model-driven approaches attest that productivity and portability are the key advantages of their use.
86|4||Comparing approaches to analyze refactoring activity on software repositories|Some approaches have been used to investigate evidence on how developers refactor their code, whether refactorings activities may decrease the number of bugs, or improve developers’ productivity. However, there are some contradicting evidence in previous studies. For instance, some investigations found evidence that if the number of refactoring changes increases in the preceding time period the number of defects decreases, different from other studies. They have used different approaches to evaluate refactoring activities. Some of them identify committed behavior-preserving transformations in software repositories by using manual analysis, commit messages, or dynamic analysis. Others focus on identifying which refactorings are applied between two programs by using manual inspection or static analysis. In this work, we compare three different approaches based on manual analysis, commit message (Ratzinger's approach) and dynamic analysis (SafeRefactor's approach) to detect whether a pair of versions determines a refactoring, in terms of behavioral preservation. Additionally, we compare two approaches (manual analysis and Ref-Finder) to identify which refactorings are performed in each pair of versions. We perform both comparisons by evaluating their accuracy, precision, and recall in a randomly selected sample of 40 pairs of versions of JHotDraw, and 20 pairs of versions of Apache Common Collections. While the manual analysis presents the best results in both comparisons, it is not as scalable as the automated approaches. Ratzinger's approach is simple and fast, but presents a low recall; differently, SafeRefactor is able to detect most applied refactorings, although limitations in its test generation backend results for some kinds of subjects in low precision values. Ref-Finder presented a low precision and recall in our evaluation.
86|4||On the impact of trace-based feature location in the performance of software maintainers|Software maintainers frequently strive to locate source code related to specific software features. This situation is mostly observable when features are scattered in the code. Considering this problem, several approaches for feature location using execution traces have been developed. Nonetheless, the practice of post-mortem analysis based on execution traces is not fully incorporated in the daily practice of software maintainers. Empirical studies that reveal strengths and weaknesses on the use of execution traces in maintenance activities could better explain the role of execution traces in software maintenance. This study reports on a controlled experiment conducted with maintainers performing actual maintenance activities on systems of different sizes unknown to them. There are benefits from systematic use of execution traces: the reduction of the maintenance activity time and greater accuracy of the activity outcome. Other qualitative observations were the lower level of activity difficulty perceived by the participants that used execution trace information and that this kind of information seems to be less useful in maintenance activities where the problem of feature scattering does not occur clearly.
86|4||Safe composition of configuration knowledge-based software product lines|Mistakes made when implementing or specifying the models of a Software Product Line (SPL) can result in ill-formed products — the safe composition problem. Such problem can hinder productivity and it might be hard to detect, since SPLs can have thousands of products. In this article, we propose a language independent approach for verifying safe composition of SPLs with dedicated Configuration Knowledge models. We translate feature model and Configuration Knowledge into propositional logic and use the Alloy Analyzer to perform the verification. To provide evidence for the generality of our approach, we instantiate this approach in different compositional settings. We deal with different kinds of assets such as use case scenarios and Eclipse RCP components. We analyze both the code and the requirements for a larger scale SPL, finding problems that affect thousands of products in minutes. Moreover, our evaluation suggests that the analysis time grows linearly with respect to the number of products in the analyzed SPLs.
86|4||Offshore insourcing in software development: Structuring the decision-making process|A variety of new forms of business are enabled through globalization and practiced by software organizations today. While companies go global to reduce their development costs, access a larger pool of resources and explore new markets, it is often assumed that the level of delivered services shall remain the same after implementing the sourcing decisions. In contrast, critical studies identified that global software development is associated with unique challenges, and a lot of global projects fail to mitigate the implications of a particular global setting. In this paper we explore offshore insourcing decisions on the basis of empirical research literature and an empirical field study conducted at Ericsson. By analyzing decisions in two different cases we found that each offshore insourcing decision consisted of deciding what, where, when, how and why to insource. Related empirical research and field observations suggest that not all combinations are successful and alignment between different decision points has thus a prominent role. To address these concerns we built an empirically based insourcing decision structure, which outlines a logical path through the decision options and helps selecting an offshore insourcing strategy that targets creation of the necessary alignment. The key element of the proposed approach is a structured and well-defined decision-making process, which is intended to support managers in their decision-making. The usefulness of the proposed approach is evaluated in an additional empirical case of a new offshore insourcing decision.
86|4||Secret image sharing scheme with authentication and remedy abilities based on cellular automata and discrete wavelet transform|A meaningful secret image sharing scheme with authentication and remedy abilities is proposed in this paper. One dimensional cellular automata, discrete wavelet transform and hash function are adopted in the proposed scheme. The stego images are allowed to be verified to determine whether they are tampered or not. Once the stego images are tampered, shared bits retrieved from these tampered areas cannot be used to reconstruct the secret image. Instead, those damaged areas in the secret image can be repaired by the hidden information. Experimental results exhibit that low computation cost, high tamper detection rate and advanced remedy ability against tampering and cropping attacks are achieved by the proposed scheme.
86|4||Petri net based techniques for constructing reliable service composition|Service composition is an important mean for integrating the individual Web services to create new value added systems that satisfy complex requirements. However, it is challenging to guarantee the reliability of service composition in a distributed, dynamic and complex environment. This paper proposes an approach to constructing the reliable service composition. The underlying formalism is Petri net, which provides means to observe behaviors of basic component, and to describe their interrelationship. The transaction attributes, reliability and failure processing mechanisms are articulated. The composition mechanism systematically integrates these schemas into a transaction mapping model. Based on this, a reliable composition strategy and its enforcement algorithm are proposed, which can verify the behaviors of service composition at design time or after runtime to repair design errors. The operational semantics and related theories of Petri nets help prove the effectiveness of the proposed method. Finally, we use a simplified Export Service system to demonstrate the feasiability of the method.
86|4||Federated broker system for pervasive context provisioning|Software systems that provide context-awareness related functions in pervasive computing environments are gaining momentum due to emerging applications, architectures and business models. In most context-aware systems, a central broker performs the functions of context acquisition, processing, reasoning and provisioning to facilitate context-consuming applications, but demonstrations of such prototypical systems are limited to small, focussed domains. In order to develop modern context-aware systems that are capable of accommodating emerging pervasive/ubiquitous computing scenarios, are easily manageable, administratively and geographically scalable, it is desirable to have multiple brokers in the system divided into administrative, network, geographic, contextual or load based domains. Context providers and consumers may be configured to interact only with their nearest, relevant or most convenient broker. This setup demands inter-broker federation so that providers and consumers attached to different brokers can interact seamlessly, but such a federation has not been proposed for context-aware systems. This article analyses the limiting factors in existing context-aware systems, postulates the design and functional requirements that modern context-aware systems need to accommodate, and presents a federated broker based architecture for provisioning of contextual information over large geographical and network spans.
86|4||Comparing risk identification techniques for safety and security requirements|When developing systems where safety and security are important aspects, these aspects have to be given special attention throughout the development, in particular in the requirements phase. There are many similar techniques within the safety and security fields, but few comparisons about what lessons that could be learnt and benefits to be gained. In this paper different techniques for identifying risk, hazard and threat of computer-supported systems are compared. This is done by assessing the techniques’ ability to identify different risks in computer-supported systems in the environment where they operate. The purpose of this paper is therefore to investigate whether and how the techniques can mutually strengthen each other. The result aids practitioners in the selection and combination of techniques and researchers in focusing on gaps between the two fields. Among other things, the findings suggest that many safety techniques enforce a creative and systematic process by applying guide-words and structuring the results in worksheets, while security techniques tend to integrate system models with security models.
86|5|http://www.sciencedirect.com/science/journal/01641212/86/5|MDE software process lines in small companies|Software organizations specify their software processes so that process knowledge can be systematically reused across projects. However, different projects may require different processes. Defining a separate process for each potential project context is expensive and error-prone, since these processes must simultaneously evolve in a consistent manner. Moreover, an organization cannot envision all possible project contexts in advance because several variables may be involved, and these may also be combined in different ways. This problem is even worse in small companies since they usually cannot afford to define more than one process. Software process lines are a specific type of software product lines, in the software process domain. A benefit of software process lines is that they allow software process customization with respect to a context. In this article we propose a model-driven approach for software process lines specification and configuration. The article also presents two industrial case studies carried out at two small Chilean software development companies. Both companies have benefited from applying our approach to their processes: new projects are now developed using custom processes, process knowledge is systematically reused, and the total time required to customize a process is much shorter than before.
86|5||Evidence of software inspection on feature specification for software product lines|In software product lines (SPL), scoping is a phase responsible for capturing, specifying and modeling features, and also their constraints, interactions and variations. The feature specification task, performed in this phase, is usually based on natural language, which may lead to lack of clarity, non-conformities and defects. Consequently, scoping analysts may introduce ambiguity, inconsistency, omissions and non-conformities. In this sense, this paper aims at gathering evidence about the effects of applying an inspection approach to feature specification for SPL. Data from a SPL reengineering project were analyzed in this work and the analysis indicated that the correction activity demanded more effort. Also, Pareto's principle showed that incompleteness and ambiguity reported higher non-conformity occurrences. Finally, the Poisson regression analysis showed that sub-domain risk information can be a good indicator for prioritization of sub-domains in the inspection activity.
86|5||Automated test data generation for branch testing using genetic algorithm: An improved approach using branch ordering, memory and elitism|One of the problems faced in generating test data for branch coverage using a metaheuristic technique is that the population may not contain any individual that encodes test data for which the execution reaches the predicate node of the target branch. In order to deal with this problem, in this paper, we (a) introduce three approaches for ordering branches for selection as targets for coverage with a genetic algorithm (GA) and (b) experimentally evaluate branch ordering together with elitism and memory to improve test data generation performance. An extensive preliminary study was carried out to help frame the research questions and fine tune GA parameters which were then used in the final experimental study.
86|5||Testing Real-Time Embedded Systems using Timed Automata based approaches|Real-Time Embedded Systems (RTESs) have an increasing role in controlling the IT that we use on a day-to-day basis. The behaviour of an RTES is not based solely on the interactions it might have with its surrounding environment, but also on the timing requirements it induces. As a result, ensuring that an RTES behaves correctly is non-trivial, especially after adding time as a new dimension to the complexity of the testing process. We previously introduced the ‘priority-based’ approach which tests the logical and timing behaviour of an RTES modelled formally as UPPAAL automata. The ‘priority-based’ approach was based on producing sets of timed test traces by achieving clock region coverage. In this paper, we empirically validate the ‘priority-based’ approach with comparison to well-known timed testing approaches based on a Timed Automata (TA) formalism using a complete test bed based on an industrial-strength case study (production cell). The validation assessment is based on both fault coverage and structural coverage by a minimal number of generated test traces; the former is achieved using the Mutation Analysis Technique (MAT) by introducing a set of timed and functional mutation operators. The latter is based on clock region coverage as a main timed structural coverage criterion. This study shows that ‘priority-based’ approach can combine a high fault coverage and clock region coverage with a relatively small number of test traces in comparison with other test approaches. A set of experiences and lessons learned are highlighted as result of the real-time test bed.
86|5||An efficient tree-based algorithm for mining sequential patterns with multiple minimum supports|Sequential pattern mining (SPM) is an important technique for determining time-related behavior in sequence databases. In real-life applications, the frequencies for various items in a sequence database are not exactly equal. If all items are set with the same minimum support, the rare item problem may result, meaning that we are unable to effectively retrieve interesting patterns regardless of whether minsup is set too high or too low. Liu (2006) first included the concept of multiple minimum supports (MMSs) to SPM. It allows users to specify the minimum item support (MIS) for each item according to its natural frequency. A generalized sequential pattern-based algorithm, named Multiple Supports – Generalized Sequential Pattern (MS-GSP), was also developed to mine complete set of sequential patterns. However, the MS-GSP adopts candidate generate-and-test approach, which has been recognized as a costly and time-consuming method in pattern discovery. For the efficient mining of sequential patterns with MMSs, this study first proposes a compact data structure, called a Preorder Linked Multiple Supports tree (PLMS-tree), to store and compress the entire sequence database. Based on a PLMS-tree, we develop an efficient algorithm, Multiple Supports – Conditional Pattern growth (MSCP-growth), to discover the complete set of patterns. The experimental result shows that the proposed approach achieves more preferable findings than the MS-GSP and the conventional SPM.
86|5||A reference architecture for organizing the internal structure of metadata-based frameworks|Metadata-based frameworks enable behavior adaptation through the configuration of custom metadata in application classes. Most of the current frameworks used in the industry for building enterprise applications adopt this approach. However, there is a lack of proven techniques for building such kind of framework, allowing for a better organization of its internal structure. In this paper we propose a pattern language and a reference architecture for better organizing the internal structure of metadata-based frameworks, which were defined as a result of a pattern mining process applied to a set of existing open source frameworks. To evaluate the resulting structure generated by the reference architecture application, a case study examined three frameworks developed according to the proposed reference architecture, each one referring to a distinct application domain. The assessment was conducted by using a metrics suite, metrics thresholds derived from a large set of open source metadata-based frameworks, a process for automatic detection of design disharmonies and manual source code analysis. As a result of this study, framework developers can understand and use the proposed reference architecture to develop new frameworks and refactor existing ones. The assessment revealed that the organization provided by the reference architecture is suitable for metadata-based frameworks, helping in the division of responsibility and functionality among their classes.
86|5||A pattern fusion model for multi-step-ahead CPU load prediction|In distributed systems, resource prediction is an important but difficult topic. In many cases, multiple prediction is needed rather than only performing prediction at a single future point in time. However, traditional approaches are not sufficient for multi-step-ahead prediction. We introduce a pattern fusion model to predict multi-step-ahead CPU loads. In this model, similar patterns are first extracted from the historical data via calculating Euclidean distance and fluctuation pattern distance between historical patterns and current sequence. For a given pattern length, multiple similar patterns of this length can often be found and each of them can produce a prediction. We also propose a pattern weight strategy to merge these prediction. Finally, a machine learning algorithm is used to combine the prediction results obtained from different length pattern sets dynamically. Empirical results on four real-world production servers show that this approach achieves higher accuracy on average than existing approaches for multi-step-ahead prediction.
86|5||Quality-adaptive visual secret sharing by random grids|Visual secret sharing (VSS), classified into visual-cryptography (VC)-based and random-grid (RG)-based, suffers from the contrast problem that the reconstructed secret with low visual quality is not easy to recognize. Even worse, the more share images stacked, the lower visual quality of reconstructed secrets revealed. Therefore, it is promising to remove this innate drawback. In this paper, with security still kept, the light transmission of share images generated by the proposed scheme is redesigned to be higher than before such that the better visual quality of reconstructed secrets is obtained. To demonstrate the feasibility, the experimental results show the reconstructed secrets are visually recognizable and the goal that the more share images stacked, the better quality of reconstructed secrets we have is achieved.
86|5||Measuring the impact of changes to the complexity and coupling properties of automotive software systems|In the past few decades exponential increase in the amount of software used in cars has been recorded together with enhanced requirements for functional safety of their embedded software. As the evolution of software systems in cars often entails changes to software architecture, it is important to be able to monitor their impact.
86|5||Software ecosystems â A systematic literature review|A software ecosystem is the interaction of a set of actors on top of a common technological platform that results in a number of software solutions or services. Arguably, software ecosystems are gaining importance with the advent of, e.g., the Google Android, Apache, and Salesforce.com ecosystems. However, there exists no systematic overview of the research done on software ecosystems from a software engineering perspective. We performed a systematic literature review of software ecosystem research, analyzing 90 papers on the subject taken from a gross collection of 420. Our main conclusions are that while research on software ecosystems is increasing (a) there is little consensus on what constitutes a software ecosystem, (b) few analytical models of software ecosystems exist, and (c) little research is done in the context of real-world ecosystems. This work provides an overview of the field, while identifying areas for future research.
86|5||3D architecture viewpoints on service automation|Service-oriented architecture is an emerging paradigm for the execution of business-oriented as well as technical infrastructure processes by means of services. Automating the execution of services is of paramount importance in order to fulfill the needs of companies. However we have found that automation – although important – is seldom addressed explicitly as a concern when stating requirements or designing the software architecture of the service-based applications (SBAs). In this paper we define three architectural viewpoints framing the concerns about service automation. These three viewpoints, called 3D (Decisions, Degree, Data), respectively: express architectural decisions about automation; help identifying the level (degree) of automation required, and represent the specific data required to support automation in services. They have been applied to three industrial case studies and one academic experiment. Results show that they successfully support both technical and non-technical stakeholders in understanding how, and communicating upon, their concerns related to service automation have been addressed. The application of the 3D service automation viewpoints to different domains exhibits promising reusability.
86|5||Continuous range k-nearest neighbor queries in vehicular ad hoc networks|A driver should constantly keep an eye on nearby vehicles in order to avoid collisions. Unfortunately, the driver often does not see nearby vehicles because of obstacles (e.g., other vehicles, trees, buildings, etc.). This paper introduces a novel type of query, called a continuous range k-nearest neighbor (CRNN) query, in vehicular ad hoc networks, and it presents a new approach to process such a query. Most existing solutions to continuous nearest neighbor (CNN) queries focus on static objects, such as gas stations and restaurants, while this work concentrates on CRNN queries over moving vehicles. This is a challenging problem due to the high mobility of the vehicles. The CRNN query has characteristics in common with continuous range (CR) and CNN queries. In terms of CNN queries, the proposed approach achieves the same goal as the existing solutions, which is to decide effectively on valid intervals during which the query result remains unchanged. The proposed scheme aims to minimize the use of wireless network bandwidth, the computational cost, and the local storage while preserving information on the continuous movement of vehicles within the broadcast range of a given vehicle. Extensive experimental results confirm the effectiveness and superiority of the proposed scheme in comparison with an existing method.
86|5||Agile requirements prioritization in large-scale outsourced system projects: An empirical study|The application of agile practices for requirements prioritization in distributed and outsourced projects is a relatively recent trend. Hence, not all of its facets are well-understood. This exploratory study sets out to uncover the concepts that practitioners in a large software organization use in the prioritization process and the practices that they deem good. We seek to provide a rich analysis and a deep understanding of three cases in an exploratory study that was carried out in a large and mature company, widely recognized for its excellence and its engagement in outsourced software development. We used in-depth interviews for data collection and grounded theory techniques for data analysis. Our exploration efforts yielded the following findings: (i) understanding requirements dependencies is of paramount importance for the successful deployment of agile approaches in large outsourced projects. (ii) Next to business value, the most important prioritization criterion in the setting of outsourced large agile projects is risk. (iii) The software organization has developed a new artefact that seems to be a worthwhile contribution to agile software development in the large: ‘delivery stories’, which complement user stories with technical implications, effort estimation and associated risk. The delivery stories play a pivotal role in requirements prioritization. (iv) The vendor's domain knowledge is a key asset for setting up successful client-developer collaboration. (v) The use of agile prioritization practices depends on the type of project outsourcing arrangement. Our findings contribute to the empirical software engineering literature by bringing a rich analysis of cases in agile and distributed contexts, from a vendor's perspective. We also discuss the possible implications of the results for research and in practice.
86|5||A survey of software testing practices in Canada|Software testing is an important activity in the software development life-cycle. In an earlier study in 2009, we reported the results of a regional survey of software testing practices among practitioners in the Canadian province of Alberta. To get a larger nationwide view on this topic (across Canada), we conducted a newer survey with a revised list of questions in 2010. Compared to our previous Alberta-wide survey (53 software practitioners), the nation-wide survey had larger number of participants (246 practitioners). We report the survey design, execution and results in this article. The survey results reveal important and interesting findings about software testing practices in Canada. Whenever possible, we also compare the results of this survey to other similar studies, such as the ones conducted in the US, Sweden and Australia, and also two previous Alberta-wide surveys, including our 2009 survey. The results of our survey will be of interest to testing professionals both in Canada and world-wide. It will also benefit researchers in observing the latest trends in software testing industry identifying the areas of strength and weakness, which would then hopefully encourage further industry-academia collaborations in this area. Among the findings are the followings: (1) the importance of testing-related training is increasing, (2) functional and unit testing are two common test types that receive the most attention and efforts spent on them, (3) usage of the mutation testing approach is getting attention among Canadian firms, (4) traditional Test-last Development (TLD) style is still dominating and a few companies are attempting the new development approaches such as Test-Driven Development (TDD), and Behavior-Driven Development (BDD), (5) in terms of the most popular test tools, NUnit and Web application testing tools overtook JUnit and IBM Rational tools, (6) most Canadian companies use a combination of two coverage metrics: decision (branch) and condition coverage, (7) number of passing user acceptance tests and number of defects found per day (week or month) are regarded as the most important quality assurance metrics and decision factors to release, (8) in most Canadian companies, testers are out-numbered by developers, with ratios ranging from 1:2 to 1:5, (9) the majority of Canadian firms spent less than 40% of their efforts (budget and time) on testing during development, and (10) more than 70% of respondents participated in online discussion forums related to testing on a regular basis.
86|5||An improvement of diamond encoding using characteristic value positioning and modulus function|The diamond encoding technique controls payload and image quality by a k value. Although a small increase in k can increase payload, it also increases image distortion. In this paper, we proposed an improvement scheme to the diamond encoding technique for reducing image distortion during a k change. Another aim is to increase payload size. A square matrix conversion of the diamond matrix is used to lower the MSE values in high payload when k = 3. A lower MSE reduces image distortion. In low payload, that is when k changes from 1 to 2, a one dimension matrix modulus is used to reduce image distortion. Experimental results showed that payload may be increased while image quality is not significantly reduced.
86|5||An enhanced variable-length arithmetic coding and encryption scheme using chaotic maps|We enhance the simultaneous arithmetic coding and encryption scheme previously proposed by us. By encoding a block of variable number of symbols to a codeword within the length of the computation register, the operating efficiency has been substantially improved. Moreover, the compressed sequence is processed by an additional diffusion operation which strengthens the security of the original scheme by having higher key and plaintext sensitivities. Simulation results show that the enhanced scheme runs faster than the original scheme and the traditional compress-then-encrypt approach at a comparable compression performance.
86|5||Towards innovation measurement in the software industry|In today's highly competitive business environments with shortened product and technology life cycle, it is critical for software industry to continuously innovate. This goal can be achieved by developing a better understanding and control of the activities and determinants of innovation. Innovation measurement initiatives assess innovation capability, output and performance to help develop such an understanding. This study explores various aspects relevant to innovation measurement ranging from definitions, measurement frameworks and metrics that have been proposed in literature and used in practice. A systematic literature review followed by an online questionnaire and interviews with practitioners and academics were employed to identify a comprehensive definition of innovation that can be used in software industry. The metrics for the evaluation of determinants, inputs, outputs and performance were also aggregated and categorised. Based on these findings, a conceptual model of the key measurable elements of innovation was constructed from the findings of the systematic review. The model was further refined after feedback from academia and industry through interviews.
86|5||Lifetime and QoS-aware energy-saving buffering schemes|The heterogeneous drive (HDrive), which combines solid-state disk (SSD) and HDD, brings opportunity for energy-saving and has received extensive attention recently. This paper focuses on the file buffering schemes and adaptive disk power management (DPM) scheme for HDrive. As for the first issue, we propose a frequency–energy based replacement (FEBR) scheme based on an energy-cost model; as for the second issue, we present a sliding-window based adaptive DPM scheme by taking the HDD's lifetime into account. To make the trade-off among performance, HDD's lifetime and energy-saving, we contrive a QoS-aware DPM scheme. With extensive experiments on four real-world traces, we have evaluated the effectiveness of existing replacement schemes on energy-efficiency, performance, and HDD's lifetime and compare with our proposed schemes. The experimental results have demonstrated that energy-saving in HDrive is feasible and can reach as high as 60–80%, and that FBR and its variant FEBR, and GDS are the best ones among all those online schemes evaluated while FEBR has some advantage over FBR and GDS on the whole. The results have also revealed that our proposed adaptive sliding-window-based DPM scheme can effectively control the disk's lifetime and the QoS-aware DPM scheme works well in making tradeoffs among performance, HDD's lifetime and energy-saving.
86|5||A novel semantic information retrieval system based on a three-level domain model|This paper presents a methodology and a prototype for extracting and indexing knowledge from natural language documents. The underlying domain model relies on a conceptual level (described by means of a domain ontology), which represents the domain knowledge, and a lexical level (based on WordNet), which represents the domain vocabulary. A stochastic model (the ME-2L-HMM2, which mixes – in a novel way – HMM and maximum entropy models) stores the mapping between such levels, taking into account the linguistic context of words. Not only does such a context contain the surrounding words; it also contains morphologic and syntactic information extracted using natural language processing tools. The stochastic model is then used, during the document indexing phase, to disambiguate word meanings. The semantic information retrieval engine we developed supports simple keyword-based queries, as well as natural language-based queries. The engine is also able to extend the domain knowledge, discovering new and relevant concepts to add to the domain model. The validation tests indicate that the system is able to disambiguate and extract concepts with good accuracy. A comparison between our prototype and a classic search engine shows that the proposed approach is effective in providing better accuracy.
86|5||Controlling ERP consultants: Client and provider practices|Hiring consultants to implement the multiple components of an ERP installation is a common practice for securing expertise not found in client organizations. Ensuring the consultants work to the benefit of their client is potentially problematic as the consultants must adopt the goals of the client, coordinate with stakeholders within the client organization, and coordinate with those installing other components of the ERP. Controls are mechanisms that keep consultants on track with the objectives of client organizations. Control forms vary depending on the nature of the activity and the levels of expertise across clients and consultants. Just what forms of control are typically employed over consultants to promote the likelihood of a successful ERP implementation is not identified in the prior literature. Control theory is employed by this study to formulate an expectation of control, which is then examined through a multiple case study. Interviews of consultants, project managers in the client organizations, and project managers in the consulting firms confirm that client organizations employ performance controls on specified outcomes while the provider firms employ both outcome controls and behavioral controls to keep the ERP implementation project in line with client goals.
86|5||Corrigendum to âT. Chen, K. Tsao, Threshold visual secret sharing by random gridsâ [J. Syst. Softw. 84 (2011) 1197â1208]|
86|6|http://www.sciencedirect.com/science/journal/01641212/86/6|New steganography algorithm to conceal a large amount of secret message using hybrid adaptive neural networks with modified adaptive genetic algorithm|In this paper, we propose a new steganography algorithm using non-uniform adaptive image segmentation (NUAIS) with an intelligent computing technique to conceal efficiently a large amount of confidential messages (Smsg) into color images. Whereas, the number of secret bits to be replaced is non uniform from byte to another byte; it based on byte characteristics, which are extracted by using 16 byte levels (BL) with variance distribution of the Neighboring Eight Bytes (NEB) around the current byte. Four security layers are introduced to increase resistance against statistical and visual attacks. These layers are designed to make an excellent imperceptible concealing Smsg with lower distortion of a color plane and high protection of Smsg. The proposed intelligent technique using the hybrid adaptive neural networks and modified adaptive genetic algorithm employing uniform adaptive relaxation (ANN_AGAUAR) is working as the fourth security layer to improve the quality of the stego image (Is). The results are discussed and compared with the previous steganography algorithms; it demonstrates that the proposed algorithm's effectiveness can be concealed efficiently the number of secret bits reached to four bits per byte with better visual quality.
86|6||Toward automated refactoring of crosscutting concerns into aspects|Aspect-oriented programing (AOP) improves the separation of concerns by encapsulating crosscutting concerns into aspects. Thus, aspect-oriented programing aims to better support the evolution of systems. Along this line, we have defined a process that assists the developer to refactor an object-oriented system into an aspect-oriented one. In this paper we propose the use of association rules and Markov models to improve the assistance in accomplishing some of the tasks of this process. Specifically, we use these techniques to help the developer in the task of encapsulating a fragment of aspectizable code into an aspect. This includes the choice of a fragment of aspectizable code to be encapsulated, the selection of a suitable aspect refactoring, and the analysis and application of additional restructurings when necessary. Our case study of the refactoring of a J2EE system shows that the use of the process reduces the intervention of the developer during the refactoring.
86|6||An exploration of technical debt|Whilst technical debt is considered to be detrimental to the long term success of software development, it appears to be poorly understood in academic literature. The absence of a clear definition and model for technical debt exacerbates the challenge of its identification and adequate management, thus preventing the realisation of technical debt's utility as a conceptual and technical communication device.
86|6||MostoDE: A tool to exchange data amongst semantic-web ontologies|A semantic-web ontology, simply known as ontology, comprises a data model and data that should comply with it. Due to their distributed nature, there exist a large amount of heterogeneous ontologies, and a strong need for exchanging data amongst them, i.e., populating a target ontology using data that come from one or more source ontologies. Data exchange may be implemented using correspondences that are later transformed into executable mappings; however, exchanging data amongst ontologies is not a trivial task, so tools that help software engineers to exchange data amongst ontologies are a must. In the literature, there are a number of tools to automatically generate executable mappings; unfortunately, they have some drawbacks, namely: (1) they were designed to work with nested-relational data models, which prevents them to be applied to ontologies; (2) they require their users to handcraft and maintain their executable mappings, which is not appealing; or (3) they do not attempt to identify groups of correspondences, which may easily lead to incoherent target data. In this article, we present MostoDE, a tool that assists software engineers in generating SPARQL executable mappings and exchanging data amongst ontologies. The salient features of our tool are as follows: it allows to automate the generation of executable mappings using correspondences and constraints; it integrates several systems that implement semantic-web technologies to exchange data; and it provides visual aids for helping software engineers to exchange data amongst ontologies.
86|6||Incremental service level agreements violation handling with time impact analysis|This research addresses a critical issue of service level agreement (SLA) violation handling, i.e., time constraint violation related to service-based systems (SBS). Whenever an SLA violation occurs to a service, it can potentially impact dependent services, leading to unreliable SBS. Therefore, an SLA violation handling support is much required to produce a robust and adaptive SBS. There are several approaches to realizing exceptions and faults handling support for SBS, focusing on the detection stage, the analysis stage, and the resolution stage. However, the current works have not considered the handling strategy that takes the impact information into account to reduce the amount of change. This is essential to effectively handle the violation while consuming a reasonable recovery execution time. Therefore, in this research, we propose an incremental SLA violation handling with time impact analysis. The main role of the time impact analysis in the approach is to automatically generate an impact region based on the negative time impact conditions. Furthermore, the time impact analysis generates the appropriate time requirements. Both the region and the requirement are useful to support the recovery process. Based on a simplified evaluation study, the outcome suggests that the proposed approach can reduce the amount of service change within a reasonable recovery execution time.
86|6||Does decision documentation help junior designers rationalize their decisions? A comparative multiple-case study|Software architecture design is challenging, especially for junior software designers. Lacking practice and experience, junior designers need process support in order to make rational architecture decisions. In this paper, we present the results of a comparative multiple-case study conducted to find out if decision viewpoints from 0245 and 0250 can provide such a support. The case study was conducted with four teams of software engineering students working in industrial software projects. Two of the four teams were instructed to document their decisions using decision viewpoints; the other two teams were not instructed to do so. We observed the students for a period of seven weeks by conducting weekly focus groups and by analyzing their work artifacts and minutes. Our findings suggest that junior designers who use decision viewpoints are more systematic in exploring and evaluating solution options. However, the decision viewpoints did not help them in managing requirements and complexity.
86|6||Map-matched trajectory compression|The wide usage of location aware devices, such as GPS-enabled cellphones or PDAs, generates vast volumes of spatiotemporal streams of location data raising management challenges, such as efficient storage and querying. Therefore, compression techniques are inevitable also in the field of moving object databases. Related work is relatively limited and mainly driven by line simplification and data sequence compression techniques. Moreover, due to the (unavoidable) erroneous measurements from GPS devices, the problem of matching the location recordings with the underlying traffic network has recently gained the attention of the research community. So far, the proposed compression techniques have not been designed for network constrained moving objects, while on the other hand, existing map matching algorithms do not take compression aspects into consideration. In this paper, we propose solutions tackling the combined, map matched trajectory compression problem, the efficiency of which is demonstrated through an extensive experimental evaluation on offline and online trajectory data using synthetic and real trajectory datasets.
86|6||A framework for query refinement with user feedback|SQL queries in the existing relational data model implement the binary satisfaction of tuples. That is, a data tuple is filtered out from the result set if it does not satisfy the constraints expressed in the predicates of the user submitted query. Posing appropriate queries for ordinary users is very difficult in the first place if they lack knowledge of the underlying dataset. Therefore, imprecise queries are commonplace for many users. In connection with this, this paper presents a framework for capturing user intent through feedback for refining the initial imprecise queries that can fulfill the users’ information needs. The feedback in our framework consists of both unexpected tuples currently present in the query output and expected tuples that are missing from the query output. We show that our framework does not require users to provide the complete set of feedback tuples because only a subset of this feedback can suffice. We provide the point domination theory to complement the other members of feedback. We also provide algorithms to handle both soft and hard requirements for the refinement of initial imprecise queries. Experimental results suggest that our approach is promising compared to the decision tree based query refinement approach.
86|6||Introducing automated procedures in 3G network planning and optimization|Third generation (3G) networks have been launched for quite some time and cellular operators have already started to face the challenges when operating such kind of networks, compared to their 2G predecessors. In this paper, we give an overview of the network planning and optimization processes in 3G networks. Emphasis is given on the interdependence between coverage and capacity, which increases the complexity of operating 3G networks in a cost effective and quality assuring manner. We stretch out the importance of introducing automated procedures in these challenging tasks and describe main inputs and outputs of these procedures, such as initial network configuration and constraints, drive test data, and key performance indicators statistics and goals. We use two case studies from a operational 3G system to indicate the improvements on the network when these automated procedures are employed.
86|6||Closed inter-sequence pattern mining|Inter-sequence pattern mining can find associations across several sequences in a sequence database, which can discover both a sequential pattern within a transaction and sequential patterns across several different transactions. However, inter-sequence pattern mining algorithms usually generate a large number of recurrent frequent patterns. We have observed mining closed inter-sequence patterns instead of frequent ones can lead to a more compact yet complete result set. Therefore, in this paper, we propose a model of closed inter-sequence pattern mining and an efficient algorithm called CISP-Miner for mining such patterns, which enumerates closed inter-sequence patterns recursively along a search tree in a depth-first search manner. In addition, several effective pruning strategies and closure checking schemes are designed to reduce the search space and thus accelerate the algorithm. Our experiment results demonstrate that the proposed CISP-Miner algorithm is very efficient and outperforms a compared EISP-Miner algorithm in most cases.
86|6||Testing techniques selection based on ODC fault types and software metrics|Software testing techniques differ in the type of faults they are more prone to detect, and their performance varies depending on the features of the application being tested. Practitioners often use informally their knowledge about the software under test in order to combine testing techniques for maximizing the number of detected faults.
86|6||Robust and secure watermarking scheme for breath sound|Due to the development of the Internet, security and intellectual property protection have attracted significant interest in the copyright protection field recently. A novel watermarking scheme for breath sounds, combining lifting wavelet transform (LWT), discrete cosine transform (DCT), singular value decomposition (SVD) and dither modulation (DM) quantization is proposed in this paper as a way to insert encrypted source and identity information in breath sounds while maintaining significant biological signals. In the proposed scheme, LWT is first performed to decompose the signal, and then DCT is applied on the approximate coefficients. SVD is carried out on the LWT–DCT coefficients to derive singular values. DM is adopted to quantize the singular values of each of the LWT–DCT blocks; thus, the watermark extraction is blind by using the DM algorithm. The novelty of our proposed method also includes the introduction of the particle swarm optimization (PSO) technique to optimize the quantization steps for the DM approach. The experimental results demonstrate that the proposed watermarking scheme obtains good robustness against common manipulation attacks and preserves imperceptivity. The performance comparison results verify that our scheme outperforms existing approaches in terms of robustness and imperceptibility.
86|6||PS-QUASAR: A publish/subscribe QoS aware middleware for Wireless Sensor and Actor Networks|It has been more than 30 years since the first research into Wireless Sensor and Actor Networks appeared. However, WSANs are still not a ubiquitous technology due to several factors which include a lack of Quality of Service (QoS) support or the absence of high level programming models. New applications with heterogeneous QoS requirements where WSANs can be successfully applied, such as Critical Infrastructure Protection (CIP), have been recognized.
86|6||A survey study of critical success factors in agile software projects in former Yugoslavia IT companies|Determining the factors that have an influence on the success of the software development projects has been the focus of extensive research for more than 30 years. In recent years agile methodology of software development has become the dominant one for all kinds of software development projects. In this paper we present the results of empirical study for determining critical factors that influence the success of agile software projects which we conducted among senior developers and project managers from IT companies located in the former Yugoslavia countries within South Eastern Europe (SEE) region. This study is inspired by the similar study conducted 5 years ago (Chow and Cao, 2008). With this study we were not able to confirm the model developed in the previous study. Moreover it disconfirmed not only part of the factors, but very much questioned the whole scheme. However, we were able to shed additional light regarding agile software development in former Yugoslavia countries from SEE region as a reference region for investigating outsourced projects done in agile way.
86|6||Graph-based reference table construction to facilitate entity matching|Entity matching plays a crucial role in information integration among heterogeneous data sources, and numerous solutions have been developed. Entity resolution based on reference table has the benefits of high efficiency and being easy to update. In such kind of methods, the reference table is important for effective entity matching. In this paper, we focus on the construction of effective reference table by relying on co-occurring relationship between tokens to identify suitable entity names. To achieve high efficiency and accuracy, we first model data set as graph, and then cluster the vertices in the graph in two stages. Based on the connectivity between vertices, we also mine synonyms and get the expansive reference table. We develop an iterative system and conduct an experimental study using real data. Experimental results show that the method in this paper achieves both high accuracy and efficiency.
86|6||Securing web-clients with instrumented code and dynamic runtime monitoring|Security and privacy concerns remain a major factor that hinders the whole scale adoption of web-based technology in sensitive situations, such as financial transactions (0085 and 0210). These concerns impact both end users and content generators. To tackle this problem requires a complimentary technology to the already developed and deployed infrastructure for web security. Hence, we have developed a multi-layer framework for web client security based on mobile code instrumentation. This architecture seeks to isolate exploitable security vulnerabilities and enforce runtime policies against malicious code constructs. Our instrumentation process uniquely integrates both static and dynamic engines and is driven by flexible (XML based) rewrite rules for a scalable operation and transparent deployment.
86|6||Compositional real-time scheduling framework for periodic reward-based task model|As the size and complexity of embedded software systems increase, compositional real-time scheduling framework is widely accepted as means to build large and complex systems. A compositional real-time scheduling framework proposes to decompose a system into independent subsystems and provides ways to assemble them into a flexible hierarchical real-time scheduling system while guaranteeing the internal real-time requirements of each subsystem. In this paper, we consider the imprecise reward-based periodic task model in compositional scheduling framework. Thus, we introduce the imprecise periodic resource model to characterize the imprecise resource allocations provided by the system to a single component, and the interface model to abstract the imprecise real-time requirements of the component. The schedulability of mandatory parts is also analyzed to meet the minimum requirement of tasks. Finally, we provide a scheduling algorithm to guarantee a certain amount of reward, which makes it feasible to efficiently compose multiple imprecise components.
86|7|http://www.sciencedirect.com/science/journal/01641212/86/7|Collaborative computing technologies and systems|
86|7||The effects of a shared free form rationale space in collaborative learning activities|We report the exploration of a technique for promoting reflective thinking in group learning activities in two field studies. In each study, we developed a collaborative tool that provided a dedicated virtual group space with no pre-structure. The students were required to articulate and share their rationales using the space, namely, the rationale space. The rationales in this program are defined as explanations of the reasons underlying one's decisions, conclusions, and interpretations. We discuss our findings on the design of the shared rationale space in a virtual group workspace and the effects of one's rationale awareness in the activities. Rationale awareness is one's awareness of the other group members’ decision-making rationale of the shared tasks. Our study suggests that group members’ rationale awareness facilitates the group's common ground. In addition, the use of a dedicated rationale space has the potential of benefiting the group process through supporting group members’ rationale awareness and encouraging group members to develop practices around sharing thoughts and perspectives.
86|7||Setting the best view of a virtual teacher in a mixed reality physical-task learning support system|In this research, we investigated the virtual teacher's positions and orientations that led to optimal learning outcome in mixed-reality environment. First, this study showed that the virtual teacher's position and orientation have an effect on learning efficiency, when some teacher-settings are more comfortable and easy to watch than others. A sequence of physical-task learning experiments have been conducted using mixed-reality technology. The result suggested that the virtual-teacher's close side-view is the optimal view for learning physical-tasks that include significant one-hand movements. However, when both hands are used, or rotates around, a rotation-angle adjustment becomes necessary. Therefore, we proposed a software automatic-adjustment method governing the virtual teacher's horizontal rotation angle, so that the learner can easily observe important body motions. The proposed software method was revealed to be effective for motions that gradually reposition the most important moving part. Finally, to enhance the proposed method in the future, we conducted an experiment to find out the effect of setting the vertical view-angle. The result recommended that the more motion's rotation involved the more vertical view angles are wanted to see the whole motion clear.
86|7||Bringing knowledge into recommender systems|Recommender systems are largely used nowadays to support collaborative tasks. However, it is important to consider each user's knowledge of the system for the recommended subject. In this paper we describe the use of user knowledge to improve the recommender system of the Business Process Cooperative Editor (BPCE), a collaborative business process modeling tool. We use the concept of the Knowledge Vector, developed in a previous work on collaborative navigation, to factor user knowledge into recommendations.
86|7||A groupware system to support collaborative programming: Design and experiences|The advances in network and collaboration technologies enable the creation of powerful environments for collaborative programming. One such environment is COLLECE, a groupware system to support collaborative edition, compilation and execution of programs in a synchronous distributed fashion, which includes advanced tools for communication, coordination and workspace awareness. The article analyses firstly some usability and design issues, discussing strengths and weaknesses of the system as a basis for the development of groupware tools to support collaborative programming. Then, the focus is on a number of experimental activities carried out. COLLECE was used to conduct a set of experimental activities about work productivity and program quality when comparing the activity of pair and solo programmers, and to analyse potential associations between ways of working and collaborating, and specific characteristics of the programs produced.
86|7||Metamodel-driven definition of a visual modeling language for specifying interactive groupware applications: An empirical study|This work is framed in the area of software development for Computer Supported Cooperative Work (CSCW). These software systems are called groupware systems. The development of groupware systems is a complex task, a problem that can be addressed applying the Model Driven Engineering (MDE) principles and techniques, where the use of models is essential. However, there are no proposals to address all issues to model in this kind of application (group work, shared context, coordination, etc.) and, in particular, there are no proposals that consider the modeling of both interactive and collaborative issues. To solve this deficiency, a domain-specific language (DSL) called Collaborative Interactive Application Notation (CIAN) has been proposed. To define this DSL a metamodel has been created describing the universe of discourse of the applications supporting interactive group work. We have defined the syntax and semantics of this language. We have also implemented a tool (called CIAT) for supporting the edition and validation of models created with CIAN. This tool has been implemented using the metamodeling facilities provided by the Eclipse platform. Finally, an empirical study was conducted with the aim of verifying the suitability of this approach and the perception of software engineers about its usefulness. The results obtained show that our proposal can facilitate the development process of groupware systems.
86|7||A high performance peer to cloud and peer model augmented with hierarchical secure communications|
86|7||SETZ logistics models and system framework for manufacturing and exporting large engineering assets|
86|7||Scheduling of scientific workflow in non-dedicated heterogeneous multicluster platform|
86|7||Group and link analysis of multi-relational scientific social networks|Analyzing social networks enables us to detect several inter and intra connections between people in and outside their organizations. We model a multi-relational scientific social network where researchers may have four different types of relationships with each other. We adopt some criteria to enable the modeling of a scientific social network as close as possible to reality. Using clustering techniques with maximum flow measure, we identify the social structure and research communities in a way that allows us to evaluate the knowledge flow in the Brazilian scientific community. Finally, we evaluate the temporal evolution of scientific social networks to suggest/predict new relationships.
86|7||A mixed-method approach for the empirical evaluation of the issue-based variability modeling|Variability management is the fundamental part of software product line engineering, which deals with customization and reuse of artifacts for developing a family of systems. Rationale approaches structure decision-making by managing the tacit-knowledge behind decisions. This paper reports a quasi-experiment for evaluating a rationale enriched collaborative variability management methodology called issue-based variability modeling.
86|7||Design and testbed evaluation of RDMA-based middleware for high-performance data transfer applications|Providing high-speed data transfer is vital to various data-intensive applications supported by data center networks. We design a middleware layer of high-speed communication based on Remote Direct Memory Access (RDMA) that serves as the common substrate to accelerate various data transfer tools, such as FTP, HTTP, file copy, sync and remote file I/O. This middleware offers better end-to-end bandwidth performance than the traditional TCP-based alternatives, while it hides the heterogeneity of the underlying high-speed architecture. This paper describes this middleware's function modules, including resource abstraction and task synchronization and scheduling, that maximize the parallelism and performance of RDMA operations. For networks without RDMA hardware acceleration, we integrate Linux kernel optimization techniques to reduce data copy and processing in the middleware. We provide a reference implementation of the popular file-transfer protocol over this RDMA-based middleware layer, called RFTP. Our experimental results show that our RFTP outperforms several TCP-based FTP tools, such as GridFTP, while it maintains very low CPU consumption on a variety of data center platforms. Furthermore, those results confirm that our RFTP tool achieves near line-speed performance in both LAN and WAN, and scales consistently from 10 Gbps Ethernet to 40 Gbps Ethernet and InfiniBand environments.
86|7||A fault induction technique based on voltage underfeeding with application to attacks against AES and RSA|Fault injection attacks have proven to be a powerful tool to exploit the implementation weaknesses of cryptographic algorithms. Several techniques perturbing the computation of a cipher have been devised and successfully employed to leak secret information from erroneous results. We present a low-cost, non-invasive and effective technique to inject transient faults into a general purpose processor through lowering its feeding voltage, and to characterize the effects on the computing system. This technique is effective enough to lead attacks against a software implementation of a cryptosystem running on a full fledged ARM9 CPU with a complete operating system. We validate the effectiveness of the fault model through attacking OpenSSL implementations of the RSA and AES cryptosystems. A new attack against AES, able to retrieve the full 256-bit key, is described, and the number of faults to be collected is delineated. In addition, we propose a generalization of the attack against the RSA encryption presented in Barenghi et al. (2009), to a multi-bit fault model, and the analysis of its computational complexity. The attacks against AES retrieve all the round keys regardless of their derivation strategy, the number of cipher rounds and the diffusion layer, while the attacks against RSA retrieve either the message or the secret key.
86|7||Software effort models should be assessed via leave-one-out validation|More than half the literature on software effort estimation (SEE) focuses on model comparisons. Each of those requires a sampling method (SM) to generate the train and test sets. Different authors use different SMs such as leave-one-out (LOO), 3Way and 10Way cross-validation. While LOO is a deterministic algorithm, the N-way methods use random selection to build their train and test sets. This introduces the problem of conclusion instability where different authors rank effort estimators in different ways.
86|7||Supporting adaptation of decentralized software based on application scenarios|Software systems formed by autonomous software entities are greatly different from traditional software systems and it challenges researchers to find effective methods of supporting the adaptation of software systems. In this paper, an approach based on application scenarios is put forward for facilitating dynamic adaptations of decentralized software systems in unpredicted situations. Scenarios offer behavior norms to regulate the behavior of autonomous software entities under specific situations so that software entities can take fitted and coordinative actions when they are confronted with diverse and even unpredicted situations. At the end of this paper, a simulation traffic system is developed and studied. The experimental results show that the adaptability of the system is improved remarkably after application scenarios are deployed. In the case study, the efficiency and scalability of the scenario-based adaptation mechanism are also experimented and analyzed.
86|7||An approach for constructing private storage services as a unified fault-tolerant system|Organizations are gradually outsourcing storage services such as online hosting files, backup, and archival to public providers. There are however concerns with this process because organizations cannot access files when the service provider is unavailable as well as they have no control and no assurance on the management procedures related to data. As a result, organizations are exploring alternatives to build their own multi-tenant storage capacities.
86|7||An approach to software reliability prediction based on time series modeling|Reliability is the key factor for software system quality. Several models have been introduced to estimate and predict reliability based on results of software testing activities. Software Reliability Growth Models (SRGMs) are considered the most commonly used to achieve this goal. Over the past decades, many researchers have discussed SRGMs’ assumptions, applicability, and predictability. They have concluded that SRGMs have many shortcomings related to their unrealistic assumptions, environment-dependent applicability, and questionable predictability. Several approaches based on non-parametric statistics, Bayesian networks, and machine learning methods have been proposed in the literature. Based on their theoretical nature, however, they cannot completely address the SRGMs’ limitations. Consequently, addressing these shortcomings is still a very crucial task in order to provide reliable software systems. This paper presents a well-established prediction approach based on time series ARIMA (Autoregressive Integrated Moving Average) modeling as an alternative solution to address the SRGMs’ limitations and provide more accurate reliability prediction. Using real-life data sets on software failures, the accuracy of the proposed approach is evaluated and compared to popular existing approaches.
86|7||Applying hybrid learning approach to RoboCup's strategy|RoboCup (Robot world cup tournament) soccer game is a competitive game that has become a popular research domain in recent years since it involves a complex system for the behavior of multiple agents. In this paper, a hybrid approach, case-based reasoning genetic algorithm (CBR-GA) is applied to the soccer game for providing better strategies. By using CBR-GA, the soccer robots can obtain the suitable strategies for different conditions and store the related experiences, which may be reused in the future. Rule-based reasoning (RBR) will be employed to create a new strategy for the soccer robots when CBR-GA cannot provide a suitable one. A multi-agent learning system, constructed by combining case-based reasoning genetic algorithm with RBR strategy (CGRS), is implemented on the latest WrightEagle simulation platform that is released in 2011. In the CGRS system, two kinds of agent, namely “coach agent” and “movement agent”, are designed for the soccer game. The coach agent is responsible for deciding on the strategy goal and assigning tasks to the movement agents. Every movement agent will then execute its respective task for achieving the strategy goal. Better basic skills will facilitate the movement agents to execute more effectively the assigned tasks or plans; hence, many basic skills are designed for training the movement agents. To increase learning efficiency, the strategy cycle time is reduced with a suitable case base. To validate the effectiveness of the proposed approach, our soccer team played with the WrightEagle soccer team which has remained in the top two positions in simulation 2d in recent years. Our team gradually gets higher winning frequency in 50 rounds. Furthermore, a comparison experiment shows that the proposed approach has higher winning frequency than other methods including CBR-GA, CBR-RBR and RBR. Finally, the proposed approach is also found to have better learning mechanisms than other learning approaches in soccer game.
86|7||Research state of the art on GoF design patterns: A mapping study|Design patterns are used in software development to provide reusable and documented solutions to common design problems. Although many studies have explored various aspects of design patterns, no research summarizing the state of research related to design patterns existed up to now. This paper presents the results of a mapping study of about 120 primary studies, to provide an overview of the research efforts on Gang of Four (GoF) design patterns. The research questions of this study deal with (a) if design pattern research can be further categorized in research subtopics, (b) which of the above subtopics are the most active ones and (c) what is the reported effect of GoF patterns on software quality attributes. The results suggest that design pattern research can be further categorized to research on GoF patterns formalization, detection and application and on the effect of GoF patterns on software quality attributes. Concerning the intensity of research activity of the abovementioned subtopics, research on pattern detection and on the effect of GoF patterns on software quality attributes appear to be the most active ones. Finally, the reported research to date on the effect of GoF patterns on software quality attributes are controversial; because some studies identify one pattern's effect as beneficial whereas others report the same pattern's effect as harmful.
86|7||A high capacity lossless data hiding scheme for JPEG images|In this paper, we propose a new high-capacity reversible data hiding method for JPEG-compressed images. This method is based on modifying the quantization table and quantized discrete cosine transformation (DCT) coefficients. Some elements of the quantization table are divided by an integer while the corresponding quantized DCT coefficients are multiplied by the same integer and added by an adjustment value to make space for embedding the data. By analyzing the effect of each single quantized DCT coefficient on the image quality, an embedding sequence is chosen in order to help control the increase of file size after hiding the data meanwhile the PSNR value between the original uncompressed image and stego JPEG image is high. Experimental results show that the proposed method achieves both high capacity and high image quality.
86|8|http://www.sciencedirect.com/science/journal/01641212/86/8|Special section on automation of software test|
86|8||An orchestrated survey of methodologies for automated software test case generation|Test case generation is among the most labour-intensive tasks in software testing. It also has a strong impact on the effectiveness and efficiency of software testing. For these reasons, it has been one of the most active research topics in software testing for several decades, resulting in many different approaches and tools. This paper presents an orchestrated survey of the most prominent techniques for automatic generation of software test cases, reviewed in self-standing sections. The techniques presented include: (a) structural testing using symbolic execution, (b) model-based testing, (c) combinatorial testing, (d) random testing and its variant of adaptive random testing, and (e) search-based testing. Each section is contributed by world-renowned active researchers on the technique, and briefly covers the basic ideas underlying the method, the current state of the art, a discussion of the open research problems, and a perspective of the future development of the approach. As a whole, the paper aims at giving an introductory, up-to-date and (relatively) short overview of research in automatic test case generation, while ensuring a comprehensive and authoritative treatment.
86|8||Improving logic-based testing|Logic-based testers design tests from logical expressions that appear in software artifacts such as source code, design models, and requirements specifications. This paper presents three improvements to logic-based test design. First, in the context of mutation testing, we present fault hierarchies for the six relational operators. Applying the ROR mutation operator causes each relational operator to generate seven mutants per clause. The fault hierarchies show that only three of these seven mutants are needed. Second, we show how to bring the power of the ROR operator to logic-based test criteria such as the widely used Multiple Condition-Decision Coverage (MCDC) test criterion. Third, we present theoretical results supported by empirical data that show that the more recent coverage criterion of minimal-MUMCUT can find significantly more faults than MCDC. The paper has three specific recommendations: (1) Change the way the ROR mutation operator is defined in existing and future mutation systems. (2) Augment logic-based test criteria to incorporate relational operator replacement from mutation. (3) Replace the use of MCDC with minimal-MUMCUT, both in practice and in standards documents like FAA-DO178B.
86|8||On the integration of model-driven design and dynamic assertion-based verification for embedded software|Model-driven design (MDD) aims at elevating design to a higher level of abstraction than that provided by third-generation programming languages. Concurrently, assertion-based verification (ABV) relies on the definition of temporal assertions to enhance functional verification targeting the correctness of the design execution with respect to the expected behavior. Both MDD and ABV have affirmed as effective methodologies for design and verification of HW components of embedded systems. Nonetheless, MDD and ABV individually suffer some limitations that prevent their integration in the embedded-software (ESW) design and verification flow. In particular, MDD requires the integration of an effective methodology for monitoring specification conformance, and dynamic ABV relies on simulation assumptions, satisfied in the HW domain, but which cannot be straightforward guaranteed during the execution of ESW. In this work, we present a suitable combination of MDD and dynamic ABV as an effective solution for ESW design and verification. A suite composed of two off-the-shelf tools has been developed for supporting this integrated approach. The MDD tool, i.e., radCASE, is a rapid-application-development environment for ESW that provides the user with a comprehensive approach to cover the complete modeling and synthesis process of ESW. The dynamic ABV environment, i.e., radCHECK, integrates computer-aided and template-based assertion definition, automatic checker generation, and effective stimuli generation, making dynamic ABV really practical to check the correctness of the radCASE outcome.
86|8||User acceptance of software as a service: Evidence from customers of China's leading e-commerce company, Alibaba|This paper proposes a model with which to analyze the user acceptance of Software as a Service (SaaS). To develop this model, empirical surveys were conducted through four rounds of questionnaires obtained from customers of China's leading e-commerce company, Alibaba. Firstly, based on the data from the first three rounds (1399 respondents), a SaaSQual of operationalizing perceived e-service quality of SaaS was developed, and its four dimensions (ease of use, security, reliability and responsiveness) were identified. Secondly, based on the data from the fourth round (1532 respondents), it was found that the level of three user perceptions (e-service quality, usefulness, and social influence) were predictive of the users’ behavioral intention to use SaaS, and their direct and indirect influences were tested. This study recommends engineering improvements to SaaS based upon a better understanding of the level of user acceptance of this service.
86|8||Cooperative clustering for software modularization|Clustering is a useful technique to group data entities. Many different algorithms have been proposed for software clustering. To combine the strengths of various algorithms, researchers have suggested the use of Consensus Based Techniques (CBTs), where more than one actors (e.g. algorithms) work together to achieve a common goal. Although the use of CBTs has been explored in various disciplines, no work has been done for modularizing software. In this paper, the main research question we investigate is whether the Cooperative Clustering Technique (CCT), a type of CBT, can improve software modularization results. The main contributions of this paper are as follows. First, we propose our CCT in which more than one similarity measure cooperates during the hierarchical clustering process. To this end, we present an analysis of well-known measures. Second, we present a cooperative clustering approach for two types of well-known agglomerative hierarchical software clustering algorithms, for binary as well as non-binary features. Third, to evaluate our proposed CCT, we conduct modularization experiments on five software systems. Our analysis identifies certain cases that reveal weaknesses of the individual similarity measures. The experimental results support our hypothesis that these weaknesses may be overcome by using more than one measure, as our CCT produces better modularization results for test systems in which these cases occur. We conclude that CCTs are capable of showing significant improvement over individual clustering algorithms for software modularization.
86|8||FORTUNAâA framework for the design and development of hardware-based secure systems|Security requires a holistic view. In this work we contribute to this goal by taking a new viewpoint, with the proposal of the logic-probabilistic framework FORTUNA to support the design and development of hardware-based-security systems (HwBSS). It extends and further substantiates our ideas presented in a previous conference paper (Gallo et al., 2011). Our contributions in this article are: (a) to extend and validate FORTUNA, and (b) to illustrate its effectiveness uncovering an unreported SPARC V8 architectural security flaw.
86|8||SPAPE: A semantic-preserving amorphous procedure extraction method for near-miss clones|Cloned code, also known as duplicated code, is among the bad “code smells”. Procedure extraction can be used to remove clones and to make a software system more maintainable. While the existing procedure extraction techniques can handle automatic extraction of exact clones effectively, they fail to do so for near-miss clones, which are the code fragments that are similar but not the same. To address this gap, we developed SPAPE, a novel semantic-preserving amorphous procedure extraction method to extract near-miss clones. SPAPE relaxes the constraint of having the same syntax and uses the structural semantic information. We evaluated the performance, effectiveness, and benefits of SPAPE. Our results show that SPAPE can extract more near-miss clones than the best applicable method for ten open-source-software products in an efficient and effective fashion. We conclude that SPAPE can be a useful contribution to the toolsets of software managers and developers, and it can help them improve code structure and reduce software maintenance and overall project costs.
86|8||Threshold visual secret sharing by random grids with improved contrast|A (k, n) visual cryptographic scheme (VCS) is a secret sharing method, which encodes a secret image S into n share images in such a way that the stacking of any more than or equal to k share images will reveal S, while any less than k share images provide no information about S.  Kafri and Keren (1987) firstly implements (2,2)-VCS by random grids (RG-based VCS). Compared to conventional solutions of VCS, RG-based VCSs need neither extra pixel expansion nor complex codebook design. However, for a long period, RG-based VCSs are confined to (2,2) access structure. Until recently,  Chen and Tsao (2011) proposed the first (k, n) RG-based VCS. In this paper, we improve the contrast of  Chen and Tsao (2011)'s threshold scheme. The experimental results show that the proposed scheme outperforms  Chen and Tsao (2011)'s scheme significantly in visual quality.
86|8||Relevance, benefits, and problems of software modelling and model driven techniquesâA survey in the Italian industry|Claimed benefits of software modelling and model driven techniques are improvements in productivity, portability, maintainability and interoperability. However, little effort has been devoted at collecting evidence to evaluate their actual relevance, benefits and usage complications.
86|8||Supporting real-time multiple data items query in multi-RSU vehicular ad hoc networks (VANETs)|There has been increasing interest in the issue of multi-item queries in wireless broadcasting systems recently. Query starvation and bandwidth utilization have been identified as key issues for improved performance. In this paper, we examine this problem in the context of VANETs with multiple cooperating Road Side Units (RSUs). We characterize a query with two deadlines: Query Total Deadline (QTD) which is the actual deadline of a query and Query Local Deadline (QLD) which is the duration a query is valid for serving in an RSU. By considering these two deadlines together with vehicle speed, RSU range and inter-RSU distance, we formulate a Cooperative Query Serving (CQS) scheme which allows multiple RSUs to share residual bandwidth and effectively address the query starvation as well as the bandwidth utilization problems, hence maximizing the chance of serving multiple items queries. Extensive simulation results confirm that CQS outperforms other existing scheduling algorithms.
86|8||A novel approach to collaborative testing in a crowdsourcing environment|Software testing processes are generally labor-intensive and often involve substantial collaboration among testers, developers, and even users. However, considerable human resource capacity exists on the Internet in social networks, expert communities, or internet forums—referred to as crowds. Effectively using crowd resources to support collaborative testing is an interesting and challenging topic. This paper defines the collaborative testing problem in a crowd environment as an NP-Complete job assignment problem and formulates it as an integer linear programming (ILP) problem. Although package tools can be used to obtain the optimal solution to an ILP problem, computational complexity makes these tools unsuitable for solving large-scale problems. This study uses a greedy approach with four heuristic strategies to solve the problem. This is called the crowdsourcing-based collaborative testing approach. This approach includes two phases, training phase and testing phase. The training phase transforms the original problem into an ILP problem. The testing phase solves the ILP using heuristic strategies. A prototype system, called the Collaborative Testing System (COTS), is also implemented. The experiment results show that the proposed heuristic algorithms produce good quality approximate solutions in an acceptable timeframe.
86|8||Clustering navigation sequences to create contexts for guiding code navigation|To guide programmer code navigation, previous approaches such as TeamTracks recommend pieces of code to visit by mining the associations between pieces of code in programmer interaction histories. However, these result in low recommendation accuracy. To create more accurate recommendations, we propose NavClus an approach that clusters navigation sequences from programmer interaction histories. NavClus automatically forms collections of code that are relevant to the tasks performed by programmers, and then retrieves the collections best matched to a programmer's current navigation path. This makes it possible to recommend the collections of code that are relevant to the programmer's given task. We compare NavClus’ recommendation accuracy with TeamTracks’ by simulating recommendations using 4397 interaction histories. The comparative experiment shows that the recommendation accuracy of NavClus is twice as high as that of TeamTracks.
86|8||An improved VLC-based lossless data hiding scheme for JPEG images|In this paper, a lossless data hiding scheme which directly embeds data into the bitstream of JPEG images is presented. For real cases, the JPEG code space is partly occupied, not all variable length codes (VLC) in the Huffman table are used during the JPEG image compression process. Thus, these unused VLCs can be made used of and data hiding can be performed by mapping one or more unused VLCs to one used VLC. Through analyzing the statistics of both used and unused VLCs, the proposed scheme can take full advantage of the unused VLCs by mapping Huffman codes according to a specific mapping strategy and reach higher capacity. The output stego image can keep exactly the same content as the original one and preserve the same file size, and if the file size is allowed to be enlarged, then our scheme can achieve a significant improvement of embedding capacity.
86|8||A robust data hiding algorithm for H.264/AVC video streams|This paper presents a robust readable data hiding algorithm for H.264/AVC video streams without intra-frame distortion drift. We first encode the embedded data using BCH (n, k, t) syndrome code before data hiding to improve robustness, then we embed the encoded data into coefficients of the 4 × 4 luminance discrete cosine transform (DCT) blocks in I frames which meet our conditions to avert the distortion drift, and finally we recover the original video as much as possible when the hidden data is extracted out. The experimental results show that our scheme can get more robustness, effectively avert intra-frame distortion drift and get high visual quality.
86|8||Performing and analyzing non-formal inspections of entity relationship diagram (ERD)|Designing and understanding of diagrammatic representations is a critical issue for the success of software projects because diagrams in this field provide a collection of related information with various perceptual signs and they help software engineers to understand operational systems at different levels of information system development process. Entity relationship diagram (ERD) is one of the main diagrammatic representations of a conceptual data model that reflects users’ data requirements in a database system. In today's business environment, the business model is in a constant change which creates highly dynamic data requirements which also requires additional processes like modifications of ERD. However, in the literature there are not many measures to better understand the behaviors of software engineers during designing and understanding these representations. Hence, the main motivation of this study is to develop measures to better understand performance of software engineers during their understanding process of ERD. Accordingly, this study proposes two measures for ERD defect detection process. The defect detection difficulty level (DF) measures how difficult a defect to be detected according to the other defects for a group of software engineers. Defect detection performance (PP) measure is also proposed to understand the performance of a software engineer during the defect detection process. The results of this study are validated through the eye tracker data collected during the defect detection process of participants. Additionally, a relationship between the defect detection performance (PP) of a software engineer and his/her search patterns within an ERD is analyzed. Second experiment with five participants is also conducted to show the correlation between the proposed metric results and eye tracker data. The results of experiment-2 also found to be similar for DF and PP values. The results of this study are expected to provide insights to the researchers, software companies, and to the educators to improve ERD reasoning process. Through these measures several design guidelines can be developed for better graphical representations and modeling of the information which would improve quality of these diagrams. Moreover, some reviewing instructions can be developed for the software engineers to improve their reviewing process in ERD. These guidelines in turn will provide some tools for the educators to improve design and review skills of future software engineers.
86|8||A data mining approach to discovering reliable sequential patterns|Sequential pattern mining is a data mining method for obtaining frequent sequential patterns in a sequential database. Conventional sequence data mining methods could be divided into two categories: Apriori-like methods and pattern growth methods. In a sequential pattern, probability of time between two adjacent events could provide valuable information for decision-makers. As far as we know, there has been no methodology developed to extract this probability in the sequential pattern mining process. We extend the PrefixSpan algorithm and propose a new sequential pattern mining approach: P-PrefixSpan. Besides minimum support-count constraint, this approach imposes minimum time-probability constraint, so that fewer but more reliable patterns will be obtained. P-PrefixSpan is compared with PrefixSpan in terms of number of patterns obtained and execution efficiency. Our experimental results show that P-PrefixSpan is an efficient and scalable method for sequential pattern mining.
86|8||Adaptive reversible data hiding based on block median preservation and modification of prediction errors|In this paper, two enhanced reversible data hiding methods are proposed; both of them are based on two novel reversible data hiding techniques. A latest predictor is adopted to achieve better data hiding capability for the first predicative reversible data hiding scheme, whereas another scheme utilizes a new approach by considering the nature of different images to classify the smoothness for each piece of image blocking regions such that more secret data can be hidden into the smooth regions rather than the non-smooth ones resulting in a better embedding capability. The experiments verify that these schemes outperform the original reversible data hiding algorithms and some state-of-the-art reversible data hiding schemes.
86|9|http://www.sciencedirect.com/science/journal/01641212/86/9|The future of software engineering IN and FOR the cloud|
86|9||Cloud engineering is Search Based Software Engineering too|Many of the problems posed by the migration of computation to cloud platforms can be formulated and solved using techniques associated with Search Based Software Engineering (SBSE). Much of cloud software engineering involves problems of optimisation: performance, allocation, assignment and the dynamic balancing of resources to achieve pragmatic trade-offs between many competing technical and business objectives. SBSE is concerned with the application of computational search and optimisation to solve precisely these kinds of software engineering challenges. Interest in both cloud computing and SBSE has grown rapidly in the past five years, yet there has been little work on SBSE as a means of addressing cloud computing challenges. Like many computationally demanding activities, SBSE has the potential to benefit from the cloud; ‘SBSE in the cloud’. However, this paper focuses, instead, of the ways in which SBSE can benefit cloud computing. It thus develops the theme of ‘SBSE for the cloud’, formulating cloud computing challenges in ways that can be addressed using SBSE.
86|9||A goal-oriented simulation approach for obtaining good private cloud-based system architectures|The fast-growing Cloud Computing paradigm makes it possible to use unprecedented amounts of computing resources at lower costs, among other benefits such as fast provisioning and reliability. In designing a good architecture – the numbers, types and layouts of devices – for a cloud-based system, which meets the goals of all stakeholders, such goals need to be factored in from the earliest stages. However, there seems to be a lack of methodologies for incorporating stakeholder goals into the design process for such systems, and for assuring with higher confidence that the designs are likely to be good enough for the stated goals. In this paper, we propose a goal-oriented simulation approach for cloud-based system design whereby stakeholder goals are captured, together with such domain characteristics as workflows, and used in creating a simulation model as a proxy for the cloud-based system architecture. Simulations are then run, in an interleaving manner, against various configurations of the model as a way of rationally exploring, evaluating and selecting among incrementally better architectural alternatives. We illustrate important aspects of this approach for the private cloud deployment model and report on our experiments, using a smartcard-based public transportation system.
86|9||Cloud computing security: The scientific challenge, and a survey of solutions|We briefly survey issues in cloud computing security. The fact that data are shared with the cloud service provider is identified as the core scientific problem that separates cloud computing security from other topics in computing security. We survey three current research directions, and evaluate them in terms of a running software-as-a-service example.
86|9||An authentication model towards cloud federation in the enterprise|Cloud computing has emerged as a new paradigm which brought business opportunities as well as software engineering challenges. In The Cloud computing, business participants such as service providers, enterprise solutions, and marketplace applications are required to adopt a cloud architecture engineered for security and performance. Marketplace applications offer a great opportunity for enterprises to employ new Cloud capabilities to add value and extend business functionality. One of the major hurdles of formal adoption of Marketplace in the enterprise is performance. Enterprise applications (e.g. Lync Server, SAP, SharePoint, and Exchange Server) require a mechanism to predict and manage performance expectations. In previous research, we provided optimization for OAuth 2.0 adoption in the Enterprise. In this research, we extend the optimization to include identity federation in the Marketplace. This optimization is achieved by introducing provisioning steps to pre-establish trust amongst enterprise applications’ Resource Servers, its associated Authorization Server and the clients interested in access to protected resources. We then introduce the notion of referral tokens to enable Marketplace applications federation across organizations. In this architecture, trust is provisioned and synchronized as a pre-requisite step to authentication amongst all communicating entities in OAuth protocol, and referral tokens are used to establish trust federation for Marketplace applications across organizations. A real-life case study and a simulation test were used to validate the results.
86|9||A framework to support selection of cloud providers based on security and privacy requirements|Cloud computing is an evolving paradigm that is radically changing the way humans store, share and access their digital files. Despite the many benefits, such as the introduction of a rapid elastic resource pool, and on-demand service, the paradigm also creates challenges for both users and providers. In particular, there are issues related to security and privacy, such as unauthorised access, loss of privacy, data replication and regulatory violation that require adequate attention. Nevertheless, and despite the recent research interest in developing software engineering techniques to support systems based on the cloud, the literature fails to provide a systematic and structured approach that enables software engineers to identify security and privacy requirements and select a suitable cloud service provider based on such requirements. This paper presents a novel framework that fills this gap. Our framework incorporates a modelling language and it provides a structured process that supports elicitation of security and privacy requirements and the selection of a cloud provider based on the satisfiability of the service provider to the relevant security and privacy requirements. To illustrate our work, we present results from a real case study.
86|9||A service-oriented framework for developing cross cloud migratable software|Whilst cloud computing has burst into the current scene as a technology that allows companies to access high computing rates at limited costs, cloud vendors have rushed to provide tools that allow developers to build software for their cloud platforms. The software developed with these tools is often tightly coupled to their services and restrictions. Consequently vendor lock in becomes a common problem which multiple cloud users have to tackle in order to exploit the full potential of cloud computing. A scenario where component-based applications are developed for being deployed across several clouds, and each component can independently be deployed in one cloud or another, remains fictitious due to the complexity and the cost of their development. This paper presents a cloud development framework for developing cloud agnostic applications that may be deployed indifferently across multiple cloud platforms. Information about cloud deployment and cloud integration is separated from the source code and managed by the framework. Interoperability between interdependent components deployed in different clouds is achieved by automatically generating services and service clients. This allows software developers to segment their applications into different modules that can easily be deployed and redistributed across heterogeneous cloud platforms.
86|9||A common API for delivering services over multi-vendor cloud resources|The increasing pace of evolution in business computing services leads enterprises to outsource secondary operations that are not part of their core business. The cloud computing market has been growing over the past few years and, consequently, many cloud companies are now offering a rich set of features to their consumers. Unfortunately, those cloud players have created new services with different APIs, which imply that cloud-oriented applications might be instantiated in one single cloud provider. This scenario is not desirable to the IT industry because their applications will become provider-dependent. In this paper we present a platform that allows applications to interoperate with distinct cloud providers’ services using a normalized interface. The proposed approach provides a common API that minimizes the present deficit of cloud API standardization and provides secure and redundant services allocation. Moreover, services from different cloud providers can be combined and decorated with additional functionalities like, for instance, redundancy and ciphering on-the-fly.
86|9||Cloud computing and its impact on mobile software development: Two roads diverged|Today, both desktop and mobile software systems are built to leverage resources available on the World Wide Web. However, in recent years desktop and mobile software systems have evolved in different directions. On desktop computers, the most popular application for accessing content and applications on the Web is the web browser. In mobile devices, in contrast, the majority of web content is consumed via custom-built native web apps. This divergence will not continue indefinitely. In the 2010's we will witness a major battle between two types of technologies: native web apps and Open Web applications that run in a web browser. This “Battle of the Decade” will determine the future of the software industry for years to come.
86|9||Solidifying the foundations of the cloud for the next generation Software Engineering|
86|9||Industry's role in data and software curation in the cloud|The cloud is a de facto place now for storage and for software. For ordinary researchers, it is not as easy to use as one would hope. This column explores how a researcher painlessly leverages the cloud to archive, share and publish scientific data that is richer than text; and to showcase experimental tools in any environment, for review or classes. We coin the phrases middleware and baseware as they apply to the cloud and give two examples of middleware that is freely available for data and for software curation and sharing. We end with some ideas about the future of the cloud from the industry-researcher standpoint.
86|9||Teaching cloud computing: A software engineering perspective|This article discusses the teaching of cloud computing in a software engineering course. It suggests that all courses should have some material introducing students to cloud computing, that practical teaching should focus on Platform as a Service and that there is scope for a graduate course in cloud software engineering covering map-reduce, schema-free databases, service-oriented computing, security and compliance and design for resilience.
86|9||A design rule language for aspect-oriented programming|Aspect-oriented programming is known as a technique for modularizing crosscutting concerns. However, constructs aimed to support crosscutting modularity might actually break class modularity. As a consequence, class developers face changeability, parallel development and comprehensibility problems, because they must be aware of aspects whenever they develop or maintain a class. At the same time, aspects are vulnerable to changes in classes, since there is no contract specifying the points of interaction amongst these elements. These problems can be mitigated by using adequate design rules between classes and aspects. We present a design rule specification language and explore its benefits since the initial phases of the development process, specially with the aim of supporting modular development of classes and aspects. We discuss how our language improves crosscutting modularity without breaking class modularity. We evaluate it using a real case study and compare it with other approaches.
86|9||Multi-sprint planning and smooth replanning: An optimization model|Most agile methods divide a project into sprints (iterations), and include a sprint planning phase that is critical to ensure the project success. Several factors impact on the optimality of a sprint plan, which makes the planning problem difficult. In this paper we formalize the planning problem and propose an optimization model that, given the estimates made by the project team and a set of development constraints, produces a multi-sprint optimal plan that maximizes the business value perceived by users. To cope with the inherent flexibility and uncertainty of agile projects, our approach ensures that a baseline plan can be revised and re-optimized during project execution without disrupting it, which we call smooth replanning. The planning problem is converted into a generalized assignment problem, given a linear programming formulation, and solved using the IBM ILOG CPLEX Optimizer. Our model is validated on both real and synthetic projects. In particular, a case study on two real projects confirms the effectiveness of our approach; as to efficiency, for medium-sized problems an exact solution is found in a few minutes, while for large problems a heuristic solution that is less than 1% far from the exact one is returned in a few seconds. Finally, some smooth replanning tests investigate the trade-off between plan quality and stability.
86|9||On evaluating commercial Cloud services: A systematic review|Cloud Computing is increasingly booming in industry with many competing providers and services. Accordingly, evaluation of commercial Cloud services is necessary. However, the existing evaluation studies are relatively chaotic. There exists tremendous confusion and gap between practices and theory about Cloud services evaluation.
86|9||User Interface Transition Diagrams for customerâdeveloper communication improvement in software development projects|We formalize the definition and construction of the User Interface Transition Diagram (UITD) which is a modelling notation for the transitions between UI presentations and the necessary conditions to trigger these transitions. We show how the UITD is able to improve the communication between stakeholders in a software development project: Human–Computer Interaction specialists, Software Engineers and customers who have little or no training in specialized modelling notations. We compare the UITD with other existing similar modelling notations highlighting the features that are better expressed in the UITD. We also include a case study in order to show how the UITD can be helpful in different phases of a software development project. The understandability of the UITD was confirmed by means of a test where different types of potential users were involved.
86|9||Countermeasure graphs for software security risk assessment: An action research|Software security risk analysis is an important part of improving software quality. In previous research we proposed countermeasure graphs (CGs), an approach to conduct risk analysis, combining the ideas of different risk analysis approaches. The approach was designed for reuse and easy evolvability to support agile software development.
86|9||Image encryption based on the Jacobian elliptic maps|In this paper, a novel image encryption algorithm based on the Jacobian elliptic maps is presented. To illustrate the effectiveness of the proposed scheme, some security analyses are presented. It can be concluded that, the proposed image encryption technique can be applied for practical applications. Although the Jacobian elliptic maps presented in this paper aim at image encryption, it is not just limited to this experience and can be directly applied in other information security fields such as video encryption.
86|9||Certified Information Access|Certified Information Access (CIA) primitive allows a user to obtain answers to database queries in a way that she can verify the correctness of the received information. The database owner answers a query by providing the information matching the query along with a proof that such information are consistent with the actual content of the database. Current solutions to this problem require a computationally intensive setup phase. We describe two secure distributed implementations of a CIA service. In the first one, the database owner distributes the evaluation of a computation intensive function (e.g., exponentiations) among a set of untrusted peers and locally reconstructs the result of such an evaluation. In the second one, we propose a protocol for securely outsourcing the whole computation of the data structures used in the implementations of the CIA primitive. In this case, the main issue to be considered is the need of guaranteeing on the one hand the confidentiality of the database contents and, on the other hand, the correctness and soundness of the answers obtained by the users. We argue that classical cryptographic primitives are not sufficient for our purposes and we introduce a new primitive, the Verifiable Deterministic Envelope, that may be of independent interest.
86|9||An object-oriented approach to language compositions for software language engineering|In this paper, it is shown that inheritance, a core concept from object-oriented programming, is a possible solution for realizing composition of computer languages. Language composability is a property of language descriptions, which can be further classified into informal (language syntax and semantics are hard-coded in compiler/interpreter) and formal language descriptions (syntax and semantics are formally specified with one of several formal methods for language definition). However, language composition is much easier to achieve with declarative formal language descriptions into which the notion of inheritance is introduced. Multiple attribute grammar inheritance, as implemented in the language implementation system LISA, can assist in realizing all of the different types of language compositions identified in Erdweg et al. (2012). Different examples are given throughout the paper using an easy to understand domain-specific language that describes simple robot movement.
87|-|http://www.sciencedirect.com/science/journal/01641212/87|On the relationships between QoS and software adaptability at the architectural level|Modern software operates in highly dynamic and often unpredictable environments that can degrade its quality of service. Therefore, it is increasingly important having systems able to adapt their behavior. However, the achievement of software adaptability can influence other software quality attributes, such as availability, performance or cost. This paper proposes an approach for analyzing tradeoffs between the system adaptability and its quality of service. The proposed approach is based on a set of metrics that allow the system adaptability evaluation. The approach can help software architects to guide decisions on system adaptation for fulfilling system quality requirements. The application and effectiveness of the approach are illustrated through examples and a wide set of experiments carried out with a tool we have developed.
87|-||Assessing the reliability, validity and acceptance of a classification scheme of usability problems (CUP)|The aim of this study was to evaluate the Classification of Usability Problems (CUP) scheme. The goal of CUP is to classify usability problems further to give user interface developers better feedback to improve their understanding of usability problems, help them manage usability maintenance, enable them to find effective fixes for UP, and prevent such problems from reoccurring in the future. First, reliability was evaluated with raters of different levels of expertise and experience in using CUP. Second, acceptability was assessed with a questionnaire. Third, validity was assessed by developers in two field studies. An analytical comparison was also made to three other classification schemes. CUP reliability results indicated that the expertise and experience of raters are critical factors for assessing reliability consistently, especially for the more complex attributes. Validity analysis results showed that tools used by developers must be tailored to their working framework, knowledge and maturity. The acceptability study showed that practitioners are concerned with the effort spent in applying any tool. To understand developers’ work and the implications of this study two theories are presented for understanding and prioritising UP. For applying classification schemes, the implications of this study are that training and context are needed.
87|-||A three-phase energy-saving strategy for cloud storage systems|In the running process of cloud data center, the idle data nodes will generate a large amount of unnecessary energy consumption. Furthermore, the resource misallocation will also cause a great waste of energy. This paper proposes a three-phase energy-saving strategy named TPES in order to save energy and operational costs for cloud suppliers. The three phases are replica management based on variable replication factor, cluster reconfiguration according to the optimal total costs and state transition based on observed and predicted workloads. These three phases save energy for the system at different levels which enhance the adaptability of our strategy. We evaluate our strategy using the expanded CloudSim toolkit and the results show that the proposed strategy achieves better energy reduction under different conditions in comparison with the existing schemes.
87|-||Modeling continuous integration practice differences in industry software development|Continuous integration is a software practice where developers integrate frequently, at least daily. While this is an ostensibly simple concept, it does leave ample room for interpretation: what is it the developers integrate with, what happens when they do, and what happens before they do? These are all open questions with regards to the details of how one implements the practice of continuous integration, and it is conceivable that not all such implementations in the industry are alike. In this paper we show through a literature review that there are differences in how the practice of continuous integration is interpreted and implemented from case to case. Based on these findings we propose a descriptive model for documenting and thereby better understanding implementations of the continuous integration practice and their differences. The application of the model to an industry software development project is then described in an illustrative case study.
87|-||A distributed framework for demand-driven software vulnerability detection|
87|-||Surviving sensor node failures by MMU-less incremental checkpointing|For some critical safety applications, sensor nodes embed valuable information, and they should be able to operate unattended and unfailing for several months or years. One promising solution is to adopt a checkpointing that periodically saves the state of a sensor node, thereby maintaining node reliability and network availability. Thus, this study first shows the design and implementation of a full checkpointing for WSNs. However, checkpointing is expensive. Therefore, incremental checkpointing was previously proposed to eliminate the checkpoint overhead by relying on the page protection hardware to identify dirty pages. Because sensor nodes are resource-constrained and do not equip with the page protection hardware, previous incremental checkpointings cannot be directly applied. To address this issue, this paper proposes three incremental checkpointings for WSNs. These three methods differ in the granularity of the checkpoint memory data unit and module execution overhead. In addition, we designed an incremental checkpoint file format that simultaneously supports proposed three different incremental checkpointings and accommodates them with sensor network characteristics. We implemented the full and three incremental checkpointings on SOS in the mica2 sensor motes. A performance evaluation of the three incremental checkpointings is presented. We also discuss and evaluate a method for selecting the appropriate incremental checkpointing. To the best of our knowledge, this study is the first to design and implement incremental checkpointing in MMU-less WSNs.
87|-||Performance optimization of deployed software-as-a-service applications|The goal of performance maintenance is to improve the performance of a software system after delivery. As the performance of a system is often characterized by unexpected combinations of metric values, manual analysis of performance is hard in complex systems. In this paper, we propose an approach that helps performance experts locate and analyze spots – so called performance improvement opportunities (PIOs) – for possible performance improvements. PIOs give performance experts a starting point for performance improvements, e.g., by pinpointing the bottleneck component. The technique uses a combination of association rules and performance counters to generate the rule coverage matrix, a matrix which assists with the bottleneck detection.
87|-||Using SMCD to reduce inconsistencies in misuse case models: A subject-based empirical evaluation|Security is a crucial requirement in software systems which need to be addressed as early as the requirements phase. The technique of misuse case modeling has been introduced slightly over a decade ago to elicit and specify functional security requirements. Development efforts downstream will be driven by the functional security requirements specified in misuse case models. Consequently, the quality of a misuse case model influences the effectiveness of downstream development efforts. Inconsistencies are an undesired attribute that can severely reduce the quality of misuse case models. In this paper, a controlled experiment involving students is presented which evaluates the reduction of inconsistencies in misuse case models resulting from utilizing a structure called SMCD (Structured Misuse Case Descriptions). The experiment also examines the impact of using SMCD upon other quality attributes of misuse case models. The results of the experiment indicate that using SMCD improves the consistency levels of the developed misuse case models.
87|-||Evolving feature model configurations in software product lines|The increasing complexity and cost of software-intensive systems has led developers to seek ways of reusing software components across development projects. One approach to increasing software reusability is to develop a software product-line (SPL), which is a software architecture that can be reconfigured and reused across projects. Rather than developing software from scratch for a new project, a new configuration of the SPL is produced. It is hard, however, to find a configuration of an SPL that meets an arbitrary requirement set and does not violate any configuration constraints in the SPL.
87|-||Corrigendum to âA novel approach to collaborative testing in a crowdsourcing environmentâ in the Journal of Systems and Software 86 (2013) 2143â2153|
88|-|http://www.sciencedirect.com/science/journal/01641212/88|Coherent clusters in source code|This paper presents the results of a large scale empirical study of coherent dependence clusters. All statements in a coherent dependence cluster depend upon the same set of statements and affect the same set of statements; a coherent cluster's statements have ‘coherent’ shared backward and forward dependence. We introduce an approximation to efficiently locate coherent clusters and show that it has a minimum precision of 97.76%. Our empirical study also finds that, despite their tight coherence constraints, coherent dependence clusters are in abundance: 23 of the 30 programs studied have coherent clusters that contain at least 10% of the whole program. Studying patterns of clustering in these programs reveals that most programs contain multiple substantial coherent clusters. A series of subsequent case studies uncover that all clusters of significant size map to a logical functionality and correspond to a program structure. For example, we show that for the program acct, the top five coherent clusters all map to specific, yet otherwise non-obvious, functionality. Cluster visualization also brings out subtle deficiencies in program structure and identifies potential refactoring candidates. A study of inter-cluster dependence is used to highlight how coherent clusters are connected to each other, revealing higher-level structures, which can be used in reverse engineering. Finally, studies are presented to illustrate how clusters are not correlated with program faults as they remain stable during most system evolution.
88|-||Conceptual modeling of natural language functional requirements|Requirements analysts consider a conceptual model to be an important artifact created during the requirements analysis phase of a software development life cycle (SDLC). A conceptual, or domain model is a visual model of the requirements domain in focus. Owing to its visual nature, the model serves as a platform for the deliberation of requirements by stakeholders and enables requirements analysts to further refine the functional requirements. Conceptual models may evolve into class diagrams during the design and execution phases of the software project. Even a partially automated conceptual model can save significant time during the requirements phase, by quickening the process of graphical communication and visualization.
88|-||Resource failures risk assessment modelling in distributed environments|Service providers offer access to resources and services in distributed environments such as Grids and Clouds through formal Service level Agreements (SLA), and need well-balanced infrastructures so that they can maximise the Quality of Service (QoS) they offer and minimise the number of SLA violations. We propose a mathematical model to predict the risk of failure of resources in such environments using a discrete-time analytical model driven by reliability functions fitted to observed data. The model relies on the resource historical data so as to predict the risk of failure for a given time interval. The model is evaluated by comparing the predicted risk of failure with the observed risk of failure, and is shown to accurately predict the resources risk of failure, allowing a service provider to selectively choose which SLA request to accept.
88|-||Identifying organizational barriersâA case study of usability work when developing software in the automation industry|This study investigates connections between usability efforts and organizational factors. This is an important field of research which so far appears to be insufficiently studied and discussed. It illustrates problems when working with software engineering tasks and usability requirements. It deals with a large company that manufactures industrial robots with an advanced user interface, which wanted to introduce usability KPIs, to improve product quality. The situation in the company makes this difficult, due to a combination of organizational and behavioural factors that led to a “wicked problem” that caused conflicts, breakdowns and barriers. Addressing these problems requires a holistic view that places context in the foreground and technological solutions in the background. Developing the right product requires communication and collaboration between multiple stakeholders. The inclusion of end users, who fully understand their own work context, is vital. Achieving this is dependent on organizational change, and management commitment. One step to beginning this change process may be through studying ways to introduce user-centred design processes.
88|-||Robust reversible watermarking scheme using Slantlet transform matrix|The need for a robust reversible watermarking method has recently attracted more attention. This paper presents a novel robust reversible watermarking scheme based on using the Slantlet transform matrix to transform small blocks of the original image and hiding the watermark bits by modifying the mean values of the carrier subbands. The problem of overflow/underflow has been avoided by using histogram modification process. Extensive experimental tests based on 100 general images and 100 medical images demonstrate the efficiency of the proposed scheme. The proposed scheme has robustness against different kinds of attacks and the results prove that it is completely reversible with improved capacity, robustness, and invisibility in comparison with the previous methods.
88|-||Software architecture review by association|During the process of software design, software architects have their reasons to choose certain software components to address particular software requirements and constraints. However, existing software architecture review techniques often rely on the design reviewers’ knowledge and experience, and perhaps using some checklists, to identify design gaps and issues, without questioning the reasoning behind the decisions made by the architects. In this paper, we approach design reviews from a design reasoning perspective. We propose to use an association-based review procedure to identify design issues by first associating all the relevant design concerns, problems and solutions systematically; and then verifying if the causal relationships between these design elements are valid. Using this procedure, we discovered new design issues in all three industrial cases, despite their internal architecture reviews and one of the three systems being operational. With the newly found design issues, we derive eight general design reasoning failure scenarios.
88|-||Adding semantic modules to improve goal-oriented analysis of data warehouses using I-star|The success rate of data warehouse (DW) development is improved by performing a requirements elicitation stage in which the users’ needs are modeled. Currently, among the different proposals for modeling requirements, there is a special focus on goal-oriented models, and in particular on the i* framework. In order to adapt this framework for DW development, we previously developed a UML profile for DWs. However, as the general i* framework, the proposal lacks modularity. This has a specially negative impact for DW development, since DW requirement models tend to include a huge number of elements with crossed relationships between them. In turn, the readability of the models is decreased, harming their utility and increasing the error rate and development time. In this paper, we propose an extension of our i* profile for DWs considering the modularization of goals. We provide a set of guidelines in order to correctly apply our proposal. Furthermore, we have performed an experiment in order to assess the validity our proposal. The benefits of our proposal are an increase in the modularity and scalability of the models which, in turn, increases the error correction capability, and makes complex models easier to understand by DW developers and non expert users.
88|-||Generation and validation of traces between requirements and architecture based on formal trace semantics|The size and complexity of software systems make integration of the new/modified requirements to the software system costly and time consuming. The impact of requirements changes on other requirements, design elements and source code should be traced to determine parts of the software to be changed. Considerable research has been devoted to relating requirements and design artifacts with source code. Less attention has been paid to relating requirements (R) with architecture (A) by using well-defined semantics of traces. Traces between R&A might be manually assigned. This is time-consuming, and error prone. Traces might be incomplete and invalid. In this paper, we present an approach for automatic trace generation and validation of traces between requirements (R) and architecture (A). Requirements relations and architecture verification techniques are used. A trace metamodel is defined with commonly used trace types between R&A. We use the semantics of traces and requirements relations for generating and validating traces with a tool support. The tool provides the following: (1) generation and validation of traces by using requirements relations and/or verification of architecture, (2) generation and validation of requirements relations by using traces. The tool is based on model transformation in ATL and term-rewriting logic in Maude.
88|-||A context awareness framework for cross-platform distributed applications|With the introduction of interconnected cross-platform middleware, a new area of opportunities for ubiquitous/pervasive computing has emerged. Context aware applications can be enhanced to practically and realistically incorporate multiple facets of human–machine interactions in everyday life that are not limited to a device-centered model for deducing context. In this paper, we propose that they can rather extend this model to a human-centered, device and platform independent model, based on a personal distributed application and data cloud ecosystem. For this to be achieved, webinos, a set of web runtime extensions that enable web applications and services to be used and shared consistently and securely over a broad spectrum of converged and connected devices, is used to provide this ecosystem. The webinos Context Awareness Framework described here is accessible to each webinos-enabled application. After strict policy enforcement, it can collect contextual information, either via an automatic mechanism that intercepts native calls made by webinos applications through the various webinos APIs, via an automatic polling mechanism to these APIs, or via custom, application-specific context schema extensions. It can then distribute the contextual information from its own personal cloud storage mechanism, in the form of simple, manageable and intuitive Context Objects, to and from all webinos-enabled devices owned by the same user, or even other, authorized users.
88|-||Recovering test-to-code traceability using slicing and textual analysis|Test suites are a valuable source of up-to-date documentation as developers continuously modify them to reflect changes in the production code and preserve an effective regression suite. While maintaining traceability links between unit test and the classes under test can be useful to selectively retest code after a change, the value of having traceability links goes far beyond this potential savings. One key use is to help developers better comprehend the dependencies between tests and classes and help maintain consistency during refactoring. Despite its importance, test-to-code traceability is not common in software development and, when needed, traceability information has to be recovered during software development and evolution. We propose an advanced approach, named SCOTCH+ (Source code and COncept based Test to Code traceability Hunter), to support the developer during the identification of links between unit tests and tested classes. Given a test class, represented by a JUnit class, the approach first exploits dynamic slicing to identify a set of candidate tested classes. Then, external and internal textual information associated with the classes retrieved by slicing is analyzed to refine this set of classes and identify the final set of candidate tested classes. The external information is derived from the analysis of the class name, while internal information is derived from identifiers and comments. The approach is evaluated on five software systems. The results indicate that the accuracy of the proposed approach far exceeds the leading techniques found in the literature.
88|-||Information centric services in Smart Cities|A “Smart City” is intended as an urban environment which, supported by pervasive ICT systems, is able to offer advanced and innovative services to citizens in order to improve the overall quality of their life. In this context, the present contribution formulates a pioneering proposal, by drawing an advanced information centric platform for supporting the typical ICT services of a Smart City. It can easily embrace all available and upcoming wireless technologies, while enforcing, at the same time, ubiquitous and secure applications in many domains, such as, e-government and public administration, intelligent transportation systems, public safety, social, health-care, educational, building and urban planning, environmental, and energy and water management applications. All the details of the proposed approach have been carefully described by means of pragmatical use-cases, such as the management of administrative procedures, the starting of a new business in a given country, the navigation assistance, the signaling of an urban accident aimed at improving the public safety, the reservation of a medical examination, the remote assistance of patients, and the management of waste in a city. This description makes evident the real effectiveness of the present proposal in future urban environments.
88|-||Software product line scoping and requirements engineering in a small and medium-sized enterprise: An industrial case study|Software product line (SPL) engineering has been applied in several domains, especially in large-scale software development. Given the benefits experienced and reported, SPL engineering has increasingly garnered interest from small to medium-sized companies. It is possible to find a wide range of studies reporting on the challenges of running a SPL project in large companies. However, very little reports exist that consider the situation for small to medium-sized enterprises and these studies try develop universal truths for SPL without lessons learned from empirical evidence need to be contextualized. This study is a step towards bridging this gap in contextual evidence by characterizing the weaknesses discovered in the scoping (SC) and requirements (RE) disciplines of SPL. Moreover, in this study we conducted a case study in a small to medium sized enterprises (SMEs) to justify the use of agile methods when introducing the SPL SC and RE disciplines through the characterization of their bottlenecks. The results of the characterization indicated that ineffective communication and collaboration, long iteration cycles, and the absence of adaptability and flexibility can increase the effort and reduce motivation during project development. These issues can be mitigated by agile methods.
88|-||An approach to testing commercial embedded systems|A wide range of commercial consumer devices such as mobile phones and smart televisions rely on embedded systems software to provide their functionality. Testing is one of the most commonly used methods for validating this software, and improved testing approaches could increase these devices’ dependability. In this article we present an approach for performing such testing. Our approach is composed of two techniques. The first technique involves the selection of test data; it utilizes test adequacy criteria that rely on dataflow analysis to distinguish points of interaction between specific layers in embedded systems and between individual software components within those layers, while also tracking interactions between tasks. The second technique involves the observation of failures: it utilizes a family of test oracles that rely on instrumentation to record various aspects of a system's execution behavior, and compare observed behavior to certain intended system properties that can be derived through program analysis. Empirical studies of our approach show that our adequacy criteria can be effective at guiding the creation of test cases that detect faults, and our oracles can help expose faults that cannot easily be found using typical output-based oracles. Moreover, the use of our criteria accentuates the fault-detection effectiveness of our oracles.
88|-||Visualizing protected variations in evolving software designs|Identifying and tracking evolving software structures at a design level is a challenging task. Although there are ways to visualize this information statically, there is a need for methods that help analyzing the evolution of software design elements. In this paper, we present a new visual approach to identify variability zones in software designs and explore how they evolve over time. To verify the usefulness of our approach, we did a user study in which participants had to browse software histories and find visual patterns. Most participants were able to find interesting observations and found our approach intuitive and useful. We present a number of design aspects that were observed by participants and the authors using our IHVis tool on four open-source projects.
88|-||Corrigendum to: âSPAPE: A semantic-preserving amorphous procedure extraction method for near-miss clonesâ: [J. Syst. Softw. 86 (2013) 2077â2093]|
89|-|http://www.sciencedirect.com/science/journal/01641212/89|Special issue on âTrustworthy Software Systems for the Digital Societyâ|
89|-||The social smart grid: Dealing with constrained energy resources through social coordination|The smart grid promises to improve the efficiency and reliability of tomorrow's energy supply. One of the biggest achievements of future smart grids will be their distributed mode of operation which effectively eliminates vulnerable nodes causing single points of failures in the grid. However, due to the lack of centralized energy production and control, the coordination of energy consumption becomes first priority. Because there do not exist technologies to store energy at large-scale yet, all energy that is required must be produced at the same time. The biggest challenge of energy producers is therefore to reliably predict and provide the right amount of required energy to avoid shortages and breakdowns. In this paper, we propose a novel way to let smart grid stakeholders, i.e., energy producers and consumers, coordinate their energy demands themselves. For that purpose we combine traditional social network models and service-oriented computing concepts with the smart grid to allow consumers to form communities according to their energy consumption behavior. These communities enable them to interact with other grid stakeholders to coordinate energy consumption plans and set up private energy sharing alliances. This way, the utility provider and industrial energy producers can rely on a better predictable and a smoother energy demand of customers. We introduce a software framework, making use of widely adopted standards, demonstrate its feasibility with an agent-based simulation, and discuss its overall applicability.
89|-||Workload-aware anomaly detection for Web applications|
89|-||Software trustworthiness 2.0âA semantic web enabled global source code analysis approach|There has been an ongoing trend toward collaborative software development using open and shared source code published in large software repositories on the Internet. While traditional source code analysis techniques perform well in single project contexts, new types of source code analysis techniques are ermerging, which focus on global source code analysis challenges. In this article, we discuss how the Semantic Web, can become an enabling technology to provide a standardized, formal, and semantic rich representations for modeling and analyzing large global source code corpora. Furthermore, inference services and other services provided by Semantic Web technologies can be used to support a variety of core source code analysis techniques, such as semantic code search, call graph construction, and clone detection. In this paper, we introduce SeCold, the first publicly available online linked data source code dataset for software engineering researchers and practitioners. Along with its dataset, SeCold also provides some Semantic Web enabled core services to support the analysis of Internet-scale source code repositories. We illustrated through several examples how this linked data combined with Semantic Web technologies can be harvested for different source code analysis tasks to support software trustworthiness. For the case studies, we combine both our linked-data set and Semantic Web enabled source code analysis services with knowledge extracted from StackOverflow, a crowdsourcing website. These case studies, we demonstrate that our approach is not only capable of crawling, processing, and scaling to traditional types of structured data (e.g., source code), but also supports emerging non-structured data sources, such as crowdsourced information (e.g., StackOverflow.com) to support a global source code analysis context.
89|-||Slice-based statistical fault localization|Recent techniques for fault localization statistically analyze coverage information of a set of test runs to measure the correlations between program entities and program failures. However, coverage information cannot identify those program entities whose execution affects the output and therefore weakens the aforementioned correlations. This paper proposes a slice-based statistical fault localization approach to address this problem. Our approach utilizes program slices of a set of test runs to capture the influence of a program entity's execution on the output, and uses statistical analysis to measure the suspiciousness of each program entity being faulty. In addition, this paper presents a new slicing approach called approximate dynamic backward slice to balance the size and accuracy of a slice, and applies this slice to our statistical approach. We use two standard benchmarks and three real-life UNIX utility programs as our subjects, and compare our approach with a sufficient number of fault localization techniques. The experimental results show that our approach can significantly improve the effectiveness of fault localization.
89|-||An evaluation model for dependability of Internet-scale software on basis of Bayesian Networks and trustworthiness|Internet-scale software becomes more and more important as a mode to construct software systems when Internet is developing rapidly. Internet-scale software comprises a set of widely distributed software entities which are running in open, dynamic and uncontrollable Internet environment. There are several aspects impacting dependability of Internet-scale software, such as technical, organizational, decisional and human aspects. It is very important to evaluate dependability of Internet-scale software by integrating all the aspects and analyzing system architecture from the most foundational elements. However, it is lack of such an evaluation model. An evaluation model of dependability for Internet-scale software on the basis of Bayesian Networks is proposed in this paper. The structure of Internet-scale software is analyzed. An evaluating system of dependability for Internet-scale software is established. It includes static metrics, dynamic metrics, prior metrics and correction metrics. A process of trust attenuation based on assessment is proposed to integrate subjective trust factors and objective dependability factors which impact on system quality. In this paper, a Bayesian Network is build according to the structure analysis. A bottom-up method that use Bayesian reasoning to analyses and calculate entity dependability and integration dependability layer by layer is described. A unified dependability of the whole system is worked out and is corrected by objective data. The analysis of experiment in a real system proves that the model in this paper is capable of evaluating the dependability of Internet-scale software clearly and objectively. Moreover, it offers effective help to the design, development, deployment and assessment of Internet-scale software.
89|-||GUI testing assisted by human knowledge: Random vs. functional|Software testing is a labor-intensive task in software development life-cycle. Human knowledge is useful in the practices of software testing, especially GUI testing. There are many strategies for GUI testing assisted by human knowledge, in which manual random testing and manual functional testing are two of widely used ones. In this paper, an empirical study is conducted to compare random testing and functional testing in order to provide guidelines for GUI testing. 234 participants were recruited to create thousands of random and functional test cases for open source GUI applications. Some of these test cases were selected with certain coverage criteria and then run on GUI applications to evaluate random testing and functional testing. We study three aspects on the two testing strategies: effectiveness, complementarity and impact of test case length. Some useful observations in the empirical study are: (1) Random testing is more effective in the early stage of testing on small applications and functional testing has more extensive applicability for testing large sized applications. (2) Random testing and functional testing exhibit some complementarity in our experiment. (3) Short test cases can reveal some faults more quickly and long test cases can reveal more faults lastingly.
89|-||A formal methodology for integral security design and verification of network protocols|In this work we propose a methodology for incorporating the verification of the security properties of network protocols as a fundamental component of their design. This methodology can be separated in two main parts: context and requirements analysis along with its informal verification; and formal representation of protocols and the corresponding procedural verification. Although the procedural verification phase does not require any specific tool or approach, automated tools for model checking and/or theorem proving offer a good trade-off between effort and results. In general, any security protocol design methodology should be an iterative process addressing in each step critical contexts of increasing complexity as result of the considered protocol goals and the underlying threats. The effort required for detecting flaws is proportional to the complexity of the critical context under evaluation, and thus our methodology avoids wasting valuable system resources by analyzing simple flaws in the first stages of the design process. In this work we provide a methodology in coherence with the step-by-step goals definition and threat analysis using informal and formal procedures, being our main concern to highlight the adequacy of such a methodology for promoting trust in the accordingly implemented communication protocols. Our proposal is illustrated by its application to three communication protocols: MANA III, WEP's Shared Key Authentication and CHAT-SRP.
89|-||Demand-based schedulability analysis for real-time multi-core scheduling|In real-time systems, schedulability analysis has been widely studied to provide offline guarantees on temporal correctness, producing many analysis methods. The demand-based schedulability analysis method has a great potential for high schedulability performance and broad applicability. However, such a potential is not yet fully realized for real-time multi-core scheduling mainly due to (i) the difficulty of calculating the resource demand under dynamic priority scheduling algorithms that are favorable to multi-cores, and (ii) the lack of understanding how to combine the analysis framework with deadline-miss conditions specialized for those scheduling algorithms. Addressing those two issues, to the best of our knowledge, this paper presents the first demand-based schedulability analysis for dynamic job-priority scheduling algorithms: EDZL (Earliest Deadline first until Zero-Laxity) and LLF (Least Laxity First), which are known to be effective for real-time multi-core scheduling. To this end, we first derive demand bound functions that compute the maximum possible amount of resource demand of jobs of each task while the priority of each job can change dynamically under EDZL and LLF. Then, we develop demand-based schedulability analyses for EDZL and LLF, by incorporating those new demand bound functions into the existing demand-based analysis framework. Finally, we combine the framework with additional deadline-miss conditions specialized for those two laxity-based dynamic job-priority scheduling algorithms, yielding tighter schedulability analyses. Via simulations, we demonstrate that the proposed schedulability analyses outperform the existing schedulability analyses for EDZL and LLF.
89|-||A reliability model for Service Component Architectures|Service-oriented applications are dynamically built by assembling existing, loosely coupled, distributed, and heterogeneous services. Predicting their reliability is very important to appropriately drive the selection and assembly of services, to evaluate design feasibility, to compare design alternatives, to identify potential failure areas and to maintain an acceptable reliability level under environmental extremes.
89|-||Sustainability of Open Source software communities beyond a fork: How and why has the LibreOffice project evolved?|Many organisations are dependent upon long-term sustainable software systems and associated communities. In this paper we consider long-term sustainability of Open Source software communities in Open Source software projects involving a fork. There is currently a lack of studies in the literature that address how specific Open Source software communities are affected by a fork. We report from a study aiming to investigate the developer community around the LibreOffice project, which is a fork from the OpenOffice.org project. In so doing, our analysis also covers the OpenOffice.org project and the related Apache OpenOffice project. The results strongly suggest a long-term sustainable LibreOffice community and that there are no signs of stagnation in the LibreOffice project 33 months after the fork. Our analysis provides details on developer communities for the LibreOffice and Apache OpenOffice projects and specifically concerning how they have evolved from the OpenOffice.org community with respect to project activity, developer commitment, and retention of committers over time. Further, we present results from an analysis of first hand experiences from contributors in the LibreOffice community. Findings from our analysis show that Open Source software communities can outlive Open Source software projects and that LibreOffice is perceived by its community as supportive, diversified, and independent. The study contributes new insights concerning challenges related to long-term sustainability of Open Source software communities.
89|-||Reviewing the quality of awareness support in collaborative applications|Awareness to users is a valuable feature of a collaborative system. Therefore, the designers of a system of this type may find it useful to receive hints on the awareness support provided by the system when it is under development or evolution. This paper proposes a tool for their use to obtain suggestions on the awareness features provided by the system and those not currently supported by it. The considered kinds of awareness were obtained from a review of a significant number of proposals from the literature. The tool is based on a checklist of design elements related to these awareness types to be applied by the application designer. The construction of this checklist was done as follows. The process started with an analysis of the types of awareness to be provided. This step ended with 54 selected design elements and six awareness types. Experts on the development of collaborative systems used their experience to provide correlations between the design elements and the types of awareness previously identified, thus encapsulating their expertise within the checklist. The proposal was applied to three existing collaborative systems and the results are presented. The obtained results suggest that the checklist is adequate to provide helpful hints that may be used to improve an application's awareness support.
89|-||Process fragmentation, distribution and execution using an event-based interaction scheme|The combination of service oriented architectures and business processes creates an enactment environment in which processes can be deployed and executed automatically. From a managerial and technical point of view, the interpretation, control and execution of a process flow happen very often at one point in the organizational and IT structure. This creates an inflexible environment in which control over and visibility of cross-departmental processes cannot be distributed across these organizational entities. Although the process model may need to be designed as a whole (to have an end-to-end definition), the actual execution of the process may need to be distributed across all participating partners. There are several ways to achieve this distribution. In this paper, we look at an event-based process deployment and execution infrastructure in which a process model can be automatically partitioned and distributed over different enactment entities, provided some given distribution definition. We compare the performance and flexibility of the proposed technique with other approaches and discuss the potential advantages and drawbacks of the event-based distribution.
89|-||A model view controller based Self-Adjusting Clustering Framework|A load balanced cluster is an abstraction for set of servers that are configured to share the workload. Each server of the cluster hosts the same set of applications or services. The process of application deployment is laborious and it is essential to efficiently manage a cluster. The existing techniques allow automation of the initial deployment and dynamic scaling. However, after the initial deployment, they do not ensure cluster members’ consistency. This is quite important, as change is inevitable in the life of a software application. A new application may need to be deployed to an existing cluster or an existing application may need to be upgraded. In this paper, we propose a Model View Controller based, Self-Adjusting Cluster Framework (SACF) that enables auto deployment, auto upgradation and cluster members’ consistency.
90|-|http://www.sciencedirect.com/science/journal/01641212/90|Special issue on Emerging Topics on Software Debugging|
90|-||HSFal: Effective fault localization using hybrid spectrum of full slices and execution slices|Most of the existing fault localization approaches use execution coverage of test cases to isolate the suspicious codes that likely contain faults. Program slicing can extract the dependencies of program entities with respect to a specific criterion. Therefore this technique is expected to have a beneficial effect on fault localization. In this paper, we propose a novel approach using a hybrid spectrum of full slices and execution slices to improve the effectiveness of fault localization. In particular, our approach firstly computes full slices of failed test cases and execution slices of passed test cases respectively. Secondly it constructs the hybrid spectrum by intersecting full slices and execution slices. Finally it computes the suspiciousness of each statement in the hybrid slice spectrum and generates a fault location report with descending suspiciousness of each statement. We also implement our proposed approach in our prototype tool HSFal by Java programming language. To verify the effectiveness of our approach, we performed an empirical study by the prototype on several widely used open source programs. Our approach is compared with eight representative coverage-based and slice-based fault localization approaches. Final experimental results show that our proposed approach is more effective in fault localization than other compared approaches, and can reduce almost 2.98–31.79% of the average cost of examined code significantly.
90|-||A dynamic code coverage approach to maximize fault localization efficiency|Spectrum-based fault localization is amongst the most effective techniques for automatic fault localization. However, abstractions of program execution traces, one of the required inputs for this technique, require instrumentation of the software under test at a statement level of granularity in order to compute a list of potential faulty statements. This introduces a considerable overhead in the fault localization process, which can even become prohibitive in, e.g., resource constrained environments. To counter this problem, we propose a new approach, coined dynamic code coverage (DCC), aimed at reducing this instrumentation overhead. This technique, by means of using coarser instrumentation, starts by analyzing coverage traces for large components of the system under test. It then progressively increases the instrumentation detail for faulty components, until the statement level of detail is reached. To assess the validity of our proposed approach, an empirical evaluation was performed, injecting faults in six real-world software projects. The empirical evaluation demonstrates that the dynamic code coverage approach reduces the execution overhead that exists in spectrum-based fault localization, and even presents a more concise potential fault ranking to the user. We have observed execution time reductions of 27% on average and diagnostic report size reductions of 77% on average.
90|-||An empirical study on the use of mutant traces for diagnosis of faults in deployed systems|Debugging deployed systems is an arduous and time consuming task. It is often difficult to generate traces from deployed systems due to the disturbance and overhead that trace collection may cause on a system in operation. Many organizations also do not keep historical traces of failures. On the other hand earlier techniques focusing on fault diagnosis in deployed systems require a collection of passing–failing traces, in-house reproduction of faults or a historical collection of failed traces. In this paper, we investigate an alternative solution. We investigate how artificial faults, generated using software mutation in test environment, can be used to diagnose actual faults in deployed software systems. The use of traces of artificial faults can provide relief when it is not feasible to collect different kinds of traces from deployed systems. Using artificial and actual faults we also investigate the similarity of function call traces of different faults in functions. To achieve our goal, we use decision trees to build a model of traces generated from mutants and test it on faulty traces generated from actual programs. The application of our approach to various real world programs shows that mutants can indeed be used to diagnose faulty functions in the original code with approximately 60–100% accuracy on reviewing 10% or less of the code; whereas, contemporary techniques using pass–fail traces show poor results in the context of software maintenance. Our results also show that different faults in closely related functions occur with similar function call traces. The use of mutation in fault diagnosis shows promising results but the experiments also show the challenges related to using mutants.
90|-||Combining mutation and fault localization for automated program debugging|This paper proposes a strategy for automatically fixing faults in a program by combining the ideas of mutation and fault localization. Statements ranked in order of their likelihood of containing faults are mutated in the same order to produce potential fixes for the faulty program. The proposed strategy is evaluated using 8 mutant operators against 19 programs each with multiple faulty versions. Our results indicate that 20.70% of the faults are fixed using selected mutant operators, suggesting that the strategy holds merit for automatically fixing faults. The impact of fault localization on efficiency of the overall fault-fixing process is investigated by experimenting with two different techniques, Tarantula and Ochiai, the latter of which has been reported to be better at fault localization than Tarantula, and also proves to be better in the context of fault-fixing using our proposed strategy. Further experiments are also presented to evaluate stopping criteria with respect to the mutant examination process and reveal that a significant fraction of the (fixable) faults can be fixed by examining a small percentage of the program code. We also report on the relative fault-fixing capabilities of mutant operators used and present discussions on future work.
90|-||Using SPIN for automated debugging of infinite executions of Java programs|This paper presents an approach for the automated debugging of reactive and concurrent Java programs, combining model checking and runtime monitoring. Runtime monitoring is used to transform the Java execution traces into the input for the model checker, the purpose of which is twofold. First, it checks these execution traces against properties written in linear temporal logic (LTL), which represent desirable or undesirable behaviors. Second, it produces several execution traces for a single Java program by generating test inputs and exploring different schedulings in multithreaded programs. As state explosion is the main drawback to model checking, we propose two abstraction approaches to reduce the memory requirements when storing Java states. We also present the formal framework to clarify which kinds of LTL safety and liveness formulas can be correctly analysed with each abstraction for both finite and infinite program executions. A major advantage of our approach comes from the model checker, which stores the trace of each failed execution, allowing the programmer to replay these executions to locate the bugs. Our current implementation, the tool TJT, uses Spin as the model checker and the Java Debug Interface (JDI) for runtime monitoring. TJT is presented as an Eclipse plug-in and it has been successfully applied to debug complex public Java programs.
90|-||Distributed debugging for mobile networks|Debuggers are an integral part, albeit often neglected, of the development of distributed applications. Ambient-oriented programming (AmOP) is a distributed paradigm for applications running on mobile ad hoc networks. In AmOP the complexity of programming in a distributed setting is married with the network fragility and open topology of mobile applications. To our knowledge, there is no debugging approach that tackles both these issues. In this paper we argue that a novel kind of distributed debugger that we term an ambient-oriented debugger, is required. We present REME-D (read as remedy), an online ambient-oriented debugger that integrates techniques from distributed debugging (event-based debugging, message breakpoints) and proposes facilities to deal with ad hoc, fragile networks – epidemic debugging, and support for frequent disconnections.
90|-||Exploiting the potential of DTN for energy-efficient internetworking|Since its original conception as a space-oriented communications architecture, Delay Tolerant Networking (DTN) has been generalized to address issues in terrestrial networks as well. In this work we employ DTN to form an internetworking overlay that exploits the surplus capacity of last hop wireless channels in order to prolong battery life for mobile networking devices. We propose a novel rendezvous mechanism and show experimentally that the DTN overlay can shape traffic, allowing the wireless interface of the mobile device to switch to the sleep state during idle intervals without degrading performance. The simulation experiments are based on a comprehensive DTN agent that incorporates the rendezvous mechanism and facilitates quantifying energy conservation. The DTN agent, implemented in ns-2, enables the study of additional Bundle Protocol design issues, such as route selection, bundle sizing, retransmission strategies, and highlights the need for cross-layer interaction between DTN and the underlying transport protocol.
90|-||Evolutionary instance selection for text classification|Text classification is usually based on constructing a model through learning from training examples to automatically classify text documents. However, as the size of text document repositories grows rapidly, the storage requirement and computational cost of model learning become higher. Instance selection is one solution to solve these limitations whose aim is to reduce the data size by filtering out noisy data from a given training dataset. In this paper, we introduce a novel algorithm for these tasks, namely a biological-based genetic algorithm (BGA). BGA fits a “biological evolution” into the evolutionary process, where the most streamlined process also complies with the reasonable rules. In other words, after long-term evolution, organisms find the most efficient way to allocate resources and evolve. Consequently, we can closely simulate the natural evolution of an algorithm, such that the algorithm will be both efficient and effective. The experimental results based on the TechTC-100 and Reuters-21578 datasets show the outperformance of BGA over five state-of-the-art algorithms. In particular, using BGA to select text documents not only results in the largest dataset reduction rate, but also requires the least computational time. Moreover, BGA can make the k-NN and SVM classifiers provide similar or slightly better classification accuracy than GA.
90|-||Uncertainty handling in goal-driven self-optimization â Limiting the negative effect on adaptation|Goal-driven self-optimization through feedback loops has shown effectiveness in reducing oscillating utilities due to a large number of uncertain factors in the runtime environments. However, such self-optimization is less satisfactory when there contains uncertainty in the predefined requirements goal models, such as imprecise contributions and unknown quality preferences, or during the switches of goal solutions, such as lack of understanding about the time for the adaptation actions to take effect. In this paper, we propose to handle such uncertainty in goal-driven self-optimization without interrupting the services. Taking the monitored quality values as the feedback, and the estimated earned value as the global indicator of self-optimization, our approach dynamically updates the quantitative contributions from alternative functionalities to quality requirements, tunes the preferences of relevant quality requirements, and determines a proper timing delay for the last adaptation action to take effect. After applying these runtime measures to limit the negative effect of the uncertainty in goal models and their suggested switches, an experimental study on a real-life online shopping system shows the improvements over goal-driven self-optimization approaches without uncertainty handling.
90|-||Power-aware fixed priority scheduling for sporadic tasks in hard real-time systems|In this paper, we consider the generalized power model in which the focus is the dynamic power and the static power, and we study the problem of the canonical sporadic task scheduling based on the rate-monotonic (RM) scheme. Moreover, we combine with the dynamic voltage scaling (DVS) and dynamic power management (DPM). We present a static low power sporadic tasks scheduling algorithm (SSTLPSA), assuming that each task presents its worst-case work-load to the processor at every instance. In addition, a more energy efficient approach called a dynamic low power sporadic tasks scheduling algorithm (DSTLPSA) is proposed, based on reclaiming the dynamic slack and adjusting the speed of other tasks on-the-fly in order to reduce energy consumption while still meeting the deadlines. The experimental results show that the SSTLPSA algorithm consumes 26.55–38.67% less energy than that of the RM algorithm and the DSTLPSA algorithm reduces the energy consumption up to 18.38–30.51% over the existing DVS algorithm.
90|-||SecureSMS: A secure SMS protocol for VAS and other applications|Nowadays, the SMS is a very popular communication channel for numerous value added services (VAS), business and commercial applications. Hence, the security of SMS is the most important aspect in such applications. Recently, the researchers have proposed approaches to provide end-to-end security for SMS during its transmission over the network. Thus, in this direction, many SMS-based frameworks and protocols like Marko's SMS framework, Songyang's SMS framework, Alfredo's SMS framework, SSMS protocol, and, Marko and Konstantin's protocol have been proposed but these frameworks/protocols do not justify themselves in terms of security analysis, communication and computation overheads, prevention from various threats and attacks, and the bandwidth utilization of these protocols. The two protocols SMSSec and PK-SIM have also been proposed to provide end-to-end security and seem to be little better in terms of security analysis as compared to the protocols/framework mentioned above. In this paper, we propose a new secure and optimal protocol called SecureSMS, which generates less communication and computation overheads. We also discuss the possible threats and attacks in the paper and provide the justified prevention against them. The proposed protocol is also better than the above two protocols in terms of the bandwidth utilization. On an average the SecureSMS protocol reduces 71% and 59% of the total bandwidth used in the authentication process as compared to the SMSSec and PK-SIM protocols respectively. Apart from this, the paper also proposes a scheme to store and implement the cryptographic algorithms onto the SIM card. The proposed scheme provides end-to-end SMS security with authentication (by the SecureSMS protocol), confidentiality (by encryption AES/Blowfish; preferred AES-CTR), integrity (SHA1/MD5; preferred SHA1) and non-repudiation (ECDSA/DSA; preferred ECDSA).
90|-||A recommendation framework for remote sensing images by spatial relation analysis|In recent years, Remote Sensing Images (RS-Images) are widely recognized as an essential geospatial data due to their superior ability to offer abundant and instantaneous ground truth information. One of the active RS-Image approaches is the RS-Image recommendation from the Internet for meeting the user's queried Area-of-Interest (AOI). Although a number of studies on RS-Image ranking and recommendation have been proposed, most of them only consider the spatial distance between RS-Image and AOI. It is inappropriate since both of the RS-Image and AOI not only have the spatial information but also the cover range information. In this paper, we propose a novel framework named Location-based rs-Image Finding Engine (LIFE) to rank and recommend a series of relevant RS-Images to users according to the user-specific AOI. In LIFE, we first propose a cluster-based RS-Image index structure to efficiently maintain the large amount of RS-Images. Then, two quantitative indicators named Available Space (AS) and Image Extension (IE) are proposed to measure the Extensibility and Centrality between RS-Image and AOI, respectively. To our best knowledge, this is the first work on RS-Image recommendation that considers the issues of extensibility and centrality simultaneously. Through comprehensive experimental evaluations, the experiment result shows that both indicators have their own distinguished ranking behaviors and are able to successfully recommend meaningful RS-Image results. Besides, the experimental results show that the proposed LIFE framework outperforms the state-of-the-art approach Hausdorff in terms of Precision, Recall and Normalized Discounted Cumulative Gain (NDCG).
90|-||Radigost: Interoperable web-based multi-agent platform|Recent improvements of web development technologies, commonly referred to as HTML5, have resulted in an excellent framework for developing a fully-featured, purely web-based multi-agent platform. This paper presents an architecture of such a platform, named Radigost. Radigost agents and parts of the system itself are implemented in JavaScript and executed inside the client's web browser, while an additional set of Java-based components is deployed on an enterprise application server. Radigost is platform-independent, capable of running, without any prior installation or configuration steps, on a wide variety of software and hardware configurations, including personal computers, smartphones, tablets, and modern television sets. The system is standards-compliant and fully interoperable, in the sense that its agents can transparently interact with agents in existing, third-party multi-agent solutions. Finally, performance evaluation results show that the execution speed of Radigost is comparable to that of a non web-based implementation.
90|-||Real-time data dissemination in mobile peer-to-peer networks|Mobile peer-to-peer networks have found many uses such as streaming of audio and video data. There are circumstances, such as emergency situations and disaster recovery, when real-time delivery is a fundamental requirement. The problem is challenging due to the limited network capacity, the variable transmission rates and the unpredictability with respect to the network conditions in the mobile peer-to-peer network.
90|-||Accurate sub-swarms particle swarm optimization algorithm for service composition|Service composition (SC) generates various composite applications quickly by using a novel service interaction model. Before composing services together, the most important thing is to find optimal candidate service instances compliant with non-functional requirements. Particle swarm optimization (PSO) is known as an effective and efficient algorithm, which is widely used in this process. However, the premature convergence and diversity loss of PSO always results in suboptimal solutions. In this paper, we propose an accurate sub-swarms particle swarm optimization (ASPSO) algorithm by adopting parallel and serial niching techniques. The ASPSO algorithm locates optimal solutions by using sub-swarms searching grid cells in which the density of feasible solutions is high. Simulation results demonstrate that the proposed algorithm improves the accuracy of the standard PSO algorithm in searching the optimal solution of service selection problem.
||||
volume|issue|url|title|abstract
91|-|http://www.sciencedirect.com/science/journal/01641212/91|Variability in software architecture â State of the art|
91|-||An overview of Dynamic Software Product Line architectures and techniques: Observations from research and industry|Over the last two decades, software product lines have been used successfully in industry for building families of systems of related products, maximizing reuse, and exploiting their variable and configurable options. In a changing world, modern software demands more and more adaptive features, many of them performed dynamically, and the requirements on the software architecture to support adaptation capabilities of systems are increasing in importance. Today, many embedded system families and application domains such as ecosystems, service-based applications, and self-adaptive systems demand runtime capabilities for flexible adaptation, reconfiguration, and post-deployment activities. However, as traditional software product line architectures fail to provide mechanisms for runtime adaptation and behavior of products, there is a shift toward designing more dynamic software architectures and building more adaptable software able to handle autonomous decision-making, according to varying conditions. Recent development approaches such as Dynamic Software Product Lines (DSPLs) attempt to face the challenges of the dynamic conditions of such systems but the state of these solution architectures is still immature. In order to provide a more comprehensive treatment of DSPL models and their solution architectures, in this research work we provide an overview of the state of the art and current techniques that, partially, attempt to face the many challenges of runtime variability mechanisms in the context of Dynamic Software Product Lines. We also provide an integrated view of the challenges and solutions that are necessary to support runtime variability mechanisms in DSPL models and software architectures.
91|-||Dynamic adaptation of service compositions with variability models|Web services run in complex contexts where arising events may compromise the quality of the whole system. Thus, it is desirable to count on autonomic mechanisms to guide the self-adaptation of service compositions according to changes in the computing infrastructure. One way to achieve this goal is by implementing variability constructs at the language level. However, this approach may become tedious, difficult to manage, and error-prone. In this paper, we propose a solution based on a semantically rich variability model to support the dynamic adaptation of service compositions. When a problematic event arises in the context, this model is leveraged for decision-making. The activation and deactivation of features in the variability model result in changes in a composition model that abstracts the underlying service composition. These changes are reflected into the service composition by adding or removing fragments of Business Process Execution Language (WS-BPEL) code, which can be deployed at runtime. In order to reach optimum adaptations, the variability model and its possible configurations are verified at design time using Constraint Programming. An evaluation demonstrates several benefits of our approach, both at design time and at runtime.
91|-||Efficient customization of multi-tenant Software-as-a-Service applications with service lines|Application-level multi-tenancy is an architectural approach for Software-as-a-Service (SaaS) applications which enables high operational cost efficiency by sharing one application instance among multiple customer organizations (the so-called tenants). However, the focus on increased resource sharing typically results in a one-size-fits-all approach. In principle, the shared application instance satisfies only the requirements common to all tenants, without supporting potentially different and varying requirements of these tenants. As a consequence, multi-tenant SaaS applications are inherently limited in terms of flexibility and variability.
91|-||Delta-oriented model-based integration testing of large-scale systems|Software architecture specifications are of growing importance for coping with the complexity of large-scale systems. They provide an abstract view on the high-level structural system entities together with their explicit dependencies and build the basis for ensuring behavioral conformance of component implementations and interactions, e.g., using model-based integration testing. The increasing inherent diversity of such large-scale variant-rich systems further complicates quality assurance. In this article, we present a combination of architecture-driven model-based testing principles and regression-inspired testing strategies for efficient, yet comprehensive variability-aware conformance testing of variant-rich systems. We propose an integrated delta-oriented architectural test modeling and testing approach for component as well as integration testing that allows the generation and reuse of test artifacts among different system variants. Furthermore, an automated derivation of retesting obligations based on accurate delta-oriented architectural change impact analysis is provided. Based on a formal conceptual framework that guarantees stable test coverage for every system variant, we present a sample implementation of our approach and an evaluation of the validity and efficiency by means of a case study from the automotive domain.
91|-||End-user development by application-domain configuration|An application generator/tailoring tool aimed at end users is described. It employs conceptual models of problem domains to drive configuration of an application generator suitable for a related set of applications, such as reservation and resource allocation. The tool supports a two-phase approach of configuring the general architecture for a domain, such as reservation-booking problems, then customisation and generation of specific applications. The tool also provides customisable natural language-style queries for spatial and temporal terms. Development and use of the tool to generate two applications, service engineer call allocation, and airline seat reservation, are reported with a specification exercise to configure the generic architecture to a new problem domain for monitoring-sensing applications. The application generator/tailoring tool is evaluated with novice end users and experts to demonstrate its effectiveness.
91|-||Improved anti-forensics of JPEG compression|The comblike histogram of DCT coefficients on each subband and the blocking artifacts among adjacent blocks are the two main fingerprints of the image that was once compressed by JPEG. Stamm and Liu proposed an anti-forensics method for removing these fingerprints by dithering the DCT coefficients and adding noise into the pixels. However, some defects emerge inside the anti-forensically processed images. First, the noise distributions are abnormal in the resulting images; and second, the quality of the processed image is poor compared with the original image. To fill these gaps, this paper proposes an improved anti-forensics method for JPEG compression. After analyzing the noise distribution, we propose a denoising algorithm to remove the grainy noise caused by image dithering, and a deblocking algorithm to combat Fan and Queiroz's forensics method against blocking artifacts. With the proposed anti-forensics method, fingerprints of the comblike histograms and the blocking artifacts are removed, noise distribution abnormality is avoided, and the quality of the processed image is improved.
91|-||A methodology to automatically optimize dynamic memory managers applying grammatical evolution|Modern consumer devices must execute multimedia applications that exhibit high resource utilization. In order to efficiently execute these applications, the dynamic memory subsystem needs to be optimized. This complex task can be tackled in two complementary ways: optimizing the application source code or designing custom dynamic memory management mechanisms. Currently, the first approach has been well established, and several automatic methodologies have been proposed. Regarding the second approach, software engineers often write custom dynamic memory managers from scratch, which is a difficult and error-prone work. This paper presents a novel way to automatically generate custom dynamic memory managers optimizing both performance and memory usage of the target application. The design space is pruned using grammatical evolution converging to the best dynamic memory manager implementation for the target application. Our methodology achieves important improvements (62.55% and 30.62% better on average in performance and memory usage, respectively) when its results are compared to five different general-purpose dynamic memory managers.
91|-||Cooperation, collaboration and pair-programming: Field studies on backup behavior|Considering that pair programming has been extensively studied for more than a decade, it can seem quite surprising that there is such a lack of consensus on both its best use and its benefits. We argue that pair programming is not a replacement of usual developer interactions, but rather a formalization and enhancement of naturally occurring interactions. Consequently, we study and classify a broader range of developer interactions, evaluating them for type, purpose and patterns of occurrence, with the aim to identify situations in which pair programming is likely to be truly needed and thus most beneficial. We study the concrete pair programming practices in both academic and industrial settings. All interactions between teammates were recorded as backup behavior activities. In each of these two projects, developers were free to interact when needed. All team interactions were self-recorded by the teammates. The analysis of the interaction tokens shows two salient features: solo work is an important component of teamwork and team interactions have two main purposes, namely cooperation and collaboration. Cooperative backup behavior occurs when a developer provides help to a teammate. Collaborative backup behavior occurs when the teammates are sharing the same goal toward solving an issue. We found that collaborative backup behavior, which occurred much less often, is close to the formal definition of pair programming. This study suggests that mandatory pair programming may be less efficient in organizations where solo work could be done and when some interactions are for cooperative activities. Based on these results, we discussed the potential implications concerning the best use of pair programming in practice, a more effective evaluation of its use, its potential benefits and emerging directions of future research.
91|-||Existence of dumb nodes in stationary wireless sensor networks|Wireless sensor networks (WSNs), which are typically autonomous and unattended, require energy-efficient and fault-tolerant protocols to maximize the network lifetime and operations. In this work, we consider a previously unexplored aspect of the sensing nodes – dumb behavior. A sensor node is termed as “dumb”, when it can sense its surroundings, but cannot communicate with its neighbors due to shrinkage in communication range attributed to adverse environmental effects and can behave normally in the presence of favorable environment. As a result of this temporary behavior, a node may get isolated from the network when adverse environmental effects are present, but re-connects with the network with the resumption of favorable environmental conditions. We consider the effects of dumb nodes on the, otherwise, energy-efficient stationary WSNs having complete network coverage achieved using sufficient number of activated sensor nodes. While the presence of redundancy in the deployment of nodes, or the number of active nodes can guarantee communication opportunities, such deployment is not necessarily energy-efficient and cost-effective. The dumb behavior of nodes results in wastage of power, thereby reducing the lifetime of a network. Such effects can be detrimental to the performance of WSN applications. The simulation results exhibit that the network performance degrades in the presence of dumb nodes in stationary WSNs.
91|-||Predictable integration and reuse of executable real-time components|We present the concept of runnable virtual node (RVN) as a means to achieve predictable integration and reuse of executable real-time components in embedded systems. A runnable virtual node is a coarse-grained software component that provides functional and temporal isolation with respect to its environment. Its interaction with the environment is bounded both by a functional and a temporal interface, and the validity of its internal temporal behaviour is preserved when integrated with other components or when reused in a new environment. Our realization of RVN exploits the latest techniques for hierarchical scheduling to achieve temporal isolation, and the principles from component-based software-engineering to achieve functional isolation. It uses a two-level deployment process, i.e. deploying functional entities to RVNs and then deploying RVNs to physical nodes, and thus also gives development benefits with respect to composability, system integration, testing, and validation. In addition, we have implemented a server-based inter-RVN communication strategy to not only support the predictable integration and reuse properties of RVNs by keeping the communication code in a separate server, but also increasing the maintainability and flexibility to change the communication code without affecting the timing properties of RVNs. We have applied our approach to a case study, implemented in the ProCom component technology executing on top of a FreeRTOS-based hierarchical scheduling framework and present the results as a proof-of-concept.
91|-||An efficient design and validation technique for secure handover between 3GPP LTE and WLANs systems|Future generations wireless systems, which integrate different wireless access networks together, will support a secured seamless mobility and a wide variety of applications and services with different quality of service (QoS) requirements. Most of the existing re-authentication protocols during vertical handover still have certain limitations such as man in the middle, eavesdropping and session hijacking attacks, and unacceptable delay for real time applications. In this article, we propose two re-authentication schemes to secure handover between 3GPP LTE and WLANs systems: Initial Handover Re-authentication Protocol, and Local Re-authentication Protocol. The second proposed protocol is executed locally in a WLAN network without contacting the authentication server of the home network for credentials verification. In fact, after a successful execution of the Initial Handover Re-authentication Protocol, the local key (LK) is shared between USIM and the authentication server of the WLAN. It is then used for securing handover and traffic in WLAN networks. Performance evaluation results obtained using simulation analysis show that the proposed re-authentication protocol enhances handover parameters such as handover latency, handover blocking rate and packet loss rate. Additionally, the proposed enhanced fast re-authentication protocol has been modeled and verified using the software AVISPA and is found to be safe.
91|-||Web application testing: A systematic literature review|The web has had a significant impact on all aspects of our society. As our society relies more and more on the web, the dependability of web applications has become increasingly important. To make these applications more dependable, for the past decade researchers have proposed various techniques for testing web-based software applications. Our literature search for related studies retrieved 193 papers in the area of web application testing, which have appeared between 2000 and 2013.
92|-|http://www.sciencedirect.com/science/journal/01641212/92|Introduction to the special issue on middleware for mobile data management|
92|-||Programming mobile context-aware applications with TOTAM|In tuple space approaches to context-aware mobile systems, the notion of context is defined by the presence or absence of certain tuples in the tuple space. Existing approaches define such presence either by collocation of devices holding the tuples or by replication of tuples across all devices. We show that both approaches can lead to an erroneous perception of context. Collocation ties the perception of context to network connectivity which does not always yield the expected result. Tuple replication can cause that a certain context is perceived even if the device has left the context a long time ago. We propose a tuple space approach in which tuples themselves carry a predicate that determines whether they are in the right context or not. We present a practical API for our approach and show its use by means of the implementation of various mobile applications. Benchmarks show that our approach can lead to a significant increase in performance compared to other approaches.
92|-||Mosco: a privacy-aware middleware for mobile social computing|The proliferation of mobile devices coupled with Internet access is generating a tremendous amount of highly personal and sensitive data. Applications such as location-based services and quantified self harness such data to bring meaningful context to users’ behavior. As social applications are becoming prevalent, there is a trend for users to share their mobile data. The nature of online social networking poses new challenges for controlling access to private data, as compared to traditional enterprise systems. First, the user may have a large number of friends, each associated with a unique access policy. Second, the access control policies must be dynamic and fine-grained, i.e. they are content-based, as opposed to all-or-nothing. In this paper, we investigate the challenges in sharing of mobile data in social applications. We design and evaluate a middleware running on Google App Engine, named Mosco, that manages and facilitates sharing of mobile data in a privacy-preserving manner. We use Mosco to develop a location sharing and a health monitoring application. Mosco helps shorten the development process. Finally, we perform benchmarking experiments with Mosco, the results of which indicate small overhead and high scalability.
92|-||SelfMotion: A declarative approach for adaptive service-oriented mobile applications|Modern society increasingly relies on mobile devices. This explains the growing demand for high quality software for such devices. To improve the efficiency of the development life-cycle, shortening time-to-market while keeping quality under control, mobile applications are typically developed by composing together ad-hoc developed components, services available on-line, and other third-party mobile applications. Applications are thus built as heterogeneous compositions, whose characteristics strongly depend on the components and services they integrate. To cope with unpredictable changes and failures, but also with the various settings offered by the plethora of available devices, mobile applications need to be as adaptive as possible. However, mainstream adaptation strategies are usually defined imperatively and require complex control strategies strongly intertwined with the application logic, yielding to applications that are difficult to build, maintain, and evolve. We address this issue by proposing a declarative approach to compose adaptive heterogeneous mobile applications. The advantages of this approach are demonstrated through an example inspired by an existing worldwide distributed mobile application, while the implementation of the proposed solution has been validated through a set of simulations and experiments aimed at illustrating its performance.
92|-||Top-k query processing for replicated data in mobile peer to peer networks|In mobile ad hoc peer to peer (M-P2P) networks, since nodes are highly resource constrained, it is effective to retrieve data items using a top-k query, in which data items are ordered by the score of a particular attribute and the query-issuing node acquires data items with the k highest scores. However, when network partitioning occurs, the query-issuing node cannot connect to some nodes having data items included in the top-k query result, and thus, the accuracy of the query result decreases. To solve this problem, data replication is a promising approach. However, if each node sends back its own data items (replicas) responding to a query without considering replicas held by others, same data items are sent back to the query-issuing node more than once through long paths, which results in increase of traffic. In this paper, we propose a top-k query processing method considering data replication in M-P2P networks. This method suppresses duplicate transmissions of same data items through long paths. Moreover, an intermediate node stops transmitting a query message on-demand.
92|-||Programmable context awareness framework|Context-awareness enables applications to provide end-users with a richer experience by enhancing their interactions with contextual information. Several frameworks have already been proposed to simplify the development of context-aware applications. These frameworks are focused on provisioning context data and on providing common semantics, definitions and representations of these context data. They assume that applications share the same semantic, which limits the range of use cases where a framework can be used, as that assumption induces a strong coupling between context management and application logic. This article proposes a framework that decouples context management from application business logic. The aim is to reduce the overhead on applications that run on resource-limited devices while still providing mechanisms to support context-awareness and behavior adaptation. The article presents an innovative approach that involves third-parties in context processing definition by structuring it using atomic functions. These functions can be designed by third-party developers using an XML-based programming language. Its implementation and evaluation demonstrates the benefits, in terms of flexibility, of using proven design patterns from software engineering for developing context-aware application.
92|-||Towards an ideal service QoS in fuzzy logic-based adaptation planning middleware|Mobile applications require an adaptation phase to adapt to the user's and application context. Utility functions or rules are most often used to make the adaptation planning or decision, i.e. select the most adapted variant for each required service. Fuzzy controllers are used when it is difficult or even impossible to construct precise mathematical models. In the case of mobile applications, the large number of Quality of Service (QoS) and context parameters causes an exponential increase in the number of rules (aka. rule explosion problem), that increases the processing time of the adaptation planning. To reduce the processing time and simplify the fuzzy control system, we propose the concept of ideal QoS. Fuzzy values of ideal QoS parameters are calculated using several fuzzy control systems to fit the context state and user preferences. A fuzzy logic similarity metric based on fuzzy sets and fuzzy operators is proposed to select the service variant having the nearest QoS values to the ideal. Experiments show that our approach can significantly improve both the number of rules and the processing time when selecting the variant that well adapts to environment changes.
92|-||Mobile Cloud Middleware|Mobile Cloud Computing (MCC) is arising as a prominent research area that is seeking to bring the massive advantages of the cloud to the constrained smartphones. Mobile devices are looking towards cloud-aware techniques, driven by their growing interest to provide ubiquitous PC-like functionality to mobile users. These functionalities mainly target at increasing storage and computational capabilities. Smartphones may integrate those functionalities from different cloud levels, in a service oriented manner within the mobile applications, so that a mobile task can be delegated by direct invocation of a service. However, developing these kind of mobile cloud applications requires to integrate and consider multiple aspects of the clouds, such as resource-intensive processing, programmatically provisioning of resources (Web APIs) and cloud intercommunication. To overcome these issues, we have developed a Mobile Cloud Middleware (MCM) framework, which addresses the issues of interoperability across multiple clouds, asynchronous delegation of mobile tasks and dynamic allocation of cloud infrastructure. MCM also fosters the integration and orchestration of mobile tasks delegated with minimal data transfer. A prototype of MCM is developed and several applications are demonstrated in different domains. To verify the scalability of MCM, load tests are also performed on the hybrid cloud resources. The detailed performance analysis of the middleware framework shows that MCM improves the quality of service for mobiles and helps in maintaining soft-real time responses for mobile cloud applications.
92|-||A cross-layer middleware for context-aware cooperative application on mobile ad hoc peer-to-peer network|Mobile ad hoc peer-to-peer (P2P) applications become popular for providing the file sharing, voice communicating, and video streaming services due to entertainments and disaster recovery. However, both the topology of wireless network and the overlay of P2P network are dynamic, so the middleware is proposed to integrate such architectures of service-oriented applications. Therefore, we propose context-aware cooperative application (CACA) to overcome the frequent churn and high mobility problems. CACA proposes a cross-layer middleware to integrate DHT-based lookup, anycast query, and P2P delivery via the IPv6 routing header. Through anycast query, the response delay can be shortened and the query duplication can be minimized. Via IPv6 routing header, the delivery efficiency can be improved. Through the cross-layer design, the finger table in overlay layer is combined with the routing table in network layer to heighten proximity. The simulation results demonstrate that CACA has the outstanding performances of short download delay, high playback continuity, and low signaling overhead in mobile ad hoc network.
92|-||A secure Boolean-based multi-secret image sharing scheme|An (n, n) multi-secret image sharing scheme shares n secret images among n shared images. In this type of schemes, n shared images can be used to recover all n secret images, but the loss of any shared image prevents the recovery of any secret image. Among existing image sharing techniques, Boolean-based secret schemes have good performance because they only require XOR calculation. This study presents a secure Boolean-based secret image sharing scheme that uses a random image generating function to generate a random image from secret images or shared images. The proposed function efficiently increases the sharing capacity on free of sharing the random image. The use of a bit shift subfunction in the random image generating function produces a random image to meet the random requirement. Experimental results show that the proposed scheme requires minimal CPU computation time to share or recover secret images. The time required to share n secret images is nearly the time as that required to recover n secret images. The bit shift subfunction takes more computation load than the XOR subfunction needs.
92|-||Factors that motivate software engineering teams: A four country empirical study|Motivation, although difficult to quantify, is considered to be the single largest factor in developer productivity; there are also suggestions that low motivation is an important factor in software development project failure. We investigate factors that motivate software engineering teams using survey data collected from software engineering practitioners based in Australia, Chile, USA and Vietnam. We also investigate the relationship between team motivation and project outcome, identifying whether the country in which software engineering practitioners are based affects this relationship. Analysis of 333 questionnaires indicates that failed projects are associated with low team motivation. We found a set of six common team motivational factors that appear to be culturally independent (project manager has good communication with project staff, project risks reassessed, controlled and managed during the project, customer has confidence in the project manager and the development team, the working environment is good, the team works well together, and the software engineer had a pleasant experience). We also found unique groupings of team motivational factors for each of the countries investigated. This indicates that there are cultural differences that project managers need to consider when working in a global environment.
92|-||Architecture for embedded open software ecosystems|Software is prevalent in embedded products and may be critical for the success of the products, but manufacturers may view software as a necessary evil rather than as a key strategic opportunity and business differentiator. One of the reasons for this can be extensive supplier and subcontractor relationships and the cost, effort or unpredictability of the deliverables from the subcontractors are experienced as a major problem.
92|-||A MIH-based approach for best network selection in heterogeneous wireless networks|In the next generation wireless networks, different technologies belonging to one or more operators should be integrated to form a heterogeneous environment based on an IP core network infrastructure. This ensures user mobility and service continuity by maintaining connections when switching between various technologies and it introduces new resources and possibilities for applications. In this context, an automatic interface selection based on instantaneous and practical constraints and user preferences (Quality of Service (QoS) parameters, available resources, security, power consumption, etc.) is therefore required. The different network selection and handover schemes proposed in the literature can be classified into three approaches according to who is responsible for making the handover decision: the terminal, the network or by a cooperation between both of them. However, these approaches keep presenting some drawbacks; namely the problem of resources management and network load balancing whenever the selection is controlled by the mobile terminal (MT) and the problem of scalability and unknown operator's management policy whenever the selection is rather controlled by the network.
92|-||Failure factors of small software projects at a global outsourcing marketplace|The presented study aims at a better understanding of when and why small-scale software projects at a global outsourcing marketplace fail. The analysis is based on a data set of 785,325 projects/tasks completed at vWorker.com. A binary logistic regression model relying solely on information known at the time of a project's start-up correctly predicted 74% of the project failures and 67% of the non-failures. The model-predicted failure probability corresponded well with the actual frequencies of failures for most levels of failure risk. The model suggests that the factors connected to the strongest reduction in the risk of failure are related to previous collaboration between the client and the provider and a low failure rate of previous projects completed by the provider. We found the characteristics of the client to be almost as important as those of the provider in explaining project failures and that the risk of project failure increased with an increased client emphasis on low price and with an increased project size. The identified relationships seem to be reasonable stable across the studied project size categories.
92|-||Privacy-preserving computation of participatory noise maps in the cloud|This paper presents a privacy-preserving system for participatory sensing, which relies on cryptographic techniques and distributed computations in the cloud. Each individual user is represented by a personal software agent, deployed in the cloud, where it collaborates on distributed computations without loss of privacy, including with respect to the cloud service providers. We present a generic system architecture involving a cryptographic protocol based on a homomorphic encryption scheme for aggregating sensing data into maps, and demonstrate security in the Honest-But-Curious model both for the users and the cloud service providers. We validate our system in the context of NoiseTube, a participatory sensing framework for noise pollution, presenting experiments with real and artificially generated data sets, and a demo on a heterogeneous set of commercial cloud providers. To the best of our knowledge our system is the first operational privacy-preserving system for participatory sensing. While our validation pertains to the noise domain, the approach used is applicable in any crowd-sourcing application relying on location-based contributions of citizens where maps are produced by aggregating data – also beyond the domain of environmental monitoring.
93|-|http://www.sciencedirect.com/science/journal/01641212/93|On the verification of UML/OCL class diagrams using constraint programming|Assessment of the correctness of software models is a key issue to ensure the quality of the final application. To this end, this paper presents an automatic method for the verification of UML class diagrams extended with OCL constraints. Our method checks compliance of the diagram with respect to several correctness properties including weak and strong satisfiability or absence of constraint redundancies among others. The method works by translating the UML/OCL model into a Constraint Satisfaction Problem (CSP) that is evaluated using state-of-the-art constraint solvers to determine the correctness of the initial model. Our approach is particularly relevant to current MDA and MDD methods where software models are the primary artifacts of the development process and the basis for the (semi-)automatic code-generation of the final application.
93|-||Predicting software defects with causality tests|In this paper, we propose a defect prediction approach centered on more robust evidences towards causality between source code metrics (as predictors) and the occurrence of defects. More specifically, we rely on the Granger causality test to evaluate whether past variations in source code metrics values can be used to forecast changes in time series of defects. Our approach triggers alarms when changes made to the source code of a target system have a high chance of producing defects. We evaluated our approach in several life stages of four Java-based systems. We reached an average precision greater than 50% in three out of the four systems we evaluated. Moreover, by comparing our approach with baselines that are not based on causality tests, it achieved a better precision.
93|-||From AADL to Timed Abstract State Machines: A verified model transformation|Architecture Analysis and Design Language (AADL) is an architecture description language standard for embedded real-time systems widely used in the avionics and aerospace industry to model safety-critical applications. To verify and analyze the AADL models, model transformation technologies are often used to automatically extract a formal specification suitable for analysis and verification. In this process, it remains a challenge to prove that the model transformation preserves the semantics of the initial AADL model or, at least, some of the specific properties or requirements it needs to satisfy. This paper presents a machine checked semantics-preserving transformation of a subset of AADL (including periodic threads, data port communications, mode changes, and the AADL behavior annex) into Timed Abstract State Machines (TASM). The AADL standard itself lacks at present a formal semantics to make this translation validation possible. Our contribution is to bridge this gap by providing two formal semantics for the subset of AADL. The execution semantics provided by the AADL standard is formalized as Timed Transition Systems (TTS). This formalization gives a reference expression of AADL semantics which can be compared with the TASM-based translation (for verification purpose). Finally, the verified transformation is mechanized in the theorem prover Coq.
93|-||Efficient distributed skyline computation using dependency-based data partitioning|Skyline queries, together with other advanced query operators, are essential in order to help identify sets of interesting data points buried within huge amount of data readily available these days. A skyline query retrieves sets of non-dominated data points in a multi-dimensional dataset. As computing infrastructures become increasingly pervasive, connected by readily available network services, data storage and management have become inevitably more distributed. Under these distributed environments, designing efficient skyline querying with desirable quick response time and progressive returning of answers faces new challenges. To address this, in this paper, we propose a novel skyline query scheme termed MpSky. MpSky is based on a novel space partitioning scheme, employing the dependency relationships among data points on different servers. By grouping points of each server using dependencies, we are able to qualify a skyline point by only comparing it with data on dependent servers, and parallelize the skyline computation among non-dependent partitions that are from different servers or individual servers. By controlling the query propagation among partitions, we are able to generate skyline results progressively and prune partitions and points efficiently. Analytical and extensive simulation results show the effectiveness of the proposed scheme.
93|-||Assessment of institutions, scholars, and contributions on agile software development (2001â2012)|The number of scholarly publications on agile software development has grown significantly in recent years. Several researchers reviewed and attempted to synthesize studies on agile software development. However, no work has ranked the contributions of scholars and institutions to publications using a thorough process. This study presents findings on top publications, institutions, and scholars in the agile software development field from 2001 to 2012 based on the publication of such works in Science Citation Index journals. This paper highlights the key outlets for agile research and summarizes the most influential researchers and institutions as well as the most studied research areas. This study concludes by providing directions for future research.
93|-||Scalable network file systems with load balancing and fault tolerance for web services|Because of the rapid growth of the World Wide Web and the popularization of smart phones, tablets and personal computers, the number of web service users is increasing rapidly. As a result, large web services require additional disk space, and the required disk space increases with the number of web service users. Therefore, it is important to design and implement a powerful network file system for large web service providers. In this paper, we present three design issues for scalable network file systems. We use a variable number of objects within a bucket to decrease internal fragmentation in small files. We also propose a free space and access load-balancing mechanism to balance overall loading on the bucket servers. Finally, we propose a mechanism for caching frequently accessed data to lower the total disk I/O. These proposed mechanisms can effectively improve scalable network file system performance for large web services.
93|-||A clustering-based model for class responsibility assignment problem in object-oriented analysis|Assigning responsibilities to classes is a vital task in object-oriented analysis and design, and it directly affects the maintainability and reusability of software systems. There are many methodologies to help recognize the responsibilities of a system and assign them to classes, but all of them depend greatly on human judgment and decision-making. In this paper, we propose a clustering-based model to solve the class responsibility assignment (CRA) problem. The proposed model employs a novel interactive graph-based method to find inheritance hierarchies, and two novel criteria to determine the appropriate number of classes. It reduces the dependency of CRA on human judgment and provides a decision-making support for CRA in class diagrams. To evaluate the proposed model, we apply three different hierarchical agglomerative clustering algorithms and two different types of similarity measures. By comparing the obtained results of clustering techniques with the models designed by multi-objective genetic algorithm (MOGA), it is revealed that clustering techniques yield promising results.
93|-||Performance models and dynamic characteristics analysis for HDFS write and read operations: A systematic view|Hadoop has emerged as a successful framework for large-scale data-intensive computing applications. However, there is no research on performance models for the Hadoop Distributed File System (HDFS). Due to the complexity of HDFS and the difficulty of modeling the multiple impact factors for HDFS performance, to establish HDFS performance models based directly on these impact factors is very complicated. In this paper, the relationship between file size and HDFS Write/Read (denoted as W/R for short) throughput, i.e., the average flow rate of a HDFS W/R operation, is studied to build HDFS performance models from a systematic view. Based on the measured data of specially designed experiments (in which HDFS W/R operations can be viewed as single-input single-output systems), a system identification-based approach is applied to construct performance models for HDFS W/R operations under different conditions. Furthermore, dynamic characteristics metrics for HDFS performance are defined, and based on the identified performance models and these metrics, the dynamic characteristics of HDFS W/R operations, such as steady state and overshoot, are studied, and the relationships between impact factors and dynamic characteristics are analyzed. These analysis results can provide effective guidance and implications for the design and configuration of HDFS and Hadoop-based applications.
93|-||A high capacity data hiding scheme for binary images based on block patterns|This paper proposes a high capacity data hiding scheme for binary images based on block patterns, which can facilitate the authentication and annotation of scanned images. The scheme proposes block patterns for a 2 × 2 block to enforce specific block-based relationship in order to embed a significant amount of data without causing noticeable artifacts. In addition, two kinds of matching pair (MP) methods, internal adjustment MP and external adjustment MP, are designed to decrease the embedding changes. Shuffling is applied before embedding to reduce the distortion and improve the security. Experimental results show that the proposed scheme gives a significantly improved embedding capacity than previous approaches in the same level of embedding distortion. We also analyze the perceptual impact and discuss the robustness and security issues.
93|-||Flexible resource monitoring of Java programs|Monitoring resource consumptions is fundamental in software engineering, e.g., in validation of quality requirements, performance engineering, or adaptive software systems. However, resource monitoring does not come for free as it typically leads to overhead in the observed program. Minimizing this overhead and increasing the reliability of the monitored data is a major goal in realizing resource monitoring tools. Typically, this is achieved by limiting capabilities, e.g., supported resources, granularity of the monitoring focus, or runtime access to results. Thus, in practice often several approaches must be combined to obtain relevant information.
93|-||CodeCloud: A platform to enable execution of programming models on the Clouds|This paper presents a platform that supports the execution of scientific applications covering different programming models (such as Master/Slave, Parallel/MPI, MapReduce and Workflows) on Cloud infrastructures. The platform includes (i) a high-level declarative language to express the requirements of the applications featuring software customization at runtime, (ii) an approach based on virtual containers to encapsulate the logic of the different programming models, (iii) an infrastructure manager to interact with different IaaS backends, (iv) a configuration software to dynamically configure the provisioned resources and (v) a catalog and repository of virtual machine images. By using this platform, an application developer can adapt, deploy and execute parallel applications agnostic to the Cloud backend.
93|-||A review on E-business Interoperability Frameworks|Interoperability frameworks present a set of assumptions, concepts, values, and practices that constitute a method of dealing with interoperability issues in the electronic business (e-business) context. Achieving interoperability in the e-business generates numerous benefits. Thus, interoperability frameworks are the main component of e-business activities. This paper describes the existing interoperability frameworks for e-business, and performs a comparative analysis among their findings to determine the similarities and differences in their philosophy and implementation. This analysis yields a set of recommendations for any party that is open to the idea of creating or improving an E-business Interoperability Framework.
93|-||A trustworthy QoS-based collaborative filtering approach for web service discovery|Many network services which process a large quantity of data and knowledge are available in the distributed network environment, and provide applications to users based on Service-Oriented Architecture (SOA) and Web services technology. Therefore, a useful web service discovery approach for data and knowledge discovery process in the complex network environment is a very significant issue. Using the traditional keyword-based search method, users find it difficult to choose the best web services from those with similar functionalities. In addition, in an untrustworthy real world environment, the QoS-based service discovery approach cannot verify the correctness of the web services’ Quality of Service (QoS) values, since such values guaranteed by a service provider are different from the real ones. This work proposes a trustworthy two-phase web service discovery mechanism based on QoS and collaborative filtering, which discovers and recommends the needed web services effectively for users in the distributed environment, and also solves the problem of services with incorrect QoS information. In the experiment, the theoretical analysis and simulation experiment results show that the proposed method can accurately recommend the needed services to users, and improve the recommendation quality.
94|-|http://www.sciencedirect.com/science/journal/01641212/94|Social cyber systemsâChallenges, opportunities, and beyond|
94|-||Peer impressions in open source organizations: A survey|In virtual organizations, such as Open Source Software (OSS) communities, we expect that the impressions members have about each other play an important role in fostering effective collaboration. However, there is little empirical evidence about how peer impressions form and change in virtual organizations. This paper reports the results from a survey designed to understand the peer impression formation process among OSS participants in terms of perceived expertise, trustworthiness, productivity, experiences collaborating, and other factors that make collaboration easy or difficult. While the majority of survey respondents reported positive experiences, a non-trivial fraction had negative experiences. In particular, volunteer participants were more likely to report negative experiences than participants who were paid. The results showed that factors related to a person's project contribution (e.g., quality and understandability of committed codes, important design related decisions, and critical fixes made) were more important than factors related to work style or personal traits. Although OSS participants are very task focused, the respondents believed that meeting their peers in person is beneficial for forming peer impressions. Having an appropriate impression of one's OSS peers is crucial, but the impression formation process is complicated and different from the process in traditional organizations.
94|-||Twitter data analysis by means of Strong Flipping Generalized Itemsets|Twitter data has recently been considered to perform a large variety of advanced analysis. Analysis of Twitter data imposes new challenges because the data distribution is intrinsically sparse, due to a large number of messages post every day by using a wide vocabulary. Aimed at addressing this issue, generalized itemsets – sets of items at different abstraction levels – can be effectively mined and used to discover interesting multiple-level correlations among data supplied with taxonomies. Each generalized itemset is characterized by a correlation type (positive, negative, or null) according to the strength of the correlation among its items.
94|-||Efficient unveiling of multi-members in a social network|With the rapid growth of the Web 2.0, the discovery of key actors in social networks, called influencers, mediators, ambassadors or experts, has recently received a renewed of attention. In this article, we consider a particular type of actor that we call a multi-member since he belongs to several communities. We introduce a methodological framework to identify these actors in a hypergraph, in which the vertices are the actors and the hyperedges are the communities. We also show that detecting such multi-members is similar to the problem of the determination of a subset of minimal transversals of a hypergraph. An efficient algorithm that relies on the connection between the definition of a multi-member and that of an essential itemset is also introduced. Experiments done on several datasets showed that the introduced algorithm outperforms the pioneering ones of the literature.
94|-||DYSCS: A platform to build geographically and semantically enhanced social content sites|Social content sites allow ordinary internet users to upload, edit, share, and annotate Web content with freely chosen keywords called tags. However, tags are only useful to the extent that they are processable by users and machines, which is often not the case since users frequently provide ambiguous and idiosyncratic tags. Thereby, many social content sites are starting to allow users to enrich their tags with semantic metadata, such as the GeoSocial Content Sites, for example, where users can annotate their tags with geographic metadata. But geographic metadata alone only unveils a very specific facet of a tag, which leads to the need for more general purpose semantic metadata. This paper introduces DYSCS – Do it Yourself Social Content Sites – a platform that combines Web 2.0 and Semantic Web technologies for assisting users in creating their own social content sites enriched with geographic and general purpose semantics. Moreover, DYSCS is highly reusable and interoperable, which are consequences of its ontology driven architecture.
94|-||Extended U+F Social Network Protocol: Interoperability, reusability, data protection and indirect relationships in Web Based Social Networks|An interconnected world is what current technologies look for, being Web Based Social Networks (WBSNs) a promising development in this regard. Four desirable WBSN features are identified, namely, interoperability, reusability, protection against WBSNs providers and indirect relationships. A protocol, called U+F, addressed interoperability and reusability of identity data, resources and access control policies between different WBSNs. In order to address the remaining couple of features, that is, achieving the protection of data against WBSNs providers and indirect relationships management across different WBSNs, this paper presents eU+F, an extension of U+F. A prototype is developed to verify the feasibility of implementing the proposed protocol in a real environment, as well as to compare its workload regarding three well-known WBSNs, Facebook, MySpace and LinkedIn.
94|-||Automatic multi-partite graph generation from arbitrary data|In this paper we present a generic model for automatic generation of basic multi-partite graphs obtained from collections of arbitrary input data following user indications. The paper also presents GraphGen, a tool that implements this model. The input data is a collection of complex objects composed by a set or list of heterogeneous elements. Our tool provides a simple interface for the user to specify the types of nodes that are relevant for the application domain in each case. The nodes and the relationships between them are derived from the input data through the application of a set of derivation rules specified by the user. The resulting graph can be exported in the standard GraphML format so that it can be further processed with other graph management and mining systems. We end by giving some examples in real scenarios that show the usefulness of this model.
94|-||A critical examination of recent industrial surveys on agile method usage|Practitioners and researchers often claim that agile methods have moved into the mainstream for the last few years. To support this claim they refer to recent industrial surveys which tend to report high rates of agile method usage. However many of these industrial surveys are conducted by agile consultants, tool vendors, professional societies and independent technology and market research organizations. This raises some important concerns about the possible conflict of interest and the overall trustworthiness of these studies.
94|-||A weight-aware channel assignment algorithm for mobile multicast in wireless mesh networks|Wireless mesh networks (WMNs) are one of key technologies for next generation wireless networks. In this paper, we propose a heuristic channel assignment algorithm with weight awareness to support mobile multicast in WMNs. To enhance network throughput, our algorithm is based on the path forwarding weight to perform channel assignment. In addition to non-overlapping channels, partially-overlapping channels are also used in channel assignment. To fully exploit all available channels in channel assignment, we devise a new channel selection metric to consider the channel separation and the distance between nodes. In mobile multicast, the multicast tree structure cannot be fixed due to receiver (multicast member) mobility. The change of the multicast tree structure will result in channel re-assignment. The proposed algorithm is based on a critical-event driven manner to reduce the times of channel re-assignment as much as possible. Finally, we perform simulation experiments to show the effectiveness of the proposed channel assignment algorithm.
94|-||A component- and connector-based approach for end-user composite web applications development|Enabling real end-user development is the next logical stage in the evolution of Internet-wide service-based applications. Successful composite applications rely on heavyweight service orchestration technologies that raise the bar far above end-user skills. This weakness can be attributed to the fact that the composition model does not satisfy end-user needs rather than to the actual infrastructure technologies. In our opinion, the best way to overcome this weakness is to offer end-to-end composition from the user interface to service invocation, plus an understandable abstraction of building blocks and a visual composition technique empowering end users to develop their own applications. In this paper, we present a visual framework for end users, called FAST, which fulfils this objective. FAST implements a novel composition model designed to empower non-programmer end users to create and share their own self-service composite applications in a fully visual fashion. We projected the development environment implementing this model as part of the European FP7 FAST Project, which was used to validate the rationale behind our approach.
94|-||Avoiding, finding and fixing spreadsheet errors â A survey of automated approaches for spreadsheet QA|Spreadsheet programs can be found everywhere in organizations and they are used for a variety of purposes, including financial calculations, planning, data aggregation and decision making tasks. A number of research surveys have however shown that such programs are particularly prone to errors. Some reasons for the error-proneness of spreadsheets are that spreadsheets are developed by end users and that standard software quality assurance processes are mostly not applied. Correspondingly, during the last two decades, researchers have proposed a number of techniques and automated tools aimed at supporting the end user in the development of error-free spreadsheets. In this paper, we provide a review of the research literature and develop a classification of automated spreadsheet quality assurance (QA) approaches, which range from spreadsheet visualization, static analysis and quality reports, over testing and support to model-based spreadsheet development. Based on this review, we outline possible opportunities for future work in the area of automated spreadsheet QA.
94|-||Handling slowly changing dimensions in data warehouses|Analysis of historical data in data warehouses contributes significantly toward future decision-making. A number of design factors including, slowly changing dimensions (SCDs), affect the quality of such analysis. In SCDs, attribute values may change over time and must be tracked. They should maintain consistency and correctness of data, and show good query performance. We identify that SCDs can have three types of validity periods: disjoint, overlapping, and same validity periods. We then show that the third type cannot be handled through the temporal star schema for temporal data warehouses (TDWs). We further show that a hybrid/Type6 scheme and temporal star schema may be used to handle this shortcoming. We demonstrate that the use of a surrogate key in the hybrid scheme efficiently identifies data, avoids most time comparisons, and improves query performance. Finally, we compare the TDWs and a surrogate key-based temporal data warehouse (SKTDW) using query formulation, query performance, and data warehouse size as parameters. The results of our experiments for 23 queries of five different types show that SKTDW outperforms TDW for all type of queries, with average and maximum performance improvements of 165% and 1071%, respectively. The results of our experiments are statistically significant.
94|-||A systematic review of software architecture visualization techniques|Given the increased interest in using visualization techniques (VTs) to help communicate and understand software architecture (SA) of large scale complex systems, several VTs and tools have been reported to represent architectural elements (such as architecture design, architectural patterns, and architectural design decisions). However, there is no attempt to systematically review and classify the VTs and associated tools reported for SA, and how they have been assessed and applied.
94|-||Architectural reliability analysis of framework-intensive applications: A web service case study|A novel methodology for modeling the reliability and performance of web services (WSs) is presented. To present the methodology, an experimental environment is developed in house, where WSs are treated as atomic entities but the underlying middleware is partitioned into layers. WSs are deployed in JBoss AS. Web service requests are generated to a remote middleware on which JBoss runs, and important performance parameters under various configurations are collected. In addition, a modularized simulation model in Petri net is developed from the architecture of the middleware and run-time behavior of the WSs. The results show that (1) the simulation model provides for measuring the performance and reliability of WSs under different loads and conditions that may be of great interest to WS designers and the professionals involved; (2) configuration parameters have substantial impact on the overall performance; (3) the simulation model provides a basis for aggregating the modules (layers), nullifying modules, or to include additional aspects of the WS architecture; and (4) the model is beneficial to predict the performance of WSs for those cases that are difficult to replicate in a field study.
94|-||Corrigendum to âPower-aware scheduling algorithms for sporadic tasks in real-time systemsâ [J. Syst. Softw. 86 (2013) 2611â2619]|
95|-|http://www.sciencedirect.com/science/journal/01641212/95|Empirical research methodologies and studies in Requirements Engineering: How far did we come?|Since the inception of the RE conference series (1992), both researchers and practitioners in the RE community have acknowledged the significance of empirical evaluation as an instrument to gain knowledge about various aspects of RE phenomena and the validity of our research results. A significant number of empirical studies have been conducted in the search for knowledge about RE problems as well as evidence of successful and less successful application of proposed solutions. This editorial presents the progress empirical RE research has made since 1992. Based on a search in the Scopus digital library, we report from an analysis of peer-reviewed systematic literature reviews and mapping studies to showcase major areas of RE research that use methods from the Empirical Software Engineering paradigm. We summarize prior empirical research in RE and introduce the contributors to this special issue on empirical research methodologies and studies in RE.
95|-||Software product management â An industry evaluation|Product management is a key success factor for software products as it spans the entire life-cycle and thus ensures both a technical and business perspective. With its many interfaces to various business processes and stakeholders across the life-cycle, it is a primary driver for requirements engineering in its focus on value-orientation and consistency across releases. This article provides an overview on product management in software and IT. It summarizes experiences with introducing, improving and deploying the role of a product manager. In order to get a profound industry overview we performed a field study with interviews and concrete insight across fifteen different organizations world-wide on the role of the product manager and its success factors. As a technical solution we present four success factors identified from the research and show how they address the challenges we identified in practice. The novel part of this research and technical study is the industry survey and evaluation of resulting solution proposals. We found that with increasing institutionalization of a consistent and empowered product management role, the success rate of projects in terms of schedule predictability, quality and project duration improves.
95|-||Empirical research methods for technology validation: Scaling up to practice|Before technology is transferred to the market, it must be validated empirically by simulating future practical use of the technology. Technology prototypes are first investigated in simplified contexts, and these simulations are scaled up to conditions of practice step by step as more becomes known about the technology. This paper discusses empirical research methods for scaling up new requirements engineering (RE) technology.
95|-||Using a grounded theory approach for exploring software product management challenges|The traditional requirements engineering (RE) research paradigm, along with most engineering research and practice, is commonly seen to belong to the philosophical tradition of positivism, which construes knowledge as accruing through the systematic observation of stable and knowable phenomena. Consequently, RE methods tend to ignore social issues. However, due to the dominant role of the human being in RE, there has been an increasing need to rely on research methods of the social sciences, arts, and humanities for RE related findings. This paper illustrates one example of how social aspects in RE have been explored with a research method adopted from social sciences research tradition. Drawing heavily on the research reported in the doctoral thesis of the principal author, we describe in this paper: (1) how a study using a grounded theory approach was designed and conducted for exploring market-driven requirements engineering (MDRE) challenges in seven companies, (2) how the analysis eventually proceeded toward a proposed theory, and (3) our experiences of using a grounded theory approach within the discipline of RE.
95|-||Stakeholder logistics of an interactive system|Although it seems that software metrics have moved beyond mere performance measurement, it is not too clear how machine effectiveness, efficiency, and effort pertain to human requirements on such matters. In industry as well as academia, the ISO 9241-11 norm provides the dominant view on usability, stating that usability is a function of effectiveness, efficiency, and satisfaction. Although intuitively, usability requirements should be part of a software's design in an early stage, conceptually and empirically, it seems more likely that performance requirements (i.e., the absence of errors) should be the center of concern. This paper offers an elaborated view on usability, satisfaction, and performance. Certain theoretical conceptions are tested with data gathered from professional users of banking and hospital systems by means of a 4-year single-item survey and a structured questionnaire, respectively. Results suggested that performance factors (i.e., efficiency) are more important than usability in understanding why stakeholders are satisfied with a system or not. Moreover, it neither is dissatisfaction with a system nor that a system is less usable that predicate requirements change. Instead, avoiding machine inaccuracy best predicted the variability in agreement to “must have” requirements, while achieving human accuracy predicted the variability in agreement to the “won’t have” requirements. The present contribution provides a consistent research framework that can bring more focus to design (i.e., prioritization), clarify discussions about design trade-offs, makes concepts measurable, and eventually may lead to better-informed designs.
95|-||Assessing a requirements evolution approach: Empirical studies in the air traffic management domain|Requirements evolution is still a challenging problem in engineering practices. In this paper, we report the results of the empirical evaluation of a novel approach for modeling and reasoning on evolving requirements. We evaluated the effectiveness of the approach in modeling requirements evolution by means of a series of empirical studies in the air traffic management (ATM) domain. As we also wanted to assess whether the knowledge of the method and/or the application domain influences the effectiveness of the approach, the studies involved researchers, master students and domain experts with different level of knowledge of the approach and of the ATM domain. The participants have applied the approach to a real evolutionary scenario which focuses on the introduction of a new queue management tool, the Arrival MANager (AMAN) and a new network for information sharing (SWIM) connecting the main ATM actors. The results from the studies show that the modeling approach is effective in capturing requirements evolution. In addition, domain knowledge and method knowledge do not have an observable effect on the effectiveness of the approach. Furthermore, the evaluation provided us useful insights on how to improve the modeling approach.
95|-||Measure-independent characterization of contrast optimal visual cryptography schemes|Visual cryptography has been studied in two models and visual cryptography schemes have been evaluated using different contrast measures. Naor and Shamir introduced the deterministic model while Kafri and Keren introduced the random grid model. In the deterministic model, three different measures of contrast have been proposed, Î³ns, Î³vv and Î³es, although only Î³ns, has been thoroughly studied. Tight bounds on Î³ns are known for several classes of schemes. In the random grid model the contrast is Î³rg.
95|-||Supporting SIP-based end-to-end Data Distribution Service QoS in WANs|Assuring end-to-end QoS in enterprise distributed real-time and embedded (DRE) systems is hard due to the heterogeneity and transient behavior of communication networks, the lack of integrated mechanisms that schedule communication and computing resources holistically, and the scalability limits of IP multicast in wide-area networks (WANs). This paper makes three contributions to research on overcoming these problems in the context of enterprise DRE systems that use the OMG Data Distribution Service (DDS) quality-of-service (QoS)-enabled publish/subscribe (pub/sub) middleware over WANs. First, it codifies the limitations of conventional DDS implementations deployed over WANs. Second, it describes a middleware component called Proxy DDS that bridges multiple, isolated DDS domains deployed over WANs. Third, it describes the NetQSIP framework that combines multi-layer, standards-based technologies including the OMG-DDS, Session Initiation Protocol (SIP), and IP DiffServ to support end-to-end QoS in a WAN and shield pub/sub applications from tedious and error-prone details of network QoS mechanisms. The results of experiments using Proxy DDS and NetQSIP show how combining DDS with SIP in DiffServ networks significantly improves dynamic resource reservation in WANs and provides effective end-to-end QoS management.
95|-||Waste identification as the means for improving communication in globally distributed agile software development|Agile approaches highly values communication between team members to improve software development processes, even though, communication in globally distributed agile teams can be difficult. Literature proposes solutions for mitigating the challenges encountered in these environments. These solutions range from general-level recommendations and practices to the use of communication tools. However, an approach covering the whole development process for identifying challenges, and improving communication in globally distributed agile development projects, is missing. In order to address this, we conducted a case study within a globally distributed agile software development project focused on using the concept of waste as a lens for identifying non-value producing communication elements. In order to achieve this, we constructed a waste identification approach through which we identified five communication wastes, and solutions to mitigate them. These wastes can help companies identify communication issues that are present in their development efforts, while the presented waste identification technique gives them a mechanism for waste identification and mitigation. This work contributes to the scientific community by increasing the knowledge about communication in globally distributed agile development efforts.
95|-||Guilt-based handling of software performance antipatterns in palladio architectural models|Antipatterns are conceptually similar to patterns in that they document recurring solutions to common design problems. Software performance antipatterns document common performance problems in the design as well as their solutions. The definition of performance antipatterns concerns software properties that can include static, dynamic, and deployment aspects. To make use of such knowledge, we propose an approach that helps software architects to identify and solve performance antipatterns. Our approach provides software performance feedback to architects, since it suggests the design alternatives that allow overcoming the detected performance problems.
95|-||Surfing the optimization space of a multiple-GPU parallel implementation of a X-ray tomography reconstruction algorithm|The increasing popularity of massively parallel architectures based on accelerators have opened up the possibility of significantly improving the performance of X-ray computed tomography (CT) applications towards achieving real-time imaging. However, achieving this goal is a challenging process, as most CT applications have not been designed for exploiting the amount of parallelism existing in these architectures. In this paper we present the massively parallel implementation and optimization of Mangoose++, a CT application for reconstructing 3D volumes from 2D images collected by scanners based on cone-beam geometry. The main contribution of this paper are the following. First, we develop a modular application design that allows to exploit the functional parallelism inside the application and to facilitate the parallelization of individual application phases. Second, we identify a set of optimizations that can be applied individually and in combination for optimally deploying the application on a massively parallel multi-GPU system. Third, we present a study of surfing the optimization space of the modularized application and demonstrate that a significant benefit can be obtained from employing the adequate combination of application optimizations.
95|-||On the use of software design models in software development practice: An empirical investigation|Research into software design models in general, and into the UML in particular, focuses on answering the question how design models are used, completely ignoring the question if they are used. There is an assumption in the literature that the UML is the de facto standard, and that use of design models has had a profound and substantial effect on how software is designed by virtue of models giving the ability to do model-checking, code generation, or automated test generation. However for this assumption to be true, there has to be significant use of design models in practice by developers.
95|-||Imperceptible visible watermarking based on postcamera histogram operation|A real-world scene captured via digital devices, such as a digital still camera, video recorder and mobile device, is a common behavior in recent decades. With the increasing availability, reproduction and sharing of media, the intellectual property of digital media is incapable of guaranty. To claim the ownership of digital camera media, the imperceptible visible watermarking (IVW) mechanism was designed based on the observation that most camera devices contain the postcamera histogram operation. The IVW approach can achieve advantages both the content readability of invisible watermarking methodology and the visual ownership identification of visible watermarking methodology. The computational complexity of IVW is low and can be effectively applied to almost any of the digital electronic devices when capturing the real-world scene without additional instruments. The following results and analysis demonstrate the novel scheme is effective and applicable for versatile images and videos captured.
95|-||Face recognition based on curvelets and local binary pattern features via using local property preservation|In this paper, we propose a new feature extraction approach for face recognition based on Curvelet transform and local binary pattern operator. The motivation of this approach is based on two observations. One is that Curvelet transform is a new anisotropic multi-resolution analysis tool, which can effectively represent image edge discontinuities; the other is that local binary pattern operator is one of the best current texture descriptors for face images. As the curvelet features in different frequency bands represent different information of the original image, we extract such features using different methods for different frequency bands. Technically, the lowest frequency band component is processed using the local binary pattern method, and only the medium frequency band components are normalized. And then, we combine them to create a feature set, and use the local preservation projection to reduce its dimension. Finally, we classify the test samples using the nearest neighbor classifier in the reduced space. Extensive experiments on the Yale database, the extended Yale B database, the PIE pose 09 database, and the FRGC database illustrate the effectiveness of the proposed method.
95|-||Xen2MX: High-performance communication in virtualized environments|Cloud computing infrastructures provide vast processing power and host a diverse set of computing workloads, ranging from service-oriented deployments to high-performance computing (HPC) applications. As HPC applications scale to a large number of VMs, providing near-native network I/O performance to each peer VM is an important challenge. In this paper we present Xen2MX, a paravirtual interconnection framework over generic Ethernet, binary compatible with Myrinet/MX and wire compatible with MXoE. Xen2MX combines the zero-copy characteristics of Open-MX with Xen's memory sharing techniques. Experimental evaluation of our prototype implementation shows that Xen2MX is able to achieve nearly the same raw performance as Open-MX running in a non-virtualized environment. On the latency front, Xen2MX performs as close as 96% to the case where virtualization layers are not present. Regarding throughput, Xen2MX saturates a 10 Gbps link, achieving 1159 MB/s, compared to 1192 MB/s of the non-virtualized case. Scales efficiently with the number of VMs, saturating the link for even smaller messages when 40 single-core VMs put pressure on the network adapters.
95|-||Distributed collaborative filtering with singular ratings for large scale recommendation|Collaborative filtering (CF) is an effective technique addressing the information overloading problem, where each user is associated with a set of rating scores on a set of items. For a chosen target user, conventional CF algorithms measure similarity between this user and other users by utilizing pairs of rating scores on common rated items, but discarding scores rated by one of them only. We call these comparative scores as dual ratings, while the non-comparative scores as singular ratings. Our experiments show that only about 10% ratings are dual ones that can be used for similarity evaluation, while the other 90% are singular ones. In this paper, we propose SingCF approach, which attempts to incorporate multiple singular ratings, in addition to dual ratings, to implement collaborative filtering, aiming at improving the recommendation accuracy. We first estimate the unrated scores for singular ratings and transform them into dual ones. Then we perform a CF process to discover neighborhood users and make predictions for each target user. Furthermore, we provide a MapReduce-based distributed framework on Hadoop for significant improvement in efficiency. Experiments in comparison with the state-of-the-art methods demonstrate the performance gains of our approaches.
95|-||Scalable news recommendation using multi-dimensional similarity and JaccardâKmeans clustering|In order to solve the scalability problem in news recommendation, a scalable news recommendation method is proposed. The method includes the multi-dimensional similarity calculation, the Jaccard–Kmeans fast clustering and the Top-N recommendation. The multi-dimensional similarity calculation method is used to compute the integrated similarity between users, which considers abundant content feature of news, behaviors of users, and the time of these behaviors occurring. Based on traditional K-means algorithm, the Jaccard–Kmeans fast clustering method is proposed. This clustering method first computes the above multi-dimensional similarity, then generates multiple cluster centers with user behavior feature and news content feature, and evaluates the clustering results according to cohesiveness. The Top-N recommendation method integrates a time factor into the final recommendation. Experiment results prove that the proposed method can enhance the scalability of news recommendation, significantly improve the recommendation accuracy in condition of data sparsity, and improve the timeliness of news recommendation.
96|-|http://www.sciencedirect.com/science/journal/01641212/96|GPU accelerated pivoting rules for the simplex algorithm|Simplex type algorithms perform successive pivoting operations (or iterations) in order to reach the optimal solution. The choice of the pivot element at each iteration is one of the most critical step in simplex type algorithms. The flexibility of the entering and leaving variable selection allows to develop various pivoting rules. In this paper, we have proposed some of the most well-known pivoting rules for the revised simplex algorithm on a CPU–GPU computing environment. All pivoting rules have been implemented in MATLAB and CUDA. Computational results on randomly generated optimal dense linear programs and on a set of benchmark problems (Netlib-optimal, Kennington, Netlib-infeasible, Mészáros) are also presented. These results showed that the proposed GPU implementations of the pivoting rules outperform the corresponding CPU implementations.
96|-||Recommending software upgrades with Mojave|Software upgrades are frequent. Unfortunately, many of the upgrades either fail or misbehave. We argue that many of these failures can be avoided for users of the new version of the software by exploiting the characteristics of the upgrade and feedback from the users that have already installed it. To demonstrate that this can be achieved, we build Mojave, the first recommendation system for software upgrades. Mojave leverages data from the existing and new users, machine learning, and static and dynamic source analyses. For each new user, Mojave computes the likelihood that the upgrade will fail for him/her. Based on this value, Mojave recommends for or against the upgrade. We evaluate Mojave for three real upgrade problems with the OpenSSH suite, and one synthetic upgrade problem each in the SQLite database and the uServer Web server. Our results show that it provides accurate recommendations to the new users.
96|-||An evolutionary approach to identify logical components|Identifying suitable components during the software design phase is an important way to obtain more maintainable software. Many methods including Graph Partitioning, Clustering-based, CRUD-based, and FCA-based methods have been proposed to identify components at an early stage of software design. However, most of these methods use classical clustering techniques, which rely on expert judgment.
96|-||Change impact analysis and changeability assessment for a change proposal: An empirical study ââ|Software change is a fundamental ingredient of software maintenance and evolution. Effectively supporting software modification is essential to provide a reliable high-quality evolution of software systems, as even a slight change may cause some unpredictable and undesirable effects on other parts of the software. To address this issue, this work used change impact analysis (CIA) to guide software modification. CIA can be used to help make correct decision on the change proposal, that is changeability assessment, and to implement effective changes for a change proposal. In this article, we conducted an empirical study on three Java open-source systems to show how CIA can be used during software modification. The results indicate that: (1) assessing changeability of a change proposal based on the impact results of the CIA is not accurate from the precision perspective; (2) the proposed impactness metric is an effective indicator of changeability assessment for the change proposal; and (3) CIA can make the change implementation process more efficient and easier.
96|-||Sources of value in application ecosystems|Mobile application stores have revolutionised the dynamics of mobile ecosystems. Research on mobile application ecosystems has been significantly driven by data that is focused on the visualisation of an ecosystem's dynamics. This is a valuable step towards understanding the nature of the ecosystems, but it is limited in its explanatory power. Thus, a theory-driven approach is needed to understand the overall dynamics of such systems. This study applies a theoretical framework of value creation in e-business in the context of mobile application ecosystems, with a focus on application developers. A qualitative research strategy is employed in testing operationalisation in a sample of developers. The sample comprises 27 application developers from the three leading mobile application ecosystems. The results show that efficiency is the main source of value, products seldom create value through complementarities, and approaches towards lock-in and novelty seem to vary among application developers. The managerial and theoretical implications of such biased value creation in mobile ecosystems are considered.
96|-||Development and validation of customized process models|Configurable reference process models encompass common and variable processes of organizations from different business domains. These reference process models are designed and reused to guide and derive customized business processes according to the requirements of stakeholders. The customization process is generally initiated by a configuration step, selecting a subset of the reference process model. Configuration is followed by a customization step, which assumes adapting or extending the configured business process based on the specific or unforeseen requirements. Hence, it is crucial to validate the correctness and compliance of the final customized business process with respect to the patterns and business constraints that are specified in the reference model. In this paper, we firstly introduce a technique to develop a customized process model and then present a set of identified inconsistency patterns that may happen during the configuration of a reference model and the customization of configured process models. Furthermore, we describe our proposed approach including formal representations and algorithms that provide logical reasoning and enable automatic inconsistency detection by leveraging description logic. In order to explore the scalability of the approach, we designed the experiments with various process models sizes and inconsistency distributions. The results of the experiments revealed the scalability of our approach with large size process models (500 activities).
96|-||FPGA implementation of reversible watermarking in digital images using reversible contrast mapping|Reversible contrast mapping (RCM) and its various modified versions are used extensively in reversible watermarking (RW) to embed secret information into the digital contents. RCM based RW accomplishes a simple integer transform applied on pair of pixels and their least significant bits (LSB) are used for data embedding. It is perfectly invertible even if the LSBs of the transformed pixels are lost during data embedding. RCM offers high embedding rate at relatively low visual distortion (embedding distortion). Moreover, low computation cost and ease of hardware realization make it attractive for real-time implementation. To this aim, this paper proposes a field programmable gate array (FPGA) based very large scale integration (VLSI) architecture of RCM-RW algorithm for digital images that can serve the purpose of media authentication in real-time environment. Two architectures, one for block size (8 × 8) and the other one for (32 × 32) block are developed. The proposed architecture allows a 6-stage pipelining technique to speed up the circuit operation. For a cover image of block size (32 × 32), the proposed architecture requires 9881 slices, 9347 slice flip-flops, 11291 number 4-input LUTs, 3 BRAMs and a data rate of 1.0395 Mbps at an operating frequency as high as 98.76 MHz.
96|-||A component-based process with separation of concerns for the development of embedded real-time software systems|Numerous component models have been proposed in the literature, a testimony of a subject domain rich with technical and scientific challenges, and considerable potential. Unfortunately however, the reported level of adoption has been comparatively low. Where successes were had, they were largely facilitated by the manifest endorsement, where not the mandate, by relevant stakeholders, either internal to the industrial adopter or with authority over the application domain. The work presented in this paper stems from a comprehensive initiative taken by the European Space Agency (ESA) and its industrial suppliers. This initiative also enjoyed significant synergy with interests shown for similar goals by the telecommunications and railways domain, thanks to the interaction between two parallel project frameworks. The ESA effort aimed at favouring the adoption of a software reference architecture across its software supply chain. The center of that strategy revolves around a component model and the software development process that builds on it. This paper presents the rationale, the design and implementation choices made in their conception, as well as the feedback obtained from a number of industrial case studies that assessed them.
96|-||Empirical evaluation of a privacy-focused threat modeling methodology|Privacy is a key issue in today's society. Software systems handle more and more sensitive information concerning citizens. It is important that such systems are privacy-friendly by design. In previous work, we proposed a privacy threat analysis methodology, named LINDDUN. The methodology supports requirements engineers and software architects in identifying privacy weaknesses in the system they contribute to developing. As this is a fairly new technique, its results when applied in realistic scenarios are yet unknown. This paper presents a series of three empirical studies that thoroughly evaluate LINDDUN from a multi-faceted perspective. Our assessment characterizes the correctness and completeness of the analysis results produced by LINDDUN, as well as the productivity associated with executing the methodology. We also look into aspects such as the ease of use and reliability of LINDDUN. The results are encouraging, overall. However, some areas for further improvement have been identified as a result of this empirical inquiry.
96|-||Memory leak detection in Java: Taxonomy and classification of approaches|Memory leaks are usually not associated with runtime environments with automatic garbage collection; however, memory leaks do happen in such environments and present a challenge to detect and find a root cause. Currently in the industry manual heap dump analysis is the most popular way of finding memory leaks, regardless of the number of automated methods proposed by scientists over the years. However, heap dump analysis alone cannot answer all questions needed to fix the leak effectively. The current paper reviews memory leak detection approaches proposed over the years and classifies them from the point of view of assessed metrics, performance overhead and intrusiveness. In addition, we classify the methods into online, offline and hybrid groups based on their features.
96|-||Transforming an enterprise model into a use case model in business process systems|One of the responsibilities of requirements engineering is to transform stakeholder requirements into system and software requirements. For enterprise systems, this transformation must consider the enterprise context where the system will be deployed. Although there are some approaches for detailing stakeholder requirements, some of them even considering the enterprise context, this task is executed manually. Based on model-driven engineering concepts, this study proposes a semi-automatic transformation from an enterprise model to a use case model. The enterprise model is used as a source of information about the stakeholder requirements and domain knowledge, while the use case model is used as software requirements model. This study presents the source and target metamodels, a set of transformation rules, and a tool to support the transformation. An experiment analyzes the use of the proposed transformation to investigate its benefits and if it can be used in practice, from the point of view of students in the context of a requirements refinement. The results indicate that the approach can be used in practice, as it did not influence the quality of the generated use cases. However, the empirical analysis does not indicate benefits of using the transformation, even if the qualitative results were positive.
96|-||Synthesizing interpreted domain-specific models to manage smart microgrids|The increase in prominence of model-driven software development (MDSD) has placed emphasis on the use of domain-specific modeling languages (DSMLs) during the development process. DSMLs allow for domain concepts to be conceptualized and represented at a high level of abstraction. Currently, most DSML models are converted into high-level languages (HLLs) through a series of model-to-model and/or model-to-text transformations before they are executed. An alternative approach for model execution is the interpretation of models directly without converting them into an HLL. These models are created using interpreted DSMLs (i-DSMLs) and realized using a semantic-rich execution engine or domain-specific virtual machine (DSVM).
96|-||Broker-based SLA-aware composite service provisioning|QoS-aware service composition aims to satisfy users’ quality of services (QoS) needs during service composition. Traditional methods simply attempt to maximize user satisfaction by provisioning the composite service instance with the best QoS. These “best-effort” methods fail to take into account that there also exist other consumers competing for the service resources and their decisions of service selection/composition can impact on QoS. Since user's QoS needs can be met once the demanded level is reached, in this paper, we propose an “on-demand” strategy for QoS-aware service composition to replace the traditional “best-effort” strategy. The service broker is introduced to facilitate implementation of this strategy: it first purchases a number of service instances for each component from providers and then provisions the composite services with different QoS classes to consumers. This paper focuses on how the broker follows the service level agreement (SLA) to provision composite services in the “on-demand” manner. This problem is formally expressed as the minimization of the QoS distance function between SLA and QoS of composite service instances, under a series of constraints. Heuristic approaches are proposed for the problem and experiments are conducted at last to verify their effectiveness and efficiency.
97|-|http://www.sciencedirect.com/science/journal/01641212/97|Search-based metamodel matching with structural and syntactic measures|The use of different domain-specific modeling languages and diverse versions of the same modeling language often entails the need to translate models between the different languages and language versions. The first step in establishing a transformation between two languages is to find their corresponding concepts, i.e., finding correspondences between their metamodel elements. Although, metamodels use heterogeneous terminologies and structures, they often still describe similar language concepts. In this paper, we propose to combine structural metrics (e.g., number of properties per concept) and syntactic metrics to generate correspondences between metamodels. Because metamodel matching requires to cope with a huge search space of possible element combinations, we adapted a local and a global metaheuristic search algorithm to find the best set of correspondences between metamodels. The efficiency and effectiveness of our proposal is evaluated on different matching scenarios based on existing benchmarks. In addition, we compared our technique to state-of-the-art ontology matching and model matching approaches.
97|-||Comparing model-based and dynamic event-extraction based GUI testing techniques: An empirical study|Graphical user interfaces are pervasive in modern software systems, and to ensure their quality it is important to test them. Two primary classes of automated GUI testing approaches, those based on static models and those based on dynamic event-extraction, present tradeoffs in cost and effectiveness. For example, static model-based GUI testing techniques can create test cases that contain nonexecutable events, whereas dynamic event-extraction based GUI testing techniques can create larger numbers of duplicate test cases. To better understand the effects of these tradeoffs, we created a GUI testing framework that facilitates fair comparison of different GUI testing techniques, and we conducted a controlled experiment comparing representative versions of static-model based and dynamic event-extraction based testing techniques on several GUI-based Java applications. Our study reveals several cost and effectiveness tradeoffs between the techniques, with implications for research and practice.
97|-||Hybrid address spaces: A methodology for implementing scalable high-level programming models on non-coherent many-core architectures|This paper introduces hybrid address spaces as a fundamental design methodology for implementing scalable runtime systems on many-core architectures without hardware support for cache coherence. We use hybrid address spaces for an implementation of MapReduce, a programming model for large-scale data processing, and the implementation of a remote memory access (RMA) model. Both implementations are available on the Intel SCC and are portable to similar architectures. We present the design and implementation of HyMR, a MapReduce runtime system whereby different stages and the synchronization operations between them alternate between a distributed memory address space and a shared memory address space, to improve performance and scalability. We compare HyMR to a reference implementation and we find that HyMR improves performance by a factor of 1.71× over a set of representative MapReduce benchmarks. We also compare HyMR with Phoenix++, a state-of-art implementation for systems with hardware-managed cache coherence in terms of scalability and sustained to peak data processing bandwidth, where HyMR demonstrates improvements of a factor of 3.1× and 3.2× respectively. We further evaluate our hybrid remote memory access (HyRMA) programming model and assess its performance to be superior of that of message passing.
97|-||A systematic literature review on the industrial use of software process simulation|Software process simulation modelling (SPSM) captures the dynamic behaviour and uncertainty in the software process. Existing literature has conflicting claims about its practical usefulness: SPSM is useful and has an industrial impact; SPSM is useful and has no industrial impact yet; SPSM is not useful and has little potential for industry.
97|-||O1FS: Flash file system with O(1) crash recovery time|The crash recovery time of NAND flash file systems increases with flash memory capacity. Crash recovery usually takes several minutes for a gigabyte of flash memory and becomes a serious problem for mobile devices. To address this problem, we propose a new flash file system, O1FS. A key concept of our system is that a small number of blocks are modified exclusively until we change the blocks explicitly. To recover from crashes, O1FS only accesses the most recently modified blocks rather than the entire flash memory. Therefore, the crash recovery time is bounded by the size of the blocks. We develop mathematical models of crash recovery techniques and prove that the time complexity of O1FS is O(1), whereas that of other methods is proportional to the number of blocks in the flash memory. Our evaluation shows that the crash recovery of O1FS is about 18.5 times faster than that of a state-of-the-art method.
97|-||FlexIQ: A flexible interactive Querying Framework by Exploiting the Skyline Operator|Skyline operator has gained much attention in the last decade and is proved to be valuable for multi-criteria decision making. This paper presents a novel Flexible Interactive Querying (FlexIQ) framework for user feedback-based Select-Project-Join (SPJ) query refinement in databases. In FlexIQ, the user feedback is used to discover the query intent. In addition, we have used the skyline operator to confine the search space of the proposed query refinement algorithms. The user feedback consists of both unexpected information currently present in the query output and expected information that is missing from the query output. Once the feedback is given by the user, our framework refines the initial query by exploiting the skyline operator to minimize the unexpected information as well as maximize the expected information in the refined query output. In our framework, the user can also control different quality metric such as quality of results (e.g., false positive rates, false negative rates and accuracy) and complexity (i.e., quantified as the number of subqueries) in the refined query. We have validated our framework both theoretically and experimentally. In particular, we have demonstrated the effectiveness of our proposed framework by comparing its performance with the naï ve decision tree based query refinement.
97|-||Efficient implementation of chaotic image encryption in transform domains|The primary goal of this paper is security management in data image transmission and storage. Because of the increased use of images in industrial operations, it is necessary to protect the secret data of the image against unauthorized access. In this paper, we introduce a novel approach for image encryption based on employing a cyclic shift and the 2-D chaotic Baker map in different transform domains. The Integer Wavelet Transform (IWT), the Discrete Wavelet Transform (DWT), and the Discrete Cosine Transform (DCT) are exploited in the proposed encryption approach. The characteristics of the transform domains are studied and used to carry out the chaotic encryption. A comparison study between the transform-domain encryption approaches in the presence of attacks shows the superiority of encryption in the DWT domain.
97|-||RGB color image encryption based on Choquet fuzzy integral|In recent years, one can see an increasing interest in the security of digital images. This research presents a new RGB color image encryption using keystream generator based on Choquet fuzzy integral (CFI). The properties of the dynamical keystream generator with mathematical analysis are presented in this work. In the proposed method, the CFI is first used to generate pseudo-random keystreams. Then, each of the color pixels is decomposed into three gray-level components. The output of the CFI is used to randomly shift the bits of three gray-level components. Finally, three components of RGB color pixels and the generated keystream are coupled to encrypt the permuted components. Performance aspects of the proposed algorithm such as the entropy analysis, differential analysis, statistical analysis, cipher random analysis, and cipher sensitivity analysis are introduced to evaluate the security of the new scheme. The experimental results reveal the fact that the proposed algorithm is suitable for practical use in protecting the security of digital image information distributed via the Internet.
97|-||Processes versus people: How should agile software development maturity be defined?|Maturity in software development is currently defined by models such as CMMI-DEV and ISO/IEC 15504, which emphasize the need to manage, establish, measure and optimize processes. Teams that develop software using these models are guided by defined, detailed processes. However, an increasing number of teams have been implementing agile software development methods that focus on people rather than processes. What, then, is maturity for these agile teams that focus less on detailed, defined processes? This is the question we sought to answer in this study. To this end, we asked agile practitioners about their perception of the maturity level of a number of practices and how they defined maturity in agile software development. We used cluster analysis to analyze quantitative data and triangulated the results with content analysis of the qualitative data. We then proposed a new definition for agile software development maturity. The findings show that practitioners do not see maturity in agile software development as process definition or quantitative management capabilities. Rather, agile maturity means fostering more subjective capabilities, such as collaboration, communication, commitment, care, sharing and self-organization.
97|-||A learning-based module extraction method for object-oriented systems|Developers apply object-oriented (OO) design principles to produce modular, reusable software. Therefore, service-specific groups of related software classes called modules arise in OO systems. Extracting the modules is critical for better software comprehension, efficient architecture recovery, determination of service candidates to migrate legacy software to a service-oriented architecture, and transportation of such services to cloud-based distributed systems. In this study, we propose a novel approach to automatic module extraction to identify services in OO software systems. In our approach, first we create a weighted and directed graph of the software system in which vertices and edges represent the classes and their relations, respectively. Then, we apply a clustering algorithm over the graph to extract the modules. We calculate the weight of an edge by considering its probability of being within a module or between modules. To estimate these positional probabilities, we propose a machine-learning-based classification system that we train with data gathered from a real-world OO reference system. We have implemented an automatic module extraction tool and evaluated the proposed approach on several open-source and industrial projects. The experimental results show that the proposed approach generates highly accurate decompositions that are close to authoritative module structures and outperforms existing methods.
97|-||Improving the communication performance of distributed animation rendering using BitTorrent file system|Rendering is a crucial process in the production of computer generated animation movies. It executes a computer program to transform 3D models into series of still images, which will eventually be sequenced into a movie. Due to the size and complexity of 3D models, rendering process becomes a tedious, time-consuming and unproductive task on a single machine. Accordingly, animation rendering is commonly carried out in a distributed computing environment where numerous computers execute in parallel to speedup the rendering process. In accordance with distribution of computing, data dissemination to all computers also needs certain mechanisms which allow large 3D models to be efficiently moved to those distributed computers to ensure the reduction of time and cost in animation production. This paper presents and evaluates BitTorrent file system (BTFS) for improving the communication performance of distributed animation rendering. The BTFS provides an efficient, secure and transparent distributed file system which decouples the applications from complicated communication mechanism. By having data disseminated in a peer-to-peer manner and using local cache, rendering time can be reduced. Its performance comparison with a production-grade 3D animation favorably shows that the BTFS outperforms traditional distributed file systems by more than 3 times in our test configuration.
97|-||Estimating confidence interval of software reliability with adaptive testing strategy|Software reliability assessment is a critical problem in safety-critical and mission-critical systems. In the reliability assessment of such a system, both an accurate reliability estimate and a tight confidence interval are required. Adaptive testing (AT) is an on-line testing framework, which dynamically selects test cases from different subdomains to achieve some optimization object. Although AT has been proved effective in minimizing reliability estimator variance, its performance on providing the corresponding confidence interval has not been investigated. In order to address this issue, an AT strategy combined with Bayesian inference (AT-BI) is proposed in this study. The novel AT-BI strategy is expected to be effective in providing both a low-variance estimator and a tight confidence interval. Experiments are set up to validate the effectiveness of the AT-BI strategy.
98|-|http://www.sciencedirect.com/science/journal/01641212/98|âWith a little help from new friendsâ: Boosting information cascades in social networks based on link injection|We investigate information cascades in the context of viral marketing applications. Recent research has identified that communities in social networks may hinder cascades. To overcome this problem, we propose a novel method for injecting social links in a social network, aiming at boosting the spread of information cascades. Unlike the proposed approach, existing link prediction methods do not consider the optimization of information cascades as an explicit objective. In our proposed method, the injected links are being predicted in a collaborative-filtering fashion, based on factorizing the adjacency matrix that represents the structure of the social network. Our method controls the number of injected links to avoid an “aggressive” injection scheme that may compromise the experience of users. We evaluate the performance of the proposed method by examining real data sets from social networks and several additional factors. Our results indicate that the proposed scheme can boost information cascades in social networks and can operate as a “people recommendations” strategy complementary to currently applied methods that are based on the number of common neighbors (e.g., “friend of friend”) or on the similarity of user profiles.
98|-||Toward a new aspect-mining approach for multi-agent systems|Many aspect mining techniques have been proposed for object-oriented systems. Unfortunately, aspect mining for multi-agent systems is an unexplored research area. The inherent specificities of multi-agent systems (such as autonomy, pro-activity, reactivity, and adaptability) make it difficult to understand, reuse and maintain their code. We propose, in this paper, a (semi-automatic) hybrid aspect mining approach for agent-oriented code. The technique is based on both static and dynamic analyzes. The main motivations of this work are (1) identifying cross-cutting concerns in existing agent-oriented code, and (2) making them explicitly available to software engineers involved in the evolution of agent-oriented code in order to facilitate its refactoring and, consequently, to improve its understandability, reusability and maintainability. The proposed approach is supported by a software tool, called MAMIT (MAS Aspect-MIning Tool), that we developed. The approach and the associated tool are illustrated using a concrete case study.
98|-||DRE system performance optimization with the SMACK cache efficiency metric|System performance improvements are critical for the resource-limited environment of multiple integrated applications executing inside a single distributed real-time and embedded (DRE) system, such as integrated avionics platform or vehtronics systems. While processor caches can effectively reduce execution time there are several factors, such as cache size, system data sharing, and task execution schedule, which make it hard to quantify, predict, and optimize the cache usage of a DRE system. This article presents SMACK, a novel heuristic for estimating the hardware cache usage of a DRE system, and describes a method of varying the runtime behavior of DRE system software without (1) requiring extensive safety recertification or (2) violating the real-time scheduling deadlines. By using SMACK as a maximization target, we were able to reduce integrated DRE system execution time by an average of 2.4% and a maximum of 4.34%.
98|-||WAS: A weighted attribute-based strategy for cluster test selection|In past decades, many techniques have been proposed to generate and execute test cases automatically. However, when a test oracle does not exist, execution results have to be examined manually. With increasing functionality and complexity of today's software, this process can be extremely time-consuming and mistake-prone. A CTS-based (cluster test selection) strategy provides a feasible solution to mitigate such deficiency by examining the execution results only with respect to a small number of selected test cases. It groups test cases with similar execution profiles into the same cluster and selects them from each cluster. Some well-known CTS-based strategies are one per cluster, n (a predefined value which is greater than 1) per cluster, adaptive sampling, and execution-spectra-based sampling (ESBS). The ultimate goal is to reduce testing cost by quickly identifying the executions that are likely to fail. However, improperly grouping the test cases will significantly diminish the effectiveness of these strategies (by examining results of more successful executions and fewer failed executions). To overcome this problem, we propose a weighted attribute-based strategy (WAS). Instead of clustering test cases based on the similarity of their execution profiles only once like the aforementioned CTS-based strategies, WAS will conduct more than one iteration of clustering using weighted execution profiles by also considering the suspiciousness of each program element (statement, basic block, decision, etc.), where the suspiciousness in terms of the likelihood of containing bugs can be computed by using various software fault localization techniques. Case studies using seven programs (make, ant, sed, flex, grep, gzip, and space) and four CTS-based strategies (one per cluster sampling, n per cluster sampling, adaptive sampling, and ESBS) were conducted to evaluate the effectiveness of WAS on 184 faulty versions containing either single or multiple bugs. Experimental results suggest that the proposed WAS strategy outperforms other four CTS-based strategies with respect to both recall and precision such that output verification is focused more strongly on failed executions.
98|-||Selecting software reliability growth models and improving their predictive accuracy using historical projects data|During software development two important decisions organizations have to make are: how to allocate testing resources optimally and when the software is ready for release. SRGMs (software reliability growth models) provide empirical basis for evaluating and predicting reliability of software systems. When using SRGMs for the purpose of optimizing testing resource allocation, the model's ability to accurately predict the expected defect inflow profile is useful. For assessing release readiness, the asymptote accuracy is the most important attribute. Although more than hundred models for software reliability have been proposed and evaluated over time, there exists no clear guide on which models should be used for a given software development process or for a given industrial domain.
98|-||A method to optimize the scope of a software product platform based on end-user features|Due to increased competition and the advent of mass customization, many software firms are utilizing product families – groups of related products derived from a product platform – to provide product variety in a cost-effective manner. The key to designing a successful software product family is the product platform, so it is important to determine the most appropriate product platform scope related to business objectives, for product line development.
98|-||Modeling and analysis of customer premise equipments registration process in IEEE 802.22 WRAN cell|The development of the IEEE 802.22 standard is aimed at providing broadband access in rural areas by effectively utilizing the unused TV band, provided no harmful interference is caused to the incumbent operation. The motivation behind TV band selection is of having lower frequencies compared to other licensed bands, which, therefore, results in lower propagation path loss. Due to this quality, the spectral power density of the radio signal reduces slowly, which results in a high coverage area. Further, it has been observed that many TV channels largely remain unoccupied, as most households and businesses rely on cable and satellite TV services. This is the first international standard for a wireless regional area network (WRAN) based on cognitive radio technologies. This standard provides both PHY and MAC layer functionalities in an infrastructure based network for communication between customer premise equipments (CPEs) through a base station (BS). The Spectrum Manager is the central part of the BS, which plays a significant role in maintaining spectrum availability information, channel selection, channel management, scheduling quiet periods for spectrum sensing, accessing to the database and implementing IEEE 802.22 policies. A WRAN can particularly accommodate up to 512 CPEs in a cell. Contention may occur during initial ranging, periodic ranging, bandwidth request and urgent coexistence situation notification. The medium access control (MAC) incorporates several schemes to control contention between CPEs within a cell and overlapping cells sharing the same channel. A CPE has to make decision to resolve collisions in the upstream direction. In the case of initial ranging and periodic ranging, code division multiple access (CDMA) is employed to resolve collisions. For bandwidth and UCS notification, either a CDMA or exponential time backoff approach can be applied for collision resolution. This paper presents the analytical framework to evaluate the number of active CPEs in a cognitive radio network. It is important to note that when the arrival rate becomes equal to the service rate, the active CPEs curve attains a constant value. Further, the active CPEs length is highly dependent on service rate. The different special cases have been addressed and the effectiveness of the proposed framework has been validated through various evaluation results.
98|-||Blending design patterns with aspects: A quantitative study|Design patterns often need to be blended (or composed) when they are instantiated in a software system. The composition of design patterns consists of assigning multiple pattern elements into overlapping sets of classes in a software system. Whenever the modularity of each design pattern is not preserved in the source code, their implementation becomes tangled with each other and with the classes’ core responsibilities. As a consequence, the change or removal of each design pattern will be costly or prohibitive as the software system evolves. In fact, composing design patterns is much harder than instantiating them in an isolated manner. Previous studies have found design pattern implementations are naturally crosscutting in object-oriented systems, thereby making it difficult to modularly compose them. Therefore, aspect-oriented programming (AOP) has been pointed out as a natural alternative for modularizing and blending design patterns. However, there is little empirical knowledge on how AOP models influence the composability of widely used design patterns. This paper investigates the influence of using AOP models for composing the Gang-of-Four design patterns. Our study categorizes different forms of pattern composition and studies the benefits and drawbacks of AOP in these contexts. We performed assessments of several pair-wise compositions taken from 3 medium-sized systems implemented in Java and two AOP models, namely, AspectJ and Compose*. We also considered complex situations where more than two patterns involved in each composition, and the patterns were interacting with other aspects implementing other crosscutting concerns of the system. In general, we observed two dominant factors impacting the pattern composability with AOP: (i) the category of the pattern composition, and (ii) the AspectJ idioms used to implement the design patterns taking part in the composition.
98|-||A new chaotic map based image encryption schemes for several image formats|This paper proposes several image encryption schemes for popular image formats as Joint Photographic Experts Group (JPEG), Graphics Interchange Format (GIF), Portable Network Graphics (PNG), and Tagged Image File Format (TIFF). A cross chaotic map proposed based on Devaney's theory and dynamic block dividing of the 3D baker using the cross chaotic map are used for diffusion and permutation in encryption. Moreover, in order to verify user's identity, authentication is carried out using information hiding based on the cross chaotic function. In our methods, image files syntax and structure are not destructed, and the original image can be recovered lossless. For GIF, it keeps the property of animation successfully. The security test results indicate the proposed methods have high security, and the speed of our algorithm is faster than classical solutions. JPEG, GIF, TIFF and PNG image formats are popular contemporarily. Therefore this paper shows that the prospect of chaotic image encryption is promising.
98|-||On building a consistent framework for executable systems architecture|The paper presents a framework for executable systems architecture. Termed as Consistent Systems Architecture Description and Behavior Framework (CSADBF), the framework shows how consistency can be maintained while modeling architectural description of systems as well as their behavior. Convergence of three established modeling techniques: ontology, UML, and Colored Petri Nets (CPN), is used to develop this framework. Each tool complements others in accomplishing the goal of consistency maintenance for the executable systems architecture. The framework suggests various mapping schemes that help in establishing strong concordance among different artifacts of these modeling techniques and maintaining consistency of overall system architecture. The first scheme maps OWL ontology to UML and is responsible for maintaining consistency of the architectural description. The second scheme maps combination of OWL ontology and UML to CPN and is responsible for maintaining consistency between static and dynamic views. The third scheme ensures the behavioral consistency of the architecture by providing mapping between Semantic Web Rule Language (SWRL) and CPN Guard conditions. Thus, the framework allows architects to model the systems architecture requirements in OWL ontology and UML and to analyze the behavior and performance of systems architecture in CPN. The paper demonstrates the framework with the help of a case study and also compares it with the existing frameworks.
98|-||Investigating the applicability of Agility assessment surveys: A case study|Agile software development has become popular in the past decade without being sufficiently defined. The Agile principles can be instantiated differently which creates different perceptions of Agility. This has resulted in several frameworks being presented in the research literature to evaluate the level of Agility. However, the evidence of their actual use in practice is limited.
98|-||Generating combinatorial test suite using combinatorial optimization|Combinatorial testing (CT) is an effective technique to test software with multiple configurable parameters. It is used to detect interaction faults caused by the combination effect of parameters. CT test generation aims at generating covering arrays that cover all t-way parameter combinations, where t is a given covering strength. In practical CT usage scenarios, there are usually constraints between parameters, and the performance of existing constraint-handling methods degrades fast when the number of constraints increases.
99|-|http://www.sciencedirect.com/science/journal/01641212/99|PROW: A Pairwise algorithm with constRaints, Order and Weight|Testing systems with many variables and/or values is often quite expensive due to the huge number of possible combinations to be tested. There are several criteria available to combine test data and produce scalable test suites. One of them is pairwise. With the pairwise criterion, each pair of values of any two parameters is included in at least one test case. Although this is a widely-used coverage criterion, two main characteristics improve considerably pairwise: constraints handling and prioritisation.
99|-||Towards energy-efficient scheduling for real-time tasks under uncertain cloud computing environment|Green cloud computing has become a major concern in both industry and academia, and efficient scheduling approaches show promising ways to reduce the energy consumption of cloud computing platforms while guaranteeing QoS requirements of tasks. Existing scheduling approaches are inadequate for real-time tasks running in uncertain cloud environments, because those approaches assume that cloud computing environments are deterministic and pre-computed schedule decisions will be statically followed during schedule execution. In this paper, we address this issue. We introduce an interval number theory to describe the uncertainty of the computing environment and a scheduling architecture to mitigate the impact of uncertainty on the task scheduling quality for a cloud data center. Based on this architecture, we present a novel scheduling algorithm (PRS1) that dynamically exploits proactive and reactive scheduling methods, for scheduling real-time, aperiodic, independent tasks. To improve energy efficiency, we propose three strategies to scale up and down the system's computing resources according to workload to improve resource utilization and to reduce energy consumption for the cloud data center. We conduct extensive experiments to compare PRS with four typical baseline scheduling algorithms. The experimental results show that PRS performs better than those algorithms, and can effectively improve the performance of a cloud data center.
99|-||Aggregate-strength interaction test suite prioritization|Combinatorial interaction testing is a widely used approach. In testing, it is often assumed that all combinatorial test cases have equal fault detection capability, however it has been shown that the execution order of an interaction test suite's test cases may be critical, especially when the testing resources are limited. To improve testing cost-effectiveness, test cases in the interaction test suite can be prioritized, and one of the best-known categories of prioritization approaches is based on “fixed-strength prioritization”, which prioritizes an interaction test suite by choosing new test cases which have the highest uncovered interaction coverage at a fixed strength (level of interaction among parameters). A drawback of these approaches, however, is that, when selecting each test case, they only consider a fixed strength, not multiple strengths. To overcome this, we propose a new “aggregate-strength prioritization”, to combine interaction coverage at different strengths. Experimental results show that in most cases our method performs better than the test-case-generation, reverse test-case-generation, and random prioritization techniques. The method also usually outperforms “fixed-strength prioritization”, while maintaining a similar time cost.
99|-||A practical approach to the assessment of quality in use of corporate web sites|The paper presents a practical approach to web site quality, based on a novel perspective that considers the relationships between the web site and its stakeholders. This perspective leads to identify four fundamental concepts of quality: final quality, quality in use, basic quality and internal quality. This paper focuses on quality in use, and proposes a new quality model including a well structured and balanced set of characteristics and sub-characteristics, which aim at capturing the main dimensions that impact on the quality of a web site. The distinction between actual and expected quality is then introduced and a practical assessment methodology for expected quality (EQ-EVAL) is proposed, which employs expert evaluators instead of actual users in order to make the evaluation less expensive, without sacrificing, however, accuracy and reliability. The results of the application of the methodology in the evaluation of a sample set of corporate web sites are finally discussed, showing how the model and the methodology can indeed meet the stated requirements.
99|-||Integrating mixed transmission and practical limitations with the worst-case response-time analysis for Controller Area Network|The existing worst-case response-time analysis for Controller Area Network (CAN) calculates upper bounds on the response times of messages that are queued for transmission either periodically or sporadically. However, it does not support the analysis of mixed messages. These messages do not exhibit a periodic activation pattern and can be queued for transmission both periodically and sporadically. They are implemented by several higher-level protocols based on CAN that are used in the automotive industry. We extend the existing analysis to support worst-case response-time calculations for periodic and sporadic as well as mixed messages. Moreover, we integrate the effect of hardware and software limitations in the CAN controllers and device drivers such as abortable and non-abortable transmit buffers with the extended analysis. The extended analysis is applicable to any higher-level protocol for CAN that uses periodic, sporadic and mixed transmission modes.
99|-||Enhanced fixed-priority real-time scheduling on multi-core platforms by exploiting task period relationship|One common approach for multi-core partitioned scheduling problem is to transform this problem into a traditional bin-packing problem, with the utilization of a task being the “size” of the object and the utilization bound of a processing core being the “capacity” of the bin. However, this approach ignores the fact that some implicit relations among tasks may significantly affect the feasibility of the tasks allocated to each local core. In this paper, we study the problem of partitioned scheduling of periodic real-time tasks on multi-core platforms under the Rate Monotonic Scheduling (RMS) policy. We present two effective and efficient partitioned scheduling algorithms, i.e. PSER and HAPS, by exploiting the fact that the utilization bound of a task set increases as task periods are closer to harmonic on a single-core platform. We formally prove the schedulability of our partitioned scheduling algorithms. Our extensive experimental results demonstrate that the proposed algorithms can significantly improve the scheduling performance compared with the existing work.
99|-||Modelling large-scale information systems using ADLs â An industrial experience report|An organisation that had developed a large information system wanted to embark on a programme that would involve large-scale evolution of it. As a precursor to this, it was decided to create a comprehensive architectural description to capture and understand the system's design. This undertaking faced a number of challenges, including a low general awareness of software modelling and software architecture practices. The approach taken by the software architects tasked with this project included the definition of a simple, very specific, architecture description language. This paper reports our experience of the project and a simple ADL that we created as part of it.
99|-||Recommender systems based on social networks|The traditional recommender systems, especially the collaborative filtering recommender systems, have been studied by many researchers in the past decade. However, they ignore the social relationships among users. In fact, these relationships can improve the accuracy of recommendation. In recent years, the study of social-based recommender systems has become an active research topic. In this paper, we propose a social regularization approach that incorporates social network information to benefit recommender systems. Both users’ friendships and rating records (tags) are employed to predict the missing values (tags) in the user-item matrix. Especially, we use a biclustering algorithm to identify the most suitable group of friends for generating different final recommendations. Empirical analyses on real datasets show that the proposed approach achieves superior performance to existing approaches.
99|-||Integrating non-parametric models with linear components for producing software cost estimations|A long-lasting endeavor in the area of software project management is minimizing the risks caused by under- or over-estimations of the overall effort required to build new software systems. Deciding which method to use for achieving accurate cost estimations among the many methods proposed in the relevant literature is a significant issue for project managers. This paper investigates whether it is possible to improve the accuracy of estimations produced by popular non-parametric techniques by coupling them with a linear component, thus producing a new set of techniques called semi-parametric models (SPMs). The non-parametric models examined in this work include estimation by analogy (EbA), artificial neural networks (ANN), support vector machines (SVM) and locally weighted regression (LOESS). Our experimentation shows that the estimation ability of SPMs is superior to their non-parametric counterparts, especially in cases where both a linear and non-linear relationship exists between software effort and the related cost drivers. The proposed approach is empirically validated through a statistical framework which uses multiple comparisons to rank and cluster the models examined in non-overlapping groups performing significantly different.
99|-||Bringing Test-Driven Development to web service choreographies|Choreographies are a distributed approach for composing web services. Compared to orchestrations, which use a centralized scheme for distributed service management, the interaction among the choreographed services is collaborative with decentralized coordination. Despite the advantages, choreography development, including the testing activities, has not yet evolved sufficiently to support the complexity of the large distributed systems. This substantially impacts the robustness of the products and overall adoption of choreographies. The goal of the research described in this paper is to support the Test-Driven Development (TDD) of choreographies to facilitate the construction of reliable, decentralized distributed systems. To achieve that, we present Rehearsal, a framework supporting the automated testing of choreographies at development-time. In addition, we present a choreography development methodology that guides the developer on applying TDD using Rehearsal. To assess the framework and the methodology, we conducted an exploratory study with developers, whose result was that Rehearsal was considered very helpful for the application of TDD and that the methodology helped the development of robust choreographies.
99|-||Adaptive thermal-aware task scheduling for multi-core systems|Thermal management is a challenging problem because of on-line thermal dynamics. An adaptive thermal-aware multi-core task scheduling framework based on run-time controllers is proposed in this paper to address the inter-core thermal effects and dynamic variations of task execution. In contrast to dynamic voltage scaling, the service rates of the tasks are adjusted in order to cool the system. Scheduling algorithms are used to prevent the system from overheating and maximize system utilization. This paper also evaluates the capability of the proposed framework through varying the workloads, ultimately demonstrating positive and stable performance.
99|-||Cost, benefits and quality of software development documentation: A systematic mapping|Software documentation is an integral part of any software development process. Researchers and practitioners have expressed concerns about costs, benefits and quality of software documentation in practice. On the one hand, there is a lack of a comprehensive model to evaluate the quality of documentation. On the other hand, researchers and practitioners need to assess whether documentation cost outweighs its benefit.
99|-||VM scaling based on Hurst exponent and Markov transition with empirical cloud data|One of the major benefits of cloud computing is virtualization scaling. Compared to existing studies on virtual machine scaling, this paper introduces Hurst exponent which gives additional characteristics for data trends to supplement the often used Markov transition approach. This approach captures both the long and short-term behaviors of the virtual machines (VMs). The dataset for testing of this approach was gathered from the computer usage of key servers supporting a large university. Performance evaluation shows our approach can assist prediction of VM CPU usage toward effective resource allocation. In turn, this allows the cloud resource provider to monitor and allocate the resource usage of all VMs in order to meet the service level agreements for each VM client.
100|-|http://www.sciencedirect.com/science/journal/01641212/100|A controlled experiment to evaluate the understandability of KAOS and i* for modeling Teleo-Reactive systems|Teleo-Reactive (TR) specifications allow engineers to define the behavior of reactive systems while taking into account goals and changes in the state of the environment.
100|-||Iterated local search for microaggregation|Microaggregation is a disclosure control method used to protect microdata. We introduce a local search method and employ it in an iterated local search algorithm for the NP-hard minimum information loss microaggregation problem. Experimental results with benchmark data sets demonstrate that our algorithm consistently identifies better quality solutions than extant microaggregation methods.
100|-||Web API growing pains: Loosely coupled yet strongly tied|Web APIs provide a systematic and extensible approach for application-to-application interaction. Developers using web APIs are forced to accompany the API providers in their software evolution tasks. In order to understand the distress caused by this imposition on web API client developers we perform a semi-structured interview with six such developers. We also investigate how major web API providers organize their API evolution, and we explore how this affects source code changes of their clients. Our exploratory qualitative study of the Twitter, Google Maps, Facebook and Netflix web APIs analyzes the state of web API evolution practices and provides insight into the impact of service evolution on client software. In order to complement the picture and also understand how web API providers deal with evolution, we investigate the server-side and client-side evolution of two open-source web APIs, namely VirtualBox and XBMC. Our study is complemented with a set of observations regarding best practices for web API evolution.
100|-||Enhanced healthcare personnel rostering solution using mobile technologies|This paper presents a novel personnel rostering system for healthcare units, which incorporates mobile technologies to minimize time overheads and boost personnel satisfaction. This way, doctors nurses and administrative staff may provide solutions and suggestions to the process of shifts’ scheduling and rostering in a group based – social and organized manner, at any given time, using their smartphone or tablet. This system is designed and implemented according to wide research on requirements’ specification, carried out in Greek public hospitals and based on a study of healthcare units’ organization, at a practical and legal level. The personnel rostering system anticipates to facilitate the staff administration task, through real-time communication between hospital's personnel. It enables the formation of a micro-community with enhanced social communication tools, to provide dynamic management, recording and updating of changes that occur in scheduled duties, without mediators and delays. The proposed solution includes an intelligent mobile device application, designed for smartphones and tablets. It is provided to the personnel and enables them to participate in the process of scheduling duties and shifts. The XML based, back-end, supporting information system offers services that allow a smoother operation of the unit, minimize time overheads in case of arbitrary changes and maximize satisfaction of personnel. The overall operation of the units, that reclaim the features offered by this system, can be improved. Minimizing the time and other bureaucratic delays in personnel scheduling is a vital part of the way a healthcare facility is organized. Thus, facilitating this process, with any available technology, may prove to be cost effective and crucial. Systems that incorporate mobile applications are already widely accepted, and become increasingly important to the healthcare sector, as well. The mobile based, personnel shifts’ scheduling solution shown is an approach that already receives encouraging support and indicates that it assists in achieving remarkable results.
100|-||Integrating usability work into a large inter-organisational agile development project: Tactics developed by usability designers|In this paper we examine the integration of usability activities into a large inter-organisational agile development project. Inter-organisational agile projects possess unique attributes. They involve multiple stakeholders from different organisational contexts and are thus characterised by competing priorities. Team members also lack a mutual awareness of what constitutes work. These issues make the collaboration between project teams challenging. Meanwhile collaboration between usability designers and agile project teams is an integral part of the integration of usability activities into agile development projects. We carried out an interpretive case study on a large inter-organisational agile development project to examine how usability designers and agile project teams collaborate in this project type. Results showed integration goals were achieved through five tactics deployed by the usability designers. These tactics were negotiating inclusion; upward influencing, placating expert users, establishing credibility and diffusing designs. The implications of these findings are summarised in the form of three propositions that pertain to how usability designer–agile project team collaborations might be organised in agile development projects. Further, the role of the usability designer in ensuring the integration of usability activities is also emphasised.
100|-||MostoDEx: A tool to exchange RDF data using exchange samples|The Web is evolving into a Web of Data in which RDF data are becoming pervasive, and it is organised into datasets that share a common purpose but have been developed in isolation. This motivates the need to devise complex integration tasks, which are usually performed using schema mappings; generating them automatically is appealing to relieve users from the burden of handcrafting them. Many tools are based on the data models to be integrated: classes, properties, and constraints. Unfortunately, many data models in the Web of Data comprise very few or no constraints at all, so relying on constraints to generate schema mappings is not appealing. Other tools rely on handcrafting the schema mappings, which is not appealing at all. A few other tools rely on exchange samples but require user intervention, or are hybrid and require constraints to be available. In this article, we present MostoDEx, a tool to generate schema mappings between two RDF datasets. It uses a single exchange sample and a set of correspondences, but does not require any constraints to be available or any user intervention. We validated and evaluated MostoDEx using many experiments that prove its effectiveness and efficiency in practice.
100|-||Comprehensible software fault and effort prediction: A data mining approach|Software fault and effort prediction are important tasks to minimize costs of a software project. In software effort prediction the aim is to forecast the effort needed to complete a software project, whereas software fault prediction tries to identify fault-prone modules. In this research both tasks are considered, thereby using different data mining techniques. The predictive models not only need to be accurate but also comprehensible, demanding that the user can understand the motivation behind the model's prediction. Unfortunately, to obtain predictive performance, comprehensibility is often sacrificed and vice versa. To overcome this problem, we extract trees from well performing Random Forests (RFs) and Support Vector Machines for regression (SVRs) making use of a rule extraction algorithm ALPA. This method builds trees (using C4.5 and REPTree) that mimic the black-box model (RF, SVR) as closely as possible. The proposed methodology is applied to publicly available datasets, complemented with new datasets that we have put together based on the Android repository. Surprisingly, the trees extracted from the black-box models by ALPA are not only comprehensible and explain how the black-box model makes (most of) its predictions, but are also more accurate than the trees obtained by working directly on the data.
100|-||Profiling and classifying the behavior of malicious codes|Malware is a major security threat confronting computer systems and networks and has increased in scale and impact from the early days of ICT. Traditional protection mechanisms are largely incapable of dealing with the diversity and volume of malware variants which is evident today. This paper examines the evolution of malware including the nature of its activity and variants, and the implication of this for computer security industry practices.
100|-||A benchmarking process to assess software requirements documentation for space applications|Poorly written requirements are a common source of software defects and, in application areas like space systems, the cost of malfunctioning software can be very high. This work proposes a benchmarking procedure for assessing the quality of software requirements that adopt the Packet Utilization Standard (PUS) defined by the European Cooperation for Space Standardization (ECSS) standards. The benchmark uses three checklists that aim at guaranteeing that the specifications comply with the PUS standard, consider faulty behaviour, and do not include errors typically found in this type of documents. The benchmark is defined for two services of the PUS standard: the telecommand verification and on board operating scheduling. A benchmark validation approach is also proposed in the paper. It uses the concept of fault injection to insert known errors in software requirements specification documents. The benchmark validation is performed through its application to three projects from different countries. Results show that our proposal provides a simple and effective way for identifying weaknesses and compare the degree of maturity of requirements documents.
100|-||From source code identifiers to natural language terms|Program comprehension techniques often explore program identifiers, to infer knowledge about programs. The relevance of source code identifiers as one relevant source of information about programs is already established in the literature, as well as their direct impact on future comprehension tasks.
100|-||A computer system architecture providing a user-friendly man machine interface for accessing assistive technology in cloud computing|Assistive Technology (AT) includes hardware peripherals, software applications and systems that enable a user with a disability to use a PC. Thus, when a disabled user needs to work in a particular environment (e.g., at work, at school, in a government office, etc.) he/she has to properly configure the used PC. However, often, the configuration of AT software interfaces is not trivial at all. This paper presents the software design, implementation, and evaluation of a computer system architecture providing a software user-friendly man machine interface for accessing AT software in cloud computing. The main objective of such an architecture is to provide a new type of software human–computer interaction for accessing AT services over the cloud. Thus, end users can interact with their personalized computer environments using any physical networked PC. The advantage of this approach is that users do not have to install and/or setup any additional software on physical PCs and they can access their own AT virtual environments from everywhere. In particular, the usability of prototype based on the Remote Desktop Protocol (RDP) is evaluated in both private and public cloud scenarios.
100|-||Defining multi-tenancy: A systematic mapping study on the academic and the industrial perspective|Software as a service is frequently offered in a multi-tenant style, where customers of the application and their end-users share resources such as software and hardware among all users, without necessarily sharing data. It is surprising that, with such a popular paradigm, little agreement exists with regard to the definition, domain, and challenges of multi-tenancy. This absence is detrimental to the research community and the industry, as it hampers progress in the domain of multi-tenancy and enables organizations and academics to wield their own definitions to further their commercial or research agendas.
100|-||Extracting REST resource models from procedure-oriented service interfaces|During the past decade a number of procedure-oriented protocols and standards have emerged for making service-offering systems available on the Web. The WS-* stack of protocols is the most prevalent example. However, this procedure and message-oriented approach has not aligned with the true potential of the Web's own architectural principles, such as the uniform identification and manipulation of resources, caching, hypermedia, and layering. In this respect, Resource Oriented Architectures based on the REST architectural style, have been proposed as a possible alternative to the operation-based view of service offerings. To date, compiling a REST API for back-end procedure-oriented services is considered as a manual process that requires as input specialized models, such as, service requirements and behavioral models. In this paper, we propose a resource extraction method in which service descriptions are analyzed, using natural language processing techniques and graph transformations, in order to yield a collection of hierarchically organized elements forming REST resources that semantically correspond to the functionality offered by the service. The proposed approach has been applied as a proof of concept with positive results, for the extraction of resource models from a sizable number of procedure-oriented Web Service interfaces that have been obtained from an open service directory.
100|-||An imperfect software debugging model considering log-logistic distribution fault content function|Numerous software reliability growth models based on the non-homogeneous Poisson process assume perfect debugging. Such models, including the Goel–Okumoto, delayed S-shaped, and inflection S-shaped models, have been successfully validated in software testing. However, complex and uncertain test factors, such as test resource, tester skill, or test tool, can seriously affect the testing process. When detected faults are removed, new faults can be introduced in practical testing. The process is referred to as imperfect debugging. Imperfect software debugging models proposed in the literature generally assume a constantly or monotonically decreasing fault introduction rate per fault. These models cannot adequately describe the fault introduction process in a practical test. In this study, we propose an imperfect software debugging model that considers a log-logistic distribution fault content function, which can capture the increasing and decreasing characteristics of the fault introduction rate per fault. We also use several historical fault data sets to validate the performance of the proposed model. The model can suitably fit historical fault data and accurately predict failure behavior. Confidence interval and sensitivity analyses are also conducted.
100|-||Using SAN formalism to evaluate Follow-The-Sun project scenarios|Performance evaluation of projects can be used by companies and institutions as a tool to help the decision making process of Follow-The-Sun (FTS) projects. This paper main goal is to discuss a stochastic model definition to evaluate the performance of different aspects of FTS projects. Examples that can be addressed using the FTS model are provided with results comparing different model instances to evaluate aspects such as project execution time and project costs composition.
100|-||Dynamic cloud service selection using an adaptive learning mechanism in multi-cloud computing|Cloud service selection in a multi-cloud computing environment is receiving more and more attentions. There is an abundance of emerging cloud service resources that makes it hard for users to select the better services for their applications in a changing multi-cloud environment, especially for online real time applications. To assist users to efficiently select their preferred cloud services, a cloud service selection model adopting the cloud service brokers is given, and based on this model, a dynamic cloud service selection strategy named DCS is put forward. In the process of selecting services, each cloud service broker manages some clustered cloud services, and performs the DCS strategy whose core is an adaptive learning mechanism that comprises the incentive, forgetting and degenerate functions. The mechanism is devised to dynamically optimize the cloud service selection and to return the best service result to the user. Correspondingly, a set of dynamic cloud service selection algorithms are presented in this paper to implement our mechanism. The results of the simulation experiments show that our strategy has better overall performance and efficiency in acquiring high quality service solutions at a lower computing cost than existing relevant approaches.
100|-||D-P2P-Sim+: A novel distributed framework for P2P protocols performance testing|In recent technologies like IoT (Internet of Things) and Web 2.0, a critical problem arises with respect to storing and processing the large amount of collected data. In this paper we develop and evaluate distributed infrastructures for storing and processing large amount of such data. We present a distributed framework that supports customized deployment of a variety of indexing engines over million-node overlays. The proposed framework provides the appropriate integrated set of tools that allows applications processing large amount of data, to evaluate and test the performance of various application protocols for very large scale deployments (multi million nodes–billions of keys). The key aim is to provide the appropriate environment that contributes in taking decisions regarding the choice of the protocol in storage P2P systems for a variety of big data applications. Using lightweight and efficient collection mechanisms, our system enables real-time registration of multiple measures, integrating support for real-life parameters such as node failure models and recovery strategies. Experiments have been performed at the PlanetLab network and at a typical research laboratory in order to verify scalability and show maximum re-usability of our setup. D-P2P-Sim+ framework is publicly available at http://code.google.com/p/d-p2p-sim/downloads/list.
volume|issue|url|title|abstract
101|-|http://www.sciencedirect.com/science/journal/01641212/101|Multi-criteria scheduling of Bag-of-Tasks applications on heterogeneous interlinked clouds with simulated annealing|Cloud computing has spurred the creation of a multitude of services that use the cloud to deliver their products on-demand. Behind it, stand multiple “Cloud Providers” that in the past few years have created data-centers, spread around the world, creating a mesh of distributed resources that can meet high availability and quality of service requirements. The growing number of cloud clients demand reliability, performance and better cost-to-performance ratios. Recently, scientific research has focused on the optimization of interlinked cloud systems, an aim which requires strategies for allocation of resources and distribution of computing tasks between them, while also considering their cost along with any factors that may differentiate them. In this study, we have evaluated the use of simulated annealing and thermodynamic simulated annealing in the scheduling of a dynamic multi-cloud system with virtual machines of heterogeneous performance serving Bag-of-Tasks applications. The scheduling heuristics applied, consider multiple criteria when scheduling said applications and try to optimize both for performance and cost, while also taking into account the heterogeneity of the virtual machines. Simulation results indicate that the use of these heuristics can have a significant impact in performance while maintaining a good cost-performance trade-off.
101|-||Capturing urgency and parallelism using quasi-deadlines for real-time multiprocessor scheduling|Recent trends toward multi-core architectures in real-time embedded systems pose challenges in designing efficient real-time multiprocessor scheduling algorithms. We believe that it is important to take into consideration both timing constraints of tasks (urgency) and parallelism restrictions of multiprocessor platforms (parallelism) together when designing scheduling algorithms. Motivated by this, we define the quasi-deadline of a job as a weighted sum of its absolute deadline (capturing urgency) and its worst case execution time (capturing parallelism) with a system-level control knob to balance urgency and parallelism effectively. Using the quasi-deadline to prioritize jobs, we propose two new scheduling algorithms, called EQDF (earliest quasi-deadline first) and EQDZL (earliest quasi-deadline until zero laxity), that are categorized into job-level fixed-priority (JFP) scheduling and job-level dynamic-priority (JDP) scheduling, respectively. This paper provides a new schedulability analysis for EQDF/EQDZL scheduling and addresses the problem of priority assignment under EQDF/EQDZL by determining a right value of the system-level control knob. It presents optimal and heuristic solutions to the problem subject to our proposed EQDF and EQDZL analysis. Our simulation results show that EQDF and EQDZL can improve schedulability significantly compared to EDF and EDZL, respectively.
101|-||Enabling improved IR-based feature location|Recent solutions to software engineering problems have incorporated tools and techniques from information retrieval (IR). The use of IR requires choosing an appropriate retrieval model and deciding on a query that best captures a particular information need. Taking feature location as a representative example, three research questions are investigated: (1) the impact of query preprocessing, (2) the impact that different scraping techniques for queries have on retrieval performance, (3) the performance impact that the underlying retrieval model has on identifying the correct source-code functions (the correct documents). These research questions are addressed using the five open source projects released as part of the SEMERU dataset. In the experiments, five methods of scraping queries from modification requests and seven retrieval model instances are considered. Using the standard evaluation metric Mean Reciprocal Rank (MRR), the experimental analysis reveals that better retrieval models are not the ones commonly used by software engineering researchers. Results find that models based on query-likelihood perform about twice as well as models in common use in software engineering such as LSI and thus deserve greater attention. Furthermore, corpus preprocessing has a significant impact as the top performing setting is over 100% better than the average.
101|-||A scalable generic transaction model scenario for distributed NoSQL databases|With the development of cloud computing and internet; e-Commerce, e-Business and corporate world revenue are increasing with high rate. These areas not only require scalable and consistent databases but also require inter database transaction support. In this paper, we present, a scalable three-tier architecture along with a distributed middle-ware protocol to support atomic transactions across heterogeneous NoSQL databases. Our methodology does not compromise on any assumption on the accuracy of failure modalities. Hence, it is suitable for a class of heterogeneous distributed systems. To achieve such a target, our architectural model exploits an innovative methodology to achieve distributed atomic transactions. We simulate this architectural setup with different latency tests under different environments to produce reliable impact and correctness.
101|-||Manufacturing execution systems: A vision for managing software development|Software development suffers from a lack of predictability with respect to cost, time, and quality. Predictability is one of the major concerns addressed by modern manufacturing execution systems (MESs). A MES does not actually execute the manufacturing (e.g., controlling equipment and producing goods), but rather collects, analyzes, integrates, and presents the data generated in industrial production so that employees have better insights into processes and can react quickly, leading to predictable manufacturing processes. In this paper, we introduce the principles and functional areas of a MES. We then analyze the gaps between MES-vision-driven software development and current practices. These gaps include: (1) lack of a unified data collection infrastructure, (2) lack of integrated people data, (3) lack of common conceptual frameworks driving improvement loops from development data, and (4) lack of support for projection and simulation. Finally, we illustrate the feasibility of leveraging MES principles to manage software development, using a Modularity Debt Management Decision Support System prototype we developed. In this prototype we demonstrate that information integration in MES-vision-driven systems enables new types of analyses, not previously available, for software development decision support. We conclude with suggestions for moving current software development practices closer to the MES vision.
101|-||A separation-based UI architecture with a DSL for role specialization|This paper proposes an architecture and associated methodology to separate front end UI concerns from back end coding concerns to improve the platform flexibility, shorten the development time, and increase the productivity of developers. Typical UI development is heavily dependent upon the underlying platform, framework, or tool used to create it, which results in a number of problems. We took a separation-based UI architecture and modified it with a domain specific language to support the independence of UI creation thereby resolving some of the aforementioned problems. A methodology incorporating this architecture into the development process is proposed. A climate science application was created to verify the validity of the methodology using modern practices of UX, DSLs, code generation, and model-driven engineering. Analyzing related work provides an overview of other methods similar to our method. Subsequently we evaluate the climate science application, conclude, and detail future work.
101|-||LAYER: A cost-efficient mechanism to support multi-tenant database as a service in cloud|This paper presents a novel mechanism to cost-efficiently support multi-tenant database as a service (MTDBaaS) in cloud for small businesses. We aim at the scenarios where a large number of small tenants are served but only some of them are active simultaneously. By small tenants, we mean that a tenant may have many small-sized tables while only a small number of those tables are accessed concurrently for each query. As most MTDBaaS providers, we consolidate multiple tenants’ data into the same database management system (DBMS) to reduce the cost of operation. However, our solution distinguishes itself from the existing solutions by a novel mechanism: Load As You quERy (LAYER in short). Concretely, tenants can define and create their own tables with LAYER, and set up possible reference constraints between any two tables. A shared table is used to store all data for all tenants, but only a moderate number of working tables are maintained for answering queries from active tenants. When a new query is submitted, tables involved in the query but not yet in the DBMS will be restored: tables are created, and data are loaded to these newly-created tables. If an active tenant becomes inactive (logs out or no query is issued in a specified time period), tables belonging to the tenant could be dropped when necessary, and updates to these tables would be mirrored to the shared table for backup. We provide two implementations of the LAYER mechanism, one is LAYER-MySQL, which is based on the traditional disk-based relational DBMS MySQL, and can yield high consolidation and acceptable performance; the other is LAYER-VoltDB, which is based on the in-memory relational DBMS VoltDB, and can provide much higher performance. Experimental results validate the feasibility of the proposed mechanism.
101|-||Energy efficiency heterogeneous wireless access selection for multiple types of applications|Mobile terminal (MT) users run various types of applications, such as e-mail, APPs, web browsers, and multimedia, through various types of wireless networks. Extending the battery life of MT, which requires a large amount of electricity for wireless transmission, has become critical. This study focused on the energy efficiency of wireless networks, such as 3G, 4G and Wi-Fi, based on application characteristics and transmission loads. The various applications are classified into idle-bound applications (e.g., e-mail service) and transmission-bound applications (e.g., multimedia) that require diverse types of wireless networks. The operation state of a wireless network includes transmitting, receiving, listening, and sleeping modes. According to the game theory of the energy consumption analysis between the characteristics of applications and wireless networks, three wireless network selection schemes IBLB (idle-bound with load-balancing), TBLB (transmission-bound with load balancing), and WLAT (weighted load and application type) were proposed to reduce the amount of power consumption. Previous studies were compared with the proposed schemes through (1) variation of the number of running applications, (2) various numbers of 3G/4G base stations and Wi-Fi access points, and (3) the combinations of various types of applications to evaluate the energy efficiency of Wi-Fi and 3G/4G access networks selections.
101|-||Algorithms for automated live migration of virtual machines|We present two strategies to balance the load in a system with multiple virtual machines (VMs) through automated live migration. When the push strategy is used, overloaded hosts try to migrate workload to less loaded nodes. On the other hand, when the pull strategy is employed, the light-loaded hosts take the initiative to offload overloaded nodes. The performance of the proposed strategies was evaluated through simulations. We have discovered that the strategies complement each other, in the sense that each strategy comes out as “best” under different types of workload. For example, the pull strategy is able to quickly re-distribute the load of the system when the load is in the range low-to-medium, while the push strategy is faster when the load is medium-to-high. Our evaluation shows that when adding or removing a large number of virtual machines in the system, the “best” strategy can re-balance the system in 4–15 min.
101|-||Neural networks for predicting the duration of new software projects|The duration of software development projects has become a competitive issue: only 39% of them are finished on time relative to the duration planned originally. The techniques for predicting project duration are most often based on expert judgment and mathematical models, such as statistical regression or machine learning. The contribution of this study is to investigate whether or not the duration prediction accuracy obtained with a multilayer feedforward neural network model, also called a multilayer perceptron (MLP), and with a radial basis function neural network (RBFNN) model is statistically better than that obtained by a multiple linear regression (MLR) model when functional size and the maximum size of the team of developers are used as the independent variables. The three models mentioned above are trained and tested by predicting the duration of new software development projects with a set of projects from the International Software Benchmarking Standards Group (ISBSG) release 11. Results based on absolute residuals, Pred(l) and a Friedman statistical test show that prediction accuracy with the MLP and the RBFNN is statistically better than with the MLR model.
101|-||Soft competency requirements in requirements engineering, software design, implementation, and testing|Global software development changes the requirements in terms of soft competency and increases the complexity of social interaction by including intercultural aspects. While soft competency is often seen as crucial for the success of global software development projects, the concrete competence requirements remain unknown. Internationalization competency represents one of the first attempts to structure and describe the soft competence requirements for global software developers. Based on the diversity of tasks, competence requirements will differ among the various phases of software development. By conducting a survey on the importance of internationalization competences for the different phases of global software development, we identified differences in terms of competence importance and requirements in the phases. “Adaptability” (of one's working style) and “Cultural Awareness” were the main differences. “Cultural Awareness” distinguishes requirements engineering and software design from testing and implementation while “Adaptability” distinguishes implementation and software design from requirements engineering and testing.
101|-||Hindering data theft with encrypted data trees|Data theft is a major threat for modern organizations with potentially large economic consequences. Although these attacks may well originate outside an organization’s information systems, the attacker—or else an insider—must eventually make contact with the system where the information resides and extract it. In this work, we propose a scheme that hinders unauthorized data extraction by modifying the basic file system primitives used to access files. Intuitively, our proposal emulates the chains used to protect valuable items in certain clothing shopping centers, where shoplifting is prevented by forcing the thief to steal the whole rack of items. We achieve this by encrypting sensitive files using nonces (i.e., pseudorandom numbers used only once) as keys. Such nonces are available, also in encrypted form, in other objects of the file system. The system globally resembles a distributed Merkle hash tree, in such a way that getting access to a file requires previous access to a number of other files. This forces any potential attacker to extract not only the targeted sensitive information, but also all the files chained to it that are necessary to compute the associated key. Furthermore, our scheme incorporates a probabilistic rekeying mechanism to limit the damage that might be caused by patient extractors. We report experimental results measuring the time overhead introduced by our proposal and compare it with the effort an attacker would need to successfully extract information from the system. Our results show that the scheme increases substantially the effort required by an insider, while the introduced overhead is feasible for standard computing platforms.
101|-||Quality of service approaches in cloud computing: A systematic mapping study|Context: Cloud computing is a new computing technology that provides services to consumers and businesses. Due to the increasing use of these services, the quality of service (QoS) of cloud computing has become an important and essential issue since there are many open challenges which need to be addressed related to trust in cloud services. Many research issues have been proposed in QoS approaches in the cloud computing area.
101|-||An investigation into the best practices for the successful design and implementation of lightweight software process assessment methods: A systematic literature review|Software process assessment (SPA) is an effective tool to understand an organization's process quality and to explore improvement opportunities. However, the knowledge that underlies the best practices required to develop assessment methods, either lightweight or heavyweight methods, is unfortunately scattered throughout the literature. This paper presents the results of a systematic literature review to organize those recognized as the best practices in a way that helps SPA researchers and practitioners in designing and implementing their assessment methods. Such practices are presented in the literature as assessment requirements, success factors, observations, and lessons learned. Consequently, a set of 38 best practices has been collected and classified into five main categories, namely practices related to SPA methods, support tools, procedures, documentation, and users. While this collected set of best practices is important for designing lightweight as well as heavyweight assessment methods, it is of utmost importance in designing lightweight assessment methods, as the design of which depends on individual experience.
101|-||A systematic mapping study on technical debt and its management|Technical debt (TD) is a metaphor reflecting technical compromises that can yield short-term benefit but may hurt the long-term health of a software system.
101|-||Enhancing a model-based engineering approach for distributed manufacturing automation systems with characteristics and design patterns|Recent trends in modern manufacturing, such as the growing need for flexibility and the increasing degree of automation in industrial facilities, require distributed control solutions. Implementations of such control schemas and underlying architectures come along with an exponential increase of the automation system's complexity. Therefore, methods for supporting automation engineers during the development processes are highly required. This paper presents an approach to supporting model-based engineering (MBE) of distributed manufacturing automation systems. The approach is based on the combination of notation, characteristics, and design patterns across multiple levels of an adapted development process. Accordingly, a prototypical support tool has been implemented. The modeling approach has been evaluated by case studies and additional usability experiments to determine the benefit of its application within the design of manufacturing automation systems.
101|-||Improving software reliability prediction through multi-criteria based dynamic model selection and combination|In spite of much research efforts to develop software reliability models, there is no single model which is appropriate in all circumstances. Accordingly, some recent studies on software reliability have attempted to use existing models more effectively in practice (e.g., model selection and combination). However, it is not easy to identify which model is likely to make the most trustworthy predictions and to assign appropriate weights to models for the combination. The improper model selection or weight assignment often causes unsuccessful software reliability prediction in practice, which leads to cost/schedule overrun. In this paper, we propose a systematic reliability prediction framework which dynamically selects and combines multiple software reliability models based on the decision trees learning of multi-criteria. For the model selection, the proposed approach uses the empirical patterns of multi-criteria derived from models. Reduced error pruning decision tree identifies the models with the best predictive patterns and automatically assign a weight to each model. Then, the identified models fall into two groups according to the likelihood of over- or under-prediction, and the competitive models from each group are combined based on their given weights. From the evaluation results, our approach outperformed existing methods on average prediction accuracy.
101|-||Quantifying usability of domain-specific languages: An empirical study on software maintenance|A domain-specific language (DSL) aims to support software development by offering abstractions to a particular domain. It is expected that DSLs improve the maintainability of artifacts otherwise produced with general-purpose languages. However, the maintainability of the DSL artifacts and, hence, their adoption in mainstream development, is largely dependent on the usability of the language itself. Unfortunately, it is often hard to identify their usability strengths and weaknesses early, as there is no guidance on how to objectively reveal them. Usability is a multi-faceted quality characteristic, which is challenging to quantify beforehand by DSL stakeholders. There is even less support on how to quantitatively evaluate the usability of DSLs used in maintenance tasks. In this context, this paper reports a study to compare the usability of textual DSLs under the perspective of software maintenance. A usability measurement framework was developed based on the cognitive dimensions of notations. The framework was evaluated both qualitatively and quantitatively using two DSLs in the context of two evolving object-oriented systems. The results suggested that the proposed metrics were useful: (1) to early identify DSL usability limitations, (2) to reveal specific DSL features favoring maintenance tasks, and (3) to successfully analyze eight critical DSL usability dimensions.
101|-||A solution of dynamic VMs placement problem for energy consumption optimization based on evolutionary game theory|Power saving of data centers has become an urgent problem in recent years. For a virtualized data center, optimizing the placement of virtual machines (VMs) dynamically is one of the most effective methods for power savings. Based on a deep study on VMs placement, a solution is proposed and described in this paper to solve the problem of dynamic placement of VMs toward optimization of their energy consumptions. A computational model of energy consumption is proposed and built. A novel algorithm based on evolutionary game theory is also presented, which successfully addresses the challenges faced by dynamic placement of VMs. It is proved that the proposed algorithm can reach the optimal solutions theoretically. Experimental results also demonstrate that, by adjusting VMs placement dynamically, the energy consumption can be reduced correspondingly. In comparison with the existing state of the arts, our proposed method outperforms other five algorithms tested and achieves savings of 30–40% on energy consumption.
102|-|http://www.sciencedirect.com/science/journal/01641212/102|Special issue on software architectures and systems for Big data|
102|-||Progressive online aggregation in a distributed stream system|Interactive query processing aims at generating approximate results with minimum response time. However, it is quite difficult for a batch-oriented processing system to progressively provide cumulatively accurate results in the context of a distributed environment. MapReduce Online extends the MapReduce framework to support online aggregation, but it is hindered by its processing speed in keeping up with ongoing real-time data events. We deploy the online aggregation algorithm over S4, a scalable stream processing system that is inspired by the combined functionalities of MapReduce and Actor model. Our system applies an asynchronous message communication mechanism from actor model to support online aggregation. It can process large scale data stream with high concurrency in a short response time. In this system, we adopt a distributed weighted random sampling algorithm to solve biased distribution between different streams. Furthermore, a multi-level query processing topology is developed to reduce overlapped processing for multiple queries. Our system can provide continuous window aggregation with a confidence interval and error bound. We have implemented our system and conducted plentiful experiments over the TPC-H benchmark. A large number of experiments are carried out to demonstrate that by using our system, high-quality query results can be generated within a short response time and that the approach outperforms MapReduce Online on data streams.
102|-||Countering the concept-drift problems in big data by an incrementally optimized stream mining model|Mining the potential value hidden behind big data has been a popular research topic around the world. For an infinite big data scenario, the underlying data distribution of newly arrived data may be appeared differently from the old one in the real world. This phenomenon is so-called the concept-drift problem that exists commonly in the scenario of big data mining. In the past decade, decision tree inductions use multi-tree learning to detect the drift using alternative trees as a solution. However, multi-tree algorithms consume more computing resources than the singletree. This paper proposes a singletree with an optimized node-splitting mechanism to detect the drift in a test-then-training tree-building process. In the experiment, we compare the performance of the new method to some state-of-art singletree and multi-tree algorithms. Result shows that the new algorithm performs with good accuracy while a more compact model size and less use of memory than the others.
102|-||HaoLap: A Hadoop based OLAP system for big data|In recent years, facing information explosion, industry and academia have adopted distributed file system and MapReduce programming model to address new challenges the big data has brought. Based on these technologies, this paper presents HaoLap (Hadoop based oLap), an OLAP (OnLine Analytical Processing) system for big data. Drawing on the experience of Multidimensional OLAP (MOLAP), HaoLap adopts the specified multidimensional model to map the dimensions and the measures; the dimension coding and traverse algorithm to achieve the roll up operation on dimension hierarchy; the partition and linearization algorithm to store dimensions and measures; the chunk selection algorithm to optimize OLAP performance; and MapReduce to execute OLAP. The paper illustrates the key techniques of HaoLap including system architecture, dimension definition, dimension coding and traversing, partition, data storage, OLAP and data loading algorithm. We evaluated HaoLap on a real application and compared it with Hive, HadoopDB, HBaseLattice, and Olap4Cloud. The experiment results show that HaoLap boost the efficiency of data loading, and has a great advantage in the OLAP performance of the data set size and query complexity, and meanwhile HaoLap also completely support dimension operations.
102|-||Load-prediction scheduling algorithm for computer simulation of electrocardiogram in hybrid environments|This paper proposes an algorithm that allows fully utilize the Central Processing Unit–Graphics Processing Unit (CPU–GPU) hybrid architecture to conduct parallel computation and reasonable scheduling for computer simulation of electrocardiogram (ECG). This algorithm is realized by accelerating calculation speed and increasing platform adaptability of the parallel algorithm.
102|-||A cloud-based framework for Home-diagnosis service over big medical data|Self-caring services are becoming more and more important for our daily life, especially under the urgent situation of global aging. Big data such as massive historical medical records makes it possible for users to have self-caring services, such as to get diagnosis by themselves with similar patients’ records. Developing such a self-caring service gives rises to challenges including highly concurrent and scalable medical record retrieval, data analysis, as well as privacy protection. In this paper, we propose a cloud-based framework for implementing a self-caring service named Home-diagnosis to address the above challenges. Concretely, a Lucene-based distributed search cluster is designed to support highly concurrent and scalable medical record retrieval, data analysis and privacy protection. Moreover, to speed up medical record retrieval, a Hadoop cluster is adopted for offline data storage and index building. The implementation of the Home-diagnosis service is discussed, where similar historical medical records as well as a disease-symptom lattice are obtained, to help users figure out which kind of disease they are probably infected with. Finally, a prototype system is designed and a running example is presented to demonstrate the scalability and efficiency of our proposal.
102|-||An effective and economical architecture for semantic-based heterogeneous multimedia big data retrieval|Data variety has been one of the most critical features for multimedia big data. Some multimedia documents, although in different data formats and storage structures, often express similar semantic information. Therefore, the way to manage and retrieve multimedia documents reflecting users’ intent in heterogeneous big data environments has become an important issue. In this paper, we present an effective and economical architecture named SHMR (Semantic-based Heterogeneous Multimedia Retrieval), which uses low cost to store and retrieve semantic information from heterogeneous multimedia data. Firstly, the particularity of heterogeneous multimedia retrieval in big data environments is addressed. Secondly, an approach to extract and represent semantic information for heterogeneous multimedia documents is proposed. Thirdly, a NoSQL-based approach to semantic storage, in which multimedia can be parallel processed in distributed nodes is provided. Finally, a MapReduce-based retrieval algorithm is presented and a user feedback supported scheme to achieve high retrieval precision and good user experience is designed. The experimental results indicate that the retrieval performance and economic efficiency of SHMR are suitable for multimedia information retrieval in heterogeneous big data environments.
102|-||Semantic based representing and organizing surveillance big data using video structural description technology|Big data is an emerging paradigm applied to datasets whose size is beyond the ability of commonly used software tools to capture, manage, and process the data within a tolerable elapsed time. Especially, the data volume of all video surveillance devices in Shanghai, China, is up to 1 TB every day. Thus, it is important to accurately describe the video content and enable the organizing and searching potential videos in order to detect and analyze related surveillance events. Unfortunately, raw data and low level features cannot meet the video based task. In this paper, a semantic based model is proposed for representing and organizing video big data. The proposed surveillance video representation method defines a number of concepts and their relations, which allows users to use them to annotate related surveillance events. The defined concepts include person, vehicles, and traffic sighs, which can be used for annotating and representing video traffic events unambiguous. In addition, the spatial and temporal relation between objects in an event is defined, which can be used for annotating and representing the semantic relation between objects in related surveillance events. Moreover, semantic link network is used for organizing video resources based on their associations. In the application, one case study is presented to analyze the surveillance big data.
102|-||Measuring the veracity of web event via uncertainty|Web events, whose data occur as one kind of big data, have attracted considerable interests during the past years. However, most existing related works fail to measure the veracity of web events. In this research, we propose an approach to measure the veracity of web event via its uncertainty based on its features distribution on different kind of confident websites. Firstly, the proposed approach mines various event features from the data of web event which may influence on the measuring process of uncertainty. Secondly, one computational model is introduced to simulate the influence process of the above features on the evolution process of web event. Thirdly, matrix operations are managed to facilitate practice. Finally, experiments are made based on the analysis above, and the results proved that the proposed uncertainty measuring algorithm is promising to measure the veracity of web event for big data.
102|-||The effects of different alphabets on free text keystroke authentication: A case study on the KoreanâEnglish users|Keystroke dynamics is one of the representative behavioral biometrics, and it has been consistently recognized as an alternative to physiological biometrics for user authentication to strengthen the level of security. This paper investigates the effects that languages with different alphabets and different familiarity levels have on the free text keystroke authentication performance using Korean–English data collected from 83 Korean participants. In order to exploit the familiarity level, two typing characteristics are measured and tested. Student’s t-test reveals that the participants have higher typing proficiency but lower typing consistency in the language with the more familiar alphabet, i.e., their primary language (Korean). Typing proficiency is found to be a critical factor when only keystroke latencies are utilized during authentication, whereas typing consistency is found to be a critical factor when key sequence information is utilized in addition to keystroke latencies. The experimental results can be applied to build a customized keystroke dynamics-based authentication system, which adaptively determines the authentication method as well as the keystroke size based on a user’s typing characteristics.
102|-||QoS prediction for dynamic reconfiguration of component based software systems|It is difficult to choose the appropriate reconfiguration approach to satisfy the quality of service (QoS) requirements of a software system if the properties of that approach are not known. This problem significantly restricts the application of dynamic reconfiguration approaches to mission-critical or non-stop systems, where QoS is a major performance indicator. This paper proposes a model to predict how the QoS of a running software system will be affected by dynamic reconfiguration and show how it out-performed the existing methods in this area in three aspects. First, unlike existing simulation based models, this prediction model was based on easily implemented mathematical functions. Second, compared with the time-consuming simulation approaches, QoS prediction using this model was achieved in a shorter timeframe. Third, unlike the existing approaches that are built on different platforms for individual scenarios, this model generalized QoS prediction onto a single virtual platform that was modeled by abstract hardware and software conditions. The proposed model has been verified by reconfiguration simulation to a reasonable level of accuracy and thus the viability and safety for the use of the model has been confirmed.
102|-||Semi-automatic architectural pattern identification and documentation using architectural primitives|In this article, we propose an interactive approach for the semi-automatic identification and documentation of architectural patterns based on a domain-specific language. To address the rich concepts and variations of patterns, we firstly propose to support pattern description through architectural primitives. These are primitive abstractions at the architectural level that can be found in realizations of multiple patterns, and they can be leveraged by software architects for pattern annotation during software architecture documentation or reconstruction. Secondly, using these annotations, our approach automatically suggests possible pattern instances based on a reusable catalog of patterns and their variants. Once a pattern instance has been documented, the annotated component models and the source code get automatically checked for consistency and traceability links are automatically generated. To study the practical applicability and performance of our approach, we have conducted three case studies for existing, non-trivial open source systems.
102|-||Power-aware scheduling of compositional real-time frameworks|The energy consumption problem has become a great challenge in all computing areas from modern handheld devices to large data centers. Dynamic voltage scaling (DVS) is widely used as mean to reduce the energy consumption of computer systems by lowering whenever possible the voltage and operating frequency of processors. Unfortunately, existing compositional real-time scheduling frameworks have been focusing only on efficient scheduling of tasks inside their components given a resource model, providing no interest on power/energy consumption. In this paper, we define the real-time DVS problem for a compositional scheduling framework. Considering the periodic resource model, we propose optimal static DVS schemes at system, component, and task levels. We also introduce component and task level dynamic DVS schemes that take advantage of runtime unused slack times and resource availability to provide even better energy savings. Finally, we provide power-aware schedulability conditions to guarantee the feasibility of each component under DVS for the Earliest Deadline First and the Rate Monotonic scheduling algorithms. Through simulations, we showed that our schemes can reduce the energy consumption of a component by up to 96%.
102|-||An insight into license tools for open source software systems|Free/Libre/Open Source Software (FLOSS) has gained a lot of attention lately allowing organizations to incorporate third party source code into their implementations. When open source software libraries are used, software resources may be linked directly or indirectly with multiple open source licenses giving rise to potential license incompatibilities. Adequate support in license use is vital in order to avoid such violations and address how diverse licenses should be handled. In the current work we investigate software licensing giving a critical and comparative overview of existing assistive approaches and tools. These approaches are centered on three main categories: license information identification from source code and binaries, software metadata stored in code repositories, and license modeling and associated reasoning actions. We also give a formalization of the license compatibility problem and demonstrate the role of existing approaches in license use decisions.
102|-||Progressive Outcomes: A framework for maturing in agile software development|Maturity models are used to guide improvements in the software engineering field and a number of maturity models for agile methods have been proposed in the last years. These models differ in their underlying structure prescribing different possible paths to maturity in agile software development, neglecting the fact that agile teams struggle to follow prescribed processes and practices. Our objective, therefore, was to empirically investigate how agile teams evolve to maturity, as a means to conceive a theory for agile software development evolvement that considers agile teams nature. The complex adaptive systems theory was used as a lens for analysis and four case studies were conducted to collect qualitative and quantitative data. As a result, we propose the Progressive Outcomes framework to describe the agile software development maturing process. It is a framework in which people have the central role, ambidexterity is a key ability to maturity, and improvement is guided by outcomes agile teams pursue, instead of prescribed practices.
102|-||A time-based approach to automatic bug report assignment|Bug assignment is one of the important activities in bug triaging that aims to assign bugs to the appropriate developers for fixing. Many recommended automatic bug assignment approaches are based on text analysis methods such as machine learning and information retrieval methods. Most of these approaches use term-weighting techniques, such as term frequency-inverse document frequency (tf-idf), to determine the value of terms. However, the existing term-weighting techniques only deal with frequency of terms without considering the metadata associated with the terms that exist in software repositories. This paper aims to improve automatic bug assignment by using time-metadata in tf-idf (Time-tf-idf). In the Time-tf-idf technique, the recency of using the term by the developer is considered in determining the values of the developer expertise. An evaluation of the recommended automatic bug assignment approach that uses Time-tf-idf, called ABA-Time-tf-idf, was conducted on three open-source projects. The evaluation shows accuracy and mean reciprocal rank (MRR) improvements of up to 11.8% and 8.94%, respectively, in comparison to the use of tf-idf. Moreover, the ABA-Time-tf-idf approach outperforms the accuracy and MRR of commonly used approaches in automatic bug assignment by up to 45.52% and 55.54%, respectively. Consequently, consideration of time-metadata in term weighting reasonably leads to improvements in automatic bug assignment.
102|-||Stochastic thermal-aware real-time task scheduling with considerations of soft errors|With the continued scaling of the CMOS devices, the exponential increase in power density has strikingly elevated the temperature of on-chip systems. Dynamic voltage/frequency scaling is a widely utilized system level power management technique to reduce the energy consumption and lower the on-chip temperature. However, scaling the voltage or frequency for thermal management leads to an increase in soft error rates, thus has adverse impact on system reliability. In this paper, the authors propose a stochastic thermal-aware task scheduling algorithm that considers soft errors in real-time embedded systems. For the given customer-defined soft error related target reliability and the maximum peak temperature, the proposed scheduling algorithm generates an energy-efficient task schedule by selecting the energy efficient operating frequency for each task and alternating the execution of hot tasks and cool tasks at the scaled operating frequency. The proposed stochastic scheduling algorithm features the consideration of uncertainty in transient fault occurrences. To handle the uncertainty, a fault adaptation variable Î± is introduced to adapt task execution to the stochastic property of fault occurrences. An energy efficiency factor Î´ is also introduced to facilitate the enhancement of energy efficiency by maximizing the energy saved per unit slack. Extensive simulations of synthetic real-time tasks and real-life benchmarking tasks were performed to validate the effectiveness of the proposed algorithm. Experimental results show that the proposed algorithm consumes up to 17.8% less energy as compared to the benchmarking schemes, and the peak temperature of the proposed algorithm is always below the maximum temperature limit and can be up to 9.6 °C lower than that of the benchmarking schemes.
102|-||An effective approach to estimating the parameters of software reliability growth models using a real-valued genetic algorithm|In this paper, we propose an effective approach to estimate the parameters of software reliability growth model (SRGM) using a real-valued genetic algorithm (RGA). The existing SRGMs require the estimation of the parameters such as the total number of failures or the failure detection rate using numerical methods, maximum likelihood estimation or least square estimation. However, these methods impose certain constraints on the parameter estimation of SRGM like requiring the continuity and existence of derivatives in the modelling function. RGA is free from the constraints on the parameter estimation of SRGM. Moreover, it is more adapted in optimization of continuous domain such as parameter estimation of SRGM than a binary genetic algorithm. Two real-valued genetic operators, heuristic crossover and non-uniform mutation, are applied to improve the accuracy and performance of the parameter estimation of SRGM. We conducted experiments on eight real world datasets for comparing the proposed approach with the numerical methods and other existing genetic algorithms. The results indicate that the RGA is more effective in the parameter estimation of SRGM than other GA approaches. We believe that RGA can be a promising solution to effectively managing software quality through the accurate reliability estimates.
103|-|http://www.sciencedirect.com/science/journal/01641212/103|Search Based Software Engineering (SBSE)|
103|-||Software requirements selection and prioritization using SBSE approaches: A systematic review and mapping of the literature|The selection and prioritization of software requirements represents an area of interest in Search-Based Software Engineering (SBSE) and its main focus is finding and selecting a set of requirements that may be part of a software release. This paper presents a systematic review and mapping that investigated, analyzed, categorized and classified the SBSE approaches that have been proposed to address software requirement selection and prioritization problems, reporting quantitative and qualitative assessment. Initially 39 papers returned from our search strategy in this area and they were analyzed by 18 previously established quality criteria. The results of this systematic review show which aspects of the requirements selection and prioritization problems were addressed by researchers, which approaches and search techniques are currently adopted to address these problems, as well as the strengths and weaknesses in this research area highlighted from the quality criteria.
103|-||A robust optimization approach to the next release problem in the presence of uncertainties|The next release problem is a significant task in the iterative and incremental software development model, involving the selection of a set of requirements to be included in the next software release. Given the dynamic environment in which modern software development occurs, the uncertainties related to the input variables of this problem should be taken into account. In this context, this paper presents a formulation to the next release problem considering the robust optimization framework, which enables the production of robust solutions. In order to measure the “price of robustness”, which is the loss in solution quality due to robustness, a large empirical evaluation was executed over synthetical and real-world instances. Several next release planning situations were considered, including different number of requirements, estimating skills and interdependencies between requirements. All empirical results are consistent to show that the penalization with regard to solution quality is relatively small. In addition, the proposed model's behavior is statistically the same for all considered instances, which qualifies it to be applied even in large-scale real-world software projects.
103|-||The optimisation of stochastic grammars to enable cost-effective probabilistic structural testing|The effectiveness of statistical testing, a probabilistic structural testing strategy, depends on the characteristics of the probability distribution from which test inputs are sampled. Metaheuristic search has been shown to be a practical method of optimising the characteristics of such distributions. However, the applicability of the existing search-based algorithm is limited by the requirement that the software’s inputs must be a fixed number of ordinal values. In this paper we propose a new algorithm that relaxes this limitation and so permits the derivation of probability distributions for a much wider range of software. The representation used by the new algorithm is based on a stochastic grammar supplemented with two novel features: conditional production weights and the dynamic partitioning of ordinal ranges. We demonstrate empirically that a search algorithm using this representation can optimise probability distributions over complex input domains and thereby enable cost-effective statistical testing, and that the use of both conditional production weights and dynamic partitioning can be beneficial to the search process.
103|-||A Memetic Algorithm for whole test suite generation|The generation of unit-level test cases for structural code coverage is a task well-suited to Genetic Algorithms. Method call sequences must be created that construct objects, put them into the right state and then execute uncovered code. However, the generation of primitive values, such as integers and doubles, characters that appear in strings, and arrays of primitive values, are not so straightforward. Often, small local changes are required to drive the value toward the one needed to execute some target structure. However, global searches like Genetic Algorithms tend to make larger changes that are not concentrated on any particular aspect of a test case. In this paper, we extend the Genetic Algorithm behind the EvoSuite test generation tool into a Memetic Algorithm, by equipping it with several local search operators. These operators are designed to efficiently optimize primitive values and other aspects of a test suite that allow the search for test cases to function more effectively. We evaluate our operators using a rigorous experimental methodology on over 12,000 Java classes, comprising open source classes of various different kinds, including numerical applications and text processors. Our study shows that increases in branch coverage of up to 53% are possible for an individual class in practice.
103|-||Subdomain-based test data generation|Considerable effort is required to test software thoroughly. Even with automated test data generation tools, it is still necessary to evaluate the output of each test case and identify unexpected results. Manual effort can be reduced by restricting the range of inputs testers need to consider to regions that are more likely to reveal faults, thus reducing the number of test cases overall, and therefore reducing the effort needed to create oracles. This article describes and evaluates search-based techniques, using evolution strategies and subset selection, for identifying regions of the input domain (known as subdomains) such that test cases sampled at random from within these regions can be used efficiently to find faults. The fault finding capability of each subdomain is evaluated using mutation analysis, a technique that is based on faults programmers are likely to make. The resulting subdomains kill more mutants than random testing (up to six times as many in one case) with the same number or fewer test cases. Optimised subdomains can be used as a starting point for program analysis and regression testing. They can easily be comprehended by a human test engineer, so may be used to provide information about the software under test and design further highly efficient test suites.
103|-||Test data generation with a Kalman filter-based adaptive genetic algorithm|Software testing is a crucial part of software development. It enables quality assurance, such as correctness, completeness and high reliability of the software systems. Current state-of-the-art software testing techniques employ search-based optimisation methods, such as genetic algorithms to handle the difficult and laborious task of test data generation. Despite their general applicability, genetic algorithms have to be parameterised in order to produce results of high quality. Different parameter values may be optimal for different problems and even different problem instances. In this work, we introduce a new approach for generating test data, based on adaptive optimisation. The adaptive optimisation framework uses feedback from the optimisation process to adjust parameter values of a genetic algorithm during the search. Our approach is compared to a state of the art test data optimisation algorithm that does not adapt parameter values online, and a representative adaptive optimisation algorithm, outperforming both methods in a wide range of problems.
103|-||An assessment of search-based techniques for reverse engineering feature models|Successful software evolves from a single system by adding and changing functionality to keep up with users’ demands and to cater to their similar and different requirements. Nowadays it is a common practice to offer a system in many variants such as community, professional, or academic editions. Each variant provides different functionality described in terms of features. Software Product Line Engineering (SPLE) is an effective software development paradigm for this scenario. At the core of SPLE is variability modelling whose goal is to represent the combinations of features that distinguish the system variants using feature models, the de facto standard for such task. As SPLE practices are becoming more pervasive, reverse engineering feature models from the feature descriptions of each individual variant has become an active research subject. In this paper we evaluated, for this reverse engineering task, three standard search based techniques (evolutionary algorithms, hill climbing, and random search) with two objective functions on 74 SPLs. We compared their performance using precision and recall, and found a clear trade-off between these two metrics which we further reified into a third objective function based on FÎ², an information retrieval measure, that showed a clear performance improvement. We believe that this work sheds light on the great potential of search-based techniques for SPLE tasks.
103|-||Cost-effective test suite minimization in product lines using search techniques|Cost-effective testing of a product in a product line requires obtaining a set of relevant test cases from the entire test suite via test selection and minimization techniques. In this paper, we particularly focus on test minimization for product lines, which identifies and eliminates redundant test cases from test suites in order to reduce the total number of test cases to execute, thereby improving the efficiency of testing. However, such minimization may result in the minimized test suite with low test coverage, low fault revealing capability, low priority test cases, and require more time than the allowed testing budget (e.g., time) as compared to the original test suite. To deal with the above issues, we formulated the minimization problem as a search problem and defined a fitness function considering various optimization objectives based on the above issues. To assess the performance of our fitness function, we conducted an extensive empirical evaluation by investigating the fitness function with three weight-based Genetic Algorithms (GAs) and seven multi-objective search algorithms using an industrial case study and 500 artificial problems inspired from the industrial case study. The results show that Random-Weighted Genetic Algorithm (RWGA) significantly outperforms the other algorithms since RWGA can balance all the objectives together by dynamically updating weights during each generation. Based on the results of our empirical evaluation, we also implemented a tool called TEst Minimization using Search Algorithms (TEMSA) to support test minimization using various search algorithms in the context of product lines.
103|-||Applying multiobjective evolutionary algorithms to dynamic software product lines for reconfiguring mobile applications|Mobile applications require dynamic reconfiguration services (DRS) to self-adapt their behavior to the context changes (e.g., scarcity of resources). Dynamic Software Product Lines (DSPL) are a well-accepted approach to manage runtime variability, by means of late binding the variation points at runtime. During the system’s execution, the DRS deploys different configurations to satisfy the changing requirements according to a multiobjective criterion (e.g., insufficient battery level, requested quality of service). Search-based software engineering and, in particular, multiobjective evolutionary algorithms (MOEAs), can generate valid configurations of a DSPL at runtime. Several approaches use MOEAs to generate optimum configurations of a Software Product Line, but none of them consider DSPLs for mobile devices. In this paper, we explore the use of MOEAs to generate at runtime optimum configurations of the DSPL according to different criteria. The optimization problem is formalized in terms of a Feature Model (FM), a variability model. We evaluate six existing MOEAs by applying them to 12 different FMs, optimizing three different objectives (usability, battery consumption and memory footprint). The results are discussed according to the particular requirements of a DRS for mobile applications, showing that PAES and NSGA-II are the most suitable algorithms for mobile environments.
103|-||Investigating the effect of âdefect co-fixâ on quality assurance resource allocation: A search-based approach|Allocation of resources to pre-release quality assurance (QA) tasks, such as source code analysis, peer review, and testing, is one of the challenges faced by a software project manager. The goal is to find as many defects as possible with the available QA resources prior to the release. This can be achieved by assigning more resources to the more defect-prone artifacts, e.g., components, classes, and methods. The state-of-the-art QA resource allocation approaches predict the defect-proneness of an artifact using the historical data of different software metrics, e.g., the number of previous defects and the changes in the artifact. Given a QA budget, an allocation technique selects the most defect-prone artifacts, for further investigation by the QA team. While there has been many research efforts on discovering more predictive software metrics and more effective defect prediction algorithms, the cost-effectiveness of the QA resource allocation approaches has always been evaluated by counting the number of defects per selected artifact. The problem with such an evaluation approach is that it ignores the fact that, in practice, fixing a software issue is not bounded to an artifact under investigation. In other words, one may start reviewing a file that is identified as defect-prone and detect a defect, but to fix the defect one may modify not only the defective part of the file under review, but also several other artifacts that are somehow related to the defective code (e.g., a method that calls the defective code). Such co-fixes (fixing several defects together) during analyzing/reviewing/testing of an artifact under investigation will change the number of remaining defects in the other artifacts. Therefore, a QA resource allocation approach is more effective if it prioritizes the artifacts that would lead to the smallest number of remaining defects. Investigating six medium-to-large releases of open source systems (Mylyn, Eclipse, and NetBeans, two releases each), we found that co-fixes happen quite often in software projects (30–42% of the fixes modify more than one artifact). Therefore, in this paper, we first introduce a new cost-effectiveness measure to evaluate QA resource allocation, based on the concept of “remaining defects” per file. We then propose several co-fix-aware prioritization approaches to dynamically optimize the new measure, based on the historical defect co-fixes. The evaluation of these approaches on the six releases shows that (a) co-fix-aware QA prioritization approaches improve the traditional defect prediction-based ones, in terms of density of remaining defects per file and (b) co-fix-aware QA prioritization can potentially benefit from search-based software engineering techniques.
103|-||MOMM: Multi-objective model merging|Nowadays, software systems are complex and large. To cope with this situation, teams of developers have to cooperate and work in parallel on software models. Thus, techniques to support the collaborative development of models are a must. To this end, several approaches exist to identify the change operations applied in parallel, to detect conflicts among them, as well as to construct a merged model by incorporating all non-conflicting operations. Conflicts often denote situations where the application of one operation disables the applicability of another one. Consequently, one operation has to be omitted to construct a valid merged model in such scenarios. When having to decide which operation to omit, the importance of its application has to be taken into account depending on the operation type and the application context. However, existing works treat the operations to merge with equal importance. We introduce in this paper, for the first time, a multi-objective formulation of the problem of model merging, based on NSGA-II, that aims to find the best trade-off between minimizing the number of omitted operations and maximizing the number of successfully applied important operations. We evaluated our approach using seven open source systems and compared it with different existing model merging approaches. The merging solutions obtained with our approach were found in all of the scenarios of our experiments to be comparable in terms of minimizing the number of conflicts to those suggested by existing approaches and to carry a high importance score of merged operations. Our results also revealed an interesting feature concerning the trade-off between the two conflicting objectives that demonstrates the practical value of taking the importance of operations into account in model merging tasks. In fact, the shape of the Pareto front represents an interesting guidance for developers to select best solutions based on their preferences.
103|-||The influence of search components and problem characteristics in early life cycle class modelling|This paper examines the factors affecting the quality of solution found by meta-heuristic search when optimising object-oriented software class models. From the algorithmic perspective, we examine the effect of encoding, choice of components such as the global search heuristic, and various means of incorporating problem- and instance-specific information. We also consider the effect of problem characteristics on the (estimated) cost of the global optimum, and the quality and distribution of local optima. The choice of global search component appears important, and adding problem and instance-specific information is generally beneficial to an evolutionary algorithm but detrimental to ant colony optimisation. The effect of problem characteristics is more complex. Neither scale nor complexity have a significant effect on the global optimum as estimated by the best solution ever found. However, using local search to locate 100,000 local optima for each problem confirms the results from meta-heuristic search: there are patterns in the distribution of local optima that increase with scale (problem size) and complexity (number of classes) and will cause problems for many classes of meta-heuristic search.
103|-||Collaboration optimization in software process composition|Purpose: The purpose of this paper is to describe an optimization approach to maximize collaboration in software process composition. The research question is: how to compose a process for a specific software development project context aiming to maximize collaboration among team members? The optimization approach uses heuristic search algorithms to navigate the solution space and look for acceptable solutions.
103|-||A comprehensive approach to the recovery of design pattern instances based on sub-patterns and method signatures|Design patterns are formalized best practices that address concerns related to high-level structures for applications being developed. The efficient recovery of design pattern instances significantly facilitates program comprehension and software reengineering. However, the recovery of design pattern instances is not a straightforward task. In this paper, we present a novel comprehensive approach to the recovery of instances of 23 GoF design patterns from source codes. The key point of the approach lies in that we consider different design pattern instances consist of some commonly recurring sub-patterns that are easier to be detected. In addition, we focus not only on the class relationship, but also on the characteristics of underlying method signatures in classes. We first transform the source codes and predefined GoF patterns into graphs, with the classes as nodes and the relationships as edges. We then identify the instances of sub-patterns that would be the possible constituents of pattern instances by means of subgraph discovery. The sub-pattern instances are further merged by the joint classes to see if the collective matches one of the predefined patterns. Finally, we compare the behavioral characteristics of method invocation with the predefined method signature templates of GoF patterns to obtain the final pattern instances directly. Compared with existing approaches, we integrate and improve some of the previous ideas and put forward a comprehensive and elaborative approach also based on our own ideas. We detect sub-patterns via graph isomorphism based on prime number composition and the joint classes to reduce the search space. Meanwhile, we employ the method signatures to investigate the behavioral features to avoid choosing the test cases with full code coverage. The results of the extensive experiments on recovering pattern instances from nine open source software systems demonstrate that our approach obtains the balanced high precision and recall.
103|-||Diagrams or structural lists in software project retrospectives â An experimental comparison|Root cause analysis (RCA) is a recommended practice in retrospectives and cause–effect diagram (CED) is a commonly recommended technique for RCA. Our objective is to evaluate whether CED improves the outcome and perceived utility of RCA. We conducted a controlled experiment with 11 student software project teams by using a single factor paired design resulting in a total of 22 experimental units. Two visualization techniques of underlying causes were compared: CED and a structural list of causes. We used the output of RCA, questionnaires, and group interviews to compare the two techniques. In our results, CED increased the total number of detected causes. CED also increased the links between causes, thus, suggesting more structured analysis of problems. Furthermore, the participants perceived that CED improved organizing and outlining the detected causes. The implication of our results is that using CED in the RCA of retrospectives is recommended, yet, not mandatory as the groups also performed well with the structural list. In addition to increased number of detected causes, CED is visually more attractive and preferred by retrospective participants, even though it is somewhat harder to read and requires specific software tools.
103|-||An empirical evaluation of ensemble adjustment methods for analogy-based effort estimation|Effort adjustment is an essential part of analogy-based effort estimation, used to tune and adapt nearest analogies in order to produce more accurate estimations. Currently, there are plenty of adjustment methods proposed in literature, but there is no consensus on which method produces more accurate estimates and under which settings.
103|-||A composite-metric based path selection technique for the Tor anonymity network|The Tor anonymous network has become quite popular with regular users on the Internet. In the Tor network, an anonymous path is created by selecting three relays through which the connection is redirected. Nevertheless, as the number of Tor users has increased substantially in recent years, the algorithm with which the relays are selected affects the performance provided by the Tor network. More importantly as the performance suffers, users will leave the network, resulting in a lower anonymity set and in turn lower security provided by Tor network. In this paper, we proposed an algorithm for improving performance and security of the Tor network, by employing a combination of different metrics in the process of the path selection between the source and destination node. These metrics are bandwidth and uptime of relays as node conditions and delays between the relays as a path condition. Through a number of experiments we show that we could double the performance observed by end users when using the proposed technique as opposed to the current Tor path selection algorithm. More importantly, the proposed technique only requires a software upgrade on the client side, and other Tor nodes do not need to be modified.
103|-||Enabling high-level application development for the Internet of Things|Application development in the Internet of Things (IoT) is challenging because it involves dealing with a wide range of related issues such as lack of separation of concerns, and lack of high-level of abstractions to address both the large scale and heterogeneity. Moreover, stakeholders involved in the application development have to address issues that can be attributed to different life-cycles phases. when developing applications. First, the application logic has to be analyzed and then separated into a set of distributed tasks for an underlying network. Then, the tasks have to be implemented for the specific hardware. Apart from handling these issues, they have to deal with other aspects of life-cycle such as changes in application requirements and deployed devices.
103|-||Classifying metrics for assessing Object-Oriented Software Maintainability: A family of metricsâ catalogs|Object-Oriented Programming is one of the most used paradigms. Complementarily, the software maintainability is considered a software attribute playing an important role in quality level. In this context, Object-Oriented Software Maintainability (OOSM) has been studied through years, and many researchers have proposed a large number of metrics to measure it. Consequently, the decision-making process about which metrics can be adopted in experiments on OOSM is a hard task. Therefore, a metrics’ categorization has been proposed to facilitate this process. As result, 7 categories and 17 subcategories were identified. These categories represent the scenarios of OOSM metrics adoption, and a family of OOSM metrics catalog was generated based on the selection of a metrics’ categorization. Additionally, a quasi-experiment was conducted to check the coverage index of the catalogs generated using our approach over the catalogs suggested by experts. 90% of coverage was obtained with 99% of confidential level using the Wilcoxon Test. Complementarily, a survey was conducted to check the experts’ opinion about the catalog generated by the portal when they were compared by the catalogs suggested by them. Therefore, this evaluation can be the first evidences of the usefulness of the family of the catalogs based on the metrics’ categorization.
103|-||On applying machine learning techniques for design pattern detection|The detection of design patterns is a useful activity giving support to the comprehension and maintenance of software systems. Many approaches and tools have been proposed in the literature providing different results. In this paper, we extend a previous work regarding the application of machine learning techniques for design pattern detection, by adding a more extensive experimentation and enhancements in the analysis method. Here we exploit a combination of graph matching and machine learning techniques, implemented in a tool we developed, called MARPLE-DPD. Our approach allows the application of machine learning techniques, leveraging a modeling of design patterns that is able to represent pattern instances composed of a variable number of classes. We describe the experimentations for the detection of five design patterns on 10 open source software systems, compare the performances obtained by different learning models with respect to a baseline, and discuss the encountered issues.
103|-||On-demand data broadcast with deadlines for avoiding conflicts in wireless networks|On-demand data broadcast (ODDB) has attracted increasing interest due to its efficiency of disseminating information in many real-world applications such as mobile social services, mobile payment and mobile e-commerce. In an ODDB system, the server places client requested data items received from the uplink to a set of downlink channels for downloading by the clients. Most existing work focused on how to allocate client requested data items to multiple channels for efficient downloading, but did not consider the time constraint of downloading which is critical for many real-world applications. For a set of requests with deadlines for downloading, this paper proposes an effective algorithm to broadcast data items of each request within its specified deadline using multiple channels under the well-known 2-conflict constraint: two data items conflict if they are broadcast in the same time slot or two adjacent time slots in different channels. Our algorithm adopts an approach of allocating most urgent and popular data item first (UPF) for minimizing the overall deadline miss ratio. The performance of the UPF method has been validated by extensive experiments on real-world data sets against three popular on-demand data broadcast schemes.
103|-||Automatic enforcement of constraints in real-time collaborative architectural decision making|Making and documenting architectural design decisions becomes increasingly important in the process of software architecting. However, the remoteness of different decision stakeholders, ranging from local distribution in an office environment to globally distributed teams, as well as the different domain knowledge, expertise and responsibilities of the stakeholders hinder effective and efficient collaboration. Existing tools and methods for collaborative architectural decision making focus mainly on sharing and reusing of knowledge, making trade-offs, and achieving consensus, but do not consider the various stakeholders’ decision making constraints due to their roles in the development process. To address this problem, we propose a meta-model for a set of decision making constraints, with precisely defined semantics, as well as a collaborative architectural decision making approach based on this meta-model. We also present tool support, called CoCoADvISE, which automatically enforces the constraints at runtime. The evaluation of this tool in a controlled experiment with 48 participants shows that our approach, besides preventing constraint violations, significantly increases both the time and effort related efficiency, as well as the effectiveness of users in collaborative decision making.
103|-||Approaches to promote product quality within software process improvement initiatives: A mapping study|Enhancing product quality might be a main goal of a software process improvement initiative (SPI). Quality is, however, a complex concept, and experts recommend identifying relevant product quality characteristics to satisfy users/customers’ needs. There is thus a need to understand how SPI initiatives contribute to the improvement of software product quality characteristics. This paper aims to provide an overview of an up-to-date state-of-the-art regarding initiatives that focus on promoting product quality improvement by applying SPI approaches. This goal was achieved by conducting a systematic mapping study, as a result of which we identified 74 primary papers including both theoretical (75.7%) and empirical (24.3%) papers. The main product quality characteristics addressed are security, usability and reliability. Security-related process models, on the other hand, are those most cited (53%). The empirical papers suggest that traditional process reference models, such as CMM, CMMI or ISO 9001, moderately increase product quality characteristics, these principally being maintainability and reliability. However, there is a need for more empirical research to evaluate the impact of SPI initiatives on software product quality by considering contextual factors. SPI initiatives should be more driven by performance goals related to product quality characteristics.
103|-||A survey study on major technical barriers affecting the decision to adopt cloud services|In the context of cloud computing, risks associated with underlying technologies, risks involving service models and outsourcing, and enterprise readiness have been recognized as potential barriers for the adoption. To accelerate cloud adoption, the concrete barriers negatively influencing the adoption decision need to be identified. Our study aims at understanding the impact of technical and security-related barriers on the organizational decision to adopt the cloud. We analyzed data collected through a web survey of 352 individuals working for enterprises consisting of decision makers as well as employees from other levels within an organization. The comparison of adopter and non-adopter sample reveals three potential adoption inhibitor, security, data privacy, and portability. The result from our logistic regression analysis confirms the criticality of the security concern, which results in an up to 26-fold increase in the non-adoption likelihood. Our study underlines the importance of the technical and security perspectives for research investigating the adoption of technology.
103|-||Practical and representative faultloads for large-scale software systems|The faultload is one of the most critical elements of experimental dependability evaluation. It should embody a repeatable, portable, representative and generally accepted fault set. Concerning software faults, the definition of that kind of faultloads is particularly difficult, as it requires a much more complex emulation method than the traditional stuck-at or bit-flip used for hardware faults. Although faultloads based on software faults have already been proposed, the choice of adequate fault injection targets (i.e., actual software components where the faults are injected) is still an open and crucial issue. Furthermore, knowing that the number of possible software faults that can be injected in a given system is potentially very large, the problem of defining a faultload made of a small number of representative faults is of utmost importance. This paper presents a comprehensive fault injection study and proposes a strategy to guide the fault injection target selection to reduce the number of faults required for the faultload and exemplifies the proposed approach with a real web-server dependability benchmark and a large-scale integer vector sort application.
103|-||Automatic deployment of distributed software systems: Definitions and state of the art|Deployment of software systems is a complex post-production process that consists in making software available for use and then keeping it operational. It must deal with constraints concerning both the system and the target machine(s), in particular their distribution, heterogeneity and dynamics, and satisfy requirements from different stakeholders. In the context of mobility and openness, deployment must react to the instability of the network of machines (failures, connections, disconnections, variations in the quality of the resources, etc.). Thus, deployment should be an uninterrupted process which also works when software is running and requires adaptiveness in order to continually satisfy the constraints and the requirements. Originally managed “by hand”, software deployment demands an increasing level of automation and autonomy.
103|-||Early effort estimation in web application development|Project planning in software industry represents one of the most complex tasks, especially when there is a need to estimate the time, cost and effort needed for development of software projects. In the field of development effort estimation for classical software projects a number of methods have been developed, tested and successfully implemented. Web projects are, by their nature, different than classical software projects, and there is a lack of methods and models that provides a high degree of confidence in development effort estimation. This paper analyzes the possibility of using a combination of functional size and conceptual models for the purpose of web application development effort estimation. Measurement of functional size can be effectively applied to the conceptual models of the data-driven web applications because of the existence of extensive count of data movements. For the purpose of this study 19 web applications with their conceptual models were employed. An effort model was built using simple linear regression analysis. Upon construction, evaluation and validation of the effort model prediction accuracy elements, R2, MMRE, and Pred(l), showed promising results for web projects used in the model construction and validation process.
103|-||Designing an open source maintenance-free Environmental Monitoring Application for Wireless Sensor Networks|We discuss the entire process for the analysis and design of an Environmental Monitoring Application for Wireless Sensor Networks, using existing open source components to create the application. We provide a thorough study of the different alternatives, from the selection of the embedded operating system to the different algorithms and strategies. The application has been designed to gather temperature and relative humidity data following the rules of quality assurance for environmental measurements, suitable for use in both research and industry. The main features of the application are: (a) runs in a multihop low-cost network based on IEEE 802.15.4, (b) improved network reliability and lifetimes, (c) easy management and maintenance-free, (d) ported to different platforms and (e) allows different configurations and network topologies. The application has been tested and validated in several long-term outdoor deployments with very good results and the conclusions are aligned with the experimental evidence.
103|-||A comprehensive study of the predictive accuracy of dynamic change-impact analysis|The correctness of software is affected by its constant changes. For that reason, developers use change-impact analysis to identify early the potential consequences of changing their software. Dynamic impact analysis is a practical technique that identifies potential impacts of changes for representative executions. However, it is unknown how reliable its results are because their accuracy has not been studied. This paper presents the first comprehensive study of the predictive accuracy of dynamic impact analysis in two complementary ways. First, we use massive numbers of random changes across numerous Java applications to cover all possible change locations. Then, we study more than 100 changes from software repositories, which are representative of developer practices. Our experimental approach uses sensitivity analysis and execution differencing to systematically measure the precision and recall of dynamic impact analysis with respect to the actual impacts observed for these changes. Our results for both types of changes show that the most cost-effective dynamic impact analysis known is surprisingly inaccurate with an average precision of 38–50% and average recall of 50–56% in most cases. This comprehensive study offers insights on the effectiveness of existing dynamic impact analyses and motivates the future development of more accurate impact analyses.
104|-|http://www.sciencedirect.com/science/journal/01641212/104|A process to identify relevant substitutes for healing failed WS-* orchestrations|Orchestrating web services aims to compose multiple services into workflows that answer complex user requirements. Web services are software components which are exposed to errors and failures that can occur during web service orchestration execution. Thus, many error-handling and healing approaches have been proposed to guarantee reliable orchestrations. Some of these approaches rely on the identification of relevant service substitutes to heal (by substitution) the defected services. In this paper, we propose an identification process of web service substitutes for healing failed web service orchestrations based on the measurement of similarity between service interfaces. The process reveals both simple and complex (compositions of) substitutes. We validated the approach via a set of experiments conducted on a collection of real web services.
104|-||Agent-based Cloud bag-of-tasks execution|Bag-of-tasks (BoTs) applications are highly parallel, unconnected and unordered tasks. Since BoT executions often require costly investments in computing infrastructures, Clouds offer an economical solution to BoT executions. Cloud BoT executions involve (1) allocating and deallocating heterogeneous resources with possibly different price rates from multiple Cloud providers, (2) distributing BoT execution across multiple, distributed resources, and (3) coordinating self-interested Cloud participants. This paper proposes a novel agent-based Cloud BoT execution tool (CloudAgent) supported by a 4-stage agent-based protocol capable of dynamically coordinating autonomous Cloud participants to concurrently execute BoTs in multiple Clouds in a parallel manner. CloudAgent is endowed with an autonomous agent-based resource provisioning system supported by the contract net protocol to dynamically allocate resources based on hourly cost rates from multiple Cloud providers. In addition, CloudAgent is also equipped with an agent-based resource deallocation system that autonomously and dynamically deallocates resources assigned to BoT executions. Empirical results show that CloudAgent can efficiently handle concurrent BoT executions, bear low BoT execution costs, and effectively scale.
104|-||When did your project start? â The software supplier's perspective|A software development project may be considered a failure because it is late. In order to be able to assess this, a project start date should be known. The purpose of the paper is to study software development projects in a business context and especially project start from the supplier's perspective. In our research, we observed different ways of assigning project start but did not observe any company-level instructions for defining it. We raise questions regarding knowledge loss, project profitability, and having a project running late even before the project has started. We provide definitions for project start and project start date, and define boundaries for software supplier's project start-up. With this paper, we emphasise the need to study software development projects in a business context. The paper contributes to research on project management success, the software project business, and the management of a software project in a business context.
104|-||Software rejuvenation via a multi-agent approach|Usually, development teams devote a huge amount of time and effort on maintaining existing software. Since many of these maintenance tasks are not planned, the software tends to degrade over time, causing side effects mainly on its non-functional requirements. This paper proposes the use of a multi-agent system in order to perform perfective maintenance tasks in a software product through refactorings. The software developer chooses the quality attribute that the agents should improve and the agents are able to autonomously search the code for opportunities to apply perfective maintenance, apply the perfective maintenance, and evaluate if the source code quality has been improved. Its main contributions are: (i) the refactorings are autonomously done by software agents during the idle development time; (ii) all changes are stored in isolated branches in order to facilitate the communication with the developers; (iii) the refactorings are applied only when the program semantics is preserved; (iv) the agents are able to learn the more suitable sequence of refactorings to improve a specific quality attribute; and (v) the approach can be extended with other metrics and refactorings. This paper also presents a set of experimental studies that provide evidences of the benefits of our approach for software rejuvenation.
104|-||QualityScan scheme for load balancing efficiency in vehicular ad hoc networks (VANETs)|The main terminal devices in vehicular ad-hoc networks (VANETs) are highly mobile moving cars that handoff much more frequently than handheld devices. Nevertheless, frequent handoff or high handoff latency can influence the quality of service (QoS) of real-time network services. Since conventional handoff mechanisms cannot fulfill the requirements of VANET, many fast handoff schemes have been proposed. However, the schemes based on the signal strength of APs (access points) ignore the loading states of different APs and thus cannot utilize the bandwidth effectively. Whenever some APs are very busy, the QoS will be degraded. In order to solve this problem, we can pre-establish the APs and regulate their number according to different traffic types. In this paper, we present a fast handoff scheme for VANET, QualityScan, which decreases the handoff latency and considers loading states of the regional APs simultaneously. By the pre-established AP controller (APC), our scheme gathers the loading states of the APs regularly and predicts network traffic of the next moment. Based on the parameters obtained by passive scanning, the mobile nodes (MNs) can choose the optimal AP for the optimal QoS. According to our simulation analysis, QualityScan not only achieves load balance of the APs, but also improves QoS and handoff efficiency in VANET.
104|-||Automated fault localization via hierarchical multiple predicate switching|Single predicate switching forcibly changes the state of a predicate instance at runtime and then identifies the root cause by examining the switched predicate, called critical predicate. However, switching one predicate instance has its limitations: in our experiments, we found that single predicate switching can only find critical predicates for 88 out of 300 common bugs in five real-life utility programs. For other 212 bugs, overcoming them may require switching multiple predicate instances. Nonetheless, taking all possible combinations of predicate instances into consideration will result in exponential explosion. Therefore, we propose a hierarchical multiple predicate switching technique, called HMPS, to locate faults effectively. Specifically, HMPS restricts the search for critical predicates to the scope of highly suspect functions identified by employing spectrum-based fault localization techniques. Besides, instrumentation methods and strategies for switch combination are presented to facilitate the search for critical predicates. The empirical studies show that HMPS is able to find critical predicates for 111 out of 212 bugs mentioned above through switching multiple predicate instances. In addition, HMPS captures 62% of these 300 bugs when examining up to 1% of the executed code, while the Barinel and Ochiai approaches locate 18% and 16% respectively.
104|-||Sentiment Analysis in monitoring software development processes: An exploratory case study on GitHub's project issues|Software process models, which allow us to develop software products, can be improved by using the corresponding quality model. However, current tendencies in the application of Global Software Engineering and Global Software Development, which forces geographically dispersed teams to collaborate, make the usual monitoring techniques obsolete. This situation has led to looking for new methods that can help in the decision making process, such as the case of the Social Network Analysis field.
104|-||Investigating security threats in architectural context: Experimental evaluations of misuse case maps|Many techniques have been proposed for eliciting software security requirements during the early requirements engineering phase. However, few techniques so far provide dedicated views of security issues in a software systems architecture context. This is a problem, because almost all requirements work today happens in a given architectural context, and understanding this architecture is vital for identifying security vulnerabilities and corresponding mitigations. Misuse case maps attempt to provide an integrated view of security and architecture by augmenting use case maps with misuse case concepts. This paper evaluates misuse case maps through two controlled experiments where 33 and 54 ICT students worked on complex real-life intrusions described in the literature. The students who used misuse case maps showed significantly better understanding of intrusions and better ability to suggest mitigations than students who used a combination of two existing techniques as an alternative treatment. Misuse case maps were also perceived more favourably overall than the alternative treatment, and participants reported using misuse case maps more when solving their tasks.
104|-||Service portfolio management: A repository-based framework|The paper discusses a framework for managing and evaluating ICT-enabled service portfolios along the service design phase. The framework adopts a service reuse perspective and it is made up of i) a model for the representation of a repository of services, ii) a model for the definition of a service portfolio representing current production lines of a service provider organization, iii) a set of metrics for service portfolio evaluation, and iv) a tool supporting managers in decision making for the achievement of design objectives. The proposed metrics and the tool are supposed to allow decision makers to get an improved view of the service design process. Furthermore, the framework supports managers in decision making for the achievement of production objectives as well as operational strategies, resulting in potential reuse initiatives, likewise. To provide evidence of the impacts of the proposed framework, experimental activities are discussed focusing on a real life case study, referring to an Italian small size service provider.
104|-||Modeling the QoS parameters of DDS for event-driven real-time applications|The Data Distribution Service (DDS) standard defines a data-centric distribution middleware that supports the development of distributed real-time systems. To this end, the standard includes a wide set of configurable parameters to provide different degrees of Quality of Service (QoS). This paper presents an analysis of these QoS parameters when DDS is used to build reactive applications normally designed under an event-driven paradigm, and shows how to represent them using the real-time end-to-end flow model defined by the MARTE standard. We also present an application-case study to illustrate the use and modeling of DDS in next-generation distributed real-time systems.
104|-||Scheduling parallel jobs with tentative runs and consolidation in the cloud|Since the success of cloud computing, more and more high performance computing parallel applications run in the cloud. Carefully scheduling parallel jobs is essential for cloud providers to maintain their quality of service. Existing parallel job scheduling mechanisms do not take the parallel workload consolidation into account to improve the scheduling performance. In this paper, after introducing a prioritized two-tier virtual machines architecture for parallel workload consolidation, we propose a consolidation-based parallel job scheduling algorithm. The algorithm employs tentative run and workload consolidation under such a two-tier virtual machines architecture to enhance the popular FCFS algorithm. Extensive experiments on well-known traces show that our algorithm significantly outperforms FCFS, and it can even produce comparable performance to the runtime-estimation-based EASY algorithm, though it does not require users to provide runtime estimation of the job. Moreover, our algorithm allows inaccurate CPU usage estimation and only requires trivial modification on FCFS. It is effective and robust for scheduling parallel workload in the cloud.
104|-||Network coding-based energy-efficient multicast routing algorithm for multi-hop wireless networks|Multi-hop multicast routing can provide better communication performance in multi-hop wireless networks. However, existing multi-hop multicast routing hardly take into account energy efficiency of networks. This paper studies the energy-efficient multicast communication aiming at multi-hop wireless networks. Firstly, we analyze energy metric and energy efficiency metric of multi-hop networks. Then the corresponding models are given. Secondly, network coding is used to improve network throughput. Different from previous methods, we here consider that network nodes are satisfied with a certain random distribution. In such a case, it is a challenge to construct the network structure that network coding requires. For the above random network topology, we propose three basic structures of network coding to overcome this problem. Thirdly, we present a flexible energy-efficient multicast routing algorithm for multi-hop wireless networks to extensively exploit the network structure proposed above to maximize network throughput and decrease network energy consumption. Finally, we perform numerical experiments by network simulation. Simulation results indicate that our approach is significantly promising.
104|-||A semantic approach for designing Assistive Software Recommender systems|Assistive Software offers a solution for people with disabilities to manage specialized hardware, devices or services. However, these users may have difficulties in selecting and installing Assistive Software in their devices for managing smart environments. This paper addresses the requirements of these kinds of systems and their design in the context of interoperability architectures. Our solution follows a semantic approach, for which ontologies are a key. The paper also presents an implementation of our design proposal, i.e., a real and usable system which is evaluated according to a set of functional and non-functional requirements here proposed.
105|-|http://www.sciencedirect.com/science/journal/01641212/105|Service-oriented approach to fault tolerance in CPSs|Cyber-physical systems (CPSs) are open and interconnected embedded systems that control or interact with physical processes. Failures in CPSs can lead to loss of production time, damage to the equipment and environment, or loss of life, meaning that dependability and resilience are key properties for their design. However, existing fault tolerance and safety approaches are inadequate for complex, networked and dynamic CPSs. Service-orientation, on the other hand, is generally considered to be a robust architectural style, but there is a limited amount of research on fault tolerance of service-oriented architecture (SOA), especially on distributed real-time systems. We propose an approach that utilizes the loosely coupled nature of services to implement fault tolerance using a middleware-based real-time SOA (RTSOA) for CPSs. The approach, based on the concepts of fault isolation and recovery at the service level, is empirically evaluated using a demanding bilateral teleoperation (remote handling) application. The empirical evaluation demonstrates that RTSOA supports real-time fault detection and recovery, use of services as a unit of fault isolation, and it provides capability to implement fault tolerance patterns flexibly and without significant overhead.
105|-||Improving multi-objective code-smells correction using development history|One of the widely used techniques to improve the quality of software systems is refactoring. Software refactoring improves the internal structure of the system while preserving its external behavior. These two concerns drive the existing approaches to refactoring automation. However, recent studies demonstrated that these concerns are not enough to produce correct and consistent refactoring solutions. In addition to quality improvement and behavior preservation, studies consider, among others, construct semantics preservation and minimization of changes. From another perspective, development history was proven as a powerful source of knowledge in many maintenance tasks. Still, development history is not widely explored in the context of automated software refactoring. In this paper, we use the development history collected from existing software projects to propose new refactoring solutions taking into account context similarity with situations seen in the past. We propose a multi-objective optimization-based approach to find good refactoring sequences that (1) minimize the number of code-smells, and (2) maximize the use of development history while (3) preserving the construct semantics. To this end, we use the non-dominated sorting genetic algorithm (NSGA-II) to find the best trade-offs between these three objectives. We evaluate our approach using a benchmark composed of five medium and large-size open-source systems and four types of code-smells (Blob, spaghetti code, functional decomposition, and data class). Our experimental results show the effectiveness of our approach, compared to three different state-of-the-art approaches, with more than 85% of code-smells fixed and 86% of suggested refactorings semantically coherent when the change history is used.
105|-||New approaches to usability evaluation in software development: Barefoot and crowdsourcing|Usability evaluations provide software development teams with insights on the degree to which software applications enable users to achieve their goals, how fast these goals can be achieved, how easy an application is to learn and how satisfactory it is in use. Although such evaluations are crucial in the process of developing software systems with a high level of usability, their use is still limited in small and medium-sized software development companies. Many of these companies are e.g. unable to allocate the resources that are needed to conduct a full-fledged usability evaluation in accordance with a conventional approach.
105|-||Emotion-led modelling for people-oriented requirements engineering: The case study of emergency systems|In the field of design, it is accepted that users’ perceptions of systems are influenced by emotion as much as cognition, and functionally-complete products will not be adopted if they do not appeal to emotions. While software engineering methodologies have matured to handle non-functional requirements such as usability, what has not been investigated fully is the emotional needs of people. That is, what do users want to feel, and how do they feel about a system? In this paper, we argue that these emotional desires should be treated as first-class citizens in software engineering methodology, and present preliminary work on including emotions in requirements models using emotional goals. We evaluate these models both with a controlled user study, and on a case study of emergency systems for older people. The results of the controlled user study indicate that people are comfortable interpreting and modifying our models, and view the inclusion of emotions as first-class entities as a positive step in software engineering. The results of our case study indicate that current emergency systems fail to address the emotional needs their users, leading to low adoption and low usage. We conceptualised, designed, and prototyped an improved emergency system, and placed it into the homes of nine older people over a period of approximately two weeks each, showing improved user satisfaction over existing systems.
105|-||Software cost estimating for CMMI Level 5 developers|This article provides analysis results of Capability Maturity Model Integrated Level 5 projects for developers earning the highest level possible, using actual software data from their initial project estimates. Since there were no measures to verify software performance, this level was used a proxy for high quality software. Ordinary least squares regression was used to predict final effort hours with initially estimated variables obviates the need to estimate growth or shrinkage for typical changes occurring in software projects, regardless of software developer (contracted or in-house). The OLS equations, or cost estimating relationship equations, were evaluated by a series of standards: statistical significance, visual inspection, goodness of fit measures, and academically set thresholds for accuracy measures used in software cost estimating: mean magnitude of relative error and prediction (for determining the percentage of records with 25%, or less, based on their magnitude of relative error score). As several initial estimated variables were strongly correlated to the reported final effort hours and each other, each variable was examined separately. Thirty records from software projects completed in 2003–2008 for the highest process maturity level were used to compute statistically significant equations with implicit growth or shrinkage in their make-up.
105|-||SMaRT: A novel framework for addressing range queries over nonlinear trajectories|A spatiotemporal database is a database that manages both space and time information. Common examples include tracking of moving objects, intelligent transportation systems, cellular communications and meteorology monitoring. A spatiotemporal query determines the objects included in a region at a specified period of time between two date-time instants referred as time window. In the context of this work, we present SMaRT: A novel Spatiotemporal Mysql ReTrieval framework, based on MySQL and PostgreSQL database management system. Moreover, we propose a demo user interface that implements all of its capabilities, in order to help user determine the most efficient spatiotemporal query method on user-defined 2D trajectories. To our knowledge, we are the first to study and compare methods of addressing range queries on nonlinear moving object trajectories, that are represented both in dual and native dimensional space. In particular, it is the first time a theoretically efficient dual approach was implemented for nonlinear trajectories and incorporated into a well-known open-source RDBMS. An experimental evaluation is included that shows the performance and efficiency of our approach.
105|-||Input-based adaptive randomized test case prioritization: A local beam search approach|Test case prioritization assigns the execution priorities of the test cases in a given test suite. Many existing test case prioritization techniques assume the full-fledged availability of code coverage data, fault history, or test specification, which are seldom well-maintained in real-world software development projects. This paper proposes a novel family of input-based local-beam-search adaptive-randomized techniques. They make adaptive tree-based randomized explorations with a randomized candidate test set strategy to even out the search space explorations among the branches of the exploration trees constructed by the test inputs in the test suite. We report a validation experiment on a suite of four medium-size benchmarks. The results show that our techniques achieve either higher APFD values than or the same mean APFD values as the existing code-coverage-based greedy or search-based prioritization techniques, including Genetic, Greedy and ART, in both our controlled experiment and case study. Our techniques are also significantly more efficient than the Genetic and Greedy, but are less efficient than ART.
105|-||Design and programming patterns for implementing usability functionalities in web applications|Usability is a software system quality attribute. There are usability issues that have an impact not only on the user interface but also on the core functionality of applications. In this paper, three web applications were developed to discover patterns for implementing two usability functionalities with an impact on core functionality: Abort Operation and Progress Feedback. We applied an inductive process in order to identify reusable elements to implement the selected functionalities. For communication purposes, these elements are specified as design and programming patterns (PHP, VB.NET and Java). Another two web applications were developed in order to evaluate the patterns. The evaluation explores several issues such as ease of pattern understanding and ease of pattern use, as well as the final result of the applications.
106|-|http://www.sciencedirect.com/science/journal/01641212/106|Learning to detect representative data for large scale instance selection|Instance selection is an important data pre-processing step in the knowledge discovery process. However, the dataset sizes of various domain problems are usually very large, and some are even non-stationary, composed of both old data and a large amount of new data samples. Current algorithms for solving this type of scalability problem have certain limitations, meaning they require a very high computational cost over very large scale datasets during instance selection. To this end, we introduce the ReDD (Representative Data Detection) approach, which is based on outlier pattern analysis and prediction. First, a machine learning model, or detector, is used to learn the patterns of (un)representative data selected by a specific instance selection method from a small amount of training data. Then, the detector can be used to detect the rest of the large amount of training data, or newly added data. We empirically evaluate ReDD over 50 domain datasets to examine the effectiveness of the learned detector, using four very large scale datasets for validation. The experimental results show that ReDD not only reduces the computational cost nearly two or three times by three baselines, but also maintains the final classification accuracy.
106|-||Engineering Future Internet applications: The Prime approach|The Future Internet is envisioned as a worldwide environment connecting a large open-ended collection of heterogeneous and autonomous resources, namely Things, Services and Contents, which interact with each other anywhere and anytime. Applications will possibly emerge dynamically as opportunistic aggregation of resources available at a given time, and will be able to self-adapt according to the environment dynamics. In this context, engineers should be provided with proper modeling and programming abstractions to develop applications able to benefit from Future Internet, by being at the same time fluid, as well as dependable. Indeed, such abstractions should (i) facilitate the development of autonomous and independent interacting resources (loose coupling), (ii) deal with the run-time variability of the application in terms of involved resources (flexibility), (iii) provide mechanisms for run-time resources discovery and access (dynamism), and (iv) enable the running application to accommodate unforeseen resources (serendipity).
106|-||Information infrastructure risk prediction through platform vulnerability analysis|The protection of information infrastructures is important for the function of other infrastructure sectors. As vital parts for the information infrastructure operation, software-based platforms, face a series of vulnerabilities and threats. This paper aims to provide a complementary approach to existing vulnerability prediction solutions and launch the measurement of zero-day risk by introducing a risk prediction methodology for an information infrastructure. The proposed methodology consists of four steps and utilizes the outcomes of a proper analysis of security measurements provided by specifications from the Security Content Automation Protocol. First, we identify software platform assets that support an information infrastructure and second we measure the historical rate of vulnerability occurrences. Third, we use a distribution fitting procedure to estimate the statistical correlation between empirical and reference probability distributions and verify the statistical significance of the distribution fitting results with the Kolmogorov–-Smirnov test. Fourth, we develop conditional probability tables that constitute a Bayesian Belief Network topology as means to enable risk prediction and estimation on security properties. The practicality of the risk prediction methodology is demonstrated with an implementation example from the electronic banking sector. The contribution of the proposed methodology is to provide auditors with a proactive approach about zero-day risks.
106|-||Safe evolution templates for software product lines|Software product lines enable generating related software products from reusable assets. Adopting a product line strategy can bring significant quality and productivity improvements. However, evolving a product line can be risky, since it might impact many products. When introducing new features or improving its design, it is important to make sure that the behavior of existing products is not affected. To ensure that, one usually has to analyze different types of artifacts, an activity that can lead to errors. To address this issue, in this work we discover and analyze concrete evolution scenarios from five different product lines. We discover a total of 13 safe evolution templates, which are generic transformations that developers can apply when evolving compositional and annotative product lines, with the goal of preserving the behavior of existing products. We also evaluate the templates by analyzing the evolution history of these product lines. In this evaluation, we observe that the templates can address the modifications that developers performed in the analyzed scenarios, which corroborates the expressiveness of our template set. We also observe that the templates could also have helped to avoid the errors that we identified during our analysis.
106|-||A large-scale study on the usage of Javaâs concurrent programming constructs|In both academia and industry, there is a strong belief that multicore technology will radically change the way software is built. However, little is known about the current state of use of concurrent programming constructs. In this work we present an empirical work aimed at studying the usage of concurrent programming constructs of 2227 real world, stable and mature Java projects from SourceForge. We have studied the usage of concurrent techniques in the most recent versions of these applications and also how usage has evolved along time. The main findings of our study are: (I) More than 75% of the latest versions of the projects either explicitly create threads or employ some concurrency control mechanism. (II) More than half of these projects exhibit at least 47 synchronized methods and 3 implementations of the Runnable interface per 100,000 LoC, which means that not only concurrent programming constructs are used often but they are also employed intensively. (III) The adoption of the java.util.concurrent library is only moderate (approximately 23% of the concurrent projects employ it). (IV) Efficient and thread-safe data structures, such as ConcurrentHashMap, are not yet widely used, despite the fact that they present numerous advantages.
106|-||An exploratory study on exception handling bugs in Java programs|Most mainstream programming languages provide constructs to throw and to handle exceptions. However, several studies argue that exception handling code is usually of poor quality and that it is commonly neglected by developers. Moreover, it is said to be the least understood, documented, and tested part of the implementation of a system. Nevertheless, there are very few studies that analyze the actual exception handling bugs that occur in real software systems or that attempt to understand developers’ perceptions of these bugs. In this work we present an exploratory study on exception handling bugs that employs two complementary approaches: a survey of 154 developers and an analysis of 220 exception handling bugs from the repositories of Eclipse and Tomcat.
106|-||Automated analysis of security requirements through risk-based argumentation|Computer-based systems are increasingly being exposed to evolving security threats, which often reveal new vulnerabilities. A formal analysis of the evolving threats is difficult due to a number of practical considerations such as incomplete knowledge about the design, limited information about attacks, and constraints on organisational resources. In our earlier work on RISA (RIsk assessment in Security Argumentation), we showed that informal risk assessment can complement the formal analysis of security requirements. In this paper, we integrate the formal and informal assessment of security by proposing a unified meta-model and an automated tool for supporting security argumentation called OpenRISA. Using a uniform representation of risks and arguments, our automated checking of formal arguments can identify relevant risks as rebuttals to those arguments, and identify mitigations from publicly available security catalogues when possible. As a result, security engineers are able to make informed and traceable decisions about the security of their computer-based systems. The application of OpenRISA is illustrated with examples from a PIN Entry Device case study.
106|-||The discourse on tool integration beyond technology, a literature survey|The tool integration research area emerged in the 1980s. This survey focuses on those strands of tool integration research that discuss issues beyond technology.
106|-||Feature extraction approaches from natural language requirements for reuse in software product lines: A systematic literature review|Requirements for implemented system can be extracted and reused for a production of a new similar system. Extraction of common and variable features from requirements leverages the benefits of the software product lines engineering (SPLE). Although various approaches have been proposed in feature extractions from natural language (NL) requirements, no related literature review has been published to date for this topic. This paper provides a systematic literature review (SLR) of the state-of-the-art approaches in feature extractions from NL requirements for reuse in SPLE. We have included 13 studies in our synthesis of evidence and the results showed that hybrid natural language processing approaches were found to be in common for overall feature extraction process. A mixture of automated and semi-automated feature clustering approaches from data mining and information retrieval were also used to group common features, with only some approaches coming with support tools. However, most of the support tools proposed in the selected studies were not made available publicly and thus making it hard for practitioners’ adoption. As for the evaluation, this SLR reveals that not all studies employed software metrics as ways to validate experiments and case studies. Finally, the quality assessment conducted confirms that practitioners’ guidelines were absent in the selected studies.
106|-||Toward the tools selection in model based system engineering for embedded systemsâA systematic literature review|Model based system engineering (MBSE) is a systematic approach of modeling which is frequently used to support requirement specification, design, verification and validation activities of system development. However, it is difficult to customize MBSE approach for the development of embedded systems due to their diverse behavioral aspects. Furthermore, appropriate tools selection to perform particular MBSE activities is always challenging. This paper focuses on the identification and classification of recent research practices pertaining to embedded systems development through MBSE approach. Consequently, a comprehensive analysis of various MBSE tools has been presented. Systematic literature review (SLR) has been used to identify 61 research practices published during 2008–2014. The identified researches have been classified into six different categories to analyze various aspects of MBSE approach for embedded systems. Consequently, 39 preliminary tools are identified that have been used in recent researches. Furthermore, classification and evaluation of tools have been presented. This research highlights important trends and approaches of MBSE to support development of embedded systems. A comprehensive investigation of tools in this article facilitates researchers, practitioners and developers to select appropriate tools according to their requirements.
107|.|http://www.sciencedirect.com/science/journal/01641212/107|An experimental investigation on the innate relationship between quality and refactoring|Previous studies have investigated the reasons behind refactoring operations performed by developers, and proposed methods and tools to recommend refactorings based on quality metric profiles, or on the presence of poor design and implementation choices, i.e., code smells. Nevertheless, the existing literature lacks observations about the relations between metrics/code smells and refactoring activities performed by developers. In other words, the characteristics of code components increasing/decreasing their chances of being object of refactoring operations are still unknown. This paper aims at bridging this gap. Specifically, we mined the evolution history of three Java open source projects to investigate whether refactoring activities occur on code components for which certain indicators—such as quality metrics or the presence of smells as detected by tools—suggest there might be need for refactoring operations. Results indicate that, more often than not, quality metrics do not show a clear relationship with refactoring. In other words, refactoring operations are generally focused on code components for which quality metrics do not suggest there might be need for refactoring operations. Finally, 42% of refactoring operations are performed on code entities affected by code smells. However, only 7% of the performed operations actually remove the code smells from the affected class.
107|.||Behavioral software engineering: A definition and systematic literature review|Throughout the history of software engineering, the human aspects have repeatedly been recognized as important. Even though research that investigates them has been growing in the past decade, these aspects should be more generally considered. The main objective of this study is to clarify the research area concerned with human aspects of software engineering and to create a common platform for future research. In order to meet the objective, we propose a definition of the research area behavioral software engineering (BSE) and present results from a systematic literature review based on the definition. The result indicates that there are knowledge gaps in the research area of behavioral software engineering and that earlier research has been focused on a few concepts, which have been applied to a limited number of software engineering areas. The individual studies have typically had a narrow perspective focusing on few concepts from a single unit of analysis. Further, the research has rarely been conducted in collaboration by researchers from both software engineering and social science. Altogether, this review can help put a broader set of human aspects higher on the agenda for future software engineering research and practice.
107|.||The prospects of a quantitative measurement of agility: A validation study on an agile maturity model|Agile development has now become a well-known approach to collaboration in professional work life. Both researchers and practitioners want validated tools to measure agility. This study sets out to validate an agile maturity measurement model with statistical tests and empirical data. First, a pretest was conducted as a case study including a survey and focus group. Second, the main study was conducted with 45 employees from two SAP customers in the US. We used internal consistency (by a Cronbach’s alpha) as the main measure for reliability and analyzed construct validity by exploratory principal factor analysis (PFA). The results suggest a new categorization of a subset of items existing in the tool and provides empirical support for these new groups of factors. However, we argue that more work is needed to reach the point where a maturity models with quantitative data can be said to validly measure agility, and even then, such a measurement still needs to include some deeper analysis with cultural and contextual items.
107|.||Modeling and verification of Functional and Non-Functional Requirements of ambient Self-Adaptive Systems|Self-Adaptive Systems modify their behavior at run-time in response to changing environmental conditions. For these systems, Non-Functional Requirements play an important role, and one has to identify as early as possible the requirements that are adaptable. We propose an integrated approach for modeling and verifying the requirements of Self-Adaptive Systems using Model Driven Engineering techniques. For this, we use Relax, which is a Requirements Engineering language which introduces flexibility in Non-Functional Requirements. We then use the concepts of Goal-Oriented Requirements Engineering for eliciting and modeling the requirements of Self-Adaptive Systems. For properties verification, we use OMEGA2/IFx profile and toolset. We illustrate our proposed approach by applying it on an academic case study.
107|.||A UML model-based approach to detect infeasible paths|UML model-based analysis is gaining wide acceptance for its cost effectiveness and lower overhead for processing compared to code-based analysis. A possible way to enhance the precision of the results of UML based analysis is by detecting infeasible paths in UML models. Our investigation reveals that two interaction patterns called Null Reference Check (NLC) and Mutually Exclusive (MUX) can cause a large number of infeasible paths in UML sequence diagrams. To detect such infeasible paths, we construct a graph model (called SIG), generate MM paths from the graph model, where an MM path refers to an execution sequence of model elements from the start to end of a method scope. Subsequently, we determine infeasibility of the MM paths with respect to MUX and NLC patterns. Our proposed model-based approach is useful to help exclude generation of test cases and test data for prior-detected infeasible paths, refine test effort estimation, and facilitate better test planning in the early stages of software development life cycle.
107|.||Entity resolution based EM for integrating heterogeneous distributed probabilistic data|Distributed computing is linked and equated to the industrial revolution. Its transformational nature is, however, associated with significant instances in the form of internet of thing operations. Entity resolution (ER) is a problem of matching and resolving records that represent the same real world entity. This is a long-standing challenge in distributed databases and information retrieval as a statistic. In a centralized approach, the problem of ER has not been scaled well as large amount of data need to be sent to a central node. In this paper, we present an algorithm which deals with heterogeneous distributed probabilistic data (HDPD) and also reduces processing time in a distributed environment. We propose two different approaches. First, we explore this instance with a matching (identification) problem to integrate different data models with expectation–maximization (EM) algorithm. Second, we apply ER methodology for HDPD to achieve major performance in terms of response time to produce the outcome. We validate HDPD through experiments over a 100-node cluster that records significant performance improvements over naive approaches. This paper is expected to provide insights in to database organizations and new technological development for the growth of distributed environment.
107|.||A comprehensive modeling framework for role-based access control policies|Prohibiting unauthorized access to critical resources and data has become a major requirement for enterprises; access control (AC) mechanisms manage requests from users to access system resources. One of the most used AC paradigms is role-based access control (RBAC), in which access rights are determined based on the user’s role. Many different types of RBAC policies have been proposed in the literature, each one accompanied by the corresponding extension of the original RBAC model. However, there is no unified framework that can be used to define all these types of policies in a coherent way, using a common model. In this paper we propose a model-driven engineering approach, based on UML and the Object Constraint Language (OCL), to enable the precise specification and verification of such policies. More specifically, we first present a taxonomy of the various types of RBAC policies proposed in the literature. We also propose the GemRBAC model, a generalized model for RBAC that includes all the entities required to define the classified policies. This model is a conceptual model that can also serve as data model to operationalize data collection and verification. Lastly, we formalize the classified policies as OCL constraints on the GemRBAC model.
107|.||Service deployment strategies for efficient execution of composite SaaS applications on cloud platform|Cloud computing has caused a revolution in our way of developing and using software. Software development and deployment based on the new models of Software as a Service (SaaS) and Service-Oriented Architecture (SOA) are expected to bring a lot of benefits for users. However, software developers and service providers have to address new challenging issues before such benefits can be realized. This paper explores one of the critical issues, service deployment, for reducing execution time of composite SaaS applications, and proposes an integrated approach to the service deployment problem which takes not only inter-service communication costs but also the potential parallelism among services into consideration. In the approach, two types of graphs are developed to model the communication costs between services, Service Dependency Graph (SDG), and potential parallelism among services, Service Concurrence Graph (SCG), respectively. Then, these two graphs are integrated into a single Service Relationship Graph (SRG) and the service deployment problem is transformed into a minimum k-cut problem for solution. A series of experiments were conducted to evaluate the proposed approach. The experimental results indicate that our approach outperforms previous deployment methods significantly in terms of service response time.
107|.||An automated approach for noise identification to assist software architecture recovery techniques|Software systems’ concrete architecture often drifts from the intended architecture throughout their evolution. Program comprehension activities, like software architecture recovery, become very demanding, especially for large and complex systems due to the existence of noise, which is created by omnipresent and utility classes that obscure the system structure. Omnipresent classes represent crosscutting concerns, utilities or elementary domain concepts. The identification and filtering of noise is a necessary preprocessing step before attempting program comprehension techniques, especially for undocumented systems. In this paper, we propose an automated methodology for noise identification. Our methodology is based on the notion that noisy classes are widely used in a system, directly or indirectly. We combine classes’ usage significance with their participation in the system’s subgraphs, in order to identify the classes that are persistently used. Usage significance is measured according to Component Rank, a well-established metric in the literature, which ranks software artifacts according to their usage significance. The experimental results show that the proposed methodology successfully captures classes that produce noise and improves the results of existing algorithms for software systems’ architectural decomposition.
107|.||Architectural tactics for cyber-foraging: Results of a systematic literature review|Mobile devices have become for many the preferred way of interacting with the Internet, social media and the enterprise. However, mobile devices still do not have the computing power and battery life that will allow them to perform effectively over long periods of time, or for executing applications that require extensive communication, computation, or low latency. Cyber-foraging is a technique to enable mobile devices to extend their computing power and storage by offloading computation or data to more powerful servers located in the cloud or in single-hop proximity. This article presents the results of a systematic literature review (SLR) on architectures that support cyber-foraging. Elements of the identified architectures were codified in the form of Architectural Tactics for Cyber-Foraging. These tactics will help architects extend their design reasoning toward cyber-foraging as a way to support the mobile applications of the present and the future.
107|.||A small world based overlay network for improving dynamic load-balancing|Load-balancing algorithms play a key role in improving the performance of distributed-computing-systems that consist of heterogeneous nodes with different capacities. The performance of load-balancing algorithms and its convergence-rate deteriorate as the number-of-nodes in the system, the network-diameter, and the communication-overhead increase. Moreover, the load-balancing technical-factors significantly affect the performance of rebalancing the load among nodes. Therefore, we propose an approach that improves the performance of load-balancing algorithms by considering the load-balancing technical-factors and the structure of the network that executes the algorithm. We present the design of an overlay network, namely, functional small world (FSW) that facilitates efficient load-balancing in heterogeneous systems. The FSW achieves the efficiency by reducing the number-of-nodes that exchange their information, decreasing the network diameter, minimizing the communication-overhead, and decreasing the time-delay results from the tasks re-migration process. We propose an improved load-balancing algorithm that will be effectively executed within the constructed FSW, where nodes consider the capacity and calculate the average effective-load. We compared our approach with two significant diffusion methods presented in the literature. The simulation results indicate that our approach considerably outperformed the original neighborhood approach and the nearest neighbor approach in terms of response time, throughput, communication overhead, and movements cost.
107|.||An empirically-developed framework for Agile transition and adoption: A Grounded Theory approach|To date, few Agile transition and adoption frameworks have been proposed in the software industry. However, using them is not easy in practice and primarily requires a huge organizational overhead because of their complex and non-flexible structure. These drawbacks make such frameworks difficult to apply in small and medium-sized companies. We have conducted a large-scale empirical research study using Grounded Theory approach with the participation of 49 Agile experts from 13 different countries. This study inductively developed a substantive Agile transition and adoption framework which appears to be simple and flexible. The main aim of this paper is to present the developed framework. The primary characteristics of this framework, including iterative, gradual, continuous, and value-based are in line with the Agile approach and show promise of being useful in software companies and organizations, regardless of size. This paper also describes how various steps of this framework could help software companies to achieve Agile transformation.
